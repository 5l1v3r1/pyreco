__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Slybot documentation build configuration file, created by
# sphinx-quickstart on Thu Aug 22 10:19:15 2013.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = []

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Slybot'
copyright = u'2013, Scrapy team'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.9'
# The full version, including alpha/beta/rc tags.
release = '0.9'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'Slybotdoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'Slybot.tex', u'Slybot Documentation',
   u'Scrapy team', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'slybot', u'Slybot Documentation',
     [u'Scrapy team'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'Slybot', u'Slybot Documentation',
   u'Scrapy team', 'Slybot', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

########NEW FILE########
__FILENAME__ = baseurl
"""
html page utils
"""
import urlparse, re
from scrapely.htmlpage import parse_html, HtmlTagType

ABSURLRE = re.compile("^https?\:\/\/")
DOCTYPERE = re.compile("<!DOCTYPE.*?>", re.S | re.I)

def _is_abs_url(url):
    return bool(ABSURLRE.match(url))

def insert_base_url(html, base):
    """
    Inserts the given base url if does not exist in html source,
    or replace the existing if needed
    """
    baseurl = baseelement = headelement = htmlelement = None
    for element in parse_html(html):
        if getattr(element, "tag", None) == "base":
            baseurl = element.attributes.get("href", None)
            baseelement = element
        elif getattr(element, "tag", None) == "head" and \
                element.tag_type == HtmlTagType.OPEN_TAG:
            headelement = element
        elif getattr(element, "tag", None) == "html" and \
                element.tag_type == HtmlTagType.OPEN_TAG:
            htmlelement = element

    if baseurl:
        if not _is_abs_url(baseurl):
            absurl = urlparse.urljoin(base, baseurl)
            # replace original base tag
            basetag = '<base href="%s" />' % absurl
            html = html[:baseelement.start] + basetag + html[baseelement.end:]

    else:
        # Generate new base element and include
        basetag = '<base href="%s" />' % base
        if headelement:
            insertpos = headelement.end
        else:
            if htmlelement:
                basetag = "\n<head>%s</head>\n" % basetag
                insertpos = htmlelement.end
            else:
                doctype_match = DOCTYPERE.search(html)
                if doctype_match:
                    insertpos = doctype_match.end()
                else:
                    insertpos = 0
        html = html[:insertpos] + basetag + html[insertpos:]

    return html

def get_base_url(htmlpage):
    """Return the base url of the given HtmlPage""" 
    for element in htmlpage.parsed_body:
        if getattr(element, "tag", None) == "base":
            return element.attributes.get("href") or htmlpage.url
    return htmlpage.url

########NEW FILE########
__FILENAME__ = closespider
"""
This extension closes spiders after they have been crawling inefficiently for a
while
Each SLYCLOSE_SPIDER_CHECK_PERIOD seconds, it checks that at least SLYCLOSE_SPIDER_PERIOD_ITEMS
have been extracted along the last time interval of same length.
"""

from twisted.internet import task

from scrapy.xlib.pydispatch import dispatcher
from scrapy import signals
from scrapy.exceptions import NotConfigured

DEFAULT_CHECK_PERIOD = 3600
DEFAULT_PERIOD_MIN_ITEMS = 200

class SlybotCloseSpider(object):
    
    def __init__(self, crawler):
        if not crawler.settings.getbool("SLYCLOSE_SPIDER_ENABLED"):
            raise NotConfigured

        self.crawler = crawler
        self.check_period = crawler.settings.getint("SLYCLOSE_SPIDER_CHECK_PERIOD", DEFAULT_CHECK_PERIOD)
        self.period_items = crawler.settings.getint("SLYCLOSE_SPIDER_PERIOD_ITEMS", DEFAULT_PERIOD_MIN_ITEMS)

        self.items_in_period = 0

        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)
        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)
        dispatcher.connect(self.item_scraped, signal=signals.item_scraped)

    def spider_opened(self, spider):
        self.task = task.LoopingCall(self._check_crawled_items, spider)
        self.task.start(self.check_period, now=False)

    def spider_closed(self, spider):
        if self.task.running:
            self.task.stop()

    def item_scraped(self, item, spider):
        self.items_in_period += 1
            
    def _check_crawled_items(self, spider):
        if self.items_in_period >= self.period_items:
            self.items_in_period = 0
        else:
            spider.log("Closing spider because of low item throughput. Items in last period: %d" % self.items_in_period)
            self.crawler.engine.close_spider(spider, 'slybot_fewitems_scraped')

    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler)

########NEW FILE########
__FILENAME__ = dupefilter
"""
Duplicates filter middleware for autoscraping
"""
from scrapy.exceptions import NotConfigured
from scrapy.exceptions import DropItem

from slybot.item import create_item_version

class DupeFilterPipeline(object):
    def __init__(self, settings):
        if not settings.getbool('SLYDUPEFILTER_ENABLED'):
            raise NotConfigured
        self._itemversion_cache = {}

    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler.settings)

    def process_item(self, item, spider):
        """Checks whether a scrapy item is a dupe, based on version (not vary)
        fields of the item class"""
        if not hasattr(item, 'version_fields'):
            return item
        version = create_item_version(item)
        if version in self._itemversion_cache:
            old_url = self._itemversion_cache[version]
            raise DropItem("Duplicate product scraped at <%s>, first one was scraped at <%s>" % (item["url"], old_url))
        self._itemversion_cache[version] = item["url"]
        return item


########NEW FILE########
__FILENAME__ = extractors
import re

from scrapely.extractors import htmlregion

from slybot.fieldtypes import FieldTypeManager
from slybot.item import SlybotFieldDescriptor

def create_regex_extractor(pattern):
    """Create extractor from a regular expression.
    Only groups from match are extracted and concatenated, so it
    is required to define at least one group. Ex:
    >>> extractor = create_regex_extractor("(\d+).*(\.\d+)")
    >>> extractor(u"The price of this product is <div>45</div> </div class='small'>.50</div> pounds")
    u'45.50'
    """
    ereg = re.compile(pattern, re.S)
    def _extractor(txt):
        m = ereg.search(txt)
        if m:
            return htmlregion(u"".join(filter(None, m.groups() or m.group())))
    
    _extractor.__name__ = "Regex: %s" % pattern.encode("utf-8")
    return _extractor

class PipelineExtractor:
    def __init__(self, *extractors):
        self.extractors = extractors
    
    def __call__(self, value):
        for extractor in self.extractors:
            value = extractor(value) if value else value
        return value

    @property
    def __name__(self):
        return repr(self.extractors) 
    

def apply_extractors(descriptor, template_extractors, extractors):
    field_type_manager = FieldTypeManager()

    for field_name, field_extractors in template_extractors.items():
        equeue = []
        for eid in field_extractors:
            extractor_doc = extractors[eid]
            if "regular_expression" in extractor_doc:
                equeue.append(create_regex_extractor(extractor_doc["regular_expression"]))
            elif "type_extractor" in extractor_doc: # overrides default one
                descriptor.attribute_map[field_name] = SlybotFieldDescriptor(field_name, 
                    field_name, field_type_manager.type_processor_class(extractor_doc["type_extractor"])())
        if not field_name in descriptor.attribute_map:
            # if not defined type extractor, use text type by default, as it is by far the most commonly used
            descriptor.attribute_map[field_name] = SlybotFieldDescriptor(field_name, 
                    field_name, field_type_manager.type_processor_class("text")())
            
        if equeue:
            equeue.insert(0, descriptor.attribute_map[field_name].extractor)
            descriptor.attribute_map[field_name].extractor = PipelineExtractor(*equeue)


########NEW FILE########
__FILENAME__ = images
"""
Images 
"""
from scrapely.extractors import extract_image_url
from slybot.fieldtypes.url import UrlFieldTypeProcessor

class ImagesFieldTypeProcessor(UrlFieldTypeProcessor):
    name = 'image'
    description = 'extracts image URLs'

    def extract(self, text):
        return extract_image_url(text)
        

########NEW FILE########
__FILENAME__ = number
"""
Numeric data extraction
"""

from scrapely.extractors import contains_any_numbers, extract_number

class NumberTypeProcessor(object):
    """NumberTypeProcessor

    Extracts a number from text

    >>> from scrapely.extractors import htmlregion
    >>> n = NumberTypeProcessor()
    >>> n.extract(htmlregion(u"there are no numbers here"))
    >>> n.extract(htmlregion(u"foo 34"))
    u'foo 34'
    >>> n.adapt(u"foo 34", None)
    u'34'

    If more than one number is present, nothing is extracted
    >>> n.adapt(u"34 48", None) is None
    True
    """
    name = 'number'
    description = 'extracts a single number in the text passed'
    
    def extract(self, htmlregion):
        """Only matches and extracts strings with at least one number"""
        return contains_any_numbers(htmlregion.text_content)
        
    def adapt(self, text, htmlpage):
        return extract_number(text)


########NEW FILE########
__FILENAME__ = point

class GeoPointFieldTypeProcessor(object):
    """Renders point with tags"""

    name = 'geopoint'
    description = 'geo point'
    multivalue = True

    def extract(self, value):
        return value

    def adapt(self, value, htmlpage):
        return value


########NEW FILE########
__FILENAME__ = price
"""
Price field types
"""
from scrapely import extractors

class PriceTypeProcessor(object):
    """Extracts price from text"""
    name = "price"
    description = "extracts a price decimal number in the text passed"

    def extract(self, htmlregion):
        return extractors.contains_any_numbers(htmlregion.text_content)

    def adapt(self, text, htmlpage):
        return extractors.extract_price(text)


########NEW FILE########
__FILENAME__ = text
"""
Text types
"""
from scrapely.extractors import text as extract_text, safehtml

class _BaseTextProcessor(object):
    """basic text processor, defines identity functions, some of which 
    are overridden in subclasses
    """
    def extract(self, text):
        """Matches and extracts any string, as it is"""
        return text
    
    def adapt(self, text, htmlpage):
        return text
    
class RawFieldTypeProcessor(_BaseTextProcessor):
    """Extracts the raw data, without processing. Data is escaped for presentation
    
    >>> from scrapely.extractors import htmlregion
    >>> r = RawFieldTypeProcessor()
    >>> html = htmlregion(u'<p>test</p>')
    >>> r.extract(html)
    u'<p>test</p>'
    >>> r.adapt(html, None)
    u'<p>test</p>'
    """
    name = 'raw html'
    description = 'raw html as it appears in the page'

class TextFieldTypeProcessor(_BaseTextProcessor):
    """Extracts strings, removing all HTML markup

    >>> from scrapely.extractors import htmlregion
    >>> p = TextFieldTypeProcessor()
    >>> html = htmlregion(u'<p>test</p><!-- comment --><script> // script</script>!')
    >>> extracted = p.extract(html)
    >>> extracted
    u'test !'
    >>> p.adapt(extracted, None)
    u'test !'
    >>> html = htmlregion(u'<p>&nbsp;\\n<p>')
    >>> p.extract(html)
    u''
    """
    name = 'text'
    description = 'extracts text from web pages, cleaning all markup'
    
    def extract(self, htmlregion):
        return extract_text(htmlregion.text_content)

    
class SafeHtmlFieldTypeProcessor(_BaseTextProcessor):
    """Extracts strings, with only a safe subset of HTML remaining

    Extraction checks for presence of text content, and adapt transforms the HTML
    >>> from scrapely.extractors import htmlregion
    >>> p = SafeHtmlFieldTypeProcessor()
    >>> html = htmlregion(u'<p>test</p> <blink>foo')
    >>> p.extract(html)
    u'<p>test</p> <blink>foo'
    >>> p.adapt(html)
    u'<p>test</p> foo'
    
    html without text must not be extracted
    >>> html = htmlregion(u'<br/>')

    """
    name = 'safe html'
    description = 'removes all but a small subset of html tags'
    def extract(self, htmlregion):
        if extract_text(htmlregion.text_content):
            return htmlregion

    def adapt(self, text, htmlpage=None):
        """Remove html markup"""
        return safehtml(text)


########NEW FILE########
__FILENAME__ = url
from urlparse import urljoin
from scrapy.utils.url import safe_download_url
from scrapy.utils.markup import unquote_markup
from slybot.baseurl import get_base_url

class UrlFieldTypeProcessor(object):
    """Renders URLs as links"""

    name = 'url'
    description = 'URL'
    limit = 80

    def extract(self, text):
        return text

    def adapt(self, text, htmlpage):
        text = text.encode(htmlpage.encoding)
        joined = urljoin(get_base_url(htmlpage).encode(htmlpage.encoding), text)
        return safe_download_url(unquote_markup(joined, encoding=htmlpage.encoding))


########NEW FILE########
__FILENAME__ = generic_form
import re
import itertools
from lxml import html

from scrapy.http.request.form import _get_inputs

class GenericForm:

    def __init__(self, **kwargs):
        self.kwargs = kwargs

    def _pick_node(self, doc, selector):
        nodes = doc.xpath(selector['xpath'])
        if nodes:
            return nodes[0]

    def _filter_by_regex(self, lines, regex):
        search_regex = re.compile(regex).search
        return [l for l in lines if search_regex(l)]

    def _get_field_values(self, form, field_descriptor):
        if 'name' in field_descriptor:
            field_name = field_descriptor['name']
        else:
            select_field = self._pick_node(form, field_descriptor)
            field_name = select_field.name

        field_type = field_descriptor['type']
        if field_type == 'constants':
            return [[field_name, option] for option in self.get_value(field_descriptor)]
        elif field_type == 'iterate':
            select_field = self._pick_node(form, field_descriptor)
            values = self._filter_by_regex(select_field.value_options,
                                           self.get_value(field_descriptor))
            return [[select_field.name, option] for option in values]
        elif field_type == 'inurl':
            return [[field_name, option] for option in field_descriptor['file_values']]

    def get_value(self, field_descriptor):
        values = field_descriptor.get('value', '')
        if isinstance(values, list):
            return [val.format(**self.kwargs) for val in values]
        else:
            return values.format(**self.kwargs)

    def set_values_url_field(self, field_descriptor, body):
        field_descriptor['file_values'] = body.split('\n')

    def get_url_field(self, form_descriptor):
        for i, field_descriptor in enumerate(form_descriptor['fields']):
            if (field_descriptor['type'] == 'inurl'
                and (not 'file_values' in field_descriptor or
                     not field_descriptor['file_values'])):
                yield i, field_descriptor

    def fill_generic_form(self, url, body, form_descriptor):

        doc = html.document_fromstring(body, base_url=url)
        form = self._pick_node(doc, form_descriptor)
        if form is None:
            raise Exception('Generic form not found')

        # Get all the possible inputs for each field
        values = [self._get_field_values(form, field)
                  for field in form_descriptor['fields']]

        for params in itertools.product(*values):
            form_values = dict(_get_inputs(form, None, False, None, None))
            for name, option in params:
                form_values[name] = option
            yield form_values.items(), form.action or form.base_url, form.method

########NEW FILE########
__FILENAME__ = item
import hashlib
from collections import defaultdict

from scrapy.item import DictItem, Field
from scrapely.descriptor import ItemDescriptor, FieldDescriptor

from slybot.fieldtypes import FieldTypeManager

class SlybotItem(DictItem):
    # like DictItem.__setitem__ but doesn't check the field is declared
    def __setitem__(self, name, value):
        self._values[name] = value
    @classmethod
    def create_iblitem_class(cls, schema):
        class IblItem(cls):
            fields = defaultdict(dict)
            version_fields = []
            for _name, _meta in schema['fields'].items():
                fields[_name] = Field(_meta)
                if not _meta.get("vary", False):
                    version_fields.append(_name)
            version_fields = sorted(version_fields)
        return IblItem
   
def create_slybot_item_descriptor(schema):
    field_type_manager = FieldTypeManager()
    descriptors = []
    for pname, pdict in schema['fields'].items():
        required = pdict['required']
        pclass = field_type_manager.type_processor_class(pdict['type'])
        processor = pclass()
        descriptor = SlybotFieldDescriptor(pname, pname, processor, required)
        descriptors.append(descriptor)
    return ItemDescriptor("", "", descriptors)

class SlybotFieldDescriptor(FieldDescriptor):
    """Extends the scrapely field descriptor to use slybot fieldtypes and
    to be created from a slybot item schema
    """

    def __init__(self, name, description, field_type_processor, required=False):
        """Create a new SlybotFieldDescriptor with the given name and description. 
        The field_type_processor is used for extraction and is publicly available
        """
        FieldDescriptor.__init__(self, name, description, 
            field_type_processor.extract, required)
        # add an adapt method
        self.adapt = field_type_processor.adapt

def create_item_version(item):
    """Item version based on hashlib.sha1 algorithm"""
    if not item.version_fields:
        return
    _hash = hashlib.sha1()
    for attrname in item.version_fields:
        _hash.update(repr(item.get(attrname)))
    return _hash.digest()


########NEW FILE########
__FILENAME__ = base
"""
Link extraction for auto scraping
"""
import re, os, posixpath
from urlparse import urlparse
from scrapy.linkextractor import IGNORED_EXTENSIONS

_ONCLICK_LINK_RE = re.compile("(?P<sep>('|\"))(?P<url>.+?)(?P=sep)")

_ignored_exts = frozenset(['.' + e for e in IGNORED_EXTENSIONS])

# allowed protocols
ALLOWED_SCHEMES = frozenset(['http', 'https', None, ''])

class BaseLinkExtractor(object):

    def __init__(self, max_url_len=2083, ignore_extensions=_ignored_exts, 
        allowed_schemes=ALLOWED_SCHEMES):
        """Creates a new LinkExtractor

        The defaults are a good guess for the first time crawl. After that, we
        expect that they can be learned.
        """
        self.max_url_len = max_url_len
        self.ignore_extensions = ignore_extensions
        self.allowed_schemes = allowed_schemes
    
    def _extract_links(self, source):
        raise NotImplementedError

    def links_to_follow(self, source):
        """Returns normalized extracted links"""
        for link in self._extract_links(source):
            link = self.normalize_link(link)
            if link is not None:
                yield link

    def normalize_link(self, link):
        """Normalize a link
        
        >>> from scrapy.link import Link
        >>> le = BaseLinkExtractor()
        >>> l = Link('http://scrapinghub.com/some/path/../dir')
        >>> le.normalize_link(l).url
        'http://scrapinghub.com/some/dir'
        >>> l = Link('http://scrapinghub.com/some//./path/')
        >>> le.normalize_link(l).url
        'http://scrapinghub.com/some/path/'

        Files with disallowed extentions or protocols are not returned
        >>> le.normalize_link(Link('myimage.jpg')) is None
        True
        >>> le.normalize_link(Link('file:///tmp/mydoc.htm')) is None
        True
        >>> le.normalize_link(Link('http://scrapinghub.com')).url
        'http://scrapinghub.com/'
        
        Fragments are removed
        >>> le.normalize_link(Link('http://example.com/#something')).url
        'http://example.com/'
        >>> le.normalize_link(Link('http://example.com/#something')).fragment
        'something'
        >>> le.normalize_link(Link('http://scrapinghub.com#some fragment')).url
        'http://scrapinghub.com/'

        Ajax crawling
        >>> le.normalize_link(Link('http://example.com/#!something')).url
        'http://example.com/?_escaped_fragment_=something'
        >>> le.normalize_link(Link('http://example.com/page.html?arg=1#!something')).url
        'http://example.com/page.html?arg=1&_escaped_fragment_=something'
        """
        if len(link.url) > self.max_url_len:
            return
        parsed = urlparse(link.url)
        extention = os.path.splitext(parsed.path)[1].lower() 
        if parsed.scheme not in self.allowed_schemes or \
                extention in self.ignore_extensions:
            return
        # path normalization
        path = parsed.path or '/'
        path = path if path[0] != '.' else '/' + path
        path = posixpath.normpath(path)
        if parsed.path.endswith('/') and not path.endswith('/'):
            path += '/'
        if parsed.fragment.startswith('!'):
            query = '_escaped_fragment_=%s' % parsed.fragment[1:]
            query = parsed.query + '&' + query if parsed.query else query
            parsed = parsed._replace(query=query)
        link.fragment = parsed.fragment
        if path != parsed.path or parsed.fragment:
            link.url = parsed._replace(path=path, fragment='').geturl()
        return link



########NEW FILE########
__FILENAME__ = ecsv
import csv
from cStringIO import StringIO

from scrapy.link import Link

from .base import BaseLinkExtractor

# see http://docs.python.org/2/library/csv.html#csv-fmt-params
_FORMAT_PARAMETERS = (
    ('delimiter', ','),
    ('quotechar', '"'),
    ('doublequote', True),
    ('escapechar', None),
    ('lineterminator', '\r\n'),
    ('skipinitialspace', False),
    ('strict', False),
)

class CsvLinkExtractor(BaseLinkExtractor):
    def __init__(self, column=0, **kwargs):
        self.fmtparams = dict((key, kwargs.pop(key, default)) for key, default in _FORMAT_PARAMETERS)
        for key, val in self.fmtparams.items():
            if isinstance(val, unicode):
                self.fmtparams[key] = val.encode()
        super(CsvLinkExtractor, self).__init__(**kwargs)
        self.allowed_schemes = filter(lambda x: x and isinstance(x, basestring), self.allowed_schemes)
        self.column = column

    def _extract_links(self, response):
        buff = StringIO(response.body)
        reader = csv.reader(buff, **self.fmtparams)
        for row in reader:
            if len(row) > self.column:
                yield Link(row[self.column])


########NEW FILE########
__FILENAME__ = html
"""
Link extraction for auto scraping
"""
import re
from urlparse import urljoin
from scrapy.utils.markup import remove_entities
from scrapy.link import Link
from scrapy.http import HtmlResponse

from scrapely.htmlpage import HtmlTag, HtmlTagType

from slybot.linkextractor.base import BaseLinkExtractor
from slybot.utils import htmlpage_from_response

_META_REFRESH_CONTENT_RE = re.compile(r"(?P<int>(\d*\.)?\d+)\s*;\s*url=(?P<url>.*)")
_ONCLICK_LINK_RE = re.compile("(?P<sep>('|\"))(?P<url>.+?)(?P=sep)")

class HtmlLinkExtractor(BaseLinkExtractor):
    """Link extraction for auto scraping

    Links (urls and the anchor text) are extracted from HtmlPage objects.

    Some safe normalization is done (always correct, does not make assumptions
    about how the site handles URLs). It allows some customization, which we
    expect to learn for specific websites from the crawl logs.
    """

    def _extract_links(self, response_or_htmlpage):
        """Extract links to follow from an html page

        This uses `iterlinks` to read the links in the page.
        """
        htmlpage = htmlpage_from_response(response_or_htmlpage) if \
                    isinstance(response_or_htmlpage, HtmlResponse) else response_or_htmlpage
        return iterlinks(htmlpage)
 
def iterlinks(htmlpage):
    """Iterate through the links in the HtmlPage passed

    For example:
    >>> from scrapely.htmlpage import HtmlPage
    >>> p = HtmlPage(body=u"Please visit <a href='http://scrapinghub.com/'>Scrapinghub</a>")
    >>> iterlinks(p).next()
    Link(url='http://scrapinghub.com/', text=u'Scrapinghub', fragment='', nofollow=False)
    >>> p = HtmlPage(body=u"Go <a href='home.html'>Home</a>")
    >>> iterlinks(p).next()
    Link(url='home.html', text=u'Home', fragment='', nofollow=False)
    
    When a url is specified, absolute urls are made:
    >>> p.url = 'http://scrapinghub.com/'
    >>> iterlinks(p).next()
    Link(url='http://scrapinghub.com/home.html', text=u'Home', fragment='', nofollow=False)

    Base href attributes in the page are respected
    >>> p.body = u"<html><head><base href='myproject/'/></head><body>see my <a href='index.html'>project</a></body>"
    >>> iterlinks(p).next()
    Link(url='http://scrapinghub.com/myproject/index.html', text=u'project', fragment='', nofollow=False)
    >>> p.body = u"<html><head><base href='http://scrape.io/myproject/'/></head><body>see my <a href='index.html'>project</a></body>"
    >>> iterlinks(p).next()
    Link(url='http://scrape.io/myproject/index.html', text=u'project', fragment='', nofollow=False)

    Frameset and iframe urls are extracted
    >>> p = HtmlPage(body=u"<html><frameset><frame src=frame1.html><frame src=frame2.html></frameset><iframe src='iframe.html'/></html>")
    >>> [l.url for l in iterlinks(p)]
    ['frame1.html', 'frame2.html', 'iframe.html']
    
    As are meta refresh tags:
    >>> p = HtmlPage(body=u"<html><head><meta http-equiv='refresh' content='5;url=http://example.com/' />")
    >>> iterlinks(p).next().url
    'http://example.com/'
    
    nofollow is set to True if the link has a rel='nofollow' attribute:
    >>> p = HtmlPage(body=u"<a href='somewhere.html' rel='nofollow'>somewhere</a>")
    >>> list(iterlinks(p))
    [Link(url='somewhere.html', text=u'somewhere', fragment='', nofollow=True)]
    
    It does not require well formed HTML and behaves similar to many browsers
    >>> p = HtmlPage(body=u"<a href='foo'>foo <a href=bar>bar</a><a href='baz'/>baz")
    >>> list(iterlinks(p))
    [Link(url='foo', text=u'foo ', fragment='', nofollow=False), Link(url='bar', text=u'bar', fragment='', nofollow=False), Link(url='baz', text=u'baz', fragment='', nofollow=False)]

    Leading and trailing whitespace should be removed, including in base href
    >>> p = HtmlPage(body=u"<head><base href=' foo/ '/></head><a href='bar '/>baz")
    >>> list(iterlinks(p))
    [Link(url='foo/bar', text=u'baz', fragment='', nofollow=False)]

    Test standard onclick links
    >>> p = HtmlPage(url="http://www.example.com", body=u"<html><td onclick=window.open('page.html?productid=23','win2') >")
    >>> list(iterlinks(p))
    [Link(url='http://www.example.com/page.html?productid=23', text=None, fragment='', nofollow=False)]

    >>> p = HtmlPage("http://www.example.com", body=u"<html><a href='#' onclick=window.open('page.html?productid=24','win2') >")
    >>> list(iterlinks(p))
    [Link(url='http://www.example.com/page.html?productid=24', text=None, fragment='', nofollow=False)]

    >>> p = HtmlPage(body=u"<html><div onclick=window.location.href='http://www.jungleberry.co.uk/Fair-Trade-Earrings/Aguas-Earrings.htm'>")
    >>> list(iterlinks(p))
    [Link(url='http://www.jungleberry.co.uk/Fair-Trade-Earrings/Aguas-Earrings.htm', text=None, fragment='', nofollow=False)]

    Onclick with no href
    >>> p = HtmlPage("http://www.example.com", body=u"<html><a onclick=window.open('page.html?productid=24','win2') >")
    >>> list(iterlinks(p))
    [Link(url='http://www.example.com/page.html?productid=24', text=None, fragment='', nofollow=False)]

    Dont generate link when target is an anchor
    >>> p = HtmlPage("http://www.example.com", body=u"<html><a href='#section1' >")
    >>> list(iterlinks(p))
    []

    Extract links from <link> tags in page header
    >>> p = HtmlPage("http://example.blogspot.com/", body=u"<html><head><link rel='me' href='http://www.blogger.com/profile/987372' /></head><body>This is my body!</body></html>")
    >>> list(iterlinks(p))
    [Link(url='http://www.blogger.com/profile/987372', text=None, fragment='', nofollow=False)]
    """
    base_href = remove_entities(htmlpage.url, encoding=htmlpage.encoding)
    def mklink(url, anchortext=None, nofollow=False):
        url = url.strip()
        fullurl = urljoin(base_href, remove_entities(url, encoding=htmlpage.encoding))
        return Link(fullurl.encode(htmlpage.encoding), text=anchortext, nofollow=nofollow)

    # iter to quickly scan only tags
    tag_iter = (t for t in htmlpage.parsed_body if isinstance(t, HtmlTag))

    # parse body
    astart = ahref = None
    nofollow = False
    for nexttag in tag_iter:
        tagname = nexttag.tag
        attributes = nexttag.attributes
        if tagname == 'a' and (nexttag.tag_type == HtmlTagType.CLOSE_TAG or attributes.get('href') \
                    and not attributes.get('href', '').startswith('#')):
            if astart:
                yield mklink(ahref, htmlpage.body[astart:nexttag.start], nofollow)
                astart = ahref = None
                nofollow = False
            href = attributes.get('href')
            if href:
                ahref = href
                astart = nexttag.end
                nofollow = attributes.get('rel') == u'nofollow'
        elif tagname == 'head':
            # scan ahead until end of head section
            for nexttag in tag_iter:
                tagname = nexttag.tag
                if (tagname == 'head' and \
                        nexttag.tag_type == HtmlTagType.CLOSE_TAG) or \
                        tagname == 'body':
                    break
                if tagname == 'base':
                    href = nexttag.attributes.get('href')
                    if href:
                        joined_base = urljoin(htmlpage.url, href.strip(),
                            htmlpage.encoding)
                        base_href = remove_entities(joined_base, 
                            encoding=htmlpage.encoding)
                elif tagname == 'meta':
                    attrs = nexttag.attributes
                    if attrs.get('http-equiv') == 'refresh':
                        m = _META_REFRESH_CONTENT_RE.search(attrs.get('content', ''))
                        if m:
                            target = m.group('url')
                            if target:
                                yield mklink(target)
                elif tagname == 'link':
                    href = nexttag.attributes.get('href')
                    if href:
                        yield mklink(href)
        elif tagname == 'area':
            href = attributes.get('href')
            if href:
                nofollow = attributes.get('rel') == u'nofollow'
                yield mklink(href, attributes.get('alt', ''), nofollow)
        elif tagname in ('frame', 'iframe'):
            target = attributes.get('src')
            if target:
                yield mklink(target)
        elif 'onclick' in attributes:
            match = _ONCLICK_LINK_RE.search(attributes["onclick"] or "")
            if not match:
                continue
            target = match.group("url")
            nofollow = attributes.get('rel') == u'nofollow'
            yield mklink(target, nofollow=nofollow)

    if astart:
        yield mklink(ahref, htmlpage.body[astart:])



########NEW FILE########
__FILENAME__ = regex
import re
from scrapy.link import Link

from .base import BaseLinkExtractor

# Based on http://blog.mattheworiordan.com/post/13174566389/url-regular-expression-for-links-with-or-without-the
# leaves aside the fragment part, not needed for link extraction
URL_DEFAULT_REGEX = r'(?:[A-Za-z0-9.\-]+|(?:www.|[-;:&=\+\$,\w]+@)[A-Za-z0-9.\-]+)(?:(?:\/[\+~%\/.\w\-_]*)?\??(?:[\-\+=&;%@.\w_]*)(?:#[.\!\/\w]*)?)?'

class RegexLinkExtractor(BaseLinkExtractor):
    def __init__(self, regex=None, **kwargs):
        super(RegexLinkExtractor, self).__init__(**kwargs)
        self.allowed_schemes = filter(lambda x: x and isinstance(x, basestring), self.allowed_schemes)
        regex = regex or '(?:%s)://%s' % ('|'.join(self.allowed_schemes), URL_DEFAULT_REGEX)
        self.regex = re.compile(regex)

    def _extract_links(self, response):
        """First extract regex groups(). If empty, extracts from regex group()"""
        for s in self.regex.finditer(response.body):
            if s.groups():
                for url in s.groups():
                    yield Link(url)
            else:
                yield Link(s.group())



########NEW FILE########
__FILENAME__ = xml
"""
Link extraction for auto scraping
"""
from scrapy.link import Link
from scrapy.selector import XmlXPathSelector

from slybot.linkextractor.base import BaseLinkExtractor

class XmlLinkExtractor(BaseLinkExtractor):
    """Link extractor for XML sources"""
    def __init__(self, xpath, **kwargs):
        self.remove_namespaces = kwargs.pop('remove_namespaces', False)
        super(XmlLinkExtractor, self).__init__(**kwargs)
        self.xpath = xpath

    def _extract_links(self, response):
        xxs = XmlXPathSelector(response)
        if self.remove_namespaces:
            xxs.remove_namespaces()
        for url in xxs.select(self.xpath).extract():
            yield Link(url.encode(response.encoding))

class RssLinkExtractor(XmlLinkExtractor):
    """Link extraction from RSS feeds"""
    def __init__(self, **kwargs):
        super(RssLinkExtractor, self).__init__("//item/link/text()", **kwargs)

class SitemapLinkExtractor(XmlLinkExtractor):
    """Link extraction for sitemap.xml feeds"""
    def __init__(self, **kwargs):
        kwargs['remove_namespaces'] = True
        super(SitemapLinkExtractor, self).__init__("//urlset/url/loc/text() | //sitemapindex/sitemap/loc/text()", **kwargs)

class AtomLinkExtractor(XmlLinkExtractor):
     def __init__(self, **kwargs):
        kwargs['remove_namespaces'] = True
        super(AtomLinkExtractor, self).__init__("//link/@href", **kwargs)


########NEW FILE########
__FILENAME__ = settings
SPIDER_MANAGER_CLASS = 'slybot.spidermanager.SlybotSpiderManager'
EXTENSIONS = {'slybot.closespider.SlybotCloseSpider': 1}
ITEM_PIPELINES = ['slybot.dupefilter.DupeFilterPipeline']
SPIDER_MIDDLEWARES = {'slybot.spiderlets.SpiderletsMiddleware': 999} # as close as possible to spider output
SLYDUPEFILTER_ENABLED = True
PROJECT_DIR = 'slybot-project'

try:
    from local_slybot_settings import *
except ImportError:
    pass

########NEW FILE########
__FILENAME__ = spider
import itertools
import operator
import re
from copy import deepcopy

from scrapy import log
from scrapy.http import Request, HtmlResponse, FormRequest
try:
    from scrapy.spider import Spider
except ImportError:
    # BaseSpider class was deprecated in Scrapy 0.21
    from scrapy.spider import BaseSpider as Spider

from scrapely.htmlpage import HtmlPage, dict_to_page
from scrapely.extraction import InstanceBasedLearningExtractor

from loginform import fill_login_form

from slybot.item import SlybotItem, create_slybot_item_descriptor
from slybot.extractors import apply_extractors
from slybot.utils import iter_unique_scheme_hostname, htmlpage_from_response
from slybot.linkextractor import HtmlLinkExtractor, RssLinkExtractor, create_linkextractor_from_specs
from slybot.generic_form import GenericForm

def _process_extracted_data(extracted_data, item_descriptor, htmlpage):
    processed_data = []
    for exdict in extracted_data or ():
        processed_attributes = []
        for key, value in exdict.items():
            if key == "variants":
                processed_attributes.append(("variants", _process_extracted_data(value, item_descriptor, htmlpage)))
            elif not key.startswith("_sticky"):
                field_descriptor = item_descriptor.attribute_map.get(key)
                if field_descriptor:
                    value = [field_descriptor.adapt(x, htmlpage) for x in value]
                processed_attributes.append((key, value))
        processed_data.append(processed_attributes)
    return [dict(p) for p in processed_data]

class IblSpider(Spider):

    def __init__(self, name, spec, item_schemas, all_extractors, **kw):
        super(IblSpider, self).__init__(name, **kw)
        spec = deepcopy(spec)
        for key, val in kw.items():
            if isinstance(val, basestring) and key in ['start_urls', 'exclude_patterns', 'follow_patterns', 'allowed_domains']:
                val = val.splitlines()
            spec[key] = val

        self._item_template_pages = sorted((
            [t['scrapes'], dict_to_page(t, 'annotated_body'),
            t.get('extractors', [])] \
            for t in spec['templates'] if t.get('page_type', 'item') == 'item'
        ), key=lambda pair: pair[0])

        # generate ibl extractor for links pages
        _links_pages = [dict_to_page(t, 'annotated_body')
                for t in spec['templates'] if t.get('page_type') == 'links']
        _links_item_descriptor = create_slybot_item_descriptor({'fields': {}})
        self._links_ibl_extractor = InstanceBasedLearningExtractor([(t, _links_item_descriptor) for t in _links_pages]) \
                if _links_pages else None

        self._ipages = [page for _, page, _ in self._item_template_pages]

        self.html_link_extractor = HtmlLinkExtractor()
        self.rss_link_extractor = RssLinkExtractor()
        self.build_url_filter(spec)

        self.itemcls_info = {}
        for itemclass_name, triplets in itertools.groupby(self._item_template_pages, operator.itemgetter(0)):
            page_extractors_pairs = map(operator.itemgetter(1, 2), triplets)
            schema = item_schemas[itemclass_name]
            item_cls = SlybotItem.create_iblitem_class(schema)

            page_descriptor_pairs = []
            for page, template_extractors in page_extractors_pairs:
                item_descriptor = create_slybot_item_descriptor(schema)
                apply_extractors(item_descriptor, template_extractors, all_extractors)
                page_descriptor_pairs.append((page, item_descriptor))

            extractor = InstanceBasedLearningExtractor(page_descriptor_pairs)

            self.itemcls_info[itemclass_name] = {
                'class': item_cls,
                'descriptor': item_descriptor,
                'extractor': extractor,
            }

        self.login_requests = []
        self.form_requests = []
        self._start_requests = []
        self.generic_form = GenericForm(**kw)
        self._create_init_requests(spec.get("init_requests", []))
        self._process_start_urls(spec)
        self.allowed_domains = spec.get('allowed_domains',
                                        self._get_allowed_domains(self._ipages))
        if not self.allowed_domains:
            self.allowed_domains = None

    def _process_start_urls(self, spec):
        self.start_urls = spec.get('start_urls')
        for url in self.start_urls:
            self._start_requests.append(Request(url, callback=self.parse, dont_filter=True))

    def _create_init_requests(self, spec):
        for rdata in spec:
            if rdata["type"] == "login":
                request = Request(url=rdata.pop("loginurl"), meta=rdata,
                                  callback=self.parse_login_page, dont_filter=True)
                self.login_requests.append(request)
            elif rdata["type"] == "form":
                self.form_requests.append(self.get_generic_form_start_request(rdata))
            elif rdata["type"] == "start":
                self._start_requests.append(self._create_start_request_from_specs(rdata))

    def parse_login_page(self, response):
        username = response.request.meta["username"]
        password = response.request.meta["password"]
        args, url, method = fill_login_form(response.url, response.body, username, password)
        return FormRequest(url, method=method, formdata=args, callback=self.after_login, dont_filter=True)

    def after_login(self, response):
        for result in self.parse(response):
            yield result
        for req in self._start_requests:
            yield req

    def get_generic_form_start_request(self, form_descriptor):
        file_fields = list(self.generic_form.get_url_field(form_descriptor))
        if file_fields:
            (field_index, field_descriptor) = file_fields.pop(0)
            form_descriptor['field_index'] = field_index
            return FormRequest(self.generic_form.get_value(field_descriptor), meta=form_descriptor,
                              callback=self.parse_field_url_page, dont_filter=True)
        else:
            return Request(url=form_descriptor.pop("form_url"), meta=form_descriptor,
                                  callback=self.parse_form_page, dont_filter=True)

    def parse_field_url_page(self, response):
        form_descriptor = response.request.meta
        field_index = form_descriptor['field_index']
        field_descriptor = form_descriptor['fields'][field_index]
        self.generic_form.set_values_url_field(field_descriptor, response.body)
        yield self.get_generic_form_start_request(form_descriptor)

    def parse_form_page(self, response):
        try:
            for (args, url, method) in self.generic_form.fill_generic_form(response.url,
                                                                           response.body,
                                                                           response.request.meta):
                yield FormRequest(url, method=method, formdata=args,
                                  callback=self.after_form_page, dont_filter=True)
        except Exception, e:
            self.log(str(e), log.WARNING)
        for req in self._start_requests:
            yield req

    def after_form_page(self, response):
        for result in self.parse(response):
            yield result

    def _get_allowed_domains(self, templates):
        urls = [x.url for x in templates]
        urls += [x.url for x in self._start_requests]
        return [x[1] for x in iter_unique_scheme_hostname(urls)]

    def _requests_to_follow(self, htmlpage):
        if self._links_ibl_extractor is not None:
            extracted = self._links_ibl_extractor.extract(htmlpage)[0]
            if extracted:
                extracted_regions = extracted[0].get('_links', [])
                seen = set()
                for region in extracted_regions:
                    htmlregion = HtmlPage(htmlpage.url, htmlpage.headers, region, encoding=htmlpage.encoding)
                    for request in self._request_to_follow_from_region(htmlregion):
                        if request.url in seen:
                            continue
                        seen.add(request.url)
                        yield request
        else:
            for request in self._request_to_follow_from_region(htmlpage):
                yield request

    def _request_to_follow_from_region(self, htmlregion):
        seen = set()
        for link in self.html_link_extractor.links_to_follow(htmlregion):
            request = self._filter_link(link, seen)
            if request is not None:
                yield request

    def _filter_link(self, link, seen):
        url = link.url
        if self.url_filterf(link):
            # filter out duplicate urls, later we should handle link text
            if url not in seen:
                seen.add(url)
                request = Request(url)
                if link.text:
                    request.meta['link_text'] = link.text
                return request

    def start_requests(self):
        start_requests = []
        if self.login_requests:
            start_requests = self.login_requests
        elif self.form_requests:
            start_requests = self.form_requests
        else:
            start_requests = self._start_requests
        for req in start_requests:
            yield req

    def _create_start_request_from_specs(self, info):
        url = info["url"]
        lspecs = info.get("link_extractor")
        if lspecs:
            linkextractor = create_linkextractor_from_specs(lspecs)
            def _callback(spider, response):
                for link in linkextractor.links_to_follow(response):
                    yield Request(url=link.url, callback=spider.parse)
            return Request(url=url, callback=_callback)
        return Request(url=url, callback=self.parse)

    def parse(self, response):
        """Main handler for all downloaded responses"""
        content_type = response.headers.get('Content-Type', '')
        if isinstance(response, HtmlResponse):
            return self.handle_html(response)
        elif "application/rss+xml" in content_type:
            return self.handle_rss(response)
        else:
            self.log("Ignoring page with content-type=%r: %s" % (content_type, \
                response.url), level=log.DEBUG)
            return []

    def _process_link_regions(self, htmlpage, link_regions):
        """Process link regions if any, and generate requests"""
        if link_regions:
            for link_region in link_regions:
                htmlregion = HtmlPage(htmlpage.url, htmlpage.headers, \
                        link_region, encoding=htmlpage.encoding)
                for request in self._requests_to_follow(htmlregion):
                    yield request
        else:
            for request in self._requests_to_follow(htmlpage):
                yield request

    def handle_rss(self, response):
        seen = set()
        for link in self.rss_link_extractor.links_to_follow(response):
            request = self._filter_link(link, seen)
            if request:
                yield request

    def handle_html(self, response):
        htmlpage = htmlpage_from_response(response)
        items, link_regions = self.extract_items(htmlpage)
        for item in items:
            yield item
        for request in self._process_link_regions(htmlpage, link_regions):
            yield request

    def extract_items(self, htmlpage):
        """This method is also called from UI webservice to extract items"""
        items = []
        link_regions = []
        for item_cls_name, info in self.itemcls_info.iteritems():
            item_descriptor = info['descriptor']
            extractor = info['extractor']
            extracted, _link_regions = self._do_extract_items_from(
                    htmlpage,
                    item_descriptor,
                    extractor,
                    item_cls_name,
            )
            items.extend(extracted)
            link_regions.extend(_link_regions)
        return items, link_regions

    def _do_extract_items_from(self, htmlpage, item_descriptor, extractor, item_cls_name):
        extracted_data, template = extractor.extract(htmlpage)
        link_regions = []
        for ddict in extracted_data or []:
            link_regions.extend(ddict.pop("_links", []))
        processed_data = _process_extracted_data(extracted_data, item_descriptor, htmlpage)
        items = []
        item_cls = self.itemcls_info[item_cls_name]['class']
        for processed_attributes in processed_data:
            item = item_cls(processed_attributes)
            item['url'] = htmlpage.url
            item['_type'] = item_cls_name
            item['_template'] = str(template.id)
            items.append(item)

        return items, link_regions

    def build_url_filter(self, spec):
        """make a filter for links"""
        respect_nofollow = spec.get('respect_nofollow', True)
        patterns = spec.get('follow_patterns')
        if spec.get("links_to_follow") == "none":
            url_filterf = lambda x: False
        elif patterns:
            pattern = patterns[0] if len(patterns) == 1 else "(?:%s)" % '|'.join(patterns)
            follow_pattern = re.compile(pattern)
            if respect_nofollow:
                url_filterf = lambda x: follow_pattern.search(x.url) and not x.nofollow
            else:
                url_filterf = lambda x: follow_pattern.search(x.url)
        elif respect_nofollow:
            url_filterf = lambda x: not x.nofollow
        else:
            url_filterf = bool
        # apply exclude patterns
        exclude_patterns = spec.get('exclude_patterns')
        if exclude_patterns:
            pattern = exclude_patterns[0] if len(exclude_patterns) == 1 else "(?:%s)" % '|'.join(exclude_patterns)
            exclude_pattern = re.compile(pattern)
            self.url_filterf = lambda x: not exclude_pattern.search(x.url) and url_filterf(x)
        else:
            self.url_filterf = url_filterf


########NEW FILE########
__FILENAME__ = spiderlets
"""
Spider middleware for AS for completing the work made by AS with a "spiderlet" code

"""
import pkgutil, inspect

from scrapy.xlib.pydispatch import dispatcher
from scrapy import signals
from scrapy.exceptions import NotConfigured
from scrapy.http import Request

class DefaultSpiderlet(object):
    name = None
    def __init__(self, spider):
        self.spider = spider
    def process_request(self, request, response):
        return request
    def process_item(self, item, response):
        return item
    def process_start_request(self, request):
        return request

    def parse_login_page(self, response):
        return self.spider.parse_login_page(response)

def list_spiderlets(spiderlets_module_path):
    spiderlets_module = __import__(spiderlets_module_path, {}, {}, [''])
    seen_classes = set()
    for _, mname, _  in pkgutil.iter_modules(spiderlets_module.__path__):
        module = __import__(".".join([spiderlets_module_path, mname]), {}, {}, [''])
        for cls in [c for c in vars(module).itervalues() if inspect.isclass(c)]:
            if cls in seen_classes:
                continue
            seen_classes.add(cls)
            name = getattr(cls, 'name', None)
            if name:
                yield cls

def _load_spiderlet(spiderlets_module_path, spider):
    for cls in list_spiderlets(spiderlets_module_path):
        if cls.name == spider.name:
            class _spiderlet_cls(cls, DefaultSpiderlet):
                pass
            spider.log("SpiderletMiddleware: loaded %s" % _spiderlet_cls.name)
            return _spiderlet_cls(spider)
    return DefaultSpiderlet(spider)

class SpiderletsMiddleware(object):
    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler.settings)

    def __init__(self, settings):
        self.annotating = "annotating" in settings.getlist('SHUB_JOB_TAGS')
        self.spiderlets_module_path = settings["SPIDERLETS_MODULE"]
        if not self.spiderlets_module_path:
            raise NotConfigured
        dispatcher.connect(self.spider_opened, signals.spider_opened)

    def spider_opened(self, spider):
        self.spiderlet = _load_spiderlet(self.spiderlets_module_path, spider)
        
    def process_spider_output(self, response, result, spider):
        for item_or_request in result:
            if isinstance(item_or_request, Request):
                yield self.spiderlet.process_request(item_or_request, response)
            else:
                yield self.spiderlet.process_item(item_or_request, response)

    def process_start_requests(self, start_requests, spider):
        for request in start_requests:
            if request.callback == spider.parse_login_page:
                request.callback = self.spiderlet.parse_login_page
            yield self.spiderlet.process_start_request(request)
    

########NEW FILE########
__FILENAME__ = spidermanager
import tempfile, shutil, atexit
from zipfile import ZipFile

from zope.interface import implements
from scrapy.interfaces import ISpiderManager
from scrapy.utils.misc import load_object

from slybot.spider import IblSpider
from slybot.utils import open_project_from_dir

class SlybotSpiderManager(object):

    implements(ISpiderManager)

    def __init__(self, datadir, spider_cls=None):
        self.spider_cls = load_object(spider_cls) if spider_cls else IblSpider
        self._specs = open_project_from_dir(datadir)

    @classmethod
    def from_crawler(cls, crawler):
        datadir = crawler.settings['PROJECT_DIR']
        spider_cls = crawler.settings['SLYBOT_SPIDER_CLASS']
        return cls(datadir, spider_cls)

    def create(self, name, **args):
        spec = self._specs["spiders"][name]
        items = self._specs["items"]
        extractors = self._specs["extractors"]
        return self.spider_cls(name, spec, items, extractors, **args)

    def list(self):
        return self._specs["spiders"].keys()

class ZipfileSlybotSpiderManager(SlybotSpiderManager):

    def __init__(self, datadir, zipfile=None, spider_cls=None):
        if zipfile:
            datadir = tempfile.mkdtemp(prefix='slybot-')
            ZipFile(zipfile).extractall(datadir)
            atexit.register(shutil.rmtree, datadir)
        super(ZipfileSlybotSpiderManager, self).__init__(datadir, spider_cls)

    @classmethod
    def from_crawler(cls, crawler):
        s = crawler.settings
        sm = cls(s['PROJECT_DIR'], s['PROJECT_ZIPFILE'], s['SLYBOT_SPIDER_CLASS'])
        return sm

########NEW FILE########
__FILENAME__ = test_baseurl
"""
Tests for apply_annotations
"""

from unittest import TestCase
from slybot.baseurl import insert_base_url, get_base_url
from scrapely.htmlpage import HtmlPage

class TestApplyAnnotations(TestCase):
    def test_insert_base_relative(self):
        """Replace relative base href"""
        html_in = '<html><head><base href="products/"><body></body></html>'
        html_target = '<html><head><base href="http://localhost:8000/products/" />\
<body></body></html>'
        html_out = insert_base_url(html_in, "http://localhost:8000/")
        self.assertEqual(html_out, html_target)

    def test_insert_base_noreplace(self):
        """base tag dont need to be replaced"""
        html_in = html_target = '<html><head><base href="http://localhost:8000/products/"><body></body></html>'
        html_out = insert_base_url(html_in, "http://localhost:8000/users/blog.html")
        self.assertEqual(html_out, html_target)
        
    def test_insert_base_addbase(self):
        """add base tag when not present"""
        html_in = '<html><head><meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">\
<body></body></html>'
        html_target = '<html><head><base href="http://localhost:8000/" />\
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">\
<body></body></html>'
        html_out = insert_base_url(html_in, "http://localhost:8000/")
        self.assertEqual(html_out, html_target)

    def test_insert_base_commented(self):
        """Test weird case when base tag is commented in origin"""
        html_in = '<html><head><!-- <base href="http://example.com/"> --></head>\
<body>Body</body></html>'
        html_target = '<html><head><base href="http://example.com/" />\
<!-- <base href="http://example.com/"> --></head><body>Body</body></html>'
        html_out = insert_base_url(html_in, "http://example.com/")
        self.assertEqual(html_out, html_target)

    def test_insert_base_nohead(self):
        """Test base insert when no head element is present"""
        html_in = '<html><body>Body</body></html>'
        html_target = '<html>\n\
<head><base href="http://localhost:8000/" /></head>\n\
<body>Body</body></html>'
        html_out = insert_base_url(html_in, "http://localhost:8000/")
        self.assertEqual(html_out, html_target)

    def test_get_base_url(self):
        """Basic get_base_url test"""
        html = u'<html><head><base href="http://example.com/products/" />\
<body></body></html>'
        page = HtmlPage("http://example.com/products/p19.html", body=html)
        self.assertEqual(get_base_url(page), "http://example.com/products/")

    def test_get_base_url_nobase(self):
        """Base tag does not exists"""
        html = u'<html><head><body></body></html>'
        page = HtmlPage("http://example.com/products/p19.html", body=html)
        self.assertEqual(get_base_url(page), "http://example.com/products/p19.html")

    def test_get_base_url_empty_basehref(self):
        """Base tag exists but href is empty"""
        html = u'<html><head><base href="" />\
<body></body></html>'
        url = "http://example.com/products/p19.html"
        page = HtmlPage(url, body=html)
        self.assertEqual(get_base_url(page), url)



########NEW FILE########
__FILENAME__ = test_dupefilter
from unittest import TestCase
from os.path import dirname

from scrapy.http import HtmlResponse
from scrapy.settings import Settings
from scrapy.item import DictItem
from scrapy.exceptions import DropItem

from slybot.spidermanager import SlybotSpiderManager
from slybot.dupefilter import DupeFilterPipeline

_PATH = dirname(__file__)

class DupeFilterTest(TestCase):
    smanager = SlybotSpiderManager("%s/data/SampleProject" % _PATH)

    def test_dupefilter(self):
        name = "seedsofchange2"
        spider = self.smanager.create(name)
        spec = self.smanager._specs["spiders"][name]
        t1, t2 = spec["templates"]

        dupefilter = DupeFilterPipeline(Settings({"SLYDUPEFILTER_ENABLED": True}))

        response1 = HtmlResponse(url=t1["url"], body=t1["original_body"].encode('utf-8'))
        response2 = HtmlResponse(url=t2["url"], body=t2["original_body"].encode('utf-8'))

        result1 = spider.handle_html(response1)
        for item1 in result1:
            if isinstance(item1, DictItem):
                break

        result2 = spider.handle_html(response2)
        for item2 in result2:
            if isinstance(item2, DictItem):
                break
       
        self.assertEqual(item1, dupefilter.process_item(item1, spider))
        self.assertEqual(item2, dupefilter.process_item(item2, spider))

        self.assertRaises(DropItem, dupefilter.process_item, item1, spider)


########NEW FILE########
__FILENAME__ = test_extractors
from unittest import TestCase

from scrapely.htmlpage import HtmlPage
from scrapely.extraction import InstanceBasedLearningExtractor

from slybot.extractors import create_regex_extractor, apply_extractors
from slybot.fieldtypes import TextFieldTypeProcessor
from slybot.item import create_slybot_item_descriptor


class ExtractorTest(TestCase):

    annotated = u"""
<tr data-scrapy-annotate="{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;gender&quot;}}">
<th class="item-key">Gender</th>
<td >Male</td></tr>"""
    _target =  u"""
<tr>
<th class="item-key">Gender</th>
<td >Male</td></tr>"""
    annotated2 = u"""
<tr data-scrapy-annotate="{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}">
<th class="item-key">Name</th>
<td >John</td></tr>
<span data-scrapy-annotate="{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;gender&quot;}}">Male</span>"""
    _target2 =  u"""
<body>
<tr>
<th class="item-key">Name</th><td>Olivia</td></tr>
<span></span>
</body>"""

    template = HtmlPage(url="http://www.test.com/", body=annotated)
    target = HtmlPage(url="http://www.test.com/", body=_target)
    template2 = HtmlPage(url="http://www.test.com/", body=annotated2)
    target2 = HtmlPage(url="http://www.test.com/a", body=_target2)

    def test_regex_extractor(self):
        extractor = create_regex_extractor("(\d+).*(\.\d+)")
        extracted = extractor(u"The price of this product is <div>45</div> </div class='small'>.50</div> pounds")
        self.assertEqual(extracted, u"45.50")
        processor = TextFieldTypeProcessor()
        self.assertEqual(processor.adapt(extracted, None), u"45.50")

    def test_raw_type_w_regex(self):
        schema = {
            'fields': {
                'gender': {
                    'required': False,
                    'type': 'raw',
                    'vary': False,
                }
            }
        }
        descriptor = create_slybot_item_descriptor(schema)
        extractors =  {1: {
                        "regular_expression": "Gender.*(<td\s*>(?:Male|Female)</td>)"
        }}
        apply_extractors(descriptor, {"gender": [1]}, extractors)

        ibl_extractor = InstanceBasedLearningExtractor([(self.template, descriptor)])
        self.assertEqual(ibl_extractor.extract(self.target)[0][0], {u'gender': [u'<td >Male</td>']})

    def test_negative_hit_w_regex(self):
        schema = {
            'fields': {
                'gender': {
                    'required': False,
                    'type': 'number',
                    'vary': False,
                }
            }
        }
        descriptor = create_slybot_item_descriptor(schema)
        extractors =  {1: {
                        "regular_expression": "Gender\\s+(Male|Female)"
        }}
        apply_extractors(descriptor, {"gender": [1]}, extractors)
        
        ibl_extractor = InstanceBasedLearningExtractor([(self.template, descriptor)])
        self.assertEqual(ibl_extractor.extract(self.target)[0], None)
      
    def test_text_type_w_regex(self):
        schema = {
            "fields": {
                'gender': {
                    'required': False,
                    'type': 'text',
                    'vary': False,
                }
            }
        }
        descriptor = create_slybot_item_descriptor(schema)
        extractors =  {1: {
                        "regular_expression": "Gender\\s+(Male|Female)"
        }}
        apply_extractors(descriptor, {"gender": [1]}, extractors)
        
        ibl_extractor = InstanceBasedLearningExtractor([(self.template, descriptor)])
        self.assertEqual(ibl_extractor.extract(self.target)[0][0], {u'gender': [u'Male']})

    def test_type_extractor(self):
        schema = {
            "fields": {
                'gender': {
                    'required': False,
                    'type': 'number',
                    'vary': False,
                }
            }
        }
        descriptor = create_slybot_item_descriptor(schema)
        extractors =  {
                    1: {
                        "type_extractor": "text"
                    },
                    2: {
                        "regular_expression": "Gender\\s+(Male|Female)"
                    }
        }
        apply_extractors(descriptor, {"gender": [1, 2]}, extractors)
        
        ibl_extractor = InstanceBasedLearningExtractor([(self.template, descriptor)])
        self.assertEqual(ibl_extractor.extract(self.target)[0][0], {u'gender': [u'Male']})

    def test_default_type_extractor(self):
        schema = {
            'fields': {}
        }
        descriptor = create_slybot_item_descriptor(schema)
        extractors =  {
                    1: {
                        "regular_expression": "Gender\\s+(Male|Female)"
                    }
        }
        apply_extractors(descriptor, {"gender": [1]}, extractors)
        
        ibl_extractor = InstanceBasedLearningExtractor([(self.template, descriptor)])
        self.assertEqual(ibl_extractor.extract(self.target)[0][0], {u'gender': [u'Male']})

    def test_text_type_w_regex_and_no_groups(self):
        schema = {
            'fields': {
                'gender': {
                    'required': False,
                    'type': 'text',
                    'vary': False,
                }
            }
        }
        descriptor = create_slybot_item_descriptor(schema)
        extractors =  {1: {
                        "regular_expression": "Gender"
        }}
        apply_extractors(descriptor, {"gender": [1]}, extractors)
        
        ibl_extractor = InstanceBasedLearningExtractor([(self.template, descriptor)])
        self.assertEqual(ibl_extractor.extract(self.target)[0][0], {u'gender': [u'Gender']})

    def test_extractor_w_empty_string_extraction(self):
        schema = {
            'fields': {
                'gender': {
                    'required': False,
                    'type': 'text',
                    'vary': False,
                },
                'name': {
                    'required': True,
                    'type': 'text',
                    'vary': False,
                }
            }
        }
        descriptor = create_slybot_item_descriptor(schema)
        extractors =  {
                    1: {
                        "regular_expression": "([0-9]+)"
                    }
        }
        apply_extractors(descriptor, {"gender": [1]}, extractors)
        
        ibl_extractor = InstanceBasedLearningExtractor([(self.template2, descriptor)])
        self.assertEqual(ibl_extractor.extract(self.target2)[0][0], {u'name': [u'Name Olivia']})
        

########NEW FILE########
__FILENAME__ = test_fieldtypes
from unittest import TestCase
from scrapely.htmlpage import HtmlPage

from slybot.fieldtypes import UrlFieldTypeProcessor

class FieldTypesUrlEncoding(TestCase):
    def test_not_standard_chars_in_url(self):
        body = u'<html><meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" /></html>'
        url = u'fotos/produtos/Mam\xe3e noel.jpg'
        htmlpage = HtmlPage(url=u"http://www.example.com/", body=body, encoding='cp1252')
        processor = UrlFieldTypeProcessor()
        self.assertEqual(processor.adapt(url, htmlpage), u'http://www.example.com/fotos/produtos/Mam%C3%A3e%20noel.jpg')


########NEW FILE########
__FILENAME__ = test_generic_form
import json
from os.path import dirname, join
from unittest import TestCase

from slybot.generic_form import GenericForm

_PATH = dirname(__file__)

class GenericFormTest(TestCase):

    def test_simple_search_form(self):
        url = 'http://www.ebay.com/sch/ebayadvsearch/?rt=nc'
        body = open(join(_PATH, "data", "ebay_advanced_search.html")).read()
        form_descriptor = json.loads("""{
            "type": "form",
            "form_url": "http://http://www.ebay.com/sch/ebayadvsearch/?rt=nc",
            "xpath": "//form[@name='adv_search_from']",
            "fields": [
                {
                  "xpath": ".//*[@name='_nkw']",
                  "type": "constants",
                  "value": ["Cars"]
                }
            ]
        }""")

        generic_form = GenericForm()
        start_requests = list(generic_form.fill_generic_form(url, body, form_descriptor))
        expected_requests = [([('_adv', '1'), ('_ex_kw', ''), ('_ftrv', '1'), ('_ftrt', '901'), ('_sabdlo', u''), ('_sabdhi', u''), ('_sop', '12'), ('_samihi', u''), ('_ipg', '50'), ('_salic', '1'), ('_sasl', ''), ('_udlo', ''), ('_okw', u''), ('_fsradio', '&LH_SpecificSeller=1'), ('_udhi', ''), ('_in_kw', '1'), ('_nkw', u'Cars'), ('_sacat', '0'), ('_oexkw', u''), ('_dmd', '1'), ('_saslop', '1'), ('_samilow', u'')], 'http://www.ebay.com/sch/i.html', 'GET')]
        self.assertEqual(start_requests, expected_requests)

    def test_simple_search_form_2_values(self):
        url = 'http://www.ebay.com/sch/ebayadvsearch/?rt=nc'
        body = open(join(_PATH, "data", "ebay_advanced_search.html")).read()
        form_descriptor = json.loads("""{
            "type": "form",
            "form_url": "http://http://www.ebay.com/sch/ebayadvsearch/?rt=nc",
            "xpath": "//form[@name='adv_search_from']",
            "fields": [
                {
                  "xpath": ".//*[@name='_nkw']",
                  "type": "constants",
                  "value": ["Cars", "Boats"]
                }
            ]
        }""")

        generic_form = GenericForm()
        start_requests = list(generic_form.fill_generic_form(url, body, form_descriptor))
        expected_requests = [([('_adv', '1'), ('_ex_kw', ''), ('_ftrv', '1'), ('_ftrt', '901'), ('_sabdlo', u''), ('_sabdhi', u''), ('_sop', '12'), ('_samihi', u''), ('_ipg', '50'), ('_salic', '1'), ('_sasl', ''), ('_udlo', ''), ('_okw', u''), ('_fsradio', '&LH_SpecificSeller=1'), ('_udhi', ''), ('_in_kw', '1'), ('_nkw', u'Cars'), ('_sacat', '0'), ('_oexkw', u''), ('_dmd', '1'), ('_saslop', '1'), ('_samilow', u'')], 'http://www.ebay.com/sch/i.html', 'GET'), ([('_adv', '1'), ('_ex_kw', ''), ('_ftrv', '1'), ('_ftrt', '901'), ('_sabdlo', u''), ('_sabdhi', u''), ('_sop', '12'), ('_samihi', u''), ('_ipg', '50'), ('_salic', '1'), ('_sasl', ''), ('_udlo', ''), ('_okw', u''), ('_fsradio', '&LH_SpecificSeller=1'), ('_udhi', ''), ('_in_kw', '1'), ('_nkw', u'Boats'), ('_sacat', '0'), ('_oexkw', u''), ('_dmd', '1'), ('_saslop', '1'), ('_samilow', u'')], 'http://www.ebay.com/sch/i.html', 'GET')]
        self.assertEqual(start_requests, expected_requests)

    def test_advanced_search_form(self):
        url = 'http://www.ebay.com/sch/ebayadvsearch/?rt=nc'
        body = open(join(_PATH, "data", "ebay_advanced_search.html")).read()
        form_descriptor = json.loads("""{
            "type": "form",
            "form_url": "http://http://www.ebay.com/sch/ebayadvsearch/?rt=nc",
            "xpath": "//form[@name='adv_search_from']",
            "fields": [
                {
                  "xpath": ".//*[@name='_nkw']",
                  "type": "constants",
                  "value": ["Cars"]
                },
                {
                  "xpath": ".//*[@name='_in_kw']",
                  "type": "iterate"
                }
            ]
        }""")

        generic_form = GenericForm()
        start_requests = list(generic_form.fill_generic_form(url, body, form_descriptor))
        expected_requests = [([('_adv', '1'), ('_ex_kw', ''), ('_ftrv', '1'), ('_ftrt', '901'), ('_sabdlo', u''), ('_sabdhi', u''), ('_sop', '12'), ('_samihi', u''), ('_ipg', '50'), ('_salic', '1'), ('_sasl', ''), ('_udlo', ''), ('_okw', u''), ('_fsradio', '&LH_SpecificSeller=1'), ('_udhi', ''), ('_in_kw', '1'), ('_nkw', u'Cars'), ('_sacat', '0'), ('_oexkw', u''), ('_dmd', '1'), ('_saslop', '1'), ('_samilow', u'')], 'http://www.ebay.com/sch/i.html', 'GET'), ([('_adv', '1'), ('_ex_kw', ''), ('_ftrv', '1'), ('_ftrt', '901'), ('_sabdlo', u''), ('_sabdhi', u''), ('_sop', '12'), ('_samihi', u''), ('_ipg', '50'), ('_salic', '1'), ('_sasl', ''), ('_udlo', ''), ('_okw', u''), ('_fsradio', '&LH_SpecificSeller=1'), ('_udhi', ''), ('_in_kw', '2'), ('_nkw', u'Cars'), ('_sacat', '0'), ('_oexkw', u''), ('_dmd', '1'), ('_saslop', '1'), ('_samilow', u'')], 'http://www.ebay.com/sch/i.html', 'GET'), ([('_adv', '1'), ('_ex_kw', ''), ('_ftrv', '1'), ('_ftrt', '901'), ('_sabdlo', u''), ('_sabdhi', u''), ('_sop', '12'), ('_samihi', u''), ('_ipg', '50'), ('_salic', '1'), ('_sasl', ''), ('_udlo', ''), ('_okw', u''), ('_fsradio', '&LH_SpecificSeller=1'), ('_udhi', ''), ('_in_kw', '3'), ('_nkw', u'Cars'), ('_sacat', '0'), ('_oexkw', u''), ('_dmd', '1'), ('_saslop', '1'), ('_samilow', u'')], 'http://www.ebay.com/sch/i.html', 'GET'), ([('_adv', '1'), ('_ex_kw', ''), ('_ftrv', '1'), ('_ftrt', '901'), ('_sabdlo', u''), ('_sabdhi', u''), ('_sop', '12'), ('_samihi', u''), ('_ipg', '50'), ('_salic', '1'), ('_sasl', ''), ('_udlo', ''), ('_okw', u''), ('_fsradio', '&LH_SpecificSeller=1'), ('_udhi', ''), ('_in_kw', '4'), ('_nkw', u'Cars'), ('_sacat', '0'), ('_oexkw', u''), ('_dmd', '1'), ('_saslop', '1'), ('_samilow', u'')], 'http://www.ebay.com/sch/i.html', 'GET')]
        self.assertEqual(start_requests, expected_requests)

    def test_advanced_search_form_regex(self):
        url = 'http://www.ebay.com/sch/ebayadvsearch/?rt=nc'
        body = open(join(_PATH, "data", "ebay_advanced_search.html")).read()
        form_descriptor = json.loads("""{
            "type": "form",
            "form_url": "http://http://www.ebay.com/sch/ebayadvsearch/?rt=nc",
            "xpath": "//form[@name='adv_search_from']",
            "fields": [
                {
                  "xpath": ".//*[@name='_nkw']",
                  "type": "constants",
                  "value": ["Cars"]
                },
                {
                  "xpath": ".//*[@name='_in_kw']",
                  "type": "iterate",
                  "value": "[1-2]"
                }
            ]
        }""")

        generic_form = GenericForm()
        start_requests = list(generic_form.fill_generic_form(url, body, form_descriptor))
        expected_requests = [([('_adv', '1'), ('_ex_kw', ''), ('_ftrv', '1'), ('_ftrt', '901'), ('_sabdlo', u''), ('_sabdhi', u''), ('_sop', '12'), ('_samihi', u''), ('_ipg', '50'), ('_salic', '1'), ('_sasl', ''), ('_udlo', ''), ('_okw', u''), ('_fsradio', '&LH_SpecificSeller=1'), ('_udhi', ''), ('_in_kw', '1'), ('_nkw', u'Cars'), ('_sacat', '0'), ('_oexkw', u''), ('_dmd', '1'), ('_saslop', '1'), ('_samilow', u'')], 'http://www.ebay.com/sch/i.html', 'GET'), ([('_adv', '1'), ('_ex_kw', ''), ('_ftrv', '1'), ('_ftrt', '901'), ('_sabdlo', u''), ('_sabdhi', u''), ('_sop', '12'), ('_samihi', u''), ('_ipg', '50'), ('_salic', '1'), ('_sasl', ''), ('_udlo', ''), ('_okw', u''), ('_fsradio', '&LH_SpecificSeller=1'), ('_udhi', ''), ('_in_kw', '2'), ('_nkw', u'Cars'), ('_sacat', '0'), ('_oexkw', u''), ('_dmd', '1'), ('_saslop', '1'), ('_samilow', u'')], 'http://www.ebay.com/sch/i.html', 'GET')]
        self.assertEqual(start_requests, expected_requests)


    def test_simple_search_form_with_named_parameter(self):
        url = 'http://www.ebay.com/sch/ebayadvsearch/?rt=nc'
        body = open(join(_PATH, "data", "ebay_advanced_search.html")).read()
        form_descriptor = json.loads("""{
            "type": "form",
            "form_url": "http://http://www.ebay.com/sch/ebayadvsearch/?rt=nc",
            "xpath": "//form[@name='adv_search_from']",
            "fields": [
                {
                  "name": "my_param",
                  "type": "constants",
                  "value": ["Cars"]
                }
            ]
        }""")

        generic_form = GenericForm()
        start_requests = list(generic_form.fill_generic_form(url, body, form_descriptor))
        expected_requests = [([('_adv', '1'), ('_ex_kw', ''), ('_ftrv', '1'), ('_ftrt', '901'), ('_sabdlo', u''), ('_sabdhi', u''), ('_sop', '12'), ('_samihi', u''), ('_ipg', '50'), ('_salic', '1'), (u'my_param', u'Cars'), ('_sasl', ''), ('_udlo', ''), ('_okw', u''), ('_fsradio', '&LH_SpecificSeller=1'), ('_udhi', ''), ('_in_kw', '1'), ('_nkw', ''), ('_sacat', '0'), ('_oexkw', u''), ('_dmd', '1'), ('_saslop', '1'), ('_samilow', u'')], 'http://www.ebay.com/sch/i.html', 'GET')]
        self.assertEqual(start_requests, expected_requests)

    def test_simple_search_form_with_file_type(self):
        url = 'http://www.ebay.com/sch/ebayadvsearch/?rt=nc'
        body = open(join(_PATH, "data", "ebay_advanced_search.html")).read()
        form_descriptor = json.loads("""{
            "type": "form",
            "form_url": "http://http://www.ebay.com/sch/ebayadvsearch/?rt=nc",
            "xpath": "//form[@name='adv_search_from']",
            "fields": [
                {
                  "name": "my_param",
                  "type": "inurl",
                  "value": "file://%s/test_params.txt",
                  "file_values": ["Cars", "Boats", "Houses", "Electronics"]
                }
            ]
        }""" % join(_PATH, "data"))

        generic_form = GenericForm()
        start_requests = list(generic_form.fill_generic_form(url, body, form_descriptor))
        expected_requests = [([('_adv', '1'), ('_ex_kw', ''), ('_ftrv', '1'), ('_ftrt', '901'), ('_sabdlo', u''), ('_sabdhi', u''), ('_sop', '12'), ('_samihi', u''), ('_ipg', '50'), ('_salic', '1'), (u'my_param', u'Cars'), ('_sasl', ''), ('_udlo', ''), ('_okw', u''), ('_fsradio', '&LH_SpecificSeller=1'), ('_udhi', ''), ('_in_kw', '1'), ('_nkw', ''), ('_sacat', '0'), ('_oexkw', u''), ('_dmd', '1'), ('_saslop', '1'), ('_samilow', u'')], 'http://www.ebay.com/sch/i.html', 'GET'), ([('_adv', '1'), ('_ex_kw', ''), ('_ftrv', '1'), ('_ftrt', '901'), ('_sabdlo', u''), ('_sabdhi', u''), ('_sop', '12'), ('_samihi', u''), ('_ipg', '50'), ('_salic', '1'), (u'my_param', u'Boats'), ('_sasl', ''), ('_udlo', ''), ('_okw', u''), ('_fsradio', '&LH_SpecificSeller=1'), ('_udhi', ''), ('_in_kw', '1'), ('_nkw', ''), ('_sacat', '0'), ('_oexkw', u''), ('_dmd', '1'), ('_saslop', '1'), ('_samilow', u'')], 'http://www.ebay.com/sch/i.html', 'GET'), ([('_adv', '1'), ('_ex_kw', ''), ('_ftrv', '1'), ('_ftrt', '901'), ('_sabdlo', u''), ('_sabdhi', u''), ('_sop', '12'), ('_samihi', u''), ('_ipg', '50'), ('_salic', '1'), (u'my_param', u'Houses'), ('_sasl', ''), ('_udlo', ''), ('_okw', u''), ('_fsradio', '&LH_SpecificSeller=1'), ('_udhi', ''), ('_in_kw', '1'), ('_nkw', ''), ('_sacat', '0'), ('_oexkw', u''), ('_dmd', '1'), ('_saslop', '1'), ('_samilow', u'')], 'http://www.ebay.com/sch/i.html', 'GET'), ([('_adv', '1'), ('_ex_kw', ''), ('_ftrv', '1'), ('_ftrt', '901'), ('_sabdlo', u''), ('_sabdhi', u''), ('_sop', '12'), ('_samihi', u''), ('_ipg', '50'), ('_salic', '1'), (u'my_param', u'Electronics'), ('_sasl', ''), ('_udlo', ''), ('_okw', u''), ('_fsradio', '&LH_SpecificSeller=1'), ('_udhi', ''), ('_in_kw', '1'), ('_nkw', ''), ('_sacat', '0'), ('_oexkw', u''), ('_dmd', '1'), ('_saslop', '1'), ('_samilow', u'')], 'http://www.ebay.com/sch/i.html', 'GET')]
        self.assertEqual(start_requests, expected_requests)


########NEW FILE########
__FILENAME__ = test_linkextractors
from unittest import TestCase
from scrapy.http import TextResponse, HtmlResponse

from slybot.linkextractor import (
        create_linkextractor_from_specs,
        RssLinkExtractor,
        SitemapLinkExtractor,
)

class Test_RegexLinkExtractor(TestCase):
    def test_default(self):
        specs = {"type": "regex", "value": ''}
        lextractor = create_linkextractor_from_specs(specs)
        text = "Hello http://www.example.com/path, more text https://aws.amazon.com/product?id=23#tre?"
        response = TextResponse(url='http://www.example.com/', body=text)
        links = list(lextractor.links_to_follow(response))
        self.assertEqual(len(links), 2)
        self.assertEqual(links[0].url, 'http://www.example.com/path')
        self.assertEqual(links[1].url, 'https://aws.amazon.com/product?id=23')
        
    def test_custom(self):
        specs = {"type": "regex", "value": 'url: ((?:http|https)://www.example.com/[\w/]+)'}
        lextractor = create_linkextractor_from_specs(specs)
        text = "url: http://www.example.com/path, more text url: https://www.example.com/path2. And more text url: https://aws.amazon.com/product?id=23#tre"
        response = TextResponse(url='http://www.example.com/', body=text)
        links = list(lextractor.links_to_follow(response))
        self.assertEqual(len(links), 2)
        self.assertEqual(links[0].url, 'http://www.example.com/path')
        self.assertEqual(links[1].url, 'https://www.example.com/path2')

    def test_custom_withargs(self):
        specs = {"type": "regex", "value": 'url: ((?:http|https)://www.example.com/[\w/]+)', 'allowed_schemes': ['http']}
        lextractor = create_linkextractor_from_specs(specs)
        text = "url: http://www.example.com/path, more text url: https://www.example.com/path2. And more text url: https://aws.amazon.com/product?id=23#tre"
        response = TextResponse(url='http://www.example.com/', body=text)
        links = list(lextractor.links_to_follow(response))
        self.assertEqual(len(links), 1)
        self.assertEqual(links[0].url, 'http://www.example.com/path')

xmlfeed = """<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
<channel>
    <title>RSS Title</title>
    <description>This is an example of an RSS feed</description>
    <link>http://www.someexamplerssdomain.com/main.html</link>
    <lastBuildDate>Mon, 06 Sep 2010 00:01:00 +0000 </lastBuildDate>
    <pubDate>Mon, 06 Sep 2009 16:20:00 +0000 </pubDate>
    <ttl>1800</ttl>

    <item>
        <title>Example entry</title>
        <description>Here is some text containing an interesting description.</description>
        <link>http://www.wikipedia.org/</link>
        <guid>unique string per item</guid>
        <pubDate>Mon, 06 Sep 2009 16:20:00 +0000 </pubDate>
    </item>
                            
</channel>
</rss>"""

sitemapfeed = """
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9" 
	xmlns:image="http://www.sitemaps.org/schemas/sitemap-image/1.1"
        xmlns:video="http://www.sitemaps.org/schemas/sitemap-video/1.1">

<url><loc>http://www.accommodationforstudents.com/</loc><changefreq>daily</changefreq><priority>1.00</priority></url>
<url><loc>http://www.accommodationforstudents.com/London.asp</loc><changefreq>daily</changefreq><priority>1.00</priority></url>
<url><loc>http://www.accommodationforstudents.com/createaccounts.asp</loc><changefreq>daily</changefreq><priority>0.85</priority></url>
</urlset>
"""

sitemapindex = """
<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
    <sitemap>
        <loc>http://www.example.com/sitemap1.xml.gz</loc>
        <lastmod>2004-10-01T18:23:17+00:00</lastmod>
    </sitemap>
</sitemapindex>
"""

atomfeed = """
<?xml version="1.0" encoding="utf-8"?>
 
<feed xmlns="http://www.w3.org/2005/Atom">
  
    <title>Example Feed</title>
    <subtitle>A subtitle.</subtitle>
    <link href="http://example.org/feed/" rel="self" />
    <link href="http://example.org/" />
    
    <entry>
        <title>Atom-Powered Robots Run Amok</title>
        <link href="http://example.org/2003/12/13/atom03" />
        <summary>Some text.</summary>
        <author>
            <name>John Doe</name>
            <email>johndoe@example.com</email>
        </author>
    </entry>
</feed>
"""

class Test_XmlLinkExtractors(TestCase):
    def setUp(self):
        self.response = TextResponse(url='http://www.example.com/', body=xmlfeed)
        self.sitemap = TextResponse(url='http://www.example.com/sitemap.xml', body=sitemapfeed)
        self.sitemapindex = TextResponse(url='http://www.example.com/sitemap.xml', body=sitemapindex)
        self.atom = TextResponse(url='http://www.example.com/atom', body=atomfeed)

    def test_rss(self):
        specs = {"type": "rss", "value": ""}
        lextractor = create_linkextractor_from_specs(specs)
        links = list(lextractor.links_to_follow(self.response))
        self.assertEqual(len(links), 1)
        self.assertEqual(links[0].url, 'http://www.wikipedia.org/')

    def test_xml(self):
        specs = {"type": "xpath", "value": "//item/link/text()"}
        lextractor = create_linkextractor_from_specs(specs)
        links = list(lextractor.links_to_follow(self.response))
        self.assertEqual(len(links), 1)
        self.assertEqual(links[0].url, 'http://www.wikipedia.org/')

    def test_sitemap(self):
        specs = {"type": "sitemap", "value": ""}
        lextractor = create_linkextractor_from_specs(specs)
        links = list(lextractor.links_to_follow(self.sitemap))
        self.assertEqual(len(links), 3)
        self.assertEqual(links[0].url, 'http://www.accommodationforstudents.com/')

        links = list(lextractor.links_to_follow(self.sitemapindex))
        self.assertEqual(len(links), 1)
        self.assertEqual(links[0].url, 'http://www.example.com/sitemap1.xml.gz')

    def test_atom(self):
        specs = {"type": "atom", "value": ""}
        lextractor = create_linkextractor_from_specs(specs)
        links = list(lextractor.links_to_follow(self.atom))
        self.assertEqual(len(links), 3)
        self.assertEqual(links[0].url, 'http://example.org/feed/')

    def test_xml_remove_namespaces(self):
        specs = {"type": "xpath", "value": "//link/@href", "remove_namespaces": True}
        lextractor = create_linkextractor_from_specs(specs)
        links = list(lextractor.links_to_follow(self.atom))
        self.assertEqual(len(links), 3)
        self.assertEqual(links[0].url, 'http://example.org/feed/')

csvfeed = """
My feed

Product A,http://www.example.com/path,A
Product B,http://www.example.com/path2,B
"""

csvfeed2 = """
My feed

Product A|http://www.example.com/path|A
Product B|http://www.example.com/path2|B
"""

csvfeed3 = """
My feed

name,url,id
Product A,http://www.example.com/path,A
Product B,http://www.example.com/path2,B
"""

class Test_CsvLinkExtractor(TestCase):
    def test_simple(self):
        specs = {"type": "column", "value": 1}
        lextractor = create_linkextractor_from_specs(specs)
        response = TextResponse(url='http://www.example.com/', body=csvfeed)
        links = list(lextractor.links_to_follow(response))
        self.assertEqual(len(links), 2)
        self.assertEqual(links[0].url, 'http://www.example.com/path')
        self.assertEqual(links[1].url, 'http://www.example.com/path2')

    def test_extra_params(self):
        specs = {"type": "column", "value": 1, "delimiter": "|"}
        lextractor = create_linkextractor_from_specs(specs)
        response = TextResponse(url='http://www.example.com/', body=csvfeed2)
        links = list(lextractor.links_to_follow(response))
        self.assertEqual(len(links), 2)
        self.assertEqual(links[0].url, 'http://www.example.com/path')
        self.assertEqual(links[1].url, 'http://www.example.com/path2')

    def test_header(self):
        specs = {"type": "column", "value": 1}
        lextractor = create_linkextractor_from_specs(specs)
        response = TextResponse(url='http://www.example.com/', body=csvfeed3)
        links = list(lextractor.links_to_follow(response))
        self.assertEqual(len(links), 2)
        self.assertEqual(links[0].url, 'http://www.example.com/path')
        self.assertEqual(links[1].url, 'http://www.example.com/path2')

html = """
<a href="http://www.example.com/path">Click here</a>
"""

class Test_HtmlLinkExtractor(TestCase):
    def test_simple(self):
        specs = {"type": "html", "value": None}
        lextractor = create_linkextractor_from_specs(specs)
        response = HtmlResponse(url='http://www.example.com/', body=html)
        links = list(lextractor.links_to_follow(response))
        self.assertEqual(len(links), 1)
        self.assertEqual(links[0].url, 'http://www.example.com/path')
        self.assertEqual(links[0].text, 'Click here')


########NEW FILE########
__FILENAME__ = test_schema_validation
import re

from unittest import TestCase
from os.path import dirname, join

from slybot.validation.schema import get_schema_validator, \
            ValidationError, validate_project_schema
from slybot.utils import open_project_from_dir

_TEST_PROJECT_DIR = join(dirname(__file__), "data/SampleProject")

class JsonSchemaTest(TestCase):

    def assertRaisesRegexp(self, eclass, pattern, func, *args):
        """assertRaisesRegexp is not provided in python versions below 2.7"""
        try:
            func(*args)
        except eclass, e:
            m = re.search(pattern, e.message)
            if not m:
                raise AssertionError('"%s" does not match "%s"' % (pattern, e.message))
        else:
            raise AssertionError("%s not raised" % eclass.__name__)

    def test_regex_formatting_wrong(self):
        obj = {
            "0": {
                "regular_expression": "Item: (\d+"
            }
        }
        validator = get_schema_validator("extractors")
        self.assertRaisesRegexp(ValidationError, "Invalid regular expression",
                    validator.validate, obj)

    def test_regex_formatting_ok(self):
        obj = {
            "0": {
                "regular_expression": "Item: (\d+)"
            }
        }
        validator = get_schema_validator("extractors")
        self.assertEqual(validator.validate(obj), None)

    def test_valid_url(self):
        obj = {
            "start_urls": ['http://www.example.com/'],
            "links_to_follow": "none",
            "respect_nofollow": True,
            "templates": [],
        }
        validator = get_schema_validator("spider")
        self.assertEqual(validator.validate(obj), None)

    def test_invalid_url(self):
        obj = {
            "start_urls": ['www.example.com'],
            "links_to_follow": "none",
            "respect_nofollow": True,
            "templates": [],
        }
        validator = get_schema_validator("spider")
        self.assertRaisesRegexp(ValidationError, "Invalid url:", validator.validate, obj)

    def test_test_project(self):
        specs = open_project_from_dir(_TEST_PROJECT_DIR)
        self.assertTrue(validate_project_schema(specs))


########NEW FILE########
__FILENAME__ = test_spider
from unittest import TestCase
from os.path import dirname, join

from scrapy.http import Response, HtmlResponse, XmlResponse, TextResponse
from scrapy.utils.reqser import request_to_dict

from scrapely.htmlpage import HtmlPage

from slybot.spidermanager import SlybotSpiderManager

_PATH = dirname(__file__)

class SpiderTest(TestCase):
    smanager = SlybotSpiderManager("%s/data/SampleProject" % _PATH)

    def test_list(self):
        self.assertEqual(set(self.smanager.list()), set(["seedsofchange", "seedsofchange2",
                "seedsofchange.com", "pinterest.com", "ebay", "ebay2", "ebay3", "ebay4", "cargurus",
                "networkhealth.com", "allowed_domains", "any_allowed_domains", "example.com", "example2.com",
                "example3.com"]))

    def test_spider_with_link_template(self):
        name = "seedsofchange"
        spider = self.smanager.create(name)
        spec = self.smanager._specs["spiders"][name]
        t1, t2 = spec["templates"]
        target1, target2 = [HtmlPage(url=t["url"], body=t["original_body"]) for t in spec["templates"]]

        items, link_regions = spider.extract_items(target1)
        self.assertEqual(items, [])
        self.assertEqual(len(list(spider._process_link_regions(target1, link_regions))), 104)

        items, link_regions = spider.extract_items(target2)
        self.assertEqual(items[0], {
                '_template': u'4fac3b47688f920c7800000f',
                '_type': u'default',
                u'category': [u'Winter Squash'],
                u'days': [None],
                u'description': [u'1-2 lbs. (75-95 days)&nbsp;This early, extremely productive, compact bush variety is ideal for small gardens.&nbsp; Miniature pumpkin-shaped fruits have pale red-orange skin and dry, sweet, dark orange flesh.&nbsp; Great for stuffing, soups and pies.'],
                u'lifecycle': [u'Tender Annual'],
                u'name': [u'Gold Nugget'],
                u'price': [u'3.49'],
                u'product_id': [u'01593'],
                u'species': [u'Cucurbita maxima'],
                'url': u'http://www.seedsofchange.com/garden_center/product_details.aspx?item_no=PS14165',
                u'weight': [None]}
        )
        self.assertEqual(link_regions, [])
        self.assertEqual(len(list(spider._process_link_regions(target2, link_regions))), 0)

    def test_spider_with_link_region_but_not_link_template(self):
        name = "seedsofchange2"
        spider = self.smanager.create(name)
        spec = self.smanager._specs["spiders"][name]
        t1, t2 = spec["templates"]

        target1, target2 = [HtmlPage(url=t["url"], body=t["original_body"]) for t in spec["templates"]]
        items, link_regions = spider.extract_items(target1)
        self.assertEqual(items[0], {
                '_template': u'4fad6a7c688f922437000014',
                '_type': u'default',
                u'category': [u'Onions'],
                u'days': [None],
                u'description': [u'(110-120 days)&nbsp; Midsized Italian variety.&nbsp; Long to intermediate day red onion that tolerates cool climates.&nbsp; Excellent keeper.&nbsp; We have grown out thousands of bulbs and re-selected this variety to be the top quality variety that it once was.&nbsp; 4-5&quot; bulbs are top-shaped, uniformly colored, and have tight skins.'],
                u'lifecycle': [u'Heirloom/Rare'],
                u'name': [u'Rossa Di Milano Onion'],
                u'price': [u'3.49'],
                u'species': [u'Alium cepa'],
                u'type': [u'Heirloom/Rare'],
                'url': u'http://www.seedsofchange.com/garden_center/product_details.aspx?item_no=PS15978'}
        )
        self.assertEqual(link_regions, [])

        items, link_regions = spider.extract_items(target2)
        self.assertEqual(items[0], {
                '_template': u'4fad6a7d688f922437000017',
                '_type': u'default',
                u'category': [u'Winter Squash'],
                u'days': [None],
                u'description': [u'1-2 lbs. (75-95 days)&nbsp;This early, extremely productive, compact bush variety is ideal for small gardens.&nbsp; Miniature pumpkin-shaped fruits have pale red-orange skin and dry, sweet, dark orange flesh.&nbsp; Great for stuffing, soups and pies.'],
                u'lifecycle': [u'Tender Annual'],
                u'name': [u'Gold Nugget'],
                u'price': [u'3.49'],
                u'species': [u'Cucurbita maxima'],
                'url': u'http://www.seedsofchange.com/garden_center/product_details.aspx?item_no=PS14165',
                u'weight': [None]}
        )
        self.assertEqual(len(link_regions), 1)
        self.assertEqual(len(list(spider._process_link_regions(target1, link_regions))), 25)

    def test_login_requests(self):
        name = "pinterest.com"
        spider = self.smanager.create(name)
        login_request = list(spider.start_requests())[0]

        response = HtmlResponse(url="https://pinterest.com/login/", body=open(join(_PATH, "data", "pinterest.html")).read())
        response.request = login_request
        form_request = login_request.callback(response)
        expected = {'_encoding': 'utf-8',
            'body': 'email=test&password=testpass&csrfmiddlewaretoken=nLZy3NMzhTswZvweHJ4KVmq9UjzaZGn3&_ch=ecnwmar2',
            'callback': 'after_login',
            'cookies': {},
            'dont_filter': True,
            'errback': None,
            'headers': {'Content-Type': ['application/x-www-form-urlencoded']},
            'meta': {},
            'method': 'POST',
            'priority': 0,
            'url': u'https://pinterest.com/login/?next=%2F'}

        self.assertEqual(request_to_dict(form_request, spider), expected)

        # simulate a simple response to login post from which extract a link
        response = HtmlResponse(url="http://pinterest.com/", body="<html><body><a href='http://pinterest.com/categories'></body></html>")
        result = list(spider.after_login(response))
        self.assertEqual([r.url for r in result], ['http://pinterest.com/categories', 'http://pinterest.com/popular/'])

    def test_generic_form_requests(self):
        name = "ebay"
        spider = self.smanager.create(name)
        generic_form_request = list(spider.start_requests())[0]

        response = HtmlResponse(url="http://http://www.ebay.com/sch/ebayadvsearch/?rt=nc", body=open(join(_PATH, "data", "ebay_advanced_search.html")).read())
        response.request = generic_form_request
        request_list = [request_to_dict(req, spider)
                             for req in generic_form_request.callback(response)]

        expected = [{'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {}, 'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=1&_nkw=Cars&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=', 'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None}, {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {}, 'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=2&_nkw=Cars&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=', 'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None}, {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {}, 'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=3&_nkw=Cars&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=', 'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None}, {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {}, 'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=4&_nkw=Cars&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=', 'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None}, {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {}, 'url': u'http://http://www.ebay.com/sch/ebayadvsearch/?rt=nc', 'dont_filter': True, 'priority': 0, 'callback': 'parse', 'method': 'GET', 'errback': None}]
        self.assertEqual(request_list, expected)

    def test_generic_form_requests_with_file_field(self):
        name = "ebay2"
        spider = self.smanager.create(name)
        generic_form_request = list(spider.start_requests())[0]

        self.assertEqual(generic_form_request.url, 'file://tmp/test_params.txt')
        response = HtmlResponse(url='file://tmp/test_params.txt', body=open(join(_PATH, "data", "test_params.txt")).read())
        response.request = generic_form_request
        requests = list(generic_form_request.callback(response))
        request_list = [request_to_dict(req, spider) for req in requests]
        expected = [{'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {u'xpath': u"//form[@name='adv_search_from']", u'form_url': u'http://http://www.ebay.com/sch/ebayadvsearch/?rt=nc', u'type': u'form', 'field_index': 1, u'fields': [{u'xpath': u".//*[@name='_nkw']", 'file_values': ['Cars', 'Boats'], u'type': u'inurl', u'value': u'file://tmp/test_params.txt'}, {u'type': u'inurl', u'name': u'_nkw2', u'value': u'file://tmp/test_params.txt'}, {u'xpath': u".//*[@name='_in_kw']", u'type': u'iterate'}]}, 'headers': {}, 'url': u'file://tmp/test_params.txt', 'dont_filter': True, 'priority': 0, 'callback': 'parse_field_url_page', 'method': 'GET', 'errback': None}]
        self.assertEqual(request_list, expected)

        generic_form_request = requests[0]
        self.assertEqual(generic_form_request.url, 'file://tmp/test_params.txt')
        response = HtmlResponse(url='file://tmp/test_params.txt', body=open(join(_PATH, "data", "test_params.txt")).read())
        response.request = generic_form_request

        requests = list(generic_form_request.callback(response))
        request_list = [request_to_dict(req, spider) for req in requests]
        expected = [{'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {u'xpath': u"//form[@name='adv_search_from']", u'fields': [{u'xpath': u".//*[@name='_nkw']", 'file_values': ['Cars', 'Boats'], u'type': u'inurl', u'value': u'file://tmp/test_params.txt'}, {'file_values': ['Cars', 'Boats'], u'type': u'inurl', u'name': u'_nkw2', u'value': u'file://tmp/test_params.txt'}, {u'xpath': u".//*[@name='_in_kw']", u'type': u'iterate'}], u'type': u'form', 'field_index': 1}, 'headers': {}, 'url': u'http://http://www.ebay.com/sch/ebayadvsearch/?rt=nc', 'dont_filter': True, 'priority': 0, 'callback': 'parse_form_page', 'method': 'GET', 'errback': None}]
        self.assertEqual(request_list, expected)

        generic_form_request = requests[0]
        self.assertEqual(generic_form_request.url, 'http://http://www.ebay.com/sch/ebayadvsearch/?rt=nc')
        response = HtmlResponse(url="http://http://www.ebay.com/sch/ebayadvsearch/?rt=nc", body=open(join(_PATH, "data", "ebay_advanced_search.html")).read())
        response.request = generic_form_request
        request_list = [request_to_dict(req, spider)
                             for req in generic_form_request.callback(response)]
        expected = [{'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {},
            'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_nkw2=Cars&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=1&_nkw=Cars&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=',
            'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None},
            {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {},
            'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_nkw2=Cars&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=2&_nkw=Cars&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=',
            'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None},
            {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {},
            'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_nkw2=Cars&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=3&_nkw=Cars&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=',
            'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None},
            {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {},
            'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_nkw2=Cars&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=4&_nkw=Cars&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=',
            'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None},
            {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {},
            'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_nkw2=Boats&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=1&_nkw=Cars&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=',
            'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None},
            {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {},
            'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_nkw2=Boats&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=2&_nkw=Cars&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=',
            'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None},
            {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {},
            'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_nkw2=Boats&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=3&_nkw=Cars&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=',
            'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None},
            {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {},
            'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_nkw2=Boats&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=4&_nkw=Cars&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=',
            'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None},
            {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {},
            'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_nkw2=Cars&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=1&_nkw=Boats&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=',
            'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None},
            {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {},
            'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_nkw2=Cars&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=2&_nkw=Boats&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=',
            'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None},
            {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {}, 'url':
            u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_nkw2=Cars&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=3&_nkw=Boats&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=',
            'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None},
            {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {}, 
            'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_nkw2=Cars&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=4&_nkw=Boats&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=',
            'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None},
            {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {},
            'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_nkw2=Boats&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=1&_nkw=Boats&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=',
            'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None},
            {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {},
            'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_nkw2=Boats&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=2&_nkw=Boats&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=',
            'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None},
            {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {},
            'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_nkw2=Boats&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=3&_nkw=Boats&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=',
            'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None},
            {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {},
            'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_nkw2=Boats&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=4&_nkw=Boats&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=',
            'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None},
            {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {},
            'url': u'http://http://www.ebay.com/sch/ebayadvsearch/?rt=nc',
            'dont_filter': True, 'priority': 0, 'callback': 'parse', 'method': 'GET', 'errback': None}]
        self.assertEqual(request_list, expected)

    def test_generic_form_requests_with_spider_args(self):
        name = "ebay3"
        args = {'search_string': 'Cars'}
        spider = self.smanager.create(name, **args)
        generic_form_request = list(spider.start_requests())[0]

        response = HtmlResponse(url="http://www.ebay.com/sch/ebayadvsearch/?rt=nc", body=open(join(_PATH, "data", "ebay_advanced_search.html")).read())
        response.request = generic_form_request
        request_list = [request_to_dict(req, spider)
                             for req in generic_form_request.callback(response)]
        expected = [{'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {}, 'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=1&_nkw=Cars&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=', 'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None}, {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {}, 'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=2&_nkw=Cars&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=', 'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None}, {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {}, 'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=3&_nkw=Cars&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=', 'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None}, {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {}, 'url': u'http://www.ebay.com/sch/i.html?_adv=1&_ex_kw=&_ftrv=1&_ftrt=901&_sabdlo=&_sabdhi=&_sop=12&_samihi=&_ipg=50&_salic=1&_sasl=&_udlo=&_okw=&_fsradio=%26LH_SpecificSeller%3D1&_udhi=&_in_kw=4&_nkw=Cars&_sacat=0&_oexkw=&_dmd=1&_saslop=1&_samilow=', 'dont_filter': True, 'priority': 0, 'callback': 'after_form_page', 'method': 'GET', 'errback': None}, {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {}, 'headers': {}, 'url': u'http://www.ebay.com/sch/ebayadvsearch/?rt=nc', 'dont_filter': True, 'priority': 0, 'callback': 'parse', 'method': 'GET', 'errback': None}]
        self.assertEqual(request_list, expected)

    def test_allowed_domains(self):
        name = "allowed_domains"
        spider = self.smanager.create(name)
        expected = ['www.ebay.com', 'www.yahoo.com']
        self.assertEqual(spider.allowed_domains, expected)

    def test_allowed_domains_all(self):
        name = "any_allowed_domains"
        spider = self.smanager.create(name)
        expected = None
        self.assertEqual(spider.allowed_domains, expected)

    def test_allowed_domains_previous_behavior(self):
        name = "cargurus"
        spider = self.smanager.create(name)
        expected = ['www.cargurus.com']
        self.assertEqual(spider.allowed_domains, expected)

    def test_links_from_rss(self):
        body = open(join(_PATH, "data", "rss_sample.xml")).read()
        response = XmlResponse(url="http://example.com/sample.xml", body=body,
                headers={'Content-Type': 'application/rss+xml;charset=ISO-8859-1'})

        name = "cargurus"
        spider = self.smanager.create(name)

        urls = [r.url for r in spider.parse(response)]
        self.assertEqual(len(urls), 3)
        self.assertEqual(set(urls), set([
                "http://www.cargurus.com/Cars/2004-Alfa-Romeo-GT-Reviews-c10012",
                "http://www.cargurus.com/Cars/2005-Alfa-Romeo-GT-Reviews-c10013",
                "http://www.cargurus.com/Cars/2007-Alfa-Romeo-GT-Reviews-c10015"]))

    def test_empty_content_type(self):
        name = "ebay4"
        spider = self.smanager.create(name)
        generic_form_request = list(spider.start_requests())[0]
        
        response = Response(url="http://http://www.ebay.com/sch/ebayadvsearch/?rt=nc", 
                            body=open(join(_PATH, "data", "ebay_advanced_search.html")).read())
        response.request = generic_form_request
        # must not raise an error
        for result in spider.parse(response):
            pass

    def test_variants(self):
        """Ensure variants are extracted as list of dicts"""

        name = "networkhealth.com"
        spider = self.smanager.create(name)
        spec = self.smanager._specs["spiders"][name]
        template, = spec["templates"]
        target = HtmlPage(url=template["url"], body=template["original_body"])
        items, link_regions = spider.extract_items(target)
        for item in items:
            for variant in item["variants"]:
                self.assertEqual(type(variant), dict)

    def test_start_requests(self):
        name = "example.com"
        spider = self.smanager.create(name)
        spec = self.smanager._specs["spiders"][name]
        start_requests = list(spider.start_requests())
        self.assertEqual(len(start_requests), 2)
        self.assertEqual(start_requests[0].url, 'http://www.example.com/products.csv')
        self.assertEqual(start_requests[1].url, 'http://www.example.com/index.html')

        csv = """ 
My feed

name,url,id
Product A,http://www.example.com/path,A
Product B,http://www.example.com/path2,B"""
        response = TextResponse(url="http://www.example.com/products.csv", body=csv)
        requests = list(start_requests[0].callback(spider, response))
        self.assertEqual(len(requests), 2)
        self.assertEqual(requests[0].url, 'http://www.example.com/path')
        self.assertEqual(requests[1].url, 'http://www.example.com/path2')

    def test_start_requests_allowed_domains(self):
        name = "example2.com"
        spider = self.smanager.create(name)
        self.assertEqual(spider.allowed_domains, ['www.example.com'])

    def test_override_start_urls(self):
        name = "example2.com"
        spider = self.smanager.create(name, start_urls=['http://www.example.com/override.html'])
        start_requests = list(spider.start_requests())
        self.assertEqual(start_requests[1].url, 'http://www.example.com/override.html')

    def test_links_to_follow(self):

        html = "<html><body><a href='http://www.example.com/link.html'>Link</a></body></html>"
        response = HtmlResponse(url='http://www.example.com/index.html', body=html)

        name = "example3.com"
        spider = self.smanager.create(name, links_to_follow='none')
        start_requests = list(spider.start_requests())

        requests = list(start_requests[0].callback(response))
        self.assertEqual(len(requests), 0)

########NEW FILE########
__FILENAME__ = utils
from urlparse import urlparse
import os
import json

from scrapely.htmlpage import HtmlPage

def iter_unique_scheme_hostname(urls):
    """Return an iterator of tuples (scheme, hostname) over the given urls,
    filtering dupes
    """
    scheme_hostname = set()
    for x in urls:
        p = urlparse(x)
        scheme_hostname.add((p.scheme, p.hostname))
    return list(scheme_hostname)

def open_project_from_dir(project_dir):
    specs = {"spiders": {}}
    with open(os.path.join(project_dir, "project.json")) as f:
        specs["project"] = json.load(f)
    with open(os.path.join(project_dir, "items.json")) as f:
        specs["items"] = json.load(f)
    with open(os.path.join(project_dir, "extractors.json")) as f:
        specs["extractors"] = json.load(f)
    for fname in os.listdir(os.path.join(project_dir, "spiders")):
        if fname.endswith(".json"):
            spider_name = os.path.splitext(fname)[0]
            with open(os.path.join(project_dir, "spiders", fname)) as f:
                try:
                    specs["spiders"][spider_name] = json.load(f)
                except ValueError, e:
                    raise ValueError("Error parsing spider (invalid JSON): %s: %s" % (fname, e))
    return specs

def htmlpage_from_response(response):
    return HtmlPage(response.url, response.headers, \
            response.body_as_unicode(), encoding=response.encoding)

########NEW FILE########
__FILENAME__ = schema
"""Simple validation of specifications passed to slybot"""
from os.path import dirname, join
import json, re
from urlparse import urlparse

from jsonschema import Draft3Validator, ValidationError, RefResolver

_PATH = dirname(__file__)

def load_schemas():
    filename = join(_PATH, "schemas.json")
    return dict((s["id"], s) for s in json.load(open(filename)))

_SCHEMAS = load_schemas()

class SlybotJsonSchemaValidator(Draft3Validator):
    DEFAULT_TYPES = Draft3Validator.DEFAULT_TYPES.copy()
    DEFAULT_TYPES.update({
        "mapping": dict,
    })
    def validate_format(self, fmt, instance, schema):
        if schema["type"] != "string":
            raise ValidationError("Invalid keyword 'format' for type '%s'" % schema["type"])

        if fmt == "regex":
            try:
                re.compile(instance)
            except:
                raise ValidationError("Invalid regular expression: %s" % repr(instance))
        elif fmt == "url":
            parsed = urlparse(instance)
            if not parsed.scheme or not parsed.netloc:
                raise ValidationError("Invalid url: '%s'" % repr(instance))
               
        return None

def get_schema_validator(schema):
    resolver = RefResolver("", schema, _SCHEMAS)
    return SlybotJsonSchemaValidator(_SCHEMAS[schema], resolver=resolver)

def validate_project_schema(specs):
    
    project = specs["project"]
    get_schema_validator("project").validate(project)

    items = specs["items"]
    get_schema_validator("items").validate(items)

    extractors = specs["extractors"]
    get_schema_validator("extractors").validate(extractors)

    spider_schema_validator = get_schema_validator("spider")
    for spider in specs["spiders"].values():
        spider_schema_validator.validate(spider)

    return True


########NEW FILE########
__FILENAME__ = annotations
"""
code for transposition of annotations from the annotated source generated
in the browser to the final template.
"""
import json
from scrapely.htmlpage import parse_html, HtmlPage, HtmlTag, HtmlTagType
from .utils import serialize_tag

TAGID = u"data-tagid"


def _is_generated(htmltag):
    template_attr = htmltag.attributes.get("data-scrapy-annotate")
    if template_attr is None:
        return False
    unescaped = template_attr.replace('&quot;', '"')
    annotation = json.loads(unescaped)
    return annotation.get("generated", False)


def _must_add_tagid(element):

    return isinstance(element, HtmlTag) and \
        element.tag_type != HtmlTagType.CLOSE_TAG and \
        not _is_generated(element)


def add_tagids(source):
    """
    Applies a unique attribute code number for each tag element in order to be
    identified later in the process of apply annotation"""
    output = []
    tagcount = 0
    if not isinstance(source, HtmlPage):
        source = HtmlPage(body=source)
    for element in source.parsed_body:
        if _must_add_tagid(element):
            element.attributes[TAGID] = str(tagcount)
            tagcount += 1
            output.append(serialize_tag(element))
        else:
            output.append(source.body[element.start:element.end])

    return ''.join(output)


def remove_tagids(source):
    """remove from the given page, all tagids previously added by add_tagids()
    """
    output = []
    if not isinstance(source, HtmlPage):
        source = HtmlPage(body=source)
    for element in source.parsed_body:
        if _must_add_tagid(element):
            element.attributes.pop(TAGID, None)
            output.append(serialize_tag(element))
        else:
            output.append(source.body[element.start:element.end])
    return ''.join(output)


def _get_data_id(annotation):
    """gets id (a str) of an annotation"""

    if isinstance(annotation, HtmlTag):
        return annotation.attributes[TAGID]
    else:  # partial annotation
        for p in annotation:
            if (isinstance(p, HtmlTag) and "insert-after" in p.attributes):
                return p.attributes["insert-after"]


def _get_closing_tags(annotation):
    """get closing tabs of an extracted partial annotation"""
    if isinstance(annotation, list):
        for p in annotation:
            if (isinstance(p, HtmlTag) and "closing-tags" in p.attributes):
                return p.attributes["closing-tags"]


def _extract_annotations(source):
    """
    Extracts the raw annotations in a way they can be applied
    unambigously to the target non annotated source
    """
    if not isinstance(source, HtmlPage):
        source = HtmlPage(body=source)

    annotations = []
    otherdata = []
    last_id = -1
    last_annotation_id = -1
    cache = []
    inside_insert = False
    insert_end = False
    closing_tags = 0
    for element in source.parsed_body:
        if isinstance(element, HtmlTag):

            if inside_insert:
                if element.tag == "ins":  # closing ins
                    annotations[-1].append(element)
                    insert_end = True
                    continue
                elif insert_end:
                    inside_insert = False  # end of insert group
                else:
                    annotations[-1].append(element)

            else:
                if TAGID in element.attributes:
                    last_id = element.attributes[TAGID]
                    cache = []
                for key in element.attributes:
                    if key == "data-scrapy-annotate":
                        # inserted tag (partial annotation)
                        if not TAGID in element.attributes:

                            if last_id == last_annotation_id and \
                                    isinstance(annotations[-1], list):
                                insertion = annotations[-1]
                                insertion.extend(cache)
                                cache = []
                            else:
                                insertion = []
                                annotations.append(insertion)
                                element.attributes["closing-tags"] = \
                                    closing_tags
                            element.attributes["insert-after"] = last_id
                            if cache and not isinstance(cache[-1], HtmlTag):
                                insertion.append(cache[-1])
                                cache = []
                            insertion.append(element)
                            inside_insert = True
                            insert_end = False
                        else:  # normal annotation
                            annotations.append(element)
                        last_annotation_id = last_id
                        break
                else:
                    for key in element.attributes:
                        if key.startswith("data-scrapy-"):
                            otherdata.append(element)

            # count number of close tags after a numerated one
            if element.tag_type == HtmlTagType.CLOSE_TAG and not inside_insert:
                closing_tags += 1
                cache.append(element)
            else:
                cache = []
                closing_tags = 0

        else:  # an HtmlDataFragment
            if inside_insert:
                annotations[-1].append(element)
                if insert_end:
                    inside_insert = False
            else:
                cache.append(element)

    return sorted(otherdata + annotations, key=lambda x: int(_get_data_id(x)))


def _get_cleansing(target_html, annotations):
    """
    Gets relevant pieces of text affected by browser cleansing.
    """

    numbered_html = add_tagids(target_html)
    target = HtmlPage(body=numbered_html)
    element = target.parsed_body[0]

    all_cleansing = {}
    for annotation in annotations:
        if isinstance(annotation, list):  # partial annotation

            # search insert point we are interested on
            target_it = iter(target.parsed_body)
            for p in annotation:
                if isinstance(p, HtmlTag) and "insert-after" in p.attributes:
                    insert_after = p.attributes["insert-after"]
                    break
            while not (isinstance(element, HtmlTag) and
                       element.attributes.get(TAGID) == insert_after):
                element = target_it.next()

            # 1. browser removes tags inside <option>...</option>
            # 2. browser adds </option> if it is not present
            if element.tag == "option" and \
                    element.tag_type == HtmlTagType.OPEN_TAG:
                cached = []
                add_cached = False
                closed_option = False
                element = target_it.next()
                while not (isinstance(element, HtmlTag) and
                           element.tag in ["option", "select"]):
                    cached.append(element)
                    if hasattr(element, 'tag'):
                        add_cached = True
                    element = target_it.next()

                if (element.tag == "option" and
                    element.tag_type == HtmlTagType.OPEN_TAG) or \
                        (element.tag == "select" and
                         element.tag_type == HtmlTagType.CLOSE_TAG):
                    closed_option = True

                if add_cached or closed_option:
                    out = "".join([numbered_html[e.start:e.end]
                                  for e in cached])
                    all_cleansing[insert_after] = out

    return all_cleansing


def _order_is_valid(parsed):
    """Checks if tag ordering is valid, so to help merge_code
    to select correct alternative among the generated ones
    """
    tag_stack = []
    for e in parsed:
        if isinstance(e, HtmlTag):
            if e.tag_type == HtmlTagType.OPEN_TAG:
                tag_stack.append(e.tag)
            elif e.tag_type == HtmlTagType.CLOSE_TAG:
                if tag_stack and tag_stack[-1] == e.tag:
                    tag_stack.pop()
                else:
                    return False
    return True


def _merge_code(code1, code2):
    """merges two pieces of html code by text content alignment."""
    parsed1 = list(parse_html(code1))
    parsed2 = list(parse_html(code2))

    insert_points1 = []
    tags1 = []
    p = 0
    text1 = ""
    for e in parsed1:
        if isinstance(e, HtmlTag):
            insert_points1.append(p)
            tags1.append(e)
        else:
            p += e.end - e.start
            text1 += code1[e.start:e.end]

    insert_points2 = []
    tags2 = []
    p = 0
    text2 = ""
    for e in parsed2:
        if isinstance(e, HtmlTag):
            insert_points2.append(p)
            tags2.append(e)
        else:
            p += e.end - e.start
            text2 += code2[e.start:e.end]

    assert(text1.startswith(text2) or text2.startswith(text1))

    # unique sorted list of insert points
    _insert_points = sorted(insert_points1 + insert_points2)
    insert_points = []
    for i in _insert_points:
        if not i in insert_points:
            insert_points.append(i)

    possible_outs = [""]
    start = 0
    # insert tags in correct order, calculate all alternatives when
    # when order is ambiguous
    for end in insert_points:
        possible_outs = [out + text1[start:end] for out in possible_outs]
        dup_possible_outs = [out for out in possible_outs]
        if end in insert_points1:
            tag1 = tags1.pop(0)
            possible_outs = [out + code1[tag1.start:tag1.end]
                             for out in possible_outs]
        if end in insert_points2:
            tag2 = tags2.pop(0)
            possible_outs = [out + code2[tag2.start:tag2.end]
                             for out in possible_outs]
            if end in insert_points1:
                dup_possible_outs = [out + code2[tag2.start:tag2.end]
                                     for out in dup_possible_outs]
                dup_possible_outs = [out + code1[tag1.start:tag1.end]
                                     for out in dup_possible_outs]
                possible_outs += dup_possible_outs
        start = end

    # choose the first valid
    for out in possible_outs:
        parsed_out = list(parse_html(out))
        if _order_is_valid(parsed_out):
            break

    if text1.startswith(text2):
        out += text1[len(text2):]
    else:
        out += text2[len(text1):]

    tag_count1 = sum(1 for i in parsed1 if isinstance(i, HtmlTag))
    tag_count_final = sum(1 for i in parsed_out if isinstance(i, HtmlTag))

    return out, tag_count_final - tag_count1


def apply_annotations(source_html, target_html):
    """
    Applies annotations present in source_html, into
    raw target_html. source_html must be taggered source,
    target_html is the original raw (no tags, no annotations)
    source.
    """
    annotations = _extract_annotations(source_html)
    target_page = HtmlPage(body=target_html)
    cleansing = _get_cleansing(target_page, annotations)

    numbered_html = add_tagids(target_page)
    target = parse_html(numbered_html)
    output = []

    element = target.next()
    eof = False
    while not (isinstance(element, HtmlTag) and TAGID in element.attributes):
        output.append(numbered_html[element.start:element.end])
        element = target.next()
    last_id = element.attributes[TAGID]
    for i in range(len(annotations)):

        annotation = annotations[i]
        # look up replacement/insertion point
        aid = _get_data_id(annotation)
        # move target until replacement/insertion point
        while int(last_id) < int(aid):
            output.append(numbered_html[element.start:element.end])
            element = target.next()
            while not (isinstance(element, HtmlTag) and
                       TAGID in element.attributes):
                output.append(numbered_html[element.start:element.end])
                element = target.next()
            last_id = element.attributes[TAGID]

        # replace/insert in target
        if isinstance(annotation, HtmlTag):
            for key, val in annotation.attributes.items():
                if key.startswith("data-scrapy-"):
                    element.attributes[key] = val
            output.append(serialize_tag(element))
            if not (i + 1 < len(annotations) and
                    _get_data_id(annotations[i + 1]) == aid):
                element = target.next()

        else:  # partial annotation
            closing_tags = _get_closing_tags(annotation)
            if not (i > 0 and _get_data_id(annotations[i - 1]) == aid):
                output.append(numbered_html[element.start:element.end])
                while closing_tags > 0:
                    element = target.next()
                    output.append(numbered_html[element.start:element.end])
                    if isinstance(element, HtmlTag) and \
                            element.tag_type == HtmlTagType.CLOSE_TAG:
                        closing_tags -= 1

            elif (i > 0 and isinstance(annotations[i - 1], HtmlTag) and
                    annotation[0].start > annotations[i - 1].end):
                element = target.next()
                while closing_tags > 0:
                    output.append(numbered_html[element.start:element.end])
                    element = target.next()
                    if isinstance(element, HtmlTag) and \
                            element.tag_type == HtmlTagType.CLOSE_TAG:
                        closing_tags -= 1

                output.append(numbered_html[element.start:element.end])

            num_tags_inside = 0
            partial_output = ""

            # computes number of tags inside a partial annotation
            for p in annotation:
                partial_output += source_html[p.start:p.end]
                if isinstance(p, HtmlTag):
                    num_tags_inside += 1
                    if "insert-after" in p.attributes:
                        num_tags_inside -= 2

            if aid in cleansing:
                partial_output, fix_tag_count = _merge_code(
                    partial_output, cleansing[aid])
                num_tags_inside += fix_tag_count

            output.append(partial_output)

            element = target.next()  # consume reference tag

            # consume the tags inside partial annotation
            while num_tags_inside > 0:
                if isinstance(element, HtmlTag):
                    num_tags_inside -= 1
                element = target.next()

            if not isinstance(element, HtmlTag):
                element = target.next()

        if not (i + 1 < len(annotations) and
                _get_data_id(annotations[i + 1]) == aid):
            try:
                while not (isinstance(element, HtmlTag) and
                           TAGID in element.attributes):
                    output.append(numbered_html[element.start:element.end])
                    element = target.next()
            except StopIteration:
                eof = True
            else:
                last_id = element.attributes[TAGID]

    if not eof:
        output.append(numbered_html[element.start:element.end])
    for element in target:
        output.append(numbered_html[element.start:element.end])

    return remove_tagids(''.join(output))
########NEW FILE########
__FILENAME__ = bot
"""
Bot resource

Defines bot/fetch endpoint, e.g.:
    curl -d '{"request": {"url": "http://scrapinghub.com/"}}' http://localhost:9001/bot/fetch

The "request" is an object whose fields match the parameters of a Scrapy
request:
    http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request

Returns a json object. If there is an "error" field, that holds the request
error to display. Otherwise you will find the following fields:
    * page -- the retrieved page - will be annotated in future

"""
import json, errno
from functools import partial
from twisted.web.resource import Resource
from twisted.web.server import NOT_DONE_YET
from scrapy.http import Request
from scrapy.spider import BaseSpider
from scrapy.item import DictItem
from scrapy import signals, log
from scrapy.crawler import Crawler
from scrapy.http import HtmlResponse
from scrapy.exceptions import DontCloseSpider
from scrapy.utils.request import request_fingerprint
from slybot.spider import IblSpider
from .html import html4annotation, extract_html
from .resource import SlydJsonResource


def create_bot_resource(spec_manager):
    bot = Bot(spec_manager.settings, spec_manager)
    bot.putChild('fetch', Fetch(bot))
    return bot


class Bot(Resource):
    spider = BaseSpider('slyd')

    def __init__(self, settings, spec_manager):
        # twisted base class is old-style so we cannot user super()
        Resource.__init__(self)
        self.spec_manager = spec_manager
        # initialize scrapy crawler
        crawler = Crawler(settings)
        crawler.configure()
        crawler.signals.connect(self.keep_spider_alive, signals.spider_idle)
        crawler.crawl(self.spider)
        crawler.start()

        self.crawler = crawler
        log.msg("bot initialized", level=log.DEBUG)

    def keep_spider_alive(self, spider):
        raise DontCloseSpider("keeping it open")

    def stop(self):
        """Stop the crawler"""
        self.crawler.stop()
        log.msg("bot stopped", level=log.DEBUG)


class BotResource(SlydJsonResource):
    def __init__(self, bot):
        Resource.__init__(self)
        self.bot = bot


class Fetch(BotResource):
    isLeaf = True

    def render_POST(self, request):
        #TODO: validate input data, handle errors, etc.
        params = self.read_json(request)
        scrapy_request_kwargs = params['request']
        scrapy_request_kwargs.update(
            callback=self.fetch_callback,
            errback=partial(self.fetch_errback, request),
            dont_filter=True,  # TODO: disable duplicate middleware
            meta=dict(
                handle_httpstatus_all=True,
                twisted_request=request,
                slyd_request_params=params
            )
        )
        request = Request(**scrapy_request_kwargs)
        self.bot.crawler.engine.schedule(request, self.bot.spider)
        return NOT_DONE_YET

    def fetch_callback(self, response):
        request = response.meta['twisted_request']
        result_response = dict(status=response.status,
                headers=response.headers.to_string())
        if response.status != 200:
            finish_request(request, response=result_response)
            return
        if not isinstance(response, HtmlResponse):
            msg = "Non-html response: %s" % response.headers.get(
                'content-type', 'no content type')
            finish_request(request, error=msg)
        try:
            params = response.meta['slyd_request_params']
            original_html = extract_html(response)
            cleaned_html = html4annotation(original_html, response.url)
            # we may want to include some headers
            fingerprint = request_fingerprint(response.request)
            result_response = dict(status=response.status,
                headers=response.headers.to_string())
            result = dict(page=cleaned_html, original=original_html, fp=fingerprint,
                response=result_response)
            spider = self.create_spider(request.project, params)
            if spider is not None:
                items = []
                links = []
                for value in spider.parse(response):
                    if isinstance(value, Request):
                        links.append(value.url)
                    elif isinstance(value, DictItem):
                        items.append(value._values)
                    else:
                        raise ValueError("Unexpected type %s from spider"
                            % type(value))
                result['items'] = items
                result['links'] = links
            finish_request(request, **result)
        except Exception as ex:
            log.err()
            finish_request(request, response=result_response,
                error="unexpected internal error: %s" % ex)

    def create_spider(self, project, params, **kwargs):
        spider = params.get('spider')
        if spider is None:
            return
        pspec = self.bot.spec_manager.project_spec(project)
        try:
            spider_spec = pspec.resource('spiders', spider)
            items_spec = pspec.resource('items')
            extractors = pspec.resource('extractors')
            return IblSpider(spider, spider_spec, items_spec, extractors,
                **kwargs)
        except IOError as ex:
            if ex.errno == errno.ENOENT:
                log.msg("skipping extraction, no spec: %s" % ex.filename)
            else:
                raise

    def fetch_errback(self, twisted_request, failure):
        msg = "unexpected error response: %s" % failure
        log.msg(msg, level=log.ERROR)
        finish_request(twisted_request, error=msg)


def finish_request(trequest, **resp_obj):
    jdata = json.dumps(resp_obj)
    trequest.setResponseCode(200)
    trequest.setHeader('Content-Type', 'application/json')
    trequest.setHeader('Content-Length', len(jdata))
    trequest.write(jdata)
    trequest.finish()

########NEW FILE########
__FILENAME__ = crawlerspec
"""
Crawler Spec

Manages definitions of the crawler specifications

This will save, validate, potentially cache, etc. Right now it just
loads data from the filesystem.
"""
import json, re, shutil, errno, os
from os.path import join, splitext
from twisted.web.resource import NoResource, ForbiddenResource
from jsonschema.exceptions import ValidationError
from slybot.utils import open_project_from_dir
from slybot.validation.schema import get_schema_validator
from .resource import SlydJsonResource
from .annotations import apply_annotations
from .html import html4annotation


def create_crawler_spec_resource(spec_manager):
    return SpecResource(spec_manager)

# stick to alphanum . and _. Do not allow only .'s (so safe for FS path)
_INVALID_SPIDER_RE = re.compile('[^A-Za-z0-9._\-~]|^\.*$')


def allowed_spider_name(name):
    return not _INVALID_SPIDER_RE.search(name)


def convert_spider_templates(spider):
    """Converts the spider templates annotated body for being used in the UI"""
    for template in spider['templates']:
            template['annotated_body'] = html4annotation(
                template['annotated_body'], template['url'])


def annotate_templates(spider):
    "Applies the annotations into the templates original body"
    if spider.get('templates', None):
        for template in spider['templates']:
            template['annotated_body'] = apply_annotations(
                template['annotated_body'], template['original_body'])


class CrawlerSpecManager(object):

    def __init__(self, settings):
        self.settings = settings
        self.basedir = self.settings['SPEC_DATA_DIR']

    def project_spec(self, project):
        return ProjectSpec(join(self.basedir, str(project)))


class ProjectSpec(object):

    resources = ('project', 'items', 'extractors')

    def __init__(self, projectdir):
        self.projectdir = projectdir

    def load_slybot_spec(self, project):
        """load the spec for a given project"""
        return open_project_from_dir(self.projectdir)

    def list_spiders(self):
        try:
            for fname in os.listdir(join(self.projectdir, "spiders")):
                if fname.endswith(".json"):
                    yield splitext(fname)[0]
        except OSError as ex:
            if ex.errno != errno.ENOENT:
                raise

    def spider_json(self, name):
        """Loads the spider spec for the give spider name

        Also converts the annotated body of the templates to be used by
        the annotation UI"""
        try:
            spider = self.resource('spiders', name)
            convert_spider_templates(spider)
            return spider
        except IOError as ex:
            if ex.errno == errno.ENOENT:
                return({})
            else:
                raise

    def rename_spider(self, from_name, to_name):
        os.rename(self._rfilename('spiders', from_name),
            self._rfilename('spiders', to_name))

    def remove_spider(self, name):
        os.remove(self._rfilename('spiders', name))

    def _rfilename(self, *resources):
        return join(self.projectdir, *resources) + '.json'

    def _rfile(self, resources, mode='rb'):
        return open(self._rfilename(*resources), mode)

    def resource(self, *resources):
        return json.load(self._rfile(resources))

    def writejson(self, outf, *resources):
        """Write json for the resource specified

        Multiple arguments are joined (e.g. spider, spidername).

        If the file does not exist, an empty dict is written
        """
        try:
            shutil.copyfileobj(self._rfile(resources), outf)
        except IOError as ex:
            if ex.errno == errno.ENOENT:
                outf.write('{}')
            else:
                raise

    def savejson(self, obj, *resources):
        # convert to json in a way that will make sense in diffs
        ouf = self._rfile(*resources, mode='wb')
        json.dump(obj, ouf, sort_keys=True, indent=4)

    def json(self, out):
        """Write spec as json to the file-like object

        This uses the file contents and avoids converting to python types
        """
        # assumes " is not allowed in spider names
        template_dict = {r: 'SPEC:%s' % r for r in self.resources}
        template_dict['spiders'] = {s: 'SPIDER:%s' % s for s in self.list_spiders()}
        json_template = json.dumps(template_dict)
        last = 0
        for match in re.finditer('"(SPEC|SPIDER):([^"]+)"', json_template):
            out.write(json_template[last:match.start()])
            mtype, resource = match.groups()
            if mtype == 'SPEC':
                self.writejson(out, resource)
            else:
                self.writejson(out, 'spiders', resource)
            last = match.end()
        out.write(json_template[last:])


class SpecResource(SlydJsonResource):
    isLeaf = True

    spider_commands = {
        'mv': ProjectSpec.rename_spider,
        'rm': ProjectSpec.remove_spider
    }

    def __init__(self, spec_manager):
        SlydJsonResource.__init__(self)
        self.spec_manager = spec_manager

    def render(self, request):
        # make sure the path is safe
        for pathelement in request.postpath:
            if pathelement and not allowed_spider_name(pathelement):
                resource_class = NoResource if request.method == 'GET' \
                    else ForbiddenResource
                resource = resource_class("Bad path element %r" % pathelement)
                return resource.render(request)
        return SlydJsonResource.render(self, request)

    def render_GET(self, request):
        project_spec = self.spec_manager.project_spec(request.project)
        rpath = request.postpath
        if not rpath:
            project_spec.json(request)
        elif len(rpath) == 1 and rpath[0] == 'spiders':
            spiders = project_spec.list_spiders()
            request.write(json.dumps(list(spiders)))
        else:
            if rpath[0] == 'spiders' and len(rpath) == 2:
                spider = project_spec.spider_json(rpath[1])
                request.write(json.dumps(spider))
            else:
                project_spec.writejson(request, *rpath)
        return '\n'

    def render_POST(self, request):
        obj = self.read_json(request)
        project_spec = self.spec_manager.project_spec(request.project)
        try:
            # validate the request path and data
            resource = request.postpath[0]
            if resource == 'spiders':
                if len(request.postpath) == 1 or not request.postpath[1]:
                    return self.handle_spider_command(project_spec, obj)
                annotate_templates(obj)
                resource = 'spider'
            get_schema_validator(resource).validate(obj)
        except (KeyError, IndexError) as _ex:
            self.error(404, "Not Found", "No such resource")
        except ValidationError as ex:
            self.bad_request("Json failed validation: %s" % ex.message)
        project_spec.savejson(obj, request.postpath)
        return ''

    def handle_spider_command(self, project_spec, command_spec):
        command = command_spec.get('cmd')
        dispatch_func = self.spider_commands.get(command)
        if dispatch_func is None:
            self.bad_request(
                "unrecognised cmd arg %s, available commands: %s" %
                (command, ', '.join(self.spider_commands.keys())))
        args = command_spec.get('args', [])
        for spider in args:
            if not allowed_spider_name(spider):
                self.bad_request('invlalid spider name %s' % spider)
        try:
            retval = dispatch_func(project_spec, *args)
        except TypeError:
            self.bad_request("incorrect args for %s" % command)
        except OSError as ex:
            if ex.errno == errno.ENOENT:
                self.error(404, "Not Found", "No such resource")
            raise
        return retval or ''

########NEW FILE########
__FILENAME__ = html
"""
    Removes JavaScript from HTML

    This module removes all existing JavaScript in an HTML document.

"""
import re
from scrapely.htmlpage import HtmlTag, HtmlTagType, parse_html
from slybot.utils import htmlpage_from_response
from slybot.baseurl import insert_base_url
from .annotations import add_tagids
from .utils import serialize_tag

### Known weaknesses
#     Doesn't deal with JS hidden in CSS
#     Doesn't deal with meta redirect javascript URIs

INTRINSIC_EVENT_ATTRIBUTES = ("onload", "onunload", "onclick", "ondblclick",
                            "onmousedown", "onmouseup", "onmouseover",
                            "onmousemove", "onmouseout", "onfocus",
                            "onblur", "onkeypress", "onkeydown",
                            "onkeyup", "onsubmit", "onreset", "onselect",
                            "onchange", "onerror", "onbeforeunload")

URI_ATTRIBUTES = ("action", "background", "cite", "classid", "codebase",
                "data", "href", "longdesc", "profile", "src", "usemap")

AS_SCRIPT_REGION_BEGIN = "<!-- begin region added by slyd-->"
AS_SCRIPT_REGION_END = "<!-- end region added by slyd-->"

_AS_COMMENT_BEGIN = "<!-- begin_ascomment:"
_AS_COMMENT_END = ":end_ascomment -->"
_ENTITY_RE = re.compile("&#(\d+);")


def _deentitize_unicode(mystr):
    """replaces all entities in the form &#\d+; by its
    unicode equivalent.
    """
    return _ENTITY_RE.sub(lambda m: unichr(int(m.groups()[0])), mystr)


def html4annotation(htmlpage, baseurl=None):
    """Convert the given html document for the annotation UI

    This adds tags, removes scripts and optionally adds a base url
    """
    htmlpage = add_tagids(htmlpage)
    cleaned_html = descriptify(htmlpage)
    if baseurl:
        cleaned_html = insert_base_url(cleaned_html, baseurl)
    return cleaned_html


def extract_html(response):
    """Extracts an html page from the response.
    """
    return htmlpage_from_response(response).body


def descriptify(doc):
    """Clean JavaScript in a html source string.
    """
    parsed = parse_html(doc)
    newdoc = []
    inserted_comment = False
    for element in parsed:
        if isinstance(element, HtmlTag):
            if not inserted_comment and element.tag == "script" and element.tag_type == HtmlTagType.OPEN_TAG:
                newdoc.append(_AS_COMMENT_BEGIN + doc[element.start:element.end] + _AS_COMMENT_END)
                inserted_comment = True
            elif element.tag == "script" and element.tag_type == HtmlTagType.CLOSE_TAG:
                if inserted_comment:
                    inserted_comment = False
                newdoc.append(_AS_COMMENT_BEGIN + doc[element.start:element.end] + _AS_COMMENT_END)
            elif element.tag == "noscript":
                newdoc.append(_AS_COMMENT_BEGIN + doc[element.start:element.end] + _AS_COMMENT_END)
            else:
                for key, val in element.attributes.copy().items():
                    # Empty intrinsic events
                    if key in INTRINSIC_EVENT_ATTRIBUTES:
                        element.attributes[key] = ""
                    # Rewrite javascript URIs
                    elif key in URI_ATTRIBUTES and val is not None and "javascript:" in _deentitize_unicode(val):
                        element.attributes[key] = "about:blank"
                    else:
                        continue
                newdoc.append(serialize_tag(element))
        else:
            text = doc[element.start:element.end]
            if inserted_comment and text.strip() and not (text.startswith("<!--") and text.endswith("-->")):
                newdoc.append(_AS_COMMENT_BEGIN + text + _AS_COMMENT_END)
            else:
                newdoc.append(text)

    return ''.join(newdoc)

########NEW FILE########
__FILENAME__ = projects
"""
Projects Resource

Manages listing/creation/deletion/renaming of slybot projects on
the local filesystem. Routes to the appropiate resource for fetching
pages and project spec manipulation.
"""

import json, re, shutil, errno, os
from os.path import join
from twisted.web.resource import NoResource
from .resource import SlydJsonResource


# stick to alphanum . and _. Do not allow only .'s (so safe for FS path)
_INVALID_PROJECT_RE = re.compile('[^A-Za-z0-9._]|^\.*$')


def allowed_project_name(name):
    return not _INVALID_PROJECT_RE.search(name)


class ProjectsResource(SlydJsonResource):

    def __init__(self, settings):
    	SlydJsonResource.__init__(self)
    	self.projectsdir = settings['SPEC_DATA_DIR']

    def getChildWithDefault(self, project_path_element, request):
        # TODO: check exists, user has access, etc.
        # rely on the CrawlerSpec for this as storage and auth
        # can be customized
        request.project = project_path_element
        try:
            next_path_element = request.postpath.pop(0)
        except IndexError:
            next_path_element = None
        if next_path_element not in self.children:
            raise NoResource("No such child resource.")
        request.prepath.append(project_path_element)
        return self.children[next_path_element]

    def list_projects(self):
        try:
            for fname in os.listdir(self.projectsdir):
                if os.path.isdir(os.path.join(self.projectsdir, fname)):
                    yield fname
        except OSError as ex:
            if ex.errno != errno.ENOENT:
                raise

    def create_project(self, project_name):
    	project_filename = self.project_filename(project_name)
        os.makedirs(project_filename)
        with open(join(project_filename, 'project.json'), 'wb') as outf:
            outf.write('{}')
        os.makedirs(join(project_filename, 'spiders'))

    def rename_project(self, from_name, to_name):
        os.rename(self.project_filename(from_name),
            self.project_filename(to_name))

    def remove_project(self, name):
        shutil.rmtree(self.project_filename(name))

    def project_filename(self, project_name):
        return join(self.projectsdir, project_name)

    def handle_project_command(self, command_spec):
        command = command_spec.get('cmd')
        dispatch_func = self.project_commands.get(command)
        if dispatch_func is None:
            self.bad_request(
                "unrecognised cmd arg %s, available commands: %s" %
                (command, ', '.join(self.project_commands.keys())))
        args = command_spec.get('args', [])
        for project in args:
            if not allowed_project_name(project):
                self.bad_request('invalid project name %s' % project)
        try:
            retval = dispatch_func(self, *args)
        except TypeError:
            self.bad_request("incorrect args for %s" % command)
        except OSError as ex:
            if ex.errno == errno.ENOENT:
                self.error(404, "Not Found", "No such resource")
            elif ex.errno == errno.EEXIST or ex.errno == errno.ENOTEMPTY:
                self.bad_request("A project with that name already exists")
            raise
        return retval or ''

    def render_GET(self, request):
        request.write(json.dumps(sorted(self.list_projects())))
        return '\n'

    def render_POST(self, request):
        obj = self.read_json(request)
        return self.handle_project_command(obj)

    project_commands = {
        'create': create_project,
        'mv': rename_project,
        'rm': remove_project
    }

########NEW FILE########
__FILENAME__ = resource
import json, errno
from twisted.web.resource import Resource, NoResource, ErrorPage


class SlydJsonResource(Resource):
    """Base Resource for Slyd Resources

    This sets the content type to JSON and handles errors
    """

    def render(self, request):
        request.setResponseCode(200)
        request.setHeader('Content-Type', 'application/json')
        try:
            return Resource.render(self, request)
        except IOError as ex:
            if ex.errno == errno.ENOENT:
                return NoResource().render(request)
            else:
                raise
        except ErrorPage as ex:
            return ex.render(request)

    def error(self, request, status, why):
        raise ErrorPage(request, status, why)

    def bad_request(self, why):
        self.error(400, "Bad Request", why)

    def read_json(self, request):
        try:
            return json.load(request.content)
        except ValueError as ex:
            self.bad_request("Error parsing json. %s" % ex.message)


class SlydJsonObjectResource(SlydJsonResource):
    """Extends SlydJsonResource, converting
    the returned data to JSON
    """

    def render(self, request):
        resp = SlydJsonResource.render(self, request)
        if resp is not None:
            return json.dumps(resp)

########NEW FILE########
__FILENAME__ = settings
"""Scrapy settings"""
from os.path import join, dirname

EXTENSIONS = {
    'scrapy.contrib.logstats.LogStats': None,
    'scrapy.webservice.WebService': None,
    'scrapy.telnet.TelnetConsole': None,
    'scrapy.contrib.throttle.AutoThrottle': None
}

LOG_LEVEL = 'DEBUG'

# location of slybot projects - assumes a subdir per project
DATA_DIR = join(dirname(dirname(__file__)), 'data')
SPEC_DATA_DIR = join(DATA_DIR, 'projects')


# recommended for development - use scrapy to cache http responses
HTTPCACHE_ENABLED = True
HTTPCACHE_DIR = join(DATA_DIR, 'cache')

########NEW FILE########
__FILENAME__ = tap
"""
The module is used by the Twisted plugin system
(twisted.plugins.slyd_plugin) to register twistd command to manage
slyd server. The command can be used with 'twistd slyd'.
"""
from os.path import join, dirname
from twisted.python import usage
from twisted.web.resource import Resource
from twisted.application.internet import TCPServer
from twisted.web.server import Site
from twisted.web.static import File

DEFAULT_PORT = 9001
DEFAULT_DOCROOT = join(dirname(dirname(__file__)), 'media')


class Options(usage.Options):
    optParameters = [
        ['port', 'p', DEFAULT_PORT, 'Port number to listen on.', int],
        ['docroot', 'd', DEFAULT_DOCROOT, 'Default doc root for static media.'],
    ]


def create_root(config):
    from scrapy import log
    from scrapy.settings import CrawlerSettings
    from slyd.crawlerspec import (CrawlerSpecManager,
        create_crawler_spec_resource)
    from slyd.bot import create_bot_resource
    import slyd.settings
    from slyd.projects import ProjectsResource

    root = Resource()
    root.putChild("static", File(config['docroot']))

    crawler_settings = CrawlerSettings(settings_module=slyd.settings)
    spec_manager = CrawlerSpecManager(crawler_settings)

    # add project management at /projects
    projects = ProjectsResource(crawler_settings)
    root.putChild('projects', projects)

    # add crawler at /projects/PROJECT_ID/bot
    log.msg("Slybot specs loading from %s/[PROJECT]" % spec_manager.basedir,
        level=log.DEBUG)
    projects.putChild("bot", create_bot_resource(spec_manager))

    # add spec at /projects/PROJECT_ID/spec
    spec = create_crawler_spec_resource(spec_manager)
    projects.putChild("spec", spec)
    return root


def makeService(config):
    root = create_root(config)
    site = Site(root)
    return TCPServer(config['port'], site)

########NEW FILE########
__FILENAME__ = utils
"""
html page utils
"""
from scrapely.htmlpage import HtmlTagType


def _quotify(mystr):
    """
    quotifies an html tag attribute value.
    Assumes then, that any ocurrence of ' or " in the
    string is escaped if original string was quoted
    with it.
    So this function does not altere the original string
    except for quotation at both ends, and is limited just
    to guess if string must be quoted with '"' or "'"
    """
    quote = '"'
    l = len(mystr)
    for i in range(l):
        if mystr[i] == "\\" and i + 1 < l and mystr[i+1] == "'":
            quote = "'"; break
        elif mystr[i] == "\\" and i + 1 < l and mystr[i+1] == '"':
            quote = '"'; break
        elif mystr[i] == "'":
            quote = '"'; break
        elif mystr[i] == '"':
            quote = "'"; break
    return quote + mystr + quote


def serialize_tag(tag):
    """
    Converts a tag into a string when a slice [tag.start:tag.end]
    over the source can't be used because tag has been modified
    """
    out = "<"
    if tag.tag_type == HtmlTagType.CLOSE_TAG:
        out += "/"
    out += tag.tag

    attributes = []
    for key, val in tag.attributes.iteritems():
        aout = key
        if val is not None:
            aout += "=" + _quotify(val)
        attributes.append(aout)
    if attributes:
        out += " " + " ".join(attributes)

    if tag.tag_type == HtmlTagType.UNPAIRED_TAG:
        out += "/"
    return out + ">"

########NEW FILE########
__FILENAME__ = mockserver
from twisted.web.server import Site
from twisted.web.resource import Resource

class Root(Resource):

    def __init__(self):
        Resource.__init__(self)
        self.putChild("status", Status())

    def getChild(self, name, request):
        return self

    def render(self, request):
        return 'Slyd mock HTTP server\n'

# TODO: make PR for scrapy to share code
if __name__ == "__main__":
    root = Root()
    factory = Site(root)
    httpPort = reactor.listenTCP(8998, factory)

    def print_listening():
        httpHost = httpPort.getHost()
        print("Mock server running at http://%s:%d/" % (
            httpHost.host, httpHost.port))
    reactor.callWhenRunning(print_listening)
    reactor.run()
########NEW FILE########
__FILENAME__ = settings
"""
Slyd test settings

Imports slyd settings and adds necessary overrides for test setup
"""
from slyd.settings import *

LOG_LEVEL = 'DEBUG'

# testing never makes remote requests. A cache may serve stale content.
HTTPCACHE_ENABLED = False

RESOURCE_DIR = join(dirname(__file__), 'resources')
DATA_DIR = join(RESOURCE_DIR, 'data')
SPEC_DATA_DIR = join(DATA_DIR, 'projects')

########NEW FILE########
__FILENAME__ = test_bot
import json
from os.path import join
from twisted.trial import unittest
from twisted.internet.defer import inlineCallbacks
from twisted.web.server import Site
from twisted.web.static import File
from twisted.internet import reactor
from slyd.bot import create_bot_resource
from .utils import TestSite, test_spec_manager
from .settings import RESOURCE_DIR


class BotTest(unittest.TestCase):
    def setUp(self):
        # configure bot resource
        sm = test_spec_manager()
        self.bot_resource = create_bot_resource(sm)
        self.botsite = TestSite(self.bot_resource)

        # configure fake website to crawl
        docroot = join(RESOURCE_DIR, 'docroot')
        factory = Site(File(docroot))
        self.listen_port = reactor.listenTCP(8997, factory)


    def _fetch(self, url, **params):
        req = dict(params)
        req.setdefault('request', {})['url'] = url
        request_json = json.dumps(req)
        return self.botsite.post('fetch', data=request_json)

    @inlineCallbacks
    def test_fetch(self):
        # test status code
        result = yield self._fetch("http://localhost:8997/notexists")
        self.assertEqual(result.responseCode, 200)
        status = json.loads(result.value())['response']['status']
        self.assertEqual(status, 404)

        # get an existing file
        test_url = "http://localhost:8997/test.html"
        result = yield self._fetch(test_url)
        self.assertEqual(result.responseCode, 200)
        value = json.loads(result.value())
        # expect 200 response and base href added
        self.assertEqual(value['response']['status'], 200)
        self.assertIn('<base href="%s"' % test_url, value['page'])

        # parse fetched data
        test_url = "http://localhost:8997/pin1.html"
        result = yield self._fetch(test_url, spider='pinterest.com')
        self.assertEqual(result.responseCode, 200)
        value = json.loads(result.value())
        # check item
        item = value['items'][0]
        self.assertEqual(item['url'], test_url)
        self.assertEqual(item['name'][0], u'Luheca Designs')
        # check links
        self.assertIn('links', value)

    def tearDown(self):
        self.bot_resource.stop()
        self.listen_port.stopListening()

########NEW FILE########
__FILENAME__ = test_projects
import json
import unittest
from tempfile import mkdtemp
from os.path import join, exists
from shutil import rmtree
from scrapy.tests.mockserver import Status
from twisted.internet.defer import inlineCallbacks
from twisted.web.resource import NoResource, Resource
from .utils import TestSite, test_projects_resource
from .settings import DATA_DIR


class ProjectsTest(unittest.TestCase):

    def setUp(self):
        self.temp_projects_dir = mkdtemp(dir=DATA_DIR,
            prefix='test-run-')
        root = Resource()
        projects = test_projects_resource(self.temp_projects_dir)
        root.putChild('projects', projects)
        projects.putChild('status', Status())
        self.projectssite = TestSite(root, None)

    def check_project_exists(self, project_name):
        self.assertTrue(exists(join(self.temp_projects_dir, project_name)))
        self.assertTrue(
            exists(join(self.temp_projects_dir, project_name, 'spiders')))

    def check_project_not_exists(self, project_name):
        self.assertFalse(exists(join(self.temp_projects_dir, project_name)))

    @inlineCallbacks
    def test_childaccess(self):
        with self.assertRaises(NoResource):
            yield self.projectssite.get("projects/noresource")
        with self.assertRaises(NoResource):
            yield self.projectssite.get("projects/project/noresource")
        yield self.projectssite.get("projects/project/status")

    @inlineCallbacks
    def post_command(self, cmd, *args, **kwargs):
        obj = {'cmd': cmd, 'args': args}
        result = yield self.projectssite.post('projects', data=json.dumps(obj))
        self.assertEqual(result.responseCode, kwargs.get('expect', 200))

    @inlineCallbacks
    def test_list_projects(self):
        result = yield self.projectssite.get('projects')
        self.assertEqual(json.loads(result.value()), [])
        self.post_command('create', 'project1')
        self.post_command('create', 'project2')
        result = yield self.projectssite.get('projects')
        self.assertEqual(json.loads(result.value()), ['project1', 'project2'])

    def test_commands(self):
        self.post_command('rm', 'doesnotexist', expect=404)
        self.post_command('create', 'project1')
        self.check_project_exists('project1')
        self.post_command('mv', 'project1', 'project2')
        self.check_project_exists('project2')
        self.post_command('rm', 'project2')
        self.check_project_not_exists('project2')
        # Don't allow overwrites when creating or renaming projects
        self.post_command('create', 'project1')
        self.post_command('create', 'project1', expect=400)
        self.post_command('create', 'project2')
        self.post_command('mv', 'project1', 'project2', expect=400)

    def tearDown(self):
        rmtree(self.temp_projects_dir)

########NEW FILE########
__FILENAME__ = test_spec
import json
from tempfile import mkdtemp
from os.path import join, basename
from shutil import rmtree
from distutils.dir_util import copy_tree
from twisted.trial import unittest
from twisted.internet.defer import inlineCallbacks
from slyd.crawlerspec import create_crawler_spec_resource
from slyd.crawlerspec import convert_spider_templates
from .utils import TestSite, test_spec_manager
from .settings import SPEC_DATA_DIR


class CrawlerSpecTest(unittest.TestCase):
    spider = """
        {
            "exclude_patterns": [],
            "follow_patterns": [
                ".+MobileHomePark.php?key=d+"
            ],
            "links_to_follow": "patterns",
            "respect_nofollow": true,
            "start_urls": [
                "http://www.mhvillage.com/"
            ],
            "templates": []
        }
    """

    def setUp(self):
        sm = test_spec_manager()
        spec_resource = create_crawler_spec_resource(sm)
        self.temp_project_dir = mkdtemp(dir=SPEC_DATA_DIR,
            prefix='test-run-')
        self.project = basename(self.temp_project_dir)
        self.specsite = TestSite(spec_resource, project=self.project)
        test_project_dir = join(SPEC_DATA_DIR, 'test')
        copy_tree(test_project_dir, self.temp_project_dir)

    @inlineCallbacks
    def _get_check_resource(self, resource, converter=None):
        result = yield self.specsite.get(resource)
        ffile = join(self.temp_project_dir, resource + ".json")
        fdata = json.load(open(ffile))
        if converter:
            converter(fdata)
        rdata = json.loads(result.value())
        self.assertEqual(fdata, rdata)

    def test_get_resource(self):
        self._get_check_resource("project")
        self._get_check_resource("spiders/pinterest.com",
            convert_spider_templates)

    @inlineCallbacks
    def post_command(self, spider, cmd, *args, **kwargs):
        obj = {'cmd': cmd, 'args': args}
        result = yield self.specsite.post(spider, data=json.dumps(obj))
        self.assertEqual(result.responseCode, kwargs.get('expect', 200))

    @inlineCallbacks
    def test_updating(self):
        result = yield self.specsite.post('spiders/testpost', data=self.spider)
        self.assertEqual(result.responseCode, 200)
        result = yield self.specsite.get('spiders/testpost')
        self.assertEqual(json.loads(result.value()), json.loads(self.spider))

        # should fail - missing required fields
        result = yield self.specsite.post('spiders/testpost', data='{}')
        self.assertEqual(result.responseCode, 400)

    @inlineCallbacks
    def test_commands(self):
        self.post_command('spiders', 'unknown', expect=400)
        self.post_command('spiders', 'mv', expect=400)
        self.post_command('spiders', 'mv', '../notallowed', 'whatever', expect=400)
        self.post_command('spiders', 'mv', 'notallowedexists', 'whatever', expect=404)
        self.post_command('spiders', 'rm', 'notexists', expect=404)
        # TODO: mv to existing spider - 400
        yield self.specsite.post('spiders/c', data=self.spider)
        self._get_check_resource('spiders/c')
        self.post_command('spiders', 'mv', 'c', 'c2')
        result = yield self.specsite.get('spiders/c')
        self.assertEqual(result.value(), '{}\n')
        self._get_check_resource('spiders/c2')
        yield self.specsite.post('spiders/c3', data=self.spider)
        # overwrites
        self.post_command('spiders', 'mv', 'c2', 'c3')
        result = yield self.specsite.get('spiders/c2')
        self.assertEqual(result.value(), '{}\n')
        self.post_command('spiders', 'rm', 'c3')
        result = yield self.specsite.get('spiders/c3')
        self.assertEqual(result.value(), '{}\n')

    def tearDown(self):
        rmtree(self.temp_project_dir)

########NEW FILE########
__FILENAME__ = utils
from cStringIO import StringIO
from twisted.internet.defer import succeed
from twisted.web import server
from twisted.web.test.test_web import DummyRequest
from scrapy.settings import CrawlerSettings
from slyd.crawlerspec import CrawlerSpecManager
from slyd.projects import ProjectsResource
import tests.settings as test_settings


def test_spec_manager():
    """Create a CrawlerSpecManager configured to use test settings"""
    crawler_settings = CrawlerSettings(settings_module=test_settings)
    return CrawlerSpecManager(crawler_settings)

def test_projects_resource(temp_projects_dir):
    """Create a ProjectsResource configured to use test settings"""
    crawler_settings = CrawlerSettings(settings_module=test_settings)
    projects = ProjectsResource(crawler_settings)
    projects.projectsdir = temp_projects_dir
    return projects


class _SlydDummyRequest(DummyRequest):
    def __init__(self, method, url, project='test', data=None, args=None, headers=None):
        DummyRequest.__init__(self, url.split('/'))
        if data is not None:
            self.content = StringIO(data)
        if project is not None:
            self.project = project
        self.method = method
        self.headers.update(headers or {})
        # set args
        args = args or {}
        for k, v in args.items():
            self.addArg(k, v)

    def value(self):
        return "".join(self.written)


class TestSite(server.Site):
    """A Site used for test_settings

    Adds some convenience methods for GET and POST and result
    capture
    """

    def __init__(self, resource, project='test'):
        server.Site.__init__(self, resource)
        self.project = project

    def get(self, url, args=None, headers=None):
        return self._request("GET", url, args, headers, None)

    def post(self, url, data, args=None, headers=None):
        return self._request("POST", url, args, headers, data)

    def _request(self, method, url, args, headers, data):
        request = _SlydDummyRequest(method, url, self.project,
            data, args, headers)
        resource = self.getResourceFor(request)
        result = resource.render(request)
        return self._resolveResult(request, result)

    def _resolveResult(self, request, result):
        if isinstance(result, str):
            request.write(result)
            request.finish()
            return succeed(request)
        elif result is server.NOT_DONE_YET:
            if request.finished:
                return succeed(request)
            else:
                return request.notifyFinish().addCallback(lambda _: request)
        else:
            raise ValueError("Unexpected return value: %r" % (result,))

########NEW FILE########
__FILENAME__ = slyd_plugin
"""Registers 'twistd slyd' command."""
from twisted.application.service import ServiceMaker

finger = ServiceMaker(
    'slyd', 'slyd.tap', 'A server for creating scrapely spiders', 'slyd')

########NEW FILE########
