portia
======
Visual scraping for Scrapy.


Overview
========

Portia is a tool for visually scraping web sites without any programming knowledge. Just annotate web pages with a point and click editor to indicate what data you want to extract, and portia will learn how to scrape similar pages
from the site.

Portia has a web based UI served by a [Twisted] server, so you can install it on almost any modern platform.

Requirements
============

* Python 2.7
* Works on Linux, Windows, Mac OSX, BSD
* Supported browsers: Latest versions of Chrome (recommended) or Firefox

Installation
============

The recommended way to install dependencies is to use __virtualenv__ and then do:

    cd slyd
    pip install -r requirements.txt

As `slybot` is a `slyd` dependency, it will also get installed. Note that you may also need to use `sudo` or `pip --user` if you get permissions problems while installing. 


Running portia
==============

First, you need to start the ui and create a project. Run __slyd__ using:

	cd slyd
	twistd -n slyd

and point your browser to: `http://localhost:9001/static/main.html`

Choose the site you want to scrape and create a project. Every project is created with a default spider named after the domain of the site you are scraping. When you are ready, you can run your project with __slybot__ to do the actual crawling/extraction.

Projects created with __slyd__ can be found at:

	slyd/data/projects

To run one of those projects use:

	portiacrawl project_path spidername

Where `spidername` should be one of the project spiders. If you don't remember the name of the spider, just use:

	portiacrawl project_path

and you will get the list of spiders for that project.

Portia spiders are ultimately [Scrapy] spiders. You can pass __scrapy__ spider arguments when running them with ```portiacrawl``` by using the ```-a``` command line option. A custom settings module may also be specified using the ```--settings``` command line option. Please refer to the [scrapy documentation] for details on arguments and settings.

Repository structure
====================

There are two main components in this repository, __slyd__ and __slybot__:

###slyd

The visual editor used to create your scraping projects.

###slybot

The Python web crawler that performs the actual site scraping. It's implemented on top of the [Scrapy] web crawling
framework and the [Scrapely] extraction library. It uses projects created with __slyd__ as input.


[Twisted]: https://twistedmatrix.com
[Scrapely]: https://github.com/scrapy/scrapely
[Scrapy]: http://scrapy.org
[scrapy documentation]: http://doc.scrapy.org/en/latest

==============
Slybot crawler
==============

Slybot is a Python web crawler for doing web scraping. It's implemented on top of the
`Scrapy`_ web crawling framework and the `Scrapely`_ extraction library.

The documentation (including installation and usage) can be found at:
http://slybot.readthedocs.org/

.. _Scrapely: https://github.com/scrapy/scrapely
.. _Scrapy: http://scrapy.org

How to try it:
--------------

The recommended way to install dependencies is to use virtualenv and
then do:

	pip install -r requirements.txt

Run the server using:

	twistd -n slyd

and point your browser to:
	http://localhost:9001/static/main.html

Chrome and Firefox are supported, but it works better with chrome.

Slyd API Notes
--------------

This will be moved to separate docs - it's currently some notes for developers

All resources are either under /static/ or /projects/.


project listing/creation/deletion/renaming

To get list all existing projects, just GET http://localhost:9001/projects:

	$ curl http://localhost:9001/projects -> ["project1", "project2"]

New projects can be created by posting to /projects, for example:

	$ curl -d '{"cmd": "create", "args": ["project_X"]}' http://localhost:9001/projects

To delete a project:

	$ curl -d '{"cmd": "rm", "args": ["project_X"]}' http://localhost:9001/projects

To rename a project:

	$ curl -d '{"cmd": "mv", "args": ["oldname", "newname"]}' http://localhost:9001/projects

Please note that projects will not be overwritten when renaming or creating new ones (if a project
with the given name already exists an error from the 400 family will be returned).

spec

The project specification is available under /projects/PROJECT_ID/spec. The path format
mirrors the slybot format documented here:
http://slybot.readthedocs.org/en/latest/project.html

Currently, this is read only, but it will soon support PUT/POST.

The entire spec is returned for a GET request to the root:

	$ curl http://localhost:9001/projects/78/spec
	{"project": {
    "version": "1308771278",
    "name": "demo"
    ..
	}

A list of available spiders can be retrieved:

  $ curl http://localhost:9001/projects/78/spec/spiders
["accommodationforstudents.com", "food.com", "pinterest.com", "pin", "mhvillage"]

and specific resources can be requested:

	$ curl http://localhost:9001/projects/78/spec/spiders/accommodationforstudents.com
	{
    	"templates":
    ...
	    "respect_nofollow": true
	}

The spec can be updating by POSTing:

  $ curl --data @newlinkedin.js http://localhost:9001/projects/78/spec/spiders/linkedin

An HTTP 400 will be returned if the uploaded spec does not validate.

Basic commands are available for manipulating spider files. For example:

  $ curl -d '{"cmd": "rm", "args": ["spidername"]}' http://localhost:9001/projects/78/spec/spiders

Available commands are:
* mv - move spider from first arg to second. If the second exists it is overwritten.
* rm - delete spider


bot/fetch

Accepts json object with the following fields:
* request - same as scrapy requst object. At least needs a url
* spider - spider name within in the project
* page_id - unique ID for this page, must match the id used in templates (not yet implemented)
* parent_fp - fingerprint of parent request. This is used for managing referrer url, cookies, etc.

Returns a json object containing (so far):
* page - page content, not yet annotated but will be
* response - object containing the response data: http code and headers
* items - array of items extracted
* fp - request fingerprint
* error - error message, present if there was an error
* links - array of links followed

Coming soon in the response:
* template_id - id of template that matched
* trace - textual trace of the matching process - for debugging


If you want to work on an existing project, put it in data/projects/PROJECTID, these can be downloaded from dash or by:

$ bin/sh2sly data/projects -p 78 -k YOURAPIKEY

Then you can extract data:

$ curl -d '{"request": {"url": "http://www.pinterest.com/pin/339740365610932893/"}, "spider": "pinterest.com"}' http://localhost:9001/projects/78/bot/fetch
{
  "fp": "0f2686acdc6a71eeddc49045b7cea0b6f81e6b61",
   "items": [
      {
         "url": "http://www.pinterest.com/pin/339740365610932893/",
         "_template": "527387aa4d6c7133c6551481",
         "image": [
            "http://media-cache-ak0.pinimg.com/736x/6c/c5/35/6cc5352046df0f8d8852cbdfb31542bb.jpg"
         ],
         "_type": "pin",
         "name": [
            "Career Driven"
         ]
      }
   ],
   "page": "<!DOCTYPE html>\n ...."
}

Testing
-------

A Karma test eviroment is available. To run the ui tests:

    npm install
    export PATH="./node_modules/.bin:$PATH"
    karma start

You can download npm from https://npmjs.org

Look at karma.conf.js to configure test options.

The tests are located in:
  media/tests

slyd can be tested using twisted:

    trial tests

