__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# SimpleCV documentation build configuration file, created by
# sphinx-quickstart on Tue Nov  8 16:38:59 2011.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys,os



# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'SimpleCV'
copyright = u'2011, Sight Machine'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '1.3'
# The full version, including alpha/beta/rc tags.
release = '1.3'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'sightmachine'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
html_theme_path = ["."]

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
html_logo = 'simplecv.png'

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
html_show_sphinx = False

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'SimpleCVdoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'SimpleCV.tex', u'SimpleCV Documentation',
   u'Ingeuitas', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'simplecv', u'SimpleCV Documentation',
     [u'Ingeuitas'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'SimpleCV', u'SimpleCV Documentation', u'Ingeuitas',
   'SimpleCV', 'One line description of project.', 'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'


# -- Options for Epub output ---------------------------------------------------

# Bibliographic Dublin Core info.
epub_title = u'SimpleCV'
epub_author = u'Ingeuitas'
epub_publisher = u'Ingeuitas'
epub_copyright = u'2011, Ingeuitas'

# The language of the text. It defaults to the language option
# or en if the language is not set.
#epub_language = ''

# The scheme of the identifier. Typical schemes are ISBN or URL.
#epub_scheme = ''

# The unique identifier of the text. This can be a ISBN number
# or the project homepage.
#epub_identifier = ''

# A unique identification for the text.
#epub_uid = ''

# A tuple containing the cover image and cover page html template filenames.
#epub_cover = ()

# HTML files that should be inserted before the pages created by sphinx.
# The format is a list of tuples containing the path and title.
#epub_pre_files = []

# HTML files shat should be inserted after the pages created by sphinx.
# The format is a list of tuples containing the path and title.
#epub_post_files = []

# A list of files that should not be packed into the epub file.
#epub_exclude_files = []

# The depth of the table of contents in toc.ncx.
#epub_tocdepth = 3

# Allow duplicate toc entries.
#epub_tocdup = True

########NEW FILE########
__FILENAME__ = findmods
#!/usr/bin/python

from SimpleCV import *
import re
from types import NoneType

#this is a utility just to make sure that the modules/versions you're
#using on the current system match what is in the install scrip

easy_installed_pkgs = dict()
easy_installed_path = "/Library/Python/2.6/site-packages";

for k in sys.modules.keys():
    if type(sys.modules[k]) == NoneType:
        continue

    fn = ""
    try:
        fn = sys.modules[k].__file__
    except:
        continue

    if (re.match(easy_installed_path, fn)):
        junk, relpath = re.split(easy_installed_path, fn)
        dirs = re.split("/", relpath)
        easy_installed_pkgs[dirs[1]] = 1


for egg in easy_installed_pkgs.keys():
    print easy_installed_path + "/" + egg

########NEW FILE########
__FILENAME__ = demo_cv_async
#!/usr/bin/env python
import freenect
import cv
import frame_convert

cv.NamedWindow('Depth')
cv.NamedWindow('RGB')
keep_running = True


def display_depth(dev, data, timestamp):
    global keep_running
    cv.ShowImage('Depth', frame_convert.pretty_depth_cv(data))
    if cv.WaitKey(10) == 27:
        keep_running = False


def display_rgb(dev, data, timestamp):
    global keep_running
    cv.ShowImage('RGB', frame_convert.video_cv(data))
    if cv.WaitKey(10) == 27:
        keep_running = False


def body(*args):
    if not keep_running:
        raise freenect.Kill


print('Press ESC in window to stop')
freenect.runloop(depth=display_depth,
                 video=display_rgb,
                 body=body)

########NEW FILE########
__FILENAME__ = demo_cv_sync
#!/usr/bin/env python
import freenect
import cv
import frame_convert

cv.NamedWindow('Depth')
cv.NamedWindow('Video')
print('Press ESC in window to stop')


def get_depth():
    return frame_convert.pretty_depth_cv(freenect.sync_get_depth()[0])


def get_video():
    return frame_convert.video_cv(freenect.sync_get_video()[0])


while 1:
    cv.ShowImage('Depth', get_depth())
    cv.ShowImage('Video', get_video())
    if cv.WaitKey(10) == 27:
        break

########NEW FILE########
__FILENAME__ = demo_cv_sync_multi
#!/usr/bin/env python
"""This goes through each kinect on your system, grabs one frame and
displays it.  Uncomment the commented line to shut down after each frame
if your system can't handle it (will get very low FPS but it should work).
This will keep trying indeces until it finds one that doesn't work, then it
starts from 0.
"""
import freenect
import cv
import frame_convert

cv.NamedWindow('Depth')
cv.NamedWindow('Video')
ind = 0
print('%s\nPress ESC to stop' % __doc__)


def get_depth(ind):
    return frame_convert.pretty_depth_cv(freenect.sync_get_depth(ind)[0])


def get_video(ind):
    return frame_convert.video_cv(freenect.sync_get_video(ind)[0])


while 1:
    print(ind)
    try:
        depth = get_depth(ind)
        video = get_video(ind)
    except TypeError:
        ind = 0
        continue
    ind += 1
    cv.ShowImage('Depth', depth)
    cv.ShowImage('Video', video)
    if cv.WaitKey(10) == 27:
        break
    #freenect.sync_stop()  # NOTE: Uncomment if your machine can't handle it

########NEW FILE########
__FILENAME__ = demo_cv_threshold
#!/usr/bin/env python
import freenect
import cv
import frame_convert
import numpy as np


threshold = 100
current_depth = 0


def change_threshold(value):
    global threshold
    threshold = value


def change_depth(value):
    global current_depth
    current_depth = value


def show_depth():
    global threshold
    global current_depth

    depth, timestamp = freenect.sync_get_depth()
    depth = 255 * np.logical_and(depth >= current_depth - threshold,
                                 depth <= current_depth + threshold)
    depth = depth.astype(np.uint8)
    image = cv.CreateImageHeader((depth.shape[1], depth.shape[0]),
                                 cv.IPL_DEPTH_8U,
                                 1)
    cv.SetData(image, depth.tostring(),
               depth.dtype.itemsize * depth.shape[1])
    cv.ShowImage('Depth', image)


def show_video():
    cv.ShowImage('Video', frame_convert.video_cv(freenect.sync_get_video()[0]))


cv.NamedWindow('Depth')
cv.NamedWindow('Video')
cv.CreateTrackbar('threshold', 'Depth', threshold,     500,  change_threshold)
cv.CreateTrackbar('depth',     'Depth', current_depth, 2048, change_depth)

print('Press ESC in window to stop')


while 1:
    show_depth()
    show_video()
    if cv.WaitKey(10) == 27:
        break

########NEW FILE########
__FILENAME__ = demo_cv_thresh_sweep
#!/usr/bin/env python
"""Sweeps throught the depth image showing 100 range at a time"""
import freenect
import cv
import numpy as np
import time

cv.NamedWindow('Depth')


def disp_thresh(lower, upper):
    depth, timestamp = freenect.sync_get_depth()
    depth = 255 * np.logical_and(depth > lower, depth < upper)
    depth = depth.astype(np.uint8)
    image = cv.CreateImageHeader((depth.shape[1], depth.shape[0]),
                                 cv.IPL_DEPTH_8U,
                                 1)
    cv.SetData(image, depth.tostring(),
               depth.dtype.itemsize * depth.shape[1])
    cv.ShowImage('Depth', image)
    cv.WaitKey(10)


lower = 0
upper = 100
max_upper = 2048
while upper < max_upper:
    print('%d < depth < %d' % (lower, upper))
    disp_thresh(lower, upper)
    time.sleep(.1)
    lower += 20
    upper += 20

########NEW FILE########
__FILENAME__ = demo_mp_async
#!/usr/bin/env python
import freenect
import matplotlib.pyplot as mp
import signal
import frame_convert

mp.ion()
image_rgb = None
image_depth = None
keep_running = True


def display_depth(dev, data, timestamp):
    global image_depth
    data = frame_convert.pretty_depth(data)
    mp.gray()
    mp.figure(1)
    if image_depth:
        image_depth.set_data(data)
    else:
        image_depth = mp.imshow(data, interpolation='nearest', animated=True)
    mp.draw()


def display_rgb(dev, data, timestamp):
    global image_rgb
    mp.figure(2)
    if image_rgb:
        image_rgb.set_data(data)
    else:
        image_rgb = mp.imshow(data, interpolation='nearest', animated=True)
    mp.draw()


def body(*args):
    if not keep_running:
        raise freenect.Kill


def handler(signum, frame):
    global keep_running
    keep_running = False


print('Press Ctrl-C in terminal to stop')
signal.signal(signal.SIGINT, handler)
freenect.runloop(depth=display_depth,
                 video=display_rgb,
                 body=body)

########NEW FILE########
__FILENAME__ = demo_mp_sync
#!/usr/bin/env python
import freenect
import matplotlib.pyplot as mp
import frame_convert
import signal

keep_running = True


def get_depth():
    return frame_convert.pretty_depth(freenect.sync_get_depth()[0])


def get_video():
    return freenect.sync_get_video()[0]


def handler(signum, frame):
    """Sets up the kill handler, catches SIGINT"""
    global keep_running
    keep_running = False


mp.ion()
mp.gray()
mp.figure(1)
image_depth = mp.imshow(get_depth(), interpolation='nearest', animated=True)
mp.figure(2)
image_rgb = mp.imshow(get_video(), interpolation='nearest', animated=True)
print('Press Ctrl-C in terminal to stop')
signal.signal(signal.SIGINT, handler)

while keep_running:
    mp.figure(1)
    image_depth.set_data(get_depth())
    mp.figure(2)
    image_rgb.set_data(get_video())
    mp.draw()
    mp.waitforbuttonpress(0.01)

########NEW FILE########
__FILENAME__ = demo_tilt
#!/usr/bin/env python
import freenect
import time
import random
import signal

keep_running = True
last_time = 0


def body(dev, ctx):
    global last_time
    if not keep_running:
        raise freenect.Kill
    if time.time() - last_time < 3:
        return
    last_time = time.time()
    led = random.randint(0, 6)
    tilt = random.randint(0, 30)
    freenect.set_led(dev, led)
    freenect.set_tilt_degs(dev, tilt)
    print('led[%d] tilt[%d] accel[%s]' % (led, tilt, freenect.get_accel(dev)))


def handler(signum, frame):
    """Sets up the kill handler, catches SIGINT"""
    global keep_running
    keep_running = False
print('Press Ctrl-C in terminal to stop')
signal.signal(signal.SIGINT, handler)
freenect.runloop(body=body)

########NEW FILE########
__FILENAME__ = frame_convert
import numpy as np


def pretty_depth(depth):
    """Converts depth into a 'nicer' format for display

    This is abstracted to allow for experimentation with normalization

    Args:
        depth: A numpy array with 2 bytes per pixel

    Returns:
        A numpy array that has been processed whos datatype is unspecified
    """
    np.clip(depth, 0, 2**10 - 1, depth)
    depth >>= 2
    depth = depth.astype(np.uint8)
    return depth


def pretty_depth_cv(depth):
    """Converts depth into a 'nicer' format for display

    This is abstracted to allow for experimentation with normalization

    Args:
        depth: A numpy array with 2 bytes per pixel

    Returns:
        An opencv image who's datatype is unspecified
    """
    import cv
    depth = pretty_depth(depth)
    image = cv.CreateImageHeader((depth.shape[1], depth.shape[0]),
                                 cv.IPL_DEPTH_8U,
                                 1)
    cv.SetData(image, depth.tostring(),
               depth.dtype.itemsize * depth.shape[1])
    return image


def video_cv(video):
    """Converts video into a BGR format for opencv

    This is abstracted out to allow for experimentation

    Args:
        video: A numpy array with 1 byte per pixel, 3 channels RGB

    Returns:
        An opencv image who's datatype is 1 byte, 3 channel BGR
    """
    import cv
    video = video[:, :, ::-1]  # RGB -> BGR
    image = cv.CreateImageHeader((video.shape[1], video.shape[0]),
                                 cv.IPL_DEPTH_8U,
                                 3)
    cv.SetData(image, video.tostring(),
               video.dtype.itemsize * 3 * video.shape[1])
    return image

########NEW FILE########
__FILENAME__ = mkvirt
import os
import virtualenv, textwrap

here = os.path.dirname(os.path.abspath(__file__))
base_dir = os.path.dirname(here)

print "Creating SimpleCV Bootstrap Install Script: simplecv-bootstrap.py"

output = virtualenv.create_bootstrap_script(textwrap.dedent("""
import os, subprocess

def after_install(options, home_dir):
    base_dir = os.path.dirname(home_dir)
    logger.notify('Installing SimpleCV into Virtual Environment')

    os.chdir(home_dir)
    print 'Current Directory:', os.getcwd()
    print 'dir list:', os.listdir(os.getcwd())
    print 'Symlinking OpenCV'
    os.symlink('/usr/local/lib/python2.7/dist-packages/cv2.so', os.path.join(os.getcwd(),'lib/python2.7/site-packages/cv2.so'))
    os.symlink('/usr/local/lib/python2.7/dist-packages/cv.py', os.path.join(os.getcwd(),'lib/python2.7/site-packages/cv.py'))
    subprocess.call(['pwd'])
    os.mkdir('src')
    os.chdir('src')
    subprocess.call(['wget','-O','pygame.tar.gz','http://github.com/xamox/pygame/tarball/master'])
    subprocess.call(['tar','zxvf','pygame.tar.gz'])
    print 'Runing setup for pygame'
    subprocess.call(['ls'])
    os.chdir('../')
    print 'Current Directory:', os.getcwd()
    print os.getcwd()
    subprocess.call(['./bin/python','src/xamox-pygame-3e48d10/setup.py','install'])


"""))
f = open('simplecv-bootstrap.py', 'w').write(output)


#~ sudo apt-get install python-opencv python-setuptools python-pip gfortran g++ liblapack-dev libsdl1.2-dev libsmpeg-dev mercurial

#~ source bin/activate
#~ cd src/pygame
#~ python setup.py -setuptools install
#~ pip install https://github.com/sightmachine/SimpleCV/zipball/masteriii
#    call_subprocess(['python','setup.py','-setuptools','install'], show_stdout=True)
#call_subprocess([join(home_dir, 'bin', 'pip'),'install','-r','requirements.txt'], show_stdout=True)

########NEW FILE########
__FILENAME__ = base
#!/usr/bin/python

# SimpleCV system includes
import os
import sys
import warnings
import time
import socket
import re
import urllib2
import types
import SocketServer
import threading
import tempfile
import zipfile
import pickle
import glob #for directory scanning
import abc #abstract base class
import colorsys
import logging
import pygame as pg
import scipy.ndimage as ndimage
import scipy.stats.stats as sss  #for auto white balance
import scipy.cluster.vq as scv
import scipy.linalg as nla  # for linear algebra / least squares
import math # math... who does that
import copy # for deep copy
import numpy as np
import scipy.spatial.distance as spsd
import scipy.cluster.vq as cluster #for kmeans
import pygame as pg
import platform
import copy
import types
import time
import itertools #for track

from numpy import linspace
from scipy.interpolate import UnivariateSpline
from warnings import warn
from copy import copy
from math import *
from pkg_resources import load_entry_point
from SimpleHTTPServer import SimpleHTTPRequestHandler
from types import IntType, LongType, FloatType, InstanceType
from cStringIO import StringIO
from numpy import int32
from numpy import uint8
from EXIF import *
from pygame import gfxdraw
from pickle import *

# SimpleCV library includes
try:
    import cv2.cv as cv
except ImportError:
    try:
        import cv
    except ImportError:
        raise ImportError("Cannot load OpenCV library which is required by SimpleCV")


#optional libraries
PIL_ENABLED = True
try:
    from PIL import Image as pil
    from PIL import ImageFont as pilImageFont
    from PIL import ImageDraw as pilImageDraw
    from PIL import GifImagePlugin
    getheader = GifImagePlugin.getheader
    getdata   = GifImagePlugin.getdata
except ImportError:
    try:
        import Image as pil
        from GifImagePlugin import getheader, getdata
    except ImportError:
        PIL_ENABLED = False

FREENECT_ENABLED = True
try:
    import freenect
except ImportError:
    FREENECT_ENABLED = False

ZXING_ENABLED = True
try:
    import zxing
except ImportError:
    ZXING_ENABLED = False

OCR_ENABLED = True
try:
    import tesseract
except ImportError:
    OCR_ENABLED = False


PYSCREENSHOT_ENABLED = True
try:
    import pyscreenshot
except ImportError:
    PYSCREENSHOT_ENABLED = False

ORANGE_ENABLED = True
try:
    try:
        import orange
    except ImportError:
        import Orange; import orange

    import orngTest #for cross validation
    import orngStat
    import orngEnsemble # for bagging / boosting

except ImportError:
    ORANGE_ENABLED = False

VIMBA_ENABLED = True
try:
    import pymba
except ImportError:
    #TODO Log an error the pymba is not installed
    VIMBA_ENABLED = False
except Exception:
    #TODO Log an error that AVT Vimba DLL is not installed properly
    VIMBA_ENABLED = False

class InitOptionsHandler(object):
    """
    **summary**

    this handler is supposed to store global variables. for now, its only value
    defines if simplecv is being run on an ipython notebook.

    """

    def __init__(self):
        self.on_notebook = False
        self.headless = False

    def enable_notebook(self):
        self.on_notebook = True

    def set_headless(self):
        # set SDL to use the dummy NULL video driver,
        # so it doesn't need a windowing system.
        os.environ["SDL_VIDEODRIVER"] = "dummy"
        self.headless = True

init_options_handler = InitOptionsHandler()

try:
    import pygame as pg
except ImportError:
    init_options_handler.set_headless()

#couple quick typecheck helper functions
def is_number(n):
    """
    Determines if it is a number or not

    Returns: Type
    """
    return type(n) in (IntType, LongType, FloatType)

def is_tuple(n):
    """
    Determines if it is a tuple or not

    Returns: Boolean
    """
    return type(n) == tuple

def reverse_tuple(n):
    """
    Reverses a tuple

    Returns: Tuple
    """
    return tuple(reversed(n))

def find(f, seq):
    """
    Search for item in a list

    Returns: Boolean
    """
    for item in seq:
        if (f == item):
            return True
    return False

def test():
    """
    This function is meant to run builtin unittests
    """

    print 'unit test'


def download_and_extract(URL):
    """
    This function takes in a URL for a zip file, extracts it and returns
    the temporary path it was extracted to
    """
    if URL == None:
        logger.warning("Please provide URL")
        return None

    tmpdir = tempfile.mkdtemp()
    filename = os.path.basename(URL)
    path = tmpdir + "/" + filename
    zdata = urllib2.urlopen(URL)

    print "Saving file to disk please wait...."
    with open(path, "wb") as local_file:
        local_file.write(zdata.read())

    zfile = zipfile.ZipFile(path)
    print "Extracting zipfile"
    try:
        zfile.extractall(tmpdir)
    except:
        logger.warning("Couldn't extract zip file")
        return None

    return tmpdir

def int_to_bin(i):
    """Integer to two bytes"""
    i1 = i % 256
    i2 = int(i/256)
    return chr(i1) + chr(i2)

def npArray2cvMat(inputMat, dataType=cv.CV_32FC1):
    """
    This function is a utility for converting numpy arrays to the cv.cvMat format.

    Returns: cvMatrix
    """
    if( type(inputMat) == np.ndarray ):
        sz = len(inputMat.shape)
        temp_mat = None
        if( dataType == cv.CV_32FC1 or dataType == cv.CV_32FC2 or dataType == cv.CV_32FC3 or dataType == cv.CV_32FC4 ):
            temp_mat = np.array(inputMat, dtype='float32')
        elif( dataType == cv.CV_8UC1 or  dataType == cv.CV_8UC2 or dataType == cv.CV_8UC3 or dataType == cv.CV_8UC3):
            temp_mat = np.array(inputMat,dtype='uint8')
        else:
            logger.warning("MatrixConversionUtil: the input matrix type is not supported")
            return None
        if( sz == 1 ): #this needs to be changed so we can do row/col vectors
            retVal = cv.CreateMat(inputMat.shape[0], 1, dataType)
            cv.SetData(retVal, temp_mat.tostring(), temp_mat.dtype.itemsize * temp_mat.shape[0])
        elif( sz == 2 ):
            retVal = cv.CreateMat(temp_mat.shape[0], temp_mat.shape[1], dataType)
            cv.SetData(retVal, temp_mat.tostring(), temp_mat.dtype.itemsize * temp_mat.shape[1])
        elif( sz > 2 ):
            logger.warning("MatrixConversionUtil: the input matrix type is not supported")
            return None
        return retVal
    else:
        logger.warning("MatrixConversionUtil: the input matrix type is not supported")

#Logging system - Global elements

consoleHandler = logging.StreamHandler()
formatter = logging.Formatter('%(levelname)s: %(message)s')
consoleHandler.setFormatter(formatter)
logger = logging.getLogger('Main Logger')
logger.addHandler(consoleHandler)

try:
    import IPython
    ipython_version = IPython.__version__
except ImportError:
    ipython_version = None

#This is used with sys.excepthook to log all uncaught exceptions.
#By default, error messages ARE print to stderr.
def exception_handler(excType, excValue, traceback):
    logger.error("", exc_info=(excType, excValue, traceback))

    #print "Hey!",excValue
    #excValue has the most important info about the error.
    #It'd be possible to display only that and hide all the (unfriendly) rest.

sys.excepthook = exception_handler

def ipython_exception_handler(shell, excType, excValue, traceback,tb_offset=0):
    logger.error("", exc_info=(excType, excValue, traceback))


#The two following functions are used internally.
def init_logging(log_level):
    logger.setLevel(log_level)

def read_logging_level(log_level):
    levels_dict = {
        1: logging.DEBUG, "debug": logging.DEBUG,
        2: logging.INFO, "info": logging.INFO,
        3: logging.WARNING, "warning": logging.WARNING,
        4: logging.ERROR, "error": logging.ERROR,
        5: logging.CRITICAL, "critical": logging.CRITICAL
    }

    if isinstance(log_level,str):
        log_level = log_level.lower()

    if log_level in levels_dict:
        return levels_dict[log_level]
    else:
        print "The logging level given is not valid"
        return None

def get_logging_level():
    """
    This function prints the current logging level of the main logger.
    """
    levels_dict = {
        10: "DEBUG",
        20: "INFO",
        30: "WARNING",
        40: "ERROR",
        50: "CRITICAL"
    }

    print "The current logging level is:", levels_dict[logger.getEffectiveLevel()]

def set_logging(log_level,myfilename = None):
    """
    This function sets the threshold for the logging system and, if desired,
    directs the messages to a logfile. Level options:

    'DEBUG' or 1
    'INFO' or 2
    'WARNING' or 3
    'ERROR' or 4
    'CRITICAL' or 5

    If the user is on the interactive shell and wants to log to file, a custom
    excepthook is set. By default, if logging to file is not enabled, the way
    errors are displayed on the interactive shell is not changed.
    """

    if myfilename and ipython_version:
        try:
            if ipython_version.startswith("0.10"):
                __IPYTHON__.set_custom_exc((Exception,), ipython_exception_handler)
            else:
                ip = get_ipython()
                ip.set_custom_exc((Exception,), ipython_exception_handler)
        except NameError: #In case the interactive shell is not being used
            sys.exc_clear()


    level = read_logging_level(log_level)

    if level and myfilename:
        fileHandler = logging.FileHandler(filename=myfilename)
        fileHandler.setLevel(level)
        fileHandler.setFormatter(formatter)
        logger.addHandler(fileHandler)
        logger.removeHandler(consoleHandler) #Console logging is disabled.
        print "Now logging to",myfilename,"with level",log_level
    elif level:
        print "Now logging with level",log_level

    logger.setLevel(level)

def system():
    """
    
    **SUMMARY**
    
    Output of this function includes various informations related to system and library.
    
    Main purpose:
    - While submiting a bug, report the output of this function
    - Checking the current version and later upgrading the library based on the output
    
    **RETURNS**
    
    None

    **EXAMPLE**
      
      >>> import SimpleCV
      >>> SimpleCV.system()
      
      
    """
    try :
        import platform
        print "System : ", platform.system()
        print "OS version : ", platform.version()
        print "Python version :", platform.python_version()
        try :
            from cv2 import __version__
            print "Open CV version : " + __version__
        except ImportError :
            print "Open CV2 version : " + "2.1"
        if (PIL_ENABLED) :
            print "PIL version : ", pil.VERSION
        else :
            print "PIL module not installed"
        if (ORANGE_ENABLED) :
            print "Orange Version : " + orange.version
        else :
            print "Orange module not installed"
        try :
            import pygame as pg
            print "PyGame Version : " + pg.__version__
        except ImportError:
            print "PyGame module not installed"
        try :
            import pickle
            print "Pickle Version : " + pickle.__version__
        except :
            print "Pickle module not installed"

    except ImportError :
        print "You need to install Platform to use this function"
        print "to install you can use:"
        print "easy_install platform"
    return

class LazyProperty(object):

    def __init__(self, func):
        self._func = func
        self.__name__ = func.__name__
        self.__doc__ = func.__doc__

    def __get__(self, obj, klass=None):
        if obj is None: return None
        result = obj.__dict__[self.__name__] = self._func(obj)
        return result

#supported image formats regular expression ignoring case
IMAGE_FORMATS = ('*.[bB][mM][Pp]','*.[Gg][Ii][Ff]','*.[Jj][Pp][Gg]','*.[jJ][pP][eE]',
'*.[jJ][Pp][Ee][Gg]','*.[pP][nN][gG]','*.[pP][bB][mM]','*.[pP][gG][mM]','*.[pP][pP][mM]',
'*.[tT][iI][fF]','*.[tT][iI][fF][fF]','*.[wW][eE][bB][pP]')

#maximum image size -
MAX_DIMENSION = 2*6000 # about twice the size of a full 35mm images - if you hit this, you got a lot data.
LAUNCH_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__)))

########NEW FILE########
__FILENAME__ = Camera
# SimpleCV Cameras & Devices

#load system libraries
from SimpleCV.base import *
from SimpleCV.ImageClass import Image, ImageSet, ColorSpace
from SimpleCV.Display import Display
from SimpleCV.Color import Color
from collections import deque
import time
import ctypes as ct
import subprocess
import cv2
import numpy as np
import traceback
import sys

#Globals
_cameras = []
_camera_polling_thread = ""
_index = []

class FrameBufferThread(threading.Thread):
    """
    **SUMMARY**

    This is a helper thread which continually debuffers the camera frames.  If
    you don't do this, cameras may constantly give you a frame behind, which
    causes problems at low sample rates.  This makes sure the frames returned
    by your camera are fresh.

    """
    def run(self):
        global _cameras
        while (1):
            for cam in _cameras:
                if cam.pygame_camera:
                    cam.pygame_buffer = cam.capture.get_image(cam.pygame_buffer)
                else:
                    cv.GrabFrame(cam.capture)
                cam._threadcapturetime = time.time()
            time.sleep(0.04)    #max 25 fps, if you're lucky



class FrameSource:
    """
    **SUMMARY**

    An abstract Camera-type class, for handling multiple types of video input.
    Any sources of images inheirit from it

    """
    _calibMat = "" #Intrinsic calibration matrix
    _distCoeff = "" #Distortion matrix
    _threadcapturetime = '' #when the last picture was taken
    capturetime = '' #timestamp of the last aquired image

    def __init__(self):
        return

    def getProperty(self, p):
        return None

    def getAllProperties(self):
        return {}

    def getImage(self):
        return None

    def calibrate(self, imageList, grid_sz=0.03, dimensions=(8, 5)):
        """
        **SUMMARY**

        Camera calibration will help remove distortion and fisheye effects
        It is agnostic of the imagery source, and can be used with any camera

        The easiest way to run calibration is to run the
        calibrate.py file under the tools directory for SimpleCV.
        This will walk you through the calibration process.

        **PARAMETERS**

        * *imageList* - is a list of images of color calibration images.

        * *grid_sz* - is the actual grid size of the calibration grid, the unit used will be
          the calibration unit value (i.e. if in doubt use meters, or U.S. standard)

        * *dimensions* - is the the count of the *interior* corners in the calibration grid.
          So for a grid where there are 4x4 black grid squares has seven interior corners.

        **RETURNS**

        The camera's intrinsic matrix.

        **EXAMPLE**

        See :py:module:calibrate.py

        """
        # This routine was adapted from code originally written by:
        # Abid. K  -- abidrahman2@gmail.com
        # See: https://github.com/abidrahmank/OpenCV-Python/blob/master/Other_Examples/camera_calibration.py

        warn_thresh = 1
        n_boards = 0    #no of boards
        board_w = int(dimensions[0])    # number of horizontal corners
        board_h = int(dimensions[1])    # number of vertical corners
        n_boards = int(len(imageList))
        board_n = board_w * board_h             # no of total corners
        board_sz = (board_w, board_h)   #size of board
        if( n_boards < warn_thresh ):
            logger.warning("FrameSource.calibrate: We suggest using 20 or more images to perform camera calibration!" )

        #  creation of memory storages
        image_points = cv.CreateMat(n_boards * board_n, 2, cv.CV_32FC1)
        object_points = cv.CreateMat(n_boards * board_n, 3, cv.CV_32FC1)
        point_counts = cv.CreateMat(n_boards, 1, cv.CV_32SC1)
        intrinsic_matrix = cv.CreateMat(3, 3, cv.CV_32FC1)
        distortion_coefficient = cv.CreateMat(5, 1, cv.CV_32FC1)

        #       capture frames of specified properties and modification of matrix values
        i = 0
        z = 0           # to print number of frames
        successes = 0
        imgIdx = 0
        #       capturing required number of views
        while(successes < n_boards):
            found = 0
            img = imageList[imgIdx]
            (found, corners) = cv.FindChessboardCorners(img.getGrayscaleMatrix(), board_sz,
                                                     cv.CV_CALIB_CB_ADAPTIVE_THRESH |
                                                     cv.CV_CALIB_CB_FILTER_QUADS)
            corners = cv.FindCornerSubPix(img.getGrayscaleMatrix(), corners,(11, 11),(-1, -1),
                                        (cv.CV_TERMCRIT_EPS + cv.CV_TERMCRIT_ITER, 30, 0.1))
            # if got a good image,draw chess board
            if found == 1:
                corner_count = len(corners)
                z = z + 1

            # if got a good image, add to matrix
            if len(corners) == board_n:
                step = successes * board_n
                k = step
                for j in range(board_n):
                    cv.Set2D(image_points, k, 0, corners[j][0])
                    cv.Set2D(image_points, k, 1, corners[j][1])
                    cv.Set2D(object_points, k, 0, grid_sz*(float(j)/float(board_w)))
                    cv.Set2D(object_points, k, 1, grid_sz*(float(j)%float(board_w)))
                    cv.Set2D(object_points, k, 2, 0.0)
                    k = k + 1
                cv.Set2D(point_counts, successes, 0, board_n)
                successes = successes + 1

        # now assigning new matrices according to view_count
        if( successes < warn_thresh ):
            logger.warning("FrameSource.calibrate: You have %s good images for calibration we recommend at least %s" % (successes, warn_thresh))

        object_points2 = cv.CreateMat(successes * board_n, 3, cv.CV_32FC1)
        image_points2 = cv.CreateMat(successes * board_n, 2, cv.CV_32FC1)
        point_counts2 = cv.CreateMat(successes, 1, cv.CV_32SC1)

        for i in range(successes * board_n):
            cv.Set2D(image_points2, i, 0, cv.Get2D(image_points, i, 0))
            cv.Set2D(image_points2, i, 1, cv.Get2D(image_points, i, 1))
            cv.Set2D(object_points2, i, 0, cv.Get2D(object_points, i, 0))
            cv.Set2D(object_points2, i, 1, cv.Get2D(object_points, i, 1))
            cv.Set2D(object_points2, i, 2, cv.Get2D(object_points, i, 2))
        for i in range(successes):
            cv.Set2D(point_counts2, i, 0, cv.Get2D(point_counts, i, 0))

        cv.Set2D(intrinsic_matrix, 0, 0, 1.0)
        cv.Set2D(intrinsic_matrix, 1, 1, 1.0)
        rcv = cv.CreateMat(n_boards, 3, cv.CV_64FC1)
        tcv = cv.CreateMat(n_boards, 3, cv.CV_64FC1)
        # camera calibration
        cv.CalibrateCamera2(object_points2, image_points2, point_counts2,
                            (img.width, img.height), intrinsic_matrix,distortion_coefficient,
                            rcv, tcv, 0)
        self._calibMat = intrinsic_matrix
        self._distCoeff = distortion_coefficient
        return intrinsic_matrix

    def getCameraMatrix(self):
        """
        **SUMMARY**

        This function returns a cvMat of the camera's intrinsic matrix.
        If there is no matrix defined the function returns None.

        """
        return self._calibMat

    def undistort(self, image_or_2darray):
        """
        **SUMMARY**

        If given an image, apply the undistortion given by the camera's matrix and return the result.

        If given a 1xN 2D cvmat or a 2xN numpy array, it will un-distort points of
        measurement and return them in the original coordinate system.

        **PARAMETERS**

        * *image_or_2darray* - an image or an ndarray.

        **RETURNS**

        The undistorted image or the undistorted points. If the camera is un-calibrated
        we return None.

        **EXAMPLE**

        >>> img = cam.getImage()
        >>> result = cam.undistort(img)


        """
        if(type(self._calibMat) != cv.cvmat or type(self._distCoeff) != cv.cvmat ):
            logger.warning("FrameSource.undistort: This operation requires calibration, please load the calibration matrix")
            return None

        if (type(image_or_2darray) == InstanceType and image_or_2darray.__class__ == Image):
            inImg = image_or_2darray # we have an image
            retVal = inImg.getEmpty()
            cv.Undistort2(inImg.getBitmap(), retVal, self._calibMat, self._distCoeff)
            return Image(retVal)
        else:
            mat = ''
            if (type(image_or_2darray) == cv.cvmat):
                mat = image_or_2darray
            else:
                arr = cv.fromarray(np.array(image_or_2darray))
                mat = cv.CreateMat(cv.GetSize(arr)[1], 1, cv.CV_64FC2)
                cv.Merge(arr[:, 0], arr[:, 1], None, None, mat)

            upoints = cv.CreateMat(cv.GetSize(mat)[1], 1, cv.CV_64FC2)
            cv.UndistortPoints(mat, upoints, self._calibMat, self._distCoeff)

            #undistorted.x = (x* focalX + principalX);
            #undistorted.y = (y* focalY + principalY);
            return (np.array(upoints[:, 0]) *\
                [self.getCameraMatrix()[0, 0], self.getCameraMatrix()[1, 1]] +\
                [self.getCameraMatrix()[0, 2], self.getCameraMatrix()[1, 2]])[:, 0]

    def getImageUndistort(self):
        """
        **SUMMARY**

        Using the overridden getImage method we retrieve the image and apply the undistortion
        operation.


        **RETURNS**

        The latest image from the camera after applying undistortion.

        **EXAMPLE**

        >>> cam = Camera()
        >>> cam.loadCalibration("mycam.xml")
        >>> while True:
        >>>    img = cam.getImageUndistort()
        >>>    img.show()

        """
        return self.undistort(self.getImage())


    def saveCalibration(self, filename):
        """
        **SUMMARY**

        Save the calibration matrices to file. The file name should be without the extension.
        The default extension is .xml.

        **PARAMETERS**

        * *filename* - The file name, without an extension, to which to save the calibration data.

        **RETURNS**

        Returns true if the file was saved , false otherwise.

        **EXAMPLE**

        See :py:module:calibrate.py


        """
        if( type(self._calibMat) != cv.cvmat ):
            logger.warning("FrameSource.saveCalibration: No calibration matrix present, can't save.")
        else:
            intrFName = filename + "Intrinsic.xml"
            cv.Save(intrFName, self._calibMat)

        if( type(self._distCoeff) != cv.cvmat ):
            logger.warning("FrameSource.saveCalibration: No calibration distortion present, can't save.")
        else:
            distFName = filename + "Distortion.xml"
            cv.Save(distFName, self._distCoeff)

        return None

    def loadCalibration(self, filename):
        """
        **SUMMARY**

        Load a calibration matrix from file.
        The filename should be the stem of the calibration files names.
        e.g. If the calibration files are MyWebcamIntrinsic.xml and MyWebcamDistortion.xml
        then load the calibration file "MyWebcam"

        **PARAMETERS**

        * *filename* - The file name, without an extension, to which to save the calibration data.

        **RETURNS**

        Returns true if the file was loaded , false otherwise.

        **EXAMPLE**

        See :py:module:calibrate.py

        """
        retVal = False
        intrFName = filename + "Intrinsic.xml"
        self._calibMat = cv.Load(intrFName)
        distFName = filename + "Distortion.xml"
        self._distCoeff = cv.Load(distFName)
        if( type(self._distCoeff) == cv.cvmat
            and type(self._calibMat) == cv.cvmat):
            retVal = True

        return retVal

    def live(self):
        """
        **SUMMARY**

        This shows a live view of the camera.

        **EXAMPLE**

        To use it's as simple as:

        >>> cam = Camera()
        >>> cam.live()

        Left click will show mouse coordinates and color
        Right click will kill the live image
        """

        start_time = time.time()

        from SimpleCV.Display import Display
        i = self.getImage()
        d = Display(i.size())
        i.save(d)
        col = Color.RED

        while d.isNotDone():
            i = self.getImage()
            elapsed_time = time.time() - start_time


            if d.mouseLeft:
                txt = "coord: (" + str(d.mouseX) + "," + str(d.mouseY) + ")"
                i.dl().text(txt, (10,i.height / 2), color=col)
                txt = "color: " + str(i.getPixel(d.mouseX,d.mouseY))
                i.dl().text(txt, (10,(i.height / 2) + 10), color=col)
                print "coord: (" + str(d.mouseX) + "," + str(d.mouseY) + "), color: " + str(i.getPixel(d.mouseX,d.mouseY))


            if elapsed_time > 0 and elapsed_time < 5:

                i.dl().text("In live mode", (10,10), color=col)
                i.dl().text("Left click will show mouse coordinates and color", (10,20), color=col)
                i.dl().text("Right click will kill the live image", (10,30), color=col)


            i.save(d)
            if d.mouseRight:
                print "Closing Window"
                d.done = True


        pg.quit()

class Camera(FrameSource):
    """
    **SUMMARY**

    The Camera class is the class for managing input from a basic camera.  Note
    that once the camera is initialized, it will be locked from being used
    by other processes.  You can check manually if you have compatible devices
    on linux by looking for /dev/video* devices.

    This class wrappers OpenCV's cvCapture class and associated methods.
    Read up on OpenCV's CaptureFromCAM method for more details if you need finer
    control than just basic frame retrieval
    """
    capture = ""   #cvCapture object
    thread = ""
    pygame_camera = False
    pygame_buffer = ""


    prop_map = {"width": cv.CV_CAP_PROP_FRAME_WIDTH,
        "height": cv.CV_CAP_PROP_FRAME_HEIGHT,
        "brightness": cv.CV_CAP_PROP_BRIGHTNESS,
        "contrast": cv.CV_CAP_PROP_CONTRAST,
        "saturation": cv.CV_CAP_PROP_SATURATION,
        "hue": cv.CV_CAP_PROP_HUE,
        "gain": cv.CV_CAP_PROP_GAIN,
        "exposure": cv.CV_CAP_PROP_EXPOSURE}
    #human readable to CV constant property mapping

    def __init__(self, camera_index = -1, prop_set = {}, threaded = True, calibrationfile = ''):
        global _cameras
        global _camera_polling_thread
        global _index
        """
        **SUMMARY**

        In the camera constructor, camera_index indicates which camera to connect to
        and props is a dictionary which can be used to set any camera attributes
        Supported props are currently: height, width, brightness, contrast,
        saturation, hue, gain, and exposure.

        You can also specify whether you want the FrameBufferThread to continuously
        debuffer the camera.  If you specify True, the camera is essentially 'on' at
        all times.  If you specify off, you will have to manage camera buffers.

        **PARAMETERS**

        * *camera_index* - The index of the camera, these go from 0 upward, and are system specific.
        * *prop_set* - The property set for the camera (i.e. a dict of camera properties).

        .. Warning::
          For most web cameras only the width and height properties are supported. Support
          for all of the other parameters varies by camera and operating system.

        * *threaded* - If True we constantly debuffer the camera, otherwise the user
          must do this manually.

        * *calibrationfile* - A calibration file to load.


        """
        self.index = None
        self.threaded = False
        self.capture = None

        if platform.system() == "Linux" and -1 in _index and camera_index != -1 and camera_index not in _index:
            process = subprocess.Popen(["lsof /dev/video"+str(camera_index)],shell=True,stdout=subprocess.PIPE)
            data = process.communicate()
            if data[0]:
                camera_index = -1

        elif platform.system() == "Linux" and camera_index == -1 and -1 not in _index:
            process = subprocess.Popen(["lsof /dev/video*"],shell=True,stdout=subprocess.PIPE)
            data = process.communicate()
            if data[0]:
                camera_index = int(data[0].split("\n")[1].split()[-1][-1])

        for cam in _cameras:
            if camera_index == cam.index:
                self.threaded = cam.threaded
                self.capture = cam.capture
                self.index = cam.index
                _cameras.append(self)
                return

        #This is to add support for XIMEA cameras.
        if isinstance(camera_index, str):
            if camera_index.lower() == 'ximea':
                camera_index = 1100
                _index.append(camera_index)

        self.capture = cv.CaptureFromCAM(camera_index) #This fixes bug with opencv not being able to grab frames from webcams on linux
        self.index = camera_index
        if "delay" in prop_set:
            time.sleep(prop_set['delay'])

        if platform.system() == "Linux" and (prop_set.has_key("height") or cv.GrabFrame(self.capture) == False):
            import pygame.camera
            pygame.camera.init()
            threaded = True  #pygame must be threaded
            if camera_index == -1:
                camera_index = 0
                self.index = camera_index
                _index.append(camera_index)
                print _index
            if(prop_set.has_key("height") and prop_set.has_key("width")):
                self.capture = pygame.camera.Camera("/dev/video" + str(camera_index), (prop_set['width'], prop_set['height']))
            else:
                self.capture = pygame.camera.Camera("/dev/video" + str(camera_index))

            try:
                self.capture.start()
            except Exception as exc:
                msg = "caught exception: %r" % exc
                logger.warning(msg)
                logger.warning("SimpleCV can't seem to find a camera on your system, or the drivers do not work with SimpleCV.")
                return
            time.sleep(0)
            self.pygame_buffer = self.capture.get_image()
            self.pygame_camera = True
        else:
            _index.append(camera_index)
            self.threaded = False
            if (platform.system() == "Windows"):
                threaded = False

            if (not self.capture):
                return None

            #set any properties in the constructor
            for p in prop_set.keys():
                if p in self.prop_map:
                    cv.SetCaptureProperty(self.capture, self.prop_map[p], prop_set[p])

        if (threaded):
            self.threaded = True
            _cameras.append(self)
            if (not _camera_polling_thread):
                _camera_polling_thread = FrameBufferThread()
                _camera_polling_thread.daemon = True
                _camera_polling_thread.start()
                time.sleep(0) #yield to thread

        if calibrationfile:
            self.loadCalibration(calibrationfile)


    #todo -- make these dynamic attributes of the Camera class
    def getProperty(self, prop):
        """
        **SUMMARY**

        Retrieve the value of a given property, wrapper for cv.GetCaptureProperty

        .. Warning::
          For most web cameras only the width and height properties are supported. Support
          for all of the other parameters varies by camera and operating system.

        **PARAMETERS**

        * *prop* - The property to retrive.

        **RETURNS**

        The specified property. If it can't be found the method returns False.

        **EXAMPLE**

        >>> cam = Camera()
        >>> prop = cam.getProperty("width")
        """
        if self.pygame_camera:
            if prop.lower() == 'width':
                return self.capture.get_size()[0]
            elif prop.lower() == 'height':
                return self.capture.get_size()[1]
            else:
                return False

        if prop in self.prop_map:
            return cv.GetCaptureProperty(self.capture, self.prop_map[prop])
        return False

    def getAllProperties(self):
        """
        **SUMMARY**

        Return all properties from the camera.

        **RETURNS**

        A dict of all the camera properties.

        """
        if self.pygame_camera:
            return False
        props = {}
        for p in self.prop_map:
            props[p] = self.getProperty(p)

        return props

    def getImage(self):
        """
        **SUMMARY**

        Retrieve an Image-object from the camera.  If you experience problems
        with stale frames from the camera's hardware buffer, increase the flushcache
        number to dequeue multiple frames before retrieval

        We're working on how to solve this problem.

        **RETURNS**

        A SimpleCV Image from the camera.

        **EXAMPLES**

        >>> cam = Camera()
        >>> while True:
        >>>    cam.getImage().show()

        """

        if self.pygame_camera:
            return Image(self.pygame_buffer.copy())

        if (not self.threaded):
            cv.GrabFrame(self.capture)
            self.capturetime = time.time()
        else:
            self.capturetime = self._threadcapturetime

        frame = cv.RetrieveFrame(self.capture)
        newimg = cv.CreateImage(cv.GetSize(frame), cv.IPL_DEPTH_8U, 3)
        cv.Copy(frame, newimg)
        return Image(newimg, self)


class VirtualCamera(FrameSource):
    """
    **SUMMARY**

    The virtual camera lets you test algorithms or functions by providing
    a Camera object which is not a physically connected device.

    Currently, VirtualCamera supports "image", "imageset" and "video" source types.

    **USAGE**

    * For image, pass the filename or URL to the image
    * For the video, the filename
    * For imageset, you can pass either a path or a list of [path, extension]
    * For directory you treat a directory to show the latest file, an example would be where a security camera logs images to the directory, calling .getImage() will get the latest in the directory

    """
    source = ""
    sourcetype = ""
    lastmtime = 0

    def __init__(self, s, st, start=1):
        """
        **SUMMARY**

        The constructor takes a source, and source type.

        **PARAMETERS**

        * *s* - the source of the imagery.
        * *st* - the type of the virtual camera. Valid strings include:
        * *start* - the number of the frame that you want to start with.

          * "image" - a single still image.
          * "video" - a video file.
          * "imageset" - a SimpleCV image set.
          * "directory" - a VirtualCamera for loading a directory

        **EXAMPLE**

        >>> vc = VirtualCamera("img.jpg", "image")
        >>> vc = VirtualCamera("video.mpg", "video")
        >>> vc = VirtualCamera("./path_to_images/", "imageset")
        >>> vc = VirtualCamera("video.mpg", "video", 300)
        >>> vc = VirtualCamera("./imgs", "directory")


        """
        self.source = s
        self.sourcetype = st
        self.counter = 0
        if start==0:
            start=1
        self.start = start

        if self.sourcetype not in ["video", "image", "imageset", "directory"]:
            print 'Error: In VirtualCamera(), Incorrect Source option. "%s" \nUsage:' % self.sourcetype
            print '\tVirtualCamera("filename","video")'
            print '\tVirtualCamera("filename","image")'
            print '\tVirtualCamera("./path_to_images","imageset")'
            print '\tVirtualCamera("./path_to_images","directory")'
            return None

        else:
            if isinstance(self.source,str) and not os.path.exists(self.source):
                print 'Error: In VirtualCamera()\n\t"%s" was not found.' % self.source
                return None

        if (self.sourcetype == "imageset"):
            if( isinstance(s,ImageSet) ):
                self.source = s
            elif( isinstance(s,(list,str)) ):
                self.source = ImageSet()
                if (isinstance(s,list)):
                    self.source.load(*s)
                else:
                    self.source.load(s)
            else:
                warnings.warn('Virtual Camera is unable to figure out the contents of your ImageSet, it must be a directory, list of directories, or an ImageSet object')
            

        elif (self.sourcetype == 'video'):
         
            self.capture = cv.CaptureFromFile(self.source)
            cv.SetCaptureProperty(self.capture, cv.CV_CAP_PROP_POS_FRAMES, self.start-1)

        elif (self.sourcetype == 'directory'):
            pass


    def getImage(self):
        """
        **SUMMARY**

        Retrieve an Image-object from the virtual camera.
        **RETURNS**

        A SimpleCV Image from the camera.

        **EXAMPLES**

        >>> cam = VirtualCamera()
        >>> while True:
        >>>    cam.getImage().show()

        """
        if (self.sourcetype == 'image'):
            self.counter = self.counter + 1
            return Image(self.source, self)

        elif (self.sourcetype == 'imageset'):
            print len(self.source)
            img = self.source[self.counter % len(self.source)]
            self.counter = self.counter + 1
            return img

        elif (self.sourcetype == 'video'):
            # cv.QueryFrame returns None if the video is finished
            frame = cv.QueryFrame(self.capture)
            if frame:
                img = cv.CreateImage(cv.GetSize(frame), cv.IPL_DEPTH_8U, 3)
                cv.Copy(frame, img)
                return Image(img, self)
            else:
                return None

        elif (self.sourcetype == 'directory'):
            img = self.findLastestImage(self.source, 'bmp')
            self.counter = self.counter + 1
            return Image(img, self)

    def rewind(self, start=None):
        """
        **SUMMARY**

        Rewind the Video source back to the given frame.
        Available for only video sources.

        **PARAMETERS**

        start - the number of the frame that you want to rewind to.
                if not provided, the video source would be rewound
                to the starting frame number you provided or rewound
                to the beginning.

        **RETURNS**

        None

        **EXAMPLES**

        >>> cam = VirtualCamera("filename.avi", "video", 120)
        >>> i=0
        >>> while i<60:
            ... cam.getImage().show()
            ... i+=1
        >>> cam.rewind()

        """
        if (self.sourcetype == 'video'):
            if not start:
                cv.SetCaptureProperty(self.capture, cv.CV_CAP_PROP_POS_FRAMES, self.start-1)
            else:
                if start==0:
                    start=1
                cv.SetCaptureProperty(self.capture, cv.CV_CAP_PROP_POS_FRAMES, start-1)

        else:
            self.counter = 0

    def getFrame(self, frame):
        """
        **SUMMARY**

        Get the provided numbered frame from the video source.
        Available for only video sources.

        **PARAMETERS**

        frame -  the number of the frame

        **RETURNS**

        Image

        **EXAMPLES**

        >>> cam = VirtualCamera("filename.avi", "video", 120)
        >>> cam.getFrame(400).show()

        """
        if (self.sourcetype == 'video'):
            number_frame = int(cv.GetCaptureProperty(self.capture, cv.CV_CAP_PROP_POS_FRAMES))
            cv.SetCaptureProperty(self.capture, cv.CV_CAP_PROP_POS_FRAMES, frame-1)
            img = self.getImage()
            cv.SetCaptureProperty(self.capture, cv.CV_CAP_PROP_POS_FRAMES, number_frame)
            return img
        elif (self.sourcetype == 'imageset'):
            img = None
            if( frame < len(self.source)):
                img = self.source[frame]
            return img
        else:
            return None


    def skipFrames(self, n):
        """
        **SUMMARY**

        Skip n number of frames.
        Available for only video sources.

        **PARAMETERS**

        n - number of frames to be skipped.

        **RETURNS**

        None

        **EXAMPLES**

        >>> cam = VirtualCamera("filename.avi", "video", 120)
        >>> i=0
        >>> while i<60:
            ... cam.getImage().show()
            ... i+=1
        >>> cam.skipFrames(100)
        >>> cam.getImage().show()

        """
        if (self.sourcetype == 'video'):
            number_frame = int(cv.GetCaptureProperty(self.capture, cv.CV_CAP_PROP_POS_FRAMES))
            cv.SetCaptureProperty(self.capture, cv.CV_CAP_PROP_POS_FRAMES, number_frame + n - 1)
        elif (self.sourcetype == 'imageset'):
            self.counter = (self.counter + n) % len(self.source)
        else:
            self.counter = self.counter + n

    def getFrameNumber(self):
        """
        **SUMMARY**

        Get the current frame number of the video source.
        Available for only video sources.

        **RETURNS**

        * *int* - number of the frame

        **EXAMPLES**

        >>> cam = VirtualCamera("filename.avi", "video", 120)
        >>> i=0
        >>> while i<60:
            ... cam.getImage().show()
            ... i+=1
        >>> cam.skipFrames(100)
        >>> cam.getFrameNumber()

        """
        if (self.sourcetype == 'video'):
            number_frame = int(cv.GetCaptureProperty(self.capture, cv.CV_CAP_PROP_POS_FRAMES))
            return number_frame
        else:
            return self.counter

    def getCurrentPlayTime(self):
        """
        **SUMMARY**

        Get the current play time in milliseconds of the video source.
        Available for only video sources.

        **RETURNS**

        * *int* - milliseconds of time from beginning of file.

        **EXAMPLES**

        >>> cam = VirtualCamera("filename.avi", "video", 120)
        >>> i=0
        >>> while i<60:
            ... cam.getImage().show()
            ... i+=1
        >>> cam.skipFrames(100)
        >>> cam.getCurrentPlayTime()

        """
        if (self.sourcetype == 'video'):
            milliseconds = int(cv.GetCaptureProperty(self.capture, cv.CV_CAP_PROP_POS_MSEC))
            return milliseconds
        else:
            raise ValueError('sources other than video do not have play time property')

    def findLastestImage(self, directory='.', extension='png'):
        """
        **SUMMARY**

        This function finds the latest file in a directory
        with a given extension.

        **PARAMETERS**

        directory - The directory you want to load images from (defaults to current directory)
        extension - The image extension you want to use (defaults to .png)

        **RETURNS**

        The filename of the latest image

        **USAGE**

        >>> cam = VirtualCamera('imgs/', 'png') #find all .png files in 'img' directory
        >>> cam.getImage() # Grab the latest image from that directory

        """
        max_mtime = 0
        max_dir = None
        max_file = None
        max_full_path = None
        for dirname,subdirs,files in os.walk(directory):
            for fname in files:
                if fname.split('.')[-1] == extension:
                    full_path = os.path.join(dirname, fname)
                    mtime = os.stat(full_path).st_mtime
                    if mtime > max_mtime:
                        max_mtime = mtime
                        max_dir = dirname
                        max_file = fname
                        self.lastmtime = mtime
                        max_full_path = os.path.abspath(os.path.join(dirname, fname))

        #if file is being written, block until mtime is at least 100ms old
        while time.mktime(time.localtime()) - os.stat(max_full_path).st_mtime < 0.1:
            time.sleep(0)

        return max_full_path

class Kinect(FrameSource):
    """
    **SUMMARY**

    This is an experimental wrapper for the Freenect python libraries
    you can getImage() and getDepth() for separate channel images

    """
    def __init__(self, device_number=0):
        """
        **SUMMARY**

        In the kinect contructor, device_number indicates which kinect to
        connect to. It defaults to 0.

        **PARAMETERS**

        * *device_number* - The index of the kinect, these go from 0 upward.
        """
        self.deviceNumber = device_number
        if not FREENECT_ENABLED:
            logger.warning("You don't seem to have the freenect library installed.  This will make it hard to use a Kinect.")

    #this code was borrowed from
    #https://github.com/amiller/libfreenect-goodies
    def getImage(self):
        """
        **SUMMARY**

        This method returns the Kinect camera image.

        **RETURNS**

        The Kinect's color camera image.

        **EXAMPLE**

        >>> k = Kinect()
        >>> while True:
        >>>   k.getImage().show()

        """
        video = freenect.sync_get_video(self.deviceNumber)[0]
        self.capturetime = time.time()
        #video = video[:, :, ::-1]  # RGB -> BGR
        return Image(video.transpose([1,0,2]), self)

    #low bits in this depth are stripped so it fits in an 8-bit image channel
    def getDepth(self):
        """
        **SUMMARY**

        This method returns the Kinect depth image.

        **RETURNS**

        The Kinect's depth camera image as a grayscale image.

        **EXAMPLE**

        >>> k = Kinect()
        >>> while True:
        >>>   d = k.getDepth()
        >>>   img = k.getImage()
        >>>   result = img.sideBySide(d)
        >>>   result.show()
        """

        depth = freenect.sync_get_depth(self.deviceNumber)[0]
        self.capturetime = time.time()
        np.clip(depth, 0, 2**10 - 1, depth)
        depth >>= 2
        depth = depth.astype(np.uint8).transpose()

        return Image(depth, self)

    #we're going to also support a higher-resolution (11-bit) depth matrix
    #if you want to actually do computations with the depth
    def getDepthMatrix(self):
        self.capturetime = time.time()
        return freenect.sync_get_depth(self.deviceNumber)[0]



class JpegStreamReader(threading.Thread):
    """
    **SUMMARY**

    A Threaded class for pulling down JPEG streams and breaking up the images. This
    is handy for reading the stream of images from a IP CAmera.

    """
    url = ""
    currentframe = ""
    _threadcapturetime = ""

    def run(self):

        f = ''

        if re.search('@', self.url):
            authstuff = re.findall('//(\S+)@', self.url)[0]
            self.url = re.sub("//\S+@", "//", self.url)
            user, password = authstuff.split(":")

            #thank you missing urllib2 manual
            #http://www.voidspace.org.uk/python/articles/urllib2.shtml#id5
            password_mgr = urllib2.HTTPPasswordMgrWithDefaultRealm()
            password_mgr.add_password(None, self.url, user, password)

            handler = urllib2.HTTPBasicAuthHandler(password_mgr)
            opener = urllib2.build_opener(handler)

            f = opener.open(self.url)
        else:
            f = urllib2.urlopen(self.url)

        headers = f.info()
        if (headers.has_key("content-type")):
            headers['Content-type'] = headers['content-type'] #force ucase first char

        if not headers.has_key("Content-type"):
            logger.warning("Tried to load a JpegStream from " + self.url + ", but didn't find a content-type header!")
            return

        (multipart, boundary) = headers['Content-type'].split("boundary=")
        if not re.search("multipart", multipart, re.I):
            logger.warning("Tried to load a JpegStream from " + self.url + ", but the content type header was " + multipart + " not multipart/replace!")
            return

        buff = ''
        data = f.readline().strip()
        length = 0
        contenttype = "jpeg"

        #the first frame contains a boundarystring and some header info
        while (1):
            #print data
            if (re.search(boundary, data.strip()) and len(buff)):
                #we have a full jpeg in buffer.  Convert to an image
                if contenttype == "jpeg":
                    self.currentframe = buff
                    self._threadcapturetime = time.time()
                buff = ''

            if (re.match("Content-Type", data, re.I)):
                #set the content type, if provided (default to jpeg)
                (header, typestring) = data.split(":")
                (junk, contenttype) = typestring.strip().split("/")

            if (re.match("Content-Length", data, re.I)):
                #once we have the content length, we know how far to go jfif
                (header, length) = data.split(":")
                length = int(length.strip())

            if (re.search("JFIF", data, re.I) or re.search("\xff\xd8\xff\xdb", data) or len(data) > 55):
                # we have reached the start of the image
                buff = ''
                if length and length > len(data):
                    buff += data + f.read(length - len(data)) #read the remainder of the image
                    if contenttype == "jpeg":
                        self.currentframe = buff
                        self._threadcapturetime = time.time()
                else:
                    while (not re.search(boundary, data)):
                        buff += data
                        data = f.readline()

                    endimg, junk = data.split(boundary)
                    buff += endimg
                    data = boundary
                    continue

            data = f.readline() #load the next (header) line
            time.sleep(0) #let the other threads go

class JpegStreamCamera(FrameSource):
    """
    **SUMMARY**

    The JpegStreamCamera takes a URL of a JPEG stream and treats it like a camera.  The current frame can always be accessed with getImage()

    Requires the Python Imaging Library: http://www.pythonware.com/library/pil/handbook/index.htm

    **EXAMPLE**

    Using your Android Phone as a Camera. Softwares like IP Webcam can be used.

    >>> cam = JpegStreamCamera("http://192.168.65.101:8080/videofeed") # your IP may be different.
    >>> img = cam.getImage()
    >>> img.show()

    """
    url = ""
    camthread = ""

    def __init__(self, url):
        if not PIL_ENABLED:
            logger.warning("You need the Python Image Library (PIL) to use the JpegStreamCamera")
            return
        if not url.startswith('http://'):
            url = "http://" + url
        self.url = url
        self.camthread = JpegStreamReader()
        self.camthread.url = self.url
        self.camthread.daemon = True
        self.camthread.start()

    def getImage(self):
        """
        **SUMMARY**

        Return the current frame of the JpegStream being monitored

        """
        if not self.camthread._threadcapturetime:
            now = time.time()
            while not self.camthread._threadcapturetime:
                if time.time() - now > 5:
                    warnings.warn("Timeout fetching JpegStream at " + self.url)
                    return
                time.sleep(0.1)

        self.capturetime = self.camthread._threadcapturetime
        return Image(pil.open(StringIO(self.camthread.currentframe)), self)


_SANE_INIT = False

class Scanner(FrameSource):
    """
    **SUMMARY**

    The Scanner lets you use any supported SANE-compatable scanner as a SimpleCV camera
    List of supported devices: http://www.sane-project.org/sane-supported-devices.html

    Requires the PySANE wrapper for libsane.  The sane scanner object
    is available for direct manipulation at Scanner.device

    This scanner object is heavily modified from
    https://bitbucket.org/DavidVilla/pysane

    Constructor takes an index (default 0) and a list of SANE options
    (default is color mode).

    **EXAMPLE**

    >>> scan = Scanner(0, { "mode": "gray" })
    >>> preview = scan.getPreview()
    >>> stuff = preview.findBlobs(minsize = 1000)
    >>> topleft = (np.min(stuff.x()), np.min(stuff.y()))
    >>> bottomright = (np.max(stuff.x()), np.max(stuff.y()))
    >>> scan.setROI(topleft, bottomright)
    >>> scan.setProperty("resolution", 1200) #set high resolution
    >>> scan.setProperty("mode", "color")
    >>> img = scan.getImage()
    >>> scan.setROI() #reset region of interest
    >>> img.show()


    """
    usbid = None
    manufacturer = None
    model = None
    kind = None
    device = None
    max_x = None
    max_y = None

    def __init__(self, id = 0, properties = { "mode": "color"}):
        global _SANE_INIT
        import sane
        if not _SANE_INIT:
            try:
                sane.init()
                _SANE_INIT = True
            except:
                warn("Initializing pysane failed, do you have pysane installed?")
                return

        devices = sane.get_devices()
        if not len(devices):
            warn("Did not find a sane-compatable device")
            return

        self.usbid, self.manufacturer, self.model, self.kind = devices[id]

        self.device = sane.open(self.usbid)
        self.max_x = self.device.br_x
        self.max_y = self.device.br_y #save our extents for later

        for k, v in properties.items():
            setattr(self.device, k, v)

    def getImage(self):
        """
        **SUMMARY**

        Retrieve an Image-object from the scanner.  Any ROI set with
        setROI() is taken into account.
        **RETURNS**

        A SimpleCV Image.  Note that whatever the scanner mode is,
        SimpleCV will return a 3-channel, 8-bit image.

        **EXAMPLES**
        >>> scan = Scanner()
        >>> scan.getImage().show()
        """
        return Image(self.device.scan())

    def getPreview(self):
        """
        **SUMMARY**

        Retrieve a preview-quality Image-object from the scanner.
        **RETURNS**

        A SimpleCV Image.  Note that whatever the scanner mode is,
        SimpleCV will return a 3-channel, 8-bit image.

        **EXAMPLES**
        >>> scan = Scanner()
        >>> scan.getPreview().show()
        """
        self.preview = True
        img = Image(self.device.scan())
        self.preview = False
        return img

    def getAllProperties(self):
        """
        **SUMMARY**

        Return a list of all properties and values from the scanner
        **RETURNS**

        Dictionary of active options and values.  Inactive options appear
        as "None"

        **EXAMPLES**
        >>> scan = Scanner()
        >>> print scan.getAllProperties()
        """
        props = {}
        for prop in self.device.optlist:
            val = None
            if hasattr(self.device, prop):
                val = getattr(self.device, prop)
            props[prop] = val

        return props

    def printProperties(self):

        """
        **SUMMARY**

        Print detailed information about the SANE device properties
        **RETURNS**

        Nothing

        **EXAMPLES**
        >>> scan = Scanner()
        >>> scan.printProperties()
        """
        for prop in self.device.optlist:
            try:
                print self.device[prop]
            except:
                pass

    def getProperty(self, prop):
        """
        **SUMMARY**
        Returns a single property value from the SANE device
        equivalent to Scanner.device.PROPERTY

        **RETURNS**
        Value for option or None if missing/inactive

        **EXAMPLES**
        >>> scan = Scanner()
        >>> print scan.getProperty('mode')
        color
        """
        if hasattr(self.device, prop):
            return getattr(self.device, prop)
        return None


    def setROI(self, topleft = (0,0), bottomright = (-1,-1)):
        """
        **SUMMARY**
        Sets an ROI for the scanner in the current resolution.  The
        two parameters, topleft and bottomright, will default to the
        device extents, so the ROI can be reset by calling setROI with
        no parameters.

        The ROI is set by SANE in resolution independent units (default
        MM) so resolution can be changed after ROI has been set.

        **RETURNS**
        None

        **EXAMPLES**
        >>> scan = Scanner()
        >>> scan.setROI((50, 50), (100,100))
        >>> scan.getImage().show() # a very small crop on the scanner


        """
        self.device.tl_x = self.px2mm(topleft[0])
        self.device.tl_y = self.px2mm(topleft[1])
        if bottomright[0] == -1:
            self.device.br_x = self.max_x
        else:
            self.device.br_x = self.px2mm(bottomright[0])

        if bottomright[1] == -1:
            self.device.br_y = self.max_y
        else:
            self.device.br_y = self.px2mm(bottomright[1])

    def setProperty(self, prop, val):
        """
        **SUMMARY**
        Assigns a property value from the SANE device
        equivalent to Scanner.device.PROPERTY = VALUE

        **RETURNS**
        None

        **EXAMPLES**
        >>> scan = Scanner()
        >>> print scan.getProperty('mode')
        color
        >>> scan.setProperty("mode") = "gray"
        """
        setattr(self.device, prop, val)



    def px2mm(self, pixels = 1):
        """
        **SUMMARY**
        Helper function to convert native scanner resolution to millimeter units

        **RETURNS**
        Float value

        **EXAMPLES**
        >>> scan = Scanner()
        >>> scan.px2mm(scan.device.resolution) #return DPI in DPMM
        """
        return float(pixels * 25.4 / float(self.device.resolution))

class DigitalCamera(FrameSource):
    """
    **SUMMARY**

    The DigitalCamera takes a point-and-shoot camera or high-end slr and uses it as a Camera.  The current frame can always be accessed with getPreview()

    Requires the PiggyPhoto Library: https://github.com/alexdu/piggyphoto

    **EXAMPLE**

    >>> cam = DigitalCamera()
    >>> pre = cam.getPreview()
    >>> pre.findBlobs().show()
    >>>
    >>> img = cam.getImage()
    >>> img.show()

    """
    camera = None
    usbid = None
    device = None

    def __init__(self, id = 0):
        try:
            import piggyphoto
        except:
            warn("Initializing piggyphoto failed, do you have piggyphoto installed?")
            return

        devices = piggyphoto.cameraList(autodetect=True).toList()
        if not len(devices):
            warn("No compatible digital cameras attached")
            return

        self.device, self.usbid = devices[id]
        self.camera = piggyphoto.camera()

    def getImage(self):
        """
        **SUMMARY**

        Retrieve an Image-object from the camera with the highest quality possible.
        **RETURNS**

        A SimpleCV Image.

        **EXAMPLES**
        >>> cam = DigitalCamera()
        >>> cam.getImage().show()
        """
        fd, path = tempfile.mkstemp()
        self.camera.capture_image(path)
        img = Image(path)
        os.close(fd)
        os.remove(path)
        return img

    def getPreview(self):
        """
        **SUMMARY**

        Retrieve an Image-object from the camera with the preview quality from the camera.
        **RETURNS**

        A SimpleCV Image.

        **EXAMPLES**
        >>> cam = DigitalCamera()
        >>> cam.getPreview().show()
        """
        fd, path = tempfile.mkstemp()
        self.camera.capture_preview(path)
        img = Image(path)
        os.close(fd)
        os.remove(path)

        return img

class ScreenCamera():
    """
    **SUMMARY**
    ScreenCapture is a camera class would allow you to capture all or part of the screen and return it as a color image.

    Requires the pyscreenshot Library: https://github.com/vijaym123/pyscreenshot

    **EXAMPLE**
    >>> sc = ScreenCamera()
    >>> res = sc.getResolution()
    >>> print res
    >>>
    >>> img = sc.getImage()
    >>> img.show()
    """
    _roi = None

    def __init__(self):
        if not PYSCREENSHOT_ENABLED:
            warn("Initializing pyscreenshot failed. Install pyscreenshot from https://github.com/vijaym123/pyscreenshot")
            return None

    def getResolution(self):
        """
        **DESCRIPTION**

        returns the resolution of the screenshot of the screen.

        **PARAMETERS**
        None

        **RETURNS**
        returns the resolution.

        **EXAMPLE**

        >>> img = ScreenCamera()
        >>> res = img.getResolution()
        >>> print res
        """
        return Image(pyscreenshot.grab()).size()

    def setROI(self,roi):
        """
        **DESCRIPTION**
        To set the region of interest.

        **PARAMETERS**
        * *roi* - tuple - It is a tuple of size 4. where region of interest is to the center of the screen.

        **RETURNS**
        None

        **EXAMPLE**
        >>> sc = ScreenCamera()
        >>> res = sc.getResolution()
        >>> sc.setROI(res[0]/4,res[1]/4,res[0]/2,res[1]/2)
        >>> img = sc.getImage()
        >>> s.show()
        """
        if isinstance(roi,tuple) and len(roi)==4:
            self._roi = roi
        return

    def getImage(self):
        """
        **DESCRIPTION**

        getImage function returns a Image object capturing the current screenshot of the screen.

        **PARAMETERS**
        None

        **RETURNS**
        Returns the region of interest if setROI is used.
        else returns the original capture of the screenshot.

        **EXAMPLE**
        >>> sc = ScreenCamera()
        >>> img = sc.getImage()
        >>> img.show()
        """
        img = Image(pyscreenshot.grab())
        try :
            if self._roi :
                img = img.crop(self._roi,centered=True)
        except :
            print "Error croping the image. ROI specified is not correct."
            return None
        return img


class StereoImage:
    """
    **SUMMARY**

    This class is for binaculor Stereopsis. That is exactrating 3D information from two differing views of a scene(Image). By comparing the two images, the relative depth information can be obtained.

    - Fundamental Matrix : F : a 3 x 3 numpy matrix, is a relationship between any two images of the same scene that constrains where the projection of points from the scene can occur in both images. see : http://en.wikipedia.org/wiki/Fundamental_matrix_(computer_vision)

    - Homography Matrix : H : a 3 x 3 numpy matrix,

    - ptsLeft : The matched points on the left image.

    - ptsRight : The matched points on the right image.

    -findDisparityMap and findDepthMap - provides 3D information.

    for more information on stereo vision, visit : http://en.wikipedia.org/wiki/Computer_stereo_vision

    **EXAMPLE**
    >>> img1 = Image('sampleimages/stereo_view1.png')
    >>> img2 = Image('sampleimages/stereo_view2.png')
    >>> stereoImg = StereoImage(img1,img2)
    >>> stereoImg.findDisparityMap(method="BM",nDisparity=20).show()
    """
    def __init__( self, imgLeft , imgRight ):
        self.ImageLeft = imgLeft
        self.ImageRight = imgRight
        if self.ImageLeft.size() != self.ImageRight.size():
            logger.warning('Left and Right images should have the same size.')
            return None
        else:
            self.size = self.ImageLeft.size()

    def findFundamentalMat(self, thresh=500.00, minDist=0.15 ):
        """
        **SUMMARY**

        This method returns the fundamental matrix F such that (P_2).T F P_1 = 0

        **PARAMETERS**

        * *thresh* - The feature quality metric. This can be any value between about 300 and 500. Higher
          values should return fewer, but higher quality features.
        * *minDist* - The value below which the feature correspondence is considered a match. This
          is the distance between two feature vectors. Good values are between 0.05 and 0.3

        **RETURNS**
        Return None if it fails.
        * *F* -  Fundamental matrix as ndarray.
        * *matched_pts1* - the matched points (x, y) in img1
        * *matched_pts2* - the matched points (x, y) in img2

        **EXAMPLE**
        >>> img1 = Image("sampleimages/stereo_view1.png")
        >>> img2 = Image("sampleimages/stereo_view2.png")
        >>> stereoImg = StereoImage(img1,img2)
        >>> F,pts1,pts2 = stereoImg.findFundamentalMat()

        **NOTE**
        If you deal with the fundamental matrix F directly, be aware of (P_2).T F P_1 = 0
        where P_2 and P_1 consist of (y, x, 1)
        """

        (kpts1, desc1) = self.ImageLeft._getRawKeypoints(thresh)
        (kpts2, desc2) = self.ImageRight._getRawKeypoints(thresh)

        if desc1 == None or desc2 == None:
            logger.warning("We didn't get any descriptors. Image might be too uniform or blurry.")
            return None

        num_pts1 = desc1.shape[0]
        num_pts2 = desc2.shape[0]

        magic_ratio = 1.00
        if num_pts1 > num_pts2:
            magic_ratio = float(num_pts1) / float(num_pts2)

        (idx, dist) = Image()._getFLANNMatches(desc1, desc2)
        p = dist.squeeze()
        result = p * magic_ratio < minDist

        try:
            import cv2
        except:
            logger.warning("Can't use fundamental matrix without OpenCV >= 2.3.0")
            return None

        pts1 = np.array([kpt.pt for kpt in kpts1])
        pts2 = np.array([kpt.pt for kpt in kpts2])

        matched_pts1 = pts1[idx[result]].squeeze()
        matched_pts2 = pts2[result]
        (F, mask) = cv2.findFundamentalMat(matched_pts1, matched_pts2, method=cv.CV_FM_LMEDS)

        inlier_ind = mask.nonzero()[0]
        matched_pts1 = matched_pts1[inlier_ind, :]
        matched_pts2 = matched_pts2[inlier_ind, :]

        matched_pts1 = matched_pts1[:, ::-1.00]
        matched_pts2 = matched_pts2[:, ::-1.00]
        return (F, matched_pts1, matched_pts2)

    def findHomography( self, thresh=500.00, minDist=0.15):
        """
        **SUMMARY**

        This method returns the homography H such that P2 ~ H P1

        **PARAMETERS**

        * *thresh* - The feature quality metric. This can be any value between about 300 and 500. Higher
          values should return fewer, but higher quality features.
        * *minDist* - The value below which the feature correspondence is considered a match. This
          is the distance between two feature vectors. Good values are between 0.05 and 0.3

        **RETURNS**

        Return None if it fails.
        * *H* -  homography as ndarray.
        * *matched_pts1* - the matched points (x, y) in img1
        * *matched_pts2* - the matched points (x, y) in img2

        **EXAMPLE**
        >>> img1 = Image("sampleimages/stereo_view1.png")
        >>> img2 = Image("sampleimages/stereo_view2.png")
        >>> stereoImg = StereoImage(img1,img2)
        >>> H,pts1,pts2 = stereoImg.findHomography()

        **NOTE**
        If you deal with the homography H directly, be aware of P2 ~ H P1
        where P2 and P1 consist of (y, x, 1)
        """

        (kpts1, desc1) = self.ImageLeft._getRawKeypoints(thresh)
        (kpts2, desc2) = self.ImageRight._getRawKeypoints(thresh)

        if desc1 == None or desc2 == None:
            logger.warning("We didn't get any descriptors. Image might be too uniform or blurry.")
            return None

        num_pts1 = desc1.shape[0]
        num_pts2 = desc2.shape[0]

        magic_ratio = 1.00
        if num_pts1 > num_pts2:
            magic_ratio = float(num_pts1) / float(num_pts2)

        (idx, dist) = Image()._getFLANNMatches(desc1, desc2)
        p = dist.squeeze()
        result = p * magic_ratio < minDist

        try:
            import cv2
        except:
            logger.warning("Can't use homography without OpenCV >= 2.3.0")
            return None

        pts1 = np.array([kpt.pt for kpt in kpts1])
        pts2 = np.array([kpt.pt for kpt in kpts2])

        matched_pts1 = pts1[idx[result]].squeeze()
        matched_pts2 = pts2[result]

        (H, mask) = cv2.findHomography(matched_pts1, matched_pts2,
                method=cv.CV_LMEDS)

        inlier_ind = mask.nonzero()[0]
        matched_pts1 = matched_pts1[inlier_ind, :]
        matched_pts2 = matched_pts2[inlier_ind, :]

        matched_pts1 = matched_pts1[:, ::-1.00]
        matched_pts2 = matched_pts2[:, ::-1.00]
        return (H, matched_pts1, matched_pts2)

    def findDisparityMap( self, nDisparity=16 ,method='BM'):
        """
        The method generates disparity map from set of stereo images.

        **PARAMETERS**

        * *method* :
                 *BM* - Block Matching algorithm, this is a real time algorithm.
                 *SGBM* - Semi Global Block Matching algorithm, this is not a real time algorithm.
                 *GC* - Graph Cut algorithm, This is not a real time algorithm.

        * *nDisparity* - Maximum disparity value. This should be multiple of 16
        * *scale* - Scale factor

        **RETURNS**

        Return None if it fails.
        Returns Disparity Map Image

        **EXAMPLE**
        >>> img1 = Image("sampleimages/stereo_view1.png")
        >>> img2 = Image("sampleimages/stereo_view2.png")
        >>> stereoImg = StereoImage(img1,img2)
        >>> disp = stereoImg.findDisparityMap(method="BM")
        """
        gray_left = self.ImageLeft.getGrayscaleMatrix()
        gray_right = self.ImageRight.getGrayscaleMatrix()
        (r, c) = self.size
        scale = int(self.ImageLeft.depth)
        if nDisparity % 16 !=0 :
            if nDisparity < 16 :
                nDisparity = 16
            nDisparity = (nDisparity/16)*16
        try :
            if method == 'BM':
                disparity = cv.CreateMat(c, r, cv.CV_32F)
                state = cv.CreateStereoBMState()
                state.SADWindowSize = 41
                state.preFilterType = 1
                state.preFilterSize = 41
                state.preFilterCap = 31
                state.minDisparity = -8
                state.numberOfDisparities = nDisparity
                state.textureThreshold = 10
                #state.speckleRange = 32
                #state.speckleWindowSize = 100
                state.uniquenessRatio=15
                cv.FindStereoCorrespondenceBM(gray_left, gray_right, disparity, state)
                disparity_visual = cv.CreateMat(c, r, cv.CV_8U)
                cv.Normalize( disparity, disparity_visual, 0, 256, cv.CV_MINMAX )
                disparity_visual = Image(disparity_visual)
                return Image(disparity_visual.getBitmap(),colorSpace=ColorSpace.GRAY)

            elif method == 'GC':
                disparity_left = cv.CreateMat(c, r, cv.CV_32F)
                disparity_right = cv.CreateMat(c, r, cv.CV_32F)
                state = cv.CreateStereoGCState(nDisparity, 8)
                state.minDisparity = -8
                cv.FindStereoCorrespondenceGC( gray_left, gray_right, disparity_left, disparity_right, state, 0)
                disparity_left_visual = cv.CreateMat(c, r, cv.CV_8U)
                cv.Normalize( disparity_left, disparity_left_visual, 0, 256, cv.CV_MINMAX )
                #cv.Scale(disparity_left, disparity_left_visual, -scale)
                disparity_left_visual = Image(disparity_left_visual)
                return Image(disparity_left_visual.getBitmap(),colorSpace=ColorSpace.GRAY)

            elif method == 'SGBM':
                try:
                    import cv2
                    ver = cv2.__version__
                    if ver.startswith("$Rev :"):
                        logger.warning("Can't use SGBM without OpenCV >= 2.4.0")
                        return None
                except:
                    logger.warning("Can't use SGBM without OpenCV >= 2.4.0")
                    return None
                state = cv2.StereoSGBM()
                state.SADWindowSize = 41
                state.preFilterCap = 31
                state.minDisparity = 0
                state.numberOfDisparities = nDisparity
                #state.speckleRange = 32
                #state.speckleWindowSize = 100
                state.disp12MaxDiff = 1
                state.fullDP=False
                state.P1 = 8 * 1 * 41 * 41
                state.P2 = 32 * 1 * 41 * 41
                state.uniquenessRatio=15
                disparity=state.compute(self.ImageLeft.getGrayNumpy(),self.ImageRight.getGrayNumpy())
                return Image(disparity)

            else :
                logger.warning("Unknown method. Choose one method amoung BM or SGBM or GC !")
                return None

        except :
            logger.warning("Error in computing the Disparity Map, may be due to the Images are stereo in nature.")
            return None

    def Eline (self, point, F, whichImage):
        """
        **SUMMARY**

        This method returns, line feature object.

        **PARAMETERS**

        * *point* - Input point (x, y)
        * *F* - Fundamental matrix.
        * *whichImage* - Index of the image (1 or 2) that contains the point

        **RETURNS**

        epipolar line, in the form of line feature object.

        **EXAMPLE**

        >>> img1 = Image("sampleimages/stereo_view1.png")
        >>> img2 = Image("sampleimages/stereo_view2.png")
        >>> stereoImg = StereoImage(img1,img2)
        >>> F,pts1,pts2 = stereoImg.findFundamentalMat()
        >>> point = pts2[0]
        >>> epiline = mapper.Eline(point,F, 1) #find corresponding Epipolar line in the left image.
        """

        from SimpleCV.Features.Detection import Line

        pts1 = (0,0)
        pts2 = self.size
        pt_cvmat = cv.CreateMat(1, 1, cv.CV_32FC2)
        pt_cvmat[0, 0] = (point[1], point[0])  # OpenCV seems to use (y, x) coordinate.
        line = cv.CreateMat(1, 1, cv.CV_32FC3)
        cv.ComputeCorrespondEpilines(pt_cvmat, whichImage, npArray2cvMat(F), line)
        line_npArray = np.array(line).squeeze()
        line_npArray = line_npArray[[1.00, 0, 2]]
        pts1 = (pts1[0],(-line_npArray[2]-line_npArray[0]*pts1[0])/line_npArray[1] )
        pts2 = (pts2[0],(-line_npArray[2]-line_npArray[0]*pts2[0])/line_npArray[1] )
        if whichImage == 1 :
            return Line(self.ImageLeft, [pts1,pts2])
        elif whichImage == 2 :
            return Line(self.ImageRight, [pts1,pts2])

    def projectPoint( self, point, H ,whichImage):
        """
        **SUMMARY**

        This method returns the corresponding point (x, y)

        **PARAMETERS**

        * *point* - Input point (x, y)
        * *whichImage* - Index of the image (1 or 2) that contains the point
        * *H* - Homography that can be estimated
                using StereoCamera.findHomography()

        **RETURNS**

        Corresponding point (x, y) as tuple

        **EXAMPLE**

        >>> img1 = Image("sampleimages/stereo_view1.png")
        >>> img2 = Image("sampleimages/stereo_view2.png")
        >>> stereoImg = StereoImage(img1,img2)
        >>> F,pts1,pts2 = stereoImg.findFundamentalMat()
        >>> point = pts2[0]
        >>> projectPoint = stereoImg.projectPoint(point,H ,1) #finds corresponding  point in the left image.
        """

        H = np.matrix(H)
        point = np.matrix((point[1], point[0],1.00))
        if whichImage == 1.00:
            corres_pt = H * point.T
        else:
            corres_pt = np.linalg.inv(H) * point.T
        corres_pt = corres_pt / corres_pt[2]
        return (float(corres_pt[1]), float(corres_pt[0]))

    def get3DImage(self, Q, method="BM", state=None):
        """
        **SUMMARY**

        This method returns the 3D depth image using reprojectImageTo3D method.

        **PARAMETERS**

        * *Q* - reprojection Matrix (disparity to depth matrix)
        * *method* - Stereo Correspondonce method to be used.
                   - "BM" - Stereo BM
                   - "SGBM" - Stereo SGBM
        * *state* - dictionary corresponding to parameters of
                    stereo correspondonce.
                    SADWindowSize - odd int
                    nDisparity - int
                    minDisparity  - int
                    preFilterCap - int
                    preFilterType - int (only BM)
                    speckleRange - int
                    speckleWindowSize - int
                    P1 - int (only SGBM)
                    P2 - int (only SGBM)
                    fullDP - Bool (only SGBM)
                    uniquenessRatio - int
                    textureThreshold - int (only BM)

        **RETURNS**

        SimpleCV.Image representing 3D depth Image
        also StereoImage.Image3D gives OpenCV 3D Depth Image of CV_32F type.

        **EXAMPLE**

        >>> lImage = Image("l.jpg")
        >>> rImage = Image("r.jpg")
        >>> stereo = StereoImage(lImage, rImage)
        >>> Q = cv.Load("Q.yml")
        >>> stereo.get3DImage(Q).show()

        >>> state = {"SADWindowSize":9, "nDisparity":112, "minDisparity":-39}
        >>> stereo.get3DImage(Q, "BM", state).show()
        >>> stereo.get3DImage(Q, "SGBM", state).show()
        """
        imgLeft = self.ImageLeft
        imgRight = self.ImageRight
        cv2flag = True
        try:
            import cv2
        except ImportError:
            cv2flag = False
        import cv2.cv as cv
        (r, c) = self.size
        if method == "BM":
            sbm = cv.CreateStereoBMState()
            disparity = cv.CreateMat(c, r, cv.CV_32F)
            if state:
                SADWindowSize = state.get("SADWindowSize")
                preFilterCap = state.get("preFilterCap")
                minDisparity = state.get("minDisparity")
                numberOfDisparities = state.get("nDisparity")
                uniquenessRatio = state.get("uniquenessRatio")
                speckleRange = state.get("speckleRange")
                speckleWindowSize = state.get("speckleWindowSize")
                textureThreshold = state.get("textureThreshold")
                speckleRange = state.get("speckleRange")
                speckleWindowSize = state.get("speckleWindowSize")
                preFilterType = state.get("perFilterType")

                if SADWindowSize is not None:
                    sbm.SADWindowSize = SADWindowSize
                if preFilterCap is not None:
                    sbm.preFilterCap = preFilterCap
                if minDisparity is not None:
                    sbm.minDisparity = minDisparity
                if numberOfDisparities is not None:
                    sbm.numberOfDisparities = numberOfDisparities
                if uniquenessRatio is not None:
                    sbm.uniquenessRatio = uniquenessRatio
                if speckleRange is not None:
                    sbm.speckleRange = speckleRange
                if speckleWindowSize is not None:
                    sbm.speckleWindowSize = speckleWindowSize
                if textureThreshold is not None:
                    sbm.textureThreshold = textureThreshold
                if preFilterType is not None:
                    sbm.preFilterType = preFilterType
            else:
                sbm.SADWindowSize = 9
                sbm.preFilterType = 1
                sbm.preFilterSize = 5
                sbm.preFilterCap = 61
                sbm.minDisparity = -39
                sbm.numberOfDisparities = 112
                sbm.textureThreshold = 507
                sbm.uniquenessRatio= 0
                sbm.speckleRange = 8
                sbm.speckleWindowSize = 0

            gray_left = imgLeft.getGrayscaleMatrix()
            gray_right = imgRight.getGrayscaleMatrix()
            cv.FindStereoCorrespondenceBM(gray_left, gray_right, disparity, sbm)
            disparity_visual = cv.CreateMat(c, r, cv.CV_8U)

        elif method == "SGBM":
            if not cv2flag:
                warnings.warn("Can't Use SGBM without OpenCV >= 2.4. Use SBM instead.")
            sbm = cv2.StereoSGBM()
            if state:
                SADWindowSize = state.get("SADWindowSize")
                preFilterCap = state.get("preFilterCap")
                minDisparity = state.get("minDisparity")
                numberOfDisparities = state.get("nDisparity")
                P1 = state.get("P1")
                P2 = state.get("P2")
                uniquenessRatio = state.get("uniquenessRatio")
                speckleRange = state.get("speckleRange")
                speckleWindowSize = state.get("speckleWindowSize")
                fullDP = state.get("fullDP")

                if SADWindowSize is not None:
                    sbm.SADWindowSize = SADWindowSize
                if preFilterCap is not None:
                    sbm.preFilterCap = preFilterCap
                if minDisparity is not None:
                    sbm.minDisparity = minDisparity
                if numberOfDisparities is not None:
                    sbm.numberOfDisparities = numberOfDisparities
                if P1 is not None:
                    sbm.P1 = P1
                if P2 is not None:
                    sbm.P2 = P2
                if uniquenessRatio is not None:
                    sbm.uniquenessRatio = uniquenessRatio
                if speckleRange is not None:
                    sbm.speckleRange = speckleRange
                if speckleWindowSize is not None:
                    sbm.speckleWindowSize = speckleWindowSize
                if fullDP is not None:
                    sbm.fullDP = fullDP
            else:
                sbm.SADWindowSize = 9;
                sbm.numberOfDisparities = 96;
                sbm.preFilterCap = 63;
                sbm.minDisparity = -21;
                sbm.uniquenessRatio = 7;
                sbm.speckleWindowSize = 0;
                sbm.speckleRange = 8;
                sbm.disp12MaxDiff = 1;
                sbm.fullDP = False;

            disparity = sbm.compute(imgLeft.getGrayNumpyCv2(), imgRight.getGrayNumpyCv2())
            
        else:
            warnings.warn("Unknown method. Returning None")
            return None

        if cv2flag:
            if not isinstance(Q, np.ndarray):
                Q = np.array(Q)
            if not isinstance(disparity, np.ndarray):
                disparity = np.array(disparity)
            Image3D = cv2.reprojectImageTo3D(disparity, Q, ddepth=cv2.cv.CV_32F)
            Image3D_normalize = cv2.normalize(Image3D, alpha=0, beta=255, norm_type=cv2.cv.CV_MINMAX, dtype=cv2.cv.CV_8UC3)
            retVal = Image(Image3D_normalize, cv2image=True)
        else:
            Image3D = cv.CreateMat(self.LeftImage.size()[1], self.LeftImage.size()[0], cv2.cv.CV_32FC3)
            Image3D_normalize = cv.CreateMat(self.LeftImage.size()[1], self.LeftImage.size()[0], cv2.cv.CV_8UC3)
            cv.ReprojectImageTo3D(disparity, Image3D, Q)
            cv.Normalize(Image3D, Image3D_normalize, 0, 255, cv.CV_MINMAX, CV_8UC3)
            retVal = Image(Image3D_normalize)
        self.Image3D = Image3D
        return retVal

    def get3DImageFromDisparity(self, disparity, Q):
        """
        **SUMMARY**

        This method returns the 3D depth image using reprojectImageTo3D method.

        **PARAMETERS**
        * *disparity* - Disparity Image
        * *Q* - reprojection Matrix (disparity to depth matrix)

        **RETURNS**

        SimpleCV.Image representing 3D depth Image
        also StereoCamera.Image3D gives OpenCV 3D Depth Image of CV_32F type.

        **EXAMPLE**

        >>> lImage = Image("l.jpg")
        >>> rImage = Image("r.jpg")
        >>> stereo = StereoCamera()
        >>> Q = cv.Load("Q.yml")
        >>> disp = stereo.findDisparityMap()
        >>> stereo.get3DImageFromDisparity(disp, Q)
        """
        cv2flag = True
        try:
            import cv2
        except ImportError:
            cv2flag = False
            import cv2.cv as cv

        if cv2flag:
            if not isinstance(Q, np.ndarray):
                Q = np.array(Q)
            disparity = disparity.getNumpyCv2()    
            Image3D = cv2.reprojectImageTo3D(disparity, Q, ddepth=cv2.cv.CV_32F)
            Image3D_normalize = cv2.normalize(Image3D, alpha=0, beta=255, norm_type=cv2.cv.CV_MINMAX, dtype=cv2.cv.CV_8UC3)
            retVal = Image(Image3D_normalize, cv2image=True)
        else:
            disparity = disparity.getMatrix()
            Image3D = cv.CreateMat(self.LeftImage.size()[1], self.LeftImage.size()[0], cv2.cv.CV_32FC3)
            Image3D_normalize = cv.CreateMat(self.LeftImage.size()[1], self.LeftImage.size()[0], cv2.cv.CV_8UC3)
            cv.ReprojectImageTo3D(disparity, Image3D, Q)
            cv.Normalize(Image3D, Image3D_normalize, 0, 255, cv.CV_MINMAX, CV_8UC3)
            retVal = Image(Image3D_normalize)
        self.Image3D = Image3D
        return retVal
        

class StereoCamera :
    """
    Stereo Camera is a class dedicated for calibration stereo camera. It also has functionalites for
    rectification and getting undistorted Images.

    This class can be used to calculate various parameters related to both the camera's :
      -> Camera Matrix
      -> Distortion coefficients
      -> Rotation and Translation matrix
      -> Rectification transform (rotation matrix)
      -> Projection matrix in the new (rectified) coordinate systems
      -> Disparity-to-depth mapping matrix (Q)
    """
    def __init__(self):
        return

    def stereoCalibration(self,camLeft, camRight, nboards=30, chessboard=(8, 5), gridsize=0.027, WinSize = (352,288)):
        """
        
        **SUMMARY**
        
        Stereo Calibration is a way in which you obtain the parameters that will allow you to calculate 3D information of the scene.
        Once both the camera's are initialized.
        Press [Space] once chessboard is identified in both the camera's.
        Press [esc] key to exit the calibration process.

        **PARAMETERS**
        
        * camLeft - Left camera index.
        * camRight - Right camera index.
        * nboards - Number of samples or multiple views of the chessboard in different positions and orientations with your stereo camera.
        * chessboard - A tuple of Cols, Rows in the chessboard (used for calibration).
        * gridsize - chessboard grid size in real units
        * WinSize - This is the window resolution.

        **RETURNS**
        
        A tuple of the form (CM1, CM2, D1, D2, R, T, E, F) on success
        CM1 - Camera Matrix for left camera,
        CM2 - Camera Matrix for right camera,
        D1 - Vector of distortion coefficients for left camera,
        D2 - Vector of distortion coefficients for right camera,
        R - Rotation matrix between the left and the right camera coordinate systems,
        T - Translation vector between the left and the right coordinate systems of the cameras,
        E - Essential matrix,
        F - Fundamental matrix

        **EXAMPLE**
        
        >>> StereoCam = StereoCamera()
        >>> calibration = StereoCam.StereoCalibration(1,2,nboards=40)

        **Note**
        
        Press space to capture the images.
        
        """
        count = 0
        n1="Left"
        n2="Right"
        try :
            captureLeft = cv.CaptureFromCAM(camLeft)
            cv.SetCaptureProperty(captureLeft, cv.CV_CAP_PROP_FRAME_WIDTH, WinSize[0])
            cv.SetCaptureProperty(captureLeft, cv.CV_CAP_PROP_FRAME_HEIGHT, WinSize[1])
            frameLeft = cv.QueryFrame(captureLeft)
            cv.FindChessboardCorners(frameLeft, (chessboard))

            captureRight = cv.CaptureFromCAM(camRight)
            cv.SetCaptureProperty(captureRight, cv.CV_CAP_PROP_FRAME_WIDTH, WinSize[0])
            cv.SetCaptureProperty(captureRight, cv.CV_CAP_PROP_FRAME_HEIGHT, WinSize[1])
            frameRight = cv.QueryFrame(captureRight)
            cv.FindChessboardCorners(frameRight, (chessboard))
        except :
            print "Error Initialising the Left and Right camera"
            return None

        imagePoints1 = cv.CreateMat(1, nboards * chessboard[0] * chessboard[1], cv.CV_64FC2)
        imagePoints2 = cv.CreateMat(1, nboards * chessboard[0] * chessboard[1], cv.CV_64FC2)

        objectPoints = cv.CreateMat(1, chessboard[0] * chessboard[1] * nboards, cv.CV_64FC3)
        nPoints = cv.CreateMat(1, nboards, cv.CV_32S)

        # the intrinsic camera matrices
        CM1 = cv.CreateMat(3, 3, cv.CV_64F)
        CM2 = cv.CreateMat(3, 3, cv.CV_64F)

        # the distortion coefficients of both cameras
        D1 = cv.CreateMat(1, 5, cv.CV_64F)
        D2 = cv.CreateMat(1, 5, cv.CV_64F)

        # matrices governing the rotation and translation from camera 1 to camera 2
        R = cv.CreateMat(3, 3, cv.CV_64F)
        T = cv.CreateMat(3, 1, cv.CV_64F)

        # the essential and fundamental matrices
        E = cv.CreateMat(3, 3, cv.CV_64F)
        F = cv.CreateMat(3, 3, cv.CV_64F)

        while True:
            frameLeft = cv.QueryFrame(captureLeft)
            cv.Flip(frameLeft, frameLeft, 1)
            frameRight = cv.QueryFrame(captureRight)
            cv.Flip(frameRight, frameRight, 1)
            k = cv.WaitKey(3)

            cor1 = cv.FindChessboardCorners(frameLeft, (chessboard))
            if cor1[0] :
                cv.DrawChessboardCorners(frameLeft, (chessboard), cor1[1], cor1[0])
                cv.ShowImage(n1, frameLeft)

            cor2 = cv.FindChessboardCorners(frameRight, (chessboard))
            if cor2[0]:
                cv.DrawChessboardCorners(frameRight, (chessboard), cor2[1], cor2[0])
                cv.ShowImage(n2, frameRight)

            if cor1[0] and cor2[0] and k==0x20:
                print count
                for i in range(0, len(cor1[1])):
                    cv.Set1D(imagePoints1, count * chessboard[0] * chessboard[1] + i, cv.Scalar(cor1[1][i][0], cor1[1][i][1]))
                    cv.Set1D(imagePoints2, count * chessboard[0] * chessboard[1] + i, cv.Scalar(cor2[1][i][0], cor2[1][i][1]))

                count += 1

                if count == nboards:
                    cv.DestroyAllWindows()
                    for i in range(nboards):
                        for j in range(chessboard[1]):
                            for k in range(chessboard[0]):
                                cv.Set1D(objectPoints, i * chessboard[1] * chessboard[0] + j * chessboard[0] + k, (k * gridsize, j * gridsize, 0))

                    for i in range(nboards):
                        cv.Set1D(nPoints, i, chessboard[0] * chessboard[1])


                    cv.SetIdentity(CM1)
                    cv.SetIdentity(CM2)
                    cv.Zero(D1)
                    cv.Zero(D2)

                    print "Running stereo calibration..."
                    del(camLeft)
                    del(camRight)
                    cv.StereoCalibrate(objectPoints, imagePoints1, imagePoints2, nPoints, CM1, D1, CM2, D2, WinSize, R, T, E, F,
                                      flags=cv.CV_CALIB_SAME_FOCAL_LENGTH | cv.CV_CALIB_ZERO_TANGENT_DIST)

                    print "Done."
                    return (CM1, CM2, D1, D2, R, T, E, F)

            cv.ShowImage(n1, frameLeft)
            cv.ShowImage(n2, frameRight)
            if k == 0x1b:
                print "ESC pressed. Exiting. WARNING: NOT ENOUGH CHESSBOARDS FOUND YET"
                cv.DestroyAllWindows()
                break

    def saveCalibration(self,calibration=None, fname="Stereo",cdir="."):
        """
        
        **SUMMARY**
        
        saveCalibration is a method to save the StereoCalibration parameters such as CM1, CM2, D1, D2, R, T, E, F of stereo pair.
        This method returns True on success and saves the calibration in the following format.
        StereoCM1.txt
        StereoCM2.txt
        StereoD1.txt
        StereoD2.txt
        StereoR.txt
        StereoT.txt
        StereoE.txt
        StereoF.txt
        
        **PARAMETERS**
        
        calibration - is a tuple os the form (CM1, CM2, D1, D2, R, T, E, F)
        CM1 -> Camera Matrix for left camera,
        CM2 -> Camera Matrix for right camera,
        D1 -> Vector of distortion coefficients for left camera,
        D2 -> Vector of distortion coefficients for right camera,
        R -> Rotation matrix between the left and the right camera coordinate systems,
        T -> Translation vector between the left and the right coordinate systems of the cameras,
        E -> Essential matrix,
        F -> Fundamental matrix


        **RETURNS**
        
        return True on success and saves the calibration files.

        **EXAMPLE**
        
        >>> StereoCam = StereoCamera()
        >>> calibration = StereoCam.StereoCalibration(1,2,nboards=40)
        >>> StereoCam.saveCalibration(calibration,fname="Stereo1")
        """
        filenames = (fname+"CM1.txt", fname+"CM2.txt", fname+"D1.txt", fname+"D2.txt", fname+"R.txt", fname+"T.txt", fname+"E.txt", fname+"F.txt")
        try :
            (CM1, CM2, D1, D2, R, T, E, F) = calibration
            cv.Save("{0}/{1}".format(cdir, filenames[0]), CM1)
            cv.Save("{0}/{1}".format(cdir, filenames[1]), CM2)
            cv.Save("{0}/{1}".format(cdir, filenames[2]), D1)
            cv.Save("{0}/{1}".format(cdir, filenames[3]), D2)
            cv.Save("{0}/{1}".format(cdir, filenames[4]), R)
            cv.Save("{0}/{1}".format(cdir, filenames[5]), T)
            cv.Save("{0}/{1}".format(cdir, filenames[6]), E)
            cv.Save("{0}/{1}".format(cdir, filenames[7]), F)
            print "Calibration parameters written to directory '{0}'.".format(cdir)
            return True

        except :
            return False

    def loadCalibration(self,fname="Stereo",dir="."):
        """
        
        **SUMMARY**
        
        loadCalibration is a method to load the StereoCalibration parameters such as CM1, CM2, D1, D2, R, T, E, F of stereo pair.
        This method loads from calibration files and return calibration on success else return false.
        
        **PARAMETERS**
        
        fname - is the prefix of the calibration files.
        dir - is the directory in which files are present.
        
        **RETURNS**
        
        a tuple of the form (CM1, CM2, D1, D2, R, T, E, F) on success.
        CM1 - Camera Matrix for left camera
        CM2 - Camera Matrix for right camera
        D1 - Vector of distortion coefficients for left camera
        D2 - Vector of distortion coefficients for right camera
        R - Rotation matrix between the left and the right camera coordinate systems
        T - Translation vector between the left and the right coordinate systems of the cameras
        E - Essential matrix
        F - Fundamental matrix
        else returns false
        
        **EXAMPLE**
        
        >>> StereoCam = StereoCamera()
        >>> loadedCalibration = StereoCam.loadCalibration(fname="Stereo1")
        
        """
        filenames = (fname+"CM1.txt", fname+"CM2.txt", fname+"D1.txt", fname+"D2.txt", fname+"R.txt", fname+"T.txt", fname+"E.txt", fname+"F.txt")
        try :
            CM1 = cv.Load("{0}/{1}".format(dir, filenames[0]))
            CM2 = cv.Load("{0}/{1}".format(dir, filenames[1]))
            D1 = cv.Load("{0}/{1}".format(dir, filenames[2]))
            D2 = cv.Load("{0}/{1}".format(dir, filenames[3]))
            R = cv.Load("{0}/{1}".format(dir, filenames[4]))
            T = cv.Load("{0}/{1}".format(dir, filenames[5]))
            E = cv.Load("{0}/{1}".format(dir, filenames[6]))
            F = cv.Load("{0}/{1}".format(dir, filenames[7]))
            print "Calibration files loaded from dir '{0}'.".format(dir)
            return (CM1, CM2, D1, D2, R, T, E, F)

        except :
            return False

    def stereoRectify(self,calib=None,WinSize=(352,288)):
        """
        
        **SUMMARY**
        
        Computes rectification transforms for each head of a calibrated stereo camera.
        
        **PARAMETERS**
        
        calibration - is a tuple os the form (CM1, CM2, D1, D2, R, T, E, F)
        CM1 - Camera Matrix for left camera,
        CM2 - Camera Matrix for right camera,
        D1 - Vector of distortion coefficients for left camera,
        D2 - Vector of distortion coefficients for right camera,
        R - Rotation matrix between the left and the right camera coordinate systems,
        T - Translation vector between the left and the right coordinate systems of the cameras,
        E - Essential matrix,
        F - Fundamental matrix
        
        **RETURNS**
        
        On success returns a a tuple of the format -> (R1, R2, P1, P2, Q, roi)
        R1 - Rectification transform (rotation matrix) for the left camera.
        R2 - Rectification transform (rotation matrix) for the right camera.
        P1 - Projection matrix in the new (rectified) coordinate systems for the left camera.
        P2 - Projection matrix in the new (rectified) coordinate systems for the right camera.
        Q - disparity-to-depth mapping matrix.
        
        **EXAMPLE**
        
        >>> StereoCam = StereoCamera()
        >>> calibration = StereoCam.loadCalibration(fname="Stereo1")
        >>> rectification = StereoCam.stereoRectify(calibration)
        
        """
        (CM1, CM2, D1, D2, R, T, E, F) = calib
        R1 = cv.CreateMat(3, 3, cv.CV_64F)
        R2 = cv.CreateMat(3, 3, cv.CV_64F)
        P1 = cv.CreateMat(3, 4, cv.CV_64F)
        P2 = cv.CreateMat(3, 4, cv.CV_64F)
        Q = cv.CreateMat(4, 4, cv.CV_64F)
        
        print "Running stereo rectification..."
        
        (leftroi, rightroi) = cv.StereoRectify(CM1, CM2, D1, D2, WinSize, R, T, R1, R2, P1, P2, Q)
        roi = []
        roi.append(max(leftroi[0], rightroi[0]))
        roi.append(max(leftroi[1], rightroi[1]))
        roi.append(min(leftroi[2], rightroi[2]))
        roi.append(min(leftroi[3], rightroi[3]))
        print "Done."
        return (R1, R2, P1, P2, Q, roi)

    def getImagesUndistort(self,imgLeft, imgRight, calibration, rectification, WinSize=(352,288)):
        """
        **SUMMARY**
        Rectify two images from the calibration and rectification parameters.

        **PARAMETERS**
        * *imgLeft* - Image captured from left camera and needs to be rectified.
        * *imgRight* - Image captures from right camera and need to be rectified.
        * *calibration* - A calibration tuple of the format (CM1, CM2, D1, D2, R, T, E, F)
        * *rectification* - A rectification tuple of the format (R1, R2, P1, P2, Q, roi)

        **RETURNS**
        returns rectified images in a tuple -> (imgLeft,imgRight)
        >>> StereoCam = StereoCamera()
        >>> calibration = StereoCam.loadCalibration(fname="Stereo1")
        >>> rectification = StereoCam.stereoRectify(loadedCalibration)
        >>> imgLeft = camLeft.getImage()
        >>> imgRight = camRight.getImage()
        >>> rectLeft,rectRight = StereoCam.getImagesUndistort(imgLeft,imgRight,calibration,rectification)
        """
        imgLeft = imgLeft.getMatrix()
        imgRight = imgRight.getMatrix()
        (CM1, CM2, D1, D2, R, T, E, F) = calibration
        (R1, R2, P1, P2, Q, roi) = rectification

        dst1 = cv.CloneMat(imgLeft)
        dst2 = cv.CloneMat(imgRight)
        map1x = cv.CreateMat(WinSize[1], WinSize[0], cv.CV_32FC1)
        map2x = cv.CreateMat(WinSize[1], WinSize[0], cv.CV_32FC1)
        map1y = cv.CreateMat(WinSize[1], WinSize[0], cv.CV_32FC1)
        map2y = cv.CreateMat(WinSize[1], WinSize[0], cv.CV_32FC1)

        #print "Rectifying images..."
        cv.InitUndistortRectifyMap(CM1, D1, R1, P1, map1x, map1y)
        cv.InitUndistortRectifyMap(CM2, D2, R2, P2, map2x, map2y)

        cv.Remap(imgLeft, dst1, map1x, map1y)
        cv.Remap(imgRight, dst2, map2x, map2y)
        return Image(dst1), Image(dst2)

    def get3DImage(self, leftIndex, rightIndex, Q, method="BM", state=None):
        """
        **SUMMARY**

        This method returns the 3D depth image using reprojectImageTo3D method.

        **PARAMETERS**
        
        * *leftIndex* - Index of left camera
        * *rightIndex* - Index of right camera
        * *Q* - reprojection Matrix (disparity to depth matrix)
        * *method* - Stereo Correspondonce method to be used.
                   - "BM" - Stereo BM
                   - "SGBM" - Stereo SGBM
        * *state* - dictionary corresponding to parameters of
                    stereo correspondonce.
                    SADWindowSize - odd int
                    nDisparity - int
                    minDisparity  - int
                    preFilterCap - int
                    preFilterType - int (only BM)
                    speckleRange - int
                    speckleWindowSize - int
                    P1 - int (only SGBM)
                    P2 - int (only SGBM)
                    fullDP - Bool (only SGBM)
                    uniquenessRatio - int
                    textureThreshold - int (only BM)
                    

        **RETURNS**

        SimpleCV.Image representing 3D depth Image
        also StereoCamera.Image3D gives OpenCV 3D Depth Image of CV_32F type.

        **EXAMPLE**

        >>> lImage = Image("l.jpg")
        >>> rImage = Image("r.jpg")
        >>> stereo = StereoCamera()
        >>> Q = cv.Load("Q.yml")
        >>> stereo.get3DImage(1, 2, Q).show()

        >>> state = {"SADWindowSize":9, "nDisparity":112, "minDisparity":-39}
        >>> stereo.get3DImage(1, 2, Q, "BM", state).show()
        >>> stereo.get3DImage(1, 2, Q, "SGBM", state).show()
        """
        cv2flag = True
        try:
            import cv2
        except ImportError:
            cv2flag = False
            import cv2.cv as cv
        if cv2flag:
            camLeft = cv2.VideoCapture(leftIndex)
            camRight = cv2.VideoCapture(rightIndex)
            if camLeft.isOpened():
                _, imgLeft = camLeft.read()
            else:
                warnings.warn("Unable to open left camera")
                return None
            if camRight.isOpened():
                _, imgRight = camRight.read()
            else:
                warnings.warn("Unable to open right camera")
                return None
            imgLeft = Image(imgLeft, cv2image=True)
            imgRight = Image(imgRight, cv2image=True)
        else:
            camLeft = cv.CaptureFromCAM(leftIndex)
            camRight = cv.CaptureFromCAM(rightIndex)
            imgLeft = cv.QueryFrame(camLeft)
            if imgLeft is None:
                warnings.warn("Unable to open left camera")
                return None

            imgRight = cv.QueryFrame(camRight)
            if imgRight is None:
                warnings.warn("Unable to open right camera")
                return None

            imgLeft = Image(imgLeft, cv2image=True)
            imgRight = Image(imgRight, cv2image=True)

        del camLeft
        del camRight

        stereoImages = StereoImage(imgLeft, imgRight)
        Image3D_normalize = stereoImages.get3DImage(Q, method, state)
        self.Image3D = stereoImages.Image3D
        return Image3D_normalize


class AVTCameraThread(threading.Thread):
    camera = None
    run = True
    verbose = False
    lock = None
    logger = None
    framerate = 0


    def __init__(self, camera):
        super(AVTCameraThread, self).__init__()
        self._stop = threading.Event()
        self.camera = camera
        self.lock = threading.Lock()
        self.name = 'Thread-Camera-ID-' + str(self.camera.uniqueid)

      
    def run(self):
        counter = 0
        timestamp = time.time()
        
        while self.run:
          self.lock.acquire()
          self.camera.runCommand("AcquisitionStart")
          frame = self.camera._getFrame(1000)
          
          if frame:            
            img = Image(pil.fromstring(self.camera.imgformat, 
              (self.camera.width, self.camera.height), 
              frame.ImageBuffer[:int(frame.ImageBufferSize)]))
            self.camera._buffer.appendleft(img)
            
          self.camera.runCommand("AcquisitionStop")
          self.lock.release()
          counter += 1
          time.sleep(0.01)

          if time.time() - timestamp >= 1:
            self.camera.framerate = counter
            counter = 0
            timestamp = time.time()
            


    def stop(self):
        self._stop.set()

    def stopped(self):
        return self._stop.isSet()
  


AVTCameraErrors = [
    ("ePvErrSuccess",        "No error"),
    ("ePvErrCameraFault",        "Unexpected camera fault"),
    ("ePvErrInternalFault",        "Unexpected fault in PvApi or driver"),
    ("ePvErrBadHandle",        "Camera handle is invalid"),
    ("ePvErrBadParameter",        "Bad parameter to API call"),
    ("ePvErrBadSequence",        "Sequence of API calls is incorrect"),
    ("ePvErrNotFound",        "Camera or attribute not found"),
    ("ePvErrAccessDenied",        "Camera cannot be opened in the specified mode"),
    ("ePvErrUnplugged",        "Camera was unplugged"),
    ("ePvErrInvalidSetup",        "Setup is invalid (an attribute is invalid)"),
    ("ePvErrResources",       "System/network resources or memory not available"),
    ("ePvErrBandwidth",       "1394 bandwidth not available"),
    ("ePvErrQueueFull",       "Too many frames on queue"),
    ("ePvErrBufferTooSmall",       "Frame buffer is too small"),
    ("ePvErrCancelled",       "Frame cancelled by user"),
    ("ePvErrDataLost",       "The data for the frame was lost"),
    ("ePvErrDataMissing",       "Some data in the frame is missing"),
    ("ePvErrTimeout",       "Timeout during wait"),
    ("ePvErrOutOfRange",       "Attribute value is out of the expected range"),
    ("ePvErrWrongType",       "Attribute is not this type (wrong access function)"),
    ("ePvErrForbidden",       "Attribute write forbidden at this time"),
    ("ePvErrUnavailable",       "Attribute is not available at this time"),
    ("ePvErrFirewall",       "A firewall is blocking the traffic (Windows only)"),
  ]
def pverr(errcode):
  if errcode:
    raise Exception(": ".join(AVTCameraErrors[errcode]))


class AVTCamera(FrameSource):
    """
    **SUMMARY**
    AVTCamera is a ctypes wrapper for the Prosilica/Allied Vision cameras,
    such as the "manta" series.

    These require the PvAVT binary driver from Allied Vision:
    http://www.alliedvisiontec.com/us/products/1108.html

    Note that as of time of writing the new VIMBA driver is not available
    for Mac/Linux - so this uses the legacy PvAVT drive

    Props to Cixelyn, whos py-avt-pvapi module showed how to get much
    of this working https://bitbucket.org/Cixelyn/py-avt-pvapi

    All camera properties are directly from the PvAVT manual -- if not
    specified it will default to whatever the camera state is.  Cameras
    can either by

    **EXAMPLE**
    >>> cam = AVTCamera(0, {"width": 656, "height": 492})
    >>>
    >>> img = cam.getImage()
    >>> img.show()
    """

    
    _buffer = None # Buffer to store images
    _buffersize = 10 # Number of images to keep in the rolling image buffer for threads
    _lastimage = None # Last image loaded into memory
    _thread = None
    _framerate = 0
    threaded = False
    _pvinfo = { }    
    _properties = {
        "AcqEndTriggerEvent": ("Enum", "R/W"),
        "AcqEndTriggerMode": ("Enum", "R/W"),
        "AcqRecTriggerEvent": ("Enum", "R/W"),
        "AcqRecTriggerMode": ("Enum", "R/W"),
        "AcqStartTriggerEvent": ("Enum", "R/W"),
        "AcqStartTriggerMode": ("Enum", "R/W"),
        "FrameRate": ("Float32", "R/W"),
        "FrameStartTriggerDelay": ("Uint32", "R/W"),
        "FrameStartTriggerEvent": ("Enum", "R/W"),
        "FrameStartTriggerMode": ("Enum", "R/W"),
        "FrameStartTriggerOverlap": ("Enum", "R/W"),
        "AcquisitionFrameCount": ("Uint32", "R/W"),
        "AcquisitionMode": ("Enum", "R/W"),
        "RecorderPreEventCount": ("Uint32", "R/W"),
        "ConfigFileIndex": ("Enum", "R/W"),
        "ConfigFilePowerup": ("Enum", "R/W"),
        "DSPSubregionBottom": ("Uint32", "R/W"),
        "DSPSubregionLeft": ("Uint32", "R/W"),
        "DSPSubregionRight": ("Uint32", "R/W"),
        "DSPSubregionTop": ("Uint32", "R/W"),
        "DefectMaskColumnEnable": ("Enum", "R/W"),
        "ExposureAutoAdjustTol": ("Uint32", "R/W"),
        "ExposureAutoAlg": ("Enum", "R/W"),
        "ExposureAutoMax": ("Uint32", "R/W"),
        "ExposureAutoMin": ("Uint32", "R/W"),
        "ExposureAutoOutliers": ("Uint32", "R/W"),
        "ExposureAutoRate": ("Uint32", "R/W"),
        "ExposureAutoTarget": ("Uint32", "R/W"),
        "ExposureMode": ("Enum", "R/W"),
        "ExposureValue": ("Uint32", "R/W"),
        "GainAutoAdjustTol": ("Uint32", "R/W"),
        "GainAutoMax": ("Uint32", "R/W"),
        "GainAutoMin": ("Uint32", "R/W"),
        "GainAutoOutliers": ("Uint32", "R/W"),
        "GainAutoRate": ("Uint32", "R/W"),
        "GainAutoTarget": ("Uint32", "R/W"),
        "GainMode": ("Enum", "R/W"),
        "GainValue": ("Uint32", "R/W"),
        "LensDriveCommand": ("Enum", "R/W"),
        "LensDriveDuration": ("Uint32", "R/W"),
        "LensVoltage": ("Uint32", "R/V"),
        "LensVoltageControl": ("Uint32", "R/W"),
        "IrisAutoTarget": ("Uint32", "R/W"),
        "IrisMode": ("Enum", "R/W"),
        "IrisVideoLevel": ("Uint32", "R/W"),
        "IrisVideoLevelMax": ("Uint32", "R/W"),
        "IrisVideoLevelMin": ("Uint32", "R/W"),
        "VsubValue": ("Uint32", "R/C"),
        "WhitebalAutoAdjustTol": ("Uint32", "R/W"),
        "WhitebalAutoRate": ("Uint32", "R/W"),
        "WhitebalMode": ("Enum", "R/W"),
        "WhitebalValueRed": ("Uint32", "R/W"),
        "WhitebalValueBlue": ("Uint32", "R/W"),
        "EventAcquisitionStart": ("Uint32", "R/C 40000"),
        "EventAcquisitionEnd": ("Uint32", "R/C 40001"),
        "EventFrameTrigger": ("Uint32", "R/C 40002"),
        "EventExposureEnd": ("Uint32", "R/C 40003"),
        "EventAcquisitionRecordTrigger": ("Uint32", "R/C 40004"),
        "EventSyncIn1Rise": ("Uint32", "R/C 40010"),
        "EventSyncIn1Fall": ("Uint32", "R/C 40011"),
        "EventSyncIn2Rise": ("Uint32", "R/C 40012"),
        "EventSyncIn2Fall": ("Uint32", "R/C 40013"),
        "EventSyncIn3Rise": ("Uint32", "R/C 40014"),
        "EventSyncIn3Fall": ("Uint32", "R/C 40015"),
        "EventSyncIn4Rise": ("Uint32", "R/C 40016"),
        "EventSyncIn4Fall": ("Uint32", "R/C 40017"),
        "EventOverflow": ("Uint32", "R/C 65534"),
        "EventError": ("Uint32", "R/C"),
        "EventNotification": ("Enum", "R/W"),
        "EventSelector": ("Enum", "R/W"),
        "EventsEnable1": ("Uint32", "R/W"),
        "BandwidthCtrlMode": ("Enum", "R/W"),
        "ChunkModeActive": ("Boolean", "R/W"),
        "NonImagePayloadSize": ("Unit32", "R/V"),
        "PayloadSize": ("Unit32", "R/V"),
        "StreamBytesPerSecond": ("Uint32", "R/W"),
        "StreamFrameRateConstrain": ("Boolean", "R/W"),
        "StreamHoldCapacity": ("Uint32", "R/V"),
        "StreamHoldEnable": ("Enum", "R/W"),
        "TimeStampFrequency": ("Uint32", "R/C"),
        "TimeStampValueHi": ("Uint32", "R/V"),
        "TimeStampValueLo": ("Uint32", "R/V"),
        "Height": ("Uint32", "R/W"),
        "RegionX": ("Uint32", "R/W"),
        "RegionY": ("Uint32", "R/W"),
        "Width": ("Uint32", "R/W"),
        "PixelFormat": ("Enum", "R/W"),
        "TotalBytesPerFrame": ("Uint32", "R/V"),
        "BinningX": ("Uint32", "R/W"),
        "BinningY": ("Uint32", "R/W"),
        "CameraName": ("String", "R/W"),
        "DeviceFirmwareVersion": ("String", "R/C"),
        "DeviceModelName": ("String", "R/W"),
        "DevicePartNumber": ("String", "R/C"),
        "DeviceSerialNumber": ("String", "R/C"),
        "DeviceVendorName": ("String", "R/C"),
        "FirmwareVerBuild": ("Uint32", "R/C"),
        "FirmwareVerMajor": ("Uint32", "R/C"),
        "FirmwareVerMinor": ("Uint32", "R/C"),
        "PartClass": ("Uint32", "R/C"),
        "PartNumber": ("Uint32", "R/C"),
        "PartRevision": ("String", "R/C"),
        "PartVersion": ("String", "R/C"),
        "SerialNumber": ("String", "R/C"),
        "SensorBits": ("Uint32", "R/C"),
        "SensorHeight": ("Uint32", "R/C"),
        "SensorType": ("Enum", "R/C"),
        "SensorWidth": ("Uint32", "R/C"),
        "UniqueID": ("Uint32", "R/C"),
        "Strobe1ControlledDuration": ("Enum", "R/W"),
        "Strobe1Delay": ("Uint32", "R/W"),
        "Strobe1Duration": ("Uint32", "R/W"),
        "Strobe1Mode": ("Enum", "R/W"),
        "SyncIn1GlitchFilter": ("Uint32", "R/W"),
        "SyncInLevels": ("Uint32", "R/V"),
        "SyncOut1Invert": ("Enum", "R/W"),
        "SyncOut1Mode": ("Enum", "R/W"),
        "SyncOutGpoLevels": ("Uint32", "R/W"),
        "DeviceEthAddress": ("String", "R/C"),
        "HostEthAddress": ("String", "R/C"),
        "DeviceIPAddress": ("String", "R/C"),
        "HostIPAddress": ("String", "R/C"),
        "GvcpRetries": ("Uint32", "R/W"),
        "GvspLookbackWindow": ("Uint32", "R/W"),
        "GvspResentPercent": ("Float32", "R/W"),
        "GvspRetries": ("Uint32", "R/W"),
        "GvspSocketBufferCount": ("Enum", "R/W"),
        "GvspTimeout": ("Uint32", "R/W"),
        "HeartbeatInterval": ("Uint32", "R/W"),
        "HeartbeatTimeout": ("Uint32", "R/W"),
        "MulticastEnable": ("Enum", "R/W"),
        "MulticastIPAddress": ("String", "R/W"),
        "PacketSize": ("Uint32", "R/W"),
        "StatDriverType": ("Enum", "R/V"),
        "StatFilterVersion": ("String", "R/C"),
        "StatFrameRate": ("Float32", "R/V"),
        "StatFramesCompleted": ("Uint32", "R/V"),
        "StatFramesDropped": ("Uint32", "R/V"),
        "StatPacketsErroneous": ("Uint32", "R/V"),
        "StatPacketsMissed": ("Uint32", "R/V"),
        "StatPacketsReceived": ("Uint32", "R/V"),
        "StatPacketsRequested": ("Uint32", "R/V"),
        "StatPacketResent": ("Uint32", "R/V")
        }



    class AVTCameraInfo(ct.Structure):
        """
        AVTCameraInfo is an internal ctypes.Structure-derived class which
        contains metadata about cameras on the local network.

        Properties include:
        * UniqueId
        * CameraName
        * ModelName
        * PartNumber
        * SerialNumber
        * FirmwareVersion
        * PermittedAccess
        * InterfaceId
        * InterfaceType
        """
        _fields_ = [
            ("StructVer", ct.c_ulong),
            ("UniqueId", ct.c_ulong),
            ("CameraName", ct.c_char*32),
            ("ModelName", ct.c_char*32),
            ("PartNumber", ct.c_char*32),
            ("SerialNumber", ct.c_char*32),
            ("FirmwareVersion", ct.c_char*32),
            ("PermittedAccess", ct.c_long),
            ("InterfaceId", ct.c_ulong),
            ("InterfaceType", ct.c_int)
        ]

        def __repr__(self):
            return "<SimpleCV.Camera.AVTCameraInfo - UniqueId: %s>" % (self.UniqueId)

    class AVTFrame(ct.Structure):
        _fields_ = [
            ("ImageBuffer", ct.POINTER(ct.c_char)),
            ("ImageBufferSize", ct.c_ulong),
            ("AncillaryBuffer", ct.c_int),
            ("AncillaryBufferSize", ct.c_int),
            ("Context", ct.c_int*4),
            ("_reserved1", ct.c_ulong*8),

            ("Status", ct.c_int),
            ("ImageSize", ct.c_ulong),
            ("AncillarySize", ct.c_ulong),
            ("Width", ct.c_ulong),
            ("Height", ct.c_ulong),
            ("RegionX", ct.c_ulong),
            ("RegionY", ct.c_ulong),
            ("Format", ct.c_int),
            ("BitDepth", ct.c_ulong),
            ("BayerPattern", ct.c_int),
            ("FrameCount", ct.c_ulong),
            ("TimestampLo", ct.c_ulong),
            ("TimestampHi", ct.c_ulong),
            ("_reserved2", ct.c_ulong*32)
        ]

        def __init__(self, buffersize):
            self.ImageBuffer = ct.create_string_buffer(buffersize)
            self.ImageBufferSize = ct.c_ulong(buffersize)
            self.AncillaryBuffer = 0
            self.AncillaryBufferSize = 0
            self.img = None
            self.hasImage = False
            self.frame = None

    def __del__(self):
      #This function should disconnect from the AVT Camera
      pverr(self.dll.PvCameraClose(self.handle))
    
    def __init__(self, camera_id = -1, properties = {}, threaded = False):
        #~ super(AVTCamera, self).__init__()
        import platform

        if platform.system() == "Windows":
            self.dll = ct.windll.LoadLibrary("PvAPI.dll")
        elif platform.system() == "Darwin":
            self.dll = ct.CDLL("libPvAPI.dylib", ct.RTLD_GLOBAL)
        else:
            self.dll = ct.CDLL("libPvAPI.so")

        if not self._pvinfo.get("initialized", False):
            self.dll.PvInitialize()
            self._pvinfo['initialized'] = True
        #initialize.  Note that we rely on listAllCameras being the next
        #call, since it blocks on cameras initializing

        camlist = self.listAllCameras()

        if not len(camlist):
            raise Exception("Couldn't find any cameras with the PvAVT driver.  Use SampleViewer to confirm you have one connected.")

        if camera_id < 9000: #camera was passed as an index reference
            if camera_id == -1:  #accept -1 for "first camera"
                camera_id = 0

            camera_id = camlist[camera_id].UniqueId

        camera_id = long(camera_id)
        self.handle = ct.c_uint()
        init_count = 0
        while self.dll.PvCameraOpen(camera_id,0,ct.byref(self.handle)) != 0: #wait until camera is availble
          if init_count > 4: # Try to connect 5 times before giving up
            raise Exception('Could not connect to camera, please verify with SampleViewer you can connect')
          init_count += 1
          time.sleep(1) # sleep and retry to connect to camera in a second

        pverr(self.dll.PvCaptureStart(self.handle))
        self.uniqueid = camera_id

        self.setProperty("AcquisitionMode","SingleFrame")
        self.setProperty("FrameStartTriggerMode","Freerun")

        if properties.get("mode", "RGB") == 'gray':
            self.setProperty("PixelFormat", "Mono8")
        else:
            self.setProperty("PixelFormat", "Rgb24")

        #give some compatablity with other cameras
        if properties.get("mode", ""):
            properties.pop("mode")

        if properties.get("height", ""):
            properties["Height"] = properties["height"]
            properties.pop("height")

        if properties.get("width", ""):
            properties["Width"] = properties["width"]
            properties.pop("width")

        for p in properties:
            self.setProperty(p, properties[p])


        if threaded:
          self._thread = AVTCameraThread(self)
          self._thread.daemon = True
          self._buffer = deque(maxlen=self._buffersize)
          self._thread.start()
          self.threaded = True
        self.frame = None
        self._refreshFrameStats()

    def restart(self):
        """
        This tries to restart the camera thread
        """
        self._thread.stop()
        self._thread = AVTCameraThread(self)
        self._thread.daemon = True
        self._buffer = deque(maxlen=self._buffersize)
        self._thread.start()


    def listAllCameras(self):
        """
        **SUMMARY**
        List all cameras attached to the host

        **RETURNS**
        List of AVTCameraInfo objects, otherwise empty list

        """
        camlist = (self.AVTCameraInfo*100)()
        starttime = time.time()
        while int(camlist[0].UniqueId) == 0 and time.time() - starttime < 10:
            self.dll.PvCameraListEx(ct.byref(camlist), 100, None, ct.sizeof(self.AVTCameraInfo))
            time.sleep(0.1) #keep checking for cameras until timeout


        return [cam for cam in camlist if cam.UniqueId != 0]

    def runCommand(self,command):
        """
        **SUMMARY**
        Runs a PvAVT Command on the camera

        Valid Commands include:
        * FrameStartTriggerSoftware
        * AcquisitionAbort
        * AcquisitionStart
        * AcquisitionStop
        * ConfigFileLoad
        * ConfigFileSave
        * TimeStampReset
        * TimeStampValueLatch

        **RETURNS**

        0 on success

        **EXAMPLE**
        >>>c = AVTCamera()
        >>>c.runCommand("TimeStampReset")
        """
        return self.dll.PvCommandRun(self.handle,command)

    def getProperty(self, name):
        """
        **SUMMARY**
        This retrieves the value of the AVT Camera attribute

        There are around 140 properties for the AVT Camera, so reference the
        AVT Camera and Driver Attributes pdf that is provided with
        the driver for detailed information

        Note that the error codes are currently ignored, so empty values
        may be returned.

        **EXAMPLE**
        >>>c = AVTCamera()
        >>>print c.getProperty("ExposureValue")
        """
        valtype, perm = self._properties.get(name, (None, None))

        if not valtype:
            return None

        val = ''
        err = 0
        if valtype == "Enum":
            val = ct.create_string_buffer(100)
            vallen = ct.c_long()
            err = self.dll.PvAttrEnumGet(self.handle, name, val, 100, ct.byref(vallen))
            val = str(val[:vallen.value])
        elif valtype == "Uint32":
            val = ct.c_uint()
            err = self.dll.PvAttrUint32Get(self.handle, name, ct.byref(val))
            val = int(val.value)
        elif valtype == "Float32":
            val = ct.c_float()
            err = self.dll.PvAttrFloat32Get(self.handle, name, ct.byref(val))
            val = float(val.value)
        elif valtype == "String":
            val = ct.create_string_buffer(100)
            vallen = ct.c_long()
            err = self.dll.PvAttrStringGet(self.handle, name, val, 100, ct.byref(vallen))
            val = str(val[:vallen.value])
        elif valtype == "Boolean":
            val = ct.c_bool()
            err = self.dll.PvAttrBooleanGet(self.handle, name, ct.byref(val))
            val = bool(val.value)

        #TODO, handle error codes

        return val

    #TODO, implement the PvAttrRange* functions
    #def getPropertyRange(self, name)

    def getAllProperties(self):
        """
        **SUMMARY**
        This returns a dict with the name and current value of the
        documented PvAVT attributes

        CAVEAT: it addresses each of the properties individually, so
        this may take time to run if there's network latency

        **EXAMPLE**
        >>>c = AVTCamera(0)
        >>>props = c.getAllProperties()
        >>>print props['ExposureValue']

        """
        props = {}
        for p in self._properties.keys():
            props[p] = self.getProperty(p)

        return props

    def setProperty(self, name, value, skip_buffer_size_check=False):
        """
        **SUMMARY**
        This sets the value of the AVT Camera attribute.

        There are around 140 properties for the AVT Camera, so reference the
        AVT Camera and Driver Attributes pdf that is provided with
        the driver for detailed information

        By default, we will also refresh the height/width and bytes per
        frame we're expecting -- you can manually bypass this if you want speed

        Returns the raw PvAVT error code (0 = success)

        **Example**
        >>>c = AVTCamera()
        >>>c.setProperty("ExposureValue", 30000)
        >>>c.getImage().show()
        """
        valtype, perm = self._properties.get(name, (None, None))

        if not valtype:
            return None

        if valtype == "Uint32":
            err = self.dll.PvAttrUint32Set(self.handle, name, ct.c_uint(int(value)))
        elif valtype == "Float32":
            err = self.dll.PvAttrFloat32Set(self.handle, name, ct.c_float(float(value)))
        elif valtype == "Enum":
            err = self.dll.PvAttrEnumSet(self.handle, name, str(value))
        elif valtype == "String":
            err = self.dll.PvAttrStringSet(self.handle, name, str(value))
        elif valtype == "Boolean":
            err = self.dll.PvAttrBooleanSet(self.handle, name, ct.c_bool(bool(value)))

        #just to be safe, re-cache the camera metadata
        if not skip_buffer_size_check:
            self._refreshFrameStats()

        return err

    def getImage(self, timeout = 5000):
        """
        **SUMMARY**
        Extract an Image from the Camera, returning the value.  No matter
        what the image characteristics on the camera, the Image returned
        will be RGB 8 bit depth, if camera is in greyscale mode it will
        be 3 identical channels.

        **EXAMPLE**
        >>>c = AVTCamera()
        >>>c.getImage().show()
        """

        if self.frame != None:
            st = time.time()
            try:
                pverr( self.dll.PvCaptureWaitForFrameDone(self.handle, ct.byref(self.frame), timeout) )
            except Exception, e:
                print "Exception waiting for frame:", e
                print "Time taken:",time.time() - st
                self.frame = None
                raise(e)
            img = self.unbuffer()
            self.frame = None
            return img
        elif self.threaded:
          self._thread.lock.acquire()
          try:
              img = self._buffer.pop()
              self._lastimage = img
          except IndexError:
              img = self._lastimage
          self._thread.lock.release()

        else:
          self.runCommand("AcquisitionStart")
          frame = self._getFrame(timeout)
          img = Image(pil.fromstring(self.imgformat, 
              (self.width, self.height), 
              frame.ImageBuffer[:int(frame.ImageBufferSize)]))
          self.runCommand("AcquisitionStop")
        return img


    def setupASyncMode(self):
        self.setProperty('AcquisitionMode','SingleFrame')
        self.setProperty('FrameStartTriggerMode','Software')


    def setupSyncMode(self):
        self.setProperty('AcquisitionMode','Continuous')
        self.setProperty('FrameStartTriggerMode','FreeRun')

    def unbuffer(self):
        img = Image(pil.fromstring(self.imgformat,
                                   (self.width, self.height),
                                   self.frame.ImageBuffer[:int(self.frame.ImageBufferSize)]))

        return img

    def _refreshFrameStats(self):
        self.width = self.getProperty("Width")
        self.height = self.getProperty("Height")
        self.buffersize = self.getProperty("TotalBytesPerFrame")
        self.pixelformat = self.getProperty("PixelFormat")
        self.imgformat = 'RGB'
        if self.pixelformat == 'Mono8':
            self.imgformat = 'L'

    def _getFrame(self, timeout = 5000):
        #return the AVTFrame object from the camera, timeout in ms
        #need to multiply by bitdepth
        try:
          frame = self.AVTFrame(self.buffersize)
          pverr( self.dll.PvCaptureQueueFrame(self.handle, ct.byref(frame), None) )
          st = time.time()
          try:
            pverr( self.dll.PvCaptureWaitForFrameDone(self.handle, ct.byref(frame), timeout) )
          except Exception, e:
            print "Exception waiting for frame:", e
            print "Time taken:",time.time() - st
            raise(e)
            
        except Exception, e:
            print "Exception aquiring frame:", e
            raise(e)  
          
        return frame

    def acquire(self):
        self.frame = self.AVTFrame(self.buffersize)
        try:
            self.runCommand("AcquisitionStart")
            pverr( self.dll.PvCaptureQueueFrame(self.handle, ct.byref(self.frame), None) )
            self.runCommand("AcquisitionStop")
        except Exception, e:
            print "Exception aquiring frame:", e
            raise(e)  


class GigECamera(Camera):
    """
        GigE Camera driver via Aravis
    """
       
    def __init__(self, camera_id = None, properties = {}, threaded = False):
        try:
            from gi.repository import Aravis
        except:
            print "GigE is supported by the Aravis library, download and build from https://github.com/sightmachine/aravis"
            print "Note that you need to set GI_TYPELIB_PATH=$GI_TYPELIB_PATH:(PATH_TO_ARAVIS)/src for the GObject Introspection"
            sys.exit()
        
        self._cam = Aravis.Camera.new (None)
        
        self._pixel_mode = "RGB"
        if properties.get("mode", False):
            self._pixel_mode = properties.pop("mode")
        
        
        if self._pixel_mode == "gray":
            self._cam.set_pixel_format (Aravis.PIXEL_FORMAT_MONO_8)
        else:
            self._cam.set_pixel_format (Aravis.PIXEL_FORMAT_BAYER_BG_8) #we'll use bayer (basler cams)
            #TODO, deal with other pixel formats
        
        if properties.get("roi", False):
            roi = properties['roi']
            self._cam.set_region(*roi)
            #TODO, check sensor size
        
        if properties.get("width", False):
            #TODO, set internal function to scale results of getimage
            pass
        
        if properties.get("framerate", False):
            self._cam.set_frame_rate(properties['framerate'])
        
        self._stream = self._cam.create_stream (None, None)
        
        payload = self._cam.get_payload()
        self._stream.push_buffer(Aravis.Buffer.new_allocate (payload))
        [x,y,width,height] = self._cam.get_region ()
        self._height, self._width = height, width
    
    def getImage(self):
        
        camera = self._cam
        camera.start_acquisition()
        buff = self._stream.pop_buffer()
        self.capturetime = buff.timestamp_ns / 1000000.0
        img = np.fromstring(ct.string_at(buff.data_address(), buff.size), dtype = np.uint8).reshape(self._height, self._width)
        rgb = cv2.cvtColor(img, cv2.COLOR_BAYER_BG2BGR)
        self._stream.push_buffer(buff)
        camera.stop_acquisition()
        #TODO, we should handle software triggering (separate capture and get image events)
        
        return Image(rgb)

    def getPropertyList(self):
      l = [
           'available_pixel_formats',
           'available_pixel_formats_as_display_names',
           'available_pixel_formats_as_strings',
           'binning',
           'device_id',
           'exposure_time',
           'exposure_time_bounds',
           'frame_rate',
           'frame_rate_bounds',
           'gain',
           'gain_bounds',
           'height_bounds',
           'model_name',
           'payload',
           'pixel_format',
           'pixel_format_as_string',
           'region',
           'sensor_size',
           'trigger_source',
           'vendor_name',
           'width_bounds'
          ]
      return l
      
    
    def getProperty(self, name = None):
      '''
      This function get's the properties availble to the camera

      Usage:
        > camera.getProperty('region')
        > (0, 0, 128, 128)
      
      Available Properties:
        see function camera.getPropertyList()
      '''
      if name == None:
        print "You need to provide a property, available properties are:"
        print ""
        for p in self.getPropertyList():
          print p
        return

      stringval = "get_{}".format(name)
      try:
        return getattr(self._cam, stringval)()
      except:
        print 'Property {} does not appear to exist'.format(name)
        return None
      
    def setProperty(self, name = None, *args):
      '''
      This function sets the property available to the camera

      Usage:
        > camera.setProperty('region',(256,256))

      Available Properties:
        see function camera.getPropertyList()

      '''

      if name == None:
        print "You need to provide a property, available properties are:"
        print ""
        for p in self.getPropertyList():
          print p
        return

      if len(args) <= 0:
        print "You must provide a value to set"
        return
        
      stringval = "set_{}".format(name)
      try:
        return getattr(self._cam, stringval)(*args)
      except:
        print 'Property {} does not appear to exist or value is not in correct format'.format(name)
        return None

       
    def getAllProperties(self):
      '''
      This function just prints out all the properties available to the camera
      '''
      
      for p in self.getPropertyList():
        print "{}: {}".format(p,self.getProperty(p))

class VimbaCameraThread(threading.Thread):
    camera = None
    run = True
    verbose = False
    lock = None
    logger = None
    framerate = 0


    def __init__(self, camera):
        super(VimbaCameraThread, self).__init__()
        self._stop = threading.Event()
        self.camera = camera
        self.lock = threading.Lock()
        self.name = 'Thread-Camera-ID-' + str(self.camera.uniqueid)

      
    def run(self):
        counter = 0
        timestamp = time.time()
        
        while self.run:
            self.lock.acquire()

            img = self.camera._captureFrame(1000)
            self.camera._buffer.appendleft(img)

            self.lock.release()
            counter += 1
            time.sleep(0.01)

            if time.time() - timestamp >= 1:
                self.camera.framerate = counter
                counter = 0
                timestamp = time.time()

    def stop(self):
        self._stop.set()

    def stopped(self):
        return self._stop.isSet()

class VimbaCamera(FrameSource):
    """
    **SUMMARY**
    VimbaCamera is a wrapper for the Allied Vision cameras,
    such as the "manta" series.

    This requires the 
    1) Vimba SDK provided from Allied Vision
    http://www.alliedvisiontec.com/us/products/software/vimba-sdk.html

    2) Pyvimba Python library
    TODO: <INSERT URL>

    Note that as of time of writing, the VIMBA driver is not available
    for Mac.

    All camera properties are directly from the Vimba SDK manual -- if not
    specified it will default to whatever the camera state is.  Cameras
    can either by

    **EXAMPLE**
    >>> cam = VimbaCamera(0, {"width": 656, "height": 492})
    >>>
    >>> img = cam.getImage()
    >>> img.show()
    """

    def _setupVimba(self):
        from pymba import Vimba

        self._vimba = Vimba()
        self._vimba.startup()
        system = self._vimba.getSystem()
        if system.GeVTLIsPresent:
            system.runFeatureCommand("GeVDiscoveryAllOnce")
            time.sleep(0.2)

    def __del__(self):
        #This function should disconnect from the Vimba Camera
        if self._camera is not None:
            if self.threaded:
                self._thread.stop()
                time.sleep(0.2)

            if self._frame is not None:
                self._frame.revokeFrame()
                self._frame = None

            self._camera.closeCamera()

        self._vimba.shutdown()

    def shutdown(self):
        """You must call this function if you are using threaded=true when you are finished
            to prevent segmentation fault"""
        # REQUIRED TO PREVENT SEGMENTATION FAULT FOR THREADED=True
        if (self._camera):
            self._camera.closeCamera()

        self._vimba.shutdown()


    def __init__(self, camera_id = -1, properties = {}, threaded = False):
        if not VIMBA_ENABLED:
            raise Exception("You don't seem to have the pymba library installed.  This will make it hard to use a AVT Vimba Camera.")

        self._vimba = None
        self._setupVimba()
        
        camlist = self.listAllCameras()
        self._camTable = {}
        self._frame = None
        self._buffer = None # Buffer to store images
        self._buffersize = 10 # Number of images to keep in the rolling image buffer for threads
        self._lastimage = None # Last image loaded into memory
        self._thread = None
        self._framerate = 0
        self.threaded = False
        self._properties = {}
        self._camera = None

        i = 0
        for cam in camlist:
            self._camTable[i] = {'id': cam.cameraIdString}
            i += 1

        if not len(camlist):
            raise Exception("Couldn't find any cameras with the Vimba driver.  Use VimbaViewer to confirm you have one connected.")

        if camera_id < 9000: #camera was passed as an index reference
            if camera_id == -1:  #accept -1 for "first camera"
                camera_id = 0

            if (camera_id > len(camlist)):
                raise Exception("Couldn't find camera at index %d." % camera_id)

            cam_guid = camlist[camera_id].cameraIdString
        else:
            raise Exception("Index %d is too large" % camera_id)

        self._camera = self._vimba.getCamera(cam_guid)
        self._camera.openCamera()

        self.uniqueid = cam_guid

        self.setProperty("AcquisitionMode","SingleFrame")
        self.setProperty("TriggerSource","Freerun")

        # TODO: FIX
        if properties.get("mode", "RGB") == 'gray':
            self.setProperty("PixelFormat", "Mono8")
        else:
            fmt = "RGB8Packed" # alternatively use BayerRG8
            self.setProperty("PixelFormat", "BayerRG8")

        #give some compatablity with other cameras
        if properties.get("mode", ""):
            properties.pop("mode")

        if properties.get("height", ""):
            properties["Height"] = properties["height"]
            properties.pop("height")

        if properties.get("width", ""):
            properties["Width"] = properties["width"]
            properties.pop("width")

        for p in properties:
            self.setProperty(p, properties[p])

        if threaded:
          self._thread = VimbaCameraThread(self)
          self._thread.daemon = True
          self._buffer = deque(maxlen=self._buffersize)
          self._thread.start()
          self.threaded = True
          
        self._refreshFrameStats()

    def restart(self):
        """
        This tries to restart the camera thread
        """
        self._thread.stop()
        self._thread = VimbaCameraThread(self)
        self._thread.daemon = True
        self._buffer = deque(maxlen=self._buffersize)
        self._thread.start()

    def listAllCameras(self):
        """
        **SUMMARY**
        List all cameras attached to the host

        **RETURNS**
        List of VimbaCamera objects, otherwise empty list
        VimbaCamera objects are defined in the pymba module
        """
        cameraIds = self._vimba.getCameraIds()
        ar = []
        for cameraId in cameraIds:
            ar.append(self._vimba.getCamera(cameraId))
        return ar

    def runCommand(self,command):
        """
        **SUMMARY**
        Runs a Vimba Command on the camera

        Valid Commands include:
        * AcquisitionAbort
        * AcquisitionStart
        * AcquisitionStop

        **RETURNS**

        0 on success

        **EXAMPLE**
        >>>c = VimbaCamera()
        >>>c.runCommand("TimeStampReset")
        """
        return self._camera.runFeatureCommand(command)

    def getProperty(self, name):
        """
        **SUMMARY**
        This retrieves the value of the Vimba Camera attribute

        There are around 140 properties for the Vimba Camera, so reference the
        Vimba Camera pdf that is provided with
        the SDK for detailed information

        Throws VimbaException if property is not found or not implemented yet.

        **EXAMPLE**
        >>>c = VimbaCamera()
        >>>print c.getProperty("ExposureMode")
        """        
        return self._camera.__getattr__(name)

    #TODO, implement the PvAttrRange* functions
    #def getPropertyRange(self, name)

    def getAllProperties(self):
        """
        **SUMMARY**
        This returns a dict with the name and current value of the
        documented Vimba attributes

        CAVEAT: it addresses each of the properties individually, so
        this may take time to run if there's network latency

        **EXAMPLE**
        >>>c = VimbaCamera(0)
        >>>props = c.getAllProperties()
        >>>print props['ExposureMode']

        """
        from pymba import VimbaException

        # TODO
        ar = {}
        c = self._camera
        cameraFeatureNames = c.getFeatureNames()
        for name in cameraFeatureNames:
            try:
                ar[name] =  c.__getattr__(name)
            except VimbaException:
                # Ignore features not yet implemented
                pass
        return ar
        

    def setProperty(self, name, value, skip_buffer_size_check=False):
        """
        **SUMMARY**
        This sets the value of the Vimba Camera attribute.

        There are around 140 properties for the Vimba Camera, so reference the
        Vimba Camera pdf that is provided with
        the SDK for detailed information

        Throws VimbaException if property not found or not yet implemented

        **Example**
        >>>c = VimbaCamera()
        >>>c.setProperty("ExposureAutoRate", 200)
        >>>c.getImage().show()
        """
        ret = self._camera.__setattr__(name, value)

        #just to be safe, re-cache the camera metadata
        if not skip_buffer_size_check:
            self._refreshFrameStats()

        return ret

    def getImage(self):
        """
        **SUMMARY**
        Extract an Image from the Camera, returning the value.  No matter
        what the image characteristics on the camera, the Image returned
        will be RGB 8 bit depth, if camera is in greyscale mode it will
        be 3 identical channels.

        **EXAMPLE**
        >>>c = VimbaCamera()
        >>>c.getImage().show()
        """

        if self.threaded:
            self._thread.lock.acquire()
            try:
                img = self._buffer.pop()
                self._lastimage = img
            except IndexError:
                img = self._lastimage
            self._thread.lock.release()

        else:
            img = self._captureFrame()

        return img


    def setupASyncMode(self):
        self.setProperty('AcquisitionMode','SingleFrame')
        self.setProperty('TriggerSource','Software')

    def setupSyncMode(self):
        self.setProperty('AcquisitionMode','SingleFrame')
        self.setProperty('TriggerSource','Freerun')

    def _refreshFrameStats(self):
        self.width = self.getProperty("Width")
        self.height = self.getProperty("Height")
        self.pixelformat = self.getProperty("PixelFormat")
        self.imgformat = 'RGB'
        if self.pixelformat == 'Mono8':
            self.imgformat = 'L'

    def _getFrame(self):
        if not self._frame:
            self._frame = self._camera.getFrame()    # creates a frame
            self._frame.announceFrame()

        return self._frame

    def _captureFrame(self, timeout = 5000):
        try:
            c = self._camera
            f = self._getFrame()

            colorSpace = ColorSpace.BGR
            if self.pixelformat == 'Mono8':
                colorSpace = ColorSpace.GRAY

            c.startCapture()
            f.queueFrameCapture()
            c.runFeatureCommand('AcquisitionStart')
            c.runFeatureCommand('AcquisitionStop')
            try:
                f.waitFrameCapture(timeout)
            except Exception, e:
                print "Exception waiting for frame: %s: %s" % (e, traceback.format_exc())
                raise(e)

            imgData = f.getBufferByteData()
            moreUsefulImgData = np.ndarray(buffer = imgData,
                                           dtype = np.uint8,
                                           shape = (f.height, f.width, 1))

            rgb = cv2.cvtColor(moreUsefulImgData, cv2.COLOR_BAYER_RG2RGB)
            c.endCapture()

            return Image(rgb, colorSpace=colorSpace, cv2image=imgData)

        except Exception, e:
            print "Exception acquiring frame: %s: %s" % (e, traceback.format_exc())
            raise(e)


########NEW FILE########
__FILENAME__ = Color
# SimpleCV Color Library
#
# This library is used to modify different color properties of images

#load required libraries
import random
from SimpleCV.base import *
from SimpleCV.ImageClass import *


class Color:
    """
    **SUMMARY**

    Color is a class that stores commonly used colors in a simple
    and easy to remember format, instead of requiring you to remember
    a colors specific RGB value.


    **EXAMPLES**

    To use the color in your code you type:
    Color.RED

    To use Red, for instance if you want to do a line.draw(Color.RED)
    """
    colorlist = []

    #Primary Colors
    BLACK = (0, 0, 0)
    WHITE = (255, 255, 255)

    BLUE = (0, 0, 255)
    YELLOW = (255, 255, 0)
    RED = (255, 0, 0)

    LEGO_BLUE = (0,50,150)
    LEGO_ORANGE = (255,150,40)

    VIOLET = (181, 126, 220)
    ORANGE = (255, 165, 0)
    GREEN = (0, 128, 0)
    GRAY = (128, 128, 128)


    #Extended Colors
    IVORY = (255, 255, 240)
    BEIGE = (245, 245, 220)
    WHEAT = (245, 222, 179)
    TAN = (210, 180, 140)
    KHAKI = (195, 176, 145)
    SILVER = (192, 192, 192)
    CHARCOAL = (70, 70, 70)
    NAVYBLUE = (0, 0, 128)
    ROYALBLUE = (8, 76, 158)
    MEDIUMBLUE = (0, 0, 205)
    AZURE = (0, 127, 255)
    CYAN = (0, 255, 255)
    AQUAMARINE = (127, 255, 212)
    TEAL = (0, 128, 128)
    FORESTGREEN = (34, 139, 34)
    OLIVE = (128, 128, 0)
    LIME = (191, 255, 0)
    GOLD = (255, 215, 0)
    SALMON = (250, 128, 114)
    HOTPINK = (252, 15, 192)
    FUCHSIA = (255, 119, 255)
    PUCE = (204, 136, 153)
    PLUM = (132, 49, 121)
    INDIGO = (75, 0, 130)
    MAROON = (128, 0, 0)
    CRIMSON = (220, 20, 60)
    DEFAULT = (0, 0, 0)
    # These are for the grab cut / findBlobsSmart
    BACKGROUND = (0,0,0)
    MAYBE_BACKGROUND = (64,64,64)
    MAYBE_FOREGROUND =  (192,192,192)
    FOREGROUND = (255,255,255)
    WATERSHED_FG = (255,255,255) # Watershed foreground
    WATERSHED_BG = (128,128,128) # Watershed background
    WATERSHED_UNSURE = (0,0,0) # Watershed either fg or bg color
    colorlist = [
                BLACK,
                WHITE,
                BLUE,
                YELLOW,
                RED,
                VIOLET,
                ORANGE,
                GREEN,
                GRAY,
                IVORY,
                BEIGE,
                WHEAT,
                TAN,
                KHAKI,
                SILVER,
                CHARCOAL,
                NAVYBLUE,
                ROYALBLUE,
                MEDIUMBLUE,
                AZURE,
                CYAN,
                AQUAMARINE,
                TEAL,
                FORESTGREEN,
                OLIVE,
                LIME,
                GOLD,
                SALMON,
                HOTPINK,
                FUCHSIA,
                PUCE,
                PLUM,
                INDIGO,
                MAROON,
                CRIMSON,
                DEFAULT
                ]

    @classmethod
    def getRandom(cls):
        """
        **SUMMARY**

        Returns a random color in tuple format.

        **RETURNS**

        A random color tuple.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> kp = img.findKeypoints()
        >>> for k in kp:
        >>>    k.draw(color=Color.getRandom())
        >>> img.show()

        """
        r = random.randint(1, (len(cls.colorlist) - 1))
        return cls.colorlist[r]

    @classmethod
    def hsv(cls, tuple):
        """
        **SUMMARY**

        Convert any color to HSV, OpenCV style (0-180 for hue)

        **PARAMETERS**

        * *tuple* - an rgb tuple to convert to HSV.

        **RETURNS**

        A color tuple in HSV format.

        **EXAMPLE**

        >>> c = Color.RED
        >>> hsvc = Color.hsv(c)


        """
        hsv_float = colorsys.rgb_to_hsv(*tuple)
        return (hsv_float[0] * 180, hsv_float[1] * 255, hsv_float[2])

    @classmethod
    def getHueFromRGB(cls, tuple):
        """
        **SUMMARY**

        Get corresponding Hue value of the given RGB values

        **PARAMETERS**

        * *tuple* - an rgb tuple to convert to HSV.

        **RETURNS**

        floating value of Hue ranging from 0 to 180

        **EXAMPLE**

        >>> i = Image("lenna")
        >>> hue = Color.getHueFromRGB(i[100,300])

        """
        h_float = colorsys.rgb_to_hsv(*tuple)[0]
        return h_float*180

    @classmethod
    def getHueFromBGR(self,color_tuple):
        """
        **SUMMARY**

        Get corresponding Hue value of the given BGR values

        **PARAMETERS**

        * *tuple* - a BGR tuple to convert to HSV.

        **RETURNS**

        floating value of Hue ranging from 0 to 180

        **EXAMPLE**

        >>> i = Image("lenna")
        >>> color_tuple = tuple(reversed(i[100,300]))
        >>> hue = Color.getHueFromRGB(color_tuple)

        """
        a = color_tuple
        print a
        h_float = colorsys.rgb_to_hsv(*tuple(reversed(color_tuple)))[0]
        return h_float*180

    @classmethod
    def hueToRGB(self, h):
        """
        **SUMMARY**

        Get corresponding RGB values of the given Hue

        **PARAMETERS**

        * *int* - a hue int to convert to RGB

        **RETURNS**

        A color tuple in RGB format.

        **EXAMPLE**

        >>> c = Color.huetoRGB(0)

        """
        h = h/180.0
        r,g,b = colorsys.hsv_to_rgb(h,1,1)
        return (round(255.0*r),round(255.0*g),round(255.0*b))

    @classmethod
    def hueToBGR(self,h):
        """
        **SUMMARY**

        Get corresponding BGR values of the given Hue

        **PARAMETERS**

        * *int* - a hue int to convert to BGR

        **RETURNS**

        A color tuple in BGR format.

        **EXAMPLE**

        >>> c = Color.huetoBGR(0)

        """
        return(tuple(reversed(self.hueToRGB(h))))

    @classmethod
    def getAverageRGB(self,rgb):
        """
        **SUMMARY**

        Get the average of the R,G,B values

        **PARAMETERS**

        * *rgb* - a tuple of RGB values

        **RETURNS**

        Average value of RGB

        **EXAMPLE**

        >>> c = Color.getAverageRGB((22,35,230))

        """
        return int(((rgb[0]+rgb[1]+rgb[2])/3))

    @classmethod
    def getLightness(self,rgb):
        """
        **SUMMARY**

        Calculates the grayscale value of R,G,B according to Lightness Method

        **PARAMETERS**

        * *rgb* - a tuple of RGB values

        **RETURNS**

        Grayscale value according to the Lightness Method

        **EXAMPLE**

        >>> c = Color.getLightness((22,35,230))

        **NOTES**
        
        Lightness Method: value = (max(R,G,B)+min(R,G,B))/2

        """
        return int(((max(rgb)+min(rgb))/2))

    @classmethod
    def getLuminosity(self,rgb):
        """
        **SUMMARY**

        Calculates the grayscale value of R,G,B according to Luminosity Method

        **PARAMETERS**

        * *rgb* - a tuple of RGB values

        **RETURNS**

        Grayscale value according to the Luminosity Method

        **EXAMPLE**

        >>> c = Color.getLuminosity((22,35,230))

        **NOTES**
        
        Luminosity Method: value = 0.21*R + 0.71*G + 0.07*B

        """
        return int((0.21*rgb[0] + 0.71*rgb[1] + 0.07*rgb[2]))

class ColorCurve:
    """
    **SUMMARY**

    ColorCurve is a color spline class for performing color correction.
    It can takeas parameters a SciPy Univariate spline, or an array with at
    least 4 point pairs.  Either of these must map in a 255x255 space.  The curve
    can then be used in the applyRGBCurve, applyHSVCurve, and
    applyInstensityCurve functions.
    
    Note:
    The points should be in strictly increasing order of their first elements
    (X-coordinates)

    **EXAMPLE**

    >>> clr = ColorCurve([[0,0], [100, 120], [180, 230], [255, 255]])
    >>> image.applyIntensityCurve(clr)

    the only property, mCurve is a linear array with 256 elements from 0 to 255
    """
    mCurve = ""

    def __init__(self, curve_vals ):
        inBins = linspace(0, 255, 256)
        if( type(curve_vals) == UnivariateSpline ):
            self.mCurve = curvVals(inBins)
        else:
            curve_vals = np.array(curve_vals)
            aSpline = UnivariateSpline(curve_vals[:, 0], curve_vals[:, 1], s=1)
            #nothing above 255, nothing below 0
            self.mCurve = np.maximum(np.minimum(aSpline(inBins),255),0) 


class ColorMap:
    """
    **SUMMARY**

    A ColorMap takes in a tuple of colors along with the start and end points
    and it lets you map colors with a range of numbers.

    If only one Color is passed second color by default is set to White

    **PARAMETERS**

    * *color* - Tuple of colors which need to be mapped

    * *startmap* * - This is the starting of the range of number with which we map the colors

    * *endmap* * - This is the end of the range of the nmber with which we map the colors

    **EXAMPLE**

    This is useful for color coding elements by an attribute:

    >>> blobs = image.findBlobs()
    >>> cm = ColorMap(color = (Color.RED,Color.YELLOW,Color.BLUE),min(blobs.area()),max(blobs.area()))
    >>>  for b in blobs:
    >>>    b.draw(cm[b.area()])

    """
    color = ()
    endcolor = ()
    startmap = 0
    endmap = 0
    colordistance = 0
    valuerange = 0


    def __init__(self, color, startmap, endmap):
        self.color = np.array(color)
        if self.color.ndim == 1:  # To check if only one color was passed
            color = ((color[0],color[1],color[2]),Color.WHITE)
            self.color = np.array(color)
        self.startmap = float(startmap)
        self.endmap = float(endmap)
        self.valuerange = float(endmap - startmap) #delta
        self.colordistance = self.valuerange / float(len(self.color)-1) #gap between colors

    def __getitem__(self, value):
        if value > self.endmap:
            value = self.endmap
        elif value < self.startmap:
            value = self.startmap
        val = (value - self.startmap)/self.colordistance
        alpha = float(val - int(val))
        index = int(val)
        if index == len(self.color)-1:
            color = tuple(self.color[index])
            return (int(color[0]), int(color[1]), int(color[2]))
        color = tuple(self.color[index] * (1-alpha) + self.color[index+1] * (alpha))
        return (int(color[0]), int(color[1]), int(color[2]))

########NEW FILE########
__FILENAME__ = ColorModel
# SimpleCV Color Model Library
#load required libraries
from SimpleCV.base import *
from SimpleCV.ImageClass import *


class ColorModel:
    """
    **SUMMARY**

    The color model is used to model the color of foreground and background objects
    by using a a training set of images.

    You can create the color model with any number of "training" images, or
    add images to the model with add() and remove().  Then for your data images,
    you can useThresholdImage() to return a segmented picture.

    """
    #TODO: Discretize the colorspace into smaller intervals,eg r=[0-7][8-15] etc
    #TODO: Work in HSV space
    mIsBackground = True
    mData = {}
    mBits = 1

    def __init__(self, data = None, isBackground=True):
        self.mIsBackground = isBackground
        self.mData = {}
        self.mBits = 1

        if data:
            try:
                [ self.add(d) for d in data ]
            except TypeError:
                self.add(data)


    def _makeCanonical(self, data):
        """
        Turn input types in a common form used by the rest of the class -- a
        4-bit shifted list of unique colors
        """
        ret = ''

        #first cast everything to a numpy array
        if(data.__class__.__name__ == 'Image'):
            ret =  data.getNumpy().reshape(-1, 3)
        elif(data.__class__.__name__ == 'cvmat'):
            ret = np.array(data).reshape(-1, 3)
        elif(data.__class__.__name__ == 'list'  ):
            temp = []
            for d in data: #do the bgr conversion
                t = (d[2],d[1],d[0])
                temp.append(t)
            ret = np.array(temp,dtype='uint8')
        elif (data.__class__.__name__=='tuple'):
            ret = np.array((data[2],data[1],data[0]),'uint8')
        elif(data.__class__.__name__=='np.array'):
            ret = data
        else:
            logger.warning("ColorModel: color is not in an accepted format!")
            return None

        rs = np.right_shift(ret, self.mBits)  #right shift 4 bits

        if( len(rs.shape) > 1 ):
            uniques = np.unique(rs.view([('',rs.dtype)]*rs.shape[1])).view(rs.dtype).reshape(-1, 3)
        else:
            uniques = [rs]
        #create a unique set of colors.  I had to look this one up

        #create a dict of encoded strings
        return dict.fromkeys(map(np.ndarray.tostring, uniques), 1)

    def reset(self):
        """
        **SUMMARY**
        Resets the color model. I.e. clears it out the stored values.


        **RETURNS**

        Nothing.

        **EXAMPLE**

        >>> cm = ColorModel()
        >>> cm.add(Image("lenna))
        >>> cm.clear()

        """
        self.mData = {}

    def add(self, data):
        """
        **SUMMARY**

        Add an image, array, or tuple to the color model.

        **PARAMETERS**

        * *data* - An image, array, or tupple of values to the color model.

        **RETURNS**

        Nothings.

        **EXAMPLE**

        >>> cm = ColorModel()
        >>> cm.add(Image("lenna))
        >>> cm.clear()

        """
        self.mData.update(self._makeCanonical(data))

    def remove(self, data):
        """
        **SUMMARY**

        Remove an image, array, or tuple from the model.

        **PARAMETERS**

        * *data* - An image, array, or tupple of value.

        **RETURNS**

        Nothings.

        **EXAMPLE**

        >>> cm = ColorModel()
        >>> cm.add(Image("lenna))
        >>> cm.remove(Color.BLACK)

        """
        self.mData = dict.fromkeys(set(self.mData) ^ set(self._makeCanonical(data)), 1)

    def threshold(self, img):
        """
        **SUMMARY**

        Perform a threshold operation on the given image. This involves iterating
        over the image and comparing each pixel to the model. If the pixel is in the
        model it is set to be either the foreground (white) or background (black) based
        on the setting of mIsBackground.

        **PARAMETERS**

        * *img* - the image to perform the threshold on.

        **RETURNS**

        The thresholded image.

        **EXAMPLE**

        >>> cm = ColorModel()
        >>> cm.add(color.RED)
        >>> cm.add(color.BLUE)
        >>> result = cm.threshold(Image("lenna")
        >>> result.show()

        """
        a = 0
        b = 255
        if( self.mIsBackground == False ):
            a = 255
            b = 0

        rs = np.right_shift(img.getNumpy(), self.mBits).reshape(-1, 3) #bitshift down and reshape to Nx3
        mapped = np.array(map(self.mData.has_key, map(np.ndarray.tostring, rs))) #map to True/False based on the model
        thresh = np.where(mapped, a, b) #replace True and False with fg and bg
        return Image(thresh.reshape(img.width, img.height))

    def contains(self, c):
        """
        **SUMMARY**

        Return true if a particular color is in our color model.

        **PARAMETERS**

        * *c* - A three value color tupple.

        **RETURNS**

        Returns True if the color is in the model, False otherwise.

        **EXAMPLE**

        >>> cm = ColorModel()
        >>> cm.add(Color.RED)
        >>> cm.add(Color.BLUE)
        >>> if( cm.contains(Color.RED) )
        >>>   print "Yo - we gots red y'all."


       """
        #reverse the color, cast to uint8, right shift, convert to string, check dict
        return self.mData.has_key(np.right_shift(np.cast['uint8'](c[::-1]), self.mBits).tostring())

    def setIsForeground(self):
        """
        **SUMMARY**

        Set our model as being foreground imagery. I.e. things in the model are the foreground
        and will be marked as white during the threhsold operation.

        **RETURNS**

        Nothing.

        """
        mIsBackground = False

    def setIsBackground(self):
        """
        **SUMMARY**

        Set our model as being background imagery. I.e. things in the model are the background
        and will be marked as black during the threhsold operation.


        **RETURNS**

        Nothing.

        """
        mIsBackground = True

    def load(self, filename):
        """
        **SUMMARY**

        Load the color model from the specified file.

        **TO DO**

        This should be converted to pickle.

        **PARAMETERS**

        * *filename* - The file name and path to load the data from.

        **RETURNS**

        Nothing.

        **EXAMPLE**

        >>> cm = ColorModel()
        >>> cm.load("myColors.txt")
        >>> cm.add(Color.RED)
        >>> cm.add(Color.BLUE)
        >>> cm.save("mymodel)

        """
        self.mData =  load(open(filename))

    def save(self, filename):
        """
        **SUMMARY**

        Save a color model file.

        **PARAMETERS**

        * *filename* - The file name and path to save the data to.

        **RETURNS**

        Nothing.

        **EXAMPLE**

        >>> cm = ColorModel()
        >>> cm.add(Color.RED)
        >>> cm.add(Color.BLUE)
        >>> cm.save("mymodel.txt")

        **TO DO**

        This should be converted to pickle.

        """
        dump(self.mData, open(filename, "wb"))

########NEW FILE########
__FILENAME__ = DFT
from SimpleCV.base import np, warnings
from SimpleCV.ImageClass import Image

class DFT:
    """
    **SUMMARY**

    The DFT class is the refactored class to crate DFT filters which can
    be used to filter images by applying Digital Fourier Transform. This
    is a factory class to create various DFT filters.

    **PARAMETERS**

    Any of the following parameters can be supplied to create 
    a simple DFT object.

    * *width*        - width of the filter
    * *height*       - height of the filter
    * *channels*     - number of channels of the filter
    * *size*         - size of the filter (width, height)
    * *_numpy*       - numpy array of the filter
    * *_image*       - SimpleCV.Image of the filter
    * *_dia*         - diameter of the filter 
                      (applicable for gaussian, butterworth, notch)
    * *_type*        - Type of the filter 
    * *_order*       - order of the butterworth filter 
    * *_freqpass*    - frequency of the filter (lowpass, highpass, bandpass)
    * *_xCutoffLow*  - Lower horizontal cut off frequency for lowpassfilter
    * *_yCutoffLow*  - Lower vertical cut off frequency for lowpassfilter
    * *_xCutoffHigh* - Upper horizontal cut off frequency for highpassfilter
    * *_yCutoffHigh* - Upper vertical cut off frequency for highassfilter
    


    **EXAMPLE**

    >>> gauss = DFT.createGaussianFilter(dia=40, size=(512,512))
    
    >>> dft = DFT()
    >>> butterworth = dft.createButterworthFilter(dia=300, order=2, size=(300, 300))

    """
    width = 0
    height = 0
    channels = 1
    _numpy = None
    _image = None
    _dia = 0
    _type = ""
    _order = 0
    _freqpass = ""
    _xCutoffLow = 0
    _yCutoffLow = 0
    _xCutoffHigh = 0
    _yCutoffHigh = 0

    def __init__(self, **kwargs):
        for key in kwargs:
            if key == 'width':
                self.width = kwargs[key]
            elif key == 'height':
                self.height = kwargs[key]
            elif key == 'channels':
                self.channels = kwargs[key]
            elif key == 'size':
                self.width, self.height = kwargs[key]
            elif key == 'numpyarray':
                self._numpy = kwargs[key]
            elif key == 'image':
                self._image = kwargs[key]
            elif key == 'dia':
                self._dia = kwargs[key]
            elif key == 'type':
                self._type = kwargs[key]
            elif key == 'order':
                self._order = kwargs[key]
            elif key == 'frequency':
                self._freqpass = kwargs[key]
            elif key == 'xCutoffLow':
                self._xCutoffLow = kwargs[key]
            elif key == 'yCutoffLow':
                self._yCutoffLow = kwargs[key]
            elif key == 'xCutoffHigh':
                self._xCutoffHigh = kwargs[key]
            elif key == 'yCutoffHigh':
                self._yCutoffHigh = kwargs[key]

    def __repr__(self):
        return "<SimpleCV.DFT Object: %s %s filter of size:(%d, %d) and channels: %d>" %(self._type, self._freqpass, self.width, self.height, self.channels)

    def __add__(self, flt):
        if not isinstance(flt, type(self)):
            warnings.warn("Provide SimpleCV.DFT object")
            return None
        if self.size() != flt.size():
            warnings.warn("Both SimpleCV.DFT object must have the same size")
            return None
        flt_numpy = self._numpy + flt._numpy
        flt_image = Image(flt_numpy)
        retVal = DFT(numpyarray=flt_numpy, image=flt_image, size=flt_image.size())
        return retVal

    def __invert__(self, flt):
        return self.invert()

    def _updateParams(self, flt):
        self.channels = flt.channels
        self._dia = flt._dia
        self._type = flt._type
        self._order = flt._order
        self._freqpass = flt._freqpass
        self._xCutoffLow = flt._xCutoffLow
        self._yCutoffLow = flt._yCutoffLow
        self._xCutoffHigh = flt._xCutoffHigh
        self._yCutoffHigh = flt._yCutoffHigh

    def invert(self):
        """
        **SUMMARY**

        Invert the filter. All values will be subtracted from 255.

        **RETURNS**

        Inverted Filter

        **EXAMPLE**

        >>> flt = DFT.createGaussianFilter()
        >>> invertflt = flt.invert()
        """

        flt = self._numpy
        flt = 255 - flt
        img = Image(flt)
        invertedfilter = DFT(numpyarray=flt, image=img,
                             size=self.size(), type=self._type)
        invertedfilter._updateParams(self)
        return invertedfilter

    @classmethod
    def createGaussianFilter(self, dia=400, size=(64, 64), highpass=False):
        """
        **SUMMARY**

        Creates a gaussian filter of given size.

        **PARAMETERS**

        * *dia*       -  int - diameter of Gaussian filter
                      - list - provide a list of three diameters to create
                               a 3 channel filter
        * *size*      - size of the filter (width, height)
        * *highpass*: -  bool 
                         True: highpass filter 
                         False: lowpass filter

        **RETURNS**

        DFT filter.

        **EXAMPLE**

        >>> gauss = DFT.createGaussianfilter(200, (512, 512),
                                            highpass=True)
        >>> gauss = DFT.createGaussianfilter([100, 120, 140], (512, 512),
                                             highpass=False)
        >>> img = Image('lenna')
        >>> gauss.applyFilter(img).show()
        """
        if isinstance(dia, list):
            if len(dia) != 3 and len(dia) != 1:
                warnings.warn("diameter list must be of size 1 or 3")
                return None
            stackedfilter = DFT()
            for d in dia:
                stackedfilter = stackedfilter._stackFilters(self.createGaussianFilter(d, size, highpass))
            image = Image(stackedfilter._numpy)
            retVal = DFT(numpyarray=stackedfilter._numpy, image=image,
                         dia=dia, channels = len(dia), size=size,
                         type="Gaussian", frequency=stackedfilter._freqpass)
            return retVal

        freqpass = "lowpass"
        sz_x, sz_y = size
        x0 = sz_x/2
        y0 = sz_y/2
        X, Y = np.meshgrid(np.arange(sz_x), np.arange(sz_y))
        D = np.sqrt((X-x0)**2+(Y-y0)**2)
        flt = 255*np.exp(-0.5*(D/dia)**2) 
        if highpass:
            flt = 255 - flt
            freqpass = "highpass"
        img = Image(flt)
        retVal = DFT(size=size, numpyarray=flt, image=img, dia=dia,
                     type="Gaussian", frequency=freqpass)
        return retVal

    @classmethod
    def createButterworthFilter(self, dia=400, size=(64, 64), order=2, highpass=False):
        """
        **SUMMARY**

        Creates a butterworth filter of given size and order.

        **PARAMETERS**

        * *dia*       - int - diameter of Gaussian filter
                      - list - provide a list of three diameters to create
                               a 3 channel filter
        * *size*      - size of the filter (width, height)
        * *order*     - order of the filter
        * *highpass*: -  bool 
                         True: highpass filter 
                         False: lowpass filter

        **RETURNS**

        DFT filter.

        **EXAMPLE**

        >>> flt = DFT.createButterworthfilter(100, (512, 512), order=3,
                                             highpass=True)
        >>> flt = DFT.createButterworthfilter([100, 120, 140], (512, 512),
                                             order=3, highpass=False)
        >>> img = Image('lenna')
        >>> flt.applyFilter(img).show()
        """
        if isinstance(dia, list):
            if len(dia) != 3 and len(dia) != 1:
                warnings.warn("diameter list must be of size 1 or 3")
                return None
            stackedfilter = DFT()
            for d in dia:
                stackedfilter = stackedfilter._stackFilters(self.createButterworthFilter(d, size, order, highpass))
            image = Image(stackedfilter._numpy)
            retVal = DFT(numpyarray=stackedfilter._numpy, image=image,
                         dia=dia, channels = len(dia), size=size,
                         type=stackedfilter._type, order=order,
                         frequency=stackedfilter._freqpass)
            return retVal
        freqpass = "lowpass"
        sz_x, sz_y = size
        x0 = sz_x/2
        y0 = sz_y/2
        X, Y = np.meshgrid(np.arange(sz_x), np.arange(sz_y))
        D = np.sqrt((X-x0)**2+(Y-y0)**2)
        flt = 255/(1.0 + (D/dia)**(order*2))
        if highpass:
            frequency = "highpass"
            flt = 255 - flt
        img = Image(flt)
        retVal = DFT(size=size, numpyarray=flt, image=img, dia=dia,
                     type="Butterworth", frequency=freqpass)
        return retVal

    @classmethod    
    def createLowpassFilter(self, xCutoff, yCutoff=None, size=(64, 64)):
        """
        **SUMMARY**

        Creates a lowpass filter of given size and order.

        **PARAMETERS**

        * *xCutoff*       - int - horizontal cut off frequency
                          - list - provide a list of three cut off frequencies
                                   to create a 3 channel filter
        * *yCutoff*       - int - vertical cut off frequency
                          - list - provide a list of three cut off frequencies
                                   to create a 3 channel filter
        * *size*      - size of the filter (width, height)

        **RETURNS**

        DFT filter.

        **EXAMPLE**

        >>> flt = DFT.createLowpassFilter(xCutoff=75, size=(320, 280))

        >>> flt = DFT.createLowpassFilter(xCutoff=[75], size=(320, 280))

        >>> flt = DFT.createLowpassFilter(xCutoff=[75, 100, 120],
                                          size=(320, 280))

        >>> flt = DFT.createLowpassFilter(xCutoff=75, yCutoff=35,
                                          size=(320, 280))

        >>> flt = DFT.createLowpassFilter(xCutoff=[75], yCutoff=[35],
                                          size=(320, 280))

        >>> flt = DFT.createLowpassFilter(xCutoff=[75, 100, 125], yCutoff=35,
                                          size=(320, 280))
        >>> # yCutoff will be [35, 35, 35]

        >>> flt = DFT.createLowpassFilter(xCutoff=[75, 113, 124],
                                          yCutoff=[35, 45, 90],
                                          size=(320, 280))
        
        >>> img = Image('lenna')
        >>> flt.applyFilter(img).show()
        """
        if isinstance(xCutoff, list):
            if len(xCutoff) != 3 and len(xCutoff) != 1:
                warnings.warn("xCutoff list must be of size 3 or 1")
                return None
            if isinstance(yCutoff, list):
                if len(yCutoff) != 3 and len(yCutoff) != 1:
                    warnings.warn("yCutoff list must be of size 3 or 1")
                    return None
                if len(yCutoff) == 1:
                    yCutoff = [yCutoff[0]]*len(xCutoff)
            else:
                yCutoff = [yCutoff]*len(xCutoff)
            stackedfilter = DFT()
            for xfreq, yfreq in zip(xCutoff, yCutoff):
                stackedfilter = stackedfilter._stackFilters(self.createLowpassFilter(xfreq, yfreq, size))
            image = Image(stackedfilter._numpy)
            retVal = DFT(numpyarray=stackedfilter._numpy, image=image,
                         xCutoffLow=xCutoff, yCutoffLow=yCutoff,
                         channels=len(xCutoff), size=size,
                         type=stackedfilter._type, order=self._order,
                         frequency=stackedfilter._freqpass)
            return retVal

        w, h = size
        xCutoff = np.clip(int(xCutoff), 0, w/2)
        if yCutoff is None:
            yCutoff = xCutoff
        yCutoff = np.clip(int(yCutoff), 0, h/2)
        flt = np.zeros((w, h))
        flt[0:xCutoff, 0:yCutoff] = 255
        flt[0:xCutoff, h-yCutoff:h] = 255
        flt[w-xCutoff:w, 0:yCutoff] = 255
        flt[w-xCutoff:w, h-yCutoff:h] = 255
        img = Image(flt)
        lowpassFilter = DFT(size=size, numpyarray=flt, image=img,
                            type="Lowpass", xCutoffLow=xCutoff,
                            yCutoffLow=yCutoff, frequency="lowpass")
        return lowpassFilter

    @classmethod
    def createHighpassFilter(self, xCutoff, yCutoff=None, size=(64, 64)):
        """
        **SUMMARY**

        Creates a highpass filter of given size and order.

        **PARAMETERS**

        * *xCutoff*       - int - horizontal cut off frequency
                          - list - provide a list of three cut off frequencies
                                   to create a 3 channel filter
        * *yCutoff*       - int - vertical cut off frequency
                          - list - provide a list of three cut off frequencies
                                   to create a 3 channel filter
        * *size*      - size of the filter (width, height)

        **RETURNS**

        DFT filter.

        **EXAMPLE**

        >>> flt = DFT.createHighpassFilter(xCutoff=75, size=(320, 280))

        >>> flt = DFT.createHighpassFilter(xCutoff=[75], size=(320, 280))

        >>> flt = DFT.createHighpassFilter(xCutoff=[75, 100, 120],
                                           size=(320, 280))

        >>> flt = DFT.createHighpassFilter(xCutoff=75, yCutoff=35, 
                                           size=(320, 280))

        >>> flt = DFT.createHighpassFilter(xCutoff=[75], yCutoff=[35],
                                           size=(320, 280))

        >>> flt = DFT.createHighpassFilter(xCutoff=[75, 100, 125], yCutoff=35,
                                           size=(320, 280))
        >>> # yCutoff will be [35, 35, 35]

        >>> flt = DFT.createHighpassFilter(xCutoff=[75, 113, 124],
                                           yCutoff=[35, 45, 90],
                                           size=(320, 280))
        
        >>> img = Image('lenna')
        >>> flt.applyFilter(img).show()
        """
        if isinstance(xCutoff, list):
            if len(xCutoff) != 3 and len(xCutoff) != 1:
                warnings.warn("xCutoff list must be of size 3 or 1")
                return None
            if isinstance(yCutoff, list):
                if len(yCutoff) != 3 and len(yCutoff) != 1:
                    warnings.warn("yCutoff list must be of size 3 or 1")
                    return None
                if len(yCutoff) == 1:
                    yCutoff = [yCutoff[0]]*len(xCutoff)
            else:
                yCutoff = [yCutoff]*len(xCutoff)
            stackedfilter = DFT()
            for xfreq, yfreq in zip(xCutoff, yCutoff):
                stackedfilter = stackedfilter._stackFilters(
                                self.createHighpassFilter(xfreq, yfreq, size))
            image = Image(stackedfilter._numpy)
            retVal = DFT(numpyarray=stackedfilter._numpy, image=image,
                         xCutoffHigh=xCutoff, yCutoffHigh=yCutoff,
                         channels=len(xCutoff), size=size,
                         type=stackedfilter._type, order=self._order,
                         frequency=stackedfilter._freqpass)
            return retVal

        lowpass = self.createLowpassFilter(xCutoff, yCutoff, size)
        w, h = lowpass.size()
        flt = lowpass._numpy
        flt = 255 - flt
        img = Image(flt)
        highpassFilter = DFT(size=size, numpyarray=flt, image=img,
                             type="Highpass", xCutoffHigh=xCutoff,
                             yCutoffHigh=yCutoff, frequency="highpass")
        return highpassFilter

    @classmethod
    def createBandpassFilter(self, xCutoffLow, xCutoffHigh, yCutoffLow=None,
                             yCutoffHigh=None, size=(64, 64)):
        """
        **SUMMARY**

        Creates a banf filter of given size and order.

        **PARAMETERS**

        * *xCutoffLow*    - int - horizontal lower cut off frequency
                          - list - provide a list of three cut off frequencies
        * *xCutoffHigh*   - int - horizontal higher cut off frequency
                          - list - provide a list of three cut off frequencies
        * *yCutoffLow*    - int - vertical lower cut off frequency
                          - list - provide a list of three cut off frequencies
        * *yCutoffHigh*   - int - verical higher cut off frequency
                          - list - provide a list of three cut off frequencies
                                   to create a 3 channel filter
        * *size*      - size of the filter (width, height)

        **RETURNS**

        DFT filter.

        **EXAMPLE**

        >>> flt = DFT.createBandpassFilter(xCutoffLow=75,
                                           xCutoffHigh=190, size=(320, 280))

        >>> flt = DFT.createBandpassFilter(xCutoffLow=[75],
                                           xCutoffHigh=[190], size=(320, 280))

        >>> flt = DFT.createBandpassFilter(xCutoffLow=[75, 120, 132],
                                           xCutoffHigh=[190, 210, 234],
                                           size=(320, 280))

        >>> flt = DFT.createBandpassFilter(xCutoffLow=75, xCutoffHigh=190,
                                           yCutoffLow=60, yCutoffHigh=210,
                                           size=(320, 280))

        >>> flt = DFT.createBandpassFilter(xCutoffLow=[75], xCutoffHigh=[190],
                                           yCutoffLow=[60], yCutoffHigh=[210],
                                           size=(320, 280))

        >>> flt = DFT.createBandpassFilter(xCutoffLow=[75, 120, 132],
                                           xCutoffHigh=[190, 210, 234], 
                                           yCutoffLow=[70, 110, 112], 
                                           yCutoffHigh=[180, 220, 220], 
                                           size=(320, 280))
        
        >>> img = Image('lenna')
        >>> flt.applyFilter(img).show()
        """
        lowpass = self.createLowpassFilter(xCutoffLow, yCutoffLow, size)
        highpass = self.createHighpassFilter(xCutoffHigh, yCutoffHigh, size)
        lowpassnumpy = lowpass._numpy
        highpassnumpy = highpass._numpy
        bandpassnumpy = lowpassnumpy + highpassnumpy
        bandpassnumpy = np.clip(bandpassnumpy, 0, 255)
        img = Image(bandpassnumpy)
        bandpassFilter = DFT(size=size, image=img,
                             numpyarray=bandpassnumpy, type="bandpass",
                             xCutoffLow=xCutoffLow, yCutoffLow=yCutoffLow,
                             xCutoffHigh=xCutoffHigh, yCutoffHigh=yCutoffHigh,
                             frequency="bandpass", channels=lowpass.channels)
        return bandpassFilter

    @classmethod
    def createNotchFilter(self, dia1, dia2=None, cen=None, size=(64, 64), type="lowpass"):
        """
        **SUMMARY**

        Creates a disk shaped notch filter of given diameter at given center.

        **PARAMETERS**

        * *dia1*       -  int - diameter of the disk shaped notch
                       - list - provide a list of three diameters to create
                               a 3 channel filter
        * *dia2*       -  int - outer diameter of the disk shaped notch
                                used for bandpass filter
                       - list - provide a list of three diameters to create
                               a 3 channel filter
        * *cen*        - tuple (x, y) center of the disk shaped notch
                         if not provided, it will be at the center of the 
                         filter
        * *size*       - size of the filter (width, height)
        * *type*:      - lowpass or highpass filter

        **RETURNS**
        DFT notch filter

        **EXAMPLE**

        >>> notch = DFT.createNotchFilter(dia1=200, cen=(200, 200),
                                          size=(512, 512), type="highpass")
        >>> notch = DFT.createNotchFilter(dia1=200, dia2=300, cen=(200, 200),
                                          size=(512, 512))
        >>> img = Image('lenna')
        >>> notch.applyFilter(img).show()
        """
        if isinstance(dia1, list):
            if len(dia1) != 3 and len(dia1) != 1:
                warnings.warn("diameter list must be of size 1 or 3")
                return None

            if isinstance(dia2, list):
                if len(dia2) != 3 and len(dia2) != 1:
                    warnings.warn("diameter list must be of size 3 or 1")
                    return None
                if len(dia2) == 1:
                    dia2 = [dia2[0]]*len(dia1)
            else:
                dia2 = [dia2]*len(dia1)

            if isinstance(cen, list):
                if len(cen) != 3 and len(cen) != 1:
                    warnings.warn("center list must be of size 3 or 1")
                    return None
                if len(cen) == 1:
                    cen = [cen[0]]*len(dia1)
            else:
                cen = [cen]*len(dia1)

            stackedfilter = DFT()
            for d1, d2, c in zip(dia1, dia2, cen):
                stackedfilter = stackedfilter._stackFilters(self.createNotchFilter(d1, d2, c, size, type))
            image = Image(stackedfilter._numpy)
            retVal = DFT(numpyarray=stackedfilter._numpy, image=image,
                         dia=dia1+dia2, channels = len(dia1), size=size,
                         type=stackedfilter._type,
                         frequency=stackedfilter._freqpass)
            return retVal

        w, h = size
        if cen is None:
            cen = (w/2, h/2)
        a, b = cen
        y, x = np.ogrid[-a:w-a, -b:h-b]
        r = dia1/2
        mask = x*x + y*y <= r*r
        flt = np.ones((w, h))
        flt[mask] = 255
        if type == "highpass":
            flt = 255-flt
        if dia2 is not None:
            a, b = cen
            y, x = np.ogrid[-a:w-a, -b:h-b]
            r = dia2/2
            mask = x*x + y*y <= r*r
            flt1 = np.ones((w, h))
            flt1[mask] = 255
            flt1 = 255 - flt1
            flt = flt + flt1
            np.clip(flt, 0, 255)
            type = "bandpass"
        img = Image(flt)
        notchfilter = DFT(size=size, numpyarray=flt, image=img, dia=dia1,
                          type="Notch", frequency=type)
        return notchfilter

    def applyFilter(self, image, grayscale=False):
        """
        **SUMMARY**

        Apply the DFT filter to given image.

        **PARAMETERS**

        * *image*     - SimpleCV.Image image
        * *grayscale* - if this value is True we perfrom the operation on the 
                        DFT of the gray version of the image and the result is
                        gray image. If grayscale is true we perform the 
                        operation on each channel and the recombine them to
                        create the result.

        **RETURNS**

        Filtered Image.

        **EXAMPLE**

        >>> notch = DFT.createNotchFilter(dia1=200, cen=(200, 200),
                                          size=(512, 512), type="highpass")
        >>> img = Image('lenna')
        >>> notch.applyFilter(img).show()
        """

        if self.width == 0 or self.height == 0:
            warnings.warn("Empty Filter. Returning the image.")
            return image
        w, h = image.size()
        if grayscale:
            image = image.toGray()
        fltImg = self._image
        if fltImg.size() != image.size():
            fltImg = fltImg.resize(w, h)
        filteredImage = image.applyDFTFilter(fltImg)
        return filteredImage

    def getImage(self):
        """
        **SUMMARY**

        Get the SimpleCV Image of the filter

        **RETURNS**

        Image of the filter.

        **EXAMPLE**

        >>> notch = DFT.createNotchFilter(dia1=200, cen=(200, 200),
                                          size=(512, 512), type="highpass")
        >>> notch.getImage().show()
        """
        if isinstance(self._image, type(None)):
            if isinstance(self._numpy, type(None)):
                warnings.warn("Filter doesn't contain any image")
            self._image = Image(self._numpy)
        return self._image

    def getNumpy(self):
        """
        **SUMMARY**

        Get the numpy array of the filter

        **RETURNS**

        numpy array of the filter.

        **EXAMPLE**

        >>> notch = DFT.createNotchFilter(dia1=200, cen=(200, 200),
                                          size=(512, 512), type="highpass")
        >>> notch.getNumpy()
        """
        if isinstance(self._numpy, type(None)):
            if isinstance(self._image, type(None)):
                warnings.warn("Filter doesn't contain any image")
            self._numpy = self._image.getNumpy()
        return self._numpy

    def getOrder(self):
        """
        **SUMMARY**

        Get order of the butterworth filter

        **RETURNS**

        order of the butterworth filter

        **EXAMPLE**

        >>> flt = DFT.createButterworthFilter(order=4)
        >>> print flt.getOrder()
        """
        return self._order

    def size(self):
        """
        **SUMMARY**

        Get size of the filter

        **RETURNS**

        tuple of (width, height)

        **EXAMPLE**

        >>> flt = DFT.createGaussianFilter(size=(380, 240))
        >>> print flt.size()
        """
        return (self.width, self.height)

    def getDia(self):
        """
        **SUMMARY**

        Get diameter of the filter

        **RETURNS**

        diameter of the filter

        **EXAMPLE**

        >>> flt = DFT.createGaussianFilter(dia=200, size=(380, 240))
        >>> print flt.getDia()
        """
        return self._dia

    def getType(self):
        """
        **SUMMARY**

        Get type of the filter

        **RETURNS**

        type of the filter

        **EXAMPLE**

        >>> flt = DFT.createGaussianFilter(dia=200, size=(380, 240))
        >>> print flt.getType() # Gaussian
        """
        return self._type

    def stackFilters(self, flt1, flt2):
        """
        **SUMMARY**

        Stack three signle channel filters of the same size to create
        a 3 channel filter.

        **PARAMETERS**

        * *flt1* - second filter to be stacked
        * *flt2* - thrid filter to be stacked

        **RETURNS**

        DFT filter

        **EXAMPLE**

        >>> flt1 = DFT.createGaussianFilter(dia=200, size=(380, 240))
        >>> flt2 = DFT.createGaussianFilter(dia=100, size=(380, 240))
        >>> flt2 = DFT.createGaussianFilter(dia=70, size=(380, 240))
        >>> flt = flt1.stackFilters(flt2, flt3) # 3 channel filter
        """
        if not(self.channels == 1 and flt1.channels == 1 and flt2.channels == 1):
            warnings.warn("Filters must have only 1 channel")
            return None
        if not (self.size() == flt1.size() and self.size() == flt2.size()):
            warnings.warn("All the filters must be of same size")
            return None
        numpyflt = self._numpy
        numpyflt1 = flt1._numpy
        numpyflt2 = flt2._numpy
        flt = np.dstack((numpyflt, numpyflt1, numpyflt2))
        img = Image(flt)
        stackedfilter = DFT(size=self.size(), numpyarray=flt, image=img, channels=3)
        return stackedfilter

    def _stackFilters(self, flt1):
        """
        **SUMMARY**

        stack two filters of same size. channels don't matter.

        **PARAMETERS**

        * *flt1* - second filter to be stacked

        **RETURNS**

        DFT filter

        """
        if isinstance(self._numpy, type(None)):
            return flt1
        if not self.size() == flt1.size():
            warnings.warn("All the filters must be of same size")
            return None
        numpyflt = self._numpy
        numpyflt1 = flt1._numpy
        flt = np.dstack((numpyflt, numpyflt1))
        stackedfilter = DFT(size=self.size(), numpyarray=flt,
                            channels=self.channels+flt1.channels,
                            type=self._type, frequency=self._freqpass)
        return stackedfilter

########NEW FILE########
__FILENAME__ = Display
from SimpleCV.base import *
import SimpleCV.ImageClass
import Queue
from base import *


PYGAME_INITIALIZED = False

class Display:
    """
    **SUMMARY**

    WindowStream opens a window (Pygame Display Surface) to which you can write
    images.  The default resolution is 640, 480 -- but you can also specify 0,0
    which will maximize the display.  Flags are pygame constants, including:


    By default display will attempt to scale the input image to fit neatly on the
    screen with minimal distorition. This means that if the aspect ratio matches
    the screen it will scale cleanly. If your image does not match the screen aspect
    ratio we will scale it to fit nicely while maintining its natural aspect ratio.

    Because SimpleCV performs this scaling there are two sets of input mouse coordinates,
    the (mousex,mousey) which scale to the image, and (mouseRawX, mouseRawY) which
    do are the actual screen coordinates.

    * pygame.FULLSCREEN: create a fullscreen display.
    * pygame.DOUBLEBUF: recommended for HWSURFACE or OPENGL.
    * pygame.HWSURFACE: hardware accelerated, only in FULLSCREEN.
    * pygame.OPENGL: create an opengl renderable display.
    * pygame.RESIZABLE: display window should be sizeable.
    * pygame.NOFRAME: display window will have no border or controls.

    Display should be used in a while loop with the isDone() method, which
    checks events and sets the following internal state controls:

    * mouseX: the x position of the mouse cursor on the input image.
    * mouseY: the y position of the mouse curson on the input image.
    * mouseRawX: The position of the mouse on the screen.
    * mouseRawY: The position of the mouse on the screen.

    **NOTES**

    The mouse position on the screen is not the mouse position on the image. If you
    are trying to draw on the image or take in coordinates use mousex and mousey
    as these values are scaled along with the image.

    * mouseLeft: the state of the left button.
    * mouseRight: the state of the right button.
    * mouseMiddle: the state of the middle button.
    * mouseWheelUp: scroll wheel has been moved up.
    * mouseWheelDown: the wheel has been clicked towards the bottom of the mouse.

    **EXAMPLE**

    >>> display = Display(resolution = (800, 600)) #create a new display to draw images on
    >>> cam = Camera() #initialize the camera
    >>> done = False # setup boolean to stop the program
    >>> while not display.isDone():
    >>>  cam.getImage().flipHorizontal().save(display) # get image, flip it so it looks mirrored, save to display
    >>>  time.sleep(0.01) # Let the program sleep for 1 millisecond so the computer can do other things
    >>>  if display.mouseLeft:
    >>>      display.done = True

    """

    resolution = ''
    sourceresolution = ''
    sourceoffset = ''
    screen = ''
    eventhandler = ''
    mq = ''
    done = False
    mouseX = 0 # These are the scaled mouse values. If you want to do image manipulation use these.
    mouseY = 0
    mouseRawX = 0 # Raw x and y are the actual position on the screen
    mouseRawY = 0 # versus the position on the image.
    mouseLeft = 0
    mouseMiddle = 0
    mouseRight = 0
    mouseWheelUp = 0
    mouseWheelDown = 0
    xscale = 1.0
    yscale = 1.0
    xoffset = 0
    yoffset = 0
    imgw = 0
    imgh = 0
    lastLeftButton = 0
    lastRightButton = 0
    leftButtonDown = None
    leftButtonUp = None
    rightButtonDown = None
    rightButtonUp = None
    displaytype = None
    pressed=[]

    def __repr__(self):
        return "<SimpleCV.Display Object resolution:(%s), Image Resolution: (%d, %d) at memory location: (%s)>" % (self.resolution, self.imgw, self.imgh, hex(id(self)))

    def __init__(self, resolution = (640, 480), flags = 0, title = "SimpleCV", displaytype='standard', headless = False):
        """
        **SUMMARY**

        This is the generic display object.  You are able to set the display type.
        The standard display type will pop up a window
        The notebook display type is to be used in conjunction with IPython Notebooks
        this is so it is web based.  If you have IPython Notebooks installed
        you just need to start IPython Notebooks and open in your browser.

        **PARAMETERS**

        * *resolution* - the size of the diplay in pixels.
        * *flags* - ???
        * *title* - the title bar on the display.
        * *displaytype* - The type of display. Options are as follows:

          * 'standard' - A pygame window.
          * 'notebook' - Ipython Web Notebook output

        * *headless* - If False we ignore healess mode. If true all rendering is suspended.

        **EXAMPLE**

        Once in IPython you can do the following:

        >>> from SimpleCV import *
        >>> disp = Display(displaytype='notebook')
        >>> img = Image('simplecv')
        >>> img.save(disp)

        """
        global PYGAME_INITIALIZED

        if headless:
            os.environ["SDL_VIDEODRIVER"] = "dummy"

        if not PYGAME_INITIALIZED:
            if not displaytype == 'notebook':
                pg.init()
            PYGAME_INITIALIZED = True
        self.xscale = 1.0
        self.yscale = 1.0
        self.xoffset = 0
        self.yoffset = 0
        self.lastLeftButton = 0
        self.lastRightButton = 0
        self.leftButtonDown = None
        self.leftButtonUp = None
        self.rightButtonDown = None
        self.rightButtonUp = None
        self.pressed = None
        self.displaytype = displaytype 
        # NOTE: NO PYGAME CALLS SHOULD BE MADE IN INIT AS THEY KILLL
        # THE DISPLAY IN IPYTHON NOTEBOOKS       
        self.mouseRawX = 0 # Raw x and y are the actual position on the screen
        self.mouseRawY = 0 # versus the position on the image.
        self.resolution = resolution
        if not displaytype == 'notebook':
            self.screen = pg.display.set_mode(resolution, flags)        
        if os.path.isfile(os.path.join(LAUNCH_PATH, 'sampleimages','simplecv.png')): #checks if simplecv.png exists
            scvLogo = SimpleCV.Image("simplecv").scale(32,32)
            pg.display.set_icon(scvLogo.getPGSurface())
        if flags != pg.FULLSCREEN and flags != pg.NOFRAME:
            pg.display.set_caption(title)

    def leftButtonUpPosition(self):
        """
        **SUMMARY**

        Returns the position where the left mouse button went up.

        .. warning::
          You must call :py:meth:`checkEvents` or :py:meth:`isDone`
          in your main display loop for this method to work.

        **RETURNS**

        An (x,y) mouse postion tuple where the mouse went up.

        **EXAMPLE**

        >>> disp = Display((600,800))
        >>> cam = Camera()
        >>> while(disp.isNotDone()):
        >>>   img = cam.getImage()
        >>>   dwn = disp.leftButtonDownPosition()
        >>>   up = disp.leftButtonUpPosition()
        >>>   if( up is not None and dwn is not None):
        >>>     bb = disp.pointsToBoundingBox(up,dwn)
        >>>     img.drawRectangle(bb[0],bb[1],bb[2],bb[3])
        >>>   img.save(disp)

        **SEE ALSO**

        :py:meth:`rightButtonUpPostion`
        :py:meth:`leftButtonDownPostion`
        :py:meth:`rightButtonDownPostion`
        :py:meth:`pointsToBoundingBox`


        """
        return self.leftButtonUp

    def leftButtonDownPosition(self):
        """
        **SUMMARY**

        Returns the position where the left mouse button went down.

        .. warning::
          You must call :py:meth:`checkEvents` or :py:meth:`isDone`
          in your main display loop for this method to work.


        **RETURNS**

        An (x,y) mouse postion tuple where the mouse went up.

        **EXAMPLE**

        >>> disp = Display((600,800))
        >>> cam = Camera()
        >>> while(disp.isNotDone()):
        >>>   img = cam.getImage()
        >>>   dwn = disp.leftButtonDownPosition()
        >>>   up = disp.leftButtonUpPosition()
        >>>   if( up is not None and dwn is not None):
        >>>     bb = disp.pointsToBoundingBox(up,dwn)
        >>>     img.drawRectangle(bb[0],bb[1],bb[2],bb[3])
        >>>   img.save(disp)

        **SEE ALSO**

        :py:meth:`leftButtonUpPostion`
        :py:meth:`rightButtonUpPostion`
        :py:meth:`rightButtonDownPostion`
        :py:meth:`pointsToBoundingBox`
        :py:meth:`checkEvents`


        """

        return self.leftButtonDown

    def rightButtonUpPosition(self):
        """
        **SUMMARY**

        Returns the position where the right mouse button went up.

        .. warning::
          You must call :py:meth:`checkEvents` or :py:meth:`isDone`
          in your main display loop for this method to work.


        **RETURNS**

        An (x,y) mouse postion tuple where the mouse went up.

        **EXAMPLE**

        >>> disp = Display((600,800))
        >>> cam = Camera()
        >>> while(disp.isNotDone()):
        >>>   img = cam.getImage()
        >>>   dwn = disp.rightButtonDownPosition()
        >>>   up = disp.rightButtonUpPosition()
        >>>   if( up is not None and dwn is not None):
        >>>     bb = disp.pointsToBoundingBox(up,dwn)
        >>>     img.drawRectangle(bb[0],bb[1],bb[2],bb[3])
        >>>   img.save(disp)


        >>> disp = Display((600,800))
        >>> dwn = disp.rightButtonDownPosition()
        >>> up = disp.rightButtonUpPosition()
        >>> bb = disp.pointsToBoundingBox(up,dwn)
        >>> #draw bb

        **SEE ALSO**

        :py:meth:`leftButtonUpPostion`
        :py:meth:`leftButtonDownPostion`
        :py:meth:`rightButtonDownPostion`
        :py:meth:`pointsToBoundingBox`
        :py:meth:`checkEvents`
        """
        return self.rightButtonUp

    def rightButtonDownPosition(self):
        """
        **SUMMARY**

        Returns the position where the right mouse button went down.

        .. warning::
          You must call :py:meth:`checkEvents` or :py:meth:`isDone`
          in your main display loop for this method to work.

        **RETURNS**

        An (x,y) mouse postion tuple where the mopuse went down.

        **EXAMPLE**

        >>> disp = Display((600,800))
        >>> cam = Camera()
        >>> while(disp.isNotDone()):
        >>>   img = cam.getImage()
        >>>   dwn = disp.rightButtonDownPosition()
        >>>   up = disp.rightButtonUpPosition()
        >>>   if( up is not None and dwn is not None):
        >>>     bb = disp.pointsToBoundingBox(up,dwn)
        >>>     img.drawRectangle(bb[0],bb[1],bb[2],bb[3])
        >>>   img.save(disp)

        **SEE ALSO**

        :py:meth:`leftButtonUpPostion`
        :py:meth:`leftButtonDownPostion`
        :py:meth:`rightButtonDownPostion`
        :py:meth:`pointsToBoundingBox`
        :py:meth:`checkEvents`
        """

        return self.rightButtonDown

    def pointsToBoundingBox(self, pt0, pt1):
        """
        **SUMMARY**

        Given two screen cooridnates return the bounding box in x,y,w,h format.
        This is helpful for drawing regions on the display.

        **RETURNS**

        The bounding box from two coordinates as a ( x,y,w,h) tuple.

        **EXAMPLE**

        >>> disp = Display((600,800))
        >>> cam = Camera()
        >>> while(disp.isNotDone()):
        >>>   img = cam.getImage()
        >>>   dwn = disp.leftButtonDownPosition()
        >>>   up = disp.leftButtonUpPosition()
        >>>   if( up is not None and dwn is not None):
        >>>     bb = disp.pointsToBoundingBox(up,dwn)
        >>>     img.drawRectangle(bb[0],bb[1],bb[2],bb[3])
        >>>   img.save(disp)


        **SEE ALSO**

        :py:meth:`leftButtonUpPostion`
        :py:meth:`leftButtonDownPostion`
        :py:meth:`rightButtonDownPostion`
        :py:meth:`rightButtonUpPostion`
        :py:meth:`checkEvents`
        """
        xmax = np.max((pt0[0],pt1[0]))
        xmin = np.min((pt0[0],pt1[0]))
        ymax = np.max((pt0[1],pt1[1]))
        ymin = np.min((pt0[1],pt1[1]))
        return xmin,ymin,xmax-xmin,ymax-ymin

    def writeFrame(self, img, fit=True):
        """
        **SUMMARY**

        writeFrame copies the given Image object to the display, you can also use
        Image.save()

        Write frame trys to fit the image to the display with the minimum ammount
        of distortion possible. When fit=True write frame will decide how to scale
        the image such that the aspect ratio is maintained and the smallest amount
        of distorition possible is completed. This means the axis that has the minimum
        scaling needed will be shrunk or enlarged to match the display.


        **PARAMETERS**

        * *img* -  the SimpleCV image to save to the display.
        * *fit* - When fit=False write frame will crop and center the image as best it can.
          If the image is too big it is cropped and centered. If it is too small
          it is centered. If it is too big along one axis that axis is cropped and
          the other axis is centered if necessary.


        **RETURNS**

        Nothing.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> disp = Display((512,512))
        >>> disp.writeFrame(img)


        """
        # Grrrrr we're going to need to re-write this functionality
        # So if the image is the right size do nothing
        # if the image has a 'nice' scale factor we should scale it e.g. 800x600=>640x480
        # if( fit )
        #   if one axis is too big -> scale it down to fit
        #   if both axes are too big and they don't match eg 800x800 img and 640x480 screen => scale to 400x400 and center
        #   if both axis are too small -> scale the biggest to fill
        #   if one axis is too small and one axis is alright we center along the too small axis
        # else(!fit)
        #   if one / both axis is too big - crop it
        #   if one / both too small - center along axis
        #
        # this is getting a little long. Probably needs to be refactored.
        wndwAR = float(self.resolution[0])/float(self.resolution[1])
        imgAR = float(img.width)/float(img.height)
        self.sourceresolution = img.size()
        self.sourceoffset = (0,0)
        self.imgw = img.width
        self.imgh = img.height
        self.xscale = 1.0
        self.yscale = 1.0
        self.xoffset = 0
        self.yoffset = 0
        if( img.size() == self.resolution): # we have to resize
            s = img.getPGSurface()
            self.screen.blit(s, s.get_rect())
            pg.display.flip()
        elif( imgAR == wndwAR ):
            self.xscale = (float(img.width)/float(self.resolution[0]))
            self.yscale = (float(img.height)/float(self.resolution[1]))
            img = img.scale(self.resolution[0], self.resolution[1])
            s = img.getPGSurface()
            self.screen.blit(s, s.get_rect())
            pg.display.flip()
        elif(fit):
            #scale factors
            wscale = (float(img.width)/float(self.resolution[0]))
            hscale = (float(img.height)/float(self.resolution[1]))
            targetw = img.width
            targeth = img.height
            if(wscale>1): #we're shrinking what is the percent reduction
                wscale=1-(1.0/wscale)
            else: # we need to grow the image by a percentage
                wscale = 1.0-wscale

            if(hscale>1):
                hscale=1-(1.0/hscale)
            else:
                hscale=1.0-hscale

            if( wscale == 0 ): #if we can get away with not scaling do that
                targetx = 0
                targety = (self.resolution[1]-img.height)/2
                targetw = img.width
                targeth = img.height
                s = img.getPGSurface()
            elif( hscale == 0 ): #if we can get away with not scaling do that
                targetx = (self.resolution[0]-img.width)/2
                targety = 0
                targetw = img.width
                targeth = img.height
                s = img.getPGSurface()
            elif(wscale < hscale): # the width has less distortion
                sfactor = float(self.resolution[0])/float(img.width)
                targetw = int(float(img.width)*sfactor)
                targeth = int(float(img.height)*sfactor)
                if( targetw > self.resolution[0] or targeth > self.resolution[1]):
                    #aw shucks that still didn't work do the other way instead
                    sfactor = float(self.resolution[1])/float(img.height)
                    targetw = int(float(img.width)*sfactor)
                    targeth = int(float(img.height)*sfactor)
                    targetx = (self.resolution[0]-targetw)/2
                    targety = 0
                else:
                    targetx = 0
                    targety = (self.resolution[1]-targeth)/2
                img = img.scale(targetw,targeth)
                s = img.getPGSurface()
            else: #the height has more distortion
                sfactor = float(self.resolution[1])/float(img.height)
                targetw = int(float(img.width)*sfactor)
                targeth = int(float(img.height)*sfactor)
                if( targetw > self.resolution[0] or targeth > self.resolution[1]):
                    #aw shucks that still didn't work do the other way instead
                    sfactor = float(self.resolution[0])/float(img.width)
                    targetw = int(float(img.width)*sfactor)
                    targeth = int(float(img.height)*sfactor)
                    targetx = 0
                    targety = (self.resolution[1]-targeth)/2
                else:
                    targetx = (self.resolution[0]-targetw)/2
                    targety = 0
                img = img.scale(targetw,targeth)
                s = img.getPGSurface()
            #clear out the screen so everything is clean
            black = pg.Surface((self.resolution[0], self.resolution[1]))
            black.fill((0,0,0))
            self.screen.blit(black,black.get_rect())
            self.screen.blit(s,(targetx,targety))
            self.sourceoffset = (targetx, targety)
            pg.display.flip()
            self.xoffset = targetx
            self.yoffset = targety
            self.xscale = (float(self.imgw)/float(targetw))
            self.yscale = (float(self.imgh)/float(targeth))
        else: # we're going to crop instead
            self.doClamp = False
            targetx = 0
            targety = 0
            cornerx = 0
            cornery = 0
            if(img.width <= self.resolution[0] and img.height <= self.resolution[1] ): # center a too small image
                #we're too small just center the thing
                targetx = (self.resolution[0]/2)-(img.width/2)
                targety = (self.resolution[1]/2)-(img.height/2)
                cornerx = targetx
                cornery = targety
                s = img.getPGSurface()
            elif(img.width > self.resolution[0] and img.height > self.resolution[1]): #crop too big on both axes
                targetw = self.resolution[0]
                targeth = self.resolution[1]
                targetx = 0
                targety = 0
                x = (img.width-self.resolution[0])/2
                y = (img.height-self.resolution[1])/2
                cornerx = -1*x
                cornery = -1*y
                img = img.crop(x,y,targetw,targeth)
                s = img.getPGSurface()
            elif( img.width < self.resolution[0] and img.height >= self.resolution[1]): #height too big
                #crop along the y dimension and center along the x dimension
                targetw = img.width
                targeth = self.resolution[1]
                targetx = (self.resolution[0]-img.width)/2
                targety = 0
                x = 0
                y = (img.height-self.resolution[1])/2
                cornerx = targetx
                cornery = -1 * y
                img = img.crop(x,y,targetw,targeth)
                s = img.getPGSurface()
            elif( img.width > self.resolution[0] and img.height <= self.resolution[1]): #width too big
                #crop along the y dimension and center along the x dimension
                targetw = self.resolution[0]
                targeth = img.height
                targetx = 0
                targety = (self.resolution[1]-img.height)/2
                x = (img.width-self.resolution[0])/2
                y = 0
                cornerx = -1 * x
                cornery = targety
                img = img.crop(x,y,targetw,targeth)
                s = img.getPGSurface()
            self.xoffset = cornerx
            self.yoffset = cornery
            black = pg.Surface((self.resolution[0], self.resolution[1]))
            black.fill((0,0,0))
            self.screen.blit(black,black.get_rect())
            self.screen.blit(s,(targetx,targety))
            pg.display.flip()



    def _setButtonState(self, state, button):
        if button == 1:
            self.mouseLeft = state
        if button == 2:
            self.mouseMiddle = state
        if button == 3:
            self.mouseRight = state
        if button == 4:
            self.mouseWheelUp = 1
        if button == 5:
            self.mouseWheelDown = 1

    def checkEvents(self,returnStrings=False):
        """
        
        **SUMMARY**

        CheckEvents checks the pygame event queue and sets the internal display
        values based on any new generated events.

        .. warning::
          This method must be called (or :py:meth:`isDone` or :py:meth:`isNotDone`) to
          perform mouse event checking.

        **PARAMETERS**

        returnStrings - pygame returns an enumerated int by default, when this is set to true we return a list of strings.
        
        **RETURNS**

        A list of key down events. Parse them with pg.K_<lowercase_letter>

        """
        self.mouseWheelUp = self.mouseWheelDown = 0
        self.lastLeftButton = self.mouseLeft
        self.lastRightButton = self.mouseRight
        self.leftButtonDown = None
        self.leftButtonUp = None
        self.rightButtonDown = None
        self.rightButtonUp = None
        key = []
        for event in pg.event.get():
            if event.type == pg.QUIT:
                pg.quit()
                self.done = True
            if event.type == pg.MOUSEMOTION:
                self.mouseRawX = event.pos[0]
                self.mouseRawY = event.pos[1]
                x = int((event.pos[0]-self.xoffset)*self.xscale)
                y = int((event.pos[1]-self.yoffset)*self.yscale)
                (self.mouseX,self.mouseY) = self._clamp(x,y)
                self.mouseLeft, self.mouseMiddle, self.mouseRight = event.buttons
            if event.type == pg.MOUSEBUTTONUP:

                self._setButtonState(0, event.button)

            if event.type == pg.MOUSEBUTTONDOWN:
                self._setButtonState(1, event.button)
            if event.type == pg.KEYDOWN:
                if(returnStrings):
                    key.append(pg.key.name(event.key))
                else:
                    key.append(event.key)

        self.pressed = pg.key.get_pressed()

        if( self.lastLeftButton == 0 and self.mouseLeft == 1 ):
            self.leftButtonDown = (self.mouseX,self.mouseY)
        if( self.lastLeftButton == 1 and self.mouseLeft == 0 ):
            self.leftButtonUp = (self.mouseX,self.mouseY)

        if( self.lastRightButton == 0 and self.mouseRight == 1 ):
            self.rightButtonDown = (self.mouseX,self.mouseY)
        if( self.lastRightButton == 1 and self.mouseRight == 0 ):
            self.rightButtonUp = (self.mouseX,self.mouseY)

        #If ESC pressed, end the display
        if(self.pressed[pg.K_ESCAPE] == 1):
            self.done = True

        return key

    def isDone(self):
        """
        **SUMMARY**

        Checks the event queue and returns True if a quit event has been issued.

        **RETURNS**

        True on a quit event, False otherwise.

        **EXAMPLE**

        >>> disp = Display()
        >>> cam = Camera()
        >>> while not disp.isDone():
        >>>   img = cam.getImage()
        >>>   img.save(disp)

        """
        self.checkEvents()
        return self.done

    def isNotDone(self):
        """
        **SUMMARY**

        Checks the event queue and returns False as long as the quit event hasn't been issued.

        **RETURNS**

        False on a quit event, True otherwise.

        **EXAMPLE**

        >>> disp = Display()
        >>> cam = Camera()
        >>> while disp.isNotDone():
        >>>   img = cam.getImage()
        >>>   img.save(disp)

        """
        return not self.isDone()

    def _clamp(self,x,y):
        """
        clamp all values between zero and the image width
        """
        rx = x
        ry = y
        if(x > self.imgw):
            rx = self.imgw
        if(x < 0 ):
            rx = 0

        if(y > self.imgh):
            ry = self.imgh
        if(y < 0 ):
            ry = 0
        return (rx,ry)

    def quit(self):
        """
        quit the pygame instance

        Example:
        >>> img = Image("simplecv")
        >>> d = img.show()
        >>> time.sleep(5)
        >>> d.quit()
        """
        pg.display.quit()
        pg.quit()

########NEW FILE########
__FILENAME__ = DrawingLayer
#!/usr/bin/env python

import sys
import os
import svgwrite
#from SimpleCV.base import *
from SimpleCV.Color import *



#DOCS
#TESTS
#IMAGE AGNOSTIC
#RESIZE
#ADD IMAGE INTERFACE

class DrawingLayer:
    """
    DrawingLayer gives you a way to mark up Image classes without changing
    the image data itself. This class wraps pygame's Surface class and
    provides basic drawing and text rendering functions


    Example:
    image = Image("/path/to/image.png")
    image2 = Image("/path/to/image2.png")
    image.dl().blit(image2) #write image 2 on top of image
    """
    _mSurface = []
    _mDefaultColor = 0
    _mFontColor = 0
    _mClearColor = 0
    _mFont = 0
    _mFontName = ""
    _mFontSize = 0
    _mDefaultAlpha = 255
    _mAlphaDelta = 1 #This is used to track the changed value in alpha
    _mSVG = ""
    width = 0
    height = 0

    def __init__(self, (width, height)):
        #pg.init()
        if( not pg.font.get_init() ):
            pg.font.init()

        self._mSVG = svgwrite.Drawing(size=(width, height))

        self.width = width
        self.height = height
        self._mSurface = pg.Surface((width, height), flags = pg.SRCALPHA)
        self._mDefaultAlpha = 255
        self._mClearColor = pg.Color(0, 0, 0, 0)

        self._mSurface.fill(self._mClearColor)
        self._mDefaultColor = Color.BLACK

        self._mFontSize = 18
        self._mFontName = None
        self._mFontBold = False
        self._mFontItalic = False
        self._mFontUnderline = False
        self._mFont = pg.font.Font(self._mFontName, self._mFontSize)

    def __repr__(self):
        return "<SimpleCV.DrawingLayer Object size (%d, %d)>" % (self.width, self.height)


    def setDefaultAlpha(self, alpha):
        """
        This method sets the default alpha value for all methods called on this
        layer. The default value starts out at 255 which is completely transparent.
        """
        if(alpha >= 0 and alpha <= 255 ):
            self._mDefaultAlpha = alpha
        return None

    def getDefaultAlpha(self):
        """
        Returns the default alpha value.
        """
        return self._mDefaultAlpha
    def setLayerAlpha(self, alpha):
        """
        This method sets the alpha value of the entire layer in a single
        pass. This is helpful for merging layers with transparency.
        """

        self._mSurface.set_alpha(alpha)
        # Get access to the alpha band of the image.
        pixels_alpha = pg.surfarray.pixels_alpha(self._mSurface)
        # Do a floating point multiply, by alpha 100, on each alpha value.
        # Then truncate the values (convert to integer) and copy back into the surface.
        pixels_alpha[...] = (np.ones(pixels_alpha.shape)*(alpha)).astype(np.uint8)

        # Unlock the surface.

        self._mAlphaDelta = alpha / 255.0 #update the changed state

        del pixels_alpha
        return None

    def getSVG(self):
        return(self._mSVG.tostring())

    def _getSurface(self):
        return(self._mSurface)

    def _csvRGB2pgColor(self, color, alpha = -1):
        if(alpha == -1):
            alpha = self._mDefaultAlpha

        if(color == Color.DEFAULT):
            color = self._mDefaultColor
        retVal = pg.Color(color[0], color[1], color[2], alpha)
        return retVal

    def setDefaultColor(self, color):
        """
        This method sets the default rendering color.

        Parameters:
            color - Color object or Color Tuple
        """
        self._mDefaultColor = color

    def line(self, start, stop, color = Color.DEFAULT, width = 1, antialias = True, alpha = -1 ):
        """
        Draw a single line from the (x,y) tuple start to the (x,y) tuple stop.
        Optional parameters:

        color - The object's color as a simple CVColor object, if no value  is sepcified
                the default is used.

        alpha - The alpha blending for the object. If this value is -1 then the
                layer default value is used. A value of 255 means opaque, while 0
                means transparent.

        width - The line width in pixels.

        antialias - Draw an antialiased object of width one.

        Parameters:
            start - Tuple
            stop - Tuple
            color - Color object or Color Tuple
            width - Int
            antialias - Boolean
            alpha - Int

        """
        if(antialias and width == 1):
            pg.draw.aaline(self._mSurface, self._csvRGB2pgColor(color, alpha), start, stop, width)
        else:
            pg.draw.line(self._mSurface, self._csvRGB2pgColor(color, alpha), start, stop, width)

        startInt = tuple(int(x) for x in start)
        stopInt = tuple(int(x) for x in stop)
        self._mSVG.add(self._mSVG.line(start=startInt, end=stopInt))

        return None

    def lines(self, points, color = Color.DEFAULT, antialias = True, alpha = -1, width = 1 ):
        """
        Draw a set of lines from the list of (x,y) tuples points. Lines are draw
        between each successive pair of points.

        Optional parameters:

        color - The object's color as a simple CVColor object, if no value  is sepcified
                the default is used.

        alpha - The alpha blending for the object. If this value is -1 then the
                layer default value is used. A value of 255 means opaque, while 0
                means transparent.

        width - The line width in pixels.

        antialias - Draw an antialiased object of width one.

        Parameters:
            points - Tuple
            color - Color object or Color Tuple
            antialias - Boolean
            alpha - Int
            width - Int

        """
        if(antialias and width == 1):
            pg.draw.aalines(self._mSurface, self._csvRGB2pgColor(color, alpha), 0, points, width)
        else:
            pg.draw.lines(self._mSurface, self._csvRGB2pgColor(color, alpha), 0, points, width)



        lastPoint = points[0]
        for point in points[1:]:
            lInt = tuple(int(x) for x in lastPoint)
            pInt = tuple(int(x) for x in point)
            self._mSVG.add(self._mSVG.line(start=lastPoint, end=point))
            lastPoint = point

        return None

    #need two points(TR,BL), center+W+H, and TR+W+H
    def rectangle(self, topLeft, dimensions, color = Color.DEFAULT, width = 1, filled = False, alpha = -1 ):
        """
        Draw a rectangle given the topLeft the (x,y) coordinate of the top left
        corner and dimensions (w,h) tge width and height

        color - The object's color as a simple CVColor object, if no value  is sepcified
                the default is used.

        alpha - The alpha blending for the object. If this value is -1 then the
                layer default value is used. A value of 255 means opaque, while 0
                means transparent.

        w -     The line width in pixels. This does not work if antialiasing is enabled.

        filled -The rectangle is filled in
        """
        if(filled):
            width = 0
        r = pg.Rect((topLeft[0], topLeft[1]), (dimensions[0], dimensions[1]))
        pg.draw.rect(self._mSurface, self._csvRGB2pgColor(color, alpha), r, width)

        tlInt = tuple(int(x) for x in topLeft)
        dimInt = tuple(int(x) for x in dimensions)
        self._mSVG.add(self._mSVG.rect(insert=tlInt, size=dimInt))

        return None

    def rectangle2pts(self, pt0, pt1, color = Color.DEFAULT, width = 1, filled = False, alpha = -1 ):
        """
        Draw a rectangle given two (x,y) points

        color - The object's color as a simple CVColor object, if no value  is sepcified
                the default is used.

        alpha - The alpha blending for the object. If this value is -1 then the
                layer default value is used. A value of 255 means opaque, while 0
                means transparent.

        w -     The line width in pixels. This does not work if antialiasing is enabled.

        filled -The rectangle is filled in
        """
        w = 0
        h = 0
        x = 0
        y = 0
        if(pt0[0] > pt1[0]):
            w = pt0[0]-pt1[0]
            x = pt1[0]
        else:
            w = pt1[0]-pt0[0]
            x = pt0[0]
        if(pt0[1] > pt1[1]):
            h = pt0[1]-pt1[1]
            y = pt1[1]
        else:
            h = pt1[1]-pt0[1]
            y = pt0[1]
        if(filled):
            width = 0
        r = pg.Rect((x,y),(w,h))
        pg.draw.rect(self._mSurface, self._csvRGB2pgColor(color, alpha), r, width)

        self._mSVG.add(self._mSVG.rect(insert=(int(x),int(y)), size=(int(w),int(h))))

        return None

    def centeredRectangle(self, center, dimensions, color = Color.DEFAULT, width = 1, filled = False, alpha = -1 ):
        """
        Draw a rectangle given the center (x,y) of the rectangle and dimensions (width, height)

        color - The object's color as a simple CVColor object, if no value  is sepcified
                the default is used.

        alpha - The alpha blending for the object. If this value is -1 then the
                layer default value is used. A value of 255 means opaque, while 0
                means transparent.

        w -     The line width in pixels. This does not work if antialiasing is enabled.

        filled -The rectangle is filled in


        parameters:
            center - Tuple
            dimenions - Tuple
            color - Color object or Color Tuple
            width - Int
            filled - Boolean
            alpha - Int

        """
        if(filled):
            width = 0
        xtl = center[0] - (dimensions[0] / 2)
        ytl = center[1] - (dimensions[1] / 2)
        r = pg.Rect(xtl, ytl, dimensions[0], dimensions[1])
        pg.draw.rect(self._mSurface, self._csvRGB2pgColor(color, alpha), r, width)

        dimInt = tuple(int(x) for x in dimensions)
        self._mSVG.add(self._mSVG.rect(insert=(int(xtl), int(ytl)), size=dimInt))

        return None


    def polygon(self, points, color = Color.DEFAULT, width = 1, filled = False, antialias = True, alpha = -1):
        """
        Draw a polygon from a list of (x,y)

        color - The object's color as a simple CVColor object, if no value  is sepcified
                the default is used.

        alpha - The alpha blending for the object. If this value is -1 then the
                layer default value is used. A value of 255 means opaque, while 0
                means transparent.

        width - The
        width in pixels. This does not work if antialiasing is enabled.

        filled -The object is filled in

        antialias - Draw the edges of the object antialiased. Note this does not work when the object is filled.
        """
        if(filled):
            width = 0
        if(not filled):
            if(antialias and width == 1):
                pg.draw.aalines(self._mSurface, self._csvRGB2pgColor(color, alpha), True, points, width)
            else:
                pg.draw.lines(self._mSurface, self._csvRGB2pgColor(color, alpha), True, points, width)
        else:
            pg.draw.polygon(self._mSurface, self._csvRGB2pgColor(color, alpha), points, width)
        return None

    def circle(self, center, radius, color = Color.DEFAULT, width = 1, filled = False, alpha = -1, antialias = True):
        """
        Draw a circle given a location and a radius.

        color - The object's color as a simple CVColor object, if no value  is sepcified
                the default is used.

        alpha - The alpha blending for the object. If this value is -1 then the
                layer default value is used. A value of 255 means opaque, while 0
                means transparent.

        width - The line width in pixels. This does not work if antialiasing is enabled.

        filled -The object is filled in

        Parameters:
            center - Tuple
            radius - Int
            color - Color object or Color Tuple
            width - Int
            filled - Boolean
            alpha - Int
            antialias - Int
        """
        if(filled):
            width = 0
        if antialias == False or width > 1 or filled:
            pg.draw.circle(self._mSurface, self._csvRGB2pgColor(color, alpha), center, int(radius), int(width))
        else:
            pg.gfxdraw.aacircle(self._mSurface, int(center[0]), int(center[1]), int(radius), self._csvRGB2pgColor(color, alpha))

        cenInt = tuple(int(x) for x in center)
        self._mSVG.add(self._mSVG.circle(center=cenInt, r=radius))

        return None

    def ellipse(self, center, dimensions, color = Color.DEFAULT, width = 1, filled = False, alpha = -1):
        """
        Draw an ellipse given a location and a dimensions.

        color - The object's color as a simple CVColor object, if no value  is sepcified
                the default is used.

        alpha - The alpha blending for the object. If this value is -1 then the
                layer default value is used. A value of 255 means opaque, while 0
                means transparent.

        width - The line width in pixels. This does not work if antialiasing is enabled.

        filled -The object is filled in

        Parameters:
            center - Tuple
            dimensions - Tuple
            color - Color object or Color tuple
            width - Int
            filled - Boolean
            alpha - Int
        """
        if(filled):
            width = 0
        r = pg.Rect(center[0] - (dimensions[0] / 2), center[1] - (dimensions[1] / 2), dimensions[0], dimensions[1])
        pg.draw.ellipse(self._mSurface, self._csvRGB2pgColor(color, alpha), r, width)

        cenInt = tuple(int(x) for x in center)
        dimInt = tuple(int(x) for x in dimensions)
        self._mSVG.add(self._mSVG.ellipse(center=centInt, r=dimInt))

        return None

    def bezier(self, points, steps, color = Color.DEFAULT, alpha = -1):
        """
        Draw a bezier curve based on a control point and the a number of stapes

        color - The object's color as a simple CVColor object, if no value  is sepcified
                the default is used.

        alpha - The alpha blending for the object. If this value is -1 then the
                layer default value is used. A value of 255 means opaque, while 0
                means transparent

        Parameters:
            points - list
            steps - Int
            color - Color object or Color Tuple
            alpha - Int


        """
        pg.gfxdraw.bezier(self._mSurface, points, steps, self._csvRGB2pgColor(color, alpha))
        return None

    def setFontBold(self, doBold):
        """
        This method sets and unsets the current font to be bold.
        """
        self._mFontBold = doBold
        self._mFont.set_bold(doBold)
        return None

    def setFontItalic(self, doItalic):
        """
        This method sets and unsets the current font to be italic.
        """
        self._mFontItalic = doItalic
        self._mFont.set_italic(doItalic)
        return None

    def setFontUnderline(self, doUnderline):
        """
        This method sets and unsets the current font to be underlined
        """
        self._mFontUnderline = doUnderline
        self._mFont.set_underline(doUnderline)
        return None

    def selectFont(self, fontName):
        """
        This method attempts to set the font from a font file. It is advisable
        to use one of the fonts listed by the listFonts() method. The input
        is a string with the font name.
        """
        fullName = pg.font.match_font(fontName)
        self._mFontName = fullName
        self._mFont = pg.font.Font(self._mFontName, self._mFontSize)
        return None

    def listFonts(self):
        """
        This method returns a list of strings corresponding to the fonts available
        on the current system.
        """
        return pg.font.get_fonts()


    def setFontSize(self, sz):
        """
        This method sets the font size roughly in points. A size of 10 is almost
        too small to read. A size of 20 is roughly 10 pixels high and a good choice.

        Parameters:
            sz = Int
        """
        self._mFontSize = sz
        self._mFont = pg.font.Font(self._mFontName, self._mFontSize)
        return None

    def text(self, text, location, color = Color.DEFAULT, alpha = -1):
        """
        Write the a text string at a given location

        text -  A text string to print.

        location-The location to place the top right corner of the text

        color - The object's color as a simple CVColor object, if no value  is sepcified
                the default is used.

        alpha - The alpha blending for the object. If this value is -1 then the
                layer default value is used. A value of 255 means opaque, while 0
                means transparent.

        Parameters:
            text - String
            location - Tuple
            color - Color object or Color tuple
            alpha - Int

        """
        if(len(text)<0):
            return None
        tsurface = self._mFont.render(text, True, self._csvRGB2pgColor(color, alpha))
        if(alpha == -1):
            alpha = self._mDefaultAlpha
        #this is going to be slow, dumb no native support.
        #see http://www.mail-archive.com/pygame-users@seul.org/msg04323.html
        # Get access to the alpha band of the image.
        pixels_alpha = pg.surfarray.pixels_alpha(tsurface)
        # Do a floating point multiply, by alpha 100, on each alpha value.
        # Then truncate the values (convert to integer) and copy back into the surface.
        pixels_alpha[...] = (pixels_alpha * (alpha / 255.0)).astype(np.uint8)
        # Unlock the surface.
        del pixels_alpha
        self._mSurface.blit(tsurface, location)

        fontStyle = "font-size: {}px;".format(self._mFontSize - 7) # Adjust for web
        if self._mFontBold:
            fontStyle += "font-weight: bold;"
        if self._mFontItalic:
            fontStyle += "font-style: italic;"
        if self._mFontUnderline:
            fontStyle += "text-decoration: underline;"
        if self._mFontName:
            fontStyle += "font-family: \"{}\";".format(self._mFontName)
        alteredLocation = (location[0], location[1] + self.textDimensions(text)[1])
        altInt = tuple(int(x) for x in alteredLocation)
        self._mSVG.add(self._mSVG.text(text, insert=altInt, style=fontStyle))
        return None

    def textDimensions(self, text):
        """
        The textDimensions function takes a string and returns the dimensions (width, height)
        of this text being rendered on the screen.
        """
        tsurface = self._mFont.render(text, True, self._csvRGB2pgColor(Color.WHITE, 255))
        return (tsurface.get_width(), tsurface.get_height())

    def ezViewText(self, text, location, fgcolor = Color.WHITE, bgcolor = Color.BLACK):
        """
        ezViewText works just like text but it sets both the foreground and background
        color and overwrites the image pixels. Use this method to make easily
        viewable text on a dynamic video stream.

        fgcolor - The color of the text.

        bgcolor - The background color for the text are.
        """
        if(len(text)<0):
            return None
        alpha = 255
        tsurface = self._mFont.render(text, True, self._csvRGB2pgColor(fgcolor, alpha), self._csvRGB2pgColor(bgcolor, alpha))
        self._mSurface.blit(tsurface, location)
        return None

    def sprite(self,img,pos=(0,0),scale=1.0,rot=0.0,alpha=255):
        """
        sprite draws a sprite (a second small image) onto the current layer.
        The sprite can be loaded directly from a supported image file like a
        gif, jpg, bmp, or png, or loaded as a surface or SCV image.

        pos - the (x,y) position of the upper left hand corner of the sprite

        scale - a scale multiplier as a float value. E.g. 1.1 makes the sprite 10% bigger

        rot = a rotation angle in degrees

        alpha = an alpha value 255=opaque 0=transparent.
        """

        if( not pg.display.get_init() ):
            pg.display.init()

        if(img.__class__.__name__=='str'):
            image = pg.image.load(img, "RGB")
        elif(img.__class__.__name__=='Image' ):
            image = img.getPGSurface()
        else:
            image = img # we assume we have a surface
        image = image.convert(self._mSurface)
        if(rot != 0.00):
            image = pg.transform.rotate(image,rot)
        if(scale != 1.0):
            image = pg.transform.scale(image,(int(image.get_width()*scale),int(image.get_height()*scale)))
        pixels_alpha = pg.surfarray.pixels_alpha(image)
        pixels_alpha[...] = (pixels_alpha * (alpha / 255.0)).astype(np.uint8)
        del pixels_alpha
        self._mSurface.blit(image,pos)


    def blit(self, img, coordinates = (0,0)):
        """
        Blit one image onto the drawing layer at upper left coordinates

        Parameters:
            img - Image
            coordinates - Tuple

        """

        #can we set a color mode so we can do a little bit of masking here?
        self._mSurface.blit(img.getPGSurface(), coordinates)

    def replaceOverlay(self, overlay):
        """
        This method allows you to set the surface manually.

        Parameters:
            overlay - Pygame Surface
        """
        self._mSurface = overlay
        return None

    #get rid of all drawing
    def clear(self):
        """
        This method removes all of the drawing on this layer (i.e. the layer is
        erased completely)
        """
        self._mSurface = pg.Surface((int(self.width), int(self.height)),flags = pg.SRCALPHA)
        return None

    def renderToSurface(self, surf):
        """
        Blit this layer to another surface.

        Parameters:
            surf - Pygame Surface
        """
        surf.blit(self._mSurface, (0, 0))
        return(surf)


    def renderToOtherLayer(self, otherLayer):
        """
        Add this layer to another layer.

        Parameters:
            otherLayer - Pygame Surface
        """
        otherLayer._mSurface.blit(self._mSurface, (0, 0))

########NEW FILE########
__FILENAME__ = CannyStream-arduino
#!/usr/bin/env python
# 
# Released under the BSD license. See LICENSE file for details.
"""
This is a simple demo that interfaces with the Arduino. To use it you have to
have the standard Firmata firmware running on the Arduino itself.
This should be built into the Arduino IDE but if you need help:
http://firmata.org

You will also need pyfirmata (https://github.com/tino/pyFirmata) installed:
pip install pyfirmata

Once they are installed you also have to verify the USB location of the Arduino.
On linux it is typically: /dev/ttyUSBO or /dev/ttyACM0

but the Arduino IDE should tell you where you should mount the Arduino from.
"""

print __doc__

import time
from SimpleCV import Camera
from pyfirmata import Arduino, util


board = Arduino('/dev/ttyUSB0')  # The location of the Arduino
analog_pin_1 = board.get_pin('a:1:i')  # Use pin 1 as input
analog_pin_2 = board.get_pin('a:2:i')  # Use pin 2 as input
button_13 = board.get_pin('d:13:i')  # Use pin 13 for button input

it = util.Iterator(board)  # Initalize the pin monitor for the Arduino
it.start()  # Start the pin monitor loop

multiplier = 400.0  # A value to adjust the edge threshold by
cam = Camera()  # Initalize the camera

while True:
    t1 = analog_pin_1.read()  # Read the value from pin 1
    t2 = analog_pin_2.read()  # Read the value from pin 2
    b13 = button_13.read()  # Read if the button has been pressed.

    if not t1:  # Set a default if no value read
        t1 = 50 
    else:
        t1 *= multiplier 

    if not t2:  # Set a default if no value read
        t2 = 100
    else:
        t2 *= multiplier

    print "t1 " + str(t1) + ", t2 " + str(t2) + ", b13 " + str(b13)
    img = cam.getImage().flipHorizontal()
    edged_img = img.edges(int(t1), int(t2)).invert().smooth()
    edged_img.show()
    time.sleep(0.1)

########NEW FILE########
__FILENAME__ = balltrack
'''
This is how to track a white ball example using SimpleCV

The parameters may need to be adjusted to match the RGB color
of your object.

The demo video can be found at:

'''
print __doc__

import SimpleCV

display = SimpleCV.Display() #create the display to show the image
cam = SimpleCV.Camera() # initalize the camera
normaldisplay = True # mode toggle for segment detection and display

while display.isNotDone(): # loop until we tell the program to stop

    if display.mouseRight: # if right mouse clicked, change mode
        normaldisplay = not(normaldisplay)
        print "Display Mode:", "Normal" if normaldisplay else "Segmented"

    img = cam.getImage().flipHorizontal() # grab image from camera
    dist = img.colorDistance(SimpleCV.Color.BLACK).dilate(2) # try to separate colors in image
    segmented = dist.stretch(200,255) # really try to push out white colors
    blobs = segmented.findBlobs() # search the image for blob objects
    if blobs: # if blobs are found
        circles = blobs.filter([b.isCircle(0.2) for b in blobs]) # filter out only circle shaped blobs
        if circles:
            img.drawCircle((circles[-1].x, circles[-1].y), circles[-1].radius(),SimpleCV.Color.BLUE,3) # draw the circle on the main image

    if normaldisplay: # if normal display mode
        img.show()
    else: # segmented mode
        segmented.show()

########NEW FILE########
__FILENAME__ = barcode_reader
#!/usr/bin/python
'''
This program is a standard barcode reader.
To use it you need to have the following library installed:
http://zbar.sourceforge.net

To install on Ubuntu Linux 12.04 or higher:
sudo apt-get install python-zbar


Then line up the item in the red box and left click the mouse to tell
the program to try and read the barcode
'''

print __doc__


import time
import csv
from SimpleCV import Color, ColorCurve, Camera, Image, pg, np, cv
from SimpleCV.Display import Display

cam = Camera()
display = Display((800,600))
data = "None"
mydict = dict()
myfile = "barcode-list.csv"

while display.isNotDone():
    display.checkEvents()#check for mouse clicks
    img = cam.getImage()
    img.drawRectangle(img.width/4,img.height/4,img.width/2,img.height/2,color=Color.RED,width=3)
    if display.mouseLeft: # click the mouse to read
        img.drawText("reading barcode... wait",10,10)
        img.save(display)
        barcode = img.findBarcode()
        if barcode: # if we have a barcode
            data = str(barcode.data)
            print data
            if mydict.has_key(data):
                mydict[data] = mydict[data] + 1
            else:
                mydict[data] = 1
    img.drawText("Click to scan.",10,10,color=Color.RED)
    myItem = "Last item scanned: " + data
    img.drawText(myItem,10,30)
    img.save(display) #display

#write to a CSV file.
target= open( myfile, "wb" )
wtr= csv.writer( target )
wtr.writerow( ["item","count"])
for d in mydict.items():
    wtr.writerow(d)
target.close()

########NEW FILE########
__FILENAME__ = CannyCam
#!/usr/bin/python
'''
This example just takes an image, finds the edges, and draws them
the threshold is used for the edge detection, if you adjust the
max_threshold and threshhold_step values and run the program you will
see it change over time
'''
print __doc__

from SimpleCV import *

cam = Camera() #initialize the camera
max_threshold = 300 # this is used for the edge detection
threshold_step = 0.5 # this is the amount to adjust the threshold by each time the display is updated
threshold = max_threshold

while True:
    image = cam.getImage() # get image (or frame) from camera
    flipped_image = image.flipHorizontal() # flip it so it looks mirrored
    edged_image = flipped_image.edges(threshold) # get the image edges

    # This just automatically cycles through threshold levels
    if(threshold <= 0):
        threshold = max_threshold
    else:
        threshold = threshold - 0.5

    edged_image.drawText("Current Edge Threshold:" + str(threshold), 10,10, fontsize=30, color=Color.GREEN)
    edged_image.show()

########NEW FILE########
__FILENAME__ = CoinDetector
'''
This example finds a quarter in the picture and then uses that measurement
to determine the rest of the coins in the picture.  Since a quarter is always
a certain size we can use it as a reference because it is known.

In this example we use millimeters to pixels to do the conversion.

The sizes of coins are as follows:
penny - 19.05 mm
nickel - 21.21 mm
dime - 17.9 mm
quarter - 24.26 mm

'''

print __doc__

from SimpleCV import *
# A quarter is 24.26mm or 0.955in
quarterSize = 24.26 #millimeters

# This will hold the ratio of the image size to a quarter's actual size
sizeRatio = 0
objectSize = 0

img = Image('coins.jpg', sample=True)
segmented = img.hueDistance(Color.BLACK)
coins = img.invert().findBlobs(minsize=200)

#Here we compute the scale factor
if coins:
    c = coins[-1]
    diameter = c.radius() * 2
    sizeRatio = quarterSize / diameter

#Now we print the measurements back on the picture
for coin in coins:
    #get the physical size of the coin
    size = (coin.radius() * 2) * sizeRatio
    #label the coin accordingly
    if size > 18 and size < 20:
        text = "Type: penny"
    elif size > 20 and size < 23:
        text = "Type: nickel"
    elif size > 16 and size < 18:
        text = "Type: dime"
    elif size > 23 and size < 26:
        text = "Type: quarter"
    else:
        text = "unknown"

    text = text + ", Size:" + str(size) + "mm"
    img.drawText(text, coin.x, coin.y)

img.show()
time.sleep(10)

########NEW FILE########
__FILENAME__ = ColorSegmentation
import time
from SimpleCV import *
from SimpleCV.Display import Display, pg
from SimpleCV.Segmentation import ColorSegmentation
segmentation = ColorSegmentation()
cam = Camera()
SegmentMode = False
x0 = 0
y0 = 0
x1 = 0
y1 = 0
img = cam.getImage()
display = Display((img.width,img.height))
mouse_down = False
while not display.isDone():
    img = cam.getImage()
    img = img.scale(160,120)
    dl = DrawingLayer((img.width,img.height))
    mystring = "( " +str(display.mouseX)+" , "+str(display.mouseY)+" )"
    #print((display.mouseX,display.mouseY))
    if SegmentMode:
        segmentation.addImage(img)
        if(segmentation.isReady()):
            img = segmentation.getSegmentedImage()
            img = img.erode(iterations = 2).dilate().invert()
            img.dl().ezViewText(mystring, (30,30))
            img = img.applyLayers()
            display.writeFrame(img)
        if(display.mouseLeft):
            SegmentMode = False
            segmentation.reset()
            display.mouseLeft = False
    else:
        if(display.mouseLeft and not mouse_down):
            #print( (display.mouseX,display.mouseY))
            x0 = display.mouseX
            y0 = display.mouseY
            mouse_down = True
        elif( display.mouseLeft and mouse_down ):
            dl.circle((x0,y0),radius=10,color=Color.RED)
            dl.rectangle2pts( (display.mouseX,display.mouseY),(x0,y0),color=Color.RED)
            dl.circle((display.mouseX,display.mouseY),radius=10,color=Color.RED)
        elif(mouse_down):
            x = min(x0,display.mouseX)
            y = min(y0,display.mouseY)
            ww = max(x0,display.mouseX)-x
            hh = max(y0,display.mouseY)-y
            if( ww > 0 and hh > 0):
                crop = img.crop(x,y,ww,hh)
                segmentation.addToModel(crop)
                SegmentMode = True
                mouse_down = False
        img.addDrawingLayer(dl)
        img.dl().ezViewText(mystring, (30,30))
        #img.save(display)
        img = img.applyLayers();
        display.writeFrame(img)

    time.sleep(0.001)

########NEW FILE########
__FILENAME__ = dealwithit
#!/usr/bin/python

import time
from SimpleCV import *

def check_eyes(eyes):
    return (eyes and len(eyes) >= 2)

def process_eyes(image, eyes):
    dx, dy = eyes[-1].coordinates() - eyes[-2].coordinates()

    if dx > 0:
        right_eye = eyes[-2]
    else:
        dx = -1*dx
        right_eye = eyes[-1]

    if dx > image.width/15: #Reduces amount of wrong matches
        return (dx, dy, right_eye)
    else:
        return (None, None, None)


def draw_glasses(image, (dx, dy, right_eye), glasses):
    rotation = 0.5*dy
    try:
        new_glasses = glasses.scale(int(2.75*dx), right_eye.height())
        mask = new_glasses.invert()

        new_glasses = new_glasses.rotate(rotation, fixed = False)
        mask = mask.rotate(rotation, fixed=False)

        image = image.blit(new_glasses, right_eye.topLeftCorner(),alphaMask=mask)
    except:
        pass

    return image.flipHorizontal()

def main():

    glasses = Image('deal_with_it.png', sample=True).flipHorizontal()
    c = Camera()
    found = False


    while True:

        image = c.getImage().scale(0.5).flipHorizontal()
        eyes = image.findHaarFeatures("eye")

        if check_eyes(eyes):
            new_position = process_eyes(image, eyes)
            if new_position[0]:
                found = True
                position = new_position

        if found:
            image = draw_glasses(image, position, glasses)
        else:
            image = image.flipHorizontal()

        image.show()

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = EdgeSnap
'''
This example illustrates the edgeSnap function in the Image class

Left-click to define points for snapping ( Blue )
Right-click to start the process, Detected Edge points will be shown in Red

'''
print __doc__

from SimpleCV import *



image = Image("shapes.png",sample = True)
edgeMap = image.edges()

display = Display((image.width,image.height))

edgeMap.drawText("Left Click to choose points, Right click to find Edge Points", 10,10,color=Color.BLACK,fontsize=20)


points = []
image.save(display)

while not display.isDone():

    time.sleep(0.01)
    left = display.leftButtonDownPosition()
    right = display.rightButtonDownPosition()


    if(left != None ):
        image.drawCircle((left[0],left[1]),5,Color.BLUE,-1)
        image.save(display)
        points += [left]

    if(right != None ):
        
        featureSet = edgeMap.edgeSnap(points)
        featureSet.image = image
        if(featureSet):
            featureSet.draw(width = 4,color = Color.RED)
            image.save(display)
        
        points = []







########NEW FILE########
__FILENAME__ = face-substition
#!/usr/bin/env python
# 
# Released under the BSD license. See LICENSE file for details.
"""
All this example does is find a face and replace it with another image. The
image should auto scale to match the size of the face.
"""
print __doc__

from SimpleCV import Camera, Display, HaarCascade, Image

#initialize the camera
cam = Camera()
# Create the display to show the image
display = Display()

# Load the new face image
troll_face = Image('troll_face.png', sample=True)

# Haar Cascade face detection, only faces
haarcascade = HaarCascade("face")

# Loop forever
while display.isNotDone():
    # Get image, flip it so it looks mirrored, scale to speed things up
    img = cam.getImage().flipHorizontal().scale(0.5)
    # load in trained face file
    faces = img.findHaarFeatures(haarcascade)
    # If there were faces found do something
    if faces:
        face = faces[-1]
        # Load the image to super impose and scale it correctly
        troll = troll_face.scale(face.height(), face.width()) 
        mymask = troll.invert()
        # Super impose the new face on the existing face
        img = img.blit(troll, face.topLeftCorner(), alphaMask=mymask)
    # Display the image
    img.save(display)

########NEW FILE########
__FILENAME__ = facetrack
#!/usr/bin/env python
# 
# Released under the BSD license. See LICENSE file for details.
"""
This program basically does face detection an blurs the face out.
"""
print __doc__

from SimpleCV import Camera, Display, HaarCascade

# Initialize the camera
cam = Camera()

# Create the display to show the image
display = Display()

# Haar Cascade face detection, only faces
haarcascade = HaarCascade("face")

# Loop forever
while display.isNotDone():
    # Get image, flip it so it looks mirrored, scale to speed things up
    img = cam.getImage().flipHorizontal().scale(0.5)
    # Load in trained face file
    faces = img.findHaarFeatures(haarcascade)
    # Pixelize the detected face
    if faces:
        bb = faces[-1].boundingBox()
        img = img.pixelize(10, region=(bb[0], bb[1], bb[2], bb[3]))
    # Display the image
    img.save(display)

########NEW FILE########
__FILENAME__ = FeatureDetection
#!/usr/bin/python
'''
This program is used to find keypoint features in an image.

You need to click the upper left hand corner of what you want to train,
then click the right lower corner of what you want to train.

This will give you a template.  For instance if you wanted to train
a face to recognize in the image you would click on the upper left corner
of the face, then click in the lower right corner of the face.  If you
want to retrain then just right click to reset.

'''

print __doc__

import time
from SimpleCV import Color, Image, np, Camera
from SimpleCV.Display import Display
cam = Camera()
display = Display((640,480)) # create our display

quality = 400.00
minDist = 0.35
minMatch = 0.2
template_img = None
mode = "untrained"
startX = None
startY = None
endY = None
endX = None

while( display.isNotDone() ):
    img = cam.getImage().resize(640,480)

    #Display this if a template has not been trained yet
    if mode == "untrained":
        if startX == None or startY == None:
            img.dl().text("Click the upper left corner to train", (10,10))
            if display.mouseLeft:
                startX = display.mouseRawX
                startY = display.mouseRawY
                time.sleep(0.2)
        elif endX == None or endY == None:
            img.dl().text("now click the lower right corner to train", (10,10))
            if display.mouseLeft:
                endX = display.mouseX
                endY = display.mouseY
                template_img = img.crop(startX,startY,endX - startX, endY - startY)
                mode = "trained"
                time.sleep(0.2)
        else:
            pass

    if mode == "trained":
        keypoints = img.findKeypointMatch(template_img,quality,minDist,minMatch)
        if keypoints:
            #keypoints.draw()
            img = img.applyLayers()
        img = img.drawKeypointMatches(template_img)   # draw the keypoint layers

        #Reset
        if display.mouseRight:
            template_img = None
            mode = "untrained"
            startX = None
            startY = None
            endY = None
            endX = None

    img = img.applyLayers() # apply the layers before resize
    img = img.resize(640,480)
    img.save(display)
    time.sleep(0.05)

########NEW FILE########
__FILENAME__ = FisherFaceRecognizer
from SimpleCV import *
import time
"""
This is an example of HOW-TO use FaceRecognizer to recognize gender
of the person.
"""


def identifyGender():
    f = FaceRecognizer()
    cam = Camera()
    img = cam.getImage()
    cascade = LAUNCH_PATH + "/" + "Features/HaarCascades/face.xml"
    feat = img.findHaarFeatures(cascade)
    if feat:
        crop_image = feat.sortArea()[-1].crop()
        feat.sortArea()[-1].draw()

    f.load(LAUNCH_PATH + "/" + "Features/FaceRecognizerData/GenderData.xml")
    w, h = f.imageSize
    crop_image = crop_image.resize(w, h)
    label, confidence = f.predict(crop_image)
    print label
    if label == 0:
        img.drawText("Female", fontsize=48)

    else:
        img.drawText("Male", fontsize=48)
    img.show()
    time.sleep(4)

identifyGender()

########NEW FILE########
__FILENAME__ = Least-Squares-Circle
'''
This program finds least squares fitting approximation for more information see:
http://en.wikipedia.org/wiki/Least_squares

The program basically takes in a shape and tries to find the size of it.

'''
print __doc__

from SimpleCV import *
from scipy import optimize

img = Image("derp.png", sample=True)
img = img.edges()
npimg = img.getGrayNumpy()
x,y = np.where(npimg > 128)
x_m = np.average(x)
y_m = np.average(y)
def calc_R(xc, yc):
    return np.sqrt((x-xc)**2 + (y-yc)**2)

def f_2(c):
    Ri = calc_R(*c)
    return Ri - Ri.mean()

center_estimate = x_m, y_m
center_2, ier = optimize.leastsq(f_2, center_estimate)

xc_2, yc_2 = center_2
Ri_2 = calc_R(*center_2)
R_2 = Ri_2.mean()
residu_2 = sum((Ri_2 - R_2)**2)

print xc_2,yc_2
print R_2
img.drawCircle((xc_2,yc_2),R_2,color=Color.RED,thickness=3)
img.show()
time.sleep(10)

########NEW FILE########
__FILENAME__ = MOGSegmentation
from SimpleCV import Camera, Display
from SimpleCV.Color import Color
from SimpleCV.Segmentation.MOGSegmentation import MOGSegmentation

mog = MOGSegmentation(history = 200, nMixtures = 5, backgroundRatio = 0.3, noiseSigma = 16, learningRate = 0.3)
  
cam = Camera()  
  
disp = Display()
  
while (disp.isNotDone()):  
    frame = cam.getImage()
    
    mog.addImage(frame)
    
    segmentedImage = mog.getSegmentedImage()
    blobs = mog.getSegmentedBlobs()
    for blob in blobs:
        segmentedImage.dl().circle((blob.x, blob.y), 10, Color.RED)
    
    segmentedImage.save(disp)

########NEW FILE########
__FILENAME__ = MotionTracker
#!/usr/bin/python
'''
This SimpleCV example uses a technique called frame differencing to determine
if motion has occured.  You take an initial image, then another, subtract
the difference, what is left over is what has changed between those two images
this are typically blobs on the images, so we do a blob search to count
the number of blobs and if they exist then motion has occured
'''
import sys, time, socket
from SimpleCV import *

cam = Camera() #setup the camera

#settings for the project
min_size = 0.1*cam.getProperty("width")*cam.getProperty("height") #make the threshold adapatable for various camera sizes
thresh = 10 # frame diff threshold
show_message_for = 2 # the amount of seconds to show the motion detected message
motion_timestamp = int(time.time())
message_text = "Motion detected"
draw_message = False

lastImg = cam.getImage()
lastImg.show()

while True:
    newImg = cam.getImage()
    trackImg = newImg - lastImg # diff the images
    blobs =  trackImg.findBlobs() #use adapative blob detection
    now = int(time.time())

    #If blobs are found then motion has occured
    if blobs:
        motion_timestamp = now
        draw_message = True

    #See if the time has exceeded to display the message
    if (now - motion_timestamp) > show_message_for:
        draw_message = False

    #Draw the message on the screen
    if(draw_message):
        newImg.drawText(message_text, 5,5)
        print message_text


    lastImg = newImg # update the image
    newImg.show()

########NEW FILE########
__FILENAME__ = optical_flow
from SimpleCV import *

def movement_check(x = 0,y = 0,t=1):
    direction = ""
    directionX = ""
    directionY = ""
    if x > t:
        directionX = "Right"
    if x < -1*t:
        directionX = "Left"
    if y < -1*t:
        directionY = "Up"
    if y > t:
        directionY = "Down"

    direction = directionX + " " + directionY
    if direction is not "":
        return direction
    else:
        return "No Motion"

def main():
    scale_amount = (200,150)
    d = Display(scale_amount)
    cam = Camera(0)
    prev = cam.getImage().scale(scale_amount[0],scale_amount[1])
    time.sleep(0.5)
    t = 0.5
    buffer = 20
    count = 0
    while d.isNotDone():
        current = cam.getImage()
        current = current.scale(scale_amount[0],scale_amount[1])
        if( count < buffer ):
            count = count + 1
        else:
            fs = current.findMotion(prev, window=15, method="BM")
            lengthOfFs = len(fs)
            if fs:
                #~ fs.draw(color=Color.RED)
                dx = 0
                dy = 0
                for f in fs:
                    dx = dx + f.dx
                    dy = dy + f.dy

                dx = (dx / lengthOfFs)
                dy = (dy / lengthOfFs)
                motionStr = movement_check(dx,dy,t)
                current.drawText(motionStr,10,10)

        prev = current
        time.sleep(0.01)
        current.save(d)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = pills
'''
This demo is used to find missing pills in a blister type of package
it would be used in quality control in manufacturing type of application
were you are verifying that the correct number of pills are present
'''


from SimpleCV import *

pillcolor = (153, 198, 252)  #This is set manually, you could either open the image you want and pick the color, or use blob detection to find the blob and do .meanColor() to get the RGB value
i = Image("pills.png", sample=True)
expected_pillcount = 12
saturation_threshold = 40  #This is how much saturation to allow in the image
pill_size = 100 #The size of the expected pills in pixels
packblobs = i.findBlobs(minsize=10) #find the bright silver on back blackground, easy


#run through the list of pills (blobs) found and check their color and markup the image when they are found
for idx in range(len(packblobs)):
    pack = packblobs[idx].crop()

    pills = pack.hueDistance(pillcolor, minsaturation = saturation_threshold)
    pills = pills.binarize(127)

    bm = BlobMaker()
    pills = bm.extractFromBinary(pills,pills,minsize=pill_size)
    if not pills:
        continue

    pillcount = len(pills)
    if pillcount != expected_pillcount:
        print "pack at %d, %d had %d pills" % (packblobs[idx].x, packblobs[idx].y, pillcount)
        i.drawText("Pills Found: " + str(pillcount), 10, 10, fontsize = 20)
        i.drawText("Pills Expected: " + str(expected_pillcount), 10, 30, fontsize = 20)
    for p in pills:
        p.image = pack
        p.drawHull( color = Color.RED, width = 5 )
    i.dl().blit(pack.applyLayers(), packblobs[idx].points[0])
    packblobs[idx].drawHull(color = Color.BLUE, width = 5)

#Continue to show the image
while True:
    i.show()

########NEW FILE########
__FILENAME__ = TemplateMatching
'''
This example uses the built in template matching.  The easiest way
to think of this is if you played the card matching game, the cards would
pretty much have to be identical.  The template method doesn't allow much
for the scale to change, nor for rotation.  This is the most basic pattern
matching SimpleCV offers.  If you are looking for something more complex
you will probably want to look into img.findKeypoints()
'''
print __doc__


from SimpleCV import *
import sys, time, socket

source = Image("templatetest.png", sample=True) # the image to search
template = Image("template.png", sample=True) # the template to search the image for
t = 5

methods = ["SQR_DIFF","SQR_DIFF_NORM","CCOEFF","CCOEFF_NORM","CCORR","CCORR_NORM"] # the various types of template matching available
for m in methods:
    print "current method:", m # print the method being used
    result = Image("templatetest.png", sample=True)
    dl = DrawingLayer((source.width,source.height))
    fs = source.findTemplate(template,threshold=t,method=m)
    for match in fs:
        dl.rectangle((match.x,match.y),(match.width(),match.height()),color=Color.RED)
    result.addDrawingLayer(dl)
    result.applyLayers()
    result.show()
    time.sleep(3)

########NEW FILE########
__FILENAME__ = TrainFacialRecognition
from SimpleCV import *
import time

# This file shows you how to train Fisher Face Recognition
# Enter the names of the faces and the output file.
cam = Camera(0) # camera
names = ['Alice','Bob'] # names of people to recognize
outfile = "test.csv" #output file 
waitTime = 10 # how long to wait between each training session

def getFaceSet(cam,myStr=""):
    # Grab a series of faces and return them.
    # quit when we press escape. 
    iset = ImageSet()
    count = 0
    disp = Display((640,480))
    while disp.isNotDone():
        img = cam.getImage()
        fs = img.findHaarFeatures('face')
        if( fs is not None ):
            fs = fs.sortArea()
            face = fs[-1].crop().resize(100,100)
            fs[-1].draw()
            iset.append(face)
            count = count + 1
        img.drawText(myStr,20,20,color=Color.RED,fontsize=32)
        img.save(disp)
    disp.quit()
    return iset

# First make sure our camera is all set up.
getFaceSet(cam,"Get Camera Ready! - ESC to Exit")
time.sleep(5)
labels = []
imgs = []
# for each person grab a training set of images
# and generate a list of labels.
for name in names:
    myStr = "Training for : " + name
    iset = getFaceSet(cam,myStr)
    imgs += iset
    labels += [name for i in range(0,len(iset))]
    time.sleep(waitTime)

# Create, train, and save the recognizer. 
f = FaceRecognizer()
print f.train(imgs, labels)
f.save(outfile)
# Now show us the results
disp = Display((640,480))
while disp.isNotDone():
    img = cam.getImage()
    fs = img.findHaarFeatures('face')
    if( fs is not None ):
        fs = fs.sortArea()
        face = fs[-1].crop().resize(100,100)
        fs[-1].draw()
        name, confidence = f.predict(face)
        img.drawText(name,30,30,fontsize=64)
    img.save(disp)

########NEW FILE########
__FILENAME__ = x-ray
#!/usr/bin/env python
# 
# Released under the BSD license. See LICENSE file for details.
"""
This program basically overlays an edge detector window that gives the
illusion of X-ray vision.  It is mearly meant to show how to perform a
basic image operation and overlay back onto the original image.
"""
print __doc__

from SimpleCV import Camera, Display

# Initialize the camera
cam = Camera() 

# Set the default size of the output window
display_width = 640
display_height = 480
# Create a new display to draw images on
display = Display(resolution = (display_width, display_height)) 

# Set the width and the height of the crop window
crop_width = 200
crop_height = 200

# Loop forever
while display.isNotDone():
    # Grab image from camera and flip it
    img = cam.getImage().flipHorizontal()
    # Set the x and the y location to scale
    crop_x = display.mouseX * img.width / display_width 
    crop_y = display.mouseY * img.height / display_height
    # Mouse outside the left or the top of the screen
    if(display.mouseX <= 1): 
        crop_x = 1
    if(display.mouseY <= 1):
        crop_y = 1
    # Region outside the right side or below the bottom of the screen
    if(display.mouseX + crop_width >= display_width):
        crop_x = (display_width - crop_width)
    if(display.mouseY + crop_height >= display_height):
        crop_y = (display_height - crop_height)
    # Crop out the section of image we want
    cropped_img = img.crop(crop_x, crop_y, crop_width, crop_height)
    # Get the edges of cropped region 
    xray_img = cropped_img.edges().smooth()
    # Draw the cropped image onto the current image
    img.getDrawingLayer().blit(xray_img, (crop_x, crop_y))
    # Display the image
    img.save(display)

########NEW FILE########
__FILENAME__ = gtk-example-camera
#!/usr/bin/python
'''
This program just shows off a simple example of using GTK with SimpleCV

This example interfaces with a Camera in real time

It's a very simple way to update an image using python and GTK.
The image is being updated as the slider is moved.
The only amount of SimpleCV code is found in the process_image() function

'''

print __doc__

import gtk
import SimpleCV
import gobject

cam = SimpleCV.Camera()

class app(gtk.Window):

    #Program Settings (You can change these)
    edge_threshold = 100
    max_threshold = 500
    min_threshold = 0
    window_width = 500
    window_height = 500
    refresh_rate = 100 #in milliseconds
    #End Program Settings

    #Variables
    current_image = None

    #This function setup's the program
    def __init__(self):
        super(app, self).__init__()
        self.set_position(gtk.WIN_POS_CENTER)
        self.set_title("Edge Threshold Adjuster")
        self.set_decorated(True)
        self.set_has_frame(False)
        self.set_resizable(False)
        self.set_default_size(self.window_width,self.window_height)
        self.connect("destroy", gtk.main_quit)
        vbox = gtk.VBox(spacing=4)


        #Setup the slider bar
        scale = gtk.HScale()
        scale.set_range(self.min_threshold, self.max_threshold)
        scale.set_size_request(500, 25)
        scale.set_value((self.max_threshold + self.min_threshold) / 2)
        scale.connect("value-changed", self.update_threshold)
        vbox.add(scale)

        #Setup the information label
        info = gtk.Label()
        info.set_label("Move the slider to adjust the edge detection threshold")
        vbox.add(info)

        #Add the image to the display
        new_image = self.process_image()
        converted_image = gtk.gdk.pixbuf_new_from_array(new_image, gtk.gdk.COLORSPACE_RGB, 8)
        image = gtk.Image()
        image.set_from_pixbuf(converted_image)
        image.show()
        vbox.add(image)


        gobject.timeout_add(self.refresh_rate, self.refresh)
        self.current_image = image
        self.add(vbox)
        self.show_all()


    def refresh(self):
        self.update_image()
        return True

    '''
    This is where you can do any of your SimpleCV processing
    it returns a Numpy array as that is what is handled by GTK
    '''
    def process_image(self):
        #Start SimpleCV Code
        img = cam.getImage().rotate90()
        edges = img.edges(self.edge_threshold)
        numpy_img = edges.getNumpy()
        #End SimpleCV Code
        return numpy_img

    def update_image(self):
        updated_image = self.process_image()
        converted_image = gtk.gdk.pixbuf_new_from_array(updated_image, gtk.gdk.COLORSPACE_RGB, 8)
        self.current_image.set_from_pixbuf(converted_image)
        self.show_all()


    #This function is called anything the slider is moved
    def update_threshold(self, w):
        #grab the value from the slider
        self.edge_threshold = w.get_value()
        self.update_image()


program1 = app()
gtk.main()

########NEW FILE########
__FILENAME__ = gtk-example
#!/usr/bin/python
'''
This program just shows off a simple example of using GTK with SimpleCV

It's a very simple way to update an image using python and GTK.
The image is being updated as the slider is moved.
The only amount of SimpleCV code is found in the process_image() function

'''

print __doc__

import gtk
import SimpleCV


class app(gtk.Window):

    #Program Settings (You can change these)
    edge_threshold = 100
    max_threshold = 500
    min_threshold = 0
    window_width = 500
    window_height = 500
    #End Program Settings


    #Variables
    current_image = None

    #This function setup's the program
    def __init__(self):
        super(app, self).__init__()
        self.set_position(gtk.WIN_POS_CENTER)
        self.set_title("Edge Threshold Adjuster")
        self.set_decorated(True)
        self.set_has_frame(False)
        self.set_resizable(False)
        self.set_default_size(self.window_width,self.window_height)
        self.connect("destroy", gtk.main_quit)
        vbox = gtk.VBox(spacing=4)


        #Setup the slider bar
        scale = gtk.HScale()
        scale.set_range(self.min_threshold, self.max_threshold)
        scale.set_size_request(500, 25)
        scale.set_value((self.max_threshold + self.min_threshold) / 2)
        scale.connect("value-changed", self.update_threshold)
        vbox.add(scale)

        #Setup the information label
        info = gtk.Label()
        info.set_label("Move the slider to adjust the edge detection threshold")
        vbox.add(info)

        #Add the image to the display
        new_image = self.process_image()
        converted_image = gtk.gdk.pixbuf_new_from_array(new_image, gtk.gdk.COLORSPACE_RGB, 8)
        image = gtk.Image()
        image.set_from_pixbuf(converted_image)
        image.show()
        vbox.add(image)


        self.current_image = image
        self.add(vbox)
        self.show_all()


    '''
    This is where you can do any of your SimpleCV processing
    it returns a Numpy array as that is what is handled by GTK
    '''
    def process_image(self):
        #Start SimpleCV Code
        img = SimpleCV.Image('lenna').rotate90()
        edges = img.edges(self.edge_threshold)
        numpy_img = edges.getNumpy()
        #End SimpleCV Code
        return numpy_img


    #This function is called anything the slider is moved
    def update_threshold(self, w):
        #grab the value from the slider
        self.edge_threshold = w.get_value()

        updated_image = self.process_image()
        converted_image = gtk.gdk.pixbuf_new_from_array(updated_image, gtk.gdk.COLORSPACE_RGB, 8)
        self.current_image.set_from_pixbuf(converted_image)
        self.show_all()

'''
Just create multiple instances of the object to have multiple windows
with varying control on each window.  These could also be extended using
threading so they could talk together, or if you closed one, it wouldn't
close the others, etc.
'''
program1 = app()
program2 = app()
gtk.main()

########NEW FILE########
__FILENAME__ = qt-example
#!/usr/bin/env python
'''
This example shows how to display a SimpleCV image in a QT window
the code was taken from the forum post here:
http://help.simplecv.org/question/1866/any-simple-pyqt-sample-regarding-ui-or-display/

Author: Rodrigo gomes 
'''

import os
import sys
import signal
from PyQt4 import uic, QtGui, QtCore
from SimpleCV import *


try:
    _fromUtf8 = QtCore.QString.fromUtf8
except AttributeError:
    def _fromUtf8(s):
        return s

try:
    _encoding = QtGui.QApplication.UnicodeUTF8
    def _translate(context, text, disambig):
        return QtGui.QApplication.translate(context, text, disambig, _encoding)
except AttributeError:
    def _translate(context, text, disambig):
        return QtGui.QApplication.translate(context, text, disambig)

class Ui_Dialog(object):
    def setupUi(self, Dialog):
        Dialog.setObjectName(_fromUtf8("Dialog"))
        Dialog.resize(632, 483)
        self.label = QtGui.QLabel(Dialog)
        self.label.setGeometry(QtCore.QRect(80, 30, 491, 391))
        self.label.setObjectName(_fromUtf8("label"))

        self.retranslateUi(Dialog)
        QtCore.QMetaObject.connectSlotsByName(Dialog)

    def retranslateUi(self, Dialog):
        Dialog.setWindowTitle(_translate("Dialog", "Dialog", None))


class Webcam(QtGui.QMainWindow):
    def __init__(self, parent=None):

        QtGui.QWidget.__init__(self,parent)        
        self.MainWindow = Ui_Dialog()
        self.MainWindow.setupUi(self)
        self.webcam = Camera(0,{ "width": 640, "height": 480 })

        self.timer = QtCore.QTimer()

        self.connect(self.timer, QtCore.SIGNAL('timeout()'), self.show_frame)

        self.timer.start(1);

    def show_frame(self):
        ipl_image = self.webcam.getImage()
        ipl_image.dl().circle((150, 75), 50, Color.RED, filled = True)
        data = ipl_image.getBitmap().tostring()
        image = QtGui.QImage(data, ipl_image.width, ipl_image.height, 3 * ipl_image.width, QtGui.QImage.Format_RGB888)
        pixmap = QtGui.QPixmap()
        pixmap.convertFromImage(image.rgbSwapped())
        self.MainWindow.label.setPixmap(pixmap)

if __name__ == "__main__":
    app = QtGui.QApplication(sys.argv)
    webcam = Webcam()
    webcam.show()
    app.exec_()

    

########NEW FILE########
__FILENAME__ = RenderExample
#!/usr/bin/python
from SimpleCV import *
from SimpleCV.DrawingLayer import DrawingLayer
import os

img = Image("../../sampleimages/color.jpg")
lineL = DrawingLayer((img.width,img.height))
a = (20,20)
b = (20,100)
c = (100,100)
d = (100,20)
lineL.line(a,b,alpha=128,width=5)
lineL.line(b,c,alpha=128)
lineL.line(c,d, antialias=True)
lineL.line(d,a,color=Color.PUCE)
lineL.line(a,c,color=Color.PLUM, alpha=52)
lineL.line(b,d,width=5)
img.addDrawingLayer(lineL)
temp = img.applyLayers()
print "line: %s" % temp.save(temp=True)
img.clearLayers()

linesL = DrawingLayer((img.width,img.height))
a = (20,20)
b = (20,100)
c = (100,100)
d = (100,20)
pts = (a,b,c,d,a)
linesL.lines(pts,alpha=128)
#translate over and down 10
pts = map(lambda x: ((x[0]+10),(x[1]+10)),pts)
linesL.lines(pts,color=Color.BEIGE,width=10)
#translate over and down 10
pts = map(lambda x: ((x[0]+10),(x[1]+10)),pts)
linesL.lines(pts,antialias=True)
img.addDrawingLayer(linesL)
temp = img.applyLayers()
print "lines: %s" % temp.save(temp=True)
img.clearLayers()

rectTR = DrawingLayer((img.width,img.height))
tr = (150,50)
wh = (200,100)
rectTR.rectangle(tr,wh,color=Color.BLUE)
tr = (170,70)
rectTR.rectangle(tr,wh,color=Color.PUCE, width=5)
tr = (190,90)
rectTR.rectangle(tr,wh,color=Color.FORESTGREEN, alpha=128,filled=True)
tr = (210,110)
rectTR.rectangle(tr,wh,color=Color.GREEN,filled=True)
img.addDrawingLayer(rectTR)
temp = img.applyLayers()
print "rectTR: %s" % temp.save(temp=True)
img.clearLayers()

rectC = DrawingLayer((img.width,img.height))
cxy = (img.width/2,img.height/2)
wh = (200,100)
rectC.centeredRectangle(cxy,wh,color=Color.BLUE)
wh = (180,80)
rectC.centeredRectangle(cxy,wh,color=Color.PUCE, width=5)
wh = (160,60)
rectC.centeredRectangle(cxy,wh,color=Color.FORESTGREEN, alpha=128,filled=True)
wh = (140,40)
rectC.centeredRectangle(cxy,wh,color=Color.GREEN,filled=True)
img.addDrawingLayer(rectC)
temp = img.applyLayers()
print "rectC: %s" % temp.save(temp=True)
img.clearLayers()

polyL = DrawingLayer((img.width,img.height))
a = (50,img.height-50)
b = (250,img.height-50)
c = (150,50)
pts = (a,b,c)
polyL.polygon(pts,alpha=128)
pts = map(lambda x: ((x[0]+10),(x[1]+10)),pts)
polyL.polygon(pts,antialias=True,width=3,alpha=210,filled=True,color=Color.LIME)
#translate over and down 10
pts = map(lambda x: ((x[0]+10),(x[1]+10)),pts)
polyL.polygon(pts,color=Color.BEIGE,width=10)
#translate over and down 10
pts = map(lambda x: ((x[0]+10),(x[1]+10)),pts)
polyL.polygon(pts,antialias=True,width=3,alpha=210)
img.addDrawingLayer(polyL)
temp = img.applyLayers()
print "poly: %s" % temp.save(temp=True)
img.clearLayers()

circleL = DrawingLayer((img.width,img.height))
c = (img.width/2,img.height/2)
r = 150
circleL.circle(c,r,color=Color.RED,filled=True)
r = 130
circleL.circle(c,r,color=Color.ORANGE,alpha=128,filled=True)
r = 110
circleL.circle(c,r,color=Color.YELLOW,alpha=128,width=10)
r = 100
circleL.circle(c,r,color=Color.GREEN)
r = 90
circleL.circle(c,r,color=Color.BLUE,alpha=172)
img.addDrawingLayer(circleL)
temp = img.applyLayers()
print "circle: %s" % temp.save(temp=True)
img.clearLayers()

ellipseL = DrawingLayer((img.width,img.height))
cxy = (img.width/2,img.height/2)
wh = (200,100)
ellipseL.ellipse(cxy,wh,color=Color.BLUE)
wh = (180,80)
ellipseL.ellipse(cxy,wh,color=Color.PUCE, width=5)
wh = (160,60)
ellipseL.ellipse(cxy,wh,color=Color.FORESTGREEN, alpha=128,filled=True)
wh = (140,40)
ellipseL.ellipse(cxy,wh,color=Color.GREEN,filled=True)
img.addDrawingLayer(ellipseL)
temp = img.applyLayers()
print "ellipse: %s" % temp.save(temp=True)
img.clearLayers()

bez = DrawingLayer((img.width,img.height))
a = (20,20)
b = (img.width-20,20)
c = (img.height-20,img.width-20)
d = (20,img.height-20)
e = (img.width/2,img.height/2)
pts = (a,b,c,d,e)
bez.bezier(pts,30)
img.addDrawingLayer(bez)
#translate over and down 10
pts = map(lambda x: ((x[0]+10),(x[1]+10)),pts)
bez.bezier(pts,5,color=Color.RED)
img.addDrawingLayer(bez)
pts = map(lambda x: ((x[0]+10),(x[1]+10)),pts)
bez.bezier(pts,30,color=Color.GREEN, alpha=128)
img.addDrawingLayer(bez)
temp = img.applyLayers()
print "bez: %s" % temp.save(temp=True)
img.clearLayers()

words = DrawingLayer((img.width,img.height))
words.setDefaultColor(Color.RED)
pos = (30,30)
words.setFontSize(30)
words.text("THIS IS BIG",pos)
pos = (50,50)
words.setFontSize(10)
words.text("THIS IS SMALL",pos)
pos = (70,70)
words.setFontSize(20)
words.text("THIS IS medium",pos)
pos = (90,90)
words.setFontBold(True)
words.text("THIS IS bold",pos)
pos = (110,110)
words.setFontItalic(True)
words.text("THIS IS italic",pos)
pos = (130,130)
words.setFontUnderline(True)
words.text("THIS IS underline",pos)
words.setFontBold(False)
words.setFontItalic(False)
words.setFontUnderline(False)
pos = (150,150)
words.text("THIS IS PUCE, YES PUCE",pos,color=Color.PUCE)
pos = (170,170)
words.text("This is magical text",pos,color=Color.PLUM,alpha=128)
pos = (190,190)
words.ezViewText("Can you read this better?",pos)
img.addDrawingLayer(words)
temp = img.applyLayers()
print "words: %s" % temp.save(temp=True)
img.clearLayers()

#Now lets do some layer stuff
img.addDrawingLayer(lineL)
img.addDrawingLayer(circleL)
img.addDrawingLayer(bez)
img.addDrawingLayer(words)
temp = img.applyLayers([0,2,3])
print "layers: %s" % temp.save(temp=True)
img.clearLayers()

#now lets do some blanket alpha work
lineL.setLayerAlpha(128)
circleL.setLayerAlpha(128)
bez.setLayerAlpha(128)
words.setLayerAlpha(128)
img.addDrawingLayer(lineL)
img.addDrawingLayer(circleL)
img.addDrawingLayer(bez)
img.addDrawingLayer(words)
temp = img.applyLayers()
print "flatlayers: %s" % temp.save(temp=True)
img.clearLayers()

sprites = DrawingLayer((img.width,img.height))
sprites.sprite("../../sampleimages/logo.png",(0,0),alpha=128, rot=45,scale=1.5)
mySprite = Image("../../sampleimages/logo.png").toPygameSurface()
sprites.sprite(mySprite,(100,100),alpha=128, rot=45,scale=1.5)
sprites.sprite(mySprite,(200,0))
sprites.sprite(mySprite,(0,200), rot=45,scale=1)
img.addDrawingLayer(sprites)
temp = img.applyLayers()
print "sprites: %s" % temp.save(temp=True)
img.clearLayers()

########NEW FILE########
__FILENAME__ = simplecam
#!/usr/bin/python
'''
This program is basically the hello world in SimpleCV
all it does is grab an image from the camera and display it
'''
print __doc__

from SimpleCV import *
cam = Camera()

while True:
    img = cam.getImage()
    img.show()

########NEW FILE########
__FILENAME__ = tkinter-example
import SimpleCV
import ImageTk #This has to be installed from the system repos
import Tkinter
import time

Tkinter.Tk()

image = SimpleCV.Image('http://i.imgur.com/FTKqh.jpg') #load the simplecv logo from the web
photo = ImageTk.PhotoImage(image.getPIL())
label = Tkinter.Label(image=photo)
label.image = photo # keep a reference!
label.pack() #show the image
time.sleep(5)

########NEW FILE########
__FILENAME__ = videowriter
#!/usr/bin/python

from SimpleCV import *
import time

c = Camera()
vs = VideoStream("foo.avi")

for i in range(0,500):
    c.getImage().edges().invert().save(vs)
    time.sleep(0.05)

########NEW FILE########
__FILENAME__ = kinect-coloring
#!/usr/bin/python

import time, webbrowser
from operator import add
from SimpleCV import Color, ColorCurve, Kinect, Image, pg, np
from SimpleCV.Display import Display

d = Display(flags = pg.FULLSCREEN)
#create video streams

cam = Kinect()
#initialize the camera

compositeframe = Image((640, 480))
#populate the compositeframe

offtime = 5.0
laststroke = time.time()

while not d.isDone():
    img = cam.getImage()
    imgscene = img.copy()

    depth = cam.getDepth()
    mindepth = np.min(depth.getNumpy())

    if mindepth < 180:
        depthbin = depth.binarize(np.min(depth.getNumpy()) + np.std(depth.getNumpy()) / 4).erode(3)
        #take the front 1/4 stdev of the depth map

        img = img.crop(0,25, 605, 455).scale(640,480)
        #img.dl().blit(img.crop(100, 25, 515, 455), (125,0))
        #this is a bit of a hack to compensate for the offset between cam and depth sensor
        #img = img.applyLayers()
        img = img - depthbin.invert()
        #img.save(d)
        meanred, meangrn, meanblue = img.meanColor()

        if meanred > meanblue and meanred > meangrn:
            depthbin, junk, junk = depthbin.splitChannels(grayscale = False)
        if meanblue > meanred and meanblue > meangrn:
            junk, junk, depthbin = depthbin.splitChannels(grayscale = False)
        if meangrn > meanred and meangrn > meanblue:
            junk, depthbin, junk = depthbin.splitChannels(grayscale = False)

        laststroke = time.time()
        compositeframe = compositeframe + depthbin
        #we're painting -- keep adding to the composite frame

    else:
        if (time.time() - laststroke > offtime):
        #if we're not painting for a certain amount of time, reset
            compositeframe = Image(cam.getImage().getEmpty())

    frame = ((imgscene - compositeframe.binarize(10).invert()) + compositeframe).flipHorizontal()
    #subtract our composite frame from our camera image, then add it back in in red. False = Show red channel as red, [0] = first (red) channel
    frame.save(d) #show in browser
    if d.mouseLeft:
        d.done = True
        pg.quit()

    time.sleep(0.01) #yield to the webserver

########NEW FILE########
__FILENAME__ = kinect-depthinedges
#!/usr/bin/python

import time, webbrowser
from SimpleCV import *

#create JPEG streamers
js = JpegStreamer(8080)
cam = Kinect()

cam.getDepth().save(js)
webbrowser.open("http://localhost:8080", 2)

while (1):
    d = cam.getDepth().edges()
    i = cam.getImage()
    i = d + i
    i.save(js)
    time.sleep(0.01) #yield to the webserver

########NEW FILE########
__FILENAME__ = kinect-motion-blur
#!/usr/bin/python

from operator import add
from SimpleCV import *
from SimpleCV.Display import Display

d = Display(flags = pg.FULLSCREEN)
#create video streams

cam = Kinect()
#initialize the camera
frames_to_blur = 4
frames = ImageSet()

depth = cam.getDepth().stretch(0,200)
while True:
    new_depth = cam.getDepth().stretch(0,200)
    img = cam.getImage()
    diff_1 = new_depth - depth
    diff_2 = depth - new_depth
    diff = diff_1 + diff_2
    img_filter = diff.binarize(0)

    motion_img = img - img_filter
    motion_img_open = motion_img.morphOpen()

    frames.append(motion_img_open)
    if len(frames) > frames_to_blur:
        frames.pop(0)

    pic = reduce(add, [i / float(len(frames)) for i in frames])
    pic.show()
    depth = new_depth

########NEW FILE########
__FILENAME__ = kinect-motion
#!/usr/bin/python

from SimpleCV import Kinect, Image, pg, np, time
from SimpleCV.Display import Display

d = Display(flags = pg.FULLSCREEN)
#create video streams

cam = Kinect()
#initialize the camera

depth = cam.getDepth().stretch(0,200)
while True:
    new_depth = cam.getDepth().stretch(0,200)
    img = cam.getImage()
    diff_1 = new_depth - depth
    diff_2 = depth - new_depth
    diff = diff_1 + diff_2
    img_filter = diff.binarize(0)

    motion_img = img - img_filter
    motion_img_open = motion_img.morphOpen()
    motion_img_open.show()
    depth = new_depth

########NEW FILE########
__FILENAME__ = color_cluster
'''
This program trys to extract the color pallette from an image
it could be used in machine learning as a color classifier
'''
print __doc__

from SimpleCV import *
disp = Display((640,528))
cam = Camera()
count = 0
pal = None
while disp.isNotDone():
    img = cam.getImage()
    if count%10 == 0:
        temp = img.scale(.3)
        p = temp.getPalette()
        pal = temp.drawPaletteColors(size=(640,48))
    result = img.rePalette(p)
    result = result.sideBySide(pal,side='bottom')
    result.save(disp)
    count = count + 1

########NEW FILE########
__FILENAME__ = machine-learning_nuts-vs-bolts
'''
This Example uses scikits-learn to do a binary classfication of images
of nuts vs. bolts.  Only the area, height, and width are used to classify
the actual images but data is extracted from the images using blobs.

This is a very crude example and could easily be built upon, but is just
meant to give an introductory example for using machine learning

The data set should auto download, if not you can get it from:
https://github.com/downloads/sightmachine/SimpleCV/nuts_bolts.zip
'''
print __doc__
from SimpleCV import *
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
import numpy as np

#Download the dataset
machine_learning_data_set = 'https://github.com/downloads/sightmachine/SimpleCV/nuts_bolts.zip'
data_path = download_and_extract(machine_learning_data_set)
print 'Test Images Downloaded at:', data_path

display = Display((800,600)) #Display to show the images
target_names = ['bolt', 'nut']

print 'Loading Bolts for Training'
bolts = ImageSet(data_path + '/data/supervised/bolts') #Load Bolts for training
bolt_blobs = [b.findBlobs()[0] for b in bolts] #exact the blobs for our features
tmp_data = [] #array to store data features
tmp_target = [] #array to store targets

for b in bolt_blobs: #Format Data for SVM
    tmp_data.append([b.area(), b.height(), b.width()])
    tmp_target.append(0)

print 'Loading Nuts for Training'
nuts = ImageSet(data_path + '/data/supervised/nuts')
nut_blobs = [n.invert().findBlobs()[0] for n in nuts]
for n in nut_blobs:
    tmp_data.append([n.area(), n.height(), n.width()])
    tmp_target.append(1)

dataset = np.array(tmp_data)
targets = np.array(tmp_target)

print 'Training Machine Learning'
clf = LinearSVC()
clf = clf.fit(dataset, targets)
clf2 = LogisticRegression().fit(dataset, targets)

print 'Running prediction on bolts now'
untrained_bolts = ImageSet(data_path + '/data/unsupervised/bolts')
unbolt_blobs = [b.findBlobs()[0] for b in untrained_bolts]
for b in unbolt_blobs:
    ary = [b.area(), b.height(), b.width()]
    name = target_names[clf.predict(ary)[0]]
    probability = clf2.predict_proba(ary)[0]
    img = b.image
    img.drawText(name)
    img.save(display)
    print "Predicted:",name,", Guess:",probability[0], target_names[0],",", probability[1], target_names[1]

print 'Running prediction on nuts now'
untrained_nuts = ImageSet(data_path + '/data/unsupervised/nuts')
unnut_blobs = [n.invert().findBlobs()[0] for n in untrained_nuts]
for n in unnut_blobs:
    ary = [n.area(), n.height(), n.width()]
    name = target_names[clf.predict(ary)[0]]
    probability = clf2.predict_proba(ary)[0]
    img = n.image
    img.drawText(name)
    img.save(display)
    print "Predicted:",name,", Guess:",probability[0], target_names[0],",", probability[1], target_names[1]

########NEW FILE########
__FILENAME__ = colorsegmentation
#!/usr/bin/python
'''
This program uses a Color model to try and do segmentation based on color
'''
print __doc__

import time
from SimpleCV import *

c = Camera()
i = c.getImage()
cm = ColorModel(i)

d = i.show()
t = int(time.time())
ticks = 0

while not d.isDone():
    cm.threshold(c.getImage()).save(d)
    time.sleep(0.01)
    ticks = ticks + 1
    if (int(time.time()) > t):
        print str(ticks) + " fps"
        ticks = 0
        t = int(time.time())

########NEW FILE########
__FILENAME__ = GreenScreen
'''
This program basically functions like a greenscreen that typically
a weather or news reporter would use.  It allows you to super impose
anything standing in front of a "green screen" in front of another image
this should even work with a camera is the user is standing in front
of a green background
'''
print __doc__

from SimpleCV import *

gs = Image("greenscreen.png", sample=True)
gs.show()
time.sleep(5)
background = Image("icecave.png", sample=True)
background.show()
time.sleep(5)
matte = gs.hueDistance(color=Color.GREEN, minvalue = 40).binarize()
matte.show()
time.sleep(5)
result = (gs-matte)+(background-matte.invert())
result.show()
time.sleep(5)

########NEW FILE########
__FILENAME__ = ImageMotionBlur
"""
This examples demonstrates the motionBlur method.
Use Up/Down Arrow keys to change power
Use Left/Right Arrow keys to change angle
"""
print __doc__

from SimpleCV import *
import pygame
import time

img = Image((500,500))
layer = DrawingLayer((500, 500))
layer.setFontSize(25)


layer.rectangle((0,0),(500,500),Color.WHITE,1,True)

#write the text
layer.text("Just some innocent looking dots",(50,25),Color.BLACK)
layer.text("Use Up/Down arrows ro change intensity",(50,50),Color.BLACK)
layer.text("Left/Right arrows to change angle",(50,75),Color.BLACK)

#draw 6 innocent looking dots
layer.circle((125,200),25,Color.RED,1,True)
layer.circle((250,200),25,Color.BLUE,1,True)
layer.circle((375,200),25,Color.GREEN,1,True)
layer.circle((125,300),25,Color.YELLOW,1,True)
layer.circle((250,300),25,Color.ORANGE,1,True)
layer.circle((375,300),25,Color.CYAN,1,True)


#apply layer
img.addDrawingLayer(layer)
img = img.applyLayers()
display = Display()
img.save(display)
power = 1
angle = 0
while not display.isDone():
    time.sleep(0.01)
    
    #detect up,down,left,right keypresses and modify power,angle
    if( pygame.key.get_pressed()[pygame.K_UP] != 0 ):
        power +=10 
        blur = img.motionBlur(power,angle)
        blur.save(display)
    if( pygame.key.get_pressed()[pygame.K_DOWN] != 0 ):
        power = max(power-10,1)
        blur = img.motionBlur(power,angle)
        blur.save(display)
    if( pygame.key.get_pressed()[pygame.K_LEFT] != 0 ):
        angle -= 5
        blur = img.motionBlur(power,angle)
        blur.save(display)
    if( pygame.key.get_pressed()[pygame.K_RIGHT] != 0 ):
        angle += 5
        blur = img.motionBlur(power,angle)
        blur.save(display)
    pass



########NEW FILE########
__FILENAME__ = mirror
from SimpleCV import *

c = Camera()

while True:
    img = c.getImage()
    split = img.split(2, 1)
    left = split[0][0]
    mirrorred = img.blit(left.flipHorizontal(),(left.width + 1, 0))
    mirrorred.show()

########NEW FILE########
__FILENAME__ = MorphologyExample
#!/usr/bin/python
'''
This example rotates through a few of the image morphology examples

for more information see:
http://en.wikipedia.org/wiki/Mathematical_morphology
'''
print __doc__
from SimpleCV import *

display = Display(resolution = (800, 600)) #create a new display to draw images on
cam = Camera() #initialize the camera
max_threshold = 1 # this is used for the edge detection
threshold_step = 1 # this is the amount to adjust the threshold by each time the display is updated
threshold = max_threshold
example = 1


while display.isNotDone():
    image = cam.getImage().flipHorizontal() # get image (or frame) from camera

    # This just automatically cycles through threshold levels
    if(threshold >= 20):
        threshold = max_threshold
        if(example >= 4):
            example = 1
        else:
            example = example + 1
    else:
        threshold = threshold + threshold_step


    if(example == 1):
        image = image.erode(threshold)
        text = "Erode Morphology Example: img.erode(" + str(threshold) + ")"

    elif(example == 2):
        image = image.dilate(threshold)
        text = "Dilate Morphology Example: img.dilate(" + str(threshold) + ")"

    elif(example == 3):
        image = image.morphOpen()
        text = "Open Morphology Example: img.morphOpen()"

    elif(example == 4):
        image = image.morphClose()
        text = "Close Morphology Example: img.morphClose()"

    elif(example == 5):
        image = image.morphGradient()
        text = "Gradient Morphology Example: img.morphGradient()"

    image.drawText(text, 10, 10, color=Color.RED, fontsize=30)
    image.show()

########NEW FILE########
__FILENAME__ = motionblur
#!/usr/bin/python
'''
This program does basic motion blurring.  It averages the number of
maxframes that are set using some basic image math
'''
print __doc__
from operator import add
from SimpleCV import *

js = JpegStreamer(8080)
#create JPEG streamers

cam = Camera() #initialize the camera

#the number of frames
maxframes = 3
frames = []
frames.append(cam.getImage())
frames[0].show()

while (1):
    frames.append(cam.getImage())
    #add the next frame to the end of the set

    if len(frames) > maxframes:
        frames.pop(0)  #remove the earliest frame if we're at max

    pic = reduce(add, [i / float(len(frames)) for i in frames])
    #add the frames in the array, weighted by 1 / number of frames

    pic.show()

########NEW FILE########
__FILENAME__ = mustachinator
#!/usr/bin/python


from operator import add
from SimpleCV import *

cam = Camera()
display = Display((800,600))
counter = 0
# load the cascades
face_cascade = HaarCascade("face")
nose_cascade = HaarCascade("nose")
stache = Image("stache.png", sample=True) # load the stache
mask = stache.createAlphaMask() # load the stache mask
count = 0
while display.isNotDone():
    img = cam.getImage()
    img = img.scale(.5) #use a smaller image
    faces = img.findHaarFeatures(face_cascade) #find faces
    if( faces is not None ): # if we have a face
        faces = faces.sortArea() #get the biggest one
        face = faces[-1]
        myFace = face.crop() # get the face image
        noses = myFace.findHaarFeatures(nose_cascade) #find the nose
        if( noses is not None ):# if we have a nose
            noses = noses.sortArea()
            nose = noses[0] # get the biggest
            # these get the upper left corner of the face/nose with respect to original image
            xf = face.x -(face.width()/2)
            yf = face.y -(face.height()/2)
            xm = nose.x -(nose.width()/2)
            ym = nose.y -(nose.height()/2)
            #calculate the mustache position
            xmust = xf+xm-(stache.width/2)+(nose.width()/2)
            ymust = yf+ym+(2*nose.height()/3)
            #blit the stache/mask onto the image
            img = img.blit(stache,pos=(xmust,ymust),mask = mask)

    img.save(display) #display

########NEW FILE########
__FILENAME__ = Partycam
#!/usr/bin/python
'''
This program basically simulates some kind of 80's music video.
'''
print __doc__
import sys, time, socket
from SimpleCV import *

cam = Camera()

#settings for the project
min_size = 0.1 * cam.getProperty("width")*cam.getProperty("height") #Change threshold
thresh = 10 # frme difference threshold

lastImg = cam.getImage();
lastImg.drawText("Move around to get the party started!", 5,5, fontsize=12)
lastImg.show()
while True:
    newImg = cam.getImage()
    trackImg = newImg - lastImg # difference the images
    blobs =  trackImg.findBlobs(-1, threshblocksize=99) #use adapative blob detection
    if blobs:
        blobs.draw(autocolor=True)
        trackImg.show()
    lastImg = newImg # update the image

########NEW FILE########
__FILENAME__ = RotationExample
#!/usr/bin/python
'''
This example shows how to perform various rotations and warps on images
and put back into a display.
'''
print __doc__

from SimpleCV import *

font_size = 30
sleep_for = 3 #seconds to sleep for
draw_color = Color.RED


while True:
    image = Image("orson_welles.jpg", sample=True)
    image.drawText("Original Size", 10,10, color=draw_color, fontsize=font_size)
    image.show()
    time.sleep(sleep_for)

    rot = image.rotate(45)
    rot.drawText("Rotated 45 degrees", 10,10, color=draw_color, fontsize=font_size)
    rot.show()
    time.sleep(sleep_for)

    rot = image.rotate(45, scale=0.5)
    rot.drawText("Rotated 45 degrees and scaled", 10,10, color=draw_color, fontsize=font_size)
    rot.show()
    time.sleep(sleep_for)

    rot = image.rotate(45,scale=0.5, point = (0,0) )
    rot.drawText("Rotated 45 degrees and scaled around a point", 10,10, color=draw_color, fontsize=font_size)
    rot.show()
    time.sleep(sleep_for)

    rot = image.rotate(45,"full")
    rot.drawText("Rotated 45 degrees and full", 10,10, color=draw_color, fontsize=font_size)
    rot.show()
    time.sleep(sleep_for)

    atrans = image.shear([(image.width/2,0),(image.width-1,image.height/2),(image.width/2,image.height-1)])
    atrans.drawText("Affine Transformation", 10,10, color=draw_color, fontsize=font_size)
    atrans.show()
    time.sleep(sleep_for)

    ptrans = image.warp([(image.width*0.05,image.height*0.03),(image.width*0.9,image.height*0.1),(image.width*0.8,image.height*0.7),(image.width*0.2,image.height*0.9)])
    ptrans.drawText("Perspective Transformation", 10,10, color=draw_color, fontsize=font_size)
    ptrans.show()
    time.sleep(sleep_for)

########NEW FILE########
__FILENAME__ = threedee
#!/usr/bin/env python

import sys
import os
from SimpleCV import *

def threedee_me(left, right, offset):
    (r,g,b)=left.splitChannels()
    left_blue = left.mergeChannels(None,b,g);
    #left_blue.save("blue.png", sample=True)
    (r,g,b) = right.splitChannels()
    right_red = right.mergeChannels(r,None,None);
    #right_red.save("red.png", sample=True)
    sz = (left.width+offset[0],left.height+offset[1])
    output = left_blue.embiggen(size=sz,pos=(0,0))
    output = output.blit(right_red,alpha=0.5,pos=offset)
    output = output.crop(offset[0],y=offset[1],w=left.width-offset[0],h=left.height-offset[1])
    return output

print "Taking pictures. Please move your camera slightly to its right"
print "after every picture."

c = Camera()
time.sleep(1)
images = []

for i in range(5):
    images.append(c.getImage())
    print "Picture %d taken" % (i + 1)
    time.sleep(1)

offset = (0,0)

for i in range(4):
    left = images[i]
    right = images[i+1]
    output = threedee_me(left, right, offset)
    print output.save(temp = True)
    output.show()
    time.sleep(2)

########NEW FILE########
__FILENAME__ = tvexample
'''
This program super imposes the camera onto the television in the picture
'''
print __doc__

from SimpleCV import Camera, Image, Display

tv_original = Image("family_watching_television_1958.jpg", sample=True)

tv_coordinates = [(353, 379), (433,380),(432, 448), (354,446)]
tv_mask = Image(tv_original.size()).invert().warp(tv_coordinates)
tv = tv_original - tv_mask

c = Camera()
d = Display(tv.size())

while d.isNotDone():
    bwimage = c.getImage().grayscale().resize(tv.width, tv.height)
    on_tv = tv + bwimage.warp(tv_coordinates)
    on_tv.save(d)

########NEW FILE########
__FILENAME__ = camshift
from SimpleCV import *
# Example for CAMShift Tracker
def foo(image):
    return image.meanColor()

def camshift():
    cam = Camera()
    img = cam.getImage()
    d = Display(img.size())
    bb1 = getBBFromUser(cam,d)
    fs1=[]
    while True:
        try:
            img1 = cam.getImage()
            fs1 = img1.track("camshift",fs1,img,bb1,num_frames=5, nframes=60, lower=(0, 40, 40), upper=(80, 200, 200))
            fs1.drawBB()
            fs1.drawPath()
            fs1.showCoordinates()
            fs1.showSizeRatio()
            fs1.showPixelVelocity()
            fs1.showPixelVelocityRT()
            img1.show()
        except KeyboardInterrupt:
            print "Total number of frames tracked",
            print fs1.trackLength()
            print fs1.processTrack(foo)
            break

def getBBFromUser(cam, d):
    p1 = None
    p2 = None
    img = cam.getImage()
    while d.isNotDone():
        try:
            img = cam.getImage()
            img.save(d)
            dwn = d.leftButtonDownPosition()
            up = d.leftButtonUpPosition()

            if dwn:
                p1 = dwn
            if up:
                p2 = up
                break

            time.sleep(0.05)
        except KeyboardInterrupt:
            break
    print p1,p2
    if not p1 or not p2:
        return None

    xmax = np.max((p1[0],p2[0]))
    xmin = np.min((p1[0],p2[0]))
    ymax = np.max((p1[1],p2[1]))
    ymin = np.min((p1[1],p2[1]))
    print xmin,ymin,xmax,ymax
    return (xmin,ymin,xmax-xmin,ymax-ymin)

camshift()

########NEW FILE########
__FILENAME__ = lk
"""
Example of Lucas Kanade Tracker
"""

from SimpleCV import *

def lktest():
    cam = Camera()
    img = cam.getImage()
    d = Display(img.size())
    img, bb1 = getBBFromUser(cam,d)
    fs1=[]
    while True:
        try:
            img1 = cam.getImage()
            fs1 = img1.track("lk",fs1,img,bb1, maxCorners = 5000, qualityLevel = 0.08, winSize = (15, 15))
            fs1.drawBB(color=Color.RED)
            print fs1[-1].getBB()
            img1.show()
        except KeyboardInterrupt:
            break

def getBBFromUser(cam, d):
    p1 = None
    p2 = None
    img = cam.getImage()
    while d.isNotDone():
        try:
            img = cam.getImage()
            img.save(d)
            dwn = d.leftButtonDownPosition()
            up = d.leftButtonUpPosition()

            if dwn:
                p1 = dwn
            if up:
                p2 = up
                break

            time.sleep(0.05)
        except KeyboardInterrupt:
            break
    if not p1 or not p2:
        return None

    xmax = np.max((p1[0],p2[0]))
    xmin = np.min((p1[0],p2[0]))
    ymax = np.max((p1[1],p2[1]))
    ymin = np.min((p1[1],p2[1]))
    return (img,(xmin,ymin,xmax-xmin,ymax-ymin))

lktest()

########NEW FILE########
__FILENAME__ = mftrack
from SimpleCV import *
# Example for Media Flow Tracker.
def foo(image):
    return image.meanColor()

def mftest():
    cam = Camera()
    img = cam.getImage()
    d = Display(img.size())
    bb1 = getBBFromUser(cam,d)
    fs1=[]
    img = cam.getImage()
    while True:
        try:
            img1 = cam.getImage()
            fs1 = img1.track("mftrack",fs1,img,bb1, numM=10, numN=10, winsize=10)
            print fs1[-1].shift, "shift"
            fs1.drawBB(color=(255,0,0))
            fs1.drawPath()
            img1.show()
        except KeyboardInterrupt:
            break

def getBBFromUser(cam, d):
    p1 = None
    p2 = None
    img = cam.getImage()
    while d.isNotDone():
        try:
            img = cam.getImage()
            img.save(d)
            dwn = d.leftButtonDownPosition()
            up = d.leftButtonUpPosition()

            if dwn:
                p1 = dwn
            if up:
                p2 = up
                break

            time.sleep(0.05)
        except KeyboardInterrupt:
            break
    print p1,p2
    if not p1 or not p2:
        return None

    xmax = np.max((p1[0],p2[0]))
    xmin = np.min((p1[0],p2[0]))
    ymax = np.max((p1[1],p2[1]))
    ymin = np.min((p1[1],p2[1]))
    print xmin,ymin,xmax,ymax
    return (xmin,ymin,xmax-xmin,ymax-ymin)

mftest()

########NEW FILE########
__FILENAME__ = surftest
"""
Example of SURFTracker
"""

from SimpleCV import *

def surftest():
    cam = Camera()
    img = cam.getImage()
    d = Display(img.size())
    img, bb1 = getBBFromUser(cam,d)
    fs1=[]
    while True:
        try:
            img1 = cam.getImage()
            fs1 = img1.track("surf",fs1,img,bb1, eps_val=0.8, dist=200, nframes=100)
            fs1.drawBB(color=Color.RED)
            fs1[-1].drawTrackerPoints()
            print fs1[-1].getBB()
            img1.show()
        except KeyboardInterrupt:
            break

def getBBFromUser(cam, d):
    p1 = None
    p2 = None
    img = cam.getImage()
    while d.isNotDone():
        try:
            img = cam.getImage()
            img.save(d)
            dwn = d.leftButtonDownPosition()
            up = d.leftButtonUpPosition()

            if dwn:
                p1 = dwn
            if up:
                p2 = up
                break

            time.sleep(0.05)
        except KeyboardInterrupt:
            break
    if not p1 or not p2:
        return None

    xmax = np.max((p1[0],p2[0]))
    xmin = np.min((p1[0],p2[0]))
    ymax = np.max((p1[1],p2[1]))
    ymin = np.min((p1[1],p2[1]))
    return (img,(xmin,ymin,xmax-xmin,ymax-ymin))

surftest()

########NEW FILE########
__FILENAME__ = CaptureEncodeUpload
from SimpleCV import Camera, VideoStream, Color, Display
from subprocess import call # to run command line programs
"""
This script requires ffmpeg and googlecl

googlecl / google command line can be found at:
https://code.google.com/p/googlecl/

To install googlecl download the source and run:
sudo python setup.py install

You will then need to get a youtube API key
You can get your key here:
https://developers.google.com/youtube/2.0/developers_guide_protocol

Install your api key by copying it to:
~/.local/share/googlecl/yt_devkey
All you do create a file called yt_devkey and
paste the key into the file.
The first time you run the script you will need to authorize
googlecl to use your account. 

ffmpeg can be found here http://www.ffmpeg.org/
see the ffmpeg website for installation instructions


"""
fname = 'test.avi'
outname = 'output.mp4'
tags = 'SimpleCV, Computer Vision, Python'
title = "SimpleCV Output"
summary = "See http://simplecv.org for more info."
access = "public" # Options are "public" "private" "protected"
# create the video stream for saving the video file
vs = VideoStream(fps=20,filename=fname,framefill=False)
# grab the camera
cam = Camera()
# create a display
disp = Display((800,600))
# while the user does not press 'esc'
while disp.isNotDone():
    # YOUR CODE GOES HERE!
    img = cam.getImage()
    img = img.edges()
    # write the frame
    vs.writeFrame(img)
    # save the image to the display
    img.save(disp)
# construct the encoding arguments
params = " -i {0} {1}".format(fname,outname)
# run ffmpeg to compress your video.
call('ffmpeg'+params,shell=True)
# construct the command line arguments for google command line
params = "{0} --title \"{1}\" --tags \"{2}\" --category \"Education\" --summary \"{3}\" --access \"{4}\" ".format(outname,title,tags,summary,access)
print params
# call the command line
call('google youtube post '+params,shell=True)

########NEW FILE########
__FILENAME__ = ColorCube
from SimpleCV import Image, Camera, Display, Color
import pygame as pg
import numpy as np
from pylab import *
from mpl_toolkits.mplot3d import axes3d
from matplotlib.backends.backend_agg import FigureCanvasAgg
import cv2

bins = 8
#precompute
idxs = []
colors = []
offset = bins/2
skip = 255/bins
for x in range(0,bins):
    for y in range(0,bins):
        for z in range(0,bins):
            b = ((x*skip)+offset)/255.0
            g = ((y*skip)+offset)/255.0
            r = ((z*skip)+offset)/255.0
            idxs.append((x,y,z,(r,g,b)))

# plot points in 3D
cam = Camera()
disp = Display((800,600))
fig = figure()
fig.set_size_inches( (10,7) )

canvas = FigureCanvasAgg(fig)
azim = 0
while disp.isNotDone():
    ax = fig.gca(projection='3d')
    ax.set_xlabel('BLUE', color=(0,0,1) )
    ax.set_ylabel('GREEN',color=(0,1,0))
    ax.set_zlabel('RED',color=(1,0,0))
    # Get the color histogram
    img = cam.getImage().scale(0.3)
    rgb = img.getNumpyCv2()
    hist = cv2.calcHist([rgb],[0,1,2],None,[bins,bins,bins],[0,256,0,256,0,256])
    hist = hist/np.max(hist)
    # render everything
    [ ax.plot([x],[y],[z],'.',markersize=max(hist[x,y,z]*100,6),color=color) for x,y,z,color in idxs if(hist[x][y][z]>0) ]
    #[ ax.plot([x],[y],[z],'.',color=color) for x,y,z,color in idxs if(hist[x][y][z]>0) ]
    ax.set_xlim3d(0, bins-1)
    ax.set_ylim3d(0, bins-1)
    ax.set_zlim3d(0, bins-1)
    azim = (azim+0.5)%360
    ax.view_init(elev=35, azim=azim)
    ########### convert matplotlib to  SimpleCV image
    canvas.draw()
    renderer = canvas.get_renderer()
    raw_data = renderer.tostring_rgb()
    size = canvas.get_width_height()    
    surf = pg.image.fromstring(raw_data, size, "RGB")
    figure = Image(surf)
    ############ All done
    figure = figure.floodFill((0,0), tolerance=5,color=Color.WHITE)
    result = figure.blit(img, pos=(20,20))
    result.save(disp)
    fig.clf()

########NEW FILE########
__FILENAME__ = cloudanimator
#!/bin/python
# This demo shows off using a web server and flash to upload and image
# and then do image processing in the cloud with SimpleCV then pass
# it back to the web browser via AJAX
#
# Using jpegcam as flash webcam library:
# http://code.google.com/p/jpegcam/
import os, tempfile, webbrowser, urllib, cherrypy, socket
from SimpleCV import *
from images2gif import writeGif
import pdb


class CloudAnimator(object):
    imageset = []
    giffile = None
    gifname = None


    def index(self):
        f = urllib.urlopen("index.html") # load the default website
        s = f.read() # read the file
        f.close()
        return s
    index.exposed = True

    def update(self):
        #update the animation
        print "update animation"

    update.exposed = True

    def upload(self):
        if(self.giffile == None):
            tmpfile = tempfile.NamedTemporaryFile(suffix=".gif")
            tmpname = tmpfile.name.split("/")[-1] #grab the generated name
            filepath = os.getcwd() + "/" + tmpname
            self.giffile = filepath
            self.gifname = tmpname
        tmpfile = tempfile.NamedTemporaryFile(suffix=".jpg") #Make a temporary gif file
        tmpname = tmpfile.name.split("/")[-1] #grab the generated name
        filepath = os.getcwd() + "/" + tmpname #get the filepath
        outfile = open(filepath, 'w') # create the filestream to write output to
        outfile.write(cherrypy.serving.request.body.fp.read()) # read the raw data from the webserver and write to the temporary directory
        outfile.close() # close the temporary file
        self.process(filepath) #Use SimpleCV to process the image
        os.unlink(filepath)
        return self.gifname #return the image path via ajax request

    upload.exposed = True

    def reset(self):
        #reset the animation
        if(self.giffile != None):
            os.unlink(self.giffile)
            tmpfile = tempfile.NamedTemporaryFile(suffix=".gif")
            tmpname = tmpfile.name.split("/")[-1] #grab the generated name
            filepath = os.getcwd() + "/" + tmpname
            #~ pdb.set_trace()
            for i in self.imageset:
                del i
            self.giffile = filepath
            self.gifname = tmpname
            self.imageset = []
            print "reset animation"

    reset.exposed = True

    def process(self, filepath):
        img = Image(filepath) # load the image into SimpleCV
        #~ img = img.edges() # Get the edges
        img.save(filepath) # save the temporary image
        self.imageset.append(img.getPIL())
        writeGif(self.giffile, self.imageset, 0.2, 9999)
        return

    process.exposed = True



if __name__ == '__main__':
    conf =  {'/':
                {
                'tools.staticdir.on': True,
                'tools.staticdir.dir': os.getcwd()
                },
            'global' :
                {
                'server.socket_port': 8000,
                'server.socket_host' : '0.0.0.0'
                }
            }
    #webbrowser.open("http://localhost:8000")
    cherrypy.quickstart(CloudAnimator(), config=conf)

########NEW FILE########
__FILENAME__ = images2gif
""" MODULE images2gif

Provides a function (writeGif) to write animated gif from a series
of PIL images or numpy arrays.

This code is provided as is, and is free to use for all.

Almar Klein (June 2009)

- based on gifmaker (in the scripts folder of the source distribution of PIL)
- based on gif file structure as provided by wikipedia

"""

try:
    import PIL
    from PIL import Image, ImageChops
    from PIL.GifImagePlugin import getheader, getdata
except ImportError:
    PIL = None

try:
    import numpy as np
except ImportError:
    np = None

# getheader gives a 87a header and a color palette (two elements in a list).
# getdata()[0] gives the Image Descriptor up to (including) "LZW min code size".
# getdatas()[1:] is the image data itself in chuncks of 256 bytes (well
# technically the first byte says how many bytes follow, after which that
# amount (max 255) follows).


def intToBin(i):
    """ Integer to two bytes """
    # devide in two parts (bytes)
    i1 = i % 256
    i2 = int( i/256)
    # make string (little endian)
    return chr(i1) + chr(i2)


def getheaderAnim(im):
    """ Animation header. To replace the getheader()[0] """
    bb = "GIF89a"
    bb += intToBin(im.size[0])
    bb += intToBin(im.size[1])
    bb += "\x87\x00\x00"
    return bb


def getAppExt(loops=0):
    """ Application extention. Part that secifies amount of loops.
    if loops is 0, if goes on infinitely.
    """
    bb = "\x21\xFF\x0B"  # application extension
    bb += "NETSCAPE2.0"
    bb += "\x03\x01"
    if loops == 0:
        loops = 2**16-1
    bb += intToBin(loops)
    bb += '\x00'  # end
    return bb


def getGraphicsControlExt(duration=0.1):
    """ Graphics Control Extension. A sort of header at the start of
    each image. Specifies transparancy and duration. """
    bb = '\x21\xF9\x04'
    bb += '\x08'  # no transparancy
    bb += intToBin( int(duration*100) ) # in 100th of seconds
    bb += '\x00'  # no transparant color
    bb += '\x00'  # end
    return bb


def _writeGifToFile(fp, images, durations, loops):
    """ Given a set of images writes the bytes to the specified stream.
    """

    # init
    frames = 0
    previous = None

    for im in images:

        if not previous:
            # first image

            # gather data
            palette = getheader(im)[1]
            data = getdata(im)
            imdes, data = data[0], data[1:]
            header = getheaderAnim(im)
            appext = getAppExt(loops)
            graphext = getGraphicsControlExt(durations[0])

            # write global header
            fp.write(header)
            fp.write(palette)
            fp.write(appext)

            # write image
            fp.write(graphext)
            fp.write(imdes)
            for d in data:
                fp.write(d)

        else:
            # gather info (compress difference)
            data = getdata(im)
            imdes, data = data[0], data[1:]
            graphext = getGraphicsControlExt(durations[frames])

            # write image
            fp.write(graphext)
            fp.write(imdes)
            for d in data:
                fp.write(d)

#             # delta frame - does not seem to work
#             delta = ImageChops.subtract_modulo(im, previous)
#             bbox = delta.getbbox()
#
#             if bbox:
#
#                 # gather info (compress difference)
#                 data = getdata(im.crop(bbox), offset = bbox[:2])
#                 imdes, data = data[0], data[1:]
#                 graphext = getGraphicsControlExt(durations[frames])
#
#                 # write image
#                 fp.write(graphext)
#                 fp.write(imdes)
#                 for d in data:
#                     fp.write(d)
#
#             else:
#                 # FIXME: what should we do in this case?
#                 pass

        # prepare for next round
        previous = im.copy()
        frames = frames + 1

    fp.write(";")  # end gif
    return frames


def writeGif(filename, images, duration=0.1, loops=0, dither=1):
    """ writeGif(filename, images, duration=0.1, loops=0, dither=1)
    Write an animated gif from the specified images.
    images should be a list of numpy arrays of PIL images.
    Numpy images of type float should have pixels between 0 and 1.
    Numpy images of other types are expected to have values between 0 and 255.
    """

    if PIL is None:
        raise RuntimeError("Need PIL to write animated gif files.")

    images2 = []

    # convert to PIL
    for im in images:

        if True: #isinstance(im,Image.Image):
            images2.append( im.convert('P',dither=dither) )

        elif np and isinstance(im, np.ndarray):
            if im.dtype == np.uint8:
                pass
            elif im.dtype in [np.float32, np.float64]:
                im = (im*255).astype(np.uint8)
            else:
                im = im.astype(np.uint8)
            # convert
            if len(im.shape)==3 and im.shape[2]==3:
                im = Image.fromarray(im,'RGB').convert('P',dither=dither)
            elif len(im.shape)==2:
                im = Image.fromarray(im,'L').convert('P',dither=dither)
            else:
                raise ValueError("Array has invalid shape to be an image.")
            images2.append(im)

        else:
            raise ValueError("Unknown image type.")

    # check duration
    if hasattr(duration, '__len__'):
        if len(duration) == len(images2):
            durations = [d for d in duration]
        else:
            raise ValueError("len(duration) doesn't match amount of images.")
    else:
        durations = [duration for im in images2]


    # open file
    fp = open(filename, 'wb')

    # write
    try:
        n = _writeGifToFile(fp, images2, durations, loops)
        print n, 'frames written'
    finally:
        fp.close()


if __name__ == '__main__':
    im = np.zeros((200,200), dtype=np.uint8)
    im[10:30,:] = 100
    im[:,80:120] = 255
    im[-50:-40,:] = 50

    images = [im*1.0, im*0.8, im*0.6, im*0.4, im*0]
    writeGif('lala3.gif',images, duration=0.5, dither=0)

########NEW FILE########
__FILENAME__ = cloudcam
#!/bin/python
# This demo shows off using a web server and flash to upload and image
# and then do image processing in the cloud with SimpleCV then pass
# it back to the web browser via AJAX
#
# Using jpegcam as flash webcam library:
# http://code.google.com/p/jpegcam/
import os, tempfile, webbrowser, urllib, cherrypy, socket
from SimpleCV import *


class CloudCam(object):

    def index(self):
        f = urllib.urlopen("index.html") # load the default website
        s = f.read() # read the file
        f.close()
        return s
    index.exposed = True

    def upload(self):
        tmpfile = tempfile.NamedTemporaryFile(suffix=".png") #Make a temporary file
        tmpname = tmpfile.name.split("/")[-1] #grab the generated name
        filepath = os.getcwd() + "/" + tmpname #get the filepath
        outfile = open(filepath, 'w') # create the filestream to write output to
        outfile.write(cherrypy.serving.request.body.fp.read()) # read the raw data from the webserver and write to the temporary directory
        outfile.close() # close the temporary file
        self.process(filepath) #Use SimpleCV to process the image

        print "url:" + cherrypy.url()
        print "socket:" + socket.gethostbyname(socket.gethostname())
        #~ return "http://localhost:8000/" + tmpname #return the image path via ajax request
        return tmpname

    def process(self, filepath):
        img = Image(filepath) # load the image into SimpleCV
        img = img.edges() # Get the edges
        img.save(filepath) # save the temporary image
        return

    upload.exposed = True



if __name__ == '__main__':
    conf =  {'/':
                {
                'tools.staticdir.on': True,
                'tools.staticdir.dir': os.getcwd()
                },
            'global' :
                {
                'server.socket_port': 8000,
                'server.socket_host' : '0.0.0.0'
                }
            }
    webbrowser.open("http://localhost:8000")
    cherrypy.quickstart(CloudCam(), config=conf)

########NEW FILE########
__FILENAME__ = flask-server
'''
This application uses Flask as a web server and jquery to trigger
pictures with SimpleCV

To use start the web server:
>>> python flask-server.py

Then to run the application:
>>> python webkit-gtk.py

*Note: You are not required to run the webkit-gtk.py, you can also
visit http://localhost:5000

'''

print __doc__


from flask import Flask, jsonify, render_template, request
from werkzeug import SharedDataMiddleware
import tempfile, os
import simplejson as json
import SimpleCV


app = Flask(__name__)
cam = SimpleCV.Camera()

@app.route('/')
def show(name=None):
    img = cam.getImage()
    tf = tempfile.NamedTemporaryFile(suffix=".png")
    loc = 'static/' + tf.name.split('/')[-1]
    tf.close()
    img.save(loc)
    return render_template('index.html', img=loc)

@app.route('/_snapshot')
def snapshot():
    '''
    Takes a picture and returns a path via json
    used as ajax callback for taking a picture
    '''
    img = cam.getImage()
    tf = tempfile.NamedTemporaryFile(suffix=".png")
    loc = 'static/' + tf.name.split('/')[-1]
    tf.close()
    img.save(loc)
    print "location",loc
    print "json", json.dumps(loc)
    return json.dumps(loc)

if __name__ == '__main__':
    if app.config['DEBUG']:
        from werkzeug import SharedDataMiddleware
        import os
        app.wsgi_app = SharedDataMiddleware(app.wsgi_app, {
                '/': os.path.join(os.path.dirname(__file__), 'static')
        })
    app.run()

########NEW FILE########
__FILENAME__ = webkit-gtk
import gtk
import webkit

view = webkit.WebView()

sw = gtk.ScrolledWindow()
sw.add(view)

win = gtk.Window(gtk.WINDOW_TOPLEVEL)
win.resize(800,600)
win.add(sw)
win.show_all()

view.open("http://localhost:5000/")
gtk.main()

########NEW FILE########
__FILENAME__ = EXIF
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Library to extract EXIF information from digital camera image files
# http://sourceforge.net/projects/exif-py/
#
# VERSION 1.1.0
#
# To use this library call with:
#    f = open(path_name, 'rb')
#    tags = EXIF.process_file(f)
#
# To ignore MakerNote tags, pass the -q or --quick
# command line arguments, or as
#    tags = EXIF.process_file(f, details=False)
#
# To stop processing after a certain tag is retrieved,
# pass the -t TAG or --stop-tag TAG argument, or as
#    tags = EXIF.process_file(f, stop_tag='TAG')
#
# where TAG is a valid tag name, ex 'DateTimeOriginal'
#
# These 2 are useful when you are retrieving a large list of images
#
#
# To return an error on invalid tags,
# pass the -s or --strict argument, or as
#    tags = EXIF.process_file(f, strict=True)
#
# Otherwise these tags will be ignored
#
# Returned tags will be a dictionary mapping names of EXIF tags to their
# values in the file named by path_name.  You can process the tags
# as you wish.  In particular, you can iterate through all the tags with:
#     for tag in tags.keys():
#         if tag not in ('JPEGThumbnail', 'TIFFThumbnail', 'Filename',
#                        'EXIF MakerNote'):
#             print "Key: %s, value %s" % (tag, tags[tag])
# (This code uses the if statement to avoid printing out a few of the
# tags that tend to be long or boring.)
#
# The tags dictionary will include keys for all of the usual EXIF
# tags, and will also include keys for Makernotes used by some
# cameras, for which we have a good specification.
#
# Note that the dictionary keys are the IFD name followed by the
# tag name. For example:
# 'EXIF DateTimeOriginal', 'Image Orientation', 'MakerNote FocusMode'
#
# Copyright (c) 2002-2007 Gene Cash All rights reserved
# Copyright (c) 2007-2008 Ianaré Sévi All rights reserved
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#
#  2. Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#
#  3. Neither the name of the authors nor the names of its contributors
#     may be used to endorse or promote products derived from this
#     software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#
#
# ----- See 'changes.txt' file for all contributors and changes ----- #
#


# Don't throw an exception when given an out of range character.
def make_string(seq):
    str = ''
    for c in seq:
        # Screen out non-printing characters
        if 32 <= c and c < 256:
            str += chr(c)
    # If no printing chars
    if not str:
        return seq
    return str

# Special version to deal with the code in the first 8 bytes of a user comment.
# First 8 bytes gives coding system e.g. ASCII vs. JIS vs Unicode
def make_string_uc(seq):
    code = seq[0:8]
    seq = seq[8:]
    # Of course, this is only correct if ASCII, and the standard explicitly
    # allows JIS and Unicode.
    return make_string(seq)

# field type descriptions as (length, abbreviation, full name) tuples
FIELD_TYPES = (
    (0, 'X', 'Proprietary'), # no such type
    (1, 'B', 'Byte'),
    (1, 'A', 'ASCII'),
    (2, 'S', 'Short'),
    (4, 'L', 'Long'),
    (8, 'R', 'Ratio'),
    (1, 'SB', 'Signed Byte'),
    (1, 'U', 'Undefined'),
    (2, 'SS', 'Signed Short'),
    (4, 'SL', 'Signed Long'),
    (8, 'SR', 'Signed Ratio'),
    )

# dictionary of main EXIF tag names
# first element of tuple is tag name, optional second element is
# another dictionary giving names to values
EXIF_TAGS = {
    0x0100: ('ImageWidth', ),
    0x0101: ('ImageLength', ),
    0x0102: ('BitsPerSample', ),
    0x0103: ('Compression',
             {1: 'Uncompressed',
              2: 'CCITT 1D',
              3: 'T4/Group 3 Fax',
              4: 'T6/Group 4 Fax',
              5: 'LZW',
              6: 'JPEG (old-style)',
              7: 'JPEG',
              8: 'Adobe Deflate',
              9: 'JBIG B&W',
              10: 'JBIG Color',
              32766: 'Next',
              32769: 'Epson ERF Compressed',
              32771: 'CCIRLEW',
              32773: 'PackBits',
              32809: 'Thunderscan',
              32895: 'IT8CTPAD',
              32896: 'IT8LW',
              32897: 'IT8MP',
              32898: 'IT8BL',
              32908: 'PixarFilm',
              32909: 'PixarLog',
              32946: 'Deflate',
              32947: 'DCS',
              34661: 'JBIG',
              34676: 'SGILog',
              34677: 'SGILog24',
              34712: 'JPEG 2000',
              34713: 'Nikon NEF Compressed',
              65000: 'Kodak DCR Compressed',
              65535: 'Pentax PEF Compressed'}),
    0x0106: ('PhotometricInterpretation', ),
    0x0107: ('Thresholding', ),
    0x010A: ('FillOrder', ),
    0x010D: ('DocumentName', ),
    0x010E: ('ImageDescription', ),
    0x010F: ('Make', ),
    0x0110: ('Model', ),
    0x0111: ('StripOffsets', ),
    0x0112: ('Orientation',
             {1: 'Horizontal (normal)',
              2: 'Mirrored horizontal',
              3: 'Rotated 180',
              4: 'Mirrored vertical',
              5: 'Mirrored horizontal then rotated 90 CCW',
              6: 'Rotated 90 CW',
              7: 'Mirrored horizontal then rotated 90 CW',
              8: 'Rotated 90 CCW'}),
    0x0115: ('SamplesPerPixel', ),
    0x0116: ('RowsPerStrip', ),
    0x0117: ('StripByteCounts', ),
    0x011A: ('XResolution', ),
    0x011B: ('YResolution', ),
    0x011C: ('PlanarConfiguration', ),
    0x011D: ('PageName', make_string),
    0x0128: ('ResolutionUnit',
             {1: 'Not Absolute',
              2: 'Pixels/Inch',
              3: 'Pixels/Centimeter'}),
    0x012D: ('TransferFunction', ),
    0x0131: ('Software', ),
    0x0132: ('DateTime', ),
    0x013B: ('Artist', ),
    0x013E: ('WhitePoint', ),
    0x013F: ('PrimaryChromaticities', ),
    0x0156: ('TransferRange', ),
    0x0200: ('JPEGProc', ),
    0x0201: ('JPEGInterchangeFormat', ),
    0x0202: ('JPEGInterchangeFormatLength', ),
    0x0211: ('YCbCrCoefficients', ),
    0x0212: ('YCbCrSubSampling', ),
    0x0213: ('YCbCrPositioning',
             {1: 'Centered',
              2: 'Co-sited'}),
    0x0214: ('ReferenceBlackWhite', ),

    0x4746: ('Rating', ),

    0x828D: ('CFARepeatPatternDim', ),
    0x828E: ('CFAPattern', ),
    0x828F: ('BatteryLevel', ),
    0x8298: ('Copyright', ),
    0x829A: ('ExposureTime', ),
    0x829D: ('FNumber', ),
    0x83BB: ('IPTC/NAA', ),
    0x8769: ('ExifOffset', ),
    0x8773: ('InterColorProfile', ),
    0x8822: ('ExposureProgram',
             {0: 'Unidentified',
              1: 'Manual',
              2: 'Program Normal',
              3: 'Aperture Priority',
              4: 'Shutter Priority',
              5: 'Program Creative',
              6: 'Program Action',
              7: 'Portrait Mode',
              8: 'Landscape Mode'}),
    0x8824: ('SpectralSensitivity', ),
    0x8825: ('GPSInfo', ),
    0x8827: ('ISOSpeedRatings', ),
    0x8828: ('OECF', ),
    0x9000: ('ExifVersion', make_string),
    0x9003: ('DateTimeOriginal', ),
    0x9004: ('DateTimeDigitized', ),
    0x9101: ('ComponentsConfiguration',
             {0: '',
              1: 'Y',
              2: 'Cb',
              3: 'Cr',
              4: 'Red',
              5: 'Green',
              6: 'Blue'}),
    0x9102: ('CompressedBitsPerPixel', ),
    0x9201: ('ShutterSpeedValue', ),
    0x9202: ('ApertureValue', ),
    0x9203: ('BrightnessValue', ),
    0x9204: ('ExposureBiasValue', ),
    0x9205: ('MaxApertureValue', ),
    0x9206: ('SubjectDistance', ),
    0x9207: ('MeteringMode',
             {0: 'Unidentified',
              1: 'Average',
              2: 'CenterWeightedAverage',
              3: 'Spot',
              4: 'MultiSpot',
              5: 'Pattern'}),
    0x9208: ('LightSource',
             {0: 'Unknown',
              1: 'Daylight',
              2: 'Fluorescent',
              3: 'Tungsten',
              9: 'Fine Weather',
              10: 'Flash',
              11: 'Shade',
              12: 'Daylight Fluorescent',
              13: 'Day White Fluorescent',
              14: 'Cool White Fluorescent',
              15: 'White Fluorescent',
              17: 'Standard Light A',
              18: 'Standard Light B',
              19: 'Standard Light C',
              20: 'D55',
              21: 'D65',
              22: 'D75',
              255: 'Other'}),
    0x9209: ('Flash',
             {0: 'No',
              1: 'Fired',
              5: 'Fired (?)', # no return sensed
              7: 'Fired (!)', # return sensed
              9: 'Fill Fired',
              13: 'Fill Fired (?)',
              15: 'Fill Fired (!)',
              16: 'Off',
              24: 'Auto Off',
              25: 'Auto Fired',
              29: 'Auto Fired (?)',
              31: 'Auto Fired (!)',
              32: 'Not Available'}),
    0x920A: ('FocalLength', ),
    0x9214: ('SubjectArea', ),
    0x927C: ('MakerNote', ),
    0x9286: ('UserComment', make_string_uc),
    0x9290: ('SubSecTime', ),
    0x9291: ('SubSecTimeOriginal', ),
    0x9292: ('SubSecTimeDigitized', ),

    # used by Windows Explorer
    0x9C9B: ('XPTitle', ),
    0x9C9C: ('XPComment', ),
    0x9C9D: ('XPAuthor', ), #(ignored by Windows Explorer if Artist exists)
    0x9C9E: ('XPKeywords', ),
    0x9C9F: ('XPSubject', ),

    0xA000: ('FlashPixVersion', make_string),
    0xA001: ('ColorSpace',
             {1: 'sRGB',
              2: 'Adobe RGB',
              65535: 'Uncalibrated'}),
    0xA002: ('ExifImageWidth', ),
    0xA003: ('ExifImageLength', ),
    0xA005: ('InteroperabilityOffset', ),
    0xA20B: ('FlashEnergy', ),               # 0x920B in TIFF/EP
    0xA20C: ('SpatialFrequencyResponse', ),  # 0x920C
    0xA20E: ('FocalPlaneXResolution', ),     # 0x920E
    0xA20F: ('FocalPlaneYResolution', ),     # 0x920F
    0xA210: ('FocalPlaneResolutionUnit', ),  # 0x9210
    0xA214: ('SubjectLocation', ),           # 0x9214
    0xA215: ('ExposureIndex', ),             # 0x9215
    0xA217: ('SensingMethod',                # 0x9217
             {1: 'Not defined',
              2: 'One-chip color area',
              3: 'Two-chip color area',
              4: 'Three-chip color area',
              5: 'Color sequential area',
              7: 'Trilinear',
              8: 'Color sequential linear'}),
    0xA300: ('FileSource',
             {1: 'Film Scanner',
              2: 'Reflection Print Scanner',
              3: 'Digital Camera'}),
    0xA301: ('SceneType',
             {1: 'Directly Photographed'}),
    0xA302: ('CVAPattern', ),
    0xA401: ('CustomRendered',
             {0: 'Normal',
              1: 'Custom'}),
    0xA402: ('ExposureMode',
             {0: 'Auto Exposure',
              1: 'Manual Exposure',
              2: 'Auto Bracket'}),
    0xA403: ('WhiteBalance',
             {0: 'Auto',
              1: 'Manual'}),
    0xA404: ('DigitalZoomRatio', ),
    0xA405: ('FocalLengthIn35mmFilm', ),
    0xA406: ('SceneCaptureType',
             {0: 'Standard',
              1: 'Landscape',
              2: 'Portrait',
              3: 'Night)'}),
    0xA407: ('GainControl',
             {0: 'None',
              1: 'Low gain up',
              2: 'High gain up',
              3: 'Low gain down',
              4: 'High gain down'}),
    0xA408: ('Contrast',
             {0: 'Normal',
              1: 'Soft',
              2: 'Hard'}),
    0xA409: ('Saturation',
             {0: 'Normal',
              1: 'Soft',
              2: 'Hard'}),
    0xA40A: ('Sharpness',
             {0: 'Normal',
              1: 'Soft',
              2: 'Hard'}),
    0xA40B: ('DeviceSettingDescription', ),
    0xA40C: ('SubjectDistanceRange', ),
    0xA500: ('Gamma', ),
    0xC4A5: ('PrintIM', ),
    0xEA1C:     ('Padding', ),
    }

# interoperability tags
INTR_TAGS = {
    0x0001: ('InteroperabilityIndex', ),
    0x0002: ('InteroperabilityVersion', ),
    0x1000: ('RelatedImageFileFormat', ),
    0x1001: ('RelatedImageWidth', ),
    0x1002: ('RelatedImageLength', ),
    }

# GPS tags (not used yet, haven't seen camera with GPS)
GPS_TAGS = {
    0x0000: ('GPSVersionID', ),
    0x0001: ('GPSLatitudeRef', ),
    0x0002: ('GPSLatitude', ),
    0x0003: ('GPSLongitudeRef', ),
    0x0004: ('GPSLongitude', ),
    0x0005: ('GPSAltitudeRef', ),
    0x0006: ('GPSAltitude', ),
    0x0007: ('GPSTimeStamp', ),
    0x0008: ('GPSSatellites', ),
    0x0009: ('GPSStatus', ),
    0x000A: ('GPSMeasureMode', ),
    0x000B: ('GPSDOP', ),
    0x000C: ('GPSSpeedRef', ),
    0x000D: ('GPSSpeed', ),
    0x000E: ('GPSTrackRef', ),
    0x000F: ('GPSTrack', ),
    0x0010: ('GPSImgDirectionRef', ),
    0x0011: ('GPSImgDirection', ),
    0x0012: ('GPSMapDatum', ),
    0x0013: ('GPSDestLatitudeRef', ),
    0x0014: ('GPSDestLatitude', ),
    0x0015: ('GPSDestLongitudeRef', ),
    0x0016: ('GPSDestLongitude', ),
    0x0017: ('GPSDestBearingRef', ),
    0x0018: ('GPSDestBearing', ),
    0x0019: ('GPSDestDistanceRef', ),
    0x001A: ('GPSDestDistance', ),
    0x001D: ('GPSDate', ),
    }

# Ignore these tags when quick processing
# 0x927C is MakerNote Tags
# 0x9286 is user comment
IGNORE_TAGS=(0x9286, 0x927C)

# http://tomtia.plala.jp/DigitalCamera/MakerNote/index.asp
def nikon_ev_bias(seq):
    # First digit seems to be in steps of 1/6 EV.
    # Does the third value mean the step size?  It is usually 6,
    # but it is 12 for the ExposureDifference.
    #
    # Check for an error condition that could cause a crash.
    # This only happens if something has gone really wrong in
    # reading the Nikon MakerNote.
    if len( seq ) < 4 : return ""
    #
    if seq == [252, 1, 6, 0]:
        return "-2/3 EV"
    if seq == [253, 1, 6, 0]:
        return "-1/2 EV"
    if seq == [254, 1, 6, 0]:
        return "-1/3 EV"
    if seq == [0, 1, 6, 0]:
        return "0 EV"
    if seq == [2, 1, 6, 0]:
        return "+1/3 EV"
    if seq == [3, 1, 6, 0]:
        return "+1/2 EV"
    if seq == [4, 1, 6, 0]:
        return "+2/3 EV"
    # Handle combinations not in the table.
    a = seq[0]
    # Causes headaches for the +/- logic, so special case it.
    if a == 0:
        return "0 EV"
    if a > 127:
        a = 256 - a
        ret_str = "-"
    else:
        ret_str = "+"
    b = seq[2]  # Assume third value means the step size
    whole = a / b
    a = a % b
    if whole != 0:
        ret_str = ret_str + str(whole) + " "
    if a == 0:
        ret_str = ret_str + "EV"
    else:
        r = Ratio(a, b)
        ret_str = ret_str + r.__repr__() + " EV"
    return ret_str

# Nikon E99x MakerNote Tags
MAKERNOTE_NIKON_NEWER_TAGS={
    0x0001: ('MakernoteVersion', make_string),  # Sometimes binary
    0x0002: ('ISOSetting', make_string),
    0x0003: ('ColorMode', ),
    0x0004: ('Quality', ),
    0x0005: ('Whitebalance', ),
    0x0006: ('ImageSharpening', ),
    0x0007: ('FocusMode', ),
    0x0008: ('FlashSetting', ),
    0x0009: ('AutoFlashMode', ),
    0x000B: ('WhiteBalanceBias', ),
    0x000C: ('WhiteBalanceRBCoeff', ),
    0x000D: ('ProgramShift', nikon_ev_bias),
    # Nearly the same as the other EV vals, but step size is 1/12 EV (?)
    0x000E: ('ExposureDifference', nikon_ev_bias),
    0x000F: ('ISOSelection', ),
    0x0011: ('NikonPreview', ),
    0x0012: ('FlashCompensation', nikon_ev_bias),
    0x0013: ('ISOSpeedRequested', ),
    0x0016: ('PhotoCornerCoordinates', ),
    # 0x0017: Unknown, but most likely an EV value
    0x0018: ('FlashBracketCompensationApplied', nikon_ev_bias),
    0x0019: ('AEBracketCompensationApplied', ),
    0x001A: ('ImageProcessing', ),
    0x001B: ('CropHiSpeed', ),
    0x001D: ('SerialNumber', ), # Conflict with 0x00A0 ?
    0x001E: ('ColorSpace', ),
    0x001F: ('VRInfo', ),
    0x0020: ('ImageAuthentication', ),
    0x0022: ('ActiveDLighting', ),
    0x0023: ('PictureControl', ),
    0x0024: ('WorldTime', ),
    0x0025: ('ISOInfo', ),
    0x0080: ('ImageAdjustment', ),
    0x0081: ('ToneCompensation', ),
    0x0082: ('AuxiliaryLens', ),
    0x0083: ('LensType', ),
    0x0084: ('LensMinMaxFocalMaxAperture', ),
    0x0085: ('ManualFocusDistance', ),
    0x0086: ('DigitalZoomFactor', ),
    0x0087: ('FlashMode',
             {0x00: 'Did Not Fire',
              0x01: 'Fired, Manual',
              0x07: 'Fired, External',
              0x08: 'Fired, Commander Mode ',
              0x09: 'Fired, TTL Mode'}),
    0x0088: ('AFFocusPosition',
             {0x0000: 'Center',
              0x0100: 'Top',
              0x0200: 'Bottom',
              0x0300: 'Left',
              0x0400: 'Right'}),
    0x0089: ('BracketingMode',
             {0x00: 'Single frame, no bracketing',
              0x01: 'Continuous, no bracketing',
              0x02: 'Timer, no bracketing',
              0x10: 'Single frame, exposure bracketing',
              0x11: 'Continuous, exposure bracketing',
              0x12: 'Timer, exposure bracketing',
              0x40: 'Single frame, white balance bracketing',
              0x41: 'Continuous, white balance bracketing',
              0x42: 'Timer, white balance bracketing'}),
    0x008A: ('AutoBracketRelease', ),
    0x008B: ('LensFStops', ),
    0x008C: ('NEFCurve1', ),    # ExifTool calls this 'ContrastCurve'
    0x008D: ('ColorMode', ),
    0x008F: ('SceneMode', ),
    0x0090: ('LightingType', ),
    0x0091: ('ShotInfo', ),     # First 4 bytes are a version number in ASCII
    0x0092: ('HueAdjustment', ),
    # ExifTool calls this 'NEFCompression', should be 1-4
    0x0093: ('Compression', ),
    0x0094: ('Saturation',
             {-3: 'B&W',
              -2: '-2',
              -1: '-1',
              0: '0',
              1: '1',
              2: '2'}),
    0x0095: ('NoiseReduction', ),
    0x0096: ('NEFCurve2', ),    # ExifTool calls this 'LinearizationTable'
    0x0097: ('ColorBalance', ), # First 4 bytes are a version number in ASCII
    0x0098: ('LensData', ),     # First 4 bytes are a version number in ASCII
    0x0099: ('RawImageCenter', ),
    0x009A: ('SensorPixelSize', ),
    0x009C: ('Scene Assist', ),
    0x009E: ('RetouchHistory', ),
    0x00A0: ('SerialNumber', ),
    0x00A2: ('ImageDataSize', ),
    # 00A3: unknown - a single byte 0
    # 00A4: In NEF, looks like a 4 byte ASCII version number ('0200')
    0x00A5: ('ImageCount', ),
    0x00A6: ('DeletedImageCount', ),
    0x00A7: ('TotalShutterReleases', ),
    # First 4 bytes are a version number in ASCII, with version specific
    # info to follow.  Its hard to treat it as a string due to embedded nulls.
    0x00A8: ('FlashInfo', ),
    0x00A9: ('ImageOptimization', ),
    0x00AA: ('Saturation', ),
    0x00AB: ('DigitalVariProgram', ),
    0x00AC: ('ImageStabilization', ),
    0x00AD: ('Responsive AF', ),        # 'AFResponse'
    0x00B0: ('MultiExposure', ),
    0x00B1: ('HighISONoiseReduction', ),
    0x00B7: ('AFInfo', ),
    0x00B8: ('FileInfo', ),
    # 00B9: unknown
    0x0100: ('DigitalICE', ),
    0x0103: ('PreviewCompression',
             {1: 'Uncompressed',
              2: 'CCITT 1D',
              3: 'T4/Group 3 Fax',
              4: 'T6/Group 4 Fax',
              5: 'LZW',
              6: 'JPEG (old-style)',
              7: 'JPEG',
              8: 'Adobe Deflate',
              9: 'JBIG B&W',
              10: 'JBIG Color',
              32766: 'Next',
              32769: 'Epson ERF Compressed',
              32771: 'CCIRLEW',
              32773: 'PackBits',
              32809: 'Thunderscan',
              32895: 'IT8CTPAD',
              32896: 'IT8LW',
              32897: 'IT8MP',
              32898: 'IT8BL',
              32908: 'PixarFilm',
              32909: 'PixarLog',
              32946: 'Deflate',
              32947: 'DCS',
              34661: 'JBIG',
              34676: 'SGILog',
              34677: 'SGILog24',
              34712: 'JPEG 2000',
              34713: 'Nikon NEF Compressed',
              65000: 'Kodak DCR Compressed',
              65535: 'Pentax PEF Compressed',}),
    0x0201: ('PreviewImageStart', ),
    0x0202: ('PreviewImageLength', ),
    0x0213: ('PreviewYCbCrPositioning',
             {1: 'Centered',
              2: 'Co-sited'}),
    0x0010: ('DataDump', ),
    }

MAKERNOTE_NIKON_OLDER_TAGS = {
    0x0003: ('Quality',
             {1: 'VGA Basic',
              2: 'VGA Normal',
              3: 'VGA Fine',
              4: 'SXGA Basic',
              5: 'SXGA Normal',
              6: 'SXGA Fine'}),
    0x0004: ('ColorMode',
             {1: 'Color',
              2: 'Monochrome'}),
    0x0005: ('ImageAdjustment',
             {0: 'Normal',
              1: 'Bright+',
              2: 'Bright-',
              3: 'Contrast+',
              4: 'Contrast-'}),
    0x0006: ('CCDSpeed',
             {0: 'ISO 80',
              2: 'ISO 160',
              4: 'ISO 320',
              5: 'ISO 100'}),
    0x0007: ('WhiteBalance',
             {0: 'Auto',
              1: 'Preset',
              2: 'Daylight',
              3: 'Incandescent',
              4: 'Fluorescent',
              5: 'Cloudy',
              6: 'Speed Light'}),
    }

# decode Olympus SpecialMode tag in MakerNote
def olympus_special_mode(v):
    a={
        0: 'Normal',
        1: 'Unknown',
        2: 'Fast',
        3: 'Panorama'}
    b={
        0: 'Non-panoramic',
        1: 'Left to right',
        2: 'Right to left',
        3: 'Bottom to top',
        4: 'Top to bottom'}
    if v[0] not in a or v[2] not in b:
        return v
    return '%s - sequence %d - %s' % (a[v[0]], v[1], b[v[2]])

MAKERNOTE_OLYMPUS_TAGS={
    # ah HAH! those sneeeeeaky bastids! this is how they get past the fact
    # that a JPEG thumbnail is not allowed in an uncompressed TIFF file
    0x0100: ('JPEGThumbnail', ),
    0x0200: ('SpecialMode', olympus_special_mode),
    0x0201: ('JPEGQual',
             {1: 'SQ',
              2: 'HQ',
              3: 'SHQ'}),
    0x0202: ('Macro',
             {0: 'Normal',
             1: 'Macro',
             2: 'SuperMacro'}),
    0x0203: ('BWMode',
             {0: 'Off',
             1: 'On'}),
    0x0204: ('DigitalZoom', ),
    0x0205: ('FocalPlaneDiagonal', ),
    0x0206: ('LensDistortionParams', ),
    0x0207: ('SoftwareRelease', ),
    0x0208: ('PictureInfo', ),
    0x0209: ('CameraID', make_string), # print as string
    0x0F00: ('DataDump', ),
    0x0300: ('PreCaptureFrames', ),
    0x0404: ('SerialNumber', ),
    0x1000: ('ShutterSpeedValue', ),
    0x1001: ('ISOValue', ),
    0x1002: ('ApertureValue', ),
    0x1003: ('BrightnessValue', ),
    0x1004: ('FlashMode', ),
    0x1004: ('FlashMode',
       {2: 'On',
        3: 'Off'}),
    0x1005: ('FlashDevice',
       {0: 'None',
        1: 'Internal',
        4: 'External',
        5: 'Internal + External'}),
    0x1006: ('ExposureCompensation', ),
    0x1007: ('SensorTemperature', ),
    0x1008: ('LensTemperature', ),
    0x100b: ('FocusMode',
       {0: 'Auto',
        1: 'Manual'}),
    0x1017: ('RedBalance', ),
    0x1018: ('BlueBalance', ),
    0x101a: ('SerialNumber', ),
    0x1023: ('FlashExposureComp', ),
    0x1026: ('ExternalFlashBounce',
       {0: 'No',
        1: 'Yes'}),
    0x1027: ('ExternalFlashZoom', ),
    0x1028: ('ExternalFlashMode', ),
    0x1029: ('Contrast  int16u',
       {0: 'High',
        1: 'Normal',
        2: 'Low'}),
    0x102a: ('SharpnessFactor', ),
    0x102b: ('ColorControl', ),
    0x102c: ('ValidBits', ),
    0x102d: ('CoringFilter', ),
    0x102e: ('OlympusImageWidth', ),
    0x102f: ('OlympusImageHeight', ),
    0x1034: ('CompressionRatio', ),
    0x1035: ('PreviewImageValid',
       {0: 'No',
        1: 'Yes'}),
    0x1036: ('PreviewImageStart', ),
    0x1037: ('PreviewImageLength', ),
    0x1039: ('CCDScanMode',
       {0: 'Interlaced',
        1: 'Progressive'}),
    0x103a: ('NoiseReduction',
       {0: 'Off',
        1: 'On'}),
    0x103b: ('InfinityLensStep', ),
    0x103c: ('NearLensStep', ),

    # TODO - these need extra definitions
    # http://search.cpan.org/src/EXIFTOOL/Image-ExifTool-6.90/html/TagNames/Olympus.html
    0x2010: ('Equipment', ),
    0x2020: ('CameraSettings', ),
    0x2030: ('RawDevelopment', ),
    0x2040: ('ImageProcessing', ),
    0x2050: ('FocusInfo', ),
    0x3000: ('RawInfo ', ),
    }

# 0x2020 CameraSettings
MAKERNOTE_OLYMPUS_TAG_0x2020={
    0x0100: ('PreviewImageValid',
             {0: 'No',
              1: 'Yes'}),
    0x0101: ('PreviewImageStart', ),
    0x0102: ('PreviewImageLength', ),
    0x0200: ('ExposureMode',
             {1: 'Manual',
              2: 'Program',
              3: 'Aperture-priority AE',
              4: 'Shutter speed priority AE',
              5: 'Program-shift'}),
    0x0201: ('AELock',
             {0: 'Off',
              1: 'On'}),
    0x0202: ('MeteringMode',
             {2: 'Center Weighted',
              3: 'Spot',
              5: 'ESP',
              261: 'Pattern+AF',
              515: 'Spot+Highlight control',
              1027: 'Spot+Shadow control'}),
    0x0300: ('MacroMode',
             {0: 'Off',
              1: 'On'}),
    0x0301: ('FocusMode',
             {0: 'Single AF',
              1: 'Sequential shooting AF',
              2: 'Continuous AF',
              3: 'Multi AF',
              10: 'MF'}),
    0x0302: ('FocusProcess',
             {0: 'AF Not Used',
              1: 'AF Used'}),
    0x0303: ('AFSearch',
             {0: 'Not Ready',
              1: 'Ready'}),
    0x0304: ('AFAreas', ),
    0x0401: ('FlashExposureCompensation', ),
    0x0500: ('WhiteBalance2',
             {0: 'Auto',
             16: '7500K (Fine Weather with Shade)',
             17: '6000K (Cloudy)',
             18: '5300K (Fine Weather)',
             20: '3000K (Tungsten light)',
             21: '3600K (Tungsten light-like)',
             33: '6600K (Daylight fluorescent)',
             34: '4500K (Neutral white fluorescent)',
             35: '4000K (Cool white fluorescent)',
             48: '3600K (Tungsten light-like)',
             256: 'Custom WB 1',
             257: 'Custom WB 2',
             258: 'Custom WB 3',
             259: 'Custom WB 4',
             512: 'Custom WB 5400K',
             513: 'Custom WB 2900K',
             514: 'Custom WB 8000K', }),
    0x0501: ('WhiteBalanceTemperature', ),
    0x0502: ('WhiteBalanceBracket', ),
    0x0503: ('CustomSaturation', ), # (3 numbers: 1. CS Value, 2. Min, 3. Max)
    0x0504: ('ModifiedSaturation',
             {0: 'Off',
              1: 'CM1 (Red Enhance)',
              2: 'CM2 (Green Enhance)',
              3: 'CM3 (Blue Enhance)',
              4: 'CM4 (Skin Tones)'}),
    0x0505: ('ContrastSetting', ), # (3 numbers: 1. Contrast, 2. Min, 3. Max)
    0x0506: ('SharpnessSetting', ), # (3 numbers: 1. Sharpness, 2. Min, 3. Max)
    0x0507: ('ColorSpace',
             {0: 'sRGB',
              1: 'Adobe RGB',
              2: 'Pro Photo RGB'}),
    0x0509: ('SceneMode',
             {0: 'Standard',
              6: 'Auto',
              7: 'Sport',
              8: 'Portrait',
              9: 'Landscape+Portrait',
             10: 'Landscape',
             11: 'Night scene',
             13: 'Panorama',
             16: 'Landscape+Portrait',
             17: 'Night+Portrait',
             19: 'Fireworks',
             20: 'Sunset',
             22: 'Macro',
             25: 'Documents',
             26: 'Museum',
             28: 'Beach&Snow',
             30: 'Candle',
             35: 'Underwater Wide1',
             36: 'Underwater Macro',
             39: 'High Key',
             40: 'Digital Image Stabilization',
             44: 'Underwater Wide2',
             45: 'Low Key',
             46: 'Children',
             48: 'Nature Macro'}),
    0x050a: ('NoiseReduction',
             {0: 'Off',
              1: 'Noise Reduction',
              2: 'Noise Filter',
              3: 'Noise Reduction + Noise Filter',
              4: 'Noise Filter (ISO Boost)',
              5: 'Noise Reduction + Noise Filter (ISO Boost)'}),
    0x050b: ('DistortionCorrection',
             {0: 'Off',
              1: 'On'}),
    0x050c: ('ShadingCompensation',
             {0: 'Off',
              1: 'On'}),
    0x050d: ('CompressionFactor', ),
    0x050f: ('Gradation',
             {'-1 -1 1': 'Low Key',
              '0 -1 1': 'Normal',
              '1 -1 1': 'High Key'}),
    0x0520: ('PictureMode',
             {1: 'Vivid',
              2: 'Natural',
              3: 'Muted',
              256: 'Monotone',
              512: 'Sepia'}),
    0x0521: ('PictureModeSaturation', ),
    0x0522: ('PictureModeHue?', ),
    0x0523: ('PictureModeContrast', ),
    0x0524: ('PictureModeSharpness', ),
    0x0525: ('PictureModeBWFilter',
             {0: 'n/a',
              1: 'Neutral',
              2: 'Yellow',
              3: 'Orange',
              4: 'Red',
              5: 'Green'}),
    0x0526: ('PictureModeTone',
             {0: 'n/a',
              1: 'Neutral',
              2: 'Sepia',
              3: 'Blue',
              4: 'Purple',
              5: 'Green'}),
    0x0600: ('Sequence', ), # 2 or 3 numbers: 1. Mode, 2. Shot number, 3. Mode bits
    0x0601: ('PanoramaMode', ), # (2 numbers: 1. Mode, 2. Shot number)
    0x0603: ('ImageQuality2',
             {1: 'SQ',
              2: 'HQ',
              3: 'SHQ',
              4: 'RAW'}),
    0x0901: ('ManometerReading', ),
    }


MAKERNOTE_CASIO_TAGS={
    0x0001: ('RecordingMode',
             {1: 'Single Shutter',
              2: 'Panorama',
              3: 'Night Scene',
              4: 'Portrait',
              5: 'Landscape'}),
    0x0002: ('Quality',
             {1: 'Economy',
              2: 'Normal',
              3: 'Fine'}),
    0x0003: ('FocusingMode',
             {2: 'Macro',
              3: 'Auto Focus',
              4: 'Manual Focus',
              5: 'Infinity'}),
    0x0004: ('FlashMode',
             {1: 'Auto',
              2: 'On',
              3: 'Off',
              4: 'Red Eye Reduction'}),
    0x0005: ('FlashIntensity',
             {11: 'Weak',
              13: 'Normal',
              15: 'Strong'}),
    0x0006: ('Object Distance', ),
    0x0007: ('WhiteBalance',
             {1: 'Auto',
              2: 'Tungsten',
              3: 'Daylight',
              4: 'Fluorescent',
              5: 'Shade',
              129: 'Manual'}),
    0x000B: ('Sharpness',
             {0: 'Normal',
              1: 'Soft',
              2: 'Hard'}),
    0x000C: ('Contrast',
             {0: 'Normal',
              1: 'Low',
              2: 'High'}),
    0x000D: ('Saturation',
             {0: 'Normal',
              1: 'Low',
              2: 'High'}),
    0x0014: ('CCDSpeed',
             {64: 'Normal',
              80: 'Normal',
              100: 'High',
              125: '+1.0',
              244: '+3.0',
              250: '+2.0'}),
    }

MAKERNOTE_FUJIFILM_TAGS={
    0x0000: ('NoteVersion', make_string),
    0x1000: ('Quality', ),
    0x1001: ('Sharpness',
             {1: 'Soft',
              2: 'Soft',
              3: 'Normal',
              4: 'Hard',
              5: 'Hard'}),
    0x1002: ('WhiteBalance',
             {0: 'Auto',
              256: 'Daylight',
              512: 'Cloudy',
              768: 'DaylightColor-Fluorescent',
              769: 'DaywhiteColor-Fluorescent',
              770: 'White-Fluorescent',
              1024: 'Incandescent',
              3840: 'Custom'}),
    0x1003: ('Color',
             {0: 'Normal',
              256: 'High',
              512: 'Low'}),
    0x1004: ('Tone',
             {0: 'Normal',
              256: 'High',
              512: 'Low'}),
    0x1010: ('FlashMode',
             {0: 'Auto',
              1: 'On',
              2: 'Off',
              3: 'Red Eye Reduction'}),
    0x1011: ('FlashStrength', ),
    0x1020: ('Macro',
             {0: 'Off',
              1: 'On'}),
    0x1021: ('FocusMode',
             {0: 'Auto',
              1: 'Manual'}),
    0x1030: ('SlowSync',
             {0: 'Off',
              1: 'On'}),
    0x1031: ('PictureMode',
             {0: 'Auto',
              1: 'Portrait',
              2: 'Landscape',
              4: 'Sports',
              5: 'Night',
              6: 'Program AE',
              256: 'Aperture Priority AE',
              512: 'Shutter Priority AE',
              768: 'Manual Exposure'}),
    0x1100: ('MotorOrBracket',
             {0: 'Off',
              1: 'On'}),
    0x1300: ('BlurWarning',
             {0: 'Off',
              1: 'On'}),
    0x1301: ('FocusWarning',
             {0: 'Off',
              1: 'On'}),
    0x1302: ('AEWarning',
             {0: 'Off',
              1: 'On'}),
    }

MAKERNOTE_CANON_TAGS = {
    0x0006: ('ImageType', ),
    0x0007: ('FirmwareVersion', ),
    0x0008: ('ImageNumber', ),
    0x0009: ('OwnerName', ),
    }

# this is in element offset, name, optional value dictionary format
MAKERNOTE_CANON_TAG_0x001 = {
    1: ('Macromode',
        {1: 'Macro',
         2: 'Normal'}),
    2: ('SelfTimer', ),
    3: ('Quality',
        {2: 'Normal',
         3: 'Fine',
         5: 'Superfine'}),
    4: ('FlashMode',
        {0: 'Flash Not Fired',
         1: 'Auto',
         2: 'On',
         3: 'Red-Eye Reduction',
         4: 'Slow Synchro',
         5: 'Auto + Red-Eye Reduction',
         6: 'On + Red-Eye Reduction',
         16: 'external flash'}),
    5: ('ContinuousDriveMode',
        {0: 'Single Or Timer',
         1: 'Continuous'}),
    7: ('FocusMode',
        {0: 'One-Shot',
         1: 'AI Servo',
         2: 'AI Focus',
         3: 'MF',
         4: 'Single',
         5: 'Continuous',
         6: 'MF'}),
    10: ('ImageSize',
         {0: 'Large',
          1: 'Medium',
          2: 'Small'}),
    11: ('EasyShootingMode',
         {0: 'Full Auto',
          1: 'Manual',
          2: 'Landscape',
          3: 'Fast Shutter',
          4: 'Slow Shutter',
          5: 'Night',
          6: 'B&W',
          7: 'Sepia',
          8: 'Portrait',
          9: 'Sports',
          10: 'Macro/Close-Up',
          11: 'Pan Focus'}),
    12: ('DigitalZoom',
         {0: 'None',
          1: '2x',
          2: '4x'}),
    13: ('Contrast',
         {0xFFFF: 'Low',
          0: 'Normal',
          1: 'High'}),
    14: ('Saturation',
         {0xFFFF: 'Low',
          0: 'Normal',
          1: 'High'}),
    15: ('Sharpness',
         {0xFFFF: 'Low',
          0: 'Normal',
          1: 'High'}),
    16: ('ISO',
         {0: 'See ISOSpeedRatings Tag',
          15: 'Auto',
          16: '50',
          17: '100',
          18: '200',
          19: '400'}),
    17: ('MeteringMode',
         {3: 'Evaluative',
          4: 'Partial',
          5: 'Center-weighted'}),
    18: ('FocusType',
         {0: 'Manual',
          1: 'Auto',
          3: 'Close-Up (Macro)',
          8: 'Locked (Pan Mode)'}),
    19: ('AFPointSelected',
         {0x3000: 'None (MF)',
          0x3001: 'Auto-Selected',
          0x3002: 'Right',
          0x3003: 'Center',
          0x3004: 'Left'}),
    20: ('ExposureMode',
         {0: 'Easy Shooting',
          1: 'Program',
          2: 'Tv-priority',
          3: 'Av-priority',
          4: 'Manual',
          5: 'A-DEP'}),
    23: ('LongFocalLengthOfLensInFocalUnits', ),
    24: ('ShortFocalLengthOfLensInFocalUnits', ),
    25: ('FocalUnitsPerMM', ),
    28: ('FlashActivity',
         {0: 'Did Not Fire',
          1: 'Fired'}),
    29: ('FlashDetails',
         {14: 'External E-TTL',
          13: 'Internal Flash',
          11: 'FP Sync Used',
          7: '2nd("Rear")-Curtain Sync Used',
          4: 'FP Sync Enabled'}),
    32: ('FocusMode',
         {0: 'Single',
          1: 'Continuous'}),
    }

MAKERNOTE_CANON_TAG_0x004 = {
    7: ('WhiteBalance',
        {0: 'Auto',
         1: 'Sunny',
         2: 'Cloudy',
         3: 'Tungsten',
         4: 'Fluorescent',
         5: 'Flash',
         6: 'Custom'}),
    9: ('SequenceNumber', ),
    14: ('AFPointUsed', ),
    15: ('FlashBias',
         {0xFFC0: '-2 EV',
          0xFFCC: '-1.67 EV',
          0xFFD0: '-1.50 EV',
          0xFFD4: '-1.33 EV',
          0xFFE0: '-1 EV',
          0xFFEC: '-0.67 EV',
          0xFFF0: '-0.50 EV',
          0xFFF4: '-0.33 EV',
          0x0000: '0 EV',
          0x000C: '0.33 EV',
          0x0010: '0.50 EV',
          0x0014: '0.67 EV',
          0x0020: '1 EV',
          0x002C: '1.33 EV',
          0x0030: '1.50 EV',
          0x0034: '1.67 EV',
          0x0040: '2 EV'}),
    19: ('SubjectDistance', ),
    }

# extract multibyte integer in Motorola format (little endian)
def s2n_motorola(str):
    x = 0
    for c in str:
        x = (x << 8) | ord(c)
    return x

# extract multibyte integer in Intel format (big endian)
def s2n_intel(str):
    x = 0
    y = 0L
    for c in str:
        x = x | (ord(c) << y)
        y = y + 8
    return x

# ratio object that eventually will be able to reduce itself to lowest
# common denominator for printing
def gcd(a, b):
    if b == 0:
        return a
    else:
        return gcd(b, a % b)

class Ratio:
    def __init__(self, num, den):
        self.num = num
        self.den = den

    def __repr__(self):
        self.reduce()
        if self.den == 1:
            return str(self.num)
        return '%d/%d' % (self.num, self.den)

    def reduce(self):
        div = gcd(self.num, self.den)
        if div > 1:
            self.num = self.num / div
            self.den = self.den / div

# for ease of dealing with tags
class IFD_Tag:
    def __init__(self, printable, tag, field_type, values, field_offset,
                 field_length):
        # printable version of data
        self.printable = printable
        # tag ID number
        self.tag = tag
        # field type as index into FIELD_TYPES
        self.field_type = field_type
        # offset of start of field in bytes from beginning of IFD
        self.field_offset = field_offset
        # length of data field in bytes
        self.field_length = field_length
        # either a string or array of data items
        self.values = values

    def __str__(self):
        return self.printable

    def __repr__(self):
        return '(0x%04X) %s=%s @ %d' % (self.tag,
                                        FIELD_TYPES[self.field_type][2],
                                        self.printable,
                                        self.field_offset)

# class that handles an EXIF header
class EXIF_header:
    def __init__(self, file, endian, offset, fake_exif, strict, debug=0):
        self.file = file
        self.endian = endian
        self.offset = offset
        self.fake_exif = fake_exif
        self.strict = strict
        self.debug = debug
        self.tags = {}

    # convert slice to integer, based on sign and endian flags
    # usually this offset is assumed to be relative to the beginning of the
    # start of the EXIF information.  For some cameras that use relative tags,
    # this offset may be relative to some other starting point.
    def s2n(self, offset, length, signed=0):
        self.file.seek(self.offset+offset)
        slice=self.file.read(length)
        if self.endian == 'I':
            val=s2n_intel(slice)
        else:
            val=s2n_motorola(slice)
        # Sign extension ?
        if signed:
            msb=1L << (8*length-1)
            if val & msb:
                val=val-(msb << 1)
        return val

    # convert offset to string
    def n2s(self, offset, length):
        s = ''
        for dummy in range(length):
            if self.endian == 'I':
                s = s + chr(offset & 0xFF)
            else:
                s = chr(offset & 0xFF) + s
            offset = offset >> 8
        return s

    # return first IFD
    def first_IFD(self):
        return self.s2n(4, 4)

    # return pointer to next IFD
    def next_IFD(self, ifd):
        entries=self.s2n(ifd, 2)
        return self.s2n(ifd+2+12*entries, 4)

    # return list of IFDs in header
    def list_IFDs(self):
        i=self.first_IFD()
        a=[]
        while i:
            a.append(i)
            i=self.next_IFD(i)
        return a

    # return list of entries in this IFD
    def dump_IFD(self, ifd, ifd_name, dict=EXIF_TAGS, relative=0, stop_tag='UNDEF'):
        entries=self.s2n(ifd, 2)
        for i in range(entries):
            # entry is index of start of this IFD in the file
            entry = ifd + 2 + 12 * i
            tag = self.s2n(entry, 2)

            # get tag name early to avoid errors, help debug
            tag_entry = dict.get(tag)
            if tag_entry:
                tag_name = tag_entry[0]
            else:
                tag_name = 'Tag 0x%04X' % tag

            # ignore certain tags for faster processing
            if not (not detailed and tag in IGNORE_TAGS):
                field_type = self.s2n(entry + 2, 2)

                # unknown field type
                if not 0 < field_type < len(FIELD_TYPES):
                    if not self.strict:
                        continue
                    else:
                        raise ValueError('unknown type %d in tag 0x%04X' % (field_type, tag))

                typelen = FIELD_TYPES[field_type][0]
                count = self.s2n(entry + 4, 4)
                # Adjust for tag id/type/count (2+2+4 bytes)
                # Now we point at either the data or the 2nd level offset
                offset = entry + 8

                # If the value fits in 4 bytes, it is inlined, else we
                # need to jump ahead again.
                if count * typelen > 4:
                    # offset is not the value; it's a pointer to the value
                    # if relative we set things up so s2n will seek to the right
                    # place when it adds self.offset.  Note that this 'relative'
                    # is for the Nikon type 3 makernote.  Other cameras may use
                    # other relative offsets, which would have to be computed here
                    # slightly differently.
                    if relative:
                        tmp_offset = self.s2n(offset, 4)
                        offset = tmp_offset + ifd - 8
                        if self.fake_exif:
                            offset = offset + 18
                    else:
                        offset = self.s2n(offset, 4)

                field_offset = offset
                if field_type == 2:
                    # special case: null-terminated ASCII string
                    # XXX investigate
                    # sometimes gets too big to fit in int value
                    if count != 0 and count < (2**31):
                        self.file.seek(self.offset + offset)
                        values = self.file.read(count)
                        #print values
                        # Drop any garbage after a null.
                        values = values.split('\x00', 1)[0]
                    else:
                        values = ''
                else:
                    values = []
                    signed = (field_type in [6, 8, 9, 10])

                    # XXX investigate
                    # some entries get too big to handle could be malformed
                    # file or problem with self.s2n
                    if count < 1000:
                        for dummy in range(count):
                            if field_type in (5, 10):
                                # a ratio
                                value = Ratio(self.s2n(offset, 4, signed),
                                              self.s2n(offset + 4, 4, signed))
                            else:
                                value = self.s2n(offset, typelen, signed)
                            values.append(value)
                            offset = offset + typelen
                    # The test above causes problems with tags that are
                    # supposed to have long values!  Fix up one important case.
                    elif tag_name == 'MakerNote' :
                        for dummy in range(count):
                            value = self.s2n(offset, typelen, signed)
                            values.append(value)
                            offset = offset + typelen
                    #else :
                    #    print "Warning: dropping large tag:", tag, tag_name

                # now 'values' is either a string or an array
                if count == 1 and field_type != 2:
                    printable=str(values[0])
                elif count > 50 and len(values) > 20 :
                    printable=str( values[0:20] )[0:-1] + ", ... ]"
                else:
                    printable=str(values)

                # compute printable version of values
                if tag_entry:
                    if len(tag_entry) != 1:
                        # optional 2nd tag element is present
                        if callable(tag_entry[1]):
                            # call mapping function
                            printable = tag_entry[1](values)
                        else:
                            printable = ''
                            for i in values:
                                # use lookup table for this tag
                                printable += tag_entry[1].get(i, repr(i))

                self.tags[ifd_name + ' ' + tag_name] = IFD_Tag(printable, tag,
                                                          field_type,
                                                          values, field_offset,
                                                          count * typelen)
                if self.debug:
                    print ' debug:   %s: %s' % (tag_name,
                                                repr(self.tags[ifd_name + ' ' + tag_name]))

            if tag_name == stop_tag:
                break

    # extract uncompressed TIFF thumbnail (like pulling teeth)
    # we take advantage of the pre-existing layout in the thumbnail IFD as
    # much as possible
    def extract_TIFF_thumbnail(self, thumb_ifd):
        entries = self.s2n(thumb_ifd, 2)
        # this is header plus offset to IFD ...
        if self.endian == 'M':
            tiff = 'MM\x00*\x00\x00\x00\x08'
        else:
            tiff = 'II*\x00\x08\x00\x00\x00'
        # ... plus thumbnail IFD data plus a null "next IFD" pointer
        self.file.seek(self.offset+thumb_ifd)
        tiff += self.file.read(entries*12+2)+'\x00\x00\x00\x00'

        # fix up large value offset pointers into data area
        for i in range(entries):
            entry = thumb_ifd + 2 + 12 * i
            tag = self.s2n(entry, 2)
            field_type = self.s2n(entry+2, 2)
            typelen = FIELD_TYPES[field_type][0]
            count = self.s2n(entry+4, 4)
            oldoff = self.s2n(entry+8, 4)
            # start of the 4-byte pointer area in entry
            ptr = i * 12 + 18
            # remember strip offsets location
            if tag == 0x0111:
                strip_off = ptr
                strip_len = count * typelen
            # is it in the data area?
            if count * typelen > 4:
                # update offset pointer (nasty "strings are immutable" crap)
                # should be able to say "tiff[ptr:ptr+4]=newoff"
                newoff = len(tiff)
                tiff = tiff[:ptr] + self.n2s(newoff, 4) + tiff[ptr+4:]
                # remember strip offsets location
                if tag == 0x0111:
                    strip_off = newoff
                    strip_len = 4
                # get original data and store it
                self.file.seek(self.offset + oldoff)
                tiff += self.file.read(count * typelen)

        # add pixel strips and update strip offset info
        old_offsets = self.tags['Thumbnail StripOffsets'].values
        old_counts = self.tags['Thumbnail StripByteCounts'].values
        for i in range(len(old_offsets)):
            # update offset pointer (more nasty "strings are immutable" crap)
            offset = self.n2s(len(tiff), strip_len)
            tiff = tiff[:strip_off] + offset + tiff[strip_off + strip_len:]
            strip_off += strip_len
            # add pixel strip to end
            self.file.seek(self.offset + old_offsets[i])
            tiff += self.file.read(old_counts[i])

        self.tags['TIFFThumbnail'] = tiff

    # decode all the camera-specific MakerNote formats

    # Note is the data that comprises this MakerNote.  The MakerNote will
    # likely have pointers in it that point to other parts of the file.  We'll
    # use self.offset as the starting point for most of those pointers, since
    # they are relative to the beginning of the file.
    #
    # If the MakerNote is in a newer format, it may use relative addressing
    # within the MakerNote.  In that case we'll use relative addresses for the
    # pointers.
    #
    # As an aside: it's not just to be annoying that the manufacturers use
    # relative offsets.  It's so that if the makernote has to be moved by the
    # picture software all of the offsets don't have to be adjusted.  Overall,
    # this is probably the right strategy for makernotes, though the spec is
    # ambiguous.  (The spec does not appear to imagine that makernotes would
    # follow EXIF format internally.  Once they did, it's ambiguous whether
    # the offsets should be from the header at the start of all the EXIF info,
    # or from the header at the start of the makernote.)
    def decode_maker_note(self):
        note = self.tags['EXIF MakerNote']

        # Some apps use MakerNote tags but do not use a format for which we
        # have a description, so just do a raw dump for these.
        #if self.tags.has_key('Image Make'):
        make = self.tags['Image Make'].printable
        #else:
        #    make = ''

        # model = self.tags['Image Model'].printable # unused

        # Nikon
        # The maker note usually starts with the word Nikon, followed by the
        # type of the makernote (1 or 2, as a short).  If the word Nikon is
        # not at the start of the makernote, it's probably type 2, since some
        # cameras work that way.
        if 'NIKON' in make:
            if note.values[0:7] == [78, 105, 107, 111, 110, 0, 1]:
                if self.debug:
                    print "Looks like a type 1 Nikon MakerNote."
                self.dump_IFD(note.field_offset+8, 'MakerNote',
                              dict=MAKERNOTE_NIKON_OLDER_TAGS)
            elif note.values[0:7] == [78, 105, 107, 111, 110, 0, 2]:
                if self.debug:
                    print "Looks like a labeled type 2 Nikon MakerNote"
                if note.values[12:14] != [0, 42] and note.values[12:14] != [42L, 0L]:
                    raise ValueError("Missing marker tag '42' in MakerNote.")
                # skip the Makernote label and the TIFF header
                self.dump_IFD(note.field_offset+10+8, 'MakerNote',
                              dict=MAKERNOTE_NIKON_NEWER_TAGS, relative=1)
            else:
                # E99x or D1
                if self.debug:
                    print "Looks like an unlabeled type 2 Nikon MakerNote"
                self.dump_IFD(note.field_offset, 'MakerNote',
                              dict=MAKERNOTE_NIKON_NEWER_TAGS)
            return

        # Olympus
        if make.startswith('OLYMPUS'):
            self.dump_IFD(note.field_offset+8, 'MakerNote',
                          dict=MAKERNOTE_OLYMPUS_TAGS)
            # XXX TODO
            #for i in (('MakerNote Tag 0x2020', MAKERNOTE_OLYMPUS_TAG_0x2020),):
            #    self.decode_olympus_tag(self.tags[i[0]].values, i[1])
            #return

        # Casio
        if 'CASIO' in make or 'Casio' in make:
            self.dump_IFD(note.field_offset, 'MakerNote',
                          dict=MAKERNOTE_CASIO_TAGS)
            return

        # Fujifilm
        if make == 'FUJIFILM':
            # bug: everything else is "Motorola" endian, but the MakerNote
            # is "Intel" endian
            endian = self.endian
            self.endian = 'I'
            # bug: IFD offsets are from beginning of MakerNote, not
            # beginning of file header
            offset = self.offset
            self.offset += note.field_offset
            # process note with bogus values (note is actually at offset 12)
            self.dump_IFD(12, 'MakerNote', dict=MAKERNOTE_FUJIFILM_TAGS)
            # reset to correct values
            self.endian = endian
            self.offset = offset
            return

        # Canon
        if make == 'Canon':
            self.dump_IFD(note.field_offset, 'MakerNote',
                          dict=MAKERNOTE_CANON_TAGS)
            for i in (('MakerNote Tag 0x0001', MAKERNOTE_CANON_TAG_0x001),
                      ('MakerNote Tag 0x0004', MAKERNOTE_CANON_TAG_0x004)):
                self.canon_decode_tag(self.tags[i[0]].values, i[1])
            return


    # XXX TODO decode Olympus MakerNote tag based on offset within tag
    def olympus_decode_tag(self, value, dict):
        pass

    # decode Canon MakerNote tag based on offset within tag
    # see http://www.burren.cx/david/canon.html by David Burren
    def canon_decode_tag(self, value, dict):
        for i in range(1, len(value)):
            x=dict.get(i, ('Unknown', ))
            if self.debug:
                print i, x
            name=x[0]
            if len(x) > 1:
                val=x[1].get(value[i], 'Unknown')
            else:
                val=value[i]
            # it's not a real IFD Tag but we fake one to make everybody
            # happy. this will have a "proprietary" type
            self.tags['MakerNote '+name]=IFD_Tag(str(val), None, 0, None,
                                                 None, None)

# process an image file (expects an open file object)
# this is the function that has to deal with all the arbitrary nasty bits
# of the EXIF standard
def process_file(f, stop_tag='UNDEF', details=True, strict=False, debug=False):
    # yah it's cheesy...
    global detailed
    detailed = details

    # by default do not fake an EXIF beginning
    fake_exif = 0

    # determine whether it's a JPEG or TIFF
    data = f.read(12)
    if data[0:4] in ['II*\x00', 'MM\x00*']:
        # it's a TIFF file
        f.seek(0)
        endian = f.read(1)
        f.read(1)
        offset = 0
    elif data[0:2] == '\xFF\xD8':
        # it's a JPEG file
        while data[2] == '\xFF' and data[6:10] in ('JFIF', 'JFXX', 'OLYM', 'Phot'):
            length = ord(data[4])*256+ord(data[5])
            f.read(length-8)
            # fake an EXIF beginning of file
            data = '\xFF\x00'+f.read(10)
            fake_exif = 1
        if data[2] == '\xFF' and data[6:10] == 'Exif':
            # detected EXIF header
            offset = f.tell()
            endian = f.read(1)
        else:
            # no EXIF information
            return {}
    else:
        # file format not recognized
        return {}

    # deal with the EXIF info we found
    if debug:
        print {'I': 'Intel', 'M': 'Motorola'}[endian], 'format'
    hdr = EXIF_header(f, endian, offset, fake_exif, strict, debug)
    ifd_list = hdr.list_IFDs()
    ctr = 0
    for i in ifd_list:
        if ctr == 0:
            IFD_name = 'Image'
        elif ctr == 1:
            IFD_name = 'Thumbnail'
            thumb_ifd = i
        else:
            IFD_name = 'IFD %d' % ctr
        if debug:
            print ' IFD %d (%s) at offset %d:' % (ctr, IFD_name, i)
        hdr.dump_IFD(i, IFD_name, stop_tag=stop_tag)
        # EXIF IFD
        exif_off = hdr.tags.get(IFD_name+' ExifOffset')
        if exif_off:
            if debug:
                print ' EXIF SubIFD at offset %d:' % exif_off.values[0]
            hdr.dump_IFD(exif_off.values[0], 'EXIF', stop_tag=stop_tag)
            # Interoperability IFD contained in EXIF IFD
            intr_off = hdr.tags.get('EXIF SubIFD InteroperabilityOffset')
            if intr_off:
                if debug:
                    print ' EXIF Interoperability SubSubIFD at offset %d:' \
                          % intr_off.values[0]
                hdr.dump_IFD(intr_off.values[0], 'EXIF Interoperability',
                             dict=INTR_TAGS, stop_tag=stop_tag)
        # GPS IFD
        gps_off = hdr.tags.get(IFD_name+' GPSInfo')
        if gps_off:
            if debug:
                print ' GPS SubIFD at offset %d:' % gps_off.values[0]
            hdr.dump_IFD(gps_off.values[0], 'GPS', dict=GPS_TAGS, stop_tag=stop_tag)
        ctr += 1

    # extract uncompressed TIFF thumbnail
    thumb = hdr.tags.get('Thumbnail Compression')
    if thumb and thumb.printable == 'Uncompressed TIFF':
        hdr.extract_TIFF_thumbnail(thumb_ifd)

    # JPEG thumbnail (thankfully the JPEG data is stored as a unit)
    thumb_off = hdr.tags.get('Thumbnail JPEGInterchangeFormat')
    if thumb_off:
        f.seek(offset+thumb_off.values[0])
        size = hdr.tags['Thumbnail JPEGInterchangeFormatLength'].values[0]
        hdr.tags['JPEGThumbnail'] = f.read(size)

    # deal with MakerNote contained in EXIF IFD
    # (Some apps use MakerNote tags but do not use a format for which we
    # have a description, do not process these).
    if 'EXIF MakerNote' in hdr.tags and 'Image Make' in hdr.tags and detailed:
        hdr.decode_maker_note()

    # Sometimes in a TIFF file, a JPEG thumbnail is hidden in the MakerNote
    # since it's not allowed in a uncompressed TIFF IFD
    if 'JPEGThumbnail' not in hdr.tags:
        thumb_off=hdr.tags.get('MakerNote JPEGThumbnail')
        if thumb_off:
            f.seek(offset+thumb_off.values[0])
            hdr.tags['JPEGThumbnail']=file.read(thumb_off.field_length)

    return hdr.tags


# show command line usage
def usage(exit_status):
    msg = 'Usage: EXIF.py [OPTIONS] file1 [file2 ...]\n'
    msg += 'Extract EXIF information from digital camera image files.\n\nOptions:\n'
    msg += '-q --quick   Do not process MakerNotes.\n'
    msg += '-t TAG --stop-tag TAG   Stop processing when this tag is retrieved.\n'
    msg += '-s --strict   Run in strict mode (stop on errors).\n'
    msg += '-d --debug   Run in debug mode (display extra info).\n'
    print msg
    sys.exit(exit_status)

# library test/debug function (dump given files)
if __name__ == '__main__':
    import sys
    import getopt

    # parse command line options/arguments
    try:
        opts, args = getopt.getopt(sys.argv[1:], "hqsdt:v", ["help", "quick", "strict", "debug", "stop-tag="])
    except getopt.GetoptError:
        usage(2)
    if args == []:
        usage(2)
    detailed = True
    stop_tag = 'UNDEF'
    debug = False
    strict = False
    for o, a in opts:
        if o in ("-h", "--help"):
            usage(0)
        if o in ("-q", "--quick"):
            detailed = False
        if o in ("-t", "--stop-tag"):
            stop_tag = a
        if o in ("-s", "--strict"):
            strict = True
        if o in ("-d", "--debug"):
            debug = True

    # output info for each file
    for filename in args:
        try:
            file=open(filename, 'rb')
        except:
            print "'%s' is unreadable\n"%filename
            continue
        print filename + ':'
        # get the tags
        data = process_file(file, stop_tag=stop_tag, details=detailed, strict=strict, debug=debug)
        if not data:
            print 'No EXIF information found'
            continue

        x=data.keys()
        x.sort()
        for i in x:
            if i in ('JPEGThumbnail', 'TIFFThumbnail'):
                continue
            try:
                print '   %s (%s): %s' % \
                      (i, FIELD_TYPES[data[i].field_type][2], data[i].printable)
            except:
                print 'error', i, '"', data[i], '"'
        if 'JPEGThumbnail' in data:
            print 'File has JPEG thumbnail'
        print

########NEW FILE########
__FILENAME__ = Blob
from SimpleCV.base import *
from SimpleCV.Features.Features import Feature, FeatureSet
from SimpleCV.Color import Color
from SimpleCV.ImageClass import Image
from SimpleCV.Features.Detection import ShapeContextDescriptor
import math
import scipy.stats as sps

class Blob(Feature):
    """
    **SUMMARY**

    A blob is a typicall a cluster of pixels that form a feature or unique
    shape that allows it to be distinguished from the rest of the image
    Blobs typically are computed very quickly so they are used often to
    find various items in a picture based on properties.  Typically these
    things like color, shape, size, etc.   Since blobs are computed quickly
    they are typically used to narrow down search regions in an image, where
    you quickly find a blob and then that blobs region is used for more
    computational intensive type image processing.

    **EXAMPLE**

    >>> img = Image("lenna")
    >>> blobs = img.findBlobs()
    >>> blobs[-1].draw()
    >>> img.show()

    **SEE ALSO**
    :py:meth:`findBlobs`
    :py:class:`BlobMaker`
    :py:meth:`findBlobsFromMask`

    """
    seq = '' #the cvseq object that defines this blob
    mContour = [] # the blob's outer perimeter as a set of (x,y) tuples
    mConvexHull = [] # the convex hull contour as a set of (x,y) tuples
    mMinRectangle = [] #the smallest box rotated to fit the blob
    # mMinRectangle[0] = centroid (x,y)
    # mMinRectangle[1] = (w,h)
    # mMinRectangle[2] = angle

    #mBoundingBox = [] #get W/H and X/Y from this
    mHu = [] # The seven Hu Moments
    mPerimeter = 0 # the length of the perimeter in pixels
    mArea = 0 # the area in pixels
    m00 = 0
    m01 = 0
    m10 = 0
    m11 = 0
    m20 = 0
    m02 = 0
    m21 = 0
    m12 = 0
    mContourAppx = None
    mLabel = "" # A user label
    mLabelColor = [] # what color to draw the label
    mAvgColor = []#The average color of the blob's area.
    #mImg =  '' #Image()# the segmented image of the blob
    #mHullImg = '' # Image() the image from the hull.
    #mMask = '' #Image()# A mask of the blob area
    #xmHullMask = '' #Image()#A mask of the hull area ... we may want to use this for the image mask.
    mHoleContour = []  # list of hole contours
    #mVertEdgeHist = [] #vertical edge histogram
    #mHortEdgeHist = [] #horizontal edge histgram
    pickle_skip_properties = set(
        ('mImg', 'mHullImg', 'mMask', 'mHullMask'))

    def __init__(self):
        self._scdescriptors = None
        self.mContour = []
        self.mConvexHull = []
        self.mMinRectangle = [-1,-1,-1,-1,-1] #angle from this
        self.mContourAppx = []
        self.mHu = [-1,-1,-1,-1,-1,-1,-1]
        self.mPerimeter = 0
        self.mArea = 0
        self.m00 = 0
        self.m01 = 0
        self.m10 = 0
        self.m11 = 0
        self.m20 = 0
        self.m02 = 0
        self.m21 = 0
        self.m12 = 0
        self.mLabel = "UNASSIGNED"
        self.mLabelColor = []
        self.mAvgColor = [-1,-1,-1]
        self.image = None
        self.mHoleContour = []
        self.points = []
        #TODO
        # I would like to clean up the Hull mask parameters
        # it seems to me that we may want the convex hull to be
        # the default way we calculate for area.

    def __getstate__(self):
        skip = self.pickle_skip_properties
        newdict = {}
        for k,v in self.__dict__.items():
            if k in skip:
                continue
            else:
                newdict[k] = v
        return newdict


    def __setstate__(self, mydict):
        iplkeys = []
        for k in mydict:
            if re.search("__string", k):
                iplkeys.append(k)
            else:
                self.__dict__[k] = mydict[k]

        #once we get all the metadata loaded, go for the bitmaps
        for k in iplkeys:
            realkey = k[:-len("__string")]
            self.__dict__[realkey] = cv.CreateImageHeader((self.width(), self.height()), cv.IPL_DEPTH_8U, 1)
            cv.SetData(self.__dict__[realkey], mydict[k])

    def perimeter(self):
        """
        **SUMMARY**

        This function returns the perimeter as an integer number of pixel lengths.

        **RETURNS**

        Integer

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> print blobs[-1].perimeter()

        """

        return self.mPerimeter

    def hull(self):
        """
        **SUMMARY**

        This function returns the convex hull points as a list of x,y tuples.

        **RETURNS**

        A list of x,y tuples.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> print blobs[-1].hull()

        """
        return self.mConvexHull

    def contour(self):
        """
        **SUMMARY**

        This function returns the contour points as a list of x,y tuples.

        **RETURNS**

        A list of x,y tuples.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> print blobs[-1].contour()

        """

        return self.mContour

    def meanColor(self):
        """
        **SUMMARY**

        This function returns a tuple representing the average color of the blob.

        **RETURNS**

        A RGB triplet of the average blob colors.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> print blobs[-1].meanColor()

        """
        #print self.mBoundingBox
        hack = (self.mBoundingBox[0],self.mBoundingBox[1],self.mBoundingBox[2],self.mBoundingBox[3])
        cv.SetImageROI(self.image.getBitmap(),hack)
        #may need the offset paramete
        avg = cv.Avg(self.image.getBitmap(),self.mMask._getGrayscaleBitmap())
        cv.ResetImageROI(self.image.getBitmap())

        return tuple(reversed(avg[0:3]))

    def area(self):
        """
        **SUMMARY**

        This method returns the area of the blob in terms of the number of
        pixels inside the contour.

        **RETURNS**

        An integer of the area of the blob in pixels.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> print blobs[-1].area()
        >>> print blobs[0].area()

        """
        return(self.mArea)


    def minRect(self):
        """
        Returns the corners for the smallest rotated rectangle to enclose the blob.
        The points are returned as a list of  (x,y) tupples.
        """
        #if( self.mMinRectangle[1][0] < self.mMinRectangle[1][1]):
        ang = self.mMinRectangle[2]
        #else:
        #    ang =  90 + self.mMinRectangle[2]
        ang = 2*pi*(float(ang)/360.00)
        tx = self.minRectX()
        ty = self.minRectY()
        w = self.minRectWidth()/2.0
        h = self.minRectHeight()/2.0
        derp = np.matrix([[cos(ang),-1*sin(ang),tx],[sin(ang),cos(ang),ty],[0,0,1]])
        # [ cos a , -sin a, tx ]
        # [ sin a , cos a , ty ]
        # [ 0     , 0     ,  1 ]
        tl = np.matrix([-1.0*w,h,1.0]) #Kat gladly supports homo. coordinates.
        tr = np.matrix([w,h,1.0])
        bl = np.matrix([-1.0*w,-1.0*h,1.0])
        br = np.matrix([w,-1.0*h,1.0])
        tlp = derp*tl.transpose()
        trp = derp*tr.transpose()
        blp = derp*bl.transpose()
        brp = derp*br.transpose()
        return( (float(tlp[0,0]),float(tlp[1,0])),(float(trp[0,0]),float(trp[1,0])),(float(blp[0,0]),float(blp[1,0])),(float(brp[0,0]),float(brp[1,0])) )

    def drawRect(self,layer=None,color=Color.DEFAULT,width=1,alpha=128):
        """
        **SUMMARY**

        Draws the bounding rectangle for the blob.

        **PARAMETERS**

        * *color* - The color to render the blob's box.
        * *alpha* - The alpha value of the rendered blob 0 = transparent 255 = opaque.
        * *width* - The width of the drawn blob in pixels
        * *layer* - if layer is not None, the blob is rendered to the layer versus the source image.

        **RETURNS**

        Returns None, this operation works on the supplied layer or the source image.


        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> blobs[-1].drawRect(color=Color.RED, width=-1,alpha=128)
        >>> img.show()

        """
        if( layer is None ):
            layer = self.image.dl()

        if( width < 1 ):
            layer.rectangle(self.topLeftCorner(),(self.width(),self.height()),color,width,filled=True,alpha=alpha)
        else:
            layer.rectangle(self.topLeftCorner(),(self.width(),self.height()),color,width,filled=False,alpha=alpha)


    def drawMinRect(self,layer=None,color=Color.DEFAULT,width=1,alpha=128):
        """
        **SUMMARY**

        Draws the minimum bounding rectangle for the blob. The minimum bounding rectangle is the smallest
        rotated rectangle that can enclose the blob.

        **PARAMETERS**

        * *color* - The color to render the blob's box.
        * *alpha* - The alpha value of the rendered blob 0 = transparent 255 = opaque.
        * *width* - The width of the drawn blob in pixels
        * *layer* - If layer is not None, the blob is rendered to the layer versus the source image.

        **RETURNS**

        Returns none, this operation works on the supplied layer or the source image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> for b in blobs:
        >>>      b.drawMinRect(color=Color.RED, width=-1,alpha=128)
        >>> img.show()

        """
        if( layer is None ):
            layer = self.image.dl()
        (tl,tr,bl,br) = self.minRect()
        layer.line(tl,tr,color,width=width,alpha=alpha,antialias = False)
        layer.line(bl,br,color,width=width,alpha=alpha,antialias = False)
        layer.line(tl,bl,color,width=width,alpha=alpha,antialias = False)
        layer.line(tr,br,color,width=width,alpha=alpha,antialias = False)

    def angle(self):
        """
        **SUMMARY**

        This method returns the angle between the horizontal and the minimum enclosing
        rectangle of the blob. The minimum enclosing rectangle IS NOT not the bounding box.
        Use the bounding box for situations where you need only an approximation of the objects
        dimensions. The minimum enclosing rectangle is slightly harder to maninpulate but
        gives much better information about the blobs dimensions.

        **RETURNS**

        Returns the angle between the minimum bounding rectangle and the horizontal.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> blob[-1].angle()

        """
        #return self.mMinRectangle[2]+90.00
        retVal = 0.00
        if self.mMinRectangle[1][0] < self.mMinRectangle[1][1]:
            retVal = self.mMinRectangle[2]
        else:
            retVal = 90.00 + self.mMinRectangle[2]
        retVal = retVal + 90.00
        if( retVal > 90.00 ):
            retVal = -180.00 + retVal
        return retVal

    def minRectX(self):
        """
        **SUMMARY**

        This is the x coordinate of the centroid for the minimum bounding rectangle

        **RETURNS**

        An integer that is the x position of the centrod of the minimum bounding rectangle.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> print blobs[-1].minRectX()

        """
        return(self.mMinRectangle[0][0])

    def minRectY(self):
        """
        **SUMMARY**

        This is the y coordinate of the centroid for the minimum bounding rectangle

        **RETURNS**

        An integer that is the y position of the centrod of the minimum bounding rectangle.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> print blobs[-1].minRectY()

        """
        return(self.mMinRectangle[0][1])

    def minRectWidth(self):
        """
        **SUMMARY**

        This is the width of the minimum bounding rectangle for the blob.

        **RETURNS**

        An integer that is the width of the minimum bounding rectangle for this blob.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> print blobs[-1].minRectWidth()

        """
        return(self.mMinRectangle[1][0])

    def minRectHeight(self):
        """
        **SUMMARY**

        This is the height, in pixels, of the minimum bounding rectangle.

        **RETURNS**

        An integer that is the height of the minimum bounding rectangle for this blob.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> print blobs[-1].minRectHeight()


        """
        return(self.mMinRectangle[1][1])

    def rectifyMajorAxis(self,axis=0):
        """
        **SUMMARY**

        Rectify the blob image and the contour such that the major
        axis is aligned to either horizontal=0 or vertical=1. This is to say, we take the blob,
        find the longest axis, and rotate the blob such that the axis is either vertical or horizontal.

        **PARAMETERS**

        * *axis* - if axis is zero we rotate the blobs to fit along the vertical axis, otherwise we use the horizontal axis.

        **RETURNS**

        This method works in place, i.e. it rotates the blob's internal data structures. This method is experimetnal.
        Use at your own risk.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> blobs[-2].mImg.show()
        >>> blobs[-2].rectifyToMajorAxis(1)
        >>> blobs[-2].mImg.show()

        """
        finalRotation = self.angle()
        w = self.minRectWidth()
        h = self.minRectHeight()

        if( w > h ):
            finalRotation = finalRotation

        if(axis > 0 ):
            finalRotation = finalRotation - 90

        self.rotate(finalRotation)
        return None

    def rotate(self,angle):
        """
        **SUMMARY**

        Rotate the blob given an  angle in degrees. If you use this method
        most of the blob elements will
        be rotated in place , however, this will "break" drawing back to the original image.
        To draw the blob create a new layer and draw to that layer. Positive rotations
        are counter clockwise.

        **PARAMETERS**

        * *angle* - A floating point angle in degrees. Positive is anti-clockwise.

        **RETURNS**

        .. Warning:
          Nothing. All rotations are performed in place. This modifies the blob's data
          and will break any image write back capabilities.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> blobs[-2].mImg.show()
        >>> blobs[-2].rotate(90)
        >>> blobs[-2].mImg.show()

        """
        #FIXME: This function should return a blob
        theta = 2*np.pi*(angle/360.0)
        mode = ""
        point =(self.x,self.y)
        self.mImg = self.mImg.rotate(angle,mode,point)
        self.mHullImg = self.mHullImg.rotate(angle,mode,point)
        self.mMask = self.mMask.rotate(angle,mode,point)
        self.mHullMask = self.mHullMask.rotate(angle,mode,point)

        self.mContour = map(lambda x:
                            (x[0]*np.cos(theta)-x[1]*np.sin(theta),
                             x[0]*np.sin(theta)+x[1]*np.cos(theta)),
                             self.mContour)
        self.mConvexHull = map(lambda x:
                               (x[0]*np.cos(theta)-x[1]*np.sin(theta),
                                x[0]*np.sin(theta)+x[1]*np.cos(theta)),
                               self.mConvexHull)

        if( self.mHoleContour is not None):
            for h in self.mHoleContour:
                h = map(lambda x:
                    (x[0]*np.cos(theta)-x[1]*np.sin(theta),
                     x[0]*np.sin(theta)+x[1]*np.cos(theta)),
                     h)


    def drawAppx(self, color = Color.HOTPINK,width=-1,alpha=-1,layer=None):
        if( self.mContourAppx is None or len(self.mContourAppx)==0 ):
            return

        if not layer:
            layer = self.image.dl()

        if width < 1:
            layer.polygon(self.mContourAppx,color,width,True,True,alpha)
        else:
            layer.polygon(self.mContourAppx,color,width,False,True,alpha)

    def draw(self, color = Color.GREEN, width=-1, alpha=-1, layer=None):
        """
        **SUMMARY**

        Draw the blob, in the given color, to the appropriate layer

        By default, this draws the entire blob filled in, with holes.  If you
        provide a width, an outline of the exterior and interior contours is drawn.

        **PARAMETERS**

        * *color* -The color to render the blob as a color tuple.
        * *alpha* - The alpha value of the rendered blob 0=transparent 255=opaque.
        * *width* - The width of the drawn blob in pixels, if -1 then filled then the polygon is filled.
        * *layer* - A source layer, if layer is not None, the blob is rendered to the layer versus the source image.

        **RETURNS**

        This method either works on the original source image, or on the drawing layer provided.
        The method does not modify object itself.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> blobs[-2].draw(color=Color.PUCE,width=-1,alpha=128)
        >>> img.show()

        """
        if not layer:
            layer = self.image.dl()

        if width == -1:
            #copy the mask into 3 channels and multiply by the appropriate color
            maskred = cv.CreateImage(cv.GetSize(self.mMask._getGrayscaleBitmap()), cv.IPL_DEPTH_8U, 1)
            maskgrn = cv.CreateImage(cv.GetSize(self.mMask._getGrayscaleBitmap()), cv.IPL_DEPTH_8U, 1)
            maskblu = cv.CreateImage(cv.GetSize(self.mMask._getGrayscaleBitmap()), cv.IPL_DEPTH_8U, 1)

            maskbit = cv.CreateImage(cv.GetSize(self.mMask._getGrayscaleBitmap()), cv.IPL_DEPTH_8U, 3)

            cv.ConvertScale(self.mMask._getGrayscaleBitmap(), maskred, color[0] / 255.0)
            cv.ConvertScale(self.mMask._getGrayscaleBitmap(), maskgrn, color[1] / 255.0)
            cv.ConvertScale(self.mMask._getGrayscaleBitmap(), maskblu, color[2] / 255.0)

            cv.Merge(maskblu, maskgrn, maskred, None, maskbit)

            masksurface = Image(maskbit).getPGSurface()
            masksurface.set_colorkey(Color.BLACK)
            if alpha != -1:
                masksurface.set_alpha(alpha)
            layer._mSurface.blit(masksurface, self.topLeftCorner()) #KAT HERE
        else:
            self.drawOutline(color, alpha, width, layer)
            self.drawHoles(color, alpha, width, layer)


    def drawOutline(self, color=Color.GREEN, alpha=255, width=1, layer=None):
        """
        **SUMMARY**

        Draw the blob contour the provided layer -- if no layer is provided, draw
        to the source image.


        **PARAMETERS**

        * *color* - The color to render the blob.
        * *alpha* - The alpha value of the rendered poly.
        * *width* - The width of the drawn blob in pixels, -1 then the polygon is filled.
        * *layer* - if layer is not None, the blob is rendered to the layer versus the source image.


        **RETURNS**

        This method either works on the original source image, or on the drawing layer provided.
        The method does not modify object itself.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> blobs[-2].drawOutline(color=Color.GREEN,width=3,alpha=128)
        >>> img.show()


        """

        if( layer is None ):
            layer = self.image.dl()

        if( width < 0 ):
            #blit the blob in
            layer.polygon(self.mContour,color,filled=True,alpha=alpha)
        else:
            lastp = self.mContour[0] #this may work better.... than the other
            for nextp in self.mContour[1::]:
                layer.line(lastp,nextp,color,width=width,alpha=alpha,antialias = False)
                lastp = nextp
            layer.line(self.mContour[0],self.mContour[-1],color,width=width,alpha=alpha, antialias = False)


    def drawHoles(self, color=Color.GREEN, alpha=-1, width=-1, layer=None):
        """
        **SUMMARY**

        This method renders all of the holes (if any) that are present in the blob.

        **PARAMETERS**

        * *color* - The color to render the blob's holes.
        * *alpha* - The alpha value of the rendered blob hole.
        * *width* - The width of the drawn blob hole in pixels, if w=-1 then the polygon is filled.
        * *layer* - If layer is not None, the blob is rendered to the layer versus the source image.

        **RETURNS**

        This method either works on the original source image, or on the drawing layer provided.
        The method does not modify object itself.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs(128)
        >>> blobs[-1].drawHoles(color=Color.GREEN,width=3,alpha=128)
        >>> img.show()

        """
        if(self.mHoleContour is None):
            return
        if( layer is None ):
            layer = self.image.dl()

        if( width < 0 ):
            #blit the blob in
            for h in self.mHoleContour:
                layer.polygon(h,color,filled=True,alpha=alpha)
        else:
            for h in self.mHoleContour:
                lastp = h[0] #this may work better.... than the other
                for nextp in h[1::]:
                    layer.line((int(lastp[0]),int(lastp[1])),(int(nextp[0]),int(nextp[1])),color,width=width,alpha=alpha,antialias = False)
                    lastp = nextp
                layer.line(h[0],h[-1],color,width=width,alpha=alpha, antialias = False)

    def drawHull(self, color=Color.GREEN, alpha=-1, width=-1, layer=None ):
        """
        **SUMMARY**

        Draw the blob's convex hull to either the source image or to the
        specified layer given by layer.

        **PARAMETERS**

        * *color* - The color to render the blob's convex hull as an RGB triplet.
        * *alpha* - The alpha value of the rendered blob.
        * *width* - The width of the drawn blob in pixels, if w=-1 then the polygon is filled.
        * *layer* - if layer is not None, the blob is rendered to the layer versus the source image.

        **RETURNS**

        This method either works on the original source image, or on the drawing layer provided.
        The method does not modify object itself.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs(128)
        >>> blobs[-1].drawHull(color=Color.GREEN,width=3,alpha=128)
        >>> img.show()

        """
        if( layer is None ):
            layer = self.image.dl()

        if( width < 0 ):
            #blit the blob in
            layer.polygon(self.mConvexHull,color,filled=True,alpha=alpha)
        else:
            lastp = self.mConvexHull[0] #this may work better.... than the other
            for nextp in self.mConvexHull[1::]:
                layer.line(lastp,nextp,color,width=width,alpha=alpha,antialias = False)
                lastp = nextp
            layer.line(self.mConvexHull[0],self.mConvexHull[-1],color,width=width,alpha=alpha, antialias = False)


    #draw the actual pixels inside the contour to the layer
    def drawMaskToLayer(self, layer = None, offset=(0,0)):
        """
        **SUMMARY**

        Draw the actual pixels of the blob to another layer. This is handy if you
        want to examine just the pixels inside the contour.

        **PARAMETERS**

        * *layer* - A drawing layer upon which to apply the mask.
        * *offset* -  The offset from the top left corner where we want to place the mask.

        **RETURNS**

        This method either works on the original source image, or on the drawing layer provided.
        The method does not modify object itself.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs(128)
        >>> dl = DrawingLayer((img.width,img.height))
        >>> blobs[-1].drawMaskToLayer(layer = dl)
        >>> dl.show()

        """
        if( layer is not None ):
            layer = self.image.dl()

        mx = self.mBoundingBox[0]+offset[0]
        my = self.mBoundingBox[1]+offset[1]
        layer.blit(self.mImg,coordinates=(mx,my))
        return None

    def isSquare(self, tolerance = 0.05, ratiotolerance = 0.05):
        """
        **SUMMARY**

        Given a tolerance, test if the blob is a rectangle, and how close its
        bounding rectangle's aspect ratio is to 1.0.

        **PARAMETERS**

        * *tolerance* - A percentage difference between an ideal rectangle and our hull mask.
        * *ratiotolerance* - A percentage difference of the aspect ratio of our blob and an ideal square.

        **RETURNS**

        Boolean True if our object falls within tolerance, false otherwise.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs(128)
        >>> if(blobs[-1].isSquare() ):
        >>>     print "it is hip to be square."

        """
        if self.isRectangle(tolerance) and abs(1 - self.aspectRatio()) < ratiotolerance:
            return True
        return False


    def isRectangle(self, tolerance = 0.05):
        """
        **SUMMARY**

        Given a tolerance, test the blob against the rectangle distance to see if
        it is rectangular.

        **PARAMETERS**

        * *tolerance* - The percentage difference between our blob and its idealized bounding box.

        **RETURNS**

        Boolean True if the blob is withing the rectangle tolerage, false otherwise.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs(128)
        >>> if(blobs[-1].isRecangle() ):
        >>>     print "it is hip to be square."

        """
        if self.rectangleDistance() < tolerance:
            return True
        return False

    def rectangleDistance(self):
        """
        **SUMMARY**

        This compares the hull mask to the bounding rectangle.  Returns the area
        of the blob's hull as a fraction of the bounding rectangle.

        **RETURNS**

        The number of pixels in the blobs hull mask over the number of pixels in its bounding box.

        """
        blackcount, whitecount = self.mHullMask.histogram(2)
        return abs(1.0 - float(whitecount) / (self.minRectWidth() * self.minRectHeight()))


    def isCircle(self, tolerance = 0.05):
        """
        **SUMMARY**

        Test circle distance against a tolerance to see if the blob is circlular.

        **PARAMETERS**

        * *tolerance* - the percentage difference between our blob and an ideal circle.

        **RETURNS**

        True if the feature is within tolerance for being a circle, false otherwise.

        """
        if self.circleDistance() < tolerance:
            return True
        return False

    def circleDistance(self):
        """
        **SUMMARY**

        Compare the hull mask to an ideal circle and count the number of pixels
        that deviate as a fraction of total area of the ideal circle.

        **RETURNS**

        The difference, as a percentage, between the hull of our blob and an idealized
        circle of our blob.

        """
        w = self.mHullMask.width
        h = self.mHullMask.height

        idealcircle = Image((w,h))
        radius = min(w,h) / 2
        idealcircle.dl().circle((w/2, h/2), radius, filled= True, color=Color.WHITE)
        idealcircle = idealcircle.applyLayers()
        netdiff = (idealcircle - self.mHullMask) + (self.mHullMask - idealcircle)
        numblack, numwhite = netdiff.histogram(2)
        return float(numwhite) / (radius * radius * np.pi)

    def centroid(self):
        """
        **SUMMARY**

        Return the centroid (mass-determined center) of the blob. Note that this is differnt from the bounding box center.

        **RETURNS**

        An (x,y) tuple that is the center of mass of the blob.

        **EXAMPLE**
        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> img.drawCircle((blobs[-1].x,blobs[-1].y),10,color=Color.RED)
        >>> img.drawCircle((blobs[-1].centroid()),10,color=Color.BLUE)
        >>> img.show()

        """
        return (self.m10 / self.m00, self.m01 / self.m00)

    def radius(self):
        """
        **SUMMARY**

        Return the radius, the avg distance of each contour point from the centroid
        """
        return float(np.mean(spsd.cdist(self.mContour, [self.centroid()])))

    def hullRadius(self):
        """
        **SUMMARY**

        Return the radius of the convex hull contour from the centroid
        """
        return float(np.mean(spsd.cdist(self.mConvexHull, [self.centroid()])))

    @LazyProperty
    def mImg(self):
        #NOTE THAT THIS IS NOT PERFECT - ISLAND WITH A LAKE WITH AN ISLAND WITH A LAKE STUFF
        retVal = cv.CreateImage((self.width(),self.height()),cv.IPL_DEPTH_8U,3)
        cv.Zero(retVal)
        bmp = self.image.getBitmap()
        mask = self.mMask.getBitmap()
        tl = self.topLeftCorner()
        cv.SetImageROI(bmp,(tl[0],tl[1], self.width(),self.height()))
        cv.Copy(bmp,retVal,mask)
        cv.ResetImageROI(bmp)
        return Image(retVal)

    @LazyProperty
    def mMask(self):
        # TODO: FIX THIS SO THAT THE INTERIOR CONTOURS GET SHIFTED AND DRAWN

        #Alas - OpenCV does not provide an offset in the fillpoly method for
        #the cv bindings (only cv2 -- which I am trying to avoid). Have to
        #manually do the offset for the ROI shift.

        retVal = cv.CreateImage((self.width(),self.height()),cv.IPL_DEPTH_8U,1)
        cv.Zero(retVal)
        l,t = self.topLeftCorner()

        # construct the exterior contour - these are tuples

        cv.FillPoly(retVal,[[(p[0] - l, p[1] - t) for p in self.mContour]],(255,255,255),8)

        #construct the hole contoursb
        holes = []
        if self.mHoleContour is not None:
            for h in self.mHoleContour: # -- these are lists
                holes.append([(h2[0]-l,h2[1]-t) for h2 in h])

            cv.FillPoly(retVal,holes,(0,0,0),8)
        return Image(retVal)


    @LazyProperty
    def mHullImg(self):
        retVal = cv.CreateImage((self.width(),self .height()),cv.IPL_DEPTH_8U,3)
        cv.Zero(retVal)
        bmp = self.image.getBitmap()
        mask = self.mHullMask.getBitmap()
        tl = self.topLeftCorner()
        cv.SetImageROI(bmp,(tl[0],tl[1], self.width(),self.height()))
        cv.Copy(bmp,retVal,mask)
        cv.ResetImageROI(bmp)
        return Image(retVal)


    @LazyProperty
    def mHullMask(self):
        retVal = cv.CreateImage((self.width(),self.height()),cv.IPL_DEPTH_8U,3)
        cv.Zero(retVal)
        #Alas - OpenCV does not provide an offset in the fillpoly method for
        #the cv bindings (only cv2 -- which I am trying to avoid). Have to
        #manually do the offset for the ROI shift.
        thull = []
        l,t = self.topLeftCorner()
        cv.FillPoly(retVal,[[(p[0] - l, p[1] - t) for p in self.mConvexHull]],(255,255,255),8)
        return Image(retVal)


    def hullImage(self):
        """
        **SUMMARY**

        The convex hull of a blob is the shape that would result if you snapped a rubber band around
        the blob. So if you had the letter "C" as your blob the convex hull would be the letter "O."
        This method returns an image where the source image around the convex hull of the blob is copied
        ontop a black background.

        **RETURNS**
        Returns a SimpleCV Image of the convex hull, cropped to fit.

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> blobs[-1].hullImage().show()

        """
        return self.mHullImg

    def hullMask(self):
        """
        **SUMMARY**

        The convex hull of a blob is the shape that would result if you snapped a rubber band around
        the blob. So if you had the letter "C" as your blob the convex hull would be the letter "O."
        This method returns an image where the area of the convex hull is white and the rest of the image
        is black. This image is cropped to the size of the blob.

        **RETURNS**

        Returns a binary SimpleCV image of the convex hull mask, cropped to fit the blob.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> blobs[-1].hullMask().show()

        """
        return self.mHullMask

    def blobImage(self):
        """
        **SUMMARY**

        This method automatically copies all of the image data around the blob and puts it in a new
        image. The resulting image has the size of the blob, with the blob data copied in place.
        Where the blob is not present the background is black.

        **RETURNS**

        Returns just the image of the blob (cropped to fit).

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> blobs[-1].blobImage().show()


        """
        return self.mImg

    def blobMask(self):
        """
        **SUMMARY**

        This method returns an image of the blob's mask. Areas where the blob are present are white
        while all other areas are black. The image is cropped to match the blob area.

        **RETURNS**

        Returns a SimplecV image of the blob's mask, cropped to fit.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blobs = img.findBlobs()
        >>> blobs[-1].blobMask().show()



        """
        return self.mMask

    def match(self, otherblob):
        """
        **SUMMARY**

        Compare the Hu moments between two blobs to see if they match.  Returns
        a comparison factor -- lower numbers are a closer match.

        **PARAMETERS**

        * *otherblob* - The other blob to compare this one to.

        **RETURNS**

        A single floating point value that is the match quality.

        **EXAMPLE**

        >>> cam = Camera()
        >>> img1 = cam.getImage()
        >>> img2 = cam.getImage()
        >>> b1 = img1.findBlobs()
        >>> b2 = img2.findBlobs()
        >>> for ba in b1:
        >>>     for bb in b2:
        >>>         print ba.match(bb)

        """
        #note: this should use cv.MatchShapes -- but that seems to be
        #broken in OpenCV 2.2  Instead, I reimplemented in numpy
        #according to the description in the docs for method I1 (reciprocal log transformed abs diff)
        #return cv.MatchShapes(self.seq, otherblob.seq, cv.CV_CONTOURS_MATCH_I1)

        mySigns = np.sign(self.mHu)
        myLogs = np.log(np.abs(self.mHu))
        myM = mySigns * myLogs

        otherSigns = np.sign(otherblob.mHu)
        otherLogs = np.log(np.abs(otherblob.mHu))
        otherM = otherSigns * otherLogs

        return np.sum(abs((1/ myM - 1/ otherM)))
        
    def getMaskedImage(self):
        """
        Get the blob size image with the masked blob 
        """
        retVal = cv.CreateImage((self.width(),self .height()),cv.IPL_DEPTH_8U,3)
        cv.Zero(retVal)
        bmp = self.image.getBitmap()
        mask = self.mMask.getBitmap()
        tl = self.topLeftCorner()
        cv.SetImageROI(bmp,(tl[0],tl[1], self.width(),self.height()))
        cv.Copy(bmp,retVal,mask)
        cv.ResetImageROI(bmp)
        return Image(retVal)


    def getFullMaskedImage(self):
        """
        Get the full size image with the masked to the blob
        """
        retVal = cv.CreateImage((self.image.width,self.image.height),cv.IPL_DEPTH_8U,3)
        cv.Zero(retVal)
        bmp = self.image.getBitmap()
        mask = self.mMask.getBitmap()
        tl = self.topLeftCorner()
        cv.SetImageROI(retVal,(tl[0],tl[1], self.width(),self.height()))
        cv.SetImageROI(bmp,(tl[0],tl[1], self.width(),self.height()))
        cv.Copy(bmp,retVal,mask)
        cv.ResetImageROI(bmp)
        cv.ResetImageROI(retVal)
        return Image(retVal)

    def getFullHullMaskedImage(self):
        """
        Get the full size image with the masked to the blob
        """
        retVal = cv.CreateImage((self.image.width,self.image.height),cv.IPL_DEPTH_8U,3)
        cv.Zero(retVal)
        bmp = self.image.getBitmap()
        mask = self.mHullMask.getBitmap()
        tl = self.topLeftCorner()
        cv.SetImageROI(retVal,(tl[0],tl[1], self.width(),self.height()))
        cv.SetImageROI(bmp,(tl[0],tl[1], self.width(),self.height()))
        cv.Copy(bmp,retVal,mask)
        cv.ResetImageROI(bmp)
        cv.ResetImageROI(retVal)
        return Image(retVal)

    def getFullMask(self):
        """
        Get the full sized image mask
        """
        retVal = cv.CreateImage((self.image.width,self.image.height),cv.IPL_DEPTH_8U,3)
        cv.Zero(retVal)
        mask = self.mMask.getBitmap()
        tl = self.topLeftCorner()
        cv.SetImageROI(retVal,(tl[0],tl[1], self.width(),self.height()))
        cv.Copy(mask,retVal)
        cv.ResetImageROI(retVal)
        return Image(retVal)

    def getFullHullMask(self):
        """
        Get the full sized image hull mask
        """
        retVal = cv.CreateImage((self.image.width,self.image.height),cv.IPL_DEPTH_8U,3)
        cv.Zero(retVal)
        mask = self.mHullMask.getBitmap()
        tl = self.topLeftCorner()
        cv.SetImageROI(retVal,(tl[0],tl[1], self.width(),self.height()))
        cv.Copy(mask,retVal)
        cv.ResetImageROI(retVal)
        return Image(retVal)

    def getHullEdgeImage(self):
        retVal = cv.CreateImage((self.width(),self.height()),cv.IPL_DEPTH_8U,3)
        cv.Zero(retVal)
        tl = self.topLeftCorner()
        translate = [(cs[0]-tl[0],cs[1]-tl[1]) for cs in self.mConvexHull]
        cv.PolyLine(retVal,[translate],1,(255,255,255))
        return Image(retVal)

    def getFullHullEdgeImage(self):
        retVal = cv.CreateImage((self.image.width,self.image.height),cv.IPL_DEPTH_8U,3)
        cv.Zero(retVal)
        cv.PolyLine(retVal,[self.mConvexHull],1,(255,255,255))
        return Image(retVal)

    def getEdgeImage(self):
        """
        Get the edge image for the outer contour (no inner holes)
        """
        retVal = cv.CreateImage((self.width(),self.height()),cv.IPL_DEPTH_8U,3)
        cv.Zero(retVal)
        tl = self.topLeftCorner()
        translate = [(cs[0]-tl[0],cs[1]-tl[1]) for cs in self.mContour]
        cv.PolyLine(retVal,[translate],1,(255,255,255))
        return Image(retVal)

    def getFullEdgeImage(self):
        """
        Get the edge image within the full size image.
        """
        retVal = cv.CreateImage((self.image.width,self.image.height),cv.IPL_DEPTH_8U,3)
        cv.Zero(retVal)
        cv.PolyLine(retVal,[self.mContour],1,(255,255,255))
        return Image(retVal)

    def __repr__(self):
        return "SimpleCV.Features.Blob.Blob object at (%d, %d) with area %d" % (self.x, self.y, self.area())


    def _respacePoints(self,contour, min_distance=1, max_distance=5):
        p0 = np.array(contour[-1])
        min_d = min_distance**2
        max_d = max_distance**2
        contour = [p0]+contour[:-1]
        contour = contour[:-1]
        retVal = [p0]
        while len(contour) > 0:
            pt = np.array(contour.pop())
            dist = ((p0[0]-pt[0])**2)+((p0[1]-pt[1])**2)
            if( dist > max_d ): # create the new point
                # get the unit vector from p0 to pt
                # from p0 to pt
                a = float((pt[0]-p0[0]))
                b = float((pt[1]-p0[1]))
                l = np.sqrt((a**2)+(b**2))
                punit = np.array([a/l,b/l])
                # make it max_distance long and add it to p0
                pn = (max_distance*punit)+p0
                retVal.append((pn[0],pn[1]))# push the new point onto the return value
                contour.append(pt)# push the new point onto the contour too
                p0 = pn
            elif( dist > min_d ):
                p0 = np.array(pt)
                retVal.append(pt)
        return retVal


    def _filterSCPoints(self,min_distance=3, max_distance=8):
        """
        Go through ever point in the contour and make sure
        that it is no less than min distance to the next point
        and no more than max_distance from the the next point.
        """
        completeContour = self._respacePoints(self.mContour,min_distance,max_distance)
        if self.mHoleContour is not None:
            for ctr in self.mHoleContour:
                completeContour = completeContour + self._respacePoints(ctr,min_distance,max_distance)
        return completeContour


    def getSCDescriptors(self):
        if( self._scdescriptors is not None ):
            return self._scdescriptors,self._completeContour
        completeContour = self._filterSCPoints()
        descriptor = self._generateSC(completeContour)
        self._scdescriptors = descriptors
        self._completeContour = completeContour
        return descriptors,completeContour


    def _generateSC(self,completeContour,dsz=6,r_bound=[.1,2.1]):
        """
        Create the shape context objects.
        dsz - The size of descriptor as a dszxdsz histogram
        completeContour - All of the edge points as a long list
        r_bound - Bounds on the log part of the shape context descriptor
        """
        data = []
        for pt in completeContour: #
            temp = []
            # take each other point in the contour, center it on pt, and covert it to log polar
            for b in completeContour:
                r = np.sqrt((b[0]-pt[0])**2+(b[1]-pt[1])**2)
#                if( r > 100 ):
#                    continue
                if( r == 0.00 ): # numpy throws an inf here that mucks the system up
                    continue
                r = np.log10(r)
                theta = np.arctan2(b[0]-pt[0],b[1]-pt[1])
                if(np.isfinite(r) and np.isfinite(theta) ):
                    temp.append((r,theta))
            data.append(temp)

        #UHG!!! need to repeat this for all of the interior contours too
        descriptors = []
        #dsz = 6
        # for each point in the contour
        for d in data:
            test = np.array(d)
            # generate a 2D histrogram, and flatten it out.
            hist,a,b = np.histogram2d(test[:,0],test[:,1],dsz,[r_bound,[np.pi*-1/2,np.pi/2]],normed=True)
            hist = hist.reshape(1,dsz**2)
            if(np.all(np.isfinite(hist[0]))):
                descriptors.append(hist[0])

        self._scdescriptors = descriptors
        return descriptors

    def getShapeContext(self):
        """
        Return the shape context descriptors as a featureset. Corrently
        this is not used for recognition but we will perhaps use it soon.
        """
        # still need to subsample big contours
        derp = self.getSCDescriptors()
        descriptors,completeContour = self.getSCDescriptors()
        fs = FeatureSet()
        for i in range(0,len(completeContour)):
            fs.append(ShapeContextDescriptor(self.image,completeContour[i],descriptors[i],self))

        return fs


    def showCorrespondence(self, otherBlob,side="left"):
        """
        This is total beta - use at your own risk.
        """
        #We're lazy right now, assume the blob images are the same size
        side = side.lower()
        myPts = self.getShapeContext()
        yourPts = otherBlob.getShapeContext()

        myImg = self.image.copy()
        yourImg = otherBlob.image.copy()

        myPts = myPts.reassignImage(myImg)
        yourPts = yourPts.reassignImage(yourImg)

        myPts.draw()
        myImg = myImg.applyLayers()
        yourPts.draw()
        yourImg = yourImg.applyLayers()

        result = myImg.sideBySide(yourImg,side=side)
        data = self.shapeContextMatch(otherBlob)
        mapvals = data[0]
        color = Color()
        for i in range(0,len(self._completeContour)):
            lhs = self._completeContour[i]
            idx = mapvals[i];
            rhs = otherBlob._completeContour[idx]
            if( side == "left" ):
                shift = (rhs[0]+yourImg.width,rhs[1])
                result.drawLine(lhs,shift,color=color.getRandom(),thickness=1)
            elif( side == "bottom" ):
                shift = (rhs[0],rhs[1]+myImg.height)
                result.drawLine(lhs,shift,color=color.getRandom(),thickness=1)
            elif( side == "right" ):
                shift = (rhs[0]+myImg.width,rhs[1])
                result.drawLine(lhs,shift,color=color.getRandom(),thickness=1)
            elif( side == "top" ):
                shift = (lhs[0],lhs[1]+myImg.height)
                result.drawLine(lhs,shift,color=color.getRandom(),thickness=1)

        return result


    def getMatchMetric(self,otherBlob):
        """
        This match metric is now deprecated.
        """
        data = self.shapeContextMatch(otherBlob)
        distances = np.array(data[1])
        sd = np.std(distances)
        x = np.mean(distances)
        min = np.min(distances)
        # not sure trimmed mean is perfect
        # realistically we should have some bimodal dist
        # and we want to throw away stuff with awful matches
        # so long as the number of points is not a huge
        # chunk of our points.
        tmean = sps.tmean(distances,(min,x+sd))
        return tmean

    def getConvexityDefects(self, returnPoints=False):
        """
        **SUMMARY**

        Get Convexity Defects of the contour.

        **PARAMETERS**

        *returnPoints* - Bool(False). 
                         If False: Returns FeatureSet of Line(start point, end point) 
                         and Corner(far point)
                         If True: Returns a list of tuples
                         (start point, end point, far point)
        **RETURNS**

        FeatureSet - A FeatureSet of Line and Corner objects
                     OR
                     A list of (start point, end point, far point)
                     See PARAMETERS.

        **EXAMPLE**

        >>> img = Image('lenna')
        >>> blobs = img.findBlobs()
        >>> blob = blobs[-1]
        >>> lines, farpoints = blob.getConvexityDefects()
        >>> lines.draw()
        >>> farpoints.draw(color=Color.RED, width=-1)
        >>> img.show()

        >>> points = blob.getConvexityDefects(returnPoints=True)
        >>> startpoints = zip(*points)[0]
        >>> endpoints = zip(*points)[0]
        >>> farpoints = zip(*points)[0]
        >>> print startpoints, endpoints, farpoints
        """
        def cvFallback():
            chull = cv.ConvexHull2(self.mContour, cv.CreateMemStorage(), return_points=False)
            defects = cv.ConvexityDefects(self.mContour, chull, cv.CreateMemStorage())
            points = [(defect[0], defect[1], defect[2]) for defect in defects]
            return points

        try:
            import cv2
            if hasattr(cv2, "convexityDefects"):
                hull = [self.mContour.index(x) for x in self.mConvexHull]
                hull = np.array(hull).reshape(len(hull), 1)
                defects = cv2.convexityDefects(np.array(self.mContour), hull)
                if isinstance(defects, type(None)):
                    warnings.warn("Unable to find defects. Returning Empty FeatureSet.")
                    defects = []
                points = [(self.mContour[defect[0][0]], self.mContour[defect[0][1]], self.mContour[defect[0][2]]) for defect in defects]
            else:
                points = cvFallback()
        except ImportError:
            points = cvFallback()

        if returnPoints:
            return FeatureSet(points)
        else:
            lines = FeatureSet([Line(self.image, (start, end)) for start, end, far in points])
            farpoints = FeatureSet([Corner(self.image, far[0], far[1]) for start, end, far in points]) 
            features = FeatureSet([lines, farpoints])
            return features


from SimpleCV.Features import Line, Corner

########NEW FILE########
__FILENAME__ = BlobMaker
from SimpleCV.base import *
#import cv2 as cv2

class BlobMaker:
    """
    Blob maker encapsulates all of the contour extraction process and data, so
    it can be used inside the image class, or extended and used outside the image
    class. The general idea is that the blob maker provides the utilites that one
    would use for blob extraction. Later implementations may include tracking and
    other features.
    """
    mMemStorage = None
    def __init__(self):
        self.mMemStorage = cv.CreateMemStorage()
        return None

    def extractUsingModel(self, img, colormodel,minsize=10, maxsize=0):
        """
        Extract blobs using a color model
        img        - The input image
        colormodel - The color model to use.
        minsize    - The minimum size of the returned features.
        maxsize    - The maximum size of the returned features 0=uses the default value.

        Parameters:
            img - Image
            colormodel - ColorModel object
            minsize - Int
            maxsize - Int
        """
        if (maxsize <= 0):
            maxsize = img.width * img.height
        gray = colormodel.threshold(img)
        blobs = self.extractFromBinary(gray,img,minArea=minsize,maxArea=maxsize)
        retVal = sorted(blobs,key=lambda x: x.mArea, reverse=True)
        return FeatureSet(retVal)

    def extract(self, img, threshval = 127, minsize=10, maxsize=0, threshblocksize=3, threshconstant=5):
        """
        This method performs a threshold operation on the input image and then
        extracts and returns the blobs.
        img       - The input image (color or b&w)
        threshval - The threshold value for the binarize operation. If threshval = -1 adaptive thresholding is used
        minsize   - The minimum blob size in pixels.
        maxsize   - The maximum blob size in pixels. 0=uses the default value.
        threshblocksize - The adaptive threhold block size.
        threshconstant  - The minimum to subtract off the adaptive threshold
        """
        if (maxsize <= 0):
            maxsize = img.width * img.height

        #create a single channel image, thresholded to parameters

        blobs = self.extractFromBinary(img.binarize(threshval, 255, threshblocksize, threshconstant).invert(),img,minsize,maxsize)
        retVal = sorted(blobs,key=lambda x: x.mArea, reverse=True)
        return FeatureSet(retVal)

    def extractFromBinary(self,binaryImg,colorImg, minsize = 5, maxsize = -1,appx_level=3):
        """
        This method performs blob extraction given a binary source image that is used
        to get the blob images, and a color source image.
        binarymg- The binary image with the blobs.
        colorImg - The color image.
        minSize  - The minimum size of the blobs in pixels.
        maxSize  - The maximum blob size in pixels.
        * *appx_level* - The blob approximation level - an integer for the maximum distance between the true edge and the approximation edge - lower numbers yield better approximation.
        """
        #If you hit this recursion limit may god have mercy on your soul.
        #If you really are having problems set the value higher, but this means
        # you have over 10,000,000 blobs in your image.
        sys.setrecursionlimit(5000)
        #h_next moves to the next external contour
        #v_next() moves to the next internal contour
        if (maxsize <= 0):
            maxsize = colorImg.width * colorImg.height

        retVal = []
        test = binaryImg.meanColor()
        if( test[0]==0.00 and test[1]==0.00 and test[2]==0.00):
            return FeatureSet(retVal)

        # There are a couple of weird corner cases with the opencv
        # connect components libraries - when you try to find contours
        # in an all black image, or an image with a single white pixel
        # that sits on the edge of an image the whole thing explodes
        # this check catches those bugs. -KAS
        # Also I am submitting a bug report to Willow Garage - please bare with us.
        ptest = (4*255.0)/(binaryImg.width*binaryImg.height) # val if two pixels are white
        if( test[0]<=ptest and test[1]<=ptest and test[2]<=ptest):
            return retVal

        seq = cv.FindContours( binaryImg._getGrayscaleBitmap(), self.mMemStorage, cv.CV_RETR_TREE, cv.CV_CHAIN_APPROX_SIMPLE)
        if not list(seq):
            warnings.warn("Unable to find Blobs. Retuning Empty FeatureSet.")
            return FeatureSet([])
        try:
            # note to self
            # http://code.activestate.com/recipes/474088-tail-call-optimization-decorator/
            retVal = self._extractFromBinary(seq,False,colorImg,minsize,maxsize,appx_level)
        except RuntimeError,e:
            logger.warning("You exceeded the recursion limit. This means you probably have too many blobs in your image. We suggest you do some morphological operations (erode/dilate) to reduce the number of blobs in your image. This function was designed to max out at about 5000 blobs per image.")
        except e:
            logger.warning("SimpleCV Find Blobs Failed - This could be an OpenCV python binding issue")
        del seq
        return FeatureSet(retVal)

    def _extractFromBinary(self, seq, isaHole, colorImg,minsize,maxsize,appx_level):
        """
        The recursive entry point for the blob extraction. The blobs and holes are presented
        as a tree and we traverse up and across the tree.
        """
        retVal = []

        if( seq is None ):
            return retVal

        nextLayerDown = []
        while True:
            if( not isaHole ): #if we aren't a hole then we are an object, so get and return our featuress
                temp =  self._extractData(seq,colorImg,minsize,maxsize,appx_level)
                if( temp is not None ):
                    retVal.append(temp)

            nextLayer = seq.v_next()

            if nextLayer is not None:
                nextLayerDown.append(nextLayer)

            seq = seq.h_next()

            if seq is None:
                break

        for nextLayer in nextLayerDown:
            retVal += self._extractFromBinary(nextLayer, not isaHole, colorImg, minsize,maxsize,appx_level)

        return retVal

    def _extractData(self,seq,color,minsize,maxsize,appx_level):
        """
        Extract the bulk of the data from a give blob. If the blob's are is too large
        or too small the method returns none.
        """
        if( seq is None or not len(seq)):
            return None
        area = cv.ContourArea(seq)
        if( area < minsize or area > maxsize):
            return None

        retVal = Blob()
        retVal.image = color
        retVal.mArea = area

        retVal.mMinRectangle = cv.MinAreaRect2(seq)
        bb = cv.BoundingRect(seq)
        retVal.x = bb[0]+(bb[2]/2)
        retVal.y = bb[1]+(bb[3]/2)
        retVal.mPerimeter = cv.ArcLength(seq)
        if( seq is not None):  #KAS
            retVal.mContour = list(seq)
            try:
                import cv2
                if( retVal.mContour is not None):
                    retVal.mContourAppx = []
                    appx = cv2.approxPolyDP(np.array([retVal.mContour],'float32'),appx_level,True)
                    for p in appx:
                        retVal.mContourAppx.append((int(p[0][0]),int(p[0][1])))
            except:
                pass

        # so this is a bit hacky....

        # For blobs that live right on the edge of the image OpenCV reports the position and width
        #   height as being one over for the true position. E.g. if a blob is at (0,0) OpenCV reports
        #   its position as (1,1). Likewise the width and height for the other corners is reported as
        #   being one less than the width and height. This is a known bug.

        xx = bb[0]
        yy = bb[1]
        ww = bb[2]
        hh = bb[3]
        retVal.points = [(xx,yy),(xx+ww,yy),(xx+ww,yy+hh),(xx,yy+hh)]
        retVal._updateExtents()
        chull = cv.ConvexHull2(seq,cv.CreateMemStorage(),return_points=1)
        retVal.mConvexHull = list(chull)
        # KAS -- FLAG FOR REPLACE 6/6/2012
        #hullMask = self._getHullMask(chull,bb)

        # KAS -- FLAG FOR REPLACE 6/6/2012
        #retVal.mHullImg = self._getBlobAsImage(chull,bb,color.getBitmap(),hullMask)

        # KAS -- FLAG FOR REPLACE 6/6/2012
        #retVal.mHullMask = Image(hullMask)

        del chull

        moments = cv.Moments(seq)

        #This is a hack for a python wrapper bug that was missing
        #the constants required from the ctype
        retVal.m00 = area
        try:
            retVal.m10 = moments.m10
            retVal.m01 = moments.m01
            retVal.m11 = moments.m11
            retVal.m20 = moments.m20
            retVal.m02 = moments.m02
            retVal.m21 = moments.m21
            retVal.m12 = moments.m12
        except:
            retVal.m10 = cv.GetSpatialMoment(moments,1,0)
            retVal.m01 = cv.GetSpatialMoment(moments,0,1)
            retVal.m11 = cv.GetSpatialMoment(moments,1,1)
            retVal.m20 = cv.GetSpatialMoment(moments,2,0)
            retVal.m02 = cv.GetSpatialMoment(moments,0,2)
            retVal.m21 = cv.GetSpatialMoment(moments,2,1)
            retVal.m12 = cv.GetSpatialMoment(moments,1,2)

        retVal.mHu = cv.GetHuMoments(moments)


        # KAS -- FLAG FOR REPLACE 6/6/2012
        mask = self._getMask(seq,bb)
        #retVal.mMask = Image(mask)

        retVal.mAvgColor = self._getAvg(color.getBitmap(),bb,mask)
        retVal.mAvgColor = retVal.mAvgColor[0:3]
        #retVal.mAvgColor = self._getAvg(color.getBitmap(),retVal.mBoundingBox,mask)
        #retVal.mAvgColor = retVal.mAvgColor[0:3]

        # KAS -- FLAG FOR REPLACE 6/6/2012
        #retVal.mImg = self._getBlobAsImage(seq,bb,color.getBitmap(),mask)

        retVal.mHoleContour = self._getHoles(seq)
        retVal.mAspectRatio = retVal.mMinRectangle[1][0]/retVal.mMinRectangle[1][1]

        return retVal


    def _getHoles(self,seq):
        """
        This method returns the holes associated with a blob as a list of tuples.
        """
        retVal = None
        holes = seq.v_next()
        if( holes is not None ):
            retVal = [list(holes)]
            while( holes.h_next() is not None ):
                holes = holes.h_next();
                temp = list(holes)
                if( len(temp) >= 3 ): #exclude single pixel holes
                    retVal.append(temp)
        return retVal


    def _getMask(self,seq,bb):
        """
        Return a binary image of a particular contour sequence.
        """
        #bb = cv.BoundingRect(seq)
        mask = cv.CreateImage((bb[2],bb[3]),cv.IPL_DEPTH_8U,1)
        cv.Zero(mask)
        cv.DrawContours(mask,seq,(255),(0),0,thickness=-1, offset=(-1*bb[0],-1*bb[1]))
        holes = seq.v_next()
        if( holes is not None ):
            cv.DrawContours(mask,holes,(0),(255),0,thickness=-1, offset=(-1*bb[0],-1*bb[1]))
            while( holes.h_next() is not None ):
                holes = holes.h_next();
                if(holes is not None):
                    cv.DrawContours(mask,holes,(0),(255),0,thickness=-1, offset=(-1*bb[0],-1*bb[1]))
        return mask

    def _getHullMask(self,hull,bb):
        """
        Return a mask of the convex hull of a blob.
        """
        bb = cv.BoundingRect(hull)
        mask = cv.CreateImage((bb[2],bb[3]),cv.IPL_DEPTH_8U,1)
        cv.Zero(mask)
        cv.DrawContours(mask,hull,(255),(0),0,thickness=-1, offset=(-1*bb[0],-1*bb[1]))
        return mask

    def _getAvg(self,colorbitmap,bb,mask):
        """
        Calculate the average color of a blob given the mask.
        """
        cv.SetImageROI(colorbitmap,bb)
        #may need the offset parameter
        avg = cv.Avg(colorbitmap,mask)
        cv.ResetImageROI(colorbitmap)
        return avg

    def _getBlobAsImage(self,seq,bb,colorbitmap,mask):
        """
        Return an image that contains just pixels defined by the blob sequence.
        """
        cv.SetImageROI(colorbitmap,bb)
        outputImg = cv.CreateImage((bb[2],bb[3]),cv.IPL_DEPTH_8U,3)
        cv.Zero(outputImg)
        cv.Copy(colorbitmap,outputImg,mask)
        cv.ResetImageROI(colorbitmap)
        return(Image(outputImg))



from SimpleCV.ImageClass import Image
from SimpleCV.Features.Features import FeatureSet
from SimpleCV.Features.Blob import Blob

########NEW FILE########
__FILENAME__ = BOFFeatureExtractor
from SimpleCV.base import *
from SimpleCV.ImageClass import Image
from SimpleCV.Features.FeatureExtractorBase import *

class BOFFeatureExtractor(object):
    """
    For a discussion of bag of features please see:
    http://en.wikipedia.org/wiki/Bag_of_words_model_in_computer_vision

    Initialize the bag of features extractor. This assumes you don't have
    the feature codebook pre-computed.
    patchsz = the dimensions of each codebook patch
    numcodes = the number of different patches in the codebook.
    imglayout = the shape of the resulting image in terms of patches
    padding = the pixel padding of each patch in the resulting image.

    """
    mPatchSize = (11,11)
    mNumCodes = 128
    mPadding = 0
    mLayout = (8,16)
    mCodebookImg = None
    mCodebook = None

    def __init__(self,patchsz=(11,11),numcodes=128,imglayout=(8,16),padding=0):

        self.mPadding = padding
        self.mLayout = imglayout
        self.mPatchSize = patchsz
        self.mNumCodes = numcodes

    def generate(self,imgdirs,numcodes=128,sz=(11,11),imgs_per_dir=50,img_layout=(8,16),padding=0, verbose=True):
        """
        This method builds the bag of features codebook from a list of directories
        with images in them. Each directory should be broken down by image class.

        * imgdirs: This list of directories.
        * patchsz: the dimensions of each codebook patch
        * numcodes: the number of different patches in the codebook.
        * imglayout: the shape of the resulting image in terms of patches - this must
          match the size of numcodes. I.e. numcodes == img_layout[0]*img_layout[1]
        * padding:the pixel padding of each patch in the resulting image.
        * imgs_per_dir: this method can use a specified number of images per directory
        * verbose: print output


        Once the method has completed it will save the results to a local file
        using the file name codebook.png


        WARNING:

            THIS METHOD WILL TAKE FOREVER
        """
        if( numcodes != img_layout[0]*img_layout[1]):
            warnings.warn("Numcodes must match the size of image layout.")
            return None

        self.mPadding = padding
        self.mLayout = img_layout
        self.mNumCodes = numcodes
        self.mPatchSize = sz
        rawFeatures = np.zeros(sz[0]*sz[1])#fakeout numpy so we can use vstack
        for path in imgdirs:
            fcount = 0
            files = []
            for ext in IMAGE_FORMATS:
                files.extend(glob.glob( os.path.join(path, ext)))
            nimgs = min(len(files),imgs_per_dir)
            for i in range(nimgs):
                infile = files[i]
                if verbose:
                    print(path+" "+str(i)+" of "+str(imgs_per_dir))
                    print "Opening file: " + infile
                img = Image(infile)
                newFeat = self._getPatches(img,sz)
                if verbose:
                    print "     Got " + str(len(newFeat)) + " features."
                rawFeatures = np.vstack((rawFeatures,newFeat))
                del img
        rawFeatures = rawFeatures[1:,:] # pop the fake value we put on the top
        if verbose:
            print "=================================="
            print "Got " + str(len(rawFeatures)) + " features "
            print "Doing K-Means .... this will take a long time"
        self.mCodebook = self._makeCodebook(rawFeatures,self.mNumCodes)
        self.mCodebookImg = self._codebook2Img(self.mCodebook,self.mPatchSize,self.mNumCodes,self.mLayout,self.mPadding)
        self.mCodebookImg.save('codebook.png')

    def extractPatches(self, img, sz=(11,11) ):
        """
        Get patches from a single images. This is an external access method. The
        user will need to maintain the list of features. See the generate method
        as a guide to doing this by hand. Sz is the image patch size.
        """
        return self._getPatches(img,sz)

    def makeCodebook(self, featureStack,ncodes=128):
        """
        This method will return the centroids of the k-means analysis of a large
        number of images. Ncodes is the number of centroids to find.
        """
        return self._makeCodebook(featureStack,ncodes)

    def _makeCodebook(self,data,ncodes=128):
        """
        Do the k-means ... this is slow as as shit
        """
        [centroids, membership] = cluster.kmeans2(data,ncodes, minit='points')
        return(centroids)

    def _img2Codebook(self, img, patchsize, count, patch_arrangement, spacersz):
        """
        img = the image
        patchsize = the patch size (ususally 11x11)
        count = total codes
        patch_arrangement = how are the patches grided in the image (eg 128 = (8x16) 256=(16x16) )
        spacersz = the number of pixels between patches
        """
        img = img.toHLS()
        lmat = cv.CreateImage((img.width,img.height), cv.IPL_DEPTH_8U, 1)
        patch = cv.CreateImage(patchsize,cv.IPL_DEPTH_8U,1)
        cv.Split(img.getBitmap(),None,lmat,None,None)
        w = patchsize[0]
        h = patchsize[1]
        length = w*h
        retVal = np.zeros(length)
        for widx in range(patch_arrangement[0]):
            for hidx in range(patch_arrangement[1]):
                x = (widx*patchsize[0])+((widx+1)*spacersz)
                y = (hidx*patchsize[1])+((hidx+1)*spacersz)
                cv.SetImageROI(lmat,(x,y,w,h))
                cv.Copy(lmat,patch)
                cv.ResetImageROI(lmat)
                retVal = np.vstack((retVal,np.array(patch[:,:]).reshape(length)))
        retVal = retVal[1:,:]
        return retVal



    def _codebook2Img(self, cb, patchsize, count, patch_arrangement, spacersz):
        """
        cb = the codebook
        patchsize = the patch size (ususally 11x11)
        count = total codes
        patch_arrangement = how are the patches grided in the image (eg 128 = (8x16) 256=(16x16) )
        spacersz = the number of pixels between patches
        """
        w = (patchsize[0]*patch_arrangement[0])+((patch_arrangement[0]+1)*spacersz)
        h = (patchsize[1]*patch_arrangement[1])+((patch_arrangement[1]+1)*spacersz)
        bm = cv.CreateImage((w,h), cv.IPL_DEPTH_8U, 1)
        cv.Zero(bm)
        img = Image(bm)
        count = 0
        for widx in range(patch_arrangement[0]):
            for hidx in range(patch_arrangement[1]):
                x = (widx*patchsize[0])+((widx+1)*spacersz)
                y = (hidx*patchsize[1])+((hidx+1)*spacersz)
                temp = Image(cb[count,:].reshape(patchsize[0],patchsize[1]))
                img.blit(temp,pos=(x,y))
                count = count + 1
        return img

    def _getPatches(self,img,sz=None):
        #retVal = [] # may need to go to np.array
        if( sz is None ):
            sz = self.mPatchSize
        img2 = img.toHLS()
        lmat = cv.CreateImage((img.width,img.height), cv.IPL_DEPTH_8U, 1)
        patch = cv.CreateImage(self.mPatchSize,cv.IPL_DEPTH_8U,1)
        cv.Split(img2.getBitmap(),None,lmat,None,None)
        wsteps = img2.width/sz[0]
        hsteps = img2.height/sz[1]
        w=sz[0]
        h=sz[1]
        length = w*h
        retVal = np.zeros(length)
        for widx in range(wsteps):
            for hidx in range(hsteps):
                x = (widx*sz[0])
                y = (hidx*sz[1])
                cv.SetImageROI(lmat,(x,y,w,h))
                cv.EqualizeHist(lmat,patch)
                #cv.Copy(lmat,patch)
                cv.ResetImageROI(lmat)

                retVal = np.vstack((retVal,np.array(patch[:,:]).reshape(length)))
                #retVal.append()
        retVal = retVal[1:,:] # pop the fake value we put on top of the stack
        return retVal



    def load(self,datafile):
        """
        Load a codebook from file using the datafile. The datafile
        should point to a local image for the source patch image.
        """
        myFile = open(datafile, 'r')
        temp = myFile.readline()
        #print(temp)
        self.mNumCodes = int(myFile.readline())
        #print(self.mNumCodes)
        w = int(myFile.readline())
        h = int(myFile.readline())
        self.mPatchSize = (w,h)
        #print(self.mPatchSize)
        self.mPadding = int(myFile.readline())
        #print(self.mPadding)
        w = int(myFile.readline())
        h = int(myFile.readline())
        self.mLayout = (w,h)
        #print(self.mLayout)
        imgfname = myFile.readline().strip()
        #print(imgfname)
        self.mCodebookImg = Image(imgfname)
        self.mCodebook = self._img2Codebook(self.mCodebookImg,
                                            self.mPatchSize,
                                            self.mNumCodes,
                                            self.mLayout,
                                            self.mPadding)
        #print(self.mCodebook)
        return

    def save(self,imgfname,datafname):
        """
        Save the bag of features codebook and data set to a local file.
        """
        myFile = open(datafname,'w')
        myFile.write("BOF Codebook Data\n")
        myFile.write(str(self.mNumCodes)+"\n")
        myFile.write(str(self.mPatchSize[0])+"\n")
        myFile.write(str(self.mPatchSize[1])+"\n")
        myFile.write(str(self.mPadding)+"\n")
        myFile.write(str(self.mLayout[0])+"\n")
        myFile.write(str(self.mLayout[1])+"\n")
        myFile.write(imgfname+"\n")
        myFile.close()
        if(self.mCodebookImg is None):
            self._codebook2Img(self.mCodebook,self.mPatchSize,self.mNumCodes,self.mLayout,self.mPadding)
        self.mCodebookImg.save(imgfname)
        return

    def __getstate__(self):
        if(self.mCodebookImg is None):
            self._codebook2Img(self.mCodebook,self.mPatchSize,self.mNumCodes,self.mLayout,self.mPadding)
        mydict = self.__dict__.copy()
        del mydict['mCodebook']
        return mydict

    def __setstate__(self, mydict):
        self.__dict__ = mydict
        self.mCodebook = self._img2Codebook(self.mCodebookImg,
                                            self.mPatchSize,
                                            self.mNumCodes,
                                            self.mLayout,
                                            self.mPadding)

    def extract(self, img):
        """
        This method extracts a bag of features histogram for the input image using
        the provided codebook. The result are the bin counts for each codebook code.
        """
        data = self._getPatches(img)
        p = spsd.cdist(data,self.mCodebook)
        codes = np.argmin(p,axis=1)
        [retVal,foo] = np.histogram(codes,self.mNumCodes,normed=True,range=(0,self.mNumCodes-1))
        return retVal

    def reconstruct(self,img):
        """
        This is a "just for fun" method as a sanity check for the BOF codeook.
        The method takes in an image, extracts each codebook code, and replaces
        the image at the position with the code.
        """
        retVal = cv.CreateImage((img.width,img.height), cv.IPL_DEPTH_8U, 1)
        data = self._getPatches(img)
        p = spsd.cdist(data,self.mCodebook)
        foo = p.shape[0]
        codes = np.argmin(p,axis=1)
        count = 0
        wsteps = img.width/self.mPatchSize[0]
        hsteps = img.height/self.mPatchSize[1]
        w=self.mPatchSize[0]
        h=self.mPatchSize[1]
        length = w*h
        retVal = Image(retVal)
        for widx in range(wsteps):
            for hidx in range(hsteps):
                x = (widx*self.mPatchSize[0])
                y = (hidx*self.mPatchSize[1])
                p = codes[count]
                temp = Image(self.mCodebook[p,:].reshape(self.mPatchSize[0],self.mPatchSize[1]))
                retVal = retVal.blit(temp,pos=(x,y))
                count = count + 1
        return retVal

    def getFieldNames(self):
        """
        This method gives the names of each field in the feature vector in the
        order in which they are returned. For example, 'xpos' or 'width'
        """
        retVal = []
        for widx in range(self.mLayout[0]):
            for hidx in range(self.mLayout[1]):
                temp = "CB_R"+str(widx)+"_C"+str(hidx)
                retVal.append(temp)
        return retVal


    def getNumFields(self):
        """
        This method returns the total number of fields in the feature vector.
        """
        return self.mNumCodes

########NEW FILE########
__FILENAME__ = Detection
'''
SimpleCV Detection Library

This library includes classes for finding things in images

FYI -
All angles shalt be described in degrees with zero pointing east in the
plane of the image with all positive rotations going counter-clockwise.
Therefore a rotation from the x-axis to to the y-axis is positive and follows
the right hand rule.
'''

#load required libraries
from SimpleCV.base import *
from SimpleCV.ImageClass import *
from SimpleCV.Color import *
from SimpleCV.Features.Features import Feature, FeatureSet

class Corner(Feature):
    """
    **SUMMARY**

    The Corner feature is a point returned by the FindCorners function
    Corners are used in machine vision as a very computationally efficient way
    to find unique features in an image.  These corners can be used in
    conjunction with many other algorithms.

    **SEE ALSO**

    :py:meth:`findCorners`
    """
    def __init__(self, i, at_x, at_y):

        points = [(at_x-1,at_y-1),(at_x-1,at_y+1),(at_x+1,at_y+1),(at_x+1,at_y-1)]
        super(Corner, self).__init__(i, at_x, at_y,points)
        #can we look at the eigenbuffer and find direction?

    def draw(self, color = (255, 0, 0),width=1):
        """
        **SUMMARY**

        Draw a small circle around the corner.  Color tuple is single parameter, default is Red.

        **PARAMETERS**

        * *color* - An RGB color triplet.
        * *width* - if width is less than zero we draw the feature filled in, otherwise we draw the
          contour using the specified width.


        **RETURNS**

        Nothing - this is an inplace operation that modifies the source images drawing layer.

        """
        self.image.drawCircle((self.x, self.y), 4, color,width)

######################################################################
class Line(Feature):
    """
    **SUMMARY**

    The Line class is returned by the findLines function, but can also be initialized with any two points.

    >>> l = Line(Image, (point1, point2))

    Where point1 and point2 are (x,y) coordinate tuples.

    >>> l.points

    Returns a tuple of the two points


    """
    #TODO - A nice feature would be to calculate the endpoints of the line.

    def __init__(self, i, line):
        self.image = i
        self.vector = None
        self.yIntercept = None
        self.end_points = copy(line)
        #print self.end_points[1][1], self.end_points[0][1], self.end_points[1][0], self.end_points[0][0]
        if self.end_points[1][0] - self.end_points[0][0] == 0:
            self.slope = float("inf")
        else:
            self.slope = float(self.end_points[1][1] - self.end_points[0][1])/float(self.end_points[1][0] - self.end_points[0][0])
        #coordinate of the line object is the midpoint
        at_x = (line[0][0] + line[1][0]) / 2
        at_y = (line[0][1] + line[1][1]) / 2
        xmin = int(np.min([line[0][0],line[1][0]]))
        xmax = int(np.max([line[0][0],line[1][0]]))
        ymax = int(np.min([line[0][1],line[1][1]]))
        ymin = int(np.max([line[0][1],line[1][1]]))
        points = [(xmin,ymin),(xmin,ymax),(xmax,ymax),(xmax,ymin)]
        super(Line, self).__init__(i, at_x, at_y,points)

    def draw(self, color = (0, 0, 255),width=1):
        """
        Draw the line, default color is blue

        **SUMMARY**

        Draw a small circle around the corner.  Color tuple is single parameter, default is Red.

        **PARAMETERS**

        * *color* - An RGB color triplet.
        * *width* - Draw the line using the specified width.

        **RETURNS**

        Nothing - this is an inplace operation that modifies the source images drawing layer.


        """
        self.image.drawLine(self.end_points[0], self.end_points[1], color,width)

    def length(self):
        """

        **SUMMARY**

        This method returns the length of the line.

        **RETURNS**

        A floating point length value.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> lines = img.findLines
        >>> for l in lines:
        >>>    if l.length() > 100:
        >>>       print "OH MY! - WHAT A BIG LINE YOU HAVE!"
        >>>       print "---I bet you say that to all the lines."

        """
        return float(spsd.euclidean(self.end_points[0], self.end_points[1]))

    def crop(self):
        """
        **SUMMARY**

        This function crops the source image to the location of the feature and returns
        a new SimpleCV image.

        **RETURNS**

        A SimpleCV image that is cropped to the feature position and size.

        **EXAMPLE**

        >>> img = Image("../sampleimages/EdgeTest2.png")
        >>> l = img.findLines()
        >>> myLine = l[0].crop()

        """
        tl = self.topLeftCorner()
        return self.image.crop(tl[0],tl[1],self.width(),self.height())

    def meanColor(self):
        """
        **SUMMARY**

        Returns the mean color of pixels under the line.  Note that when the line falls "between" pixels, each pixels color contributes to the weighted average.


        **RETURNS**

        Returns an RGB triplet corresponding to the mean color of the feature.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> l = img.findLines()
        >>> c = l[0].meanColor()

        """
        (pt1, pt2) = self.end_points
        #we're going to walk the line, and take the mean color from all the px
        #points -- there's probably a much more optimal way to do this
        (maxx,minx,maxy,miny) = self.extents()

        d_x = maxx - minx
        d_y = maxy - miny
        #orient the line so it is going in the positive direction

        #if it's a straight one, we can just get mean color on the slice
        if (d_x == 0.0):
            return self.image[pt1[0]:pt1[0] + 1, miny:maxy].meanColor()
        if (d_y == 0.0):
            return self.image[minx:maxx, pt1[1]:pt1[1] + 1].meanColor()

        error = 0.0
        d_err = d_y / d_x  #this is how much our "error" will increase in every step
        px = []
        weights = []
        if (d_err < 1):
            y = miny
            #iterate over X
            for x in range(minx, maxx):
                #this is the pixel we would draw on, check the color at that px
                #weight is reduced from 1.0 by the abs amount of error
                px.append(self.image[x, y])
                weights.append(1.0 - abs(error))

                #if we have error in either direction, we're going to use the px
                #above or below
                if (error > 0): #
                    px.append(self.image[x, y+1])
                    weights.append(error)

                if (error < 0):
                    px.append(self.image[x, y-1])
                    weights.append(abs(error))

                error = error + d_err
                if (error >= 0.5):
                    y = y + 1
                    error = error - 1.0
        else:
            #this is a "steep" line, so we iterate over X
            #copy and paste.  Ugh, sorry.
            x = minx
            for y in range(miny, maxy):
                #this is the pixel we would draw on, check the color at that px
                #weight is reduced from 1.0 by the abs amount of error
                px.append(self.image[x, y])
                weights.append(1.0 - abs(error))

                #if we have error in either direction, we're going to use the px
                #above or below
                if (error > 0): #
                    px.append(self.image[x + 1, y])
                    weights.append(error)

                if (error < 0):
                    px.append(self.image[x - 1, y])
                    weights.append(abs(error))

                error = error + (1.0 / d_err) #we use the reciprocal of error
                if (error >= 0.5):
                    x = x + 1
                    error = error - 1.0

        #once we have iterated over every pixel in the line, we avg the weights
        clr_arr = np.array(px)
        weight_arr = np.array(weights)

        weighted_clrs = np.transpose(np.transpose(clr_arr) * weight_arr)
        #multiply each color tuple by its weight

        temp = sum(weighted_clrs) / sum(weight_arr)  #return the weighted avg
        return (float(temp[0]),float(temp[1]),float(temp[2]))

    def findIntersection(self, line):
        """
        **SUMMARY**

        Returns the interesction point of two lines.

        **RETURNS**

        A point tuple.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> l = img.findLines()
        >>> c = l[0].findIntersection[1]


        TODO: THIS NEEDS TO RETURN A TUPLE OF FLOATS
        """
        if self.slope == float("inf"):
            x = self.end_points[0][0]
            y = line.slope*(x-line.end_points[1][0])+line.end_points[1][1]
            return (x, y)

        if line.slope == float("inf"):
            x = line.end_points[0][0]
            y = self.slope*(x-self.end_points[1][0])+self.end_points[1][1]
            return (x, y)

        m1 = self.slope
        x12, y12 = self.end_points[1]
        m2 = line.slope
        x22, y22 = line.end_points[1]

        x = (m1*x12 - m2*x22 + y22 - y12)/float(m1-m2)
        y = (m1*m2*(x12-x22) - m2*y12 + m1*y22)/float(m1-m2)

        return (x, y)

    def isParallel(self, line):
        """
        **SUMMARY**

        Checks whether two lines are parallel or not.

        **RETURNS**

        Bool. True or False

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> l = img.findLines()
        >>> c = l[0].isParallel(l[1])

        """
        if self.slope == line.slope:
            return True
        return False

    def isPerpendicular(self, line):
        """
        **SUMMARY**

        Checks whether two lines are perpendicular or not.

        **RETURNS**

        Bool. True or False

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> l = img.findLines()
        >>> c = l[0].isPerpendicular(l[1])

        """
        if self.slope == float("inf"):
            if line.slope == 0:
                return True
            return False

        if line.slope == float("inf"):
            if self.slope == 0:
                return True
            return False

        if self.slope*line.slope == -1:
            return True
        return False

    def imgIntersections(self, img):
        """
        **SUMMARY**

        Returns a set of pixels where the line intersects with the binary image.

        **RETURNS**

        list of points.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> l = img.findLines()
        >>> c = l[0].imgIntersections(img.binarize())

        """
        pixels = []
        if self.slope == float("inf"):
            for y in range(self.end_points[0][1], self.end_points[1][1]+1):
                pixels.append((self.end_points[0][0], y))
        else:
            for x in range(self.end_points[0][0], self.end_points[1][0]+1):
                pixels.append((x, int(self.end_points[1][1] + self.slope*(x-self.end_points[1][0]))))
            for y in range(self.end_points[0][1], self.end_points[1][1]+1):
                pixels.append((int(((y-self.end_points[1][1])/self.slope)+self.end_points[1][0]), y))
        pixels = list(set(pixels))
        matched_pixels=[]
        for pixel in pixels:
            if img[pixel[0], pixel[1]] == (255.0, 255.0, 255.0):
                matched_pixels.append(pixel)
        matched_pixels.sort()

        return matched_pixels

    def angle(self):
        """
        **SUMMARY**

        This is the angle of the line, from the leftmost point to the rightmost point
        Returns angle (theta) in radians, with 0 = horizontal, -pi/2 = vertical positive slope, pi/2 = vertical negative slope

        **RETURNS**

        An angle value in degrees.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> ls = img.findLines
        >>> for l in ls:
        >>>    if l.angle() == 0:
        >>>       print "I AM HORIZONTAL."


        """
        #first find the leftmost point
        a = 0
        b = 1
        if (self.end_points[a][0] > self.end_points[b][0]):
            b = 0
            a = 1

        d_x = self.end_points[b][0] - self.end_points[a][0]
        d_y = self.end_points[b][1] - self.end_points[a][1]
        #our internal standard is degrees
        return float(360.00 * (atan2(d_y, d_x)/(2 * np.pi))) #formerly 0 was west

    def cropToImageEdges(self):
        """
        **SUMMARY**
        
        Returns the line with endpoints on edges of image. If some endpoints lies inside image 
        then those points remain the same without extension to the edges.

        **RETURNS**

        Returns a :py:class:`Line` object. If line does not cross the image's edges or cross at one point returns None.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> l = Line(img, ((-100, -50), (1000, 25))
        >>> cr_l = l.cropToImageEdges()

        """
        pt1, pt2 = self.end_points
        pt1, pt2 = min(pt1, pt2), max(pt1, pt2)
        x1, y1 = pt1
        x2, y2 = pt2
        w, h = self.image.width-1, self.image.height-1
        slope = self.slope
               
        ep = []
        if slope == float('inf'):
            if 0 <= x1 <= w and 0 <= x2 <= w:
                ep.append((x1, 0))
                ep.append((x2, h))
        elif slope == 0:
            if 0 <= y1 <= w and 0 <= y2 <= w:
                ep.append((0, y1))
                ep.append((w, y2))
        else:
            x = (slope*x1 - y1)/slope   # top edge y = 0
            if 0 <= x <= w:
                ep.append((int(round(x)), 0))

            x = (slope*x1 + h - y1)/slope   # bottom edge y = h
            if 0 <= x <= w:
                ep.append((int(round(x)), h))
            
            y = -slope*x1 + y1  # left edge x = 0
            if 0 <= y <= h:
                ep.append( (0, (int(round(y)))) )
            
            y = slope*(w - x1) + y1 # right edge x = w
            if 0 <= y <= h:
                ep.append( (w, (int(round(y)))) )

        ep = list(set(ep))  # remove duplicates of points if line cross image at corners
        ep.sort()
        if len(ep) == 2:
            # if points lies outside image then change them
            if not (0 < x1 < w and 0 < y1 < h):
                pt1 = ep[0]
            if not (0 < x2 < w and 0 < y2 < h):
                pt2 = ep[1]
        elif len(ep) == 1:
            logger.warning("Line cross the image only at one point")
            return None
        else:
            logger.warning("Line does not cross the image")
            return None
        
        return Line(self.image, (pt1, pt2))
        
    def getVector(self):
        # this should be a lazy property
        if( self.vector is None):
            self.vector = [float(self.end_points[1][0]-self.end_points[0][0]),
                           float(self.end_points[1][1]-self.end_points[0][1])]
        return self.vector

    def dot(self,other):
        return np.dot(self.getVector(),other.getVector())

    def cross(self,other):
        return np.cross(self.getVector(),other.getVector())

    def getYIntercept(self):
        """
        **SUMMARY**
        
        Returns the y intercept based on the lines equation.  Note that this point is potentially not contained in the image itself

        **RETURNS**

        Returns a floating point intersection value

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> l = Line(img, ((50, 150), (2, 225))
        >>> b = l.getYIntercept()
        """
        if self.yIntercept is None:
            pt1, pt2 = self.end_points
            m = self.slope
            #y = mx + b | b = y-mx
            self.yIntercept = pt1[1] - m * pt1[0]
        return self.yIntercept

    def extendToImageEdges(self):
        """
        **SUMMARY**
        
        Returns the line with endpoints on edges of image. 

        **RETURNS**

        Returns a :py:class:`Line` object. If line does not lies entirely inside image then returns None.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> l = Line(img, ((50, 150), (2, 225))
        >>> cr_l = l.extendToImageEdges()

        """
        pt1, pt2 = self.end_points
        pt1, pt2 = min(pt1, pt2), max(pt1, pt2)
        x1, y1 = pt1
        x2, y2 = pt2
        w, h = self.image.width-1, self.image.height-1
        slope = self.slope
        
        if not 0 <= x1 <= w or not 0 <= x2 <= w or not 0 <= y1 <= w or not 0 <= y2 <= w:
            logger.warning("At first the line should be cropped")
            return None
               
        ep = []
        if slope == float('inf'):
            if 0 <= x1 <= w and 0 <= x2 <= w:
                return Line(self.image, ((x1, 0), (x2, h)))
        elif slope == 0:
            if 0 <= y1 <= w and 0 <= y2 <= w:
                return Line(self.image, ((0, y1), (w, y2)))
        else:
            x = (slope*x1 - y1)/slope   # top edge y = 0
            if 0 <= x <= w:
                ep.append((int(round(x)), 0))

            x = (slope*x1 + h - y1)/slope   # bottom edge y = h
            if 0 <= x <= w:
                ep.append((int(round(x)), h))
            
            y = -slope*x1 + y1  # left edge x = 0
            if 0 <= y <= h:
                ep.append( (0, (int(round(y)))) )
            
            y = slope*(w - x1) + y1 # right edge x = w
            if 0 <= y <= h:
                ep.append( (w, (int(round(y)))) )

        ep = list(set(ep))  # remove duplicates of points if line cross image at corners
        ep.sort()
        
        return Line(self.image, ep)


######################################################################
class Barcode(Feature):
    """
    **SUMMARY**

    The Barcode Feature wrappers the object returned by findBarcode(), a zbar symbol

    * The x,y coordinate is the center of the code.
    * points represents the four boundary points of the feature.  Note: for QR codes, these points are the reference rectangls, and are quadrangular, rather than rectangular with other datamatrix types.
    * data is the parsed data of the code.

    **SEE ALSO**

    :py:meth:`ImageClass.findBarcodes()`
    """
    data = ""

    #given a ZXing bar
    def __init__(self, i, zbsymbol):
        self.image = i

        locs = zbsymbol.location
        if len(locs) > 4:
            xs = [l[0] for l in locs]
            ys = [l[1] for l in locs]
            xmax = np.max(xs)
            xmin = np.min(xs)
            ymax = np.max(ys)
            ymin = np.min(ys)
            points = ((xmin, ymin),(xmin,ymax),(xmax, ymax),(xmax,ymin))
        else:
            points = copy(locs) # hopefully this is in tl clockwise order

        super(Barcode, self).__init__(i, 0, 0,points)
        self.data = zbsymbol.data
        self.points = copy(points)
        numpoints = len(self.points)
        self.x = 0
        self.y = 0

        for p in self.points:
            self.x += p[0]
            self.y += p[1]

        if (numpoints):
            self.x /= numpoints
            self.y /= numpoints

    def __repr__(self):
        return "%s.%s at (%d,%d), read data: %s" % (self.__class__.__module__, self.__class__.__name__, self.x, self.y, self.data)

    def draw(self, color = (255, 0, 0),width=1):
        """

        **SUMMARY**

        Draws the bounding area of the barcode, given by points.  Note that for
        QR codes, these points are the reference boxes, and so may "stray" into
        the actual code.


        **PARAMETERS**

        * *color* - An RGB color triplet.
        * *width* - if width is less than zero we draw the feature filled in, otherwise we draw the
          contour using the specified width.


        **RETURNS**

        Nothing - this is an inplace operation that modifies the source images drawing layer.


        """
        self.image.drawLine(self.points[0], self.points[1], color,width)
        self.image.drawLine(self.points[1], self.points[2], color,width)
        self.image.drawLine(self.points[2], self.points[3], color,width)
        self.image.drawLine(self.points[3], self.points[0], color,width)

    def length(self):
        """
        **SUMMARY**

        Returns the longest side of the quandrangle formed by the boundary points.

        **RETURNS**

        A floating point length value.

        **EXAMPLE**

        >>> img = Image("mycode.jpg")
        >>> bc = img.findBarcode()
        >>> print bc[-1].length()

        """
        sqform = spsd.squareform(spsd.pdist(self.points, "euclidean"))
        #get pairwise distances for all points
        #note that the code is a quadrilateral
        return max(sqform[0][1], sqform[1][2], sqform[2][3], sqform[3][0])

    def area(self):
        """
        **SUMMARY**

        Returns the area defined by the quandrangle formed by the boundary points

 
        **RETURNS**

        An integer area value.

        **EXAMPLE**

        >>> img = Image("mycode.jpg")
        >>> bc = img.findBarcode()
        >>> print bc[-1].area()


        """
        #calc the length of each side in a square distance matrix
        sqform = spsd.squareform(spsd.pdist(self.points, "euclidean"))

        #squareform returns a N by N matrix
        #boundry line lengths
        a = sqform[0][1]
        b = sqform[1][2]
        c = sqform[2][3]
        d = sqform[3][0]

        #diagonals
        p = sqform[0][2]
        q = sqform[1][3]

        #perimeter / 2
        s = (a + b + c + d)/2.0

        #i found the formula to do this on wikihow.  Yes, I am that lame.
        #http://www.wikihow.com/Find-the-Area-of-a-Quadrilateral
        return sqrt((s - a) * (s - b) * (s - c) * (s - d) - (a * c + b * d + p * q) * (a * c + b * d - p * q) / 4)

######################################################################
class HaarFeature(Feature):
    """
    **SUMMARY**

    The HaarFeature is a rectangle returned by the FindHaarFeature() function.

    * The x,y coordinates are defined by the center of the bounding rectangle.
    * The classifier property refers to the cascade file used for detection .
    * Points are the clockwise points of the bounding rectangle, starting in upper left.

    """
    classifier = ""
    _width = ""
    _height = ""
    neighbors = ''
    featureName = 'None'

    def __init__(self, i, haarobject, haarclassifier = None, cv2flag=True):
        self.image = i
        if cv2flag == False:
            ((x, y, width, height), self.neighbors) = haarobject
        elif cv2flag == True:
            (x, y, width, height) = haarobject
        at_x = x + width/2
        at_y = y + height/2 #set location of feature to middle of rectangle
        points = ((x, y), (x + width, y), (x + width, y + height), (x, y + height))

         #set bounding points of the rectangle
        self.classifier = haarclassifier
        if( haarclassifier is not None ):
            self.featureName = haarclassifier.getName()

        super(HaarFeature, self).__init__(i, at_x, at_y, points)


    def draw(self, color = (0, 255, 0),width=1):
        """
        **SUMMARY**

        Draw the bounding rectangle, default color green.

        **PARAMETERS**

        * *color* - An RGB color triplet.
        * *width* - if width is less than zero we draw the feature filled in, otherwise we draw the
          contour using the specified width.


        **RETURNS**

        Nothing - this is an inplace operation that modifies the source images drawing layer.

        """
        self.image.drawLine(self.points[0], self.points[1], color,width)
        self.image.drawLine(self.points[1], self.points[2], color,width)
        self.image.drawLine(self.points[2], self.points[3], color,width)
        self.image.drawLine(self.points[3], self.points[0], color,width)

    def __getstate__(self):
        dict = self.__dict__.copy()
        if 'classifier' in dict:
            del dict["classifier"]
        return dict

    def meanColor(self):
        """
        **SUMMARY**

        Find the mean color of the boundary rectangle.

        **RETURNS**

        Returns an  RGB triplet that corresponds to the mean color of the feature.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> face = HaarCascade("face.xml")
        >>> faces = img.findHaarFeatures(face)
        >>> print faces[-1].meanColor()

        """
        crop = self.image[self.points[0][0]:self.points[1][0], self.points[0][1]:self.points[2][1]]
        return crop.meanColor()


    def area(self):
        """
        **SUMMARY**

        Returns the area of the feature in pixels.

        **RETURNS**

        The area of the feature in pixels.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> face = HaarCascade("face.xml")
        >>> faces = img.findHaarFeatures(face)
        >>> print faces[-1].area()

        """
        return self.width() * self.height()

######################################################################
class Chessboard(Feature):
    """
    **SUMMARY**

    This class is used for Calibration, it uses a chessboard
    to calibrate from pixels to real world measurements.
    """
    spCorners = []
    dimensions = ()

    def __init__(self, i, dim, subpixelCorners):
        self.dimensions = dim
        self.spCorners = subpixelCorners
        at_x = np.average(np.array(self.spCorners)[:, 0])
        at_y = np.average(np.array(self.spCorners)[:, 1])

        posdiagsorted = sorted(self.spCorners, key = lambda corner: corner[0] + corner[1])
        #sort corners along the x + y axis
        negdiagsorted = sorted(self.spCorners, key = lambda corner: corner[0] - corner[1])
        #sort corners along the x - y axis

        points = (posdiagsorted[0], negdiagsorted[-1], posdiagsorted[-1], negdiagsorted[0])
        super(Chessboard, self).__init__(i, at_x, at_y, points)


    def draw(self, no_needed_color = None):
        """
        **SUMMARY**


        Draws the chessboard corners.  We take a color param, but ignore it.

        **PARAMETERS**

        * *no_needed_color* - An RGB color triplet that isn't used


        **RETURNS**

        Nothing - this is an inplace operation that modifies the source images drawing layer.

        """
        cv.DrawChessboardCorners(self.image.getBitmap(), self.dimensions, self.spCorners, 1)

    def area(self):
        """
        **SUMMARY**

        Returns the mean of the distance between corner points in the chessboard
        Given that the chessboard is of a known size, this can be used as a
        proxy for distance from the camera

        **RETURNS**

        Returns the mean distance between the corners.

        **EXAMPLE**

        >>> img = Image("corners.jpg")
        >>> feats = img.findChessboardCorners()
        >>> print feats[-1].area()

        """
        #note, copying this from barcode means we probably need a subclass of
        #feature called "quandrangle"
        sqform = spsd.squareform(spsd.pdist(self.points, "euclidean"))
        a = sqform[0][1]
        b = sqform[1][2]
        c = sqform[2][3]
        d = sqform[3][0]
        p = sqform[0][2]
        q = sqform[1][3]
        s = (a + b + c + d)/2.0
        return 2 * sqrt((s - a) * (s - b) * (s - c) * (s - d) - (a * c + b * d + p * q) * (a * c + b * d - p * q) / 4)

######################################################################
class TemplateMatch(Feature):
    """
    **SUMMARY**

    This class is used for template (pattern) matching in images.
    The template matching cannot handle scale or rotation.

    """

    template_image = None
    quality = 0
    w = 0
    h = 0

    def __init__(self, image, template, location, quality):
        self.template_image = template # -- KAT - TRYING SOMETHING
        self.image = image
        self.quality = quality
        w = template.width
        h = template.height
        at_x = location[0]
        at_y = location[1]
        points = [(at_x,at_y),(at_x+w,at_y),(at_x+w,at_y+h),(at_x,at_y+h)]

        super(TemplateMatch, self).__init__(image, at_x, at_y, points)

    def _templateOverlaps(self,other):
        """
        Returns true if this feature overlaps another template feature.
        """
        (maxx,minx,maxy,miny) = self.extents()
        overlap = False
        for p in other.points:
            if( p[0] <= maxx and p[0] >= minx and p[1] <= maxy and p[1] >= miny ):
                overlap = True
                break

        return overlap


    def consume(self, other):
        """
        Given another template feature, make this feature the size of the two features combined.
        """
        (maxx,minx,maxy,miny) = self.extents()
        (maxx0,minx0,maxy0,miny0) = other.extents()

        maxx = max(maxx,maxx0)
        minx = min(minx,minx0)
        maxy = max(maxy,maxy0)
        miny = min(miny,miny0)
        self.x = minx
        self.y = miny
        self.points = [(minx,miny),(minx,maxy),(maxx,maxy),(maxx,miny)]
        self._updateExtents()


    def rescale(self,w,h):
        """
        This method keeps the feature's center the same but sets a new width and height
        """
        (maxx,minx,maxy,miny) = self.extents()
        xc = minx+((maxx-minx)/2)
        yc = miny+((maxy-miny)/2)
        x = xc-(w/2)
        y = yc-(h/2)
        self.x = x
        self.y = y
        self.points = [(x,y),
                       (x+w,y),
                       (x+w,y+h),
                       (x,y+h)]
        self._updateExtents()

    def crop(self):
        (maxx,minx,maxy,miny) = self.extents()
        return self.image.crop(minx,miny,maxx-minx,maxy-miny)

    def draw(self, color = Color.GREEN, width = 1):
        """
        **SUMMARY**

        Draw the bounding rectangle, default color green.

        **PARAMETERS**

        * *color* - An RGB color triplet.
        * *width* - if width is less than zero we draw the feature filled in, otherwise we draw the
          contour using the specified width.

        **RETURNS**

        Nothing - this is an inplace operation that modifies the source images drawing layer.
        """
        self.image.dl().rectangle((self.x,self.y), (self.width(), self.height()), color = color, width=width)
######################################################################
class Circle(Feature):
    """
    **SUMMARY**

    Class for a general circle feature with a center at (x,y) and a radius r

    """
    x = 0.00
    y = 0.00
    r = 0.00
    image = "" #parent image
    points = []
    avgColor = None

    def __init__(self, i, at_x, at_y, r):
        self.r = r
        self.avgColor = None
        points = [(at_x-r,at_y-r),(at_x+r,at_y-r),(at_x+r,at_y+r),(at_x-r,at_y+r)]
        super(Circle, self).__init__(i, at_x, at_y, points)
        segments = 18
        rng = range(1,segments+1)
        self.mContour = []
        for theta in rng:
            rp = 2.0*math.pi*float(theta)/float(segments)
            x = (r*math.sin(rp))+at_x
            y = (r*math.cos(rp))+at_y
            self.mContour.append((x,y))



    def draw(self, color = Color.GREEN,width=1):
        """
        **SUMMARY**

        With no dimension information, color the x,y point for the feature.

        **PARAMETERS**

        * *color* - An RGB color triplet.
        * *width* - if width is less than zero we draw the feature filled in, otherwise we draw the
          contour using the specified width.

        **RETURNS**

        Nothing - this is an inplace operation that modifies the source images drawing layer.

        """
        self.image.dl().circle((self.x,self.y),self.r,color,width)

    def show(self, color = Color.GREEN):
        """
        **SUMMARY**

        This function will automatically draw the features on the image and show it.
        It is a basically a shortcut function for development and is the same as:

        **PARAMETERS**

        * *color* - the color of the feature as an rgb triplet.

        **RETURNS**

        Nothing - this is an inplace operation that modifies the source images drawing layer.

        **EXAMPLE**

        >>> img = Image("logo")
        >>> feat = img.findCircle()
        >>> feat[0].show()

        """
        self.draw(color)
        self.image.show()

    def distanceFrom(self, point = (-1, -1)):
        """
        **SUMMARY**

        Given a point (default to center of the image), return the euclidean distance of x,y from this point.

        **PARAMETERS**

        * *point* - The point, as an (x,y) tuple on the image to measure distance from.

        **RETURNS**

        The distance as a floating point value in pixels.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findCircle()
        >>> blobs[-1].distanceFrom(blobs[-2].coordinates())

        """
        if (point[0] == -1 or point[1] == -1):
            point = np.array(self.image.size()) / 2
        return spsd.euclidean(point, [self.x, self.y])

    def meanColor(self):
        """

        **SUMMARY**

        Returns the average color within the circle.

        **RETURNS**

        Returns an RGB triplet that corresponds to the mean color of the feature.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> c = img.findCircle()
        >>> c[-1].meanColor()

        """
        #generate the mask
        if( self.avgColor is None):
            mask = self.image.getEmpty(1)
            cv.Zero(mask)
            cv.Circle(mask,(self.x,self.y),self.r,color=(255,255,255),thickness=-1)
            temp = cv.Avg(self.image.getBitmap(),mask)
            self.avgColor = (temp[0],temp[1],temp[2])
        return self.avgColor

    def area(self):
        """
        Area covered by the feature -- for a pixel, 1

        **SUMMARY**

        Returns a numpy array of the area of each feature in pixels.

        **RETURNS**

        A numpy array of all the positions in the featureset.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> feats = img.findBlobs()
        >>> xs = feats.coordinates()
        >>> print xs


        """
        return self.r*self.r*pi

    def perimeter(self):
        """
        **SUMMARY**

        Returns the perimeter of the circle feature in pixels.
        """
        return 2*pi*self.r

    def width(self):
        """
        **SUMMARY**

        Returns the width of the feature -- for compliance just r*2

        """
        return self.r*2

    def height(self):
        """
        **SUMMARY**

        Returns the height of the feature -- for compliance just r*2
        """
        return self.r*2

    def radius(self):
        """
        **SUMMARY**

        Returns the radius of the circle in pixels.

        """
        return self.r

    def diameter(self):
        """
        **SUMMARY**

        Returns the diameter of the circle in pixels.

        """
        return self.r*2

    def crop(self,noMask=False):
        """
        **SUMMARY**

        This function returns the largest bounding box for an image.

        **PARAMETERS**

        * *noMask* - if noMask=True we return the bounding box image of the circle.
          if noMask=False (default) we return the masked circle with the rest of the area set to black

        **RETURNS**

        The masked circle image.

        """
        if( noMask ):
            return self.image.crop(self.x, self.y, self.width(), self.height(), centered = True)
        else:
            mask = self.image.getEmpty(1)
            result = self.image.getEmpty()
            cv.Zero(mask)
            cv.Zero(result)
            #if you want to shave a bit of time we go do the crop before the blit
            cv.Circle(mask,(self.x,self.y),self.r,color=(255,255,255),thickness=-1)
            cv.Copy(self.image.getBitmap(),result,mask)
            retVal = Image(result)
            retVal = retVal.crop(self.x, self.y, self.width(), self.height(), centered = True)
            return retVal

##################################################################################
class KeyPoint(Feature):
    """
    **SUMMARY**

    The class is place holder for SURF/SIFT/ORB/STAR keypoints.

    """
    x = 0.00
    y = 0.00
    r = 0.00
    image = "" #parent image
    points = []
    __avgColor = None
    mAngle = 0
    mOctave = 0
    mResponse = 0.00
    mFlavor = ""
    mDescriptor = None
    mKeyPoint = None
    def __init__(self, i, keypoint, descriptor=None, flavor="SURF" ):
#i, point, diameter, descriptor=None,angle=-1, octave=0,response=0.00,flavor="SURF"):
        self.mKeyPoint = keypoint
        x = keypoint.pt[1] #KAT
        y = keypoint.pt[0]
        self._r = keypoint.size/2.0
        self._avgColor = None
        self.image = i
        self.mAngle = keypoint.angle
        self.mOctave = keypoint.octave
        self.mResponse = keypoint.response
        self.mFlavor = flavor
        self.mDescriptor = descriptor
        r = self._r
        points  = ((x+r,y+r),(x+r,y-r),(x-r,y-r),(x-r,y+r))
        super(KeyPoint, self).__init__(i, x, y, points)

        segments = 18
        rng = range(1,segments+1)
        self.points = []
        for theta in rng:
            rp = 2.0*math.pi*float(theta)/float(segments)
            x = (r*math.sin(rp))+self.x
            y = (r*math.cos(rp))+self.y
            self.points.append((x,y))


    def getObject(self):
        """
        **SUMMARY**

        Returns the raw keypoint object.

        """
        return self.mKeyPoint

    def descriptor(self):
        """
        **SUMMARY**

        Returns the raw keypoint descriptor.

        """
        return self.mDescriptor

    def quality(self):
        """
        **SUMMARY**

        Returns the quality metric for the keypoint object.

        """
        return self.mResponse

    def octave(self):
        """
        **SUMMARY**

        Returns the raw keypoint's octave (if it has one).

        """
        return self.mOctave

    def flavor(self):
        """
        **SUMMARY**

        Returns the type of keypoint as a string (e.g. SURF/MSER/ETC)

        """
        return self.mFlavor

    def angle(self):
        """
        **SUMMARY**

        Return the angle (theta) in degrees of the feature. The default is 0 (horizontal).

        **RETURNS**

        An angle value in degrees.

        """
        return self.mAngle


    def draw(self, color = Color.GREEN, width=1):
        """
        **SUMMARY**

        Draw a circle around the feature.  Color tuple is single parameter, default is Green.

        **PARAMETERS**

        * *color* - An RGB color triplet.
        * *width* - if width is less than zero we draw the feature filled in, otherwise we draw the
          contour using the specified width.


        **RETURNS**

        Nothing - this is an inplace operation that modifies the source images drawing layer.

        """
        self.image.dl().circle((self.x,self.y),self._r,color,width)
        pt1 = (int(self.x),int(self.y))
        pt2 = (int(self.x+(self.radius()*sin(radians(self.angle())))),
               int(self.y+(self.radius()*cos(radians(self.angle())))))
        self.image.dl().line(pt1,pt2,color,width)

    def show(self, color = Color.GREEN):
        """
        **SUMMARY**

        This function will automatically draw the features on the image and show it.
        It is a basically a shortcut function for development and is the same as:

        >>> img = Image("logo")
        >>> feat = img.findBlobs()
        >>> if feat: feat.draw()
        >>> img.show()

        """
        self.draw(color)
        self.image.show()

    def distanceFrom(self, point = (-1, -1)):
        """
        **SUMMARY**

        Given a point (default to center of the image), return the euclidean distance of x,y from this point
        """
        if (point[0] == -1 or point[1] == -1):
            point = np.array(self.image.size()) / 2
        return spsd.euclidean(point, [self.x, self.y])

    def meanColor(self):
        """
        **SUMMARY**

        Return the average color within the feature's radius

        **RETURNS**

        Returns an  RGB triplet that corresponds to the mean color of the feature.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> kp = img.findKeypoints()
        >>> c = kp[0].meanColor()

        """
        #generate the mask
        if( self._avgColor is None):
            mask = self.image.getEmpty(1)
            cv.Zero(mask)
            cv.Circle(mask,(int(self.x),int(self.y)),int(self._r),color=(255,255,255),thickness=-1)
            temp = cv.Avg(self.image.getBitmap(),mask)
            self._avgColor = (temp[0],temp[1],temp[2])
        return self._avgColor

    def colorDistance(self, color = (0, 0, 0)):
        """
          Return the euclidean color distance of the color tuple at x,y from a given color (default black)
        """
        return spsd.euclidean(np.array(color), np.array(self.meanColor()))

    def perimeter(self):
        """
        **SUMMARY**

        Returns the perimeter of the circle feature in pixels.
        """
        return 2*pi*self._r

    def width(self):
        """
        **SUMMARY**

        Returns the width of the feature -- for compliance just r*2

        """
        return self._r*2

    def height(self):
        """
        **SUMMARY**

        Returns the height of the feature -- for compliance just r*2
        """
        return self._r*2

    def radius(self):
        """
        **SUMMARY**

        Returns the radius of the circle in pixels.

        """
        return self._r

    def diameter(self):
        """
        **SUMMARY**

        Returns the diameter of the circle in pixels.

        """
        return self._r*2

    def crop(self,noMask=False):
        """
        **SUMMARY**

        This function returns the largest bounding box for an image.

        **PARAMETERS**

        * *noMask* - if noMask=True we return the bounding box image of the circle.
          if noMask=False (default) we return the masked circle with the rest of the area set to black

        **RETURNS**

        The masked circle image.

        """
        if( noMask ):
            return self.image.crop(self.x, self.y, self.width(), self.height(), centered = True)
        else:
            mask = self.image.getEmpty(1)
            result = self.image.getEmpty()
            cv.Zero(mask)
            cv.Zero(result)
            #if you want to shave a bit of time we go do the crop before the blit
            cv.Circle(mask,(int(self.x),int(self.y)),int(self._r),color=(255,255,255),thickness=-1)
            cv.Copy(self.image.getBitmap(),result,mask)
            retVal = Image(result)
            retVal = retVal.crop(self.x, self.y, self.width(), self.height(), centered = True)
            return retVal

######################################################################
class Motion(Feature):
    """
    **SUMMARY**

    The motion feature is used to encapsulate optical flow vectors. The feature
    holds the length and direction of the vector.

    """
    x = 0.0
    y = 0.0
    image = "" #parent image
    points = []
    dx = 0.00
    dy = 0.00
    norm_dy = 0.00
    norm_dx = 0.00
    window = 7

    def __init__(self, i, at_x, at_y,dx,dy,wndw):
        """
        i    - the source image.
        at_x - the sample x pixel position on the image.
        at_y - the sample y pixel position on the image.
        dx   - the x component of the optical flow vector.
        dy   - the y component of the optical flow vector.
        wndw - the size of the sample window (we assume it is square).
        """
        self.dx = dx  # the direction of the vector
        self.dy = dy
        self.window = wndw # the size of the sample window
        sz = wndw/2
        # so we center at the flow vector
        points  = [(at_x+sz,at_y+sz),(at_x-sz,at_y+sz),(at_x+sz,at_y+sz),(at_x+sz,at_y-sz)]
        super(Motion, self).__init__(i, at_x, at_y, points)

    def draw(self, color = Color.GREEN, width=1,normalize=True):
        """
        **SUMMARY**
        Draw the optical flow vector going from the sample point along the length of the motion vector.

        **PARAMETERS**

        * *color* - An RGB color triplet.
        * *width* - if width is less than zero we draw the feature filled in, otherwise we draw the
          contour using the specified width.
        * *normalize* - normalize the vector size to the size of the block (i.e. the biggest optical flow
          vector is scaled to the size of the block, all other vectors are scaled relative to
          the longest vector.

        **RETURNS**

        Nothing - this is an inplace operation that modifies the source images drawing layer.

        """
        new_x = 0
        new_y = 0
        if( normalize ):
            win = self.window/2
            w = math.sqrt((win*win)*2)
            new_x = (self.norm_dx*w) + self.x
            new_y = (self.norm_dy*w) + self.y
        else:
            new_x = self.x + self.dx
            new_y = self.y + self.dy

        self.image.dl().line((self.x,self.y),(new_x,new_y),color,width)


    def normalizeTo(self, max_mag):
        """
        **SUMMARY**

        This helper method normalizes the vector give an input magnitude.
        This is helpful for keeping the flow vector inside the sample window.
        """
        if( max_mag == 0 ):
            self.norm_dx = 0
            self.norm_dy = 0
            return None
        mag = self.magnitude()
        new_mag = mag/max_mag
        unit = self.unitVector()
        self.norm_dx = unit[0]*new_mag
        self.norm_dy = unit[1]*new_mag

    def magnitude(self):
        """
        Returns the magnitude of the optical flow vector.
        """
        return sqrt((self.dx*self.dx)+(self.dy*self.dy))

    def unitVector(self):
        """
        Returns the unit vector direction of the flow vector as an (x,y) tuple.
        """
        mag = self.magnitude()
        if( mag != 0.00 ):
            return (float(self.dx)/mag,float(self.dy)/mag)
        else:
            return (0.00,0.00)

    def vector(self):
        """
        Returns the raw direction vector as an (x,y) tuple.
        """
        return (self.dx,self.dy)

    def windowSz(self):
        """
        Return the window size that we sampled over.
        """
        return self.window

    def meanColor(self):
        """
        Return the color tuple from x,y
        **SUMMARY**

        Return a numpy array of the average color of the area covered by each Feature.

        **RETURNS**

        Returns an array of RGB triplets the correspond to the mean color of the feature.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> kp = img.findKeypoints()
        >>> c = kp.meanColor()

        """
        x = int(self.x-(self.window/2))
        y = int(self.y-(self.window/2))
        return self.image.crop(x,y,int(self.window),int(self.window)).meanColor()


    def crop(self):
        """
        This function returns the image in the sample window around the flow vector.

        Returns Image
        """
        x = int(self.x-(self.window/2))
        y = int(self.y-(self.window/2))

        return self.image.crop(x,y,int(self.window),int(self.window))

######################################################################
class KeypointMatch(Feature):
    """
    This class encapsulates a keypoint match between images of an object.
    It is used to record a template of one image as it appears in another image
    """
    x = 0.00
    y = 0.00
    image = "" #parent image
    points = []
    _minRect = []
    _avgColor = None
    _homography = []
    _template = None
    def __init__(self, image,template,minRect,_homography):
        self._template = template
        self._minRect = minRect
        self._homography = _homography
        xmax = 0
        ymax = 0
        xmin = image.width
        ymin = image.height
        for p in minRect:
            if( p[0] > xmax ):
                xmax = p[0]
            if( p[0] < xmin ):
                xmin = p[0]
            if( p[1] > ymax ):
                ymax = p[1]
            if( p[1] < ymin ):
                ymin = p[1]

        width = (xmax-xmin)
        height = (ymax-ymin)
        at_x = xmin + (width/2)
        at_y = ymin + (height/2)
        #self.x = at_x
        #self.y = at_y
        points = [(xmin,ymin),(xmin,ymax),(xmax,ymax),(xmax,ymin)]
        #self._updateExtents()
        #self.image = image
        #points =
        super(KeypointMatch, self).__init__(image, at_x, at_y, points)

    def draw(self, color = Color.GREEN,width=1):
        """
        The default drawing operation is to draw the min bounding
        rectangle in an image.

        **SUMMARY**

        Draw a small circle around the corner.  Color tuple is single parameter, default is Red.

        **PARAMETERS**

        * *color* - An RGB color triplet.
        * *width* - if width is less than zero we draw the feature filled in, otherwise we draw the
          contour using the specified width.


        **RETURNS**

        Nothing - this is an inplace operation that modifies the source images drawing layer.


        """
        self.image.dl().line(self._minRect[0],self._minRect[1],color,width)
        self.image.dl().line(self._minRect[1],self._minRect[2],color,width)
        self.image.dl().line(self._minRect[2],self._minRect[3],color,width)
        self.image.dl().line(self._minRect[3],self._minRect[0],color,width)

    def drawRect(self, color = Color.GREEN,width=1):
        """
        This method draws the axes alligned square box of the template
        match. This box holds the minimum bounding rectangle that describes
        the object. If the minimum bounding rectangle is axes aligned
        then the two bounding rectangles will match.
        """
        self.image.dl().line(self.points[0],self.points[1],color,width)
        self.image.dl().line(self.points[1],self.points[2],color,width)
        self.image.dl().line(self.points[2],self.points[3],color,width)
        self.image.dl().line(self.points[3],self.points[0],color,width)


    def crop(self):
        """
        Returns a cropped image of the feature match. This cropped version is the
        axes aligned box masked to just include the image data of the minimum bounding
        rectangle.
        """
        tl = self.topLeftCorner()
        raw = self.image.crop(tl[0],tl[1],self.width(),self.height()) # crop the minbouding rect
        return raw


    def meanColor(self):
        """
        return the average color within the circle
        **SUMMARY**

        Return a numpy array of the average color of the area covered by each Feature.

        **RETURNS**

        Returns an array of RGB triplets the correspond to the mean color of the feature.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> kp = img.findKeypoints()
        >>> c = kp.meanColor()

        """
        if( self._avgColor is None ):
            TL = self.topLeftCorner()
            raw = self.image.crop(TL[0],TL[0],self.width(),self.height()) # crop the minbouding rect
            mask = Image((self.width(),self.height()))
            mask.dl().polygon(self._minRect,color=Color.WHITE,filled=TRUE)
            mask = mask.applyLayers()
            retVal = cv.Avg(raw.getBitmap(),mask._getGrayscaleBitmap())
            self._avgColor = retVal
        else:
            retVal = self._avgColor
        return retVal


    def getMinRect(self):
        """
        Returns the minimum bounding rectangle of the feature as a list
        of (x,y) tuples.
        """
        return self._minRect

    def getHomography(self):
        """
        Returns the _homography matrix used to calulate the minimum bounding
        rectangle.
        """
        return self._homography
######################################################################
"""
Create a shape context descriptor.
"""
class ShapeContextDescriptor(Feature):
    x = 0.00
    y = 0.00
    image = "" #parent image
    points = []
    _minRect = []
    _avgColor = None
    _descriptor = None
    _sourceBlob = None
    def __init__(self, image,point,descriptor,blob):
        self._descriptor = descriptor
        self._sourceBlob = blob
        x = point[0]
        y = point[1]
        points = [(x-1,y-1),(x+1,y-1),(x+1,y+1),(x-1,y+1)]
        super(ShapeContextDescriptor, self).__init__(image, x, y, points)

    def draw(self, color = Color.GREEN,width=1):
        """
        The default drawing operation is to draw the min bounding
        rectangle in an image.

        **SUMMARY**

        Draw a small circle around the corner.  Color tuple is single parameter, default is Red.

        **PARAMETERS**

        * *color* - An RGB color triplet.
        * *width* - if width is less than zero we draw the feature filled in, otherwise we draw the
          contour using the specified width.


        **RETURNS**

        Nothing - this is an inplace operation that modifies the source images drawing layer.


        """
        self.image.dl().circle((self.x,self.y),3,color,width)
######################################################################
class ROI(Feature):
    """
    This class creates a region of interest that inherit from one
    or more features or no features at all. 
    """
    x = 0 # the center x coordinate
    y = 0 # the center y coordinate
    w = 0
    h = 0
    xtl = 0 # top left x
    ytl = 0 # top left y
    # we are going to assume x,y,w,h is our canonical form
    points = [] # point list for cross compatibility
    image = None
    subFeatures = []
    _meanColor = None
    def __init__(self,x,y=None,w=None,h=None,image=None ):
        """
        **SUMMARY**

        This function can handle just about whatever you throw at it
        and makes a it into a feature. Valid input items are tuples and lists
        of x,y points, features, featuresets, two x,y points, and a
        set of x,y,width,height values.


        **PARAMETERS**

        * *x* - this can be just about anything, a list or tuple of x points,
        a corner of the image, a list of (x,y) points, a Feature, a FeatureSet
        * *y* - this is usually a second point or set of y values.
        * *w* - a width
        * *h* - a height.
       
        **RETURNS**

        Nothing.

        **EXAMPLE**

        >>> img = Image('lenna')
        >>> x,y = np.where(img.threshold(230).getGrayNumpy() > 128 )
        >>> roi = ROI(zip(x,y),img)
        >>> roi = ROI(x,y,img)

        """
        #After forgetting to set img=Image I put this catch
        # in to save some debugging headache. 
        if( isinstance(y,Image) ):
            self.image = y
            y = None
        elif( isinstance(w,Image) ):
            self.image = w
            w = None
        elif( isinstance(h,Image) ):
            self.image = h
            h = None
        else:
            self.image = image
            
        if( image is None and isinstance(x,(Feature,FeatureSet))):
            if( isinstance(x,Feature) ):
                self.image = x.image
            if( isinstance(x,FeatureSet) and len(x) > 0 ):
                self.image = x[0].image
                
        if(isinstance(x,Feature)):
            self.subFeatures = FeatureSet([x])
        elif(isinstance(x,(list,tuple)) and len(x) > 0 and isinstance(x,Feature)):
            self.subFeatures = FeatureSet(x)

        result = self._standardize(x,y,w,h)
        if result is None:
            logger.warning("Could not create an ROI from your data.")
            return
        self._rebase(result)
        

    def resize(self,w,h=None,percentage=True):
        """
        **SUMMARY**

        Contract/Expand the roi. By default use a percentage, otherwise use pixels.
        This is all done relative to the center of the roi

        
        **PARAMETERS**

        * *w* - the percent to grow shrink the region is the only parameter, otherwise
                it is the new ROI width
        * *h* - The new roi height in terms of pixels or a percentage.
        * *percentage* - If true use percentages (e.g. 2 doubles the size), otherwise
                         use pixel values. 
        * *h* - a height.
       
        **RETURNS**

        Nothing.

        **EXAMPLE**

        >>> roi = ROI(10,10,100,100,img)
        >>> roi.resize(2)
        >>> roi.show()

        """
        if(h is None and isinstance(w,(tuple,list))):
            h = w[1]
            w = w[0]
        if(percentage):
            if( h is None ):
                h = w
            nw = self.w * w
            nh = self.h * h
            nx = self.xtl + ((self.w-nw)/2.0)
            ny = self.ytl + ((self.h-nh)/2.0)
            self._rebase([nx,ny,nw,nh])
        else:
            nw = self.w+w
            nh = self.h+h
            nx = self.xtl + ((self.w-nw)/2.0)
            ny = self.ytl + ((self.h-nh)/2.0)
            self._rebase([nx,ny,nw,nh])

    def overlaps(self,otherROI):
        for p in otherROI.points:
            if( p[0] <= self.maxX() and p[0] >= self.minX() and
                p[1] <= self.maxY() and p[1] >= self.minY() ):
                return True
        return False

    def translate(self,x=0,y=0):
        """
        **SUMMARY**
        
        Move the roi.
        
        **PARAMETERS**

        * *x* - Move the ROI horizontally.
        * *y* - Move the ROI vertically
               
        **RETURNS**

        Nothing.

        **EXAMPLE**

        >>> roi = ROI(10,10,100,100,img)
        >>> roi.translate(30,30)
        >>> roi.show()

        """
        if( x == 0 and y == 0 ):
            return
            
        if(y == 0 and isinstance(x,(tuple,list))):
            y = x[1]
            x = x[0]
            
        if( isinstance(x,(float,int)) and isinstance(y,(float,int))):
            self._rebase([self.xtl+x,self.ytl+y,self.w,self.h])

    def toXYWH(self):
        """
        **SUMMARY**
        
        Get the ROI as a list of the top left corner's x and y position
        and the roi's width and height in pixels.

        **RETURNS**

        A list of the form [x,y,w,h]

        **EXAMPLE**

        >>> roi = ROI(10,10,100,100,img)
        >>> roi.translate(30,30)
        >>> print roi.toXYWH()

        """        
        return [self.xtl,self.ytl,self.w,self.h]
        
    def toTLAndBR(self):
        """
        **SUMMARY**
        
        Get the ROI as a list of tuples of the ROI's top left
        corner and bottom right corner.

        **RETURNS**

        A list of the form [(x,y),(x,y)]

        **EXAMPLE**

        >>> roi = ROI(10,10,100,100,img)
        >>> roi.translate(30,30)
        >>> print roi.toTLAndBR()
        
        """        
        return [(self.xtl,self.ytl),(self.xtl+self.w,self.ytl+self.h)]


    def toPoints(self):
        """
        **SUMMARY**
        
        Get the ROI as a list of four points that make up the bounding rectangle.
       
        
        **RETURNS**

        A list of the form [(x,y),(x,y),(x,y),(x,y)]

        **EXAMPLE**

        >>> roi = ROI(10,10,100,100,img)
        >>> print roi.toPoints()
        """        

        tl = (self.xtl,self.ytl)
        tr = (self.xtl+self.w,self.ytl)
        br = (self.xtl+self.w,self.ytl+self.h)
        bl = (self.xtl,self.ytl+self.h)
        return [tl,tr,br,bl]
        
    def toUnitXYWH(self):
        """
        **SUMMARY**
        
        Get the ROI as a list, the values are top left x, to left y,
        width and height. These values are scaled to unit values with
        respect to the source image.. 
       
        
        **RETURNS**

        A list of the form [x,y,w,h]

        **EXAMPLE**

        >>> roi = ROI(10,10,100,100,img)
        >>> print roi.toUnitXYWH()
        """        
        if(self.image is None):
            return None
        srcw = float(self.image.width)
        srch = float(self.image.height)
        x,y,w,h = self.toXYWH()
        nx = 0
        ny = 0
        if( x != 0 ):
            nx = x/srcw
        if( y != 0 ):
            ny = y/srch
        
        return [nx,ny,w/srcw,h/srch]
        
    def toUnitTLAndBR(self):
        """
        **SUMMARY**
        
        Get the ROI as a list of tuples of the ROI's top left
        corner and bottom right corner. These coordinates are in unit
        length values with respect to the source image.

        **RETURNS**

        A list of the form [(x,y),(x,y)]

        **EXAMPLE**

        >>> roi = ROI(10,10,100,100,img)
        >>> roi.translate(30,30)
        >>> print roi.toUnitTLAndBR()
        
        """
        
        if(self.image is None):
            return None
        srcw = float(self.image.width)
        srch = float(self.image.height)
        x,y,w,h = self.toXYWH()
        nx = 0
        ny = 0
        nw = w/srcw
        nh = h/srch
        if( x != 0 ):
            nx = x/srcw
        if( y != 0 ):
            ny = y/srch
        
        return [(nx,ny),(nx+nw,ny+nh)]


    def toUnitPoints(self):
        """
        **SUMMARY**
        
        Get the ROI as a list of four points that make up the bounding rectangle.
        Each point is represented in unit coordinates with respect to the
        souce image.
        
        **RETURNS**

        A list of the form [(x,y),(x,y),(x,y),(x,y)]

        **EXAMPLE**

        >>> roi = ROI(10,10,100,100,img)
        >>> print roi.toUnitPoints()
        """        

        if(self.image is None):
            return None
        srcw = float(self.image.width)
        srch = float(self.image.height)
        pts = self.toPoints()
        retVal = []
        for p in pts:
            x,y = p
            if(x != 0):
                x = x/srcw
            if(y != 0):
                y = y/srch
            retVal.append((x,y))
        return retVal
        
    def CoordTransformX(self,x,intype="ROI",output="SRC"):
        """
        **SUMMARY**
        
        Transform a single or a set of x values from one reference frame to another.

        Options are:
        
        SRC - the coordinates of the source image.
        ROI - the coordinates of the ROI
        ROI_UNIT - unit coordinates in the frame of reference of the ROI
        SRC_UNIT - unit coordinates in the frame of reference of source image.

        **PARAMETERS**

        * *x* - A list of x values or a single x value.
        * *intype* - A string indicating the input format of the data.
        * *output* - A string indicating the output format of the data.

        **RETURNS**

        A list of the transformed values.


        **EXAMPLE**

        >>> img = Image('lenna')
        >>> blobs = img.findBlobs()
        >>> roi = ROI(blobs[0])
        >>> x = roi.crop()..... /find some x values in the crop region
        >>> xt = roi.CoordTransformX(x)
        >>> #xt are no in the space of the original image.
        """
        if( self.image is None ):
            logger.warning("No image to perform that calculation")
            return None
        if( isinstance(x,(float,int))):
            x = [x]            
        intype = intype.upper()
        output = output.upper()
        if( intype == output ):
            return x
        return self._transform(x,self.image.width,self.w,self.xtl,intype,output)

    def CoordTransformY(self,y,intype="ROI",output="SRC"):
        """
        **SUMMARY**
        
        Transform a single or a set of y values from one reference frame to another.

        Options are:
        
        SRC - the coordinates of the source image.
        ROI - the coordinates of the ROI
        ROI_UNIT - unit coordinates in the frame of reference of the ROI
        SRC_UNIT - unit coordinates in the frame of reference of source image.

        **PARAMETERS**

        * *y* - A list of y values or a single y value.
        * *intype* - A string indicating the input format of the data.
        * *output* - A string indicating the output format of the data.

        **RETURNS**

        A list of the transformed values.


        **EXAMPLE**

        >>> img = Image('lenna')
        >>> blobs = img.findBlobs()
        >>> roi = ROI(blobs[0])
        >>> y = roi.crop()..... /find some y values in the crop region
        >>> yt = roi.CoordTransformY(y)
        >>> #yt are no in the space of the original image.
        """

        if( self.image is None ):
            logger.warning("No image to perform that calculation")
            return None
        if( isinstance(y,(float,int))):
            y = [y]            
        intype = intype.upper()
        output = output.upper()
        if( intype == output ):
            return y
        return self._transform(y,self.image.height,self.h,self.ytl,intype,output)

            
    def CoordTransformPts(self,pts,intype="ROI",output="SRC"):
        """
        **SUMMARY**
        
        Transform a set of (x,y) values from one reference frame to another.

        Options are:
        
        SRC - the coordinates of the source image.
        ROI - the coordinates of the ROI
        ROI_UNIT - unit coordinates in the frame of reference of the ROI
        SRC_UNIT - unit coordinates in the frame of reference of source image.

        **PARAMETERS**

        * *pts* - A list of (x,y) values or a single (x,y) value.
        * *intype* - A string indicating the input format of the data.
        * *output* - A string indicating the output format of the data.

        **RETURNS**

        A list of the transformed values.


        **EXAMPLE**

        >>> img = Image('lenna')
        >>> blobs = img.findBlobs()
        >>> roi = ROI(blobs[0])
        >>> pts = roi.crop()..... /find some x,y values in the crop region
        >>> pts = roi.CoordTransformPts(pts)
        >>> #yt are no in the space of the original image.
        """
        if( self.image is None ):
            logger.warning("No image to perform that calculation")
            return None
        if( isinstance(pts,tuple) and len(pts)==2):
            pts = [pts]            
        intype = intype.upper()
        output = output.upper()
        x = [pt[0] for pt in pts]
        y = [pt[1] for pt in pts]
        
        if( intype == output ):
            return pts
            
        x = self._transform(x,self.image.width,self.w,self.xtl,intype,output)
        y = self._transform(y,self.image.height,self.h,self.ytl,intype,output)
        return zip(x,y) 
       

    def _transform(self,x,imgsz,roisz,offset,intype,output):
        xtemp = []
        # we are going to go to src unit coordinates
        # and then we'll go back.
        if( intype == "SRC" ):
            xtemp = [xt/float(imgsz) for xt in x]
        elif( intype == "ROI" ):
            xtemp = [(xt+offset)/float(imgsz) for xt in x]
        elif( intype == "ROI_UNIT"):
            xtemp = [((xt*roisz)+offset)/float(imgsz) for xt in x]
        elif( intype == "SRC_UNIT"):
            xtemp = x
        else:
            logger.warning("Bad Parameter to CoordTransformX")
            return None

        retVal = []
        if( output == "SRC" ):
            retVal = [int(xt*imgsz) for xt in xtemp]
        elif( output == "ROI" ):
            retVal = [int((xt*imgsz)-offset) for xt in xtemp]
        elif( output == "ROI_UNIT"):
            retVal = [int(((xt*imgsz)-offset)/float(roisz)) for xt in xtemp]
        elif( output == "SRC_UNIT"):
            retVal = xtemp
        else:
            logger.warning("Bad Parameter to CoordTransformX")
            return None
        
        return retVal

        

    def splitX(self,x,unitVals=False,srcVals=False):
        """
        **SUMMARY**
        Split the ROI at an x value.

        x can be a list of sequentianl tuples of x split points  e.g [0.3,0.6]
        where we assume the top and bottom are also on the list.
        Use units to split as a percentage (e.g. 30% down).
        The srcVals means use coordinates of the original image.


        **PARAMETERS**

        * *x*-The split point. Can be a single point or a list of points. the type is determined by the flags.
        * *unitVals* - Use unit vals for the split point. E.g. 0.5 means split at 50% of the ROI.
        * *srcVals* - Use x values relative to the source image rather than relative to the ROI.
        
        
        **RETURNS**
        
        Returns a feature set of ROIs split from the source ROI. 

        **EXAMPLE**

        >>> roi = ROI(0,0,100,100,img)
        >>> splits = roi.splitX(50) # create two ROIs
        
        """
        retVal = FeatureSet()
        if(unitVals and srcVals):
            logger.warning("Not sure how you would like to split the feature")
            return None
            
        if(not isinstance(x,(list,tuple))):
            x = [x]

        if unitVals:
            x = self.CoordTransformX(x,intype="ROI_UNIT",output="SRC")
        elif not srcVals:
            x = self.CoordTransformX(x,intype="ROI",output="SRC")

        for xt in x:
            if( xt < self.xtl or xt > self.xtl+self.w ):
                logger.warning("Invalid split point.")
                return None
                
        x.insert(0,self.xtl)
        x.append(self.xtl+self.w)
        for i in xrange(0,len(x)-1):
            xstart = x[i]
            xstop = x[i+1]
            w = xstop-xstart
            retVal.append(ROI(xstart,self.ytl,w,self.h,self.image ))
        return retVal

    def splitY(self,y,unitVals=False,srcVals=False):
        """
        **SUMMARY**
        Split the ROI at an x value.

        y can be a list of sequentianl tuples of y split points  e.g [0.3,0.6]
        where we assume the top and bottom are also on the list.
        Use units to split as a percentage (e.g. 30% down).
        The srcVals means use coordinates of the original image.


        **PARAMETERS**

        * *y*-The split point. Can be a single point or a list of points. the type is determined by the flags.
        * *unitVals* - Use unit vals for the split point. E.g. 0.5 means split at 50% of the ROI.
        * *srcVals* - Use x values relative to the source image rather than relative to the ROI.
        
        **RETURNS**
        
        Returns a feature set of ROIs split from the source ROI. 

        **EXAMPLE**

        >>> roi = ROI(0,0,100,100,img)
        >>> splits = roi.splitY(50) # create two ROIs
        
        """
        retVal = FeatureSet()
        if(unitVals and srcVals):
            logger.warning("Not sure how you would like to split the feature")
            return None
            
        if(not isinstance(y,(list,tuple))):
            y = [y]

        if unitVals:
            y = self.CoordTransformY(y,intype="ROI_UNIT",output="SRC")
        elif not srcVals:
            y = self.CoordTransformY(y,intype="ROI",output="SRC")

        for yt in y:
            if( yt < self.ytl or yt > self.ytl+self.h ):
                logger.warning("Invalid split point.")
                return None
                
        y.insert(0,self.ytl)
        y.append(self.ytl+self.h)
        for i in xrange(0,len(y)-1):
            ystart = y[i]
            ystop = y[i+1]
            h = ystop-ystart
            retVal.append(ROI(self.xtl,ystart,self.w,h,self.image ))
        return retVal        


    def merge(self, regions):
        """
        **SUMMARY**
        
        Combine another region, or regions with this ROI. Everything must be
        in the source image coordinates. Regions can be a ROIs, [ROI], features,
        FeatureSets, or anything that can be cajoled into a region.

        **PARAMETERS**

        * *regions* - A region or list of regions. Regions are just about anything that has position.
        

        **RETURNS**

        Nothing, but modifies this region.

        **EXAMPLE**

        >>>  blobs = img.findBlobs()
        >>>  roi = ROI(blob[0])
        >>>  print roi.toXYWH()
        >>>  roi.merge(blob[2])
        >>>  print roi.toXYWH()
        
        """
        result = self._standardize(regions)
        if( result is not None ):
            xo,yo,wo,ho = result
            x = np.min([xo,self.xtl])
            y = np.min([yo,self.ytl])
            w = np.max([self.xtl+self.w,xo+wo])-x
            h = np.max([self.ytl+self.h,yo+ho])-y
            if( self.image is not None ):
                x = np.clip(x,0,self.image.width)
                y = np.clip(y,0,self.image.height)
                w = np.clip(w,0,self.image.width-x)
                h = np.clip(h,0,self.image.height-y)
            self._rebase([x,y,w,h])
            if( isinstance(regions,ROI) ):
                self.subFeatures += regions
            elif( isinstance(regions,Feature) ):
                self.subFeatures.append(regions)
            elif( isinstance(regions,(list,tuple)) ):
                if(isinstance(regions[0],ROI)):
                    for r in regions:
                        self.subFeatures += r.subFeatures
                elif(isinstance(regions[0],Feature)):
                    for r in regions:
                        self.subFeatures.append(r)
                
    def rebase(self, x,y=None,w=None,h=None):
        """

        Completely alter roi using whatever source coordinates you wish.
        
        """
        if(isinstance(x,Feature)):
            self.subFeatures.append(x)
        elif(isinstance(x,(list,tuple)) and len[x] > 0 and isinstance(x,Feature)):
            self.subFeatures += list(x)
        result = self._standardize(x,y,w,h)
        if result is None:
            logger.warning("Could not create an ROI from your data.")
            return
        self._rebase(result)


    def draw(self, color = Color.GREEN,width=3):
        """
        **SUMMARY**

        This method will draw the feature on the source image.

        **PARAMETERS**

        * *color* - The color as an RGB tuple to render the image.

        **RETURNS**

        Nothing.

        **EXAMPLE**

        >>> img = Image("RedDog2.jpg")
        >>> blobs = img.findBlobs()
        >>> blobs[-1].draw()
        >>> img.show()

        """
        x,y,w,h = self.toXYWH()
        self.image.drawRectangle(x,y,w,h,width=width,color=color)

    def show(self, color = Color.GREEN, width=2):
        """
        **SUMMARY**

        This function will automatically draw the features on the image and show it.

        **RETURNS**

        Nothing.

        **EXAMPLE**

        >>> img = Image("logo")
        >>> feat = img.findBlobs()
        >>> feat[-1].show() #window pops up.

        """
        self.draw(color,width)
        self.image.show()
        
    def meanColor(self):
        """
        **SUMMARY**

        Return the average color within the feature as a tuple.

        **RETURNS**

        An RGB color tuple.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> for b in blobs:
        >>>    if (b.meanColor() == color.WHITE):
        >>>       print "Found a white thing"

        """
        x,y,w,h = self.toXYWH()
        return self.image.crop(x,y,w,h).meanColor()
    
    def _rebase(self,roi):
        x,y,w,h = roi
        self._mMaxX = None
        self._mMaxY =  None 
        self._mMinX = None
        self._mMinY = None 
        self._mWidth = None
        self._mHeight = None 
        self.mExtents = None
        self.mBoundingBox = None
        self.xtl = x
        self.ytl = y
        self.w = w
        self.h = h
        self.points = [(x,y),(x+w,y),(x,y+h),(x+w,y+h)]
        #WE MAY WANT TO DO A SANITY CHECK HERE
        self._updateExtents()

    def _standardize(self,x,y=None,w=None,h=None):
        if(isinstance(x,np.ndarray)):
            x = x.tolist()
        if(isinstance(y,np.ndarray)):
            y = y.tolist()

        # make the common case fast
        if( isinstance(x,(int,float)) and isinstance(y,(int,float)) and
            isinstance(w,(int,float)) and isinstance(h,(int,float)) ):
            if( self.image is not None ):
                x = np.clip(x,0,self.image.width)
                y = np.clip(y,0,self.image.height)
                w = np.clip(w,0,self.image.width-x)
                h = np.clip(h,0,self.image.height-y)

                return [x,y,w,h]
        elif(isinstance(x,ROI)):
            x,y,w,h = x.toXYWH()
        #If it's a feature extract what we need    
        elif(isinstance(x,FeatureSet) and len(x) > 0 ):
            #double check that everything in the list is a feature
            features = [feat for feat in x if isinstance(feat,Feature)]
            xmax = np.max([feat.maxX() for feat in features])
            xmin = np.min([feat.minX() for feat in features])
            ymax = np.max([feat.maxY() for feat in features])
            ymin = np.min([feat.minY() for feat in features])
            x = xmin
            y = ymin
            w = xmax-xmin
            h = ymax-ymin
            
        elif(isinstance(x, Feature)):
            theFeature = x
            x = theFeature.points[0][0]
            y = theFeature.points[0][1]
            w = theFeature.width()
            h = theFeature.height()

        # [x,y,w,h] (x,y,w,h)
        elif(isinstance(x, (tuple,list)) and len(x) == 4 and isinstance(x[0],(int, long, float))
             and y == None and w == None and h == None):
            x,y,w,h = x
        # x of the form [(x,y),(x1,y1),(x2,y2),(x3,y3)]
        # x of the form [[x,y],[x1,y1],[x2,y2],[x3,y3]]
        # x of the form ([x,y],[x1,y1],[x2,y2],[x3,y3])
        # x of the form ((x,y),(x1,y1),(x2,y2),(x3,y3))
        elif( isinstance(x, (list,tuple)) and
              isinstance(x[0],(list,tuple)) and
              (len(x) == 4 and len(x[0]) == 2 ) and
              y == None and w == None and h == None):
            if (len(x[0])==2 and len(x[1])==2 and len(x[2])==2 and len(x[3])==2):
                xmax = np.max([x[0][0],x[1][0],x[2][0],x[3][0]])
                ymax = np.max([x[0][1],x[1][1],x[2][1],x[3][1]])
                xmin = np.min([x[0][0],x[1][0],x[2][0],x[3][0]])
                ymin = np.min([x[0][1],x[1][1],x[2][1],x[3][1]])
                x = xmin
                y = ymin
                w = xmax-xmin
                h = ymax-ymin
            else:
                logger.warning("x should be in the form  ((x,y),(x1,y1),(x2,y2),(x3,y3))")
                return None
 
        # x,y of the form [x1,x2,x3,x4,x5....] and y similar
        elif(isinstance(x, (tuple,list)) and
             isinstance(y, (tuple,list)) and
             len(x) > 4 and len(y) > 4 ):
            if(isinstance(x[0],(int, long, float)) and isinstance(y[0],(int, long, float))):
                xmax = np.max(x)
                ymax = np.max(y)
                xmin = np.min(x)
                ymin = np.min(y)
                x = xmin
                y = ymin
                w = xmax-xmin
                h = ymax-ymin
            else:
                logger.warning("x should be in the form x = [1,2,3,4,5] y =[0,2,4,6,8]")
                return None

        # x of the form [(x,y),(x,y),(x,y),(x,y),(x,y),(x,y)]
        elif(isinstance(x, (list,tuple)) and
             len(x) > 4 and len(x[0]) == 2 and y == None and w == None and h == None):
            if(isinstance(x[0][0],(int, long, float))):
                xs = [pt[0] for pt in x]
                ys = [pt[1] for pt in x]
                xmax = np.max(xs)
                ymax = np.max(ys)
                xmin = np.min(xs)
                ymin = np.min(ys)
                x = xmin
                y = ymin
                w = xmax-xmin
                h = ymax-ymin
            else:
                logger.warning("x should be in the form [(x,y),(x,y),(x,y),(x,y),(x,y),(x,y)]")
                return None

        # x of the form [(x,y),(x1,y1)]
        elif(isinstance(x,(list,tuple)) and len(x) == 2 and isinstance(x[0],(list,tuple)) and isinstance(x[1],(list,tuple)) and y == None and w == None and h == None):
            if (len(x[0])==2 and len(x[1])==2):
                xt = np.min([x[0][0],x[1][0]])
                yt = np.min([x[0][0],x[1][0]])
                w = np.abs(x[0][0]-x[1][0])
                h = np.abs(x[0][1]-x[1][1])
                x = xt
                y = yt
            else:
                logger.warning("x should be in the form [(x1,y1),(x2,y2)]")
                return None

        # x and y of the form (x,y),(x1,y2)
        elif(isinstance(x, (tuple,list)) and isinstance(y,(tuple,list)) and w == None and h == None):
            if (len(x)==2 and len(y)==2):
                xt = np.min([x[0],y[0]])
                yt = np.min([x[1],y[1]])
                w = np.abs(y[0]-x[0])
                h = np.abs(y[1]-x[1])
                x = xt
                y = yt
                
            else:
                logger.warning("if x and y are tuple it should be in the form (x1,y1) and (x2,y2)")
                return None

        if(y == None or w == None or h == None):
            logger.warning('Not a valid roi')
        elif( w <= 0 or h <= 0 ):
            logger.warning("ROI can't have a negative dimension")
            return None

        if( self.image is not None ):
            x = np.clip(x,0,self.image.width)
            y = np.clip(y,0,self.image.height)
            w = np.clip(w,0,self.image.width-x)
            h = np.clip(h,0,self.image.height-y)

        return [x,y,w,h]
            
    def crop(self):
        retVal = None
        if(self.image is not None):
            retVal = self.image.crop(self.xtl,self.ytl,self.w,self.h)
        return retVal

########NEW FILE########
__FILENAME__ = EdgeHistogramFeatureExtractor
from SimpleCV.base import *
from SimpleCV.ImageClass import Image
from SimpleCV.Features.FeatureExtractorBase import *


class EdgeHistogramFeatureExtractor(FeatureExtractorBase):
    """
    Create a 1D edge length histogram and 1D edge angle histogram.

    This method takes in an image, applies an edge detector, and calculates
    the length and direction of lines in the image.

    bins = the number of bins
    """
    mNBins = 10
    def __init__(self, bins=10):

        self.mNBins = bins

    def extract(self, img):
        """
        Extract the line orientation and and length histogram.
        """
        #I am not sure this is the best normalization constant.
        retVal = []
        p = max(img.width,img.height)/2
        minLine = 0.01*p
        gap = 0.1*p
        fs = img.findLines(threshold=10,minlinelength=minLine,maxlinegap=gap)
        ls = fs.length()/p #normalize to image length
        angs = fs.angle()
        lhist = np.histogram(ls,self.mNBins,normed=True,range=(0,1))
        ahist = np.histogram(angs,self.mNBins,normed=True,range=(-180,180))
        retVal.extend(lhist[0].tolist())
        retVal.extend(ahist[0].tolist())
        return retVal



    def getFieldNames(self):
        """
        Return the names of all of the length and angle fields.
        """
        retVal = []
        for i in range(self.mNBins):
            name = "Length"+str(i)
            retVal.append(name)
        for i in range(self.mNBins):
            name = "Angle"+str(i)
            retVal.append(name)

        return retVal
        """
        This method gives the names of each field in the feature vector in the
        order in which they are returned. For example, 'xpos' or 'width'
        """

    def getNumFields(self):
        """
        This method returns the total number of fields in the feature vector.
        """
        return self.mNBins*2

########NEW FILE########
__FILENAME__ = FaceRecognizer
from SimpleCV.base import *
from SimpleCV.ImageClass import Image


class FaceRecognizer():

    def __init__(self):
        """
        Create a Face Recognizer Class using Fisher Face Recognizer. Uses
        OpenCV's FaceRecognizer class. Currently supports Fisher Faces.
        """
        self.supported = True
        self.model = None
        self.train_imgs = None
        self.train_labels = None
        self.csvfiles = []
        self.imageSize = None
        self.labels_dict = {}
        self.labels_set = []
        self.int_labels = []
        self.labels_dict_rev = {}
        # Not yet supported
        # self.eigenValues = None
        # self.eigenVectors = None
        # self.mean = None

        try:
            import cv2
            self.model = cv2.createFisherFaceRecognizer()
        except ImportError, AttributeError:
            self.supported = False
            warnings.warn("Fisher Recognizer is supported by OpenCV >= 2.4.4")

    def train(self, images=None, labels=None, csvfile=None, delimiter=";"):
        """
        **SUMMARY**

        Train the face recognizer with images and labels.

        **PARAMETERS**

        * *images*    - A list of Images or ImageSet. All the images must be of
                        same size.
        * *labels*    - A list of labels(int) corresponding to the image in
                        images.
                        There must be at least two different labels.
        * *csvfile*   - You can also provide a csv file with image filenames
                        and labels instead of providing labels and images
                        separately.
        * *delimiter* - The delimiter used in csv files.

        **RETURNS**

        Nothing. None.

        **EXAMPLES**

        >>> f = FaceRecognizer()
        >>> imgs1 = ImageSet(path/to/images_of_type1)
        >>> labels1 = [0]*len(imgs1)
        >>> imgs2 = ImageSet(path/to/images_of_type2)
        >>> labels2 = [1]*len(imgs2)
        >>> imgs3 = ImageSet(path/to/images_of_type3)
        >>> labels3 = [2]*len(imgs3)
        >>> imgs = imgs1 + imgs2 + imgs3
        >>> labels = labels1 + labels2 + labels3
        >>> f.train(imgs, labels)
        >>> img = Image("some_image_of_any_of_the_above_type")
        >>> print f.predict(img)

        Save Fisher Training Data
        >>> f.save("trainingdata.xml")

        Load Fisher Training Data and directly use without trainging
        >>> f1 = FaceRecognizer()
        >>> f1.load("trainingdata.xml")
        >>> img = Image("some_image_of_any_of_the_above_type")
        >>> print f1.predict(img)

        Use CSV files for training
        >>> f = FaceRecognizer()
        >>> f.train(csvfile="CSV_file_name", delimiter=";")
        >>> img = Image("some_image_of_any_of_the_type_in_csv_file")
        >>> print f.predict(img)
        """
        if not self.supported:
            warnings.warn("Fisher Recognizer is supported by OpenCV >= 2.4.4")
            return None

        if csvfile:
            images = []
            labels = []
            import csv
            try:
                f = open(csvfile, "rb")
            except IOError:
                warnings.warn("No such file found. Training not initiated")
                return None

            self.csvfiles.append(csvfile)
            filereader = csv.reader(f, delimiter=delimiter)
            for row in filereader:
                images.append(Image(row[0]))
                labels.append(row[1])

        if isinstance(labels, type(None)):
            warnings.warn("Labels not provided. Training not inititated.")
            return None

        self.labels_set = list(set(labels))
        i = 0
        for label in self.labels_set:
            self.labels_dict.update({label: i})
            self.labels_dict_rev.update({i: label})
            i += 1

        if len(self.labels_set) < 2:
            warnings.warn("At least two classes/labels are required"
                          "for training. Training not inititated.")
            return None

        if len(images) != len(labels):
            warnings.warn("Mismatch in number of labels and number of"
                          "training images. Training not initiated.")
            return None

        self.imageSize = images[0].size()
        w, h = self.imageSize
        images = [img if img.size() == self.imageSize else img.resize(w, h)
                  for img in images]

        self.int_labels = [self.labels_dict[key] for key in labels]
        self.train_labels = labels
        labels = np.array(self.int_labels)
        self.train_imgs = images
        cv2imgs = [img.getGrayNumpyCv2() for img in images]

        self.model.train(cv2imgs, labels)
        # Not yet supported
        # self.eigenValues = self.model.getMat("eigenValues")
        # self.eigenVectors = self.model.getMat("eigenVectors")
        # self.mean = self.model.getMat("mean")

    def predict(self, image):
        """
        **SUMMARY**

        Predict the class of the image using trained face recognizer.

        **PARAMETERS**

        * *image*    -  Image.The images must be of the same size as provided
                        in training.

        **RETURNS**

        * *label* - Class of the image which it belongs to.

        **EXAMPLES**

        >>> f = FaceRecognizer()
        >>> imgs1 = ImageSet(path/to/images_of_type1)
        >>> labels1 = ["type1"]*len(imgs1)
        >>> imgs2 = ImageSet(path/to/images_of_type2)
        >>> labels2 = ["type2"]*len(imgs2)
        >>> imgs3 = ImageSet(path/to/images_of_type3)
        >>> labels3 = ["type3"]*len(imgs3)
        >>> imgs = imgs1 + imgs2 + imgs3
        >>> labels = labels1 + labels2 + labels3
        >>> f.train(imgs, labels)
        >>> img = Image("some_image_of_any_of_the_above_type")
        >>> print f.predict(img)

        Save Fisher Training Data
        >>> f.save("trainingdata.xml")

        Load Fisher Training Data and directly use without trainging
        >>> f1 = FaceRecognizer()
        >>> f1.load("trainingdata.xml")
        >>> img = Image("some_image_of_any_of_the_above_type")
        >>> print f1.predict(img)

        Use CSV files for training
        >>> f = FaceRecognizer()
        >>> f.train(csvfile="CSV_file_name", delimiter=";")
        >>> img = Image("some_image_of_any_of_the_type_in_csv_file")
        >>> print f.predict(img)
        """
        if not self.supported:
            warnings.warn("Fisher Recognizer is supported by OpenCV >= 2.4.4")
            return None

        if image.size() != self.imageSize:
            w, h = self.imageSize
            image = image.resize(w, h)

        cv2img = image.getGrayNumpyCv2()
        label, confidence = self.model.predict(cv2img)
        retLabel = self.labels_dict_rev.get(label)
        if not retLabel:
            retLabel = label
        return retLabel,confidence

    # def update():
    #     OpenCV 2.4.4 doens't support update yet. It asks to train.
    #     But it's not updating it.
    #     Once OpenCV starts supporting update, this function should be added
    #     it can be found at https://gist.github.com/jayrambhia/5400347

    def save(self, filename):
        """
        **SUMMARY**

        Save the trainging data.

        **PARAMETERS**

        * *filename* - File where you want to save the data.

        **RETURNS**

        Nothing. None.

        **EXAMPLES**

        >>> f = FaceRecognizer()
        >>> imgs1 = ImageSet(path/to/images_of_type1)
        >>> labels1 = [0]*len(imgs1)
        >>> imgs2 = ImageSet(path/to/images_of_type2)
        >>> labels2 = [1]*len(imgs2)
        >>> imgs3 = ImageSet(path/to/images_of_type3)
        >>> labels3 = [2]*len(imgs3)
        >>> imgs = imgs1 + imgs2 + imgs3
        >>> labels = labels1 + labels2 + labels3
        >>> f.train(imgs, labels)
        >>> img = Image("some_image_of_any_of_the_above_type")
        >>> print f.predict(img)

        #Save New Fisher Training Data
        >>> f.save("new_trainingdata.xml")
        """
        if not self.supported:
            warnings.warn("Fisher Recognizer is supported by OpenCV >= 2.4.4")
            return None

        self.model.save(filename)

    def load(self, filename):
        """
        **SUMMARY**

        Load the trainging data.

        **PARAMETERS**

        * *filename* - File where you want to load the data from.

        **RETURNS**

        Nothing. None.

        **EXAMPLES**

        >>> f = FaceRecognizer()
        >>> f.load("trainingdata.xml")
        >>> img = Image("some_image_of_any_of_the_type_present_in_data")
        >>> print f.predict(img)
        """
        if not self.supported:
            warnings.warn("Fisher Recognizer is supported by OpenCV >= 2.4.4")
            return None

        self.model.load(filename)
        loadfile = open(filename, "r")
        for line in loadfile.readlines():
            if "cols" in line:
                match = re.search("(?<=\>)\w+", line)
                tsize = int(match.group(0))
                break
        loadfile.close()
        w = int(tsize ** 0.5)
        h = tsize / w
        while(w * h != tsize):
            w += 1
            h = tsize / w
        self.imageSize = (w, h)

########NEW FILE########
__FILENAME__ = FeatureExtractorBase
from SimpleCV.base import *
from SimpleCV.Features.Features import Feature, FeatureSet
from SimpleCV.Color import Color
from SimpleCV.ImageClass import Image

class FeatureExtractorBase(object):
    """
    The featureExtractorBase class is a way of abstracting the process of collecting
    descriptive features within an image. A feature is some description of the image
    like the mean color, or the width of a center image, or a histogram of edge
    lengths. This feature vectors can then be composed together and used within
    a machine learning algorithm to descriminate between different classes of objects.
    """

    __metaclass__ = abc.ABCMeta
    def load(cls, fname):
        """
        load segmentation settings to file.
        """
        return pickle.load(file(fname))
    load = classmethod(load)


    def save(self, fname):
        """
        Save segmentation settings to file.
        """
        output = open(fname, 'wb')
        pickle.dump(self,output,2) # use two otherwise it borks the system
        output.close()

    @abc.abstractmethod
    def extract(self, img):
        """
        Given an image extract the feature vector. The output should be a list
        object of all of the features. These features can be of any interal type
        (string, float, integer) but must contain no sub lists.
        """

    @abc.abstractmethod
    def getFieldNames(self):
        """
        This method gives the names of each field in the feature vector in the
        order in which they are returned. For example, 'xpos' or 'width'
        """

    @abc.abstractmethod
    def getNumFields(self):
        """
        This method returns the total number of fields in the feature vector.
        """

########NEW FILE########
__FILENAME__ = Features
# SimpleCV Feature library
#
# Tools return basic features in feature sets
# #    x = 0.00
#     y = 0.00
#     _mMaxX = None
#     _mMaxY = None
#     _mMinX = None
#     _mMinY = None
#     _mWidth = None
#     _mHeight = None
#     _mSrcImgW = None
#     mSrcImgH = None


#load system libraries
from SimpleCV.base import *
from SimpleCV.Color import *
import copy


class FeatureSet(list):
    """
    **SUMMARY**

    FeatureSet is a class extended from Python's list which has special functions so that it is useful for handling feature metadata on an image.

    In general, functions dealing with attributes will return numpy arrays, and functions dealing with sorting or filtering will return new FeatureSets.

    **EXAMPLE**

    >>> image = Image("/path/to/image.png")
    >>> lines = image.findLines()  #lines are the feature set
    >>> lines.draw()
    >>> lines.x()
    >>> lines.crop()
    """
    def __getitem__(self,key):
        """
        **SUMMARY**

        Returns a FeatureSet when sliced. Previously used to
        return list. Now it is possible to use FeatureSet member
        functions on sub-lists

        """
        if type(key) is types.SliceType: #Or can use 'try:' for speed
            return FeatureSet(list.__getitem__(self, key))
        else:
            return list.__getitem__(self,key)

    def __getslice__(self, i, j):
        """
        Deprecated since python 2.0, now using __getitem__
        """
        return self.__getitem__(slice(i,j))

    def count(self):
        '''
        This function returns the length / count of the all the items in the FeatureSet
        '''

        return len(self)

    def draw(self, color = Color.GREEN,width=1, autocolor = False, alpha=-1):
        """
        **SUMMARY**

        Call the draw() method on each feature in the FeatureSet.

        **PARAMETERS**
        
        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *width* - The width to draw the feature in pixels. A value of -1 usually indicates a filled region.
        * *autocolor* - If true a color is randomly selected for each feature.
        

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> feats = img.findBlobs()
        >>> feats.draw(color=Color.PUCE, width=3)
        >>> img.show()

        """
        for f in self:
            if(autocolor):
                color = Color().getRandom()
            if alpha != -1:
                f.draw(color=color,width=width,alpha=alpha)
            else:
                f.draw(color=color,width=width)

    def show(self, color = Color.GREEN, autocolor = False,width=1):
        """
        **EXAMPLE**

        This function will automatically draw the features on the image and show it.
        It is a basically a shortcut function for development and is the same as:

        **PARAMETERS**

        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *width* - The width to draw the feature in pixels. A value of -1 usually indicates a filled region.
        * *autocolor* - If true a color is randomly selected for each feature.

        **RETURNS**

        Nada. Nothing. Zilch.


        **EXAMPLE**
        >>> img = Image("logo")
        >>> feat = img.findBlobs()
        >>> if feat: feat.draw()
        >>> img.show()

        """
        self.draw(color, width, autocolor)
        self[-1].image.show()


    def reassignImage(self, newImg):
        """
        **SUMMARY**

        Return a new featureset where the features are assigned to a new image.

        **PARAMETERS**

        * *img* - the new image to which to assign the feature.

        .. Warning::
          THIS DOES NOT PERFORM A SIZE CHECK. IF YOUR NEW IMAGE IS NOT THE EXACT SAME SIZE YOU WILL CERTAINLY CAUSE ERRORS.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> img2 = img.invert()
        >>> l = img.findLines()
        >>> l2 = img.reassignImage(img2)
        >>> l2.show()

        """
        retVal = FeatureSet()
        for i in self:
            retVal.append(i.reassign(newImg))
        return retVal

    def x(self):
        """
        **SUMMARY**

        Returns a numpy array of the x (horizontal) coordinate of each feature.

        **RETURNS**

        A numpy array.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> feats = img.findBlobs()
        >>> xs = feats.x()
        >>> print xs

        """
        return np.array([f.x for f in self])

    def y(self):
        """
        **SUMMARY**

        Returns a numpy array of the y (vertical) coordinate of each feature.

        **RETURNS**

        A numpy array.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> feats = img.findBlobs()
        >>> xs = feats.y()
        >>> print xs

        """
        return np.array([f.y for f in self])

    def coordinates(self):
        """
        **SUMMARY**

        Returns a 2d numpy array of the x,y coordinates of each feature.  This
        is particularly useful if you want to use Scipy's Spatial Distance module

        **RETURNS**

        A numpy array of all the positions in the featureset.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> feats = img.findBlobs()
        >>> xs = feats.coordinates()
        >>> print xs


        """
        return np.array([[f.x, f.y] for f in self])

    def center(self):
        return self.coordinates()

    def area(self):
        """
        **SUMMARY**

        Returns a numpy array of the area of each feature in pixels.

        **RETURNS**

        A numpy array of all the positions in the featureset.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> feats = img.findBlobs()
        >>> xs = feats.area()
        >>> print xs

        """
        return np.array([f.area() for f in self])

    def sortArea(self):
        """
        **SUMMARY**

        Returns a new FeatureSet, with the largest area features first.

        **RETURNS**

        A featureset sorted based on area.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> feats = img.findBlobs()
        >>> feats = feats.sortArea()
        >>> print feats[-1] # biggest blob
        >>> print feats[0] # smallest blob

        """
        return FeatureSet(sorted(self, key = lambda f: f.area()))

    def sortX(self):
        """
        **SUMMARY**

        Returns a new FeatureSet, with the smallest x coordinates features first.

        **RETURNS**

        A featureset sorted based on area.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> feats = img.findBlobs()
        >>> feats = feats.sortX()
        >>> print feats[-1] # biggest blob
        >>> print feats[0] # smallest blob

        """
        return FeatureSet(sorted(self, key = lambda f: f.x))

    def sortY(self):
        """
        **SUMMARY**

        Returns a new FeatureSet, with the smallest y coordinates features first.

        **RETURNS**

        A featureset sorted based on area.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> feats = img.findBlobs()
        >>> feats = feats.sortY()
        >>> print feats[-1] # biggest blob
        >>> print feats[0] # smallest blob

        """
        return FeatureSet(sorted(self, key = lambda f: f.y)) 

    def distanceFrom(self, point = (-1, -1)):
        """
        **SUMMARY**

        Returns a numpy array of the distance each Feature is from a given coordinate.
        Default is the center of the image.

        **PARAMETERS**

        * *point* - A point on the image from which we will calculate distance.

        **RETURNS**

        A numpy array of distance values.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> feats = img.findBlobs()
        >>> d = feats.distanceFrom()
        >>> d[0]  #show the 0th blobs distance to the center.

        **TO DO**

        Make this accept other features to measure from.

        """
        if (point[0] == -1 or point[1] == -1 and len(self)):
            point = self[0].image.size()

        return spsd.cdist(self.coordinates(), [point])[:,0]

    def sortDistance(self, point = (-1, -1)):
        """
        **SUMMARY**

        Returns a sorted FeatureSet with the features closest to a given coordinate first.
        Default is from the center of the image.

        **PARAMETERS**

        * *point* - A point on the image from which we will calculate distance.

        **RETURNS**

        A numpy array of distance values.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> feats = img.findBlobs()
        >>> d = feats.sortDistance()
        >>> d[-1].show()  #show the 0th blobs distance to the center.


        """
        return FeatureSet(sorted(self, key = lambda f: f.distanceFrom(point)))

    def distancePairs(self):
        """
        **SUMMARY**

        Returns the square-form of pairwise distances for the featureset.
        The resulting N x N array can be used to quickly look up distances
        between features.

        **RETURNS**

        A NxN np matrix of distance values.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> feats = img.findBlobs()
        >>> d = feats.distancePairs()
        >>> print d

        """
        return spsd.squareform(spsd.pdist(self.coordinates()))

    def angle(self):
        """
        **SUMMARY**

        Return a numpy array of the angles (theta) of each feature.
        Note that theta is given in degrees, with 0 being horizontal.

        **RETURNS**

        An array of angle values corresponding to the features.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> l = img.findLines()
        >>> angs = l.angle()
        >>> print angs


        """
        return np.array([f.angle() for f in self])

    def sortAngle(self, theta = 0):
        """
        Return a sorted FeatureSet with the features closest to a given angle first.
        Note that theta is given in radians, with 0 being horizontal.

        **RETURNS**

        An array of angle values corresponding to the features.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> l = img.findLines()
        >>> l = l.sortAngle()
        >>> print angs

        """
        return FeatureSet(sorted(self, key = lambda f: abs(f.angle() - theta)))

    def length(self):
        """
        **SUMMARY**

        Return a numpy array of the length (longest dimension) of each feature.

        **RETURNS**

        A numpy array of the length, in pixels, of eatch feature object.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> l = img.findLines()
        >>> lengt = l.length()
        >>> lengt[0] # length of the 0th element.

        """

        return np.array([f.length() for f in self])

    def sortLength(self):
        """
        **SUMMARY**

        Return a sorted FeatureSet with the longest features first.

        **RETURNS**

        A sorted FeatureSet.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> l = img.findLines().sortLength()
        >>> lengt[-1] # length of the 0th element.

        """
        return FeatureSet(sorted(self, key = lambda f: f.length()))

    def meanColor(self):
        """
        **SUMMARY**

        Return a numpy array of the average color of the area covered by each Feature.

        **RETURNS**

        Returns an array of RGB triplets the correspond to the mean color of the feature.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> kp = img.findKeypoints()
        >>> c = kp.meanColor()


        """
        return np.array([f.meanColor() for f in self])

    def colorDistance(self, color = (0, 0, 0)):
        """
        **SUMMARY**

        Return a numpy array of the distance each features average color is from
        a given color tuple (default black, so colorDistance() returns intensity)

        **PARAMETERS**

        * *color* - The color to calculate the distance from.

        **RETURNS**

        The distance of the average color for the feature from given color as a numpy array.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> circs = img.findCircle()
        >>> d = circs.colorDistance(color=Color.BLUE)
        >>> print d

        """
        return spsd.cdist(self.meanColor(), [color])[:,0]

    def sortColorDistance(self, color = (0, 0, 0)):
        """
        Return a sorted FeatureSet with features closest to a given color first.
        Default is black, so sortColorDistance() will return darkest to brightest
        """
        return FeatureSet(sorted(self, key = lambda f: f.colorDistance(color)))

    def filter(self, filterarray):
        """
        **SUMMARY**

        Return a FeatureSet which is filtered on a numpy boolean array.  This
        will let you use the attribute functions to easily screen Features out
        of return FeatureSets.

        **PARAMETERS**

        * *filterarray* - A numpy array, matching  the size of the feature set,
          made of Boolean values, we return the true values and reject the False value.

        **RETURNS**

        The revised feature set.

        **EXAMPLE**

        Return all lines < 200px

        >>> my_lines.filter(my_lines.length() < 200) # returns all lines < 200px
        >>> my_blobs.filter(my_blobs.area() > 0.9 * my_blobs.length**2) # returns blobs that are nearly square
        >>> my_lines.filter(abs(my_lines.angle()) < numpy.pi / 4) #any lines within 45 degrees of horizontal
        >>> my_corners.filter(my_corners.x() - my_corners.y() > 0) #only return corners in the upper diagonal of the image

        """
        return FeatureSet(list(np.array(self)[np.array(filterarray)]))

    def width(self):
        """
        **SUMMARY**

        Returns a nparray which is the width of all the objects in the FeatureSet.

        **RETURNS**

        A numpy array of width values.


        **EXAMPLE**

        >>> img = Image("NotLenna")
        >>> l = img.findLines()
        >>> l.width()

        """
        return np.array([f.width() for f in self])

    def height(self):
        """
        Returns a nparray which is the height of all the objects in the FeatureSet

        **RETURNS**

        A numpy array of width values.


        **EXAMPLE**

        >>> img = Image("NotLenna")
        >>> l = img.findLines()
        >>> l.height()

        """
        return np.array([f.height() for f in self])

    def crop(self):
        """
        **SUMMARY**

        Returns a nparray with the cropped features as SimpleCV image.

        **RETURNS**

        A SimpleCV image cropped to each image.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> blobs = img.findBlobs(128)
        >>> for b in blobs:
        >>>   newImg = b.crop()
        >>>   newImg.show()
        >>>   time.sleep(1)

        """
        return np.array([f.crop() for f in self])

    def inside(self,region):
        """
        **SUMMARY**

        Return only the features inside the region. where region can be a bounding box,
        bounding circle, a list of tuples in a closed polygon, or any other featutres.

        **PARAMETERS**

        * *region*

          * A bounding box - of the form (x,y,w,h) where x,y is the upper left corner
          * A bounding circle of the form (x,y,r)
          * A list of x,y tuples defining a closed polygon e.g. ((x,y),(x,y),....)
          * Any two dimensional feature (e.g. blobs, circle ...)

        **RETURNS**

        Returns a featureset of features that are inside the region.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> blobs = img.findBlobs()
        >>> b = blobs[-1]
        >>> lines = img.findLines()
        >>> inside = lines.inside(b)

        **NOTE**

        This currently performs a bounding box test, not a full polygon test for speed.


        """
        fs = FeatureSet()
        for f in self:
            if(f.isContainedWithin(region)):
                fs.append(f)
        return fs


    def outside(self,region):
        """
        **SUMMARY**

        Return only the features outside the region. where region can be a bounding box,
        bounding circle, a list of tuples in a closed polygon, or any other featutres.

        **PARAMETERS**

        * *region*

          * A bounding box - of the form (x,y,w,h) where x,y is the upper left corner
          * A bounding circle of the form (x,y,r)
          * A list of x,y tuples defining a closed polygon e.g. ((x,y),(x,y),....)
          * Any two dimensional feature (e.g. blobs, circle ...)

        **RETURNS**

        Returns a featureset of features that are outside the region.


        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> blobs = img.findBlobs()
        >>> b = blobs[-1]
        >>> lines = img.findLines()
        >>> outside = lines.outside(b)

        **NOTE**

        This currently performs a bounding box test, not a full polygon test for speed.

        """
        fs = FeatureSet()
        for f in self:
            if(f.isNotContainedWithin(region)):
                fs.append(f)
        return fs

    def overlaps(self,region):
        """
        **SUMMARY**

        Return only the features that overlap or the region. Where region can be a bounding box,
        bounding circle, a list of tuples in a closed polygon, or any other featutres.

        **PARAMETERS**

        * *region*

          * A bounding box - of the form (x,y,w,h) where x,y is the upper left corner
          * A bounding circle of the form (x,y,r)
          * A list of x,y tuples defining a closed polygon e.g. ((x,y),(x,y),....)
          * Any two dimensional feature (e.g. blobs, circle ...)

        **RETURNS**

        Returns a featureset of features that overlap the region.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> blobs = img.findBlobs()
        >>> b = blobs[-1]
        >>> lines = img.findLines()
        >>> outside = lines.overlaps(b)

        **NOTE**

        This currently performs a bounding box test, not a full polygon test for speed.

        """
        fs = FeatureSet()
        for f in self:
            if( f.overlaps(region) ):
                fs.append(f)
        return fs

    def above(self,region):
        """
        **SUMMARY**

        Return only the features that are above a  region. Where region can be a bounding box,
        bounding circle, a list of tuples in a closed polygon, or any other featutres.

        **PARAMETERS**

        * *region*

          * A bounding box - of the form (x,y,w,h) where x,y is the upper left corner
          * A bounding circle of the form (x,y,r)
          * A list of x,y tuples defining a closed polygon e.g. ((x,y),(x,y),....)
          * Any two dimensional feature (e.g. blobs, circle ...)

        **RETURNS**

        Returns a featureset of features that are above the region.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> blobs = img.findBlobs()
        >>> b = blobs[-1]
        >>> lines = img.findLines()
        >>> outside = lines.above(b)

        **NOTE**

        This currently performs a bounding box test, not a full polygon test for speed.

        """
        fs = FeatureSet()
        for f in self:
            if(f.above(region)):
                fs.append(f)
        return fs

    def below(self,region):
        """
        **SUMMARY**

        Return only the features below the region. where region can be a bounding box,
        bounding circle, a list of tuples in a closed polygon, or any other featutres.

        **PARAMETERS**

        * *region*

          * A bounding box - of the form (x,y,w,h) where x,y is the upper left corner
          * A bounding circle of the form (x,y,r)
          * A list of x,y tuples defining a closed polygon e.g. ((x,y),(x,y),....)
          * Any two dimensional feature (e.g. blobs, circle ...)

        **RETURNS**

        Returns a featureset of features that are below the region.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> blobs = img.findBlobs()
        >>> b = blobs[-1]
        >>> lines = img.findLines()
        >>> inside = lines.below(b)

        **NOTE**

        This currently performs a bounding box test, not a full polygon test for speed.

        """
        fs = FeatureSet()
        for f in self:
            if(f.below(region)):
                fs.append(f)
        return fs

    def left(self,region):
        """
        **SUMMARY**

        Return only the features left of the region. where region can be a bounding box,
        bounding circle, a list of tuples in a closed polygon, or any other featutres.

        **PARAMETERS**

        * *region*

          * A bounding box - of the form (x,y,w,h) where x,y is the upper left corner
          * A bounding circle of the form (x,y,r)
          * A list of x,y tuples defining a closed polygon e.g. ((x,y),(x,y),....)
          * Any two dimensional feature (e.g. blobs, circle ...)

        **RETURNS**

        Returns a featureset of features that are left of the region.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> blobs = img.findBlobs()
        >>> b = blobs[-1]
        >>> lines = img.findLines()
        >>> left = lines.left(b)

        **NOTE**

        This currently performs a bounding box test, not a full polygon test for speed.

        """
        fs = FeatureSet()
        for f in self:
            if(f.left(region)):
                fs.append(f)
        return fs

    def right(self,region):
        """
        **SUMMARY**

        Return only the features right of the region. where region can be a bounding box,
        bounding circle, a list of tuples in a closed polygon, or any other featutres.

        **PARAMETERS**

        * *region*

          * A bounding box - of the form (x,y,w,h) where x,y is the upper left corner
          * A bounding circle of the form (x,y,r)
          * A list of x,y tuples defining a closed polygon e.g. ((x,y),(x,y),....)
          * Any two dimensional feature (e.g. blobs, circle ...)

        **RETURNS**

        Returns a featureset of features that are right of the region.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> blobs = img.findBlobs()
        >>> b = blobs[-1]
        >>> lines = img.findLines()
        >>> right = lines.right(b)

        **NOTE**

        This currently performs a bounding box test, not a full polygon test for speed.

        """
        fs = FeatureSet()
        for f in self:
            if(f.right(region)):
                fs.append(f)
        return fs

    def onImageEdge(self, tolerance=1):
        """
        **SUMMARY**

        The method returns a feature set of features that are on or "near" the edge of
        the image. This is really helpful for removing features that are edge effects.

        **PARAMETERS**

        * *tolerance* - the distance in pixels from the edge at which a feature
          qualifies as being "on" the edge of the image.

        **RETURNS**

        Returns a featureset of features that are on the edge of the image.

        **EXAMPLE**

        >>> img = Image("./sampleimages/EdgeTest1.png")
        >>> blobs = img.findBlobs()
        >>> es = blobs.onImageEdge()
        >>> es.draw(color=Color.RED)
        >>> img.show()

        """
        fs = FeatureSet()
        for f in self:
            if(f.onImageEdge(tolerance)):
                fs.append(f)
        return fs

    def notOnImageEdge(self, tolerance=1):
        """
        **SUMMARY**

        The method returns a feature set of features that are not on or "near" the edge of
        the image. This is really helpful for removing features that are edge effects.

        **PARAMETERS**

        * *tolerance* - the distance in pixels from the edge at which a feature
          qualifies as being "on" the edge of the image.

        **RETURNS**

        Returns a featureset of features that are not on the edge of the image.

        **EXAMPLE**

        >>> img = Image("./sampleimages/EdgeTest1.png")
        >>> blobs = img.findBlobs()
        >>> es = blobs.notOnImageEdge()
        >>> es.draw(color=Color.RED)
        >>> img.show()

        """
        fs = FeatureSet()
        for f in self:
            if(f.notOnImageEdge(tolerance)):
                fs.append(f)
        return fs


    def topLeftCorners(self):
        """
        **SUMMARY**

        This method returns the top left corner of each feature's bounding box.

        **RETURNS**

        A numpy array of x,y position values.

        **EXAMPLE**

        >>> img = Image("./sampleimages/EdgeTest1.png")
        >>> blobs = img.findBlobs()
        >>> tl = img.topLeftCorners()
        >>> print tl[0]
        """
        return np.array([f.topLeftCorner() for f in self])



    def bottomLeftCorners(self):
        """
        **SUMMARY**

        This method returns the bottom left corner of each feature's bounding box.

        **RETURNS**

        A numpy array of x,y position values.

        **EXAMPLE**

        >>> img = Image("./sampleimages/EdgeTest1.png")
        >>> blobs = img.findBlobs()
        >>> bl = img.bottomLeftCorners()
        >>> print bl[0]

        """
        return np.array([f.bottomLeftCorner() for f in self])

    def topLeftCorners(self):
        """
        **SUMMARY**

        This method returns the top left corner of each feature's bounding box.

        **RETURNS**

        A numpy array of x,y position values.

        **EXAMPLE**

        >>> img = Image("./sampleimages/EdgeTest1.png")
        >>> blobs = img.findBlobs()
        >>> tl = img.bottomLeftCorners()
        >>> print tl[0]

        """
        return np.array([f.topLeftCorner() for f in self])


    def topRightCorners(self):
        """
        **SUMMARY**

        This method returns the top right corner of each feature's bounding box.

        **RETURNS**

        A numpy array of x,y position values.

        **EXAMPLE**

        >>> img = Image("./sampleimages/EdgeTest1.png")
        >>> blobs = img.findBlobs()
        >>> tr = img.topRightCorners()
        >>> print tr[0]

        """
        return np.array([f.topRightCorner() for f in self])



    def bottomRightCorners(self):
        """
        **SUMMARY**

        This method returns the bottom right corner of each feature's bounding box.

        **RETURNS**

        A numpy array of x,y position values.

        **EXAMPLE**

        >>> img = Image("./sampleimages/EdgeTest1.png")
        >>> blobs = img.findBlobs()
        >>> br = img.bottomRightCorners()
        >>> print br[0]

        """
        return np.array([f.bottomRightCorner() for f in self])

    def aspectRatios(self):
        """
        **SUMMARY**

        Return the aspect ratio of all the features in the feature set, For our purposes
        aspect ration is max(width,height)/min(width,height).

        **RETURNS**

        A numpy array of the aspect ratio of the features in the featureset.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> print blobs.aspectRatio()

        """
        return np.array([f.aspectRatio() for f in self])

    def cluster(self,method="kmeans",properties=None,k=3):
        """
        
        **SUMMARY**

        This function clusters the blobs in the featureSet based on the properties. Properties can be "color", "shape" or "position" of blobs.
        Clustering is done using K-Means or Hierarchical clustering(Ward) algorithm.

        **PARAMETERS**
        
        * *properties* - It should be a list with any combination of "color", "shape", "position". properties = ["color","position"]. properties = ["position","shape"]. properties = ["shape"]
        * *method* - if method is "kmeans", it will cluster using K-Means algorithm, if the method is "hierarchical", no need to spicify the number of clusters
        * *k* - The number of clusters(kmeans).
        

        **RETURNS**

        A list of featureset, each being a cluster itself.

        **EXAMPLE**

          >>> img = Image("lenna")
          >>> blobs = img.findBlobs()
          >>> clusters = blobs.cluster(method="kmeans",properties=["color"],k=5)
          >>> for i in clusters:
          >>>     i.draw(color=Color.getRandom(),width=5)
          >>> img.show()
        
        """
        try :
            from sklearn.cluster import KMeans, Ward
            from sklearn import __version__
        except :
            logger.warning("install scikits-learning package")
            return
        X = [] #List of feature vector of each blob
        if not properties:
            properties = ['color','shape','position']
        if k > len(self):
            logger.warning("Number of clusters cannot be greater then the number of blobs in the featureset")
            return
        for i in self:
            featureVector = []
            if 'color' in properties:
                featureVector.extend(i.mAvgColor)
            if 'shape' in properties:
                featureVector.extend(i.mHu)
            if 'position' in properties:
                featureVector.extend(i.extents())
            if not featureVector :
                logger.warning("properties parameter is not specified properly")
                return
            X.append(featureVector)

        if method == "kmeans":
            
            # Ignore minor version numbers.
            sklearn_version = re.search(r'\d+\.\d+', __version__).group()
            
            if (float(sklearn_version) > 0.11):
                k_means = KMeans(init='random', n_clusters=k, n_init=10).fit(X)
            else:
                k_means = KMeans(init='random', k=k, n_init=10).fit(X)
            KClusters = [ FeatureSet([]) for i in range(k)]
            for i in range(len(self)):
                KClusters[k_means.labels_[i]].append(self[i])
            return KClusters

        if method == "hierarchical":
            ward = Ward(n_clusters=int(sqrt(len(self)))).fit(X) #n_clusters = sqrt(n)
            WClusters = [ FeatureSet([]) for i in range(int(sqrt(len(self))))]
            for i in range(len(self)):
                WClusters[ward.labels_[i]].append(self[i])
            return WClusters

    @property
    def image(self):
        if not len(self):
            return None
        return self[0].image

    @image.setter
    def image(self, i):
        for f in self:
            f.image = i

### ----------------------------------------------------------------------------
### ----------------------------------------------------------------------------
### ----------------------------FEATURE CLASS-----------------------------------
### ----------------------------------------------------------------------------
### ----------------------------------------------------------------------------
class Feature(object):
    """
    **SUMMARY**

    The Feature object is an abstract class which real features descend from.
    Each feature object has:

    * a draw() method,
    * an image property, referencing the originating Image object
    * x and y coordinates
    * default functions for determining angle, area, meanColor, etc for FeatureSets
    * in the Feature class, these functions assume the feature is 1px

    """
    x = 0.00
    y = 0.00
    _mMaxX = None
    _mMaxY = None
    _mMinX = None
    _mMinY = None
    _mWidth = None
    _mHeight = None
    _mSrcImgW = None
    _mSrcImgH = None

    # This is 2.0 refactoring
    mBoundingBox = None # THIS SHALT BE TOP LEFT (X,Y) THEN W H i.e. [X,Y,W,H]
    mExtents = None # THIS SHALT BE [MAXX,MINX,MAXY,MINY]
    points = None  # THIS SHALT BE (x,y) tuples in the ORDER [(TopLeft),(TopRight),(BottomLeft),(BottomRight)]

    image = "" #parent image
    #points = []
    #boundingBox = []

    def __init__(self, i, at_x, at_y, points):
        #THE COVENANT IS THAT YOU PROVIDE THE POINTS IN THE SPECIFIED FORMAT AND ALL OTHER VALUES SHALT FLOW
        self.x = at_x
        self.y = at_y
        self.image = i
        self.points = points
        self._updateExtents(new_feature=True)

    def reassign(self, img):
        """
        **SUMMARY**

        Reassign the image of this feature and return an updated copy of the feature.

        **PARAMETERS**

        * *img* - the new image to which to assign the feature.

        .. Warning::
          THIS DOES NOT PERFORM A SIZE CHECK. IF YOUR NEW IMAGE IS NOT THE EXACT SAME SIZE YOU WILL CERTAINLY CAUSE ERRORS.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> img2 = img.invert()
        >>> l = img.findLines()
        >>> l2 = img.reassignImage(img2)
        >>> l2.show()
        """
        retVal = copy.deepcopy(self)
        if( self.image.width != img.width or
            self.image.height != img.height ):
            warnings.warn("DON'T REASSIGN IMAGES OF DIFFERENT SIZES")
        retVal.image = img

        return retVal

    def corners(self):
        self._updateExtents()
        return self.points

    def coordinates(self):
        """
        **SUMMARY**

        Returns the x,y position of the feature. This is usually the center coordinate.

        **RETURNS**

        Returns an (x,y) tuple of the position of the feature.

        **EXAMPLE**

        >>> img = Image("aerospace.png")
        >>> blobs = img.findBlobs()
        >>> for b in blobs:
        >>>    print b.coordinates()

        """
        return np.array([self.x, self.y])


    def draw(self, color = Color.GREEN):
        """
        **SUMMARY**

        This method will draw the feature on the source image.

        **PARAMETERS**

        * *color* - The color as an RGB tuple to render the image.

        **RETURNS**

        Nothing.

        **EXAMPLE**

        >>> img = Image("RedDog2.jpg")
        >>> blobs = img.findBlobs()
        >>> blobs[-1].draw()
        >>> img.show()

        """
        self.image[self.x, self.y] = color

    def show(self, color = Color.GREEN):
        """
        **SUMMARY**

        This function will automatically draw the features on the image and show it.

        **RETURNS**

        Nothing.

        **EXAMPLE**

        >>> img = Image("logo")
        >>> feat = img.findBlobs()
        >>> feat[-1].show() #window pops up.

        """
        self.draw(color)
        self.image.show()

    def distanceFrom(self, point = (-1, -1)):
        """
        **SUMMARY**

        Given a point (default to center of the image), return the euclidean distance of x,y from this point.

        **PARAMETERS**

        * *point* - The point, as an (x,y) tuple on the image to measure distance from.

        **RETURNS**

        The distance as a floating point value in pixels.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> blobs[-1].distanceFrom(blobs[-2].coordinates())


        """
        if (point[0] == -1 or point[1] == -1):
            point = np.array(self.image.size()) / 2
        return spsd.euclidean(point, [self.x, self.y])

    def meanColor(self):
        """
        **SUMMARY**

        Return the average color within the feature as a tuple.

        **RETURNS**

        An RGB color tuple.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> for b in blobs:
        >>>    if (b.meanColor() == color.WHITE):
        >>>       print "Found a white thing"

        """
        return self.image[self.x, self.y]

    def colorDistance(self, color = (0, 0, 0)):
        """
        **SUMMARY**

        Return the euclidean color distance of the color tuple at x,y from a given color (default black).

        **PARAMETERS**

        * *color* - An RGB triplet to calculate from which to calculate the color distance.

        **RETURNS**

        A floating point color distance value.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> for b in blobs:
        >>>    print b.colorDistance(color.WHITE):

        """
        return spsd.euclidean(np.array(color), np.array(self.meanColor()))

    def angle(self):
        """
        **SUMMARY**

        Return the angle (theta) in degrees of the feature. The default is 0 (horizontal).

        .. Warning::
          This is not a valid operation for all features.


        **RETURNS**

        An angle value in degrees.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> for b in blobs:
        >>>    if b.angle() == 0:
        >>>       print "I AM HORIZONTAL."

        **TODO**

        Double check that values are being returned consistently.
        """
        return 0

    def length(self):
        """
        **SUMMARY**

        This method returns the longest dimension of the feature (i.e max(width,height)).

        **RETURNS**

        A floating point length value.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> for b in blobs:
        >>>    if b.length() > 200:
        >>>       print "OH MY! - WHAT A BIG FEATURE YOU HAVE!"
        >>>       print "---I bet you say that to all the features."

        **TODO**

        Should this be sqrt(x*x+y*y)?
        """
        return float(np.max([self.width(),self.height()]))

    def distanceToNearestEdge(self):
        """
        **SUMMARY**

        This method returns the distance, in pixels, from the nearest image edge.

        **RETURNS**

        The integer distance to the nearest edge.

        **EXAMPLE**

        >>> img = Image("../sampleimages/EdgeTest1.png")
        >>> b = img.findBlobs()
        >>> b[0].distanceToNearestEdge()

        """
        w = self.image.width
        h = self.image.height
        return np.min([self._mMinX,self._mMinY, w-self._mMaxX,h-self._mMaxY])

    def onImageEdge(self,tolerance=1):
        """
        **SUMMARY**

        This method returns True if the feature is less than `tolerance`
        pixels away from the nearest edge.

        **PARAMETERS**

        * *tolerance* - the distance in pixels at which a feature qualifies
          as being on the image edge.

        **RETURNS**

        True if the feature is on the edge, False otherwise.

        **EXAMPLE**

        >>> img = Image("../sampleimages/EdgeTest1.png")
        >>> b = img.findBlobs()
        >>> if(b[0].onImageEdge()):
        >>>     print "HELP! I AM ABOUT TO FALL OFF THE IMAGE"

        """
        # this has to be one to deal with blob library weirdness that goes deep down to opencv
        return ( self.distanceToNearestEdge() <= tolerance )

    def notOnImageEdge(self,tolerance=1):
        """
        **SUMMARY**

        This method returns True if the feature is greate than `tolerance`
        pixels away from the nearest edge.

        **PARAMETERS**

        * *tolerance* - the distance in pixels at which a feature qualifies
          as not being on the image edge.

        **RETURNS**

        True if the feature is not on the edge of the image, False otherwise.

        **EXAMPLE**

        >>> img = Image("../sampleimages/EdgeTest1.png")
        >>> b = img.findBlobs()
        >>> if(b[0].notOnImageEdge()):
        >>>     print "I am safe and sound."

        """

        # this has to be one to deal with blob library weirdness that goes deep down to opencv
        return ( self.distanceToNearestEdge() > tolerance )


    def aspectRatio(self):
        """
        **SUMMARY**

        Return the aspect ratio of the feature, which for our purposes
        is max(width,height)/min(width,height).

        **RETURNS**

        A single floating point value of the aspect ration.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> b[0].aspectRatio()

        """
        self._updateExtents()
        return self.mAspectRatio

    def area(self):
        """
        **SUMMARY**

        Returns the area (number of pixels)  covered by the feature.

        **RETURNS**

        An integer area of the feature.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> for b in blobs:
        >>>    if b.area() > 200:
        >>>       print b.area()

        """
        return self.width() * self.height()

    def width(self):
        """
        **SUMMARY**

        Returns the height of the feature.

        **RETURNS**

        An integer value for the feature's width.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> for b in blobs:
        >>>    if b.width() > b.height():
        >>>       print "wider than tall"
        >>>       b.draw()
        >>> img.show()

        """
        self._updateExtents()
        return self._mWidth


    def height(self):
        """
        **SUMMARY**

        Returns the height of the feature.

        **RETURNS**

        An integer value of the feature's height.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> for b in blobs:
        >>>    if b.width() > b.height():
        >>>       print "wider than tall"
        >>>       b.draw()
        >>> img.show()
        """
        self._updateExtents()
        return self._mHeight

    def crop(self):
        """
        **SUMMARY**

        This function crops the source image to the location of the feature and returns
        a new SimpleCV image.

        **RETURNS**

        A SimpleCV image that is cropped to the feature position and size.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> big = blobs[-1].crop()
        >>> big.show()

        """

        return self.image.crop(self.x, self.y, self.width(), self.height(), centered = True)

    def __repr__(self):
        return "%s.%s at (%d,%d)" % (self.__class__.__module__, self.__class__.__name__, self.x, self.y)


    def _updateExtents(self, new_feature=False):
#    mBoundingBox = None # THIS SHALT BE TOP LEFT (X,Y) THEN W H i.e. [X,Y,W,H]
#    mExtents = None # THIS SHALT BE [MAXX,MINX,MAXY,MINY]
#    points = None  # THIS SHALT BE (x,y) tuples in the ORDER [(TopLeft),(TopRight),(BottomLeft),(BottomRight)]

        max_x = self._mMaxX
        min_x = self._mMinX
        max_y = self._mMaxY
        min_y = self._mMinY
        width = self._mWidth
        height = self._mHeight
        extents = self.mExtents
        bounding_box = self.mBoundingBox

        #if new_feature or None in [self._mMaxX, self._mMinX, self._mMaxY, self._mMinY,
        #            self._mWidth, self._mHeight, self.mExtents, self.mBoundingBox]:

        if new_feature or None in [max_x, min_x, max_y, min_y, width, height, extents, bounding_box]:

            max_x = max_y = float("-infinity")
            min_x = min_y = float("infinity")

            for p in self.points:
                if (p[0] > max_x):
                    max_x = p[0]
                if (p[0] < min_x):
                    min_x = p[0]
                if (p[1] > max_y):
                    max_y = p[1]
                if (p[1] < min_y):
                    min_y = p[1]

            width = max_x - min_x
            height = max_y - min_y

            if (width <= 0):
                width = 1

            if (height <= 0):
                height = 1

            self.mBoundingBox = [min_x, min_y, width, height]
            self.mExtents = [max_x, min_x, max_y, min_y]

            if width > height:
                self.mAspectRatio = float(width/height)
            else:
                self.mAspectRatio = float(height/width)

            self._mMaxX = max_x
            self._mMinX = min_x
            self._mMaxY = max_y
            self._mMinY = min_y
            self._mWidth = width
            self._mHeight = height

    def boundingBox(self):
        """
        **SUMMARY**

        This function returns a rectangle which bounds the blob.

        **RETURNS**

        A list of [x, y, w, h] where (x, y) are the top left point of the rectangle
        and w, h are its width and height respectively.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> print blobs[-1].boundingBox()

        """
        self._updateExtents()
        return self.mBoundingBox

    def extents(self):
        """
        **SUMMARY**

        This function returns the maximum and minimum x and y values for the feature and
        returns them as a tuple.

        **RETURNS**

        A tuple of the extents of the feature. The order is (MaxX,MaxY,MinX,MinY).

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> print blobs[-1].extents()

        """
        self._updateExtents()
        return self.mExtents

    def minY(self):
        """
        **SUMMARY**

        This method return the minimum y value of the bounding box of the
        the feature.

        **RETURNS**

        An integer value of the minimum y value of the feature.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> print blobs[-1].minY()

        """
        self._updateExtents()
        return self._mMinY

    def maxY(self):
        """
        **SUMMARY**

        This method return the maximum y value of the bounding box of the
        the feature.

        **RETURNS**

        An integer value of the maximum y value of the feature.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> print blobs[-1].maxY()

        """
        self._updateExtents()
        return self._mMaxY


    def minX(self):
        """
        **SUMMARY**

        This method return the minimum x value of the bounding box of the
        the feature.

        **RETURNS**

        An integer value of the minimum x value of the feature.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> print blobs[-1].minX()

        """
        self._updateExtents()
        return self._mMinX

    def maxX(self):
        """
        **SUMMARY**

        This method return the minimum x value of the bounding box of the
        the feature.

        **RETURNS**

        An integer value of the maxium x value of the feature.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> print blobs[-1].maxX()

        """
        self._updateExtents()
        return self._mMaxX

    def topLeftCorner(self):
        """
        **SUMMARY**

        This method returns the top left corner of the bounding box of
        the blob as an (x,y) tuple.

        **RESULT**

        Returns a tupple of the top left corner.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> print blobs[-1].topLeftCorner()

        """
        self._updateExtents()
        return (self._mMinX,self._mMinY)

    def bottomRightCorner(self):
        """
        **SUMMARY**

        This method returns the bottom right corner of the bounding box of
        the blob as an (x,y) tuple.

        **RESULT**

        Returns a tupple of the bottom right corner.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> print blobs[-1].bottomRightCorner()

        """
        self._updateExtents()
        return (self._mMaxX,self._mMaxY)

    def bottomLeftCorner(self):
        """
        **SUMMARY**

        This method returns the bottom left corner of the bounding box of
        the blob as an (x,y) tuple.

        **RESULT**

        Returns a tupple of the bottom left corner.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> print blobs[-1].bottomLeftCorner()

        """
        self._updateExtents()
        return (self._mMinX,self._mMaxY)

    def topRightCorner(self):
        """
        **SUMMARY**

        This method returns the top right corner of the bounding box of
        the blob as an (x,y) tuple.

        **RESULT**

        Returns a tupple of the top right  corner.

        **EXAMPLE**

        >>> img = Image("OWS.jpg")
        >>> blobs = img.findBlobs(128)
        >>> print blobs[-1].topRightCorner()

        """
        self._updateExtents()
        return (self._mMaxX,self._mMinY)


    def above(self,object):
        """
        **SUMMARY**

        Return true if the feature is above the object, where object can be a bounding box,
        bounding circle, a list of tuples in a closed polygon, or any other featutres.

        **PARAMETERS**

        * *object*

          * A bounding box - of the form (x,y,w,h) where x,y is the upper left corner
          * A bounding circle of the form (x,y,r)
          * A list of x,y tuples defining a closed polygon e.g. ((x,y),(x,y),....)
          * Any two dimensional feature (e.g. blobs, circle ...)

        **RETURNS**

        Returns a Boolean, True if the feature is above the object, False otherwise.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> blobs = img.findBlobs()
        >>> b = blobs[0]
        >>> if( blobs[-1].above(b) ):
        >>>    print "above the biggest blob"

        """
        if( isinstance(object,Feature) ):
            return( self.maxY() < object.minY() )
        elif( isinstance(object,tuple) or isinstance(object,np.ndarray) ):
            return( self.maxY() < object[1]  )
        elif( isinstance(object,float) or isinstance(object,int) ):
            return( self.maxY() < object )
        else:
            logger.warning("SimpleCV did not recognize the input type to feature.above(). This method only takes another feature, an (x,y) tuple, or a ndarray type.")
            return None

    def below(self,object):
        """
        **SUMMARY**

        Return true if the feature is below the object, where object can be a bounding box,
        bounding circle, a list of tuples in a closed polygon, or any other featutres.

        **PARAMETERS**

        * *object*

          * A bounding box - of the form (x,y,w,h) where x,y is the upper left corner
          * A bounding circle of the form (x,y,r)
          * A list of x,y tuples defining a closed polygon e.g. ((x,y),(x,y),....)
          * Any two dimensional feature (e.g. blobs, circle ...)

        **RETURNS**

        Returns a Boolean, True if the feature is below the object, False otherwise.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> blobs = img.findBlobs()
        >>> b = blobs[0]
        >>> if( blobs[-1].below(b) ):
        >>>    print "above the biggest blob"

        """
        if( isinstance(object,Feature) ):
            return( self.minY() > object.maxY() )
        elif( isinstance(object,tuple) or isinstance(object,np.ndarray) ):
            return( self.minY() > object[1]  )
        elif( isinstance(object,float) or isinstance(object,int) ):
            return( self.minY() > object )
        else:
            logger.warning("SimpleCV did not recognize the input type to feature.below(). This method only takes another feature, an (x,y) tuple, or a ndarray type.")
            return None


    def right(self,object):
        """
        **SUMMARY**

        Return true if the feature is to the right object, where object can be a bounding box,
        bounding circle, a list of tuples in a closed polygon, or any other featutres.

        **PARAMETERS**

        * *object*

          * A bounding box - of the form (x,y,w,h) where x,y is the upper left corner
          * A bounding circle of the form (x,y,r)
          * A list of x,y tuples defining a closed polygon e.g. ((x,y),(x,y),....)
          * Any two dimensional feature (e.g. blobs, circle ...)

        **RETURNS**

        Returns a Boolean, True if the feature is to the right object, False otherwise.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> blobs = img.findBlobs()
        >>> b = blobs[0]
        >>> if( blobs[-1].right(b) ):
        >>>    print "right of the the blob"

        """
        if( isinstance(object,Feature) ):
            return( self.minX() > object.maxX() )
        elif( isinstance(object,tuple) or isinstance(object,np.ndarray) ):
            return( self.minX() > object[0]  )
        elif( isinstance(object,float) or isinstance(object,int) ):
            return( self.minX() > object )
        else:
            logger.warning("SimpleCV did not recognize the input type to feature.right(). This method only takes another feature, an (x,y) tuple, or a ndarray type.")
            return None

    def left(self,object):
        """
        **SUMMARY**

        Return true if the feature is to the left of  the object, where object can be a bounding box,
        bounding circle, a list of tuples in a closed polygon, or any other featutres.

        **PARAMETERS**

        * *object*

          * A bounding box - of the form (x,y,w,h) where x,y is the upper left corner
          * A bounding circle of the form (x,y,r)
          * A list of x,y tuples defining a closed polygon e.g. ((x,y),(x,y),....)
          * Any two dimensional feature (e.g. blobs, circle ...)

        **RETURNS**

        Returns a Boolean, True if the feature is to the left of  the object, False otherwise.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> blobs = img.findBlobs()
        >>> b = blobs[0]
        >>> if( blobs[-1].left(b) ):
        >>>    print "left of  the biggest blob"


        """
        if( isinstance(object,Feature) ):
            return( self.maxX() < object.minX() )
        elif( isinstance(object,tuple) or isinstance(object,np.ndarray) ):
            return( self.maxX() < object[0]  )
        elif( isinstance(object,float) or isinstance(object,int) ):
            return( self.maxX() < object )
        else:
            logger.warning("SimpleCV did not recognize the input type to feature.left(). This method only takes another feature, an (x,y) tuple, or a ndarray type.")
            return None

    def contains(self,other):
        """
        **SUMMARY**

        Return true if the feature contains  the object, where object can be a bounding box,
        bounding circle, a list of tuples in a closed polygon, or any other featutres.

        **PARAMETERS**

        * *object*

          * A bounding box - of the form (x,y,w,h) where x,y is the upper left corner
          * A bounding circle of the form (x,y,r)
          * A list of x,y tuples defining a closed polygon e.g. ((x,y),(x,y),....)
          * Any two dimensional feature (e.g. blobs, circle ...)

        **RETURNS**

        Returns a Boolean, True if the feature contains the object, False otherwise.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> blobs = img.findBlobs()
        >>> b = blobs[0]
        >>> if( blobs[-1].contains(b) ):
        >>>    print "this blob is contained in the biggest blob"

        **NOTE**

        This currently performs a bounding box test, not a full polygon test for speed.

        """
        retVal = False

        bounds = self.points
        if( isinstance(other,Feature) ):# A feature
            retVal = True
            for p in other.points: # this isn't completely correct - only tests if points lie in poly, not edges.
                p2 = (int(p[0]),int(p[1]))
                retVal = self._pointInsidePolygon(p2,bounds)
                if( not retVal ):
                    break
        # a single point
        elif( (isinstance(other,tuple) and len(other)==2) or ( isinstance(other,np.ndarray) and other.shape[0]==2) ):
            retVal = self._pointInsidePolygon(other,bounds)

        elif( isinstance(other,tuple) and len(other)==3 ): # A circle
            #assume we are in x,y, r format
            retVal = True
            rr = other[2]*other[2]
            x = other[0]
            y = other[1]
            for p in bounds:
                test = ((x-p[0])*(x-p[0]))+((y-p[1])*(y-p[1]))
                if( test < rr ):
                    retVal = False
                    break

        elif( isinstance(other,tuple) and len(other)==4 and ( isinstance(other[0],float) or isinstance(other[0],int))):
            retVal = ( self.maxX() <= other[0]+other[2] and
                       self.minX() >= other[0] and
                       self.maxY() <= other[1]+other[3] and
                       self.minY() >= other[1] )
        elif(isinstance(other,list) and len(other) >= 4): # an arbitrary polygon
            #everything else ....
            retVal = True
            for p in other:
                test = self._pointInsidePolygon(p,bounds)
                if(not test):
                    retVal = False
                    break
        else:
            logger.warning("SimpleCV did not recognize the input type to features.contains. This method only takes another blob, an (x,y) tuple, or a ndarray type.")
            return False

        return retVal

    def overlaps(self, other):
        """
        **SUMMARY**

        Return true if the feature overlaps the object, where object can be a bounding box,
        bounding circle, a list of tuples in a closed polygon, or any other featutres.

        **PARAMETERS**

        * *object*

          * A bounding box - of the form (x,y,w,h) where x,y is the upper left corner
          * A bounding circle of the form (x,y,r)
          * A list of x,y tuples defining a closed polygon e.g. ((x,y),(x,y),....)
          * Any two dimensional feature (e.g. blobs, circle ...)

        **RETURNS**

        Returns a Boolean, True if the feature overlaps  object, False otherwise.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> blobs = img.findBlobs()
        >>> b = blobs[0]
        >>> if( blobs[-1].overlaps(b) ):
        >>>    print "This blob overlaps the biggest blob"

        Returns true if this blob contains at least one point, part of a collection
        of points, or any part of a blob.

        **NOTE**

        This currently performs a bounding box test, not a full polygon test for speed.

       """
        retVal = False
        bounds = self.points

        if( isinstance(other,Feature) ):# A feature
            retVal = True
            for p in other.points: # this isn't completely correct - only tests if points lie in poly, not edges.
                retVal = self._pointInsidePolygon(p,bounds)
                if( retVal ):
                    break

        elif( (isinstance(other,tuple) and len(other)==2) or ( isinstance(other,np.ndarray) and other.shape[0]==2) ):
            retVal = self._pointInsidePolygon(other,bounds)

        elif( isinstance(other,tuple) and len(other)==3 and not isinstance(other[0],tuple)): # A circle
            #assume we are in x,y, r format
            retVal = False
            rr = other[2]*other[2]
            x = other[0]
            y = other[1]
            for p in bounds:
                test = ((x-p[0])*(x-p[0]))+((y-p[1])*(y-p[1]))
                if( test < rr ):
                    retVal = True
                    break

        elif( isinstance(other,tuple) and len(other)==4 and ( isinstance(other[0],float) or isinstance(other[0],int))):
            retVal = ( self.contains( (other[0],other[1] ) ) or # see if we contain any corner
                       self.contains( (other[0]+other[2],other[1] ) ) or
                       self.contains( (other[0],other[1]+other[3] ) ) or
                       self.contains( (other[0]+other[2],other[1]+other[3] ) ) )
        elif(isinstance(other,list) and len(other)  >= 3): # an arbitrary polygon
            #everything else ....
            retVal = False
            for p in other:
                test = self._pointInsidePolygon(p,bounds)
                if(test):
                    retVal = True
                    break
        else:
            logger.warning("SimpleCV did not recognize the input type to features.overlaps. This method only takes another blob, an (x,y) tuple, or a ndarray type.")
            return False

        return retVal

    def doesNotContain(self, other):
        """
        **SUMMARY**

        Return true if the feature does not contain  the other object, where other can be a bounding box,
        bounding circle, a list of tuples in a closed polygon, or any other featutres.

        **PARAMETERS**

        * *other*

          * A bounding box - of the form (x,y,w,h) where x,y is the upper left corner
          * A bounding circle of the form (x,y,r)
          * A list of x,y tuples defining a closed polygon e.g. ((x,y),(x,y),....)
          * Any two dimensional feature (e.g. blobs, circle ...)

        **RETURNS**

        Returns a Boolean, True if the feature does not contain the object, False otherwise.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> blobs = img.findBlobs()
        >>> b = blobs[0]
        >>> if( blobs[-1].doesNotContain(b) ):
        >>>    print "above the biggest blob"

        Returns true if all of features points are inside this point.
        """
        return not self.contains(other)

    def doesNotOverlap( self, other):
        """
        **SUMMARY**

        Return true if the feature does not overlap the object other, where other can be a bounding box,
        bounding circle, a list of tuples in a closed polygon, or any other featutres.

        **PARAMETERS**

        * *other*

          * A bounding box - of the form (x,y,w,h) where x,y is the upper left corner
          * A bounding circle of the form (x,y,r)
          * A list of x,y tuples defining a closed polygon e.g. ((x,y),(x,y),....)
          * Any two dimensional feature (e.g. blobs, circle ...)

        **RETURNS**

        Returns a Boolean, True if the feature does not Overlap  the object, False otherwise.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> blobs = img.findBlobs()
        >>> b = blobs[0]
        >>> if( blobs[-1].doesNotOverlap(b) ):
        >>>    print "does not over overlap biggest blob"


        """
        return not self.overlaps( other)


    def isContainedWithin(self,other):
        """
        **SUMMARY**

        Return true if the feature is contained withing  the object other, where other can be a bounding box,
        bounding circle, a list of tuples in a closed polygon, or any other featutres.

        **PARAMETERS**

        * *other*

          * A bounding box - of the form (x,y,w,h) where x,y is the upper left corner
          * A bounding circle of the form (x,y,r)
          * A list of x,y tuples defining a closed polygon e.g. ((x,y),(x,y),....)
          * Any two dimensional feature (e.g. blobs, circle ...)

        **RETURNS**

        Returns a Boolean, True if the feature is above the object, False otherwise.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> blobs = img.findBlobs()
        >>> b = blobs[0]
        >>> if( blobs[-1].isContainedWithin(b) ):
        >>>    print "inside the blob"

        """
        retVal = True
        bounds = self.points

        if( isinstance(other,Feature) ): # another feature do the containment test
            retVal = other.contains(self)
        elif( isinstance(other,tuple) and len(other)==3 ): # a circle
            #assume we are in x,y, r format
            rr = other[2]*other[2] # radius squared
            x = other[0]
            y = other[1]
            for p in bounds:
                test = ((x-p[0])*(x-p[0]))+((y-p[1])*(y-p[1]))
                if( test > rr ):
                    retVal = False
                    break
        elif( isinstance(other,tuple) and len(other)==4 and  # a bounding box
            ( isinstance(other[0],float) or isinstance(other[0],int))): # we assume a tuple of four is (x,y,w,h)
            retVal = ( self.maxX() <= other[0]+other[2] and
                       self.minX() >= other[0] and
                       self.maxY() <= other[1]+other[3] and
                       self.minY() >= other[1] )
        elif(isinstance(other,list) and len(other) > 2 ): # an arbitrary polygon
            #everything else ....
            retVal = True
            for p in bounds:
                test = self._pointInsidePolygon(p,other)
                if(not test):
                    retVal = False
                    break

        else:
            logger.warning("SimpleCV did not recognize the input type to features.contains. This method only takes another blob, an (x,y) tuple, or a ndarray type.")
            retVal = False
        return retVal


    def isNotContainedWithin(self,shape):
        """
        **SUMMARY**

        Return true if the feature is not contained within the shape, where shape can be a bounding box,
        bounding circle, a list of tuples in a closed polygon, or any other featutres.

        **PARAMETERS**

        * *shape*

          * A bounding box - of the form (x,y,w,h) where x,y is the upper left corner
          * A bounding circle of the form (x,y,r)
          * A list of x,y tuples defining a closed polygon e.g. ((x,y),(x,y),....)
          * Any two dimensional feature (e.g. blobs, circle ...)

        **RETURNS**

        Returns a Boolean, True if the feature is not contained within the shape, False otherwise.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> blobs = img.findBlobs()
        >>> b = blobs[0]
        >>> if( blobs[-1].isNotContainedWithin(b) ):
        >>>    print "Not inside the biggest blob"

        """
        return not self.isContainedWithin(shape)

    def _pointInsidePolygon(self,point,polygon):
        """
        returns true if tuple point (x,y) is inside polygon of the form ((a,b),(c,d),...,(a,b)) the polygon should be closed

        """
        # try:
        #     import cv2
        # except:
        #     logger.warning("Unable to import cv2")
        #     return False

        if( len(polygon) < 3 ):
            logger.warning("feature._pointInsidePolygon - this is not a valid polygon")
            return False

        if( not isinstance(polygon,list)):
            logger.warning("feature._pointInsidePolygon - this is not a valid polygon")
            return False

        #if( not isinstance(point,tuple) ):
            #if( len(point) == 2 ):
            #    point = tuple(point)
            #else:
            #    logger.warning("feature._pointInsidePolygon - this is not a valid point")
            #    return False
        #if( cv2.__version__ == '$Rev:4557'):
        counter = 0
        retVal = True
        p1 = None
        #print "point: " + str(point)
        poly = copy.deepcopy(polygon)
        poly.append(polygon[0])
        #for p2 in poly:
        N = len(poly)
        p1 = poly[0]
        for i in range(1,N+1):
            p2 = poly[i%N]
            if( point[1] > np.min((p1[1],p2[1])) ):
                if( point[1] <= np.max((p1[1],p2[1])) ):
                    if( point[0] <= np.max((p1[0],p2[0])) ):
                        if( p1[1] != p2[1] ):
                            test = float((point[1]-p1[1])*(p2[0]-p1[0]))/float(((p2[1]-p1[1])+p1[0]))
                            if( p1[0] == p2[0] or point[0] <= test ):
                                counter = counter + 1
            p1 = p2

        if( counter % 2 == 0 ):
            retVal = False
            return retVal
        return retVal
        #else:
        #    result = cv2.pointPolygonTest(np.array(polygon,dtype='float32'),point,0)
        #    return result > 0 

    def boundingCircle(self):
        """
        **SUMMARY**

        This function calculates the minimum bounding circle of the blob in the image
        as an (x,y,r) tuple

        **RETURNS**

        An (x,y,r) tuple where (x,y) is the center of the circle and r is the radius

        **EXAMPLE**

        >>> img = Image("RatMask.png")
        >>> blobs = img.findBlobs()
        >>> print blobs[-1].boundingCircle()

        """

        try:
            import cv2
        except:
            logger.warning("Unable to import cv2")
            return None

        # contour of the blob in image
        contour = self.contour()

        points = []
        # list of contour points converted to suitable format to pass into cv2.minEnclosingCircle()
        for pair in contour:
            points.append([[pair[0], pair[1]]])

        points = np.array(points)

        (cen, rad) = cv2.minEnclosingCircle(points);

        return (cen[0], cen[1], rad)


#---------------------------------------------

########NEW FILE########
__FILENAME__ = FeatureUtils
from SimpleCV.base import *
from SimpleCV.ImageClass import *
from SimpleCV.Color import *
from SimpleCV.Features.Features import Feature, FeatureSet
from SimpleCV.Features.Detection import *
"""
So this is a place holder for some routines that should live in
featureset if we can make it specific to a type of features
"""

def GetParallelSets(line_fs,parallel_thresh=2):
    result = []
    sz = len(line_fs)
    #construct the pairwise cross product ignoring dupes
    for i in range(0,sz):
        for j in range(0,sz):
            if( j<=i ):
                result.append(np.Inf)
            else:
                result.append(np.abs(line_fs[i].cross(line_fs[j])))

    result = np.array(result)
    # reshape it
    result = result.reshape(sz,sz)
    # find the lines that are less than our thresh
    l1,l2=np.where(result<parallel_thresh)
    idxs = zip(l1,l2)
    retVal = []
    # now construct the line pairs
    for idx in idxs:
        retVal.append((line_fs[idx[0]],line_fs[idx[1]]))
    return retVal

def ParallelDistance(line1,line2):
    pass

########NEW FILE########
__FILENAME__ = HaarCascade
from SimpleCV.base import *

class HaarCascade():
    """
    This class wraps HaarCascade files for the findHaarFeatures file.
    To use the class provide it with the path to a Haar cascade XML file and
    optionally a name.
    """
    _mCascade = None
    _mName = None
    _cache = {}
    _fhandle = None


    def __init__(self, fname=None, name=None):
        #if fname.isalpha():
        #     fname = MY_CASCADES_DIR + fname + ".xml"

        if( name is None ):
            self._mName = fname
        else:
            self._mName = name

        #First checks the path given by the user, if not then checks SimpleCV's default folder
        if fname is not None:
            if os.path.exists(fname):
                self._fhandle = os.path.abspath(fname)
            else:
                self._fhandle = os.path.join(LAUNCH_PATH, 'Features','HaarCascades',fname)
                if (not os.path.exists(self._fhandle)):
                    logger.warning("Could not find Haar Cascade file " + fname)
                    logger.warning("Try running the function img.listHaarFeatures() to see what is available")
                    return None
            
            self._mCascade = cv.Load(self._fhandle)

            if HaarCascade._cache.has_key(self._fhandle):
                self._mCascade = HaarCascade._cache[self._fhandle]
                return
            HaarCascade._cache[self._fhandle] = self._mCascade

    def load(self, fname=None, name = None):
        if( name is None ):
            self._mName = fname
        else:
            self._mName = name

        if fname is not None:
            if os.path.exists(fname):
                self._fhandle = os.path.abspath(fname)
            else:
                self._fhandle = os.path.join(LAUNCH_PATH, 'Features','HaarCascades',fname)
                if (not os.path.exists(self._fhandle)):
                    logger.warning("Could not find Haar Cascade file " + fname)
                    logger.warning("Try running the function img.listHaarFeatures() to see what is available")
                    return None
            
            self._mCascade = cv.Load(self._fhandle)

            if HaarCascade._cache.has_key(self._fhandle):
                self._mCascade = HaarCascade._cache[fname]
                return
            HaarCascade._cache[self._fhandle] = self._mCascade
        else:
            logger.warning("No file path mentioned.")

    def getCascade(self):
        return self._mCascade

    def getName(self):
        return self._mName

    def setName(self,name):
        self._mName = name

    def getFHandle(self):
        return self._fhandle

########NEW FILE########
__FILENAME__ = HaarLikeFeature
from SimpleCV.base import *
from SimpleCV.ImageClass import Image


class HaarLikeFeature():
    """
    Create a single Haar feature and optionally set the regions that define
    the Haar feature and its name. The formal of the feature is

    The format is [[[TL],[BR],SIGN],[[TL],[BR],SIGN].....]
    Where TR and BL are the unit coorinates for the top right and bottom
    left coodinates.

    For example
    [[[0,0],[0.5,0.5],1],[[0.5.0],[1.0,1.0],-1]]

    Takes the right side of the image and subtracts from the left hand side
    of the image.
    """
    mName = None
    mRegions = None
    def __init__(self, name=None,regions=None):

        self.mName = name;
        self.mRegions = regions;


    def setRegions(self,regions):
        """
        Set the list of regions. The regions are square coordinates on a unit
        sized image followed by the sign of a region.

        The format is [[[TL],[BR],SIGN],[[TL],[BR],SIGN].....]
        Where TR and BL are the unit coorinates for the top right and bottom
        left coodinates.

        For example
        [[[0,0],[0.5,0.5],1],[[0.5.0],[1.0,1.0],-1]]

        Takes the right side of the image and subtracts from the left hand side
        of the image.
        """
        self.mRegions = regions

    def setName(self,name):
        """
        Set the name of this feature, the name must be unique.
        """
        self.mName = name

    def apply(self, intImg ):
        """
        This method takes in an integral image and applies the haar-cascade
        to the image, and returns the result.
        """
        w = intImg.shape[0]-1
        h = intImg.shape[1]-1
        accumulator = 0
        for i in range(len(self.mRegions)):
            # using the integral image
            # A = Lower Right Hand Corner
            # B = upper right hand corner
            # C = lower left hand corner
            # D = upper left hand corner
            # sum = A - B - C  + D
            # regions are in
            # (p,q,r,s,t) format
            p = self.mRegions[i][0] # p = left (all are unit length)
            q = self.mRegions[i][1] # q = top
            r = self.mRegions[i][2] # r = right
            s = self.mRegions[i][3] # s = bottom
            sign = self.mRegions[i][4] # t = sign
            xA = int(w*r)
            yA = int(h*s)
            xB = int(w*r)
            yB = int(h*q)
            xC = int(w*p)
            yC = int(h*s)
            xD = int(w*p)
            yD = int(h*q)
            accumulator += sign*(intImg[xA,yA]-intImg[xB,yB]-intImg[xC,yC]+intImg[xD,yD])
        return accumulator

    def writeToFile(self,file):
        """
        Write the Haar cascade to a human readable file. file is an open file pointer.
        """
        file.write(self.mName)
        file.write(" "+str(len(self.mRegions))+"\n")
        for i in range(len(self.mRegions)):
            temp = self.mRegions[i]
            for j in range(len(temp)):
                file.write(str(temp[j])+' ')
            file.write('\n')
        file.write('\n')

########NEW FILE########
__FILENAME__ = HaarLikeFeatureExtractor
from SimpleCV.base import *
from SimpleCV.ImageClass import Image
from SimpleCV.Features.HaarLikeFeature import *
from SimpleCV.Features.FeatureExtractorBase import *

class HaarLikeFeatureExtractor(FeatureExtractorBase):
    """
    This is used generate Haar like features from an image.  These
    Haar like features are used by a the classifiers of machine learning
    to help identify objects or things in the picture by their features,
    or in this case haar features.

    For a more in-depth review of Haar Like features see:
    http://en.wikipedia.org/wiki/Haar-like_features
    """

    mFeatureSet = None
    mDo45 = True
    def __init__(self, fname=None, do45=True):
        """
        fname - The feature file name
        do45 - if this is true we use the regular integral image plus the
        45 degree integral image
        """
        #we define the black (positive) and white (negative) regions of an image
        #to get our haar wavelet
        self.mDo45 = True
        self.mFeatureset=None;
        if(fname is not None):
            self.readWavelets(fname)

    def readWavelets(self, fname,nfeats=-1):
        """
        fname = file name
        nfeats = number of features to load from file -1 -> All features
        """
        # We borrowed the wavelet file from  Chesnokov Yuriy
        # He has a great windows tutorial here:
        # http://www.codeproject.com/KB/audio-video/haar_detection.aspx
        # SimpleCV Took a vote and we think he is an all around swell guy!
        # nfeats = number of features to load
        # -1 loads all
        # otherwise loads min(nfeats,features in file)
        self.mFeatureSet = []
        f = open(fname,'r')
        #line = f.readline()
        #count = int(line)
        temp = f.read()
        f.close()
        data = temp.split()
        count = int(data.pop(0))
        self.mFeatureset = []
        if(nfeats > -1):
            count = min(count,nfeats)
        while len(data) > 0:
            name = data.pop(0)
            nRegions = int(data.pop(0))
            region = []
            for i in range(nRegions):
                region.append(tuple(map(float,data[0:5])))
                data = data[5:]

            feat = HaarLikeFeature(name,region)
            self.mFeatureSet.append(feat)
        return None

    def saveWavelets(self, fname):
        """
        Save wavelets to file
        """
        f = open(fname,'w')
        f.write(str(len(self.mFeatureSet))+'\n\n')
        for i in range(len(self.mFeatureSet)):
            self.mFeatureSet[i].writeToFile(f)
        f.close()
        return None

    def extract(self, img):
        """
        This extractor takes in an image, creates the integral image, applies
        the Haar cascades, and returns the result as a feature vector.
        """
        regular = img.integralImage()
        retVal = []

        for i in range(len(self.mFeatureSet)):
            retVal.append(self.mFeatureSet[i].apply(regular))
        if(self.mDo45):
            slant = img.integralImage(tilted=True)
            for i in range(len(self.mFeatureSet)):
                retVal.append(self.mFeatureSet[i].apply(regular))
        return retVal

    def getFieldNames(self):
        """
        This method gives the names of each field in the feature vector in the
        order in which they are returned. For example, 'xpos' or 'width'
        """
        retVal = []
        for i in range( len(self.mFeatureSet)):
            retVal.append(self.mFeatureSet[i].mName)
        if( self.mDo45 ):
            for i in range( len(self.mFeatureSet)):
                name = "Angle_"+self.mFeatureSet[i].mName
                retVal.append(name)
        return retVal


    def getNumFields(self):
        """
        This method returns the total number of fields in the feature vector.
        """
        mult = 1
        if(self.mDo45):
            mult = 2
        return mult*len(self.mFeatureset)

########NEW FILE########
__FILENAME__ = HueHistogramFeatureExtractor
from SimpleCV.base import *
from SimpleCV.ImageClass import Image
from SimpleCV.Features.FeatureExtractorBase import *

class HueHistogramFeatureExtractor(FeatureExtractorBase):
    """
    Create a Hue Histogram feature extractor. This feature extractor
    takes in an image, gets the hue channel, bins the number of pixels
    with a particular Hue, and returns the results.

    mNBins - the number of Hue bins.
    """
    mNBins = 16
    def __init__(self, mNBins=16):
        #we define the black (positive) and white (negative) regions of an image
        #to get our haar wavelet
        self.mNBins = mNBins

    def extract(self, img):
        """
        This feature extractor takes in a color image and returns a normalized color
        histogram of the pixel counts of each hue.
        """
        img = img.toHLS()
        h = img.getEmpty(1)
        cv.Split(img.getBitmap(),h,None,None,None)
        npa = np.array(h[:,:])
        npa = npa.reshape(1,npa.shape[0]*npa.shape[1])
        hist = np.histogram(npa,self.mNBins,normed=True,range=(0,255))
        return hist[0].tolist()


    def getFieldNames(self):
        """
        This method gives the names of each field in the feature vector in the
        order in which they are returned. For example, 'xpos' or 'width'
        """
        retVal = []
        for i in range(self.mNBins):
            name = "Hue"+str(i)
            retVal.append(name)
        return retVal

    def getNumFields(self):
        """
        This method returns the total number of fields in the feature vector.
        """
        return self.mNBins

########NEW FILE########
__FILENAME__ = MorphologyFeatureExtractor
from SimpleCV.base import *
from SimpleCV.ImageClass import Image
from SimpleCV.Features.FeatureExtractorBase import *
from SimpleCV.Features.BlobMaker import *

class MorphologyFeatureExtractor(FeatureExtractorBase):
    """
    This feature extractor collects some basic morphology infromation about a given
    image. It is assumed that the object to be recognized is the largest object
    in the image. The user must provide a segmented white on black blob image.
    This operation then straightens the image and collects the data.
    """
    mNBins = 9
    mBlobMaker = None
    mThresholdOpeation = None
    def __init__(self, thresholdOperation=None):
        """
        The threshold operation is a function of the form
        binaryimg = threshold(img)

        the simplest example would be:
        def binarize_wrap(img):

        """
        self.mNBins = 9
        self.mBlobMaker = BlobMaker()
        self.mThresholdOpeation = thresholdOperation

    def setThresholdOperation(self, threshOp):
        """
        The threshold operation is a function of the form
        binaryimg = threshold(img)

        Example:

        >>> def binarize_wrap(img):
        >>>    return img.binarize()
        """
        self.mThresholdOperation = threshOp

    def extract(self, img):
        """
        This method takes in a image and returns some basic morphology
        characteristics about the largest blob in the image. The
        if a color image is provided the threshold operation is applied.
        """
        retVal = None
        if(self.mThresholdOpeation is not None):
            bwImg = self.mThresholdOpeation(img)
        else:
            bwImg = img.binarize()

        if( self.mBlobMaker is None ):
            self.mBlobMaker = BlobMaker()

        fs = self.mBlobMaker.extractFromBinary(bwImg,img)
        if( fs is not None and len(fs) > 0 ):
            fs = fs.sortArea()
            retVal = []
            retVal.append(fs[0].mArea/fs[0].mPerimeter)
            retVal.append(fs[0].mAspectRatio)
            retVal.append(fs[0].mHu[0])
            retVal.append(fs[0].mHu[1])
            retVal.append(fs[0].mHu[2])
            retVal.append(fs[0].mHu[3])
            retVal.append(fs[0].mHu[4])
            retVal.append(fs[0].mHu[5])
            retVal.append(fs[0].mHu[6])
        return retVal


    def getFieldNames(self):
        """
        This method gives the names of each field in the feature vector in the
        order in which they are returned. For example, 'xpos' or 'width'
        """
        retVal = []
        retVal.append('area over perim')
        retVal.append('AR')
        retVal.append('Hu0')
        retVal.append('Hu1')
        retVal.append('Hu2')
        retVal.append('Hu3')
        retVal.append('Hu4')
        retVal.append('Hu5')
        retVal.append('Hu6')
        return retVal


    def getNumFields(self):
        """
        This method returns the total number of fields in the feature vector.
        """
        return self.mNBins

    def __getstate__(self):
        mydict = self.__dict__.copy()
        self.mBlobMaker = None
        del mydict['mBlobMaker']
        return mydict

    def __setstate__(self, mydict):
        self.__dict__ = mydict
        self.mBlobMaker = BlobMaker()

########NEW FILE########
__FILENAME__ = CardDataCollection
from SimpleCV import Image, Display,Camera,Color
import glob,os
import pygame as pg
from CardUtil import SUITS, RANKS, MISC
#SUITS = ('c', 'd', 'h', 's')
#RANKS = ('2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K', 'A')
#MISC = ( 'none','bad','joker')

disp = Display((640,480))
cam = Camera()

path = "./data/"
ext = ".png"
suit_ptr = 0
rank_ptr = 0
current_dir = ""
allDone = False
for s in SUITS:
    for r in RANKS:
        directory = path+s+"/"+r+"/"
        if not os.path.exists(directory):
            os.makedirs(directory)
print "Current Data: " + str(RANKS[rank_ptr])+str(SUITS[suit_ptr])
while not allDone :
    keys = disp.checkEvents()
    img = cam.getImage()
    for k in keys:
        if( k == pg.K_SPACE ):
            directory = path+SUITS[suit_ptr]+"/"+RANKS[rank_ptr]+"/"
            files = glob.glob(directory+"*.*")
            count = len(files)
            fname = directory+RANKS[rank_ptr]+SUITS[suit_ptr]+"-"+str(count)+ext
            img.save(fname)
            print "Saved: " + fname
        if( k == pg.K_n ):
            if rank_ptr == len(RANKS)-1 and suit_ptr == len(SUITS)-1:
                allDone = True
            elif rank_ptr == len(RANKS)-1:
                rank_ptr = 0
                suit_ptr = suit_ptr + 1
            else:
                print rank_ptr
                rank_ptr = rank_ptr + 1
        print "Current Data" + str(RANKS[rank_ptr])+str(SUITS[suit_ptr])

    img.drawLine((0,img.height/4),(img.width,img.height/4),color=Color.RED,thickness=3)
    img.drawLine((0,3*img.height/4),(img.width,3*img.height/4),color=Color.RED,thickness=3)
    img.drawLine((img.width/3,0),(img.width/3,img.height),color=Color.RED,thickness=3)
    img.drawLine((2*img.width/3,0),(2*img.width/3,img.height   ),color=Color.RED,thickness=3)
    img.save(disp)

########NEW FILE########
__FILENAME__ = cards
import itertools
import random
import pickle
import time

# cribbed from
# http://pastebin.com/mzNmCdV5 
# http://stackoverflow.com/questions/2518753/best-way-to-implement-a-deck-for-a-card-game-in-python
# data
SUITS = ('c', 'd', 'h', 's')
RANKS = ('2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K', 'A')

DECK = tuple(''.join(card) for card in itertools.product(RANKS, SUITS))

ORDER_LOOKUP = dict(zip(DECK, range(52)))
RANK_LOOKUP = dict(zip(RANKS, range(13)))
SUIT_LOOKUP = dict(zip(SUITS, range(4)))

# utility functions
def cmp_cards(a, b):
    return cmp(ORDER_LOOKUP[a], ORDER_LOOKUP[b])
    
def cmp_tuples(a, b):
    n1 = len(a)
    n2 = len(b)
    if n1 != n2:
        return cmp(n1, n2)
    return cmp(a, b)
    
def suit(card):
    return card[1]
    
def suit_int(card):
    return SUIT_LOOKUP[card[1]]
    
def rank(card):
    return card[0]
    
def rank_int(card):
    return RANK_LOOKUP[card[0]]
    
def card_int(card):
    s = 1 << suit_int(card)
    r = rank_int(card)
    c = (s << 4) | r
    return c
    
# test functions
def is_straight(cards):
    previous = rank_int(cards[0]) - 1
    for card in cards:
        r = rank_int(card)
        if r != previous + 1:
            if not (r == 12 and previous == 3):
                return False
        previous = r
    return True
    
def is_flush(cards):
    s = suit(cards[0])
    return all(suit(card) == s for card in cards)
    
def same_rank(cards):
    r = rank(cards[0])
    return all(rank(card) == r for card in cards)
    
def split_ranks(cards, indexes):
    for index in indexes:
        a, b = cards[:index], cards[index:]
        if same_rank(a) and same_rank(b):
            return True
    return False
    
def is_full_house(cards):
    return split_ranks(cards, (2, 3))
    
def is_four(cards):
    return split_ranks(cards, (1, 4))
    
def is_pat(cards):
    return is_straight(cards) or is_flush(cards) or is_full_house(cards) or is_four(cards)
    
def is_straight_flush(cards):
    return is_straight(cards) and is_flush(cards)
    
def rank_count(cards):
    result = {}
    for card in cards:
        r = rank_int(card)
        result[r] = result.get(r, 0) + 1
    return result
    
def is_three(cards, counts=None):
    counts = counts or rank_count(cards)
    for rank, count in counts.iteritems():
        if count == 3:
            return True
    return False
    
def is_two_pair(cards, counts=None):
    pairs = 0
    counts = counts or rank_count(cards)
    for rank, count in counts.iteritems():
        if count == 2:
            pairs += 1
    return pairs == 2
    
def is_pair(cards, counts=None):
    counts = counts or rank_count(cards)
    for rank, count in counts.iteritems():
        if count == 2:
            return True
    return False
    
def get_ranks(counts):
    values = [(count, rank) for rank, count in counts.iteritems()]
    values.sort(reverse=True)
    values = [n[1] for n in values]
    return values
    
def get_straight_rank(cards):
    top = rank_int(cards[-1])
    bottom = rank_int(cards[0])
    if top == 12 and bottom == 0:
        return 3
    return top
    
def evaluate_hand(cards):
    flush = is_flush(cards)
    straight = is_straight(cards)
    counts = rank_count(cards)
    ranks = get_ranks(counts)
    if straight:
        ranks = [get_straight_rank(cards)]
    if straight and flush:
        value = 9
    elif is_four(cards):
        value = 8
    elif is_full_house(cards):
        value = 7
    elif flush:
        value = 6
    elif straight:
        value = 5
    elif is_three(cards, counts):
        value = 4
    elif is_two_pair(cards, counts):
        value = 3
    elif is_pair(cards, counts):
        value = 2
    else:
        value = 1
    ranks.insert(0, value)
    return tuple(ranks)

########NEW FILE########
__FILENAME__ = CardUtil
from glob import glob
from SimpleCV import Image, ImageSet

SUITS = ('c', 'd', 'h', 's')
RANKS = ('2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K', 'A')
MISC = ( 'none','bad','joker')

def GetSpecificCardData(suit,rank,path="./data/",label=True):
    fullpath = path+"/"+suit+"/"+rank+"/"
    print fullpath
    iset = ImageSet(fullpath)
    if( label ):
        label_vals = []
        l = (suit,rank) # not sure this is how we want to do this
        sz = len(iset)
        for i in range(0,sz):
            label_vals.append(l)
        return iset,label_vals
    else:
        return iset

def GetFullDataSet(path="./data",label=True): # just load everything
    data = ImageSet()
    labels = []
    for s in SUITS:
        for r in RANKS:
            if( label ):
                d,l = GetSpecificCardData(s,r,path,label)
                data += d
                labels += l
            else:
                d = GetSpecificCardData(s,r,path,label)
                data += d
    if( label ):
        return data,labels
    else:
        return data

########NEW FILE########
__FILENAME__ = PlayingCard
from SimpleCV.base import *
from SimpleCV.ImageClass import *
from SimpleCV.Color import *
from SimpleCV.Features.Features import Feature, FeatureSet

class PlayingCard(Feature):
    def __init__(self, i, at_x, at_y, rank=None,suit=None):

        self.rank = rank
        self.suit = suit

        points = [(at_x-1,at_y-1),(at_x-1,at_y+1),(at_x+1,at_y+1),(at_x+1,at_y-1)]
        super(PlayingCard, self).__init__(i, at_x, at_y,points)

    def getCard(self):
        return (self.suit,self.rank)

    def draw(self, color = (255, 0, 0),width=1):
        """
        **SUMMARY**

        Draw a small circle around the corner.  Color tuple is single parameter, default is Red.

        **PARAMETERS**

        * *color* - An RGB color triplet.
        * *width* - if width is less than zero we draw the feature filled in, otherwise we draw the
          contour using the specified width.


        **RETURNS**

        Nothing - this is an inplace operation that modifies the source images drawing layer.

        """
        self.image.drawCircle((self.x, self.y), 4, color,width)

########NEW FILE########
__FILENAME__ = PlayingCardFactory
from SimpleCV.base import *
from SimpleCV.ImageClass import *
from SimpleCV.Color import *
from SimpleCV.Features.Features import Feature, FeatureSet
from SimpleCV.Features.PlayingCards.PlayingCard import *
class CardError(Exception):
    def __init__(self, card=None,message=None):
        self.card = card
        self.msg = message
    def __str__(self):
        return repr(self.msg)


class PlayingCardFactory():

    def __init__(self,parameterDict=None):
        if(parameterDict is not None):
            self.parameterize(parameterDict)

    def parameterize(self,parameterDict):
        """
        Parameterize from a dictionary so we can optimize performance.
        """
        pass

    def process(self,img):
        """
        Process the image. Return a featureset with a single
        PlayingCard feature or None
        """
        # Can we find anything that looks like a card
        card = self._findCardEdges(img)
        if( card is None ): # if we don't see it just bail
            warnings.warn("Could not find a card.")
            return None
        try:
            # extract the basic features and get color
            card = self._estimateColor(card)
            # okay, we got a color and some features
            # go ahead and estimate the suit
            card = self._estimateSuit(card)
            # Do we think this is a face card this
            # is an easier test
            isFace,card = self._isFaceCard(card)
            if(isFace):
                # if we are a face card get the face. This is had
                card = self._estimateFaceCard(card)
            else:
                # otherwise get the rank
                # first pass is corners second
                # pass is the card body
                card = self._estimateRank(card)
            # now go back do some sanity checks
            # and cleanup the features so it is not
            # too heavy
            card = self._refineEstimates(card)
        except CardError as ce:
            card = ce.card
            if( card is not None):
            # maybe we got a joker or someone
            # is being a jackass and showing us the
            # back of the card.
                card = self._isNonStandardCard(card)
            warnings.warn(ce.msg) # we may swallow this later
            # optionally we may want to log these to
            # see where we fail and why or do a parameter
            # adjustment and try again
        except:
            # this means we had an error somewhere
            # else maybe numpy
            print "Generic Error."
            return None
        return FeatureSet([card])

    def _preprocess(self,img):
        """
        Any image preprocessing options go here.
        """
        return img

    def _findCardEdges(self,img):
        """
        Try to find a card, if we do return a card feature
        otherwise return None
        """
        ppimg = self._preprocess(img)
        retVal = PlayingCard(img,img.width/2,img.height/2)
        # create the feature, hang any preprocessing
        # steps on the feature
        return retVal

    def _estimateColor(self,card):
        """
        Take in a card feature and determine the color.
        if we find a color return the feature otherwise
        throw.
        """
        return card

    def _estimateSuit(self,card):
        """
        Using the card feature determine suit.

        If we find something return the card, otherwise
        throw.
        """
        return card

    def _isFaceCard(self,card):
        """
        Determine if we have a face card
        return any updates to the card and the state.
        """
        return False,card

    def _estimateFaceCard(self,card):
        """
        Determine if we have a face card K/Q/J and some A
        """
        return card

    def _estimateRank(self,card):
        """
        Determine the rank and reutrn the card otherwise throw.
        """
        return card

    def _isNonStandardCard(self,card):
        """
        Determine if our card is not a normal card like a joker
        the backside of a card or something similar. Return either the
        card or throw.
        """
        return card

    def _refineEstimates(self,card):
        """
        Do a post process step if we want.
        e.g. find corners with sub-pix accuracy to
        do a swell overlay. Do any numpy sanitation
        for Seer.
        """
        return card

########NEW FILE########
__FILENAME__ = TestScript
from SimpleCV import *
#from FeatureUtils import *
from CardUtil import *
from PlayingCardFactory import *
#import FeatureUtils
import numpy as np

def GetParallelSets(line_fs,parallel_thresh=25):
    result = []
    sz = len(line_fs)
    #construct the pairwise cross product ignoring dupes
    for i in range(0,sz):
        for j in range(0,sz):
            if( j<=i ):
                result.append(np.Inf)
            else:
                result.append(np.abs(line_fs[i].cross(line_fs[j])))

    result = np.array(result)
    # reshape it
    result = result.reshape(sz,sz)
    # find the lines that are less than our thresh
    l1,l2=np.where(result<parallel_thresh)
    idxs = zip(l1,l2)
    retVal = []
    # now construct the line pairs
    for idx in idxs:
        retVal.append((line_fs[idx[0]],line_fs[idx[1]]))
    return retVal


pcf = PlayingCardFactory()
data,labels = GetFullDataSet()
print len(data)
datapoints = zip(data,labels)
datapoints = datapoints[0:200]
result = []
passing = 0
for d in datapoints:
    img = d[0]
    label = d[1]
#    img = img.crop(img.width/3,0,2*img.width/3,img.height)
    #    img = img.equalize()
    img = img.edges()
    l = img.findLines(threshold=10)
    if( l is not None ):
        v = 70
        h = 30
        vl = l.filter(np.abs(l.angle()) > v)
        vl = vl.filter(vl.length() > img.height/6)
        hl = l.filter(np.abs(l.angle()) < h)
        hl = hl.filter(hl.length() > img.width/8)
        vl.draw(color=Color.RED,width=3)
        hl.draw(color=Color.BLUE,width=3)

#         derp = GetParallelSets(l)
#         color = Color()
#         for d in derp:
# #            img.clearLayers()
#             l.draw(color=Color.RED,width=3)
#         #     l.draw()
#             c = color.getRandom()
#             d[0].draw(color=c,width=3)
#             d[1].draw(color=c,width=3)
#             img.show()

        #top = np.min([8,len(l)])
        #l[-1*top:-1].draw(color=Color.RED,width=3)
        img.show()
        time.sleep(.5)
        #l.show(color=Color.RED,width=3)

    # fs = pcf.process(img)
    # r = (None,None)
    # if( fs is not None ):
    #     fs.show()
    #     r = fs[0].getCard()
    # result.append(r)
    # if(r==label):
    #     passing += 1
    #     print "PASS: "+str(label)+"->"+str(r)
    # else:
    #     print "FAIL: "+str(label)+"->"+str(r)
    #time.sleep(0.1)
#print labels

########NEW FILE########
__FILENAME__ = Font
# SimpleCV Font Library
#
# This library is used to add fonts to images

#load required libraries
from SimpleCV.base import *


class Font:
    """
    The Font class allows you to create a font object to be
    used in drawing or writing to images.
    There are some defaults available, to see them, just type
    Font.printFonts()
    """

    _fontpath = "SimpleCV/fonts/"
    _extension = ".ttf"
    _fontface = "ubuntu"
    _fontsize = 16
    _font = None

    # These fonts were downloaded from Google at:
    # http://www.http://www.google.com/webfonts
    _fonts = [
                                            "ubuntu",
                                            "astloch",
                                            "carter_one",
                                            "kranky",
                                            "la_belle_aurore",
                                            "monofett",
                                            "reenie_beanie",
                                            "shadows_Into_light",
                                            "special_elite",
                                            "unifrakturmaguntia",
                                            "vt323",
                                            "wallpoet",
                                            "wire_one"
                                            ]


    def __init__(self, fontface = "ubuntu", fontsize = 16):
        """
        This creates a new font object, it uses ubuntu as the default font
        To give it a custom font you can just pass the absolute path
        to the truetype font file.
        """
        self.setSize(fontsize)
        self.setFont(fontface)


    def getFont(self):
        """
        Get the font from the object to be used in drawing

        Returns: PIL Image Font
        """
        return self._font

    def setFont(self, new_font = 'ubuntu'):
        """
        Set the name of the font listed in the font family
        if the font isn't listed in the font family then pass it the absolute
        path of the truetype font file.
        Example: Font.setFont("/home/simplecv/my_font.ttf")
        """
        if isinstance(new_font, basestring):
            print "Please pass a string"
            return None

        if find(new_font, self._fonts):
            self._fontface = new_font
            font_to_use = self._fontpath + self._fontface + "/" + self._fontface + self._extension
        else:
            self._fontface = new_font
            font_to_use = new_font

        self._font = pilImageFont.truetype(font_to_use, self._fontsize)

    def setSize(self, size):
        """
        Set the font point size. i.e. 16pt
        """
        print type(size)
        if type(size) == int:
            self._fontsize = size
        else:
            print "please provide an integer"

    def getSize(self):
        """
        Gets the size of the current font

        Returns: Integer
        """

        return self._fontsize

    def getFonts(self):
        """
        This returns the list of fonts built into SimpleCV
        """

        return self._fonts

    def printFonts(self):
        """
        This prints a list of fonts built into SimpleCV
        """

        for f in self._fonts:
            print f

########NEW FILE########
__FILENAME__ = ImageClass
# Load required libraries
from SimpleCV.base import *
from SimpleCV.Color import *
from SimpleCV.LineScan import *
from numpy import int32
from numpy import uint8
import cv2

from EXIF import *

if not init_options_handler.headless:
    import pygame as pg

import scipy.ndimage as ndimage
import scipy.stats.stats as sss  #for auto white balance
import scipy.cluster.vq as scv
import scipy.linalg as nla  # for linear algebra / least squares
import math # math... who does that
import copy # for deep copy
#import scipy.stats.mode as spsmode

class ColorSpace:
    """
    **SUMMARY**

    The colorspace  class is used to encapsulate the color space of a given image.
    This class acts like C/C++ style enumerated type.


    See: http://stackoverflow.com/questions/2122706/detect-color-space-with-opencv
    """
    UNKNOWN = 0
    BGR = 1
    GRAY = 2
    RGB = 3
    HLS = 4
    HSV = 5
    XYZ  = 6
    YCrCb = 7

class ImageSet(list):
    """
    **SUMMARY**

    This is an abstract class for keeping a list of images.  It has a few
    advantages in that you can use it to auto load data sets from a directory
    or the net.

    Keep in mind it inherits from a list too, so all the functionality a
    normal python list has this will too.

    **EXAMPLES**


    >>> imgs = ImageSet()
    >>> imgs.download("ninjas")
    >>> imgs.show(ninjas)


    or you can load a directory path:

    >>> imgs = ImageSet('/path/to/imgs/')
    >>> imgs.show()

    This will download and show a bunch of random ninjas.  If you want to
    save all those images locally then just use:

    >>> imgs.save()

    You can also load up the sample images that come with simplecv as:

    >>> imgs = ImageSet('samples')
    >>> imgs.filelist
    >>> logo = imgs.find('simplecv.png')

    **TO DO**

    Eventually this should allow us to pull image urls / paths from csv files.
    The method also allow us to associate an arbitraty bunch of data with each
    image, and on load/save pickle that data or write it to a CSV file.

    """

    filelist = None
    def __init__(self, directory = None):
        if not directory:
            return

        if isinstance(directory,list):
            if isinstance(directory[0], Image):
                super(ImageSet,self).__init__(directory)
            elif isinstance(directory[0], str) or isinstance(directory[0], unicode):
                super(ImageSet,self).__init__(map(Image, directory))

        elif directory.lower() == 'samples' or directory.lower() == 'sample':
            pth = LAUNCH_PATH
            pth = os.path.realpath(pth)
            directory = os.path.join(pth, 'sampleimages')
            self.load(directory)
        else:
            self.load(directory)

    def download(self, tag=None, number=10, size='thumb'):
        """
        **SUMMARY**

        This function downloads images from Google Image search based
        on the tag you provide. The number is the number of images you
        want to have in the list. Valid values for size are 'thumb', 'small',
        'medium', 'large' or a tuple of exact dimensions i.e. (640,480).
        Note that 'thumb' is exceptionally faster than others.

        .. Warning::
          This requires the python library Beautiful Soup to be installed
          http://www.crummy.com/software/BeautifulSoup/

        **PARAMETERS**

        * *tag* - A string of tag values you would like to download.
        * *number* - An integer of the number of images to try and download.
        * *size* - the size of the images to download. Valid options a tuple
          of the exact size or a string of the following approximate sizes:

          * thumb ~ less than 128x128
          * small  ~ approximately less than 640x480 but larger than 128x128
          * medium ~  approximately less than 1024x768 but larger than 640x480.
          * large ~ > 1024x768

        **RETURNS**

        Nothing - but caches local copy of images.

        **EXAMPLE**

        >>> imgs = ImageSet()
        >>> imgs.download("ninjas")
        >>> imgs.show(ninjas)


        """

        try:
            from BeautifulSoup import BeautifulSoup

        except:
            print "You need to install Beatutiul Soup to use this function"
            print "to install you can use:"
            print "easy_install beautifulsoup"

            return


        INVALID_SIZE_MSG = """I don't understand what size images you want.
  Valid options: 'thumb', 'small', 'medium', 'large'
   or a tuple of exact dimensions i.e. (640,480)."""

        if isinstance(size, basestring):
            size = size.lower()
            if size == 'thumb':
                size_param = ''
            elif size == 'small':
                size_param = '&tbs=isz:s'
            elif size == 'medium':
                size_param = '&tbs=isz:m'
            elif size == 'large':
                size_param = '&tbs=isz:l'
            else:
                print INVALID_SIZE_MSG
                return None

        elif type(size) == tuple:
            width, height = size
            size_param = '&tbs=isz:ex,iszw:' + str(width) + ',iszh:' + str(height)

        else:
            print INVALID_SIZE_MSG
            return None

        # Used to extract imgurl parameter value from a URL
        imgurl_re = re.compile('(?<=(&|\?)imgurl=)[^&]*((?=&)|$)')

        add_set = ImageSet()
        candidate_count = 0


        while len(add_set) < number:
            opener = urllib2.build_opener()
            opener.addheaders = [('User-agent', 'Mozilla/5.0')]
            url = ("http://www.google.com/search?tbm=isch&q=" + urllib2.quote(tag) +
                   size_param + "&start=" + str(candidate_count))
            page = opener.open(url)
            soup = BeautifulSoup(page)

            img_urls = []

            # Gets URLs of the thumbnail images
            if size == 'thumb':
                imgs = soup.findAll('img')
                for img in imgs:
                    dl_url = str(dict(img.attrs)['src'])
                    img_urls.append(dl_url)

            # Gets the direct image URLs
            else:
                for link_tag in soup.findAll('a', {'href': re.compile('imgurl=')}):
                    dirty_url = link_tag.get('href') # URL to an image as given by Google Images
                    dl_url = str(re.search(imgurl_re, dirty_url).group()) # The direct URL to the image
                    img_urls.append(dl_url)


            for dl_url in img_urls:
                try:
                    add_img = Image(dl_url, verbose=False)

                    # Don't know a better way to check if the image was actually returned
                    if add_img.height <> 0 and add_img.width <> 0:
                        add_set.append(add_img)

                except:
                    #do nothing
                    None

                if len(add_set) >= number:
                    break

        self.extend(add_set)


    def upload(self,dest,api_key=None,api_secret=None, verbose = True):
        """
        
        **SUMMARY**
        
        Uploads all the images to imgur or flickr or dropbox. In verbose mode URL values are printed.
        
        
        **PARAMETERS**
        
        * *api_key* - a string of the API key.
        * *api_secret* - (required only for flickr and dropbox ) a string of the API secret.
        * *verbose* - If verbose is true all values are printed to the screen
        
        
        **RETURNS**
        
        if uploading is successful
        
        - Imgur return the original image URL on success and None if it fails.
        - Flick returns True on success, else returns False.
        - dropbox returns True on success.
        

        **EXAMPLE**
        
        TO upload image to imgur::
        
          >>> imgset = ImageSet("/home/user/Desktop")
          >>> result = imgset.upload( 'imgur',"MY_API_KEY1234567890" )
          >>> print "Uploaded To: " + result[0]
          

        To upload image to flickr::
        
          >>> imgset.upload('flickr','api_key','api_secret')
          >>> imgset.upload('flickr') #Once the api keys and secret keys are cached.

        To upload image to dropbox::
        
          >>> imgset.upload('dropbox','api_key','api_secret')
          >>> imgset.upload('dropbox') #Once the api keys and secret keys are cached.

        **NOTES**
        
        .. Warning::
          This method requires two packages to be installed
          -PyCurl
          -flickr api.
          -dropbox
        
        
        .. Warning::
          You must supply your own API key.


        Find more about API keys:
        
        - http://imgur.com/register/api_anon
        - http://www.flickr.com/services/api/misc.api_keys.html
        - https://www.dropbox.com/developers/start/setup#python
        
        
        """
        try :
            for i in self:
                i.upload(dest,api_key,api_secret, verbose)
            return True

        except :
            return False

    def show(self, showtime = 0.25):
        """
        **SUMMARY**

        This is a quick way to show all the items in a ImageSet.
        The time is in seconds. You can also provide a decimal value, so
        showtime can be 1.5, 0.02, etc.
        to show each image.

        **PARAMETERS**

        * *showtime* - the time, in seconds, to show each image in the set.

        **RETURNS**

        Nothing.

        **EXAMPLE**

        >>> imgs = ImageSet()
        >>> imgs.download("ninjas")
        >>> imgs.show()


       """

        for i in self:
            i.show()
            time.sleep(showtime)

    def _get_app_ext(self, loops=0):
        """ Application extention. Part that secifies amount of loops.
        if loops is 0, if goes on infinitely.
        """
        bb = "\x21\xFF\x0B"  # application extension
        bb += "NETSCAPE2.0"
        bb += "\x03\x01"
        if loops == 0:
            loops = 2**16-1
        bb += int_to_bin(loops)
        bb += '\x00'  # end
        return bb

    def _get_graphics_control_ext(self, duration=0.1):
        """ Graphics Control Extension. A sort of header at the start of
        each image. Specifies transparancy and duration. """
        bb = '\x21\xF9\x04'
        bb += '\x08'  # no transparency
        bb += int_to_bin( int(duration*100) ) # in 100th of seconds
        bb += '\x00'  # no transparent color
        bb += '\x00'  # end
        return bb

    def _write_gif(self, filename, duration=0.1, loops=0, dither=1):
        """ Given a set of images writes the bytes to the specified stream.
        """
        frames = 0
        previous = None
        fp = open(filename, 'wb')

        if not PIL_ENABLED:
            logger.warning("Need PIL to write animated gif files.")
            return

        converted = []

        for img in self:
            if not isinstance(img,pil.Image):
                pil_img = img.getPIL()
            else:
                pil_img = img

            converted.append((pil_img.convert('P',dither=dither), img._get_header_anim()))

        try:
            for img, header_anim in converted:
                if not previous:
                    # gather data
                    palette = getheader(img)[1]
                    data = getdata(img)
                    imdes, data = data[0], data[1:]
                    header = header_anim
                    appext = self._get_app_ext(loops)
                    graphext = self._get_graphics_control_ext(duration)

                    # write global header
                    fp.write(header)
                    fp.write(palette)
                    fp.write(appext)

                    # write image
                    fp.write(graphext)
                    fp.write(imdes)
                    for d in data:
                        fp.write(d)

                else:
                    # gather info (compress difference)
                    data = getdata(img)
                    imdes, data = data[0], data[1:]
                    graphext = self._get_graphics_control_ext(duration)

                    # write image
                    fp.write(graphext)
                    fp.write(imdes)
                    for d in data:
                        fp.write(d)

                previous = img.copy()
                frames = frames + 1

            fp.write(";") # end gif

        finally:
            fp.close()
            return frames

    def save(self, destination=None, dt=0.2, verbose = False, displaytype=None):
        """
        
        **SUMMARY**

        This is a quick way to save all the images in a data set.
        Or to Display in webInterface.

        If you didn't specify a path one will randomly be generated.
        To see the location the files are being saved to then pass
        verbose = True.

        **PARAMETERS**

        * *destination* - path to which images should be saved, or name of gif
        * file. If this ends in .gif, the pictures will be saved accordingly.
        * *dt* - time between frames, for creating gif files.
        * *verbose* - print the path of the saved files to the console.
        * *displaytype* - the method use for saving or displaying images.
        
        
        valid values are:

        * 'notebook' - display to the ipython notebook.
        * None - save to a temporary file.

        **RETURNS**

        Nothing.

        **EXAMPLE**

        >>> imgs = ImageSet()
        >>> imgs.download("ninjas")
        >>> imgs.save(destination="ninjas_folder", verbose=True)

        >>> imgs.save(destination="ninjas.gif", verbose=True)

        """
        if displaytype=='notebook':
            try:
                from IPython.core.display import Image as IPImage
            except ImportError:
                print "You need IPython Notebooks to use this display mode"
                return
            from IPython.core import display as Idisplay
            for i in self:
                tf = tempfile.NamedTemporaryFile(suffix=".png")
                loc = tf.name
                tf.close()
                i.save(loc)
                Idisplay.display(IPImage(filename=loc))
                return
        else:
            if destination:
                if destination.endswith(".gif"):
                    return self._write_gif(destination, dt)
                else:
                    for i in self:
                        i.save(path=destination, temp=True, verbose=verbose)
            else:
                for i in self:
                    i.save(verbose=verbose)

    def showPaths(self):
        """
        **SUMMARY**

        This shows the file paths of all the images in the set.

        If they haven't been saved to disk then they will not have a filepath


        **RETURNS**

        Nothing.

        **EXAMPLE**

        >>> imgs = ImageSet()
        >>> imgs.download("ninjas")
        >>> imgs.save(verbose=True)
        >>> imgs.showPaths()


        **TO DO**

        This should return paths as a list too.

        """

        for i in self:
            print i.filename

    def _read_gif(self, filename):
        """ read_gif(filename)

        Reads images from an animated GIF file. Returns the number of images loaded.
        """

        if not PIL_ENABLED:
            return
        elif not os.path.isfile(filename):
            return

        pil_img = pil.open(filename)
        pil_img.seek(0)

        pil_images = []
        try:
            while True:
                pil_images.append(pil_img.copy())
                pil_img.seek(pil_img.tell()+1)

        except EOFError:
            pass

        loaded = 0
        for img in pil_images:
            self.append(Image(img))
            loaded += 1

        return loaded

    def load(self, directory = None, extension = None, sort_by=None):
        """
        **SUMMARY**

        This function loads up files automatically from the directory you pass
        it.  If you give it an extension it will only load that extension
        otherwise it will try to load all know file types in that directory.

        extension should be in the format:
        extension = 'png'

        **PARAMETERS**

        * *directory* - The path or directory from which to load images.
        * *extension* - The extension to use. If none is given png is the default.
        * *sort_by* - Sort the directory based on one of the following parameters passed as strings.
          * *time* - the modification time of the file.
          * *name* - the name of the file.
          * *size* - the size of the file.

          The default behavior is to leave the directory unsorted.

        **RETURNS**

        The number of images in the image set.

        **EXAMPLE**

        >>> imgs = ImageSet()
        >>> imgs.load("images/faces")
        >>> imgs.load("images/eyes", "png")

        """
        if not directory:
            logger.warning("You need to give a directory to load files from.")
            return

        if not os.path.exists(directory):
            logger.warning( "Invalid image path given.")
            return


        if extension:
            #regexes to ignore case
            regexList = [ '[' + letter + letter.upper() + ']' for letter in extension]
            regex = ''.join(regexList)
            regex = "*." + regex
            formats = [os.path.join(directory, regex)]

        else:
            formats = [os.path.join(directory, x) for x in IMAGE_FORMATS]


        file_set = [glob.glob(p) for p in formats]
        full_set = []
        for f in file_set:
            for i in f:
                full_set.append(i)

        file_set = full_set
        if(sort_by is not None):
            if( sort_by.lower() == "time"):
                file_set = sorted(file_set,key=os.path.getmtime)
            if( sort_by.lower() == "name"):
                file_set = sorted(file_set)
            if( sort_by.lower() == "size"):
                file_set = sorted(file_set,key=os.path.getsize)

        self.filelist = dict()

        for i in file_set:
            tmp = None
            try:
                tmp = Image(i)
                if( tmp is not None and tmp.width > 0 and tmp.height > 0):
                    if sys.platform.lower() == 'win32' or sys.platform.lower() == 'win64':
                        self.filelist[tmp.filename.split('\\')[-1]] = tmp
                    else:
                        self.filelist[tmp.filename.split('/')[-1]] = tmp
                    self.append(tmp)
            except:
                continue
        return len(self)

    def standardize(self,width,height):
        """
        **SUMMARY**

        Resize every image in the set to a standard size.

        **PARAMETERS**

        * *width* - the width that we want for every image in the set.
        * *height* - the height that we want for every image in the set.

        **RETURNS**

        A new image set where every image in the set is scaled to the desired size.

        **EXAMPLE**

        >>>> iset = ImageSet("./b/")
        >>>> thumbnails = iset.standardize(64,64)
        >>>> for t in thumbnails:
        >>>>   t.show()

        """
        retVal = ImageSet()
        for i in self:
            retVal.append(i.resize(width,height))
        return retVal

    def dimensions(self):
        """
        **SUMMARY**

        Return an np.array that are the width and height of every image in the image set.

        **PARAMETERS**

        --NONE--

        **RETURNS**
        A 2xN numpy array where N is the number of images in the set. The first column is
        the width, and the second collumn is the height.

        **EXAMPLE**
        >>> iset = ImageSet("./b/")
        >>> sz = iset.dimensions()
        >>> np.max(sz[:,0]) # returns the largest width in the set.

        """
        retVal = []
        for i in self:
            retVal.append((i.width,i.height))
        return np.array(retVal)

    def average(self, mode="first", size=(None,None)):
        """
        **SUMMARY**

        Casts each in the image set into a 32F image, averages them together and returns the results.
        If the images are different sizes the method attempts to standarize them.

        **PARAMETERS**

        * *mode* -
          * "first" - resize everything to the size of the first image.
          * "max" - resize everything to be the max width and max height of the set.
          * "min" - resize everything to be the min width and min height of the set.
          * "average" - resize everything to be the average width and height of the set
          * "fixed" - fixed, use the size tuple provided.

        * *size* - if the mode is set to fixed use this tuple as the size of the resulting image.

        **RETURNS**

        Returns a single image that is the average of all the values.

        **EXAMPLE**

        >>> imgs = ImageSet()
        >>> imgs.load("images/faces")
        >>> result = imgs.average(mode="first")
        >>> result.show()

        **TODO**
        * Allow the user to pass in an offset parameters that blit the images into the resutl.
        """
        fw = 0
        fh = 0
        # figger out how we will handle everything
        if( len(self) <= 0 ):
            return ImageSet()

        vals = self.dimensions()
        if( mode.lower()  == "first" ):
            fw = self[0].width
            fh = self[0].height
        elif( mode.lower()  == "fixed" ):
            fw = size[0]
            fh = size[1]
        elif( mode.lower()  == "max" ):
            fw = np.max(vals[:,0])
            fh = np.max(vals[:,1])
        elif( mode.lower()  == "min" ):
            fw = np.min(vals[:,0])
            fh = np.min(vals[:,1])
        elif( mode.lower()  == "average" ):
            fw = int(np.average(vals[:,0]))
            fh = int(np.average(vals[:,1]))
        #determine if we really need to resize the images
        t1 = np.sum(vals[:,0]-fw)
        t2 = np.sum(vals[:,1]-fh)
        if( t1 != 0 or t2 != 0 ):
            resized = self.standardize(fw,fh)
        else:
            resized = self
        # Now do the average calculation
        accumulator = cv.CreateImage((fw,fh), cv.IPL_DEPTH_8U,3)
        cv.Zero(accumulator)
        alpha = float(1.0/len(resized))
        beta = float((len(resized)-1.0)/len(resized))
        for i in resized:
            cv.AddWeighted(i.getBitmap(),alpha,accumulator,beta,0,accumulator)
        retVal =  Image(accumulator)
        return retVal


    def __getitem__(self,key):
        """
        **SUMMARY**

        Returns a ImageSet when sliced. Previously used to
        return list. Now it is possible to ImageSet member
        functions on sub-lists

        """
        if type(key) is types.SliceType: #Or can use 'try:' for speed
            return ImageSet(list.__getitem__(self, key))
        else:
            return list.__getitem__(self,key)

    def __getslice__(self, i, j):
        """
        Deprecated since python 2.0, now using __getitem__
        """
        return self.__getitem__(slice(i,j))


class Image:
    """
    **SUMMARY**

    The Image class is the heart of SimpleCV and allows you to convert to and
    from a number of source types with ease.  It also has intelligent buffer
    management, so that modified copies of the Image required for algorithms
    such as edge detection, etc can be cached and reused when appropriate.


    Image are converted into 8-bit, 3-channel images in RGB colorspace.  It will
    automatically handle conversion from other representations into this
    standard format.  If dimensions are passed, an empty image is created.

    **EXAMPLE**

    >>> i = Image("/path/to/image.png")
    >>> i = Camera().getImage()


    You can also just load the SimpleCV logo using:

    >>> img = Image("simplecv")
    >>> img = Image("logo")
    >>> img = Image("logo_inverted")
    >>> img = Image("logo_transparent")

    Or you can load an image from a URL:

    >>> img = Image("http://www.simplecv.org/image.png")

    """

    width = 0    #width and height in px
    height = 0
    depth = 0
    filename = "" #source filename
    filehandle = "" #filehandle if used
    camera = ""
    _mLayers = []

    _mDoHuePalette = False
    _mPaletteBins = None
    _mPalette = None
    _mPaletteMembers = None
    _mPalettePercentages = None

    _barcodeReader = "" #property for the ZXing barcode reader


    #these are buffer frames for various operations on the image
    _bitmap = ""  #the bitmap (iplimage)  representation of the image
    _matrix = ""  #the matrix (cvmat) representation
    _grayMatrix = "" #the gray scale (cvmat) representation -KAS
    _graybitmap = ""  #a reusable 8-bit grayscale bitmap
    _equalizedgraybitmap = "" #the above bitmap, normalized
    _blobLabel = ""  #the label image for blobbing
    _edgeMap = "" #holding reference for edge map
    _cannyparam = (0, 0) #parameters that created _edgeMap
    _pil = "" #holds a PIL object in buffer
    _numpy = "" #numpy form buffer
    _grayNumpy = "" # grayscale numpy for keypoint stuff
    _colorSpace = ColorSpace.UNKNOWN #Colorspace Object
    _pgsurface = ""
    _cv2Numpy = None #numpy array for OpenCV >= 2.3
    _cv2GrayNumpy = None #grayscale numpy array for OpenCV >= 2.3
    _gridLayer = [None,[0,0]]#to store grid details | Format -> [gridIndex , gridDimensions]

    #For DFT Caching
    _DFT = [] #an array of 2 channel (real,imaginary) 64F images

    #Keypoint caching values
    _mKeyPoints = None
    _mKPDescriptors = None
    _mKPFlavor = "NONE"

    #temp files
    _tempFiles = []

    #when we empty the buffers, populate with this:
    _initialized_buffers = {
        "_bitmap": "",
        "_matrix": "",
        "_grayMatrix": "",
        "_graybitmap": "",
        "_equalizedgraybitmap": "",
        "_blobLabel": "",
        "_edgeMap": "",
        "_cannyparam": (0, 0),
        "_pil": "",
        "_numpy": "",
        "_grayNumpy":"",
        "_pgsurface": "",
        "_cv2GrayNumpy": "",
        "_cv2Numpy":""}

    #The variables _uncroppedX and _uncroppedY are used to buffer the points when we crop the image.
    _uncroppedX = 0
    _uncroppedY = 0

    def __repr__(self):
        if len(self.filename) == 0:
            fn = "None"
        else:
            fn = self.filename
        return "<SimpleCV.Image Object size:(%d, %d), filename: (%s), at memory location: (%s)>" % (self.width, self.height, fn, hex(id(self)))


    #initialize the frame
    #parameters: source designation (filename)
    #todo: handle camera/capture from file cases (detect on file extension)
    def __init__(self, source = None, camera = None, colorSpace = ColorSpace.UNKNOWN,verbose=True, sample=False, cv2image=False, webp=False):
        """
        **SUMMARY**

        The constructor takes a single polymorphic parameter, which it tests
        to see how it should convert into an RGB image.  Supported types include:

        **PARAMETERS**

        * *source* - The source of the image. This can be just about anything, a numpy arrray, a file name, a width and height
          tuple, a url. Certain strings such as "lenna" or "logo" are loaded automatically for quick testing.

        * *camera* - A camera to pull a live image.

        * *colorspace* - A default camera color space. If none is specified this will usually default to the BGR colorspace.

        * *sample* - This is set to true if you want to load some of the included sample images without having to specify the complete path


        **EXAMPLES**

        >>> img = Image('simplecv')
        >>> img = Image('test.png')
        >>> img = Image('http://www.website.com/my_image.jpg')
        >>> img.show()

        **NOTES**

        OpenCV: iplImage and cvMat types
        Python Image Library: Image type
        Filename: All opencv supported types (jpg, png, bmp, gif, etc)
        URL: The source can be a url, but must include the http://

        """
        self._mLayers = []
        self.camera = camera
        self._colorSpace = colorSpace
        #Keypoint Descriptors
        self._mKeyPoints = []
        self._mKPDescriptors = []
        self._mKPFlavor = "NONE"
        #Pallete Stuff
        self._mDoHuePalette = False
        self._mPaletteBins = None
        self._mPalette = None
        self._mPaletteMembers = None
        self._mPalettePercentages = None
        #Temp files
        self._tempFiles = []


        #Check if need to load from URL
        #(this can be made shorter)if type(source) == str and (source[:7].lower() == "http://" or source[:8].lower() == "https://"):
        if isinstance(source, basestring) and (source.lower().startswith("http://") or source.lower().startswith("https://")):
            #try:
            # added spoofed user agent for images that are blocking bots (like wikipedia)
            req = urllib2.Request(source, headers={'User-Agent' : "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_4) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.54 Safari/536.5"})
            img_file = urllib2.urlopen(req)
            #except:
            #if verbose:
                #print "Couldn't open Image from URL:" + source
            #return None

            im = StringIO(img_file.read())
            source = pil.open(im).convert("RGB")
            
        #Check if loaded from base64 URI
        if isinstance(source, basestring) and (source.lower().startswith("data:image/png;base64,")):
            img = source[22:].decode("base64")
            im = StringIO(img)
            source = pil.open(im).convert("RGB")

        #This section loads custom built-in images
        if isinstance(source, basestring):
            tmpname = source.lower()

            if tmpname == "simplecv" or tmpname == "logo":
                imgpth = os.path.join(LAUNCH_PATH, 'sampleimages','simplecv.png')
                source = imgpth
            elif tmpname == "simplecv_inverted" or tmpname == "inverted" or tmpname == "logo_inverted":
                imgpth = os.path.join(LAUNCH_PATH, 'sampleimages','simplecv_inverted.png')
                source = imgpth
            elif tmpname == "lenna":
                imgpth = os.path.join(LAUNCH_PATH, 'sampleimages','lenna.png')
                source = imgpth
            elif tmpname == "lyle":
                imgpth = os.path.join(LAUNCH_PATH, 'sampleimages','LyleJune1973.png')
                source = imgpth
            elif tmpname == "parity":
                choice = random.choice(['LyleJune1973.png','lenna.png'])
                imgpth = os.path.join(LAUNCH_PATH, 'sampleimages',choice)
                source = imgpth
                
            elif sample:
                imgpth = os.path.join(LAUNCH_PATH, 'sampleimages', source)
                source = imgpth

        if (type(source) == tuple):
            w = int(source[0])
            h = int(source[1])
            source = cv.CreateImage((w,h), cv.IPL_DEPTH_8U, 3)
            cv.Zero(source)
        if (type(source) == cv.cvmat):
            self._matrix = cv.CreateMat(source.rows, source.cols, cv.CV_8UC3)
            if((source.step/source.cols)==3): #this is just a guess
                cv.Copy(source, self._matrix, None)
                self._colorSpace = ColorSpace.BGR
            elif((source.step/source.cols)==1):
                cv.Merge(source, source, source, None, self._matrix)
                self._colorSpace = ColorSpace.GRAY 
            else:
                self._colorSpace = ColorSpace.UNKNOWN
                warnings.warn("Unable to process the provided cvmat") 


        elif (type(source) == np.ndarray):  #handle a numpy array conversion
            if (type(source[0, 0]) == np.ndarray): #we have a 3 channel array
                #convert to an iplimage bitmap
                source = source.astype(np.uint8)
                self._numpy = source
                if not cv2image:
                    invertedsource = source[:, :, ::-1].transpose([1, 0, 2])
                else:
                    # If the numpy array is from cv2, then it must not be transposed.
                    invertedsource = source

                #invertedsource = source[:, :, ::-1].transpose([1, 0, 2]) # do not un-comment. breaks cv2 image support
                self._bitmap = cv.CreateImageHeader((invertedsource.shape[1], invertedsource.shape[0]), cv.IPL_DEPTH_8U, 3)
                cv.SetData(self._bitmap, invertedsource.tostring(),
                    invertedsource.dtype.itemsize * 3 * invertedsource.shape[1])
                self._colorSpace = ColorSpace.BGR #this is an educated guess
            else:
                #we have a single channel array, convert to an RGB iplimage

                source = source.astype(np.uint8)
                if not cv2image:
                    source = source.transpose([1,0]) #we expect width/height but use col/row
                self._bitmap = cv.CreateImage((source.shape[1], source.shape[0]), cv.IPL_DEPTH_8U, 3)
                channel = cv.CreateImageHeader((source.shape[1], source.shape[0]), cv.IPL_DEPTH_8U, 1)
                #initialize an empty channel bitmap
                cv.SetData(channel, source.tostring(),
                    source.dtype.itemsize * source.shape[1])
                cv.Merge(channel, channel, channel, None, self._bitmap)
                self._colorSpace = ColorSpace.BGR


        elif (type(source) == cv.iplimage):
            if (source.nChannels == 1):
                self._bitmap = cv.CreateImage(cv.GetSize(source), source.depth, 3)
                cv.Merge(source, source, source, None, self._bitmap)
                self._colorSpace = ColorSpace.GRAY
            else:
                self._bitmap = cv.CreateImage(cv.GetSize(source), source.depth, 3)
                cv.Copy(source, self._bitmap, None) 
                self._colorSpace = ColorSpace.BGR
        elif (type(source) == type(str()) or source.__class__.__name__ == 'StringIO'):
            if source == '':
                raise IOError("No filename provided to Image constructor")

        
            elif webp or source.split('.')[-1] == 'webp':
                try:
                    if source.__class__.__name__ == 'StringIO':
                      source.seek(0) # set the stringIO to the begining
                    self._pil = pil.open(source)
                    self._bitmap = cv.CreateImageHeader(self._pil.size, cv.IPL_DEPTH_8U, 3)
                except:
                    try:
                        from webm import decode as webmDecode
                    except ImportError:
                        logger.warning('The webm module or latest PIL / PILLOW module needs to be installed to load webp files: https://github.com/sightmachine/python-webm')
                        return

                    WEBP_IMAGE_DATA = bytearray(file(source, "rb").read())
                    result = webmDecode.DecodeRGB(WEBP_IMAGE_DATA)
                    webpImage = pil.frombuffer(
                        "RGB", (result.width, result.height), str(result.bitmap),
                        "raw", "RGB", 0, 1
                    )
                    self._pil = webpImage.convert("RGB")
                    self._bitmap = cv.CreateImageHeader(self._pil.size, cv.IPL_DEPTH_8U, 3)
                    self.filename = source
                cv.SetData(self._bitmap, self._pil.tostring())
                cv.CvtColor(self._bitmap, self._bitmap, cv.CV_RGB2BGR)

            else:
                self.filename = source
                try:
                    self._bitmap = cv.LoadImage(self.filename, iscolor=cv.CV_LOAD_IMAGE_COLOR)
                except:
                    self._pil = pil.open(self.filename).convert("RGB")
                    self._bitmap = cv.CreateImageHeader(self._pil.size, cv.IPL_DEPTH_8U, 3)
                    cv.SetData(self._bitmap, self._pil.tostring())
                    cv.CvtColor(self._bitmap, self._bitmap, cv.CV_RGB2BGR)

                #TODO, on IOError fail back to PIL
                self._colorSpace = ColorSpace.BGR


        elif (type(source) == pg.Surface):
            self._pgsurface = source
            self._bitmap = cv.CreateImageHeader(self._pgsurface.get_size(), cv.IPL_DEPTH_8U, 3)
            cv.SetData(self._bitmap, pg.image.tostring(self._pgsurface, "RGB"))
            cv.CvtColor(self._bitmap, self._bitmap, cv.CV_RGB2BGR)
            self._colorSpace = ColorSpace.BGR


        elif (PIL_ENABLED and (
                (len(source.__class__.__bases__) and source.__class__.__bases__[0].__name__ == "ImageFile")
                or source.__class__.__name__ == "JpegImageFile"
                or source.__class__.__name__ == "WebPPImageFile"
                or  source.__class__.__name__ == "Image")):

            if source.mode != 'RGB':
                source = source.convert('RGB')
            self._pil = source
            #from the opencv cookbook
            #http://opencv.willowgarage.com/documentation/python/cookbook.html
            self._bitmap = cv.CreateImageHeader(self._pil.size, cv.IPL_DEPTH_8U, 3)
            cv.SetData(self._bitmap, self._pil.tostring())
            self._colorSpace = ColorSpace.BGR
            cv.CvtColor(self._bitmap, self._bitmap, cv.CV_RGB2BGR)
            #self._bitmap = cv.iplimage(self._bitmap)


        else:
            return None

        #if the caller passes in a colorspace we overide it
        if(colorSpace != ColorSpace.UNKNOWN):
            self._colorSpace = colorSpace


        bm = self.getBitmap()
        self.width = bm.width
        self.height = bm.height
        self.depth = bm.depth


    def __del__(self):
        """
        This is called when the instance is about to be destroyed also called a destructor.
        """
        try :
            for i in self._tempFiles:
                if (i[1]):
                    os.remove(i[0])
        except :
            pass

    def getEXIFData(self):
        """
        **SUMMARY**

        This function extracts the exif data from an image file like JPEG or TIFF. The data is returned as a dict.

        **RETURNS**

        A dictionary of key value pairs. The value pairs are defined in the EXIF.py file.

        **EXAMPLE**

        >>> img = Image("./SimpleCV/sampleimages/OWS.jpg")
        >>> data = img.getEXIFData()
        >>> data['Image GPSInfo'].values

        **NOTES**

        * Compliments of: http://exif-py.sourceforge.net/

        * See also: http://en.wikipedia.org/wiki/Exchangeable_image_file_format

        **See Also**

        :py:class:`EXIF`
        """
        import os, string
        if( len(self.filename) < 5 or self.filename is None ):
            #I am not going to warn, better of img sets
            #logger.warning("ImageClass.getEXIFData: This image did not come from a file, can't get EXIF data.")
            return {}

        fileName, fileExtension = os.path.splitext(self.filename)
        fileExtension = string.lower(fileExtension)
        if( fileExtension != '.jpeg' and fileExtension != '.jpg' and
            fileExtension != 'tiff' and fileExtension != '.tif'):
            #logger.warning("ImageClass.getEXIFData: This image format does not support EXIF")
            return {}

        raw = open(self.filename,'rb')
        data = process_file(raw)
        return data

    def live(self):
        """
        **SUMMARY**

        This shows a live view of the camera.
        * Left click will show mouse coordinates and color.
        * Right click will kill the live image.

        **RETURNS**

        Nothing. In place method.

        **EXAMPLE**

        >>> cam = Camera()
        >>> cam.live()

        """

        start_time = time.time()

        from SimpleCV.Display import Display
        i = self
        d = Display(i.size())
        i.save(d)
        col = Color.RED

        while d.isNotDone():
            i = self
            i.clearLayers()
            elapsed_time = time.time() - start_time


            if d.mouseLeft:
                txt = "coord: (" + str(d.mouseX) + "," + str(d.mouseY) + ")"
                i.dl().text(txt, (10,i.height / 2), color=col)
                txt = "color: " + str(i.getPixel(d.mouseX,d.mouseY))
                i.dl().text(txt, (10,(i.height / 2) + 10), color=col)
                print "coord: (" + str(d.mouseX) + "," + str(d.mouseY) + "), color: " + str(i.getPixel(d.mouseX,d.mouseY))


            if elapsed_time > 0 and elapsed_time < 5:

                i.dl().text("In live mode", (10,10), color=col)
                i.dl().text("Left click will show mouse coordinates and color", (10,20), color=col)
                i.dl().text("Right click will kill the live image", (10,30), color=col)


            i.save(d)
            if d.mouseRight:
                print "Closing Window"
                d.done = True


        pg.quit()

    def getColorSpace(self):
        """
        **SUMMARY**

        Returns the value matched in the color space class

        **RETURNS**

        Integer corresponding to the color space.

        **EXAMPLE**

        >>> if(image.getColorSpace() == ColorSpace.RGB)

        **SEE ALSO**

        :py:class:`ColorSpace`

        """
        return self._colorSpace


    def isRGB(self):
        """
        **SUMMARY**

        Returns true if this image uses the RGB colorspace.

        **RETURNS**

        True if the image uses the RGB colorspace, False otherwise.

        **EXAMPLE**

        >>> if( img.isRGB() ):
        >>>    r,g,b = img.splitChannels()

        **SEE ALSO**

        :py:meth:`toRGB`


        """
        return(self._colorSpace==ColorSpace.RGB)


    def isBGR(self):
        """
        **SUMMARY**

        Returns true if this image uses the BGR colorspace.

        **RETURNS**

        True if the image uses the BGR colorspace, False otherwise.

        **EXAMPLE**

        >>> if( img.isBGR() ):
        >>>    b,g,r = img.splitChannels()

        **SEE ALSO**

        :py:meth:`toBGR`

        """
        return(self._colorSpace==ColorSpace.BGR)


    def isHSV(self):
        """
        **SUMMARY**

        Returns true if this image uses the HSV colorspace.

        **RETURNS**

        True if the image uses the HSV colorspace, False otherwise.

        **EXAMPLE**

        >>> if( img.isHSV() ):
        >>>    h,s,v = img.splitChannels()

        **SEE ALSO**

        :py:meth:`toHSV`

        """
        return(self._colorSpace==ColorSpace.HSV)


    def isHLS(self):
        """
        **SUMMARY**

        Returns true if this image uses the HLS colorspace.

        **RETURNS**

        True if the image uses the HLS colorspace, False otherwise.

        **EXAMPLE**

        >>> if( img.isHLS() ):
        >>>    h,l,s = img.splitChannels()

        **SEE ALSO**

        :py:meth:`toHLS`

        """
        return(self._colorSpace==ColorSpace.HLS)


    def isXYZ(self):
        """
        **SUMMARY**

        Returns true if this image uses the XYZ colorspace.

        **RETURNS**

        True if the image uses the XYZ colorspace, False otherwise.

        **EXAMPLE**

        >>> if( img.isXYZ() ):
        >>>    x,y,z = img.splitChannels()

        **SEE ALSO**

        :py:meth:`toXYZ`

        """
        return(self._colorSpace==ColorSpace.XYZ)


    def isGray(self):
        """
        **SUMMARY**

        Returns true if this image uses the Gray colorspace.

        **RETURNS**

        True if the image uses the Gray colorspace, False otherwise.

        **EXAMPLE**

        >>> if( img.isGray() ):
        >>>    print "The image is in Grayscale."

        **SEE ALSO**

        :py:meth:`toGray`

        """
        return(self._colorSpace==ColorSpace.GRAY)

    def isYCrCb(self):
        """
        **SUMMARY**

        Returns true if this image uses the YCrCb colorspace.

        **RETURNS**

        True if the image uses the YCrCb colorspace, False otherwise.

        **EXAMPLE**

        >>> if( img.isYCrCb() ):
        >>>    Y,Cr,Cb = img.splitChannels()

        **SEE ALSO**

        :py:meth:`toYCrCb`

        """
        return(self._colorSpace==ColorSpace.YCrCb)

    def toRGB(self):
        """
        **SUMMARY**

        This method attemps to convert the image to the RGB colorspace.
        If the color space is unknown we assume it is in the BGR format

        **RETURNS**

        Returns the converted image if the conversion was successful,
        otherwise None is returned.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> RGBImg = img.toRGB()

        **SEE ALSO**

        :py:meth:`isRGB`

        """

        retVal = self.getEmpty()
        if( self._colorSpace == ColorSpace.BGR or
                self._colorSpace == ColorSpace.UNKNOWN ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_BGR2RGB)
        elif( self._colorSpace == ColorSpace.HSV ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_HSV2RGB)
        elif( self._colorSpace == ColorSpace.HLS ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_HLS2RGB)
        elif( self._colorSpace == ColorSpace.XYZ ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_XYZ2RGB)
        elif( self._colorSpace == ColorSpace.YCrCb ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_YCrCb2RGB)
        elif( self._colorSpace == ColorSpace.RGB ):
            retVal = self.getBitmap()
        else:
            logger.warning("Image.toRGB: There is no supported conversion to RGB colorspace")
            return None
        return Image(retVal, colorSpace=ColorSpace.RGB )


    def toBGR(self):
        """
        **SUMMARY**

        This method attemps to convert the image to the BGR colorspace.
        If the color space is unknown we assume it is in the BGR format.

        **RETURNS**

        Returns the converted image if the conversion was successful,
        otherwise None is returned.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> BGRImg = img.toBGR()

        **SEE ALSO**

        :py:meth:`isBGR`

        """
        retVal = self.getEmpty()
        if( self._colorSpace == ColorSpace.RGB or
                self._colorSpace == ColorSpace.UNKNOWN ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_RGB2BGR)
        elif( self._colorSpace == ColorSpace.HSV ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_HSV2BGR)
        elif( self._colorSpace == ColorSpace.HLS ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_HLS2BGR)
        elif( self._colorSpace == ColorSpace.XYZ ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_XYZ2BGR)
        elif( self._colorSpace == ColorSpace.YCrCb ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_YCrCb2BGR)
        elif( self._colorSpace == ColorSpace.BGR ):
            retVal = self.getBitmap()
        else:
            logger.warning("Image.toBGR: There is no supported conversion to BGR colorspace")
            return None
        return Image(retVal, colorSpace = ColorSpace.BGR )


    def toHLS(self):
        """
        **SUMMARY**

        This method attempts to convert the image to the HLS colorspace.
        If the color space is unknown we assume it is in the BGR format.

        **RETURNS**

        Returns the converted image if the conversion was successful,
        otherwise None is returned.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> HLSImg = img.toHLS()

        **SEE ALSO**

        :py:meth:`isHLS`

        """

        retVal = self.getEmpty()
        if( self._colorSpace == ColorSpace.BGR or
                self._colorSpace == ColorSpace.UNKNOWN ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_BGR2HLS)
        elif( self._colorSpace == ColorSpace.RGB):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_RGB2HLS)
        elif( self._colorSpace == ColorSpace.HSV ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_HSV2RGB)
            cv.CvtColor(retVal, retVal, cv.CV_RGB2HLS)
        elif( self._colorSpace == ColorSpace.XYZ ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_XYZ2RGB)
            cv.CvtColor(retVal, retVal, cv.CV_RGB2HLS)
        elif( self._colorSpace == ColorSpace.YCrCb ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_YCrCb2RGB)
            cv.CvtColor(retVal, retVal, cv.CV_RGB2HLS)
        elif( self._colorSpace == ColorSpace.HLS ):
            retVal = self.getBitmap()
        else:
            logger.warning("Image.toHSL: There is no supported conversion to HSL colorspace")
            return None
        return Image(retVal, colorSpace = ColorSpace.HLS )


    def toHSV(self):
        """
        **SUMMARY**

        This method attempts to convert the image to the HSV colorspace.
        If the color space is unknown we assume it is in the BGR format

        **RETURNS**

        Returns the converted image if the conversion was successful,
        otherwise None is returned.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> HSVImg = img.toHSV()

        **SEE ALSO**

        :py:meth:`isHSV`

        """
        retVal = self.getEmpty()
        if( self._colorSpace == ColorSpace.BGR or
                self._colorSpace == ColorSpace.UNKNOWN ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_BGR2HSV)
        elif( self._colorSpace == ColorSpace.RGB):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_RGB2HSV)
        elif( self._colorSpace == ColorSpace.HLS ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_HLS2RGB)
            cv.CvtColor(retVal, retVal, cv.CV_RGB2HSV)
        elif( self._colorSpace == ColorSpace.XYZ ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_XYZ2RGB)
            cv.CvtColor(retVal, retVal, cv.CV_RGB2HSV)
        elif( self._colorSpace == ColorSpace.YCrCb ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_YCrCb2RGB)
            cv.CvtColor(retVal, retVal, cv.CV_RGB2HSV)
        elif( self._colorSpace == ColorSpace.HSV ):
            retVal = self.getBitmap()
        else:
            logger.warning("Image.toHSV: There is no supported conversion to HSV colorspace")
            return None
        return Image(retVal, colorSpace = ColorSpace.HSV )


    def toXYZ(self):
        """
        **SUMMARY**

        This method attemps to convert the image to the XYZ colorspace.
        If the color space is unknown we assume it is in the BGR format

        **RETURNS**

        Returns the converted image if the conversion was successful,
        otherwise None is returned.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> XYZImg = img.toXYZ()

        **SEE ALSO**

        :py:meth:`isXYZ`

        """

        retVal = self.getEmpty()
        if( self._colorSpace == ColorSpace.BGR or
                self._colorSpace == ColorSpace.UNKNOWN ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_BGR2XYZ)
        elif( self._colorSpace == ColorSpace.RGB):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_RGB2XYZ)
        elif( self._colorSpace == ColorSpace.HLS ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_HLS2RGB)
            cv.CvtColor(retVal, retVal, cv.CV_RGB2XYZ)
        elif( self._colorSpace == ColorSpace.HSV ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_HSV2RGB)
            cv.CvtColor(retVal, retVal, cv.CV_RGB2XYZ)
        elif( self._colorSpace == ColorSpace.YCrCb ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_YCrCb2RGB)
            cv.CvtColor(retVal, retVal, cv.CV_RGB2XYZ)
        elif( self._colorSpace == ColorSpace.XYZ ):
            retVal = self.getBitmap()
        else:
            logger.warning("Image.toXYZ: There is no supported conversion to XYZ colorspace")
            return None
        return Image(retVal, colorSpace=ColorSpace.XYZ )


    def toGray(self):
        """
        **SUMMARY**

        This method attemps to convert the image to the grayscale colorspace.
        If the color space is unknown we assume it is in the BGR format.

        **RETURNS**

        A grayscale SimpleCV image if successful.
        otherwise None is returned.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> img.toGray().binarize().show()

        **SEE ALSO**

        :py:meth:`isGray`
        :py:meth:`binarize`

        """

        retVal = self.getEmpty(1)
        if( self._colorSpace == ColorSpace.BGR or
                self._colorSpace == ColorSpace.UNKNOWN ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_BGR2GRAY)
        elif( self._colorSpace == ColorSpace.RGB):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_RGB2GRAY)
        elif( self._colorSpace == ColorSpace.HLS ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_HLS2RGB)
            cv.CvtColor(retVal, retVal, cv.CV_RGB2GRAY)
        elif( self._colorSpace == ColorSpace.HSV ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_HSV2RGB)
            cv.CvtColor(retVal, retVal, cv.CV_RGB2GRAY)
        elif( self._colorSpace == ColorSpace.XYZ ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_XYZ2RGB)
            cv.CvtColor(retVal, retVal, cv.CV_RGB2GRAY)
        elif( self._colorSpace == ColorSpace.YCrCb ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_YCrCb2RGB)
            cv.CvtColor(retVal, retVal, cv.CV_RGB2GRAY)
        elif( self._colorSpace == ColorSpace.GRAY ):
            retVal = self.getBitmap()
        else:
            logger.warning("Image.toGray: There is no supported conversion to gray colorspace")
            return None
        return Image(retVal, colorSpace = ColorSpace.GRAY )

    def toYCrCb(self):
        """
        **SUMMARY**

        This method attemps to convert the image to the YCrCb colorspace.
        If the color space is unknown we assume it is in the BGR format

        **RETURNS**

        Returns the converted image if the conversion was successful,
        otherwise None is returned.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> RGBImg = img.toYCrCb()

        **SEE ALSO**

        :py:meth:`isYCrCb`

        """

        retVal = self.getEmpty()
        if( self._colorSpace == ColorSpace.BGR or
                self._colorSpace == ColorSpace.UNKNOWN ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_BGR2YCrCb)
        elif( self._colorSpace == ColorSpace.RGB ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_RGB2YCrCb)
        elif( self._colorSpace == ColorSpace.HSV ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_HSV2RGB)
            cv.CvtColor(retVal, retVal, cv.CV_RGB2YCrCb)
        elif( self._colorSpace == ColorSpace.HLS ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_HLS2RGB)
            cv.CvtColor(retVal, retVal, cv.CV_RGB2YCrCb)
        elif( self._colorSpace == ColorSpace.XYZ ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_XYZ2RGB)
            cv.CvtColor(retVal, retVal, cv.CV_RGB2YCrCb)
        elif( self._colorSpace == ColorSpace.YCrCb ):
            retVal = self.getBitmap()
        else:
            logger.warning("Image.toYCrCb: There is no supported conversion to YCrCb colorspace")
            return None
        return Image(retVal, colorSpace=ColorSpace.YCrCb )


    def getEmpty(self, channels=3):
        """
        **SUMMARY**

        Create a new, empty OpenCV bitmap with the specified number of channels (default 3).
        This method basically creates an empty copy of the image. This is handy for
        interfacing with OpenCV functions directly.

        **PARAMETERS**

        * *channels* - The number of channels in the returned OpenCV image.

        **RETURNS**

        Returns an black OpenCV IplImage that matches the width, height, and color
        depth of the source image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> rawImg  = img.getEmpty()
        >>> cv.SomeOpenCVFunc(img.getBitmap(),rawImg)

        **SEE ALSO**

        :py:meth:`getBitmap`
        :py:meth:`getFPMatrix`
        :py:meth:`getPIL`
        :py:meth:`getNumpy`
        :py:meth:`getGrayNumpy`
        :py:meth:`getGrayscaleMatrix`

        """
        bitmap = cv.CreateImage(self.size(), cv.IPL_DEPTH_8U, channels)
        cv.SetZero(bitmap)
        return bitmap


    def getBitmap(self):
        """
        **SUMMARY**

        Retrieve the bitmap (iplImage) of the Image.  This is useful if you want
        to use functions from OpenCV with SimpleCV's image class

        **RETURNS**

        Returns black OpenCV IplImage from this image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> rawImg  = img.getBitmap()
        >>> rawOut  = img.getEmpty()
        >>> cv.SomeOpenCVFunc(rawImg,rawOut)

        **SEE ALSO**

        :py:meth:`getEmpty`
        :py:meth:`getFPMatrix`
        :py:meth:`getPIL`
        :py:meth:`getNumpy`
        :py:meth:`getGrayNumpy`
        :py:meth:`getGrayscaleMatrix`

        """
        if (self._bitmap):
            return self._bitmap
        elif (self._matrix):
            self._bitmap = cv.GetImage(self._matrix)
        return self._bitmap


    def getMatrix(self):
        """
        **SUMMARY**

        Get the matrix (cvMat) version of the image, required for some OpenCV algorithms.

        **RETURNS**

        Returns the OpenCV CvMat version of this image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> rawImg  = img.getMatrix()
        >>> rawOut  = img.getEmpty()
        >>> cv.SomeOpenCVFunc(rawImg,rawOut)

        **SEE ALSO**

        :py:meth:`getEmpty`
        :py:meth:`getBitmap`
        :py:meth:`getFPMatrix`
        :py:meth:`getPIL`
        :py:meth:`getNumpy`
        :py:meth:`getGrayNumpy`
        :py:meth:`getGrayscaleMatrix`

        """
        if (self._matrix):
            return self._matrix
        else:
            self._matrix = cv.GetMat(self.getBitmap()) #convert the bitmap to a matrix
            return self._matrix


    def getFPMatrix(self):
        """
        **SUMMARY**

        Converts the standard int bitmap to a floating point bitmap.
        This is handy for some OpenCV functions.


        **RETURNS**

        Returns the floating point OpenCV CvMat version of this image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> rawImg  = img.getFPMatrix()
        >>> rawOut  = img.getEmpty()
        >>> cv.SomeOpenCVFunc(rawImg,rawOut)

        **SEE ALSO**

        :py:meth:`getEmpty`
        :py:meth:`getBitmap`
        :py:meth:`getMatrix`
        :py:meth:`getPIL`
        :py:meth:`getNumpy`
        :py:meth:`getGrayNumpy`
        :py:meth:`getGrayscaleMatrix`

        """
        retVal =  cv.CreateImage((self.width,self.height), cv.IPL_DEPTH_32F, 3)
        cv.Convert(self.getBitmap(),retVal)
        return retVal

    def getPIL(self):
        """
        **SUMMARY**

        Get a PIL Image object for use with the Python Image Library
        This is handy for some PIL functions.


        **RETURNS**

        Returns the Python Imaging Library (PIL) version of this image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> rawImg  = img.getPIL()


        **SEE ALSO**

        :py:meth:`getEmpty`
        :py:meth:`getBitmap`
        :py:meth:`getMatrix`
        :py:meth:`getFPMatrix`
        :py:meth:`getNumpy`
        :py:meth:`getGrayNumpy`
        :py:meth:`getGrayscaleMatrix`

        """
        if (not PIL_ENABLED):
            return None
        if (not self._pil):
            rgbbitmap = self.getEmpty()
            cv.CvtColor(self.getBitmap(), rgbbitmap, cv.CV_BGR2RGB)
            self._pil = pil.fromstring("RGB", self.size(), rgbbitmap.tostring())
        return self._pil


    def getGrayNumpy(self):
        """
        **SUMMARY**

        Return a grayscale Numpy array of the image.

        **RETURNS**

        Returns the image, converted first to grayscale and then converted to a 2D numpy array.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> rawImg  = img.getGrayNumpy()

        **SEE ALSO**

        :py:meth:`getEmpty`
        :py:meth:`getBitmap`
        :py:meth:`getMatrix`
        :py:meth:`getPIL`
        :py:meth:`getNumpy`
        :py:meth:`getGrayNumpy`
        :py:meth:`getGrayscaleMatrix`

        """
        if( self._grayNumpy != "" ):
            return self._grayNumpy
        else:
            self._grayNumpy = uint8(np.array(cv.GetMat(self._getGrayscaleBitmap())).transpose())

        return self._grayNumpy

    def getNumpy(self):
        """
        **SUMMARY**

        Get a Numpy array of the image in width x height x RGB dimensions

        **RETURNS**

        Returns the image, converted first to grayscale and then converted to a 3D numpy array.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> rawImg  = img.getNumpy()

        **SEE ALSO**

        :py:meth:`getEmpty`
        :py:meth:`getBitmap`
        :py:meth:`getMatrix`
        :py:meth:`getPIL`
        :py:meth:`getGrayNumpy`
        :py:meth:`getGrayscaleMatrix`

        """

        if self._numpy != "":
            return self._numpy


        self._numpy = np.array(self.getMatrix())[:, :, ::-1].transpose([1, 0, 2])
        return self._numpy

    def getNumpyCv2(self):
        """
        **SUMMARY**

        Get a Numpy array of the image in width x height x RGB dimensions compatible with OpenCV >= 2.3

        **RETURNS**

        Returns the  3D numpy array of the image compatible with OpenCV >= 2.3

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> rawImg  = img.getNumpyCv2()

        **SEE ALSO**

        :py:meth:`getEmpty`
        :py:meth:`getBitmap`
        :py:meth:`getMatrix`
        :py:meth:`getPIL`
        :py:meth:`getGrayNumpy`
        :py:meth:`getGrayscaleMatrix`
        :py:meth:`getNumpy`
        :py:meth:`getGrayNumpyCv2`

        """

        if type(self._cv2Numpy) is not np.ndarray:
            self._cv2Numpy = np.array(self.getMatrix())
        return self._cv2Numpy

    def getGrayNumpyCv2(self):
        """
        **SUMMARY**

        Get a Grayscale Numpy array of the image in width x height y compatible with OpenCV >= 2.3

        **RETURNS**

        Returns the grayscale numpy array compatible with OpenCV >= 2.3

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> rawImg  = img.getNumpyCv2()

        **SEE ALSO**

        :py:meth:`getEmpty`
        :py:meth:`getBitmap`
        :py:meth:`getMatrix`
        :py:meth:`getPIL`
        :py:meth:`getGrayNumpy`
        :py:meth:`getGrayscaleMatrix`
        :py:meth:`getNumpy`
        :py:meth:`getGrayNumpyCv2`

        """
        if type(self._cv2GrayNumpy) is not np.ndarray:
            self._cv2GrayNumpy = np.array(self.getGrayscaleMatrix())
        return self._cv2GrayNumpy

    def _getGrayscaleBitmap(self):
        if (self._graybitmap):
            return self._graybitmap


        self._graybitmap = self.getEmpty(1)
        temp = self.getEmpty(3)
        if( self._colorSpace == ColorSpace.BGR or
                self._colorSpace == ColorSpace.UNKNOWN ):
            cv.CvtColor(self.getBitmap(), self._graybitmap, cv.CV_BGR2GRAY)
        elif( self._colorSpace == ColorSpace.RGB):
            cv.CvtColor(self.getBitmap(), self._graybitmap, cv.CV_RGB2GRAY)
        elif( self._colorSpace == ColorSpace.HLS ):
            cv.CvtColor(self.getBitmap(), temp, cv.CV_HLS2RGB)
            cv.CvtColor(temp, self._graybitmap, cv.CV_RGB2GRAY)
        elif( self._colorSpace == ColorSpace.HSV ):
            cv.CvtColor(self.getBitmap(), temp, cv.CV_HSV2RGB)
            cv.CvtColor(temp, self._graybitmap, cv.CV_RGB2GRAY)
        elif( self._colorSpace == ColorSpace.XYZ ):
            cv.CvtColor(self.getBitmap(), retVal, cv.CV_XYZ2RGB)
            cv.CvtColor(temp, self._graybitmap, cv.CV_RGB2GRAY)
        elif( self._colorSpace == ColorSpace.GRAY):
            cv.Split(self.getBitmap(), self._graybitmap, self._graybitmap, self._graybitmap, None)
        else:
            logger.warning("Image._getGrayscaleBitmap: There is no supported conversion to gray colorspace")
            return None
        return self._graybitmap


    def getGrayscaleMatrix(self):
        """
        **SUMMARY**

        Get the grayscale matrix (cvMat) version of the image, required for some OpenCV algorithms.

        **RETURNS**

        Returns the OpenCV CvMat version of this image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> rawImg  = img.getGrayscaleMatrix()
        >>> rawOut  = img.getEmpty()
        >>> cv.SomeOpenCVFunc(rawImg,rawOut)

        **SEE ALSO**

        :py:meth:`getEmpty`
        :py:meth:`getBitmap`
        :py:meth:`getFPMatrix`
        :py:meth:`getPIL`
        :py:meth:`getNumpy`
        :py:meth:`getGrayNumpy`
        :py:meth:`getMatrix`

        """
        if (self._grayMatrix):
            return self._grayMatrix
        else:
            self._grayMatrix = cv.GetMat(self._getGrayscaleBitmap()) #convert the bitmap to a matrix
            return self._grayMatrix


    def _getEqualizedGrayscaleBitmap(self):
        if (self._equalizedgraybitmap):
            return self._equalizedgraybitmap


        self._equalizedgraybitmap = self.getEmpty(1)
        cv.EqualizeHist(self._getGrayscaleBitmap(), self._equalizedgraybitmap)


        return self._equalizedgraybitmap


    def equalize(self):
        """
        **SUMMARY**

        Perform a histogram equalization on the image.

        **RETURNS**

        Returns a grayscale SimpleCV image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> img = img.equalize()


        """
        return Image(self._getEqualizedGrayscaleBitmap())

    def getPGSurface(self):
        """
        **SUMMARY**

        Returns the image as a pygame surface.  This is used for rendering the display

        **RETURNS**

        A pygame surface object used for rendering.


        """
        if (self._pgsurface):
            return self._pgsurface
        else:
            if self.isGray():
                self._pgsurface = pg.image.fromstring(self.getBitmap().tostring(), self.size(), "RGB")
            else:
                self._pgsurface = pg.image.fromstring(self.toRGB().getBitmap().tostring(), self.size(), "RGB")
            return self._pgsurface

    def toString(self):
        """
        **SUMMARY**

        Returns the image as a string, useful for moving data around.


        **RETURNS**

        The image, converted to rgb, then converted to a string.

        """
        return self.toRGB().getBitmap().tostring()


    def save(self, filehandle_or_filename="", mode="", verbose=False, temp=False, path=None, filename=None, cleanTemp=False ,**params):
        """
        **SUMMARY**

        Save the image to the specified filename.  If no filename is provided then
        then it will use the filename the Image was loaded from or the last
        place it was saved to. You can save to lots of places, not just files.
        For example you can save to the Display, a JpegStream, VideoStream,
        temporary file, or Ipython Notebook.


        Save will implicitly render the image's layers before saving, but the layers are
        not applied to the Image itself.


        **PARAMETERS**

        * *filehandle_or_filename* - the filename to which to store the file. The method will infer the file type.

        * *mode* - This flag is used for saving using pul.

        * *verbose* - If this flag is true we return the path where we saved the file.

        * *temp* - If temp is True we save the image as a temporary file and return the path

        * *path* - path where temporary files needed to be stored

        * *filename* - name(Prefix) of the temporary file.

        * *cleanTemp* - This flag is made True if tempfiles are tobe deleted once the object is to be destroyed.

        * *params* - This object is used for overloading the PIL save methods. In particular
          this method is useful for setting the jpeg compression level. For JPG see this documentation:
          http://www.pythonware.com/library/pil/handbook/format-jpeg.htm

        **EXAMPLES**

        To save as a temporary file just use:

        >>> img = Image('simplecv')
        >>> img.save(temp=True)

        It will return the path that it saved to.

        Save also supports IPython Notebooks when passing it a Display object
        that has been instainted with the notebook flag.

        To do this just use::

          >>> disp = Display(displaytype='notebook')
          >>> img.save(disp)

        .. Note::
          You must have IPython notebooks installed for this to work path and filename are valid if and only if temp is set to True.

        .. attention::
          We need examples for all save methods as they are unintuitve.
        
        """
        #TODO, we use the term mode here when we mean format
        #TODO, if any params are passed, use PIL

        if temp :
            import glob
            if filename == None :
                filename = 'Image'
            if path == None :
                path=tempfile.gettempdir()
            if glob.os.path.exists(path):
                path = glob.os.path.abspath(path)
                imagefiles = glob.glob(glob.os.path.join(path,filename+"*.png"))
                num = [0]
                for img in imagefiles :
                    num.append(int(glob.re.findall('[0-9]+$',img[:-4])[-1]))
                num.sort()
                fnum = num[-1]+1
                filename = glob.os.path.join(path,filename+("%07d" % fnum)+".png")
                self._tempFiles.append((filename,cleanTemp))
                self.save(self._tempFiles[-1][0])
                return self._tempFiles[-1][0]
            else :
                print "Path does not exist!"

        else :
            if (filename) :
                filehandle_or_filename = filename + ".png"

        if (not filehandle_or_filename):
            if (self.filename):
                filehandle_or_filename = self.filename
            else:
                filehandle_or_filename = self.filehandle

        if (len(self._mLayers)):
            saveimg = self.applyLayers()
        else:
            saveimg = self

        if self._colorSpace != ColorSpace.BGR and self._colorSpace != ColorSpace.GRAY:
            saveimg = saveimg.toBGR()
        
        if not isinstance(filehandle_or_filename, basestring):
            
            fh = filehandle_or_filename

            if (not PIL_ENABLED):
                logger.warning("You need the python image library to save by filehandle")
                return 0


            if (type(fh) == InstanceType and fh.__class__.__name__ == "JpegStreamer"):
                fh.jpgdata = StringIO()
                saveimg.getPIL().save(fh.jpgdata, "jpeg", **params) #save via PIL to a StringIO handle
                fh.refreshtime = time.time()
                self.filename = ""
                self.filehandle = fh


            elif (type(fh) == InstanceType and fh.__class__.__name__ == "VideoStream"):
                self.filename = ""
                self.filehandle = fh
                fh.writeFrame(saveimg)


            elif (type(fh) == InstanceType and fh.__class__.__name__ == "Display"):

                if fh.displaytype == 'notebook':
                    try:
                        from IPython.core.display import Image as IPImage
                    except ImportError:
                        print "You need IPython Notebooks to use this display mode"
                        return

                    from IPython.core import display as Idisplay
                    tf = tempfile.NamedTemporaryFile(suffix=".png")
                    loc = tf.name
                    tf.close()
                    self.save(loc)
                    Idisplay.display(IPImage(filename=loc))
                    return
                else:
                    #self.filename = ""
                    self.filehandle = fh
                    fh.writeFrame(saveimg)


            else:
                if (not mode):
                    mode = "jpeg"

                try:
                  saveimg.getPIL().save(fh, mode, **params) # The latest version of PIL / PILLOW supports webp, try this first, if not gracefully fallback
                  self.filehandle = fh #set the filename for future save operations
                  self.filename = ""
                  return 1
                except Exception, e:
                  if mode.lower() != 'webp':
                    raise e
                

            if verbose:
                print self.filename

            if not mode.lower() == 'webp':
                return 1

        #make a temporary file location if there isn't one
        if not filehandle_or_filename:
            filename = tempfile.mkstemp(suffix=".png")[-1]
        else:
            filename = filehandle_or_filename
        
        #allow saving in webp format
        if mode == 'webp' or re.search('\.webp$', filename):
            try:
                #newer versions of PIL support webp format, try that first
                self.getPIL().save(filename, **params)
            except:
                #if PIL doesn't support it, maybe we have the python-webm library
                try:
                    from webm import encode as webmEncode
                    from webm.handlers import BitmapHandler, WebPHandler
                except:
                    logger.warning('You need the webm library to save to webp format. You can download from: https://github.com/sightmachine/python-webm')
                    return 0

                #PNG_BITMAP_DATA = bytearray(Image.open(PNG_IMAGE_FILE).tostring())
                PNG_BITMAP_DATA = bytearray(self.toString())
                IMAGE_WIDTH = self.width
                IMAGE_HEIGHT = self.height


                image = BitmapHandler(
                    PNG_BITMAP_DATA, BitmapHandler.RGB,
                    IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_WIDTH * 3
                )
                result = webmEncode.EncodeRGB(image)
                
                if filehandle_or_filename.__class__.__name__ == 'StringIO':
                  filehandle_or_filename.write(result.data)
                else:
                  file(filename.format("RGB"), "wb").write(result.data)
                return 1
        #if the user is passing kwargs use the PIL save method.
        if( params ): #usually this is just the compression rate for the image
            if (not mode):
                mode = "jpeg"
            saveimg.getPIL().save(filename, mode, **params)
            return 1

        if (filename):
            cv.SaveImage(filename, saveimg.getBitmap())
            self.filename = filename #set the filename for future save operations
            self.filehandle = ""
        elif (self.filename):
            cv.SaveImage(self.filename, saveimg.getBitmap())
        else:
            return 0

        if verbose:
            print self.filename

        if temp:
            return filename
        else:
            return 1


    def copy(self):
        """
        **SUMMARY**

        Return a full copy of the Image's bitmap.  Note that this is different
        from using python's implicit copy function in that only the bitmap itself
        is copied. This method essentially performs a deep copy.

        **RETURNS**

        A copy of this SimpleCV image.

        **EXAMPLE**

        >>> img = Image("logo")
        >>> img2 = img.copy()

        """
        newimg = self.getEmpty()
        cv.Copy(self.getBitmap(), newimg)
        return Image(newimg, colorSpace=self._colorSpace)

    def upload(self,dest,api_key=None,api_secret=None, verbose = True):
        """
        **SUMMARY**
        
        Uploads image to imgur or flickr or dropbox. In verbose mode URL values are printed.

        **PARAMETERS**
        
        * *api_key* - a string of the API key.
        * *api_secret* (required only for flickr and dropbox ) - a string of the API secret.
        * *verbose* - If verbose is true all values are printed to the screen
        

        **RETURNS**
        
        if uploading is successful
        
        - Imgur return the original image URL on success and None if it fails.
        - Flick returns True on success, else returns False.
        - dropbox returns True on success.
        

        **EXAMPLE**
        
        TO upload image to imgur::
          
          >>> img = Image("lenna")
          >>> result = img.upload( 'imgur',"MY_API_KEY1234567890" )
          >>> print "Uploaded To: " + result[0]
          

        To upload image to flickr::
          
          >>> img.upload('flickr','api_key','api_secret')
          >>> img.invert().upload('flickr') #Once the api keys and secret keys are cached.
          

        To upload image to dropbox::
          
          >>> img.upload('dropbox','api_key','api_secret')
          >>> img.invert().upload('dropbox') #Once the api keys and secret keys are cached.
          

        **NOTES**
        
        .. Warning::
          This method requires two packages to be installed
          
          - PyCurl
          - flickr api.
          - dropbox
          

        .. Warning::
          You must supply your own API key. See here:
          
          - http://imgur.com/register/api_anon
          - http://www.flickr.com/services/api/misc.api_keys.html
          - https://www.dropbox.com/developers/start/setup#python
          
        """
        if ( dest=='imgur' ) :
            try:
                import pycurl
            except ImportError:
                print "PycURL Library not installed."
                return

            response = StringIO()
            c = pycurl.Curl()
            values = [("key", api_key),
                      ("image", (c.FORM_FILE, self.filename))]
            c.setopt(c.URL, "http://api.imgur.com/2/upload.xml")
            c.setopt(c.HTTPPOST, values)
            c.setopt(c.WRITEFUNCTION, response.write)
            c.perform()
            c.close()

            match = re.search(r'<hash>(\w+).*?<deletehash>(\w+).*?<original>(http://[\w.]+/[\w.]+)', response.getvalue() , re.DOTALL)
            if match:
                if(verbose):
                    print "Imgur page: http://imgur.com/" + match.group(1)
                    print "Original image: " + match.group(3)
                    print "Delete page: http://imgur.com/delete/" + match.group(2)
                return [match.group(1),match.group(3),match.group(2)]
            else :
                if(verbose):
                    print "The API Key given is not valid"
                return None

        elif (dest=='flickr'):
            global temp_token
            flickr = None
            try :
                import flickrapi
            except ImportError:
                print "Flickr API is not installed. Please install it from http://pypi.python.org/pypi/flickrapi"
                return False
            try :
                if (not(api_key==None and api_secret==None)):
                    self.flickr = flickrapi.FlickrAPI(api_key,api_secret,cache=True)
                    self.flickr.cache = flickrapi.SimpleCache(timeout=3600, max_entries=200)
                    self.flickr.authenticate_console('write')
                    temp_token = (api_key,api_secret)
                else :
                    try :
                        self.flickr = flickrapi.FlickrAPI(temp_token[0],temp_token[1],cache=True)
                        self.flickr.authenticate_console('write')
                    except NameError :
                        print "API key and Secret key are not set."
                        return
            except :
                print "The API Key and Secret Key are not valid"
                return False
            if (self.filename) :
                try :
                    self.flickr.upload(self.filename,self.filehandle)
                except :
                    print "Uploading Failed !"
                    return False
            else :
                tf = self.save(temp=True)
                self.flickr.upload(tf,"Image")
            return True

        elif (dest=='dropbox'):
            global dropbox_token
            access_type = 'dropbox'
            try :
                from dropbox import client, rest, session
                import webbrowser
            except ImportError:
                print "Dropbox API is not installed. For more info refer : https://www.dropbox.com/developers/start/setup#python "
                return False
            try :
                if ( 'dropbox_token' not in globals() and api_key!=None and api_secret!=None ):
                    sess = session.DropboxSession(api_key, api_secret, access_type)
                    request_token = sess.obtain_request_token()
                    url = sess.build_authorize_url(request_token)
                    webbrowser.open(url)
                    print "Please visit this website and press the 'Allow' button, then hit 'Enter' here."
                    raw_input()
                    access_token = sess.obtain_access_token(request_token)
                    dropbox_token = client.DropboxClient(sess)
                else :
                    if (dropbox_token) :
                        pass
                    else :
                        return None
            except :
                print "The API Key and Secret Key are not valid"
                return False
            if (self.filename) :
                try :
                    f = open(self.filename)
                    dropbox_token.put_file('/SimpleCVImages/'+os.path.split(self.filename)[-1], f)
                except :
                    print "Uploading Failed !"
                    return False
            else :
                tf = self.save(temp=True)
                f = open(tf)
                dropbox_token.put_file('/SimpleCVImages/'+'Image', f)
                return True

    def scale(self, width, height = -1, interpolation=cv2.INTER_LINEAR):
        """
        **SUMMARY**

        Scale the image to a new width and height.

        If no height is provided, the width is considered a scaling value.

        **PARAMETERS**

        * *width* - either the new width in pixels, if the height parameter is > 0, or if this value
          is a floating point value, this is the scaling factor.

        * *height* - the new height in pixels.

        * *interpolation* - how to generate new pixels that don't match the original pixels. Argument goes direction to cv.Resize. See http://docs.opencv.org/modules/imgproc/doc/geometric_transformations.html?highlight=resize#cv2.resize for more details

        **RETURNS**

        The resized image.

        **EXAMPLE**

        >>> img.scale(200, 100) #scales the image to 200px x 100px
        >>> img.scale(2.0) #enlarges the image to 2x its current size


        .. Warning::
          The two value scale command is deprecated. To set width and height
          use the resize function.

        :py:meth:`resize`

        """
        w, h = width, height
        if height == -1:
            w = int(self.width * width)
            h = int(self.height * width)
            if( w > MAX_DIMENSION or h > MAX_DIMENSION or h < 1 or w < 1 ):
                logger.warning("Holy Heck! You tried to make an image really big or impossibly small. I can't scale that")
                return self

        scaledArray = np.zeros((w,h,3),dtype='uint8')
        retVal = cv2.resize(self.getNumpyCv2(), (w,h), interpolation = interpolation)
        return Image(retVal, colorSpace=self._colorSpace,cv2image = True)


    def resize(self, w=None,h=None):
        """
        **SUMMARY**

        This method resizes an image based on a width, a height, or both.
        If either width or height is not provided the value is inferred by keeping the aspect ratio.
        If both values are provided then the image is resized accordingly.

        **PARAMETERS**

        * *width* - The width of the output image in pixels.

        * *height* - The height of the output image in pixels.

        **RETURNS**

        Returns a resized image, if the size is invalid a warning is issued and
        None is returned.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> img2 = img.resize(w=1024) # h is guessed from w
        >>> img3 = img.resize(h=1024) # w is guessed from h
        >>> img4 = img.resize(w=200,h=100)

        """
        retVal = None
        if( w is None and h is None ):
            logger.warning("Image.resize has no parameters. No operation is performed")
            return None
        elif( w is not None and h is None):
            sfactor = float(w)/float(self.width)
            h = int( sfactor*float(self.height) )
        elif( w is None and h is not None):
            sfactor = float(h)/float(self.height)
            w = int( sfactor*float(self.width) )
        if( w > MAX_DIMENSION or h > MAX_DIMENSION ):
            logger.warning("Image.resize Holy Heck! You tried to make an image really big or impossibly small. I can't scale that")
            return retVal
        scaled_bitmap = cv.CreateImage((w, h), 8, 3)
        cv.Resize(self.getBitmap(), scaled_bitmap)
        return Image(scaled_bitmap, colorSpace=self._colorSpace)


    def smooth(self, algorithm_name='gaussian', aperture=(3,3), sigma=0, spatial_sigma=0, grayscale=False, aperature=None):
        """
        **SUMMARY**

        Smooth the image, by default with the Gaussian blur.  If desired,
        additional algorithms and apertures can be specified.  Optional parameters
        are passed directly to OpenCV's cv.Smooth() function.

        If grayscale is true the smoothing operation is only performed on a single channel
        otherwise the operation is performed on each channel of the image.

        for OpenCV versions >= 2.3.0 it is advisible to take a look at
               - :py:meth:`bilateralFilter`
               - :py:meth:`medianFilter`
               - :py:meth:`blur`
               - :py:meth:`gaussianBlur`

        **PARAMETERS**

        * *algorithm_name* - valid options are 'blur' or gaussian, 'bilateral', and 'median'.

          * `Median Filter <http://en.wikipedia.org/wiki/Median_filter>`_

          * `Gaussian Blur <http://en.wikipedia.org/wiki/Gaussian_blur>`_

          * `Bilateral Filter <http://en.wikipedia.org/wiki/Bilateral_filter>`_

        * *aperture* - A tuple for the aperture of the gaussian blur as an (x,y) tuple.
                     - Note there was rampant spelling mistakes in both smooth & sobel,
                       aperture is spelled as such, and not "aperature". This code is backwards
                       compatible.

        .. Warning::
          These must be odd numbers.

        * *sigma* -

        * *spatial_sigma* -

        * *grayscale* - Return just the grayscale image.



        **RETURNS**

        The smoothed image.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> img2 = img.smooth()
        >>> img3 = img.smooth('median')

        **SEE ALSO**

        :py:meth:`bilateralFilter`
        :py:meth:`medianFilter`
        :py:meth:`blur`

        """
        # see comment on argument documentation (spelling error)
        aperture = aperature if aperature else aperture

        if is_tuple(aperture):
            win_x, win_y = aperture
            if win_x <= 0 or win_y <= 0 or win_x % 2 == 0 or win_y % 2 == 0:
                logger.warning("The aperture (x,y) must be odd number and greater than 0.")
                return None
        else:
            raise ValueError("Please provide a tuple to aperture, got: %s" % type(aperture))


        #gauss and blur can work in-place, others need a buffer frame
        #use a string to ID rather than the openCV constant
        if algorithm_name == "blur":
            algorithm = cv.CV_BLUR
        elif algorithm_name == "bilateral":
            algorithm = cv.CV_BILATERAL
            win_y = win_x #aperture must be square
        elif algorithm_name == "median":
            algorithm = cv.CV_MEDIAN
            win_y = win_x #aperture must be square
        else:
            algorithm = cv.CV_GAUSSIAN #default algorithm is gaussian

        if grayscale:
            newimg = self.getEmpty(1)
            cv.Smooth(self._getGrayscaleBitmap(), newimg, algorithm, win_x, win_y, sigma, spatial_sigma)
        else:
            newimg = self.getEmpty(3)
            r = self.getEmpty(1)
            g = self.getEmpty(1)
            b = self.getEmpty(1)
            ro = self.getEmpty(1)
            go = self.getEmpty(1)
            bo = self.getEmpty(1)
            cv.Split(self.getBitmap(), b, g, r, None)
            cv.Smooth(r, ro, algorithm, win_x, win_y, sigma, spatial_sigma)
            cv.Smooth(g, go, algorithm, win_x, win_y, sigma, spatial_sigma)
            cv.Smooth(b, bo, algorithm, win_x, win_y, sigma, spatial_sigma)
            cv.Merge(bo,go,ro, None, newimg)

        return Image(newimg, colorSpace=self._colorSpace)


    def medianFilter(self, window='',grayscale=False):
        """
        **SUMMARY**

        Smooths the image, with the median filter. Performs a median filtering operation to denoise/despeckle the image.
        The optional parameter is the window size.
        see : http://en.wikipedia.org/wiki/Median_filter

        **Parameters**
        
        * *window* - should be in the form a tuple (win_x,win_y). Where win_x should be equal to win_y. By default it is set to 3x3, i.e window = (3x3).
        
        **Note**
        
        win_x and win_y should be greater than zero, a odd number and equal.

        For OpenCV versions <= 2.3.0
        this acts as Convience function derived from the :py:meth:`smooth` method. Which internally calls cv.Smooth

        For OpenCV versions >= 2.3.0
        cv2.medianBlur function is called.
        
        """
        try:
            import cv2
            new_version = True
        except :
            new_version = False
            pass


        if is_tuple(window):
            win_x, win_y = window
            if ( win_x>=0 and win_y>=0 and win_x%2==1 and win_y%2==1 ) :
                if win_x != win_y :
                    win_x=win_y
            else :
                logger.warning("The aperture (win_x,win_y) must be odd number and greater than 0.")
                return None

        elif( is_number(window) ):
            win_x = window
        else :
            win_x = 3 #set the default aperture window size (3x3)

        if ( not new_version ) :
            grayscale_ = grayscale
            return self.smooth(algorithm_name='median', aperture=(win_x,win_y),grayscale=grayscale_)
        else :
            if (grayscale) :
                img_medianBlur = cv2.medianBlur(self.getGrayNumpy(),win_x)
                return Image(img_medianBlur, colorSpace=ColorSpace.GRAY)
            else :
                img_medianBlur = cv2.medianBlur(self.getNumpy()[:,:, ::-1].transpose([1,0,2]),win_x)
                img_medianBlur = img_medianBlur[:,:, ::-1].transpose([1,0,2])
                return Image(img_medianBlur, colorSpace=self._colorSpace)


    def bilateralFilter(self, diameter=5,sigmaColor=10, sigmaSpace=10,grayscale=False):
        """
        **SUMMARY**

        Smooths the image, using bilateral filtering. Potential of bilateral filtering is for the removal of texture.
        The optional parameter are diameter, sigmaColor, sigmaSpace.

        Bilateral Filter
        see : http://en.wikipedia.org/wiki/Bilateral_filter
        see : http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html

        **Parameters**

        * *diameter* - A tuple for the window of the form (diameter,diameter). By default window = (3x3). ( for OpenCV versions <= 2.3.0)
                     - Diameter of each pixel neighborhood that is used during filtering. ( for OpenCV versions >= 2.3.0)


        * *sigmaColor* - Filter the specified value in the color space. A larger value of the parameter means that farther colors within the pixel neighborhood (see sigmaSpace ) will be mixed together, resulting in larger areas of semi-equal color.

        * *sigmaSpace* - Filter the specified value in the coordinate space. A larger value of the parameter means that farther pixels will influence each other as long as their colors are close enough

        **NOTE**
        For OpenCV versions <= 2.3.0
        -- this acts as Convience function derived from the :py:meth:`smooth` method. Which internally calls cv.Smooth.
        -- where aperture(window) is (diameter,diameter)
        -- sigmaColor and sigmanSpace become obsolete

        For OpenCV versions higher than 2.3.0. i.e >= 2.3.0
        -- cv.bilateralFilter function is called
        -- If the sigmaColor and sigmaSpace values are small (< 10), the filter will not have much effect, whereas if they are large (> 150), they will have a very strong effect, making the image look 'cartoonish'
        -- It is recommended to use diamter=5 for real time applications, and perhaps diameter=9 for offile applications that needs heavy noise filtering.
        """
        try:
            import cv2
            new_version = True
        except :
            new_version = False
            pass

        if is_tuple(diameter):
            win_x, win_y = diameter
            if ( win_x>=0 and win_y>=0 and win_x%2==1 and win_y%2==1 ) :
                if win_x != win_y :
                    diameter = (win_x, win_y)
            else :
                logger.warning("The aperture (win_x,win_y) must be odd number and greater than 0.")
                return None

        elif( is_number(diameter) ):
            pass

        else :
            win_x = 3 #set the default aperture window size (3x3)
            diameter = (win_x,win_x)

        if ( not new_version ) :
            grayscale_ = grayscale
            if( is_number(diameter) ) :
                diameter = (diameter,diameter)
            return self.smooth(algorithm_name='bilateral', aperture=diameter,grayscale=grayscale_)
        else :
            if (grayscale) :
                img_bilateral = cv2.bilateralFilter(self.getGrayNumpy(),diameter,sigmaColor, sigmaSpace)
                return Image(img_bilateral, colorSpace=ColorSpace.GRAY)
            else :
                img_bilateral = cv2.bilateralFilter(self.getNumpy()[:,:, ::-1].transpose([1,0,2]),diameter,sigmaColor, sigmaSpace)
                img_bilateral = img_bilateral[:,:, ::-1].transpose([1,0,2])
                return Image(img_bilateral,colorSpace=self._colorSpace)

    def blur(self, window = '', grayscale=False):
        """
        **SUMMARY**

        Smoothes an image using the normalized box filter.
        The optional parameter is window.

        see : http://en.wikipedia.org/wiki/Blur

        **Parameters**

        * *window* - should be in the form a tuple (win_x,win_y).
                   - By default it is set to 3x3, i.e window = (3x3).

        **NOTE**
        For OpenCV versions <= 2.3.0
        -- this acts as Convience function derived from the :py:meth:`smooth` method. Which internally calls cv.Smooth

        For OpenCV versions higher than 2.3.0. i.e >= 2.3.0
        -- cv.blur function is called
        """
        try:
            import cv2
            new_version = True
        except :
            new_version = False
            pass

        if is_tuple(window):
            win_x, win_y = window
            if ( win_x<=0 or win_y<=0 ) :
                logger.warning("win_x and win_y should be greater than 0.")
                return None
        elif( is_number(window) ):
            window = (window,window)
        else :
            window = (3,3)

        if ( not new_version ) :
            grayscale_ = grayscale
            return self.smooth(algorithm_name='blur', aperture=window, grayscale=grayscale_)
        else :
            if grayscale:
                img_blur = cv2.blur(self.getGrayNumpy(),window)
                return Image(img_blur,colorSpace=ColorSpace.GRAY)
            else :
                img_blur = cv2.blur(self.getNumpy()[:,:, ::-1].transpose([1,0,2]),window)
                img_blur = img_blur[:,:, ::-1].transpose([1,0,2])
                return Image(img_blur,colorSpace=self._colorSpace)

    def gaussianBlur(self, window = '', sigmaX=0 , sigmaY=0 ,grayscale=False):
        """
        **SUMMARY**

        Smoothes an image, typically used to reduce image noise and reduce detail.
        The optional parameter is window.

        see : http://en.wikipedia.org/wiki/Gaussian_blur

        **Parameters**

        * *window* - should be in the form a tuple (win_x,win_y). Where win_x and win_y should be positive and odd.
                   - By default it is set to 3x3, i.e window = (3x3).

        * *sigmaX* - Gaussian kernel standard deviation in X direction.

        * *sigmaY* - Gaussian kernel standard deviation in Y direction.

        * *grayscale* - If true, the effect is applied on grayscale images.

        **NOTE**
        For OpenCV versions <= 2.3.0
        -- this acts as Convience function derived from the :py:meth:`smooth` method. Which internally calls cv.Smooth

        For OpenCV versions higher than 2.3.0. i.e >= 2.3.0
        -- cv.GaussianBlur function is called
        """
        try:
            import cv2
            ver = cv2.__version__
            new_version = False
            #For OpenCV versions till 2.4.0,  cv2.__versions__ are of the form "$Rev: 4557 $"
            if not ver.startswith('$Rev:'):
                if int(ver.replace('.','0'))>=20300 :
                    new_version = True
        except :
            new_version = False
            pass

        if is_tuple(window):
            win_x, win_y = window
            if ( win_x>=0 and win_y>=0 and win_x%2==1 and win_y%2==1 ) :
                pass
            else :
                logger.warning("The aperture (win_x,win_y) must be odd number and greater than 0.")
                return None

        elif (is_number(window)):
            window = (window, window)

        else:
            window = (3,3) #set the default aperture window size (3x3)

        if (not new_version):
            grayscale_ = grayscale
            return self.smooth(algorithm_name='blur', aperture=window, grayscale=grayscale_)
        else:
            image_gauss = cv2.GaussianBlur(self.getNumpyCv2(), window, sigmaX, sigmaY=sigmaY)

            if grayscale:
                return Image(image_gauss, colorSpace=ColorSpace.GRAY, cv2image=True)
            else:
                return Image(image_gauss, colorSpace=self._colorSpace, cv2image=True)

    def invert(self):
        """
        **SUMMARY**

        Invert (negative) the image note that this can also be done with the
        unary minus (-) operator. For binary image this turns black into white and white into black (i.e. white is the new black).

        **RETURNS**

        The opposite of the current image.

        **EXAMPLE**

        >>> img  = Image("polar_bear_in_the_snow.png")
        >>> img.invert().save("black_bear_at_night.png")

        **SEE ALSO**

        :py:meth:`binarize`

        """
        return -self


    def grayscale(self):
        """
        **SUMMARY**

        This method returns a gray scale version of the image. It makes everything look like an old movie.

        **RETURNS**

        A grayscale SimpleCV image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> img.grayscale().binarize().show()

        **SEE ALSO**

        :py:meth:`binarize`
        """
        return Image(self._getGrayscaleBitmap(), colorSpace = ColorSpace.GRAY)


    def flipHorizontal(self):
        """
        **SUMMARY**

        Horizontally mirror an image.


        .. Warning::
          Note that flip does not mean rotate 180 degrees! The two are different.

        **RETURNS**

        The flipped SimpleCV image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> upsidedown = img.flipHorizontal()


        **SEE ALSO**

        :py:meth:`flipVertical`
        :py:meth:`rotate`

        """
        newimg = self.getEmpty()
        cv.Flip(self.getBitmap(), newimg, 1)
        return Image(newimg, colorSpace=self._colorSpace)

    def flipVertical(self):
        """
        **SUMMARY**

        Vertically mirror an image.


        .. Warning::
          Note that flip does not mean rotate 180 degrees! The two are different.

        **RETURNS**

        The flipped SimpleCV image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> upsidedown = img.flipHorizontal()


        **SEE ALSO**

        :py:meth:`rotate`
        :py:meth:`flipHorizontal`

        """

        newimg = self.getEmpty()
        cv.Flip(self.getBitmap(), newimg, 0)
        return Image(newimg, colorSpace=self._colorSpace)


    def stretch(self, thresh_low = 0, thresh_high = 255):
        """
        **SUMMARY**

        The stretch filter works on a greyscale image, if the image
        is color, it returns a greyscale image.  The filter works by
        taking in a lower and upper threshold.  Anything below the lower
        threshold is pushed to black (0) and anything above the upper
        threshold is pushed to white (255)

        **PARAMETERS**

        * *thresh_low* - The lower threshold for the stretch operation.
          This should be a value between 0 and 255.

        * *thresh_high* - The upper threshold for the stretch operation.
          This should be a value between 0 and 255.

        **RETURNS**

        A gray scale version of the image with the appropriate histogram stretching.


        **EXAMPLE**

        >>> img = Image("orson_welles.jpg")
        >>> img2 = img.stretch(56.200)
        >>> img2.show()

        **NOTES**

        TODO - make this work on RGB images with thresholds for each channel.

        **SEE ALSO**

        :py:meth:`binarize`
        :py:meth:`equalize`

        """
        try:
            newimg = self.getEmpty(1)
            cv.Threshold(self._getGrayscaleBitmap(), newimg, thresh_low, 255, cv.CV_THRESH_TOZERO)
            cv.Not(newimg, newimg)
            cv.Threshold(newimg, newimg, 255 - thresh_high, 255, cv.CV_THRESH_TOZERO)
            cv.Not(newimg, newimg)
            return Image(newimg)
        except:
            return None

    def gammaCorrect(self, gamma = 1):
        
        """
        **DESCRIPTION**

        Transforms an image according to Gamma Correction also known as 
        Power Law Transform.
        
        **PARAMETERS**

        * *gamma* - A non-negative real number.

        **RETURNS**

        A Gamma corrected image.

        **EXAMPLE**

        >>> img = Image('SimpleCV/sampleimages/family_watching_television_1958.jpg')
        >>> img.show()
        >>> img.gammaCorrect(1.5).show()
        >>> img.gammaCorrect(0.7).show()
 
        """
        if gamma < 0:
            return "Gamma should be a non-negative real number"
        scale = 255.0
        src = self.getNumpy()
        dst = (((1.0/scale)*src)**gamma)*scale
        return Image(dst)

    def binarize(self, thresh = -1, maxv = 255, blocksize = 0, p = 5):
        """
        **SUMMARY**

        Do a binary threshold the image, changing all values below thresh to maxv
        and all above to black.  If a color tuple is provided, each color channel
        is thresholded separately.


        If threshold is -1 (default), an adaptive method (OTSU's method) is used.
        If then a blocksize is specified, a moving average over each region of block*block
        pixels a threshold is applied where threshold = local_mean - p.

        **PARAMETERS**

        * *thresh* - the threshold as an integer or an (r,g,b) tuple , where pixels below (darker) than thresh are set to to max value,
          and all values above this value are set to black. If this parameter is -1 we use Otsu's method.

        * *maxv* - The maximum value for pixels below the threshold. Ordinarily this should be 255 (white)

        * *blocksize* - the size of the block used in the adaptive binarize operation.

        .. Warning::
          This parameter must be an odd number.

        * *p* - The difference from the local mean to use for thresholding in Otsu's method.

        **RETURNS**

        A binary (two colors, usually black and white) SimpleCV image. This works great for the findBlobs
        family of functions.

        **EXAMPLE**

        Example of a vanila threshold versus an adaptive threshold:

        >>> img = Image("orson_welles.jpg")
        >>> b1 = img.binarize(128)
        >>> b2 = img.binarize(blocksize=11,p=7)
        >>> b3 = b1.sideBySide(b2)
        >>> b3.show()


        **NOTES**

        `Otsu's Method Description<http://en.wikipedia.org/wiki/Otsu's_method>`

        **SEE ALSO**

        :py:meth:`threshold`
        :py:meth:`findBlobs`
        :py:meth:`invert`
        :py:meth:`dilate`
        :py:meth:`erode`

        """
        if is_tuple(thresh):
            r = self.getEmpty(1)
            g = self.getEmpty(1)
            b = self.getEmpty(1)
            cv.Split(self.getBitmap(), b, g, r, None)


            cv.Threshold(r, r, thresh[0], maxv, cv.CV_THRESH_BINARY_INV)
            cv.Threshold(g, g, thresh[1], maxv, cv.CV_THRESH_BINARY_INV)
            cv.Threshold(b, b, thresh[2], maxv, cv.CV_THRESH_BINARY_INV)


            cv.Add(r, g, r)
            cv.Add(r, b, r)


            return Image(r, colorSpace=self._colorSpace)


        elif thresh == -1:
            newbitmap = self.getEmpty(1)
            if blocksize:
                cv.AdaptiveThreshold(self._getGrayscaleBitmap(), newbitmap, maxv,
                    cv.CV_ADAPTIVE_THRESH_GAUSSIAN_C, cv.CV_THRESH_BINARY_INV, blocksize, p)
            else:
                cv.Threshold(self._getGrayscaleBitmap(), newbitmap, thresh, float(maxv), cv.CV_THRESH_BINARY_INV + cv.CV_THRESH_OTSU)
            return Image(newbitmap, colorSpace=self._colorSpace)
        else:
            newbitmap = self.getEmpty(1)
            #desaturate the image, and apply the new threshold
            cv.Threshold(self._getGrayscaleBitmap(), newbitmap, thresh, float(maxv), cv.CV_THRESH_BINARY_INV)
            return Image(newbitmap, colorSpace=self._colorSpace)




    def meanColor(self, colorSpace = None):
        """
        **SUMMARY**

        This method finds the average color of all the pixels in the image and displays tuple in the colorspace specfied by the user.
        If no colorspace is specified , (B,G,R) colorspace is taken as default.

        **RETURNS**

        A tuple of the average image values. Tuples are in the channel order. *For most images this means the results are (B,G,R).*

        **EXAMPLE**

        >>> img = Image('lenna')
        >>> colors = img.meanColor()        # returns tuple in Image's colorspace format.
        >>> colors = img.meanColor('BGR')   # returns tuple in (B,G,R) format.
        >>> colors = img.meanColor('RGB')   # returns tuple in (R,G,B) format.
        >>> colors = img.meanColor('HSV')   # returns tuple in (H,S,V) format.
        >>> colors = img.meanColor('XYZ')   # returns tuple in (X,Y,Z) format.
        >>> colors = img.meanColor('Gray')  # returns float of mean intensity.
        >>> colors = img.meanColor('YCrCb') # returns tuple in (Y,Cr,Cb) format.
        >>> colors = img.meanColor('HLS')   # returns tuple in (H,L,S) format.
        
         
        """
        
        if colorSpace == None:
			return tuple(cv.Avg(self.getBitmap())[0:3]) 
			
        elif colorSpace == 'BGR':
            return tuple(cv.Avg(self.toBGR().getBitmap())[0:3])
        
        elif colorSpace == 'RGB':
            return tuple(cv.Avg(self.toRGB().getBitmap())[0:3])
        
        elif colorSpace == 'HSV':
            return tuple(cv.Avg(self.toHSV().getBitmap())[0:3])

        elif colorSpace == 'XYZ':
            return tuple(cv.Avg(self.toXYZ().getBitmap())[0:3])

        elif colorSpace == 'Gray':
            return (cv.Avg(self._getGrayscaleBitmap())[0])

        elif colorSpace == 'YCrCb':
            return tuple(cv.Avg(self.toYCrCb().getBitmap())[0:3])

        elif colorSpace == 'HLS':
            return tuple(cv.Avg(self.toHLS().getBitmap())[0:3])

        else:
            logger.warning("Image.meanColor: There is no supported conversion to the specified colorspace. Use one of these as argument: 'BGR' , 'RGB' , 'HSV' , 'Gray' , 'XYZ' , 'YCrCb' , 'HLS' .")
            return None
			
        

    def findCorners(self, maxnum = 50, minquality = 0.04, mindistance = 1.0):
        """
        **SUMMARY**

        This will find corner Feature objects and return them as a FeatureSet
        strongest corners first.  The parameters give the number of corners to look
        for, the minimum quality of the corner feature, and the minimum distance
        between corners.

        **PARAMETERS**

        * *maxnum* - The maximum number of corners to return.

        * *minquality* - The minimum quality metric. This shoudl be a number between zero and one.

        * *mindistance* - The minimum distance, in pixels, between successive corners.

        **RETURNS**

        A featureset of :py:class:`Corner` features or None if no corners are found.


        **EXAMPLE**

        Standard Test:

        >>> img = Image("sampleimages/simplecv.png")
        >>> corners = img.findCorners()
        >>> if corners: True

        True

        Validation Test:

        >>> img = Image("sampleimages/black.png")
        >>> corners = img.findCorners()
        >>> if not corners: True

        True

        **SEE ALSO**

        :py:class:`Corner`
        :py:meth:`findKeypoints`

        """
        #initialize buffer frames
        eig_image = cv.CreateImage(cv.GetSize(self.getBitmap()), cv.IPL_DEPTH_32F, 1)
        temp_image = cv.CreateImage(cv.GetSize(self.getBitmap()), cv.IPL_DEPTH_32F, 1)


        corner_coordinates = cv.GoodFeaturesToTrack(self._getGrayscaleBitmap(), eig_image, temp_image, maxnum, minquality, mindistance, None)


        corner_features = []
        for (x, y) in corner_coordinates:
            corner_features.append(Corner(self, x, y))


        return FeatureSet(corner_features)


    def findBlobs(self, threshval = -1, minsize=10, maxsize=0, threshblocksize=0, threshconstant=5,appx_level=3):
        """
        
        **SUMMARY**

        Find blobs  will look for continuous
        light regions and return them as Blob features in a FeatureSet.  Parameters
        specify the binarize filter threshold value, and minimum and maximum size for blobs.
        If a threshold value is -1, it will use an adaptive threshold.  See binarize() for
        more information about thresholding.  The threshblocksize and threshconstant
        parameters are only used for adaptive threshold.


        **PARAMETERS**

        * *threshval* - the threshold as an integer or an (r,g,b) tuple , where pixels below (darker) than thresh are set to to max value,
          and all values above this value are set to black. If this parameter is -1 we use Otsu's method.

        * *minsize* - the minimum size of the blobs, in pixels, of the returned blobs. This helps to filter out noise.

        * *maxsize* - the maximim size of the blobs, in pixels, of the returned blobs.

        * *threshblocksize* - the size of the block used in the adaptive binarize operation. *TODO - make this match binarize*

        * *appx_level* - The blob approximation level - an integer for the maximum distance between the true edge and the
          approximation edge - lower numbers yield better approximation.

          .. warning::
            This parameter must be an odd number.

        * *threshconstant* - The difference from the local mean to use for thresholding in Otsu's method. *TODO - make this match binarize*


        **RETURNS**

        Returns a featureset (basically a list) of :py:class:`blob` features. If no blobs are found this method returns None.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> fs = img.findBlobs()
        >>> if( fs is not None ):
        >>>     fs.draw()

        **NOTES**

        .. Warning::
          For blobs that live right on the edge of the image OpenCV reports the position and width
          height as being one over for the true position. E.g. if a blob is at (0,0) OpenCV reports
          its position as (1,1). Likewise the width and height for the other corners is reported as
          being one less than the width and height. This is a known bug.

        **SEE ALSO**
        :py:meth:`threshold`
        :py:meth:`binarize`
        :py:meth:`invert`
        :py:meth:`dilate`
        :py:meth:`erode`
        :py:meth:`findBlobsFromPalette`
        :py:meth:`smartFindBlobs`
        """
        if (maxsize == 0):
            maxsize = self.width * self.height
        #create a single channel image, thresholded to parameters

        blobmaker = BlobMaker()
        blobs = blobmaker.extractFromBinary(self.binarize(threshval, 255, threshblocksize, threshconstant).invert(),
            self, minsize = minsize, maxsize = maxsize,appx_level=appx_level)

        if not len(blobs):
            return None

        return FeatureSet(blobs).sortArea()

    def findSkintoneBlobs(self, minsize=10, maxsize=0,dilate_iter=1):
        """
        **SUMMARY**

        Find Skintone blobs will look for continuous
        regions of Skintone in a color image and return them as Blob features in a FeatureSet.
        Parameters specify the binarize filter threshold value, and minimum and maximum size for
        blobs. If a threshold value is -1, it will use an adaptive threshold.  See binarize() for
        more information about thresholding.  The threshblocksize and threshconstant
        parameters are only used for adaptive threshold.


        **PARAMETERS**

        * *minsize* - the minimum size of the blobs, in pixels, of the returned blobs. This helps to filter out noise.

        * *maxsize* - the maximim size of the blobs, in pixels, of the returned blobs.

        * *dilate_iter* - the number of times to run the dilation operation.

        **RETURNS**

        Returns a featureset (basically a list) of :py:class:`blob` features. If no blobs are found this method returns None.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> fs = img.findSkintoneBlobs()
        >>> if( fs is not None ):
        >>>     fs.draw()

        **NOTES**
        It will be really awesome for making UI type stuff, where you want to track a hand or a face.

        **SEE ALSO**
        :py:meth:`threshold`
        :py:meth:`binarize`
        :py:meth:`invert`
        :py:meth:`dilate`
        :py:meth:`erode`
        :py:meth:`findBlobsFromPalette`
        :py:meth:`smartFindBlobs`
        """
        if (maxsize == 0):
            maxsize = self.width * self.height
        mask = self.getSkintoneMask(dilate_iter)
        blobmaker = BlobMaker()
        blobs = blobmaker.extractFromBinary(mask, self, minsize = minsize, maxsize = maxsize)
        if not len(blobs):
            return None
        return FeatureSet(blobs).sortArea()

    def getSkintoneMask(self, dilate_iter=0):
        """
        **SUMMARY**

        Find Skintone mask will look for continuous
        regions of Skintone in a color image and return a binary mask where the white pixels denote Skintone region.

        **PARAMETERS**

        * *dilate_iter* - the number of times to run the dilation operation.
        
        
        **RETURNS**

        Returns a binary mask.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> mask = img.findSkintoneMask()
        >>> mask.show()

        """
        if( self._colorSpace != ColorSpace.YCrCb ):
            YCrCb = self.toYCrCb()
        else:
            YCrCb = self

        Y =  np.ones((256,1),dtype=uint8)*0
        Y[5:] = 255
        Cr =  np.ones((256,1),dtype=uint8)*0
        Cr[140:180] = 255
        Cb =  np.ones((256,1),dtype=uint8)*0
        Cb[77:135] = 255
        Y_img = YCrCb.getEmpty(1)
        Cr_img = YCrCb.getEmpty(1)
        Cb_img = YCrCb.getEmpty(1)
        cv.Split(YCrCb.getBitmap(),Y_img,Cr_img,Cb_img,None)
        cv.LUT(Y_img,Y_img,cv.fromarray(Y))
        cv.LUT(Cr_img,Cr_img,cv.fromarray(Cr))
        cv.LUT(Cb_img,Cb_img,cv.fromarray(Cb))
        temp = self.getEmpty()
        cv.Merge(Y_img,Cr_img,Cb_img,None,temp)
        mask=Image(temp,colorSpace = ColorSpace.YCrCb)
        mask = mask.binarize((128,128,128))
        mask = mask.toRGB().binarize()
        mask.dilate(dilate_iter)
        return mask

    #this code is based on code that's based on code from
    #http://blog.jozilla.net/2008/06/27/fun-with-python-opencv-and-face-detection/
    def findHaarFeatures(self, cascade, scale_factor=1.2, min_neighbors=2, use_canny=cv.CV_HAAR_DO_CANNY_PRUNING, min_size=(20,20), max_size=(1000,1000)):
        """
        **SUMMARY**

        A Haar like feature cascase is a really robust way of finding the location
        of a known object. This technique works really well for a few specific applications
        like face, pedestrian, and vehicle detection. It is worth noting that this
        approach **IS NOT A MAGIC BULLET** . Creating a cascade file requires a large
        number of images that have been sorted by a human.vIf you want to find Haar
        Features (useful for face detection among other purposes) this will return
        Haar feature objects in a FeatureSet.

        For more information, consult the cv.HaarDetectObjects documentation.

        To see what features are available run img.listHaarFeatures() or you can
        provide your own haarcascade file if you have one available.

        Note that the cascade parameter can be either a filename, or a HaarCascade
        loaded with cv.Load(), or a SimpleCV HaarCascade object.

        **PARAMETERS**

        * *cascade* - The Haar Cascade file, this can be either the path to a cascade
          file or a HaarCascased SimpleCV object that has already been
          loaded.

        * *scale_factor* - The scaling factor for subsequent rounds of the Haar cascade
          (default 1.2) in terms of a percentage (i.e. 1.2 = 20% increase in size)

        * *min_neighbors* - The minimum number of rectangles that makes up an object. Ususally
          detected faces are clustered around the face, this is the number
          of detections in a cluster that we need for detection. Higher
          values here should reduce false positives and decrease false negatives.

        * *use-canny* - Whether or not to use Canny pruning to reject areas with too many edges
          (default yes, set to 0 to disable)

        * *min_size* - Minimum window size. By default, it is set to the size
          of samples the classifier has been trained on ((20,20) for face detection)

        * *max_size* - Maximum window size. By default, it is set to the size
          of samples the classifier has been trained on ((1000,1000) for face detection)

        **RETURNS**

        A feature set of HaarFeatures

        **EXAMPLE**

        >>> faces = HaarCascade("./SimpleCV/Features/HaarCascades/face.xml","myFaces")
        >>> cam = Camera()
        >>> while True:
        >>>     f = cam.getImage().findHaarFeatures(faces)
        >>>     if( f is not None ):
        >>>          f.show()

        **NOTES**

        OpenCV Docs:
        - http://opencv.willowgarage.com/documentation/python/objdetect_cascade_classification.html

        Wikipedia:
        - http://en.wikipedia.org/wiki/Viola-Jones_object_detection_framework
        - http://en.wikipedia.org/wiki/Haar-like_features

        The video on this pages shows how Haar features and cascades work to located faces:
        - http://dismagazine.com/dystopia/evolved-lifestyles/8115/anti-surveillance-how-to-hide-from-machines/

        """
        storage = cv.CreateMemStorage(0)


        #lovely.  This segfaults if not present
        from SimpleCV.Features.HaarCascade import HaarCascade
        if isinstance(cascade, basestring):
            cascade = HaarCascade(cascade)
            if not cascade.getCascade(): 
                return None
        elif isinstance(cascade,HaarCascade):
            pass
        else:
            logger.warning('Could not initialize HaarCascade. Enter Valid cascade value.')

        # added all of the arguments from the opencv docs arglist
        try:
            import cv2
            haarClassify = cv2.CascadeClassifier(cascade.getFHandle())
            objects = haarClassify.detectMultiScale(self.getGrayNumpyCv2(),scaleFactor=scale_factor,minNeighbors=min_neighbors,minSize=min_size,flags=use_canny)
            cv2flag = True

        except ImportError:
            objects = cv.HaarDetectObjects(self._getEqualizedGrayscaleBitmap(),
                cascade.getCascade(), storage, scale_factor, min_neighbors,
                use_canny, min_size)
            cv2flag = False

        if objects is not None:
            return FeatureSet([HaarFeature(self, o, cascade,cv2flag) for o in objects])

        return None


    def drawCircle(self, ctr, rad, color = (0, 0, 0), thickness = 1):
        """
        **SUMMARY**

        Draw a circle on the image.

        **PARAMETERS**

        * *ctr* - The center of the circle as an (x,y) tuple.
        * *rad* - The radius of the circle in pixels
        * *color* - A color tuple (default black)
        * *thickness* - The thickness of the circle, -1 means filled in.

        **RETURNS**

        .. Warning::
          This is an inline operation. Nothing is returned, but a circle is drawn on the images's
          drawing layer.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> img.drawCircle((img.width/2,img.height/2),r=50,color=Colors.RED,width=3)
        >>> img.show()

        **NOTES**

        .. Warning::
          Note that this function is depricated, try to use DrawingLayer.circle() instead.

        **SEE ALSO**

        :py:meth:`drawLine`
        :py:meth:`drawText`
        :py:meth:`dl`
        :py:meth:`drawRectangle`
        :py:class:`DrawingLayer`

        """
        if( thickness < 0):
            self.getDrawingLayer().circle((int(ctr[0]), int(ctr[1])), int(rad), color, int(thickness),filled=True)
        else:
            self.getDrawingLayer().circle((int(ctr[0]), int(ctr[1])), int(rad), color, int(thickness))


    def drawLine(self, pt1, pt2, color = (0, 0, 0), thickness = 1):
        """
        **SUMMARY**
        Draw a line on the image.


        **PARAMETERS**

        * *pt1* - the first point for the line (tuple).
        * *pt2* - the second point on the line (tuple).
        * *color* - a color tuple (default black).
        * *thickness* the thickness of the line in pixels.

        **RETURNS**

        .. Warning::
          This is an inline operation. Nothing is returned, but a circle is drawn on the images's
          drawing layer.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> img.drawLine((0,0),(img.width,img.height),color=Color.RED,thickness=3)
        >>> img.show()

        **NOTES**

        .. Warning::
           Note that this function is depricated, try to use DrawingLayer.line() instead.

        **SEE ALSO**

        :py:meth:`drawText`
        :py:meth:`dl`
        :py:meth:`drawCircle`
        :py:meth:`drawRectangle`

        """
        pt1 = (int(pt1[0]), int(pt1[1]))
        pt2 = (int(pt2[0]), int(pt2[1]))
        self.getDrawingLayer().line(pt1, pt2, color, thickness)

    def size(self):
        """
        **SUMMARY**

        Returns a tuple that lists the width and height of the image.

        **RETURNS**

        The width and height as a tuple.


        """
        if self.width and self.height:
            return cv.GetSize(self.getBitmap())
        else:
            return (0, 0)

    def isEmpty(self):
        """
        **SUMMARY**

        Checks if the image is empty by checking its width and height.

        **RETURNS**

        True if the image's size is (0, 0), False for any other size.

        """
        return self.size() == (0, 0)

    def split(self, cols, rows):
        """
        **SUMMARY**

        This method can be used to brak and image into a series of image chunks.
        Given number of cols and rows, splits the image into a cols x rows 2d array
        of cropped images

        **PARAMETERS**

        * *rows* - an integer number of rows.
        * *cols* - an integer number of cols.

        **RETURNS**

        A list of SimpleCV images.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> quadrant =img.split(2,2)
        >>> for f in quadrant:
        >>>    f.show()
        >>>    time.sleep(1)


        **NOTES**

        TODO: This should return and ImageList

        """
        crops = []

        wratio = self.width / cols
        hratio = self.height / rows

        for i in range(rows):
            row = []
            for j in range(cols):
                row.append(self.crop(j * wratio, i * hratio, wratio, hratio))
            crops.append(row)

        return crops

    def splitChannels(self, grayscale = True):
        """
        **SUMMARY**

        Split the channels of an image into RGB (not the default BGR)
        single parameter is whether to return the channels as grey images (default)
        or to return them as tinted color image

        **PARAMETERS**

        * *grayscale* - If this is true we return three grayscale images, one per channel.
          if it is False return tinted images.


        **RETURNS**

        A tuple of of 3 image objects.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> data = img.splitChannels()
        >>> for d in data:
        >>>    d.show()
        >>>    time.sleep(1)

        **SEE ALSO**

        :py:meth:`mergeChannels`
        """
        r = self.getEmpty(1)
        g = self.getEmpty(1)
        b = self.getEmpty(1)
        cv.Split(self.getBitmap(), b, g, r, None)


        red = self.getEmpty()
        green = self.getEmpty()
        blue = self.getEmpty()


        if (grayscale):
            cv.Merge(r, r, r, None, red)
            cv.Merge(g, g, g, None, green)
            cv.Merge(b, b, b, None, blue)
        else:
            cv.Merge(None, None, r, None, red)
            cv.Merge(None, g, None, None, green)
            cv.Merge(b, None, None, None, blue)


        return (Image(red), Image(green), Image(blue))

    def mergeChannels(self,r=None,g=None,b=None):
        """
        **SUMMARY**

        Merge channels is the oposite of splitChannels. The image takes one image for each
        of the R,G,B channels and then recombines them into a single image. Optionally any of these
        channels can be None.

        **PARAMETERS**

        * *r* - The r or last channel  of the result SimpleCV Image.
        * *g* - The g or center channel of the result SimpleCV Image.
        * *b* - The b or first channel of the result SimpleCV Image.


        **RETURNS**

        A SimpleCV Image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> [r,g,b] = img.splitChannels()
        >>> r = r.binarize()
        >>> g = g.binarize()
        >>> b = b.binarize()
        >>> result = img.mergeChannels(r,g,b)
        >>> result.show()


        **SEE ALSO**
        :py:meth:`splitChannels`

        """
        if( r is None and g is None and b is None ):
            logger.warning("ImageClass.mergeChannels - we need at least one valid channel")
            return None
        if( r is None ):
            r = self.getEmpty(1)
            cv.Zero(r);
        else:
            rt = r.getEmpty(1)
            cv.Split(r.getBitmap(),rt,rt,rt,None)
            r = rt
        if( g is None ):
            g = self.getEmpty(1)
            cv.Zero(g);
        else:
            gt = g.getEmpty(1)
            cv.Split(g.getBitmap(),gt,gt,gt,None)
            g = gt
        if( b is None ):
            b = self.getEmpty(1)
            cv.Zero(b);
        else:
            bt = b.getEmpty(1)
            cv.Split(b.getBitmap(),bt,bt,bt,None)
            b = bt

        retVal = self.getEmpty()
        cv.Merge(b,g,r,None,retVal)
        return Image(retVal)

    def applyHLSCurve(self, hCurve, lCurve, sCurve):
        """
        **SUMMARY**

        Apply a color correction curve in HSL space. This method can be used
        to change values for each channel. The curves are :py:class:`ColorCurve` class objects.

        **PARAMETERS**

        * *hCurve* - the hue ColorCurve object.
        * *lCurve* - the lightnes / value ColorCurve object.
        * *sCurve* - the saturation ColorCurve object

        **RETURNS**

        A SimpleCV Image

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> hc = ColorCurve([[0,0], [100, 120], [180, 230], [255, 255]])
        >>> lc = ColorCurve([[0,0], [90, 120], [180, 230], [255, 255]])
        >>> sc = ColorCurve([[0,0], [70, 110], [180, 230], [240, 255]])
        >>> img2 = img.applyHLSCurve(hc,lc,sc)

        **SEE ALSO**

        :py:class:`ColorCurve`
        :py:meth:`applyRGBCurve`
        """


        #TODO CHECK ROI
        #TODO CHECK CURVE SIZE
        #TODO CHECK COLORSPACE
        #TODO CHECK CURVE SIZE
        temp  = cv.CreateImage(self.size(), 8, 3)
        #Move to HLS space
        cv.CvtColor(self._bitmap, temp, cv.CV_RGB2HLS)
        tempMat = cv.GetMat(temp) #convert the bitmap to a matrix
        #now apply the color curve correction
        tempMat = np.array(self.getMatrix()).copy()
        tempMat[:, :, 0] = np.take(hCurve.mCurve, tempMat[:, :, 0])
        tempMat[:, :, 1] = np.take(sCurve.mCurve, tempMat[:, :, 1])
        tempMat[:, :, 2] = np.take(lCurve.mCurve, tempMat[:, :, 2])
        #Now we jimmy the np array into a cvMat
        image = cv.CreateImageHeader((tempMat.shape[1], tempMat.shape[0]), cv.IPL_DEPTH_8U, 3)
        cv.SetData(image, tempMat.tostring(), tempMat.dtype.itemsize * 3 * tempMat.shape[1])
        cv.CvtColor(image, image, cv.CV_HLS2RGB)
        return Image(image, colorSpace=self._colorSpace)


    def applyRGBCurve(self, rCurve, gCurve, bCurve):
        """
        **SUMMARY**

        Apply a color correction curve in RGB space. This method can be used
        to change values for each channel. The curves are :py:class:`ColorCurve` class objects.

        **PARAMETERS**

        * *rCurve* - the red ColorCurve object, or appropriately formatted list
        * *gCurve* - the green ColorCurve object, or appropriately formatted list
        * *bCurve* - the blue ColorCurve object, or appropriately formatted list

        **RETURNS**

        A SimpleCV Image

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> rc = ColorCurve([[0,0], [100, 120], [180, 230], [255, 255]])
        >>> gc = ColorCurve([[0,0], [90, 120], [180, 230], [255, 255]])
        >>> bc = ColorCurve([[0,0], [70, 110], [180, 230], [240, 255]])
        >>> img2 = img.applyRGBCurve(rc,gc,bc)

        **SEE ALSO**

        :py:class:`ColorCurve`
        :py:meth:`applyHLSCurve`

        """
        if isinstance(bCurve, list):
            bCurve = ColorCurve(bCurve)
        if isinstance(gCurve, list):
            gCurve = ColorCurve(gCurve)
        if isinstance(rCurve, list):
            rCurve = ColorCurve(rCurve)

        tempMat = np.array(self.getMatrix()).copy()
        tempMat[:, :, 0] = np.take(bCurve.mCurve, tempMat[:, :, 0])
        tempMat[:, :, 1] = np.take(gCurve.mCurve, tempMat[:, :, 1])
        tempMat[:, :, 2] = np.take(rCurve.mCurve, tempMat[:, :, 2])
        #Now we jimmy the np array into a cvMat
        image = cv.CreateImageHeader((tempMat.shape[1], tempMat.shape[0]), cv.IPL_DEPTH_8U, 3)
        cv.SetData(image, tempMat.tostring(), tempMat.dtype.itemsize * 3 * tempMat.shape[1])
        return Image(image, colorSpace=self._colorSpace)


    def applyIntensityCurve(self, curve):
        """
        **SUMMARY**

        Intensity applied to all three color channels

        **PARAMETERS**

        * *curve* - a ColorCurve object, or 2d list that can be conditioned into one

        **RETURNS**

        A SimpleCV Image

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> rc = ColorCurve([[0,0], [100, 120], [180, 230], [255, 255]])
        >>> gc = ColorCurve([[0,0], [90, 120], [180, 230], [255, 255]])
        >>> bc = ColorCurve([[0,0], [70, 110], [180, 230], [240, 255]])
        >>> img2 = img.applyRGBCurve(rc,gc,bc)

        **SEE ALSO**

        :py:class:`ColorCurve`
        :py:meth:`applyHLSCurve`

        """
        return self.applyRGBCurve(curve, curve, curve)


    def colorDistance(self, color = Color.BLACK):
        """
        **SUMMARY**

        Returns an image representing the distance of each pixel from a given color
        tuple, scaled between 0 (the given color) and 255.  Pixels distant from the
        given tuple will appear as brighter and pixels closest to the target color
        will be darker.


        By default this will give image intensity (distance from pure black)

        **PARAMETERS**

        * *color*  - Color object or Color Tuple

        **RETURNS**

        A SimpleCV Image.

        **EXAMPLE**

        >>> img = Image("logo")
        >>> img2 = img.colorDistance(color=Color.BLACK)
        >>> img2.show()


        **SEE ALSO**

        :py:meth:`binarize`
        :py:meth:`hueDistance`
        :py:meth:`findBlobsFromMask`
        """
        pixels = np.array(self.getNumpy()).reshape(-1, 3)   #reshape our matrix to 1xN
        distances = spsd.cdist(pixels, [color]) #calculate the distance each pixel is
        distances *= (255.0/distances.max()) #normalize to 0 - 255
        return Image(distances.reshape(self.width, self.height)) #return an Image

    def hueDistance(self, color = Color.BLACK, minsaturation = 20, minvalue = 20, maxvalue=255):
        """
        **SUMMARY**

        Returns an image representing the distance of each pixel from the given hue
        of a specific color.  The hue is "wrapped" at 180, so we have to take the shorter
        of the distances between them -- this gives a hue distance of max 90, which we'll
        scale into a 0-255 grayscale image.

        The minsaturation and minvalue are optional parameters to weed out very weak hue
        signals in the picture, they will be pushed to max distance [255]


        **PARAMETERS**

        * *color* - Color object or Color Tuple.
        * *minsaturation*  - the minimum saturation value for color (from 0 to 255).
        * *minvalue*  - the minimum hue value for the color (from 0 to 255).

        **RETURNS**

        A simpleCV image.

        **EXAMPLE**

        >>> img = Image("logo")
        >>> img2 = img.hueDistance(color=Color.BLACK)
        >>> img2.show()

        **SEE ALSO**

        :py:meth:`binarize`
        :py:meth:`hueDistance`
        :py:meth:`morphOpen`
        :py:meth:`morphClose`
        :py:meth:`morphGradient`
        :py:meth:`findBlobsFromMask`

        """
        if isinstance(color,  (float,int,long,complex)):
            color_hue = color
        else:
            color_hue = Color.hsv(color)[0]

        vsh_matrix = self.toHSV().getNumpy().reshape(-1,3) #again, gets transposed to vsh
        hue_channel = np.cast['int'](vsh_matrix[:,2])

        if color_hue < 90:
            hue_loop = 180
        else:
            hue_loop = -180
        #set whether we need to move back or forward on the hue circle

        distances = np.minimum( np.abs(hue_channel - color_hue), np.abs(hue_channel - (color_hue + hue_loop)))
        #take the minimum distance for each pixel


        distances = np.where(
            np.logical_and(vsh_matrix[:,0] > minvalue, vsh_matrix[:,1] > minsaturation),
            distances * (255.0 / 90.0), #normalize 0 - 90 -> 0 - 255
            255.0) #use the maxvalue if it false outside of our value/saturation tolerances

        return Image(distances.reshape(self.width, self.height))


    def erode(self, iterations=1, kernelsize=3):
        """
        **SUMMARY**

        Apply a morphological erosion. An erosion has the effect of removing small bits of noise
        and smothing blobs.

        This implementation uses the default openCV 3X3 square kernel

        Erosion is effectively a local minima detector, the kernel moves over the image and
        takes the minimum value inside the kernel.
        iterations - this parameters is the number of times to apply/reapply the operation

        * See: http://en.wikipedia.org/wiki/Erosion_(morphology).

        * See: http://opencv.willowgarage.com/documentation/cpp/image_filtering.html#cv-erode

        * Example Use: A threshold/blob image has 'salt and pepper' noise.

        * Example Code: /examples/MorphologyExample.py

        **PARAMETERS**

        * *iterations* - the number of times to run the erosion operation.

        **RETURNS**

        A SimpleCV image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> derp = img.binarize()
        >>> derp.erode(3).show()

        **SEE ALSO**
        :py:meth:`dilate`
        :py:meth:`binarize`
        :py:meth:`morphOpen`
        :py:meth:`morphClose`
        :py:meth:`morphGradient`
        :py:meth:`findBlobsFromMask`

        """
        retVal = self.getEmpty()
        kern = cv.CreateStructuringElementEx(kernelsize,kernelsize, 1, 1, cv.CV_SHAPE_RECT)
        cv.Erode(self.getBitmap(), retVal, kern, iterations)
        return Image(retVal, colorSpace=self._colorSpace)


    def dilate(self, iterations=1):
        """
        **SUMMARY**

        Apply a morphological dilation. An dilation has the effect of smoothing blobs while
        intensifying the amount of noise blobs.
        This implementation uses the default openCV 3X3 square kernel
        Erosion is effectively a local maxima detector, the kernel moves over the image and
        takes the maxima value inside the kernel.

        * See: http://en.wikipedia.org/wiki/Dilation_(morphology)

        * See: http://opencv.willowgarage.com/documentation/cpp/image_filtering.html#cv-dilate

        * Example Use: A part's blob needs to be smoother

        * Example Code: ./examples/MorphologyExample.py

        **PARAMETERS**

        * *iterations* - the number of times to run the dilation operation.

        **RETURNS**

        A SimpleCV image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> derp = img.binarize()
        >>> derp.dilate(3).show()

        **SEE ALSO**

        :py:meth:`erode`
        :py:meth:`binarize`
        :py:meth:`morphOpen`
        :py:meth:`morphClose`
        :py:meth:`morphGradient`
        :py:meth:`findBlobsFromMask`

        """
        retVal = self.getEmpty()
        kern = cv.CreateStructuringElementEx(3, 3, 1, 1, cv.CV_SHAPE_RECT)
        cv.Dilate(self.getBitmap(), retVal, kern, iterations)
        return Image(retVal, colorSpace=self._colorSpace)


    def morphOpen(self):
        """
        **SUMMARY**

        morphologyOpen applies a morphological open operation which is effectively
        an erosion operation followed by a morphological dilation. This operation
        helps to 'break apart' or 'open' binary regions which are close together.


        * `Morphological opening on Wikipedia <http://en.wikipedia.org/wiki/Opening_(morphology)>`_

        * `OpenCV documentation <http://opencv.willowgarage.com/documentation/cpp/image_filtering.html#cv-morphologyex>`_

        * Example Use: two part blobs are 'sticking' together.

        * Example Code: ./examples/MorphologyExample.py

        **RETURNS**

        A SimpleCV image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> derp = img.binarize()
        >>> derp.morphOpen.show()

        **SEE ALSO**

        :py:meth:`erode`
        :py:meth:`dilate`
        :py:meth:`binarize`
        :py:meth:`morphClose`
        :py:meth:`morphGradient`
        :py:meth:`findBlobsFromMask`

        """
        retVal = self.getEmpty()
        temp = self.getEmpty()
        kern = cv.CreateStructuringElementEx(3, 3, 1, 1, cv.CV_SHAPE_RECT)
        try:
            cv.MorphologyEx(self.getBitmap(), retVal, temp, kern, cv.MORPH_OPEN, 1)
        except:
            cv.MorphologyEx(self.getBitmap(), retVal, temp, kern, cv.CV_MOP_OPEN, 1)
            #OPENCV 2.2 vs 2.3 compatability

        return( Image(retVal) )


    def morphClose(self):
        """
        **SUMMARY**

        morphologyClose applies a morphological close operation which is effectively
        a dilation operation followed by a morphological erosion. This operation
        helps to 'bring together' or 'close' binary regions which are close together.


        * See: `Closing <http://en.wikipedia.org/wiki/Closing_(morphology)>`_

        * See: `Morphology from OpenCV <http://opencv.willowgarage.com/documentation/cpp/image_filtering.html#cv-morphologyex>`_

        * Example Use: Use when a part, which should be one blob is really two blobs.

        * Example Code: ./examples/MorphologyExample.py

        **RETURNS**

        A SimpleCV image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> derp = img.binarize()
        >>> derp.morphClose.show()

        **SEE ALSO**

        :py:meth:`erode`
        :py:meth:`dilate`
        :py:meth:`binarize`
        :py:meth:`morphOpen`
        :py:meth:`morphGradient`
        :py:meth:`findBlobsFromMask`

        """

        retVal = self.getEmpty()
        temp = self.getEmpty()
        kern = cv.CreateStructuringElementEx(3, 3, 1, 1, cv.CV_SHAPE_RECT)
        try:
            cv.MorphologyEx(self.getBitmap(), retVal, temp, kern, cv.MORPH_CLOSE, 1)
        except:
            cv.MorphologyEx(self.getBitmap(), retVal, temp, kern, cv.CV_MOP_CLOSE, 1)
            #OPENCV 2.2 vs 2.3 compatability

        return Image(retVal, colorSpace=self._colorSpace)


    def morphGradient(self):
        """
        **SUMMARY**

        The morphological gradient is the difference betwen the morphological
        dilation and the morphological gradient. This operation extracts the
        edges of a blobs in the image.


        * `See Morph Gradient of Wikipedia <http://en.wikipedia.org/wiki/Morphological_Gradient>`_

        * `OpenCV documentation <http://opencv.willowgarage.com/documentation/cpp/image_filtering.html#cv-morphologyex>`_

        * Example Use: Use when you have blobs but you really just want to know the blob edges.

        * Example Code: ./examples/MorphologyExample.py


        **RETURNS**

        A SimpleCV image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> derp = img.binarize()
        >>> derp.morphGradient.show()

        **SEE ALSO**

        :py:meth:`erode`
        :py:meth:`dilate`
        :py:meth:`binarize`
        :py:meth:`morphOpen`
        :py:meth:`morphClose`
        :py:meth:`findBlobsFromMask`

        """

        retVal = self.getEmpty()
        temp = self.getEmpty()
        kern = cv.CreateStructuringElementEx(3, 3, 1, 1, cv.CV_SHAPE_RECT)
        try:
            cv.MorphologyEx(self.getBitmap(), retVal, temp, kern, cv.MORPH_GRADIENT, 1)
        except:
            cv.MorphologyEx(self.getBitmap(), retVal, temp, kern, cv.CV_MOP_GRADIENT, 1)
        return Image(retVal, colorSpace=self._colorSpace )


    def histogram(self, numbins = 50):
        """
        **SUMMARY**

        Return a numpy array of the 1D histogram of intensity for pixels in the image
        Single parameter is how many "bins" to have.


        **PARAMETERS**

        * *numbins* - An interger number of bins in a histogram.

        **RETURNS**

        A list of histogram bin values.

        **EXAMPLE**

        >>> img = Image('lenna')
        >>> hist = img.histogram()

        **SEE ALSO**

        :py:meth:`hueHistogram`

        """
        gray = self._getGrayscaleBitmap()


        (hist, bin_edges) = np.histogram(np.asarray(cv.GetMat(gray)), bins=numbins)
        return hist.tolist()

    def hueHistogram(self, bins = 179, dynamicRange=True):

        """
        **SUMMARY**

        Returns the histogram of the hue channel for the image


        **PARAMETERS**

        * *numbins* - An interger number of bins in a histogram.

        **RETURNS**

        A list of histogram bin values.

        **SEE ALSO**

        :py:meth:`histogram`

        """
        if dynamicRange:
            return np.histogram(self.toHSV().getNumpy()[:,:,2], bins = bins)[0]
        else:
            return np.histogram(self.toHSV().getNumpy()[:,:,2], bins = bins, range=(0.0,360.0))[0]

    def huePeaks(self, bins = 179):
        """
        **SUMMARY**

        Takes the histogram of hues, and returns the peak hue values, which
        can be useful for determining what the "main colors" in a picture.

        The bins parameter can be used to lump hues together, by default it is 179
        (the full resolution in OpenCV's HSV format)

        Peak detection code taken from https://gist.github.com/1178136
        Converted from/based on a MATLAB script at http://billauer.co.il/peakdet.html

        Returns a list of tuples, each tuple contains the hue, and the fraction
        of the image that has it.

        **PARAMETERS**

        * *bins* - the integer number of bins, between 0 and 179.

        **RETURNS**

        A list of (hue,fraction) tuples.

        """
        #         keyword arguments:
        #         y_axis -- A list containg the signal over which to find peaks
        #         x_axis -- A x-axis whose values correspond to the 'y_axis' list and is used
        #             in the return to specify the postion of the peaks. If omitted the index
        #             of the y_axis is used. (default: None)
        #         lookahead -- (optional) distance to look ahead from a peak candidate to
        #             determine if it is the actual peak (default: 500)
        #             '(sample / period) / f' where '4 >= f >= 1.25' might be a good value
        #         delta -- (optional) this specifies a minimum difference between a peak and
        #             the following points, before a peak may be considered a peak. Useful
        #             to hinder the algorithm from picking up false peaks towards to end of
        #             the signal. To work well delta should be set to 'delta >= RMSnoise * 5'.
        #             (default: 0)
        #                 Delta function causes a 20% decrease in speed, when omitted
        #                 Correctly used it can double the speed of the algorithm
        #         return --  Each cell of the lists contains a tupple of:
        #             (position, peak_value)
        #             to get the average peak value do 'np.mean(maxtab, 0)[1]' on the results

        y_axis, x_axis = np.histogram(self.toHSV().getNumpy()[:,:,2], bins = bins)
        x_axis = x_axis[0:bins]
        lookahead = int(bins / 17)
        delta = 0

        maxtab = []
        mintab = []
        dump = []   #Used to pop the first hit which always if false

        length = len(y_axis)
        if x_axis is None:
            x_axis = range(length)

        #perform some checks
        if length != len(x_axis):
            raise ValueError, "Input vectors y_axis and x_axis must have same length"
        if lookahead < 1:
            raise ValueError, "Lookahead must be above '1' in value"
        if not (np.isscalar(delta) and delta >= 0):
            raise ValueError, "delta must be a positive number"

        #needs to be a numpy array
        y_axis = np.asarray(y_axis)

        #maxima and minima candidates are temporarily stored in
        #mx and mn respectively
        mn, mx = np.Inf, -np.Inf

        #Only detect peak if there is 'lookahead' amount of points after it
        for index, (x, y) in enumerate(zip(x_axis[:-lookahead], y_axis[:-lookahead])):
            if y > mx:
                mx = y
                mxpos = x
            if y < mn:
                mn = y
                mnpos = x

            ####look for max####
            if y < mx-delta and mx != np.Inf:
                #Maxima peak candidate found
                #look ahead in signal to ensure that this is a peak and not jitter
                if y_axis[index:index+lookahead].max() < mx:
                    maxtab.append((mxpos, mx))
                    dump.append(True)
                    #set algorithm to only find minima now
                    mx = np.Inf
                    mn = np.Inf

            ####look for min####
            if y > mn+delta and mn != -np.Inf:
                #Minima peak candidate found
                #look ahead in signal to ensure that this is a peak and not jitter
                if y_axis[index:index+lookahead].min() > mn:
                    mintab.append((mnpos, mn))
                    dump.append(False)
                    #set algorithm to only find maxima now
                    mn = -np.Inf
                    mx = -np.Inf


        #Remove the false hit on the first value of the y_axis
        try:
            if dump[0]:
                maxtab.pop(0)
                #print "pop max"
            else:
                mintab.pop(0)
                #print "pop min"
            del dump
        except IndexError:
            #no peaks were found, should the function return empty lists?
            pass

        huetab = []
        for hue, pixelcount in maxtab:
            huetab.append((hue, pixelcount / float(self.width * self.height)))
        return huetab



    def __getitem__(self, coord):
        ret = self.getMatrix()[tuple(reversed(coord))]
        if (type(ret) == cv.cvmat):
            (width, height) = cv.GetSize(ret)
            newmat = cv.CreateMat(height, width, ret.type)
            cv.Copy(ret, newmat) #this seems to be a bug in opencv
            #if you don't copy the matrix slice, when you convert to bmp you get
            #a slice-sized hunk starting at 0, 0
            return Image(newmat)

        if self.isBGR():
            return tuple(reversed(ret))
        else:
            return tuple(ret)


    def __setitem__(self, coord, value):
        value = tuple(reversed(value))  #RGB -> BGR

        if(isinstance(coord[0],slice)):
            cv.Set(self.getMatrix()[tuple(reversed(coord))], value)
            self._clearBuffers("_matrix")
        else:
            self.getMatrix()[tuple(reversed(coord))] = value
            self._clearBuffers("_matrix")



    def __sub__(self, other):
        newbitmap = self.getEmpty()
        if is_number(other):
            cv.SubS(self.getBitmap(), cv.Scalar(other,other,other), newbitmap)
        else:
            cv.Sub(self.getBitmap(), other.getBitmap(), newbitmap)
        return Image(newbitmap, colorSpace=self._colorSpace)


    def __add__(self, other):
        newbitmap = self.getEmpty()
        if is_number(other):
            cv.AddS(self.getBitmap(), cv.Scalar(other,other,other), newbitmap)
        else:
            cv.Add(self.getBitmap(), other.getBitmap(), newbitmap)
        return Image(newbitmap, colorSpace=self._colorSpace)


    def __and__(self, other):
        newbitmap = self.getEmpty()
        if is_number(other):
            cv.AndS(self.getBitmap(), cv.Scalar(other,other,other), newbitmap)
        else:
            cv.And(self.getBitmap(), other.getBitmap(), newbitmap)
        return Image(newbitmap, colorSpace=self._colorSpace)


    def __or__(self, other):
        newbitmap = self.getEmpty()
        if is_number(other):
            cv.OrS(self.getBitmap(), cv.Scalar(other,other,other), newbitmap)
        else:
            cv.Or(self.getBitmap(), other.getBitmap(), newbitmap)
        return Image(newbitmap, colorSpace=self._colorSpace)


    def __div__(self, other):
        newbitmap = self.getEmpty()
        if (not is_number(other)):
            cv.Div(self.getBitmap(), other.getBitmap(), newbitmap)
        else:
            cv.ConvertScale(self.getBitmap(), newbitmap, 1.0/float(other))
        return Image(newbitmap, colorSpace=self._colorSpace)


    def __mul__(self, other):
        newbitmap = self.getEmpty()
        if (not is_number(other)):
            cv.Mul(self.getBitmap(), other.getBitmap(), newbitmap)
        else:
            cv.ConvertScale(self.getBitmap(), newbitmap, float(other))
        return Image(newbitmap, colorSpace=self._colorSpace)

    def __pow__(self, other):
        newbitmap = self.getEmpty()
        cv.Pow(self.getBitmap(), newbitmap, other)
        return Image(newbitmap, colorSpace=self._colorSpace)

    def __neg__(self):
        newbitmap = self.getEmpty()
        cv.Not(self.getBitmap(), newbitmap)
        return Image(newbitmap, colorSpace=self._colorSpace)

    def __invert__(self):
        return self.invert()

    def max(self, other):
        """
        **SUMMARY**

        The maximum value of my image, and the other image, in each channel
        If other is a number, returns the maximum of that and the number

        **PARAMETERS**

        * *other* - Image of the same size or a number.

        **RETURNS**

        A SimpelCV image.

        """

        newbitmap = self.getEmpty()
        if is_number(other):
            cv.MaxS(self.getBitmap(), other, newbitmap)
        else:
            if self.size() != other.size():
                warnings.warn("Both images should have same sizes. Returning None.")
                return None
            cv.Max(self.getBitmap(), other.getBitmap(), newbitmap)
        return Image(newbitmap, colorSpace=self._colorSpace)


    def min(self, other):
        """
        **SUMMARY**

        The minimum value of my image, and the other image, in each channel
        If other is a number, returns the minimum of that and the number

        **Parameter**

        * *other* - Image of the same size or number

        **Returns**

        IMAGE
        """

        newbitmap = self.getEmpty()
        if is_number(other):
            cv.MinS(self.getBitmap(), other, newbitmap)
        else:
            if self.size() != other.size():
                warnings.warn("Both images should have same sizes. Returning None.")
                return None
            cv.Min(self.getBitmap(), other.getBitmap(), newbitmap)
        return Image(newbitmap, colorSpace=self._colorSpace)


    def _clearBuffers(self, clearexcept = "_bitmap"):
        for k, v in self._initialized_buffers.items():
            if k == clearexcept:
                continue
            self.__dict__[k] = v


    def findBarcode(self,doZLib=True,zxing_path=""):
        """
        **SUMMARY**

        This function requires zbar and the zbar python wrapper
        to be installed or zxing and the zxing python library.

        **ZBAR**

        To install please visit:
        http://zbar.sourceforge.net/

        On Ubuntu Linux 12.04 or greater:
        sudo apt-get install python-zbar


        **ZXING**

        If you have the python-zxing library installed, you can find 2d and 1d
        barcodes in your image.  These are returned as Barcode feature objects
        in a FeatureSet.  The single parameter is the ZXing_path along with
        setting the doZLib flag to False. You do not need the parameter if you
        don't have the ZXING_LIBRARY env parameter set.

        You can clone python-zxing at:

        http://github.com/oostendo/python-zxing

        **INSTALLING ZEBRA CROSSING**

        * Download the latest version of zebra crossing from: http://code.google.com/p/zxing/

        * unpack the zip file where ever you see fit

          >>> cd zxing-x.x, where x.x is the version number of zebra crossing
          >>> ant -f core/build.xml
          >>> ant -f javase/build.xml

          This should build the library, but double check the readme

        * Get our helper library

          >>> git clone git://github.com/oostendo/python-zxing.git
          >>> cd python-zxing
          >>> python setup.py install

        * Our library does not have a setup file. You will need to add
           it to your path variables. On OSX/Linux use a text editor to modify your shell file (e.g. .bashrc)

          export ZXING_LIBRARY=<FULL PATH OF ZXING LIBRARY - (i.e. step 2)>
          for example:

          export ZXING_LIBRARY=/my/install/path/zxing-x.x/

          On windows you will need to add these same variables to the system variable, e.g.

          http://www.computerhope.com/issues/ch000549.htm

        * On OSX/Linux source your shell rc file (e.g. source .bashrc). Windows users may need to restart.

        * Go grab some barcodes!

        .. Warning::
          Users on OSX may see the following error:

          RuntimeWarning: tmpnam is a potential security risk to your program

          We are working to resolve this issue. For normal use this should not be a problem.

        **Returns**

        A :py:class:`FeatureSet` of :py:class:`Barcode` objects. If no barcodes are detected the method returns None.

        **EXAMPLE**

        >>> bc = cam.getImage()
        >>> barcodes = img.findBarcodes()
        >>> for b in barcodes:
        >>>     b.draw()

        **SEE ALSO**

        :py:class:`FeatureSet`
        :py:class:`Barcode`

        """
        if( doZLib ):
            try:
                import zbar
            except:
                logger.warning('The zbar library is not installed, please install to read barcodes')
                return None

            #configure zbar
            scanner = zbar.ImageScanner()
            scanner.parse_config('enable')
            raw = self.getPIL().convert('L').tostring()
            width = self.width
            height = self.height

            # wrap image data
            image = zbar.Image(width, height, 'Y800', raw)

            # scan the image for barcodes
            scanner.scan(image)
            barcode = None
            # extract results
            for symbol in image:
                # do something useful with results
                barcode = symbol
            # clean up
            del(image)

        else:
            if not ZXING_ENABLED:
                warnings.warn("Zebra Crossing (ZXing) Library not installed. Please see the release notes.")
                return None

            if (not self._barcodeReader):
                if not zxing_path:
                    self._barcodeReader = zxing.BarCodeReader()
                else:
                    self._barcodeReader = zxing.BarCodeReader(zxing_path)

            tmp_filename = os.tmpnam() + ".png"
            self.save(tmp_filename)
            barcode = self._barcodeReader.decode(tmp_filename)
            os.unlink(tmp_filename)

        if barcode:
            f = Barcode(self, barcode)
            return FeatureSet([f])
        else:
            return None


    #this function contains two functions -- the basic edge detection algorithm
    #and then a function to break the lines down given a threshold parameter
    def findLines(self, threshold=80, minlinelength=30, maxlinegap=10, cannyth1=50, cannyth2=100, useStandard=False, nLines=-1, maxpixelgap=1):
        """
        **SUMMARY**

        findLines will find line segments in your image and returns line feature
        objects in a FeatureSet. This method uses the Hough (pronounced "HUFF") transform.

        See http://en.wikipedia.org/wiki/Hough_transform

        **PARAMETERS**

        * *threshold* - which determines the minimum "strength" of the line.
        * *minlinelength* - how many pixels long the line must be to be returned.
        * *maxlinegap* - how much gap is allowed between line segments to consider them the same line .
        * *cannyth1* - thresholds used in the edge detection step, refer to :py:meth:`_getEdgeMap` for details.
        * *cannyth2* - thresholds used in the edge detection step, refer to :py:meth:`_getEdgeMap` for details.
        * *useStandard* - use standard or probabilistic Hough transform.
        * *nLines* - maximum number of lines for return.
        * *maxpixelgap* - how much distance between pixels is allowed to consider them the same line.

        **RETURNS**

        Returns a :py:class:`FeatureSet` of :py:class:`Line` objects. If no lines are found the method returns None.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> lines = img.findLines()
        >>> lines.draw()
        >>> img.show()

        **SEE ALSO**
        :py:class:`FeatureSet`
        :py:class:`Line`
        :py:meth:`edges`

        """
        em = self._getEdgeMap(cannyth1, cannyth2)
        
        linesFS = FeatureSet()
        if useStandard:
            lines = cv.HoughLines2(em, cv.CreateMemStorage(), cv.CV_HOUGH_STANDARD, 1.0, cv.CV_PI/180.0, threshold, minlinelength, maxlinegap)
            if nLines == -1:
                nLines = len(lines)
            # All white points (edges) in Canny edge image
            em = Image(em)
            x,y = np.where(em.getGrayNumpy() > 128)
            # Put points in dictionary for fast checkout if point is white
            pts = dict((p, 1) for p in zip(x, y))   
            
            w, h = self.width-1, self.height-1
            for rho, theta in lines[:nLines]:
                ep = []
                ls = []
                a = math.cos(theta)
                b = math.sin(theta)
                # Find endpoints of line on the image's edges
                if round(b, 4) == 0:        # slope of the line is infinity
                    ep.append( (int(round(abs(rho))), 0) )
                    ep.append( (int(round(abs(rho))), h) )
                elif round(a, 4) == 0:        # slope of the line is zero 
                    ep.append( (0, int(round(abs(rho)))) )
                    ep.append( (w, int(round(abs(rho)))) )
                else:
                    # top edge
                    x = rho/float(a)
                    if 0 <= x <= w:
                        ep.append((int(round(x)), 0))
                    # bottom edge
                    x = (rho - h*b)/float(a)
                    if 0 <= x <= w:
                        ep.append((int(round(x)), h))
                    # left edge
                    y = rho/float(b)
                    if 0 <= y <= h:
                        ep.append((0, int(round(y))))
                    # right edge
                    y = (rho - w*a)/float(b)
                    if 0 <= y <= h:
                        ep.append((w, int(round(y))))
                ep = list(set(ep))        # remove duplicates if line crosses the image at corners
                ep.sort()
                brl = self.bresenham_line(ep[0], ep[1])
                
                # Follow the points on Bresenham's line. Look for white points. 
                # If the distance between two adjacent white points (dist) is less than or 
                # equal maxpixelgap then consider them the same line. If dist is bigger 
                # maxpixelgap then check if length of the line is bigger than minlinelength.
                # If so then add line.
                dist = float('inf')        # distance between two adjacent white points
                len_l = float('-inf')        # length of the line
                for p in brl:
                    if p in pts:
                        if dist > maxpixelgap:        # found the end of the previous line and the start of the new line
                            if len_l >= minlinelength:
                                if ls:
                                    # If the gap between current line and previous  
                                    # is less than maxlinegap then merge this lines
                                    l = ls[-1]
                                    gap = round(math.sqrt( (start_p[0]-l[1][0])**2 + (start_p[1]-l[1][1])**2 ))
                                    if gap <= maxlinegap:
                                        ls.pop()
                                        start_p = l[0] 
                                ls.append( (start_p, last_p) )
                            # First white point of the new line found
                            dist = 1
                            len_l = 1
                            start_p = p        # first endpoint of the line
                        else:
                            # dist is less than or equal maxpixelgap, so line doesn't end yet
                            len_l += dist
                            dist = 1
                        last_p = p        # last white point
                    else:
                        dist += 1
    
                for l in ls:
                    linesFS.append(Line(self, l))
            linesFS = linesFS[:nLines]
        else:
            lines = cv.HoughLines2(em, cv.CreateMemStorage(), cv.CV_HOUGH_PROBABILISTIC, 1.0, cv.CV_PI/180.0, threshold, minlinelength, maxlinegap)
            if nLines == -1:
                nLines = len(lines)

            for l in lines[:nLines]:
                linesFS.append(Line(self, l))
        
        return linesFS


    def findChessboard(self, dimensions = (8, 5), subpixel = True):
        """
        **SUMMARY**

        Given an image, finds a chessboard within that image.  Returns the Chessboard featureset.
        The Chessboard is typically used for calibration because of its evenly spaced corners.


        The single parameter is the dimensions of the chessboard, typical one can be found in \SimpleCV\tools\CalibGrid.png

        **PARAMETERS**

        * *dimensions* - A tuple of the size of the chessboard in width and height in grid objects.
        * *subpixel* - Boolean if True use sub-pixel accuracy, otherwise use regular pixel accuracy.

        **RETURNS**

        A :py:class:`FeatureSet` of :py:class:`Chessboard` objects. If no chessboards are found None is returned.

        **EXAMPLE**

        >>> img = cam.getImage()
        >>> cb = img.findChessboard()
        >>> cb.draw()

        **SEE ALSO**

        :py:class:`FeatureSet`
        :py:class:`Chessboard`

        """
        corners = cv.FindChessboardCorners(self._getEqualizedGrayscaleBitmap(), dimensions, cv.CV_CALIB_CB_ADAPTIVE_THRESH + cv.CV_CALIB_CB_NORMALIZE_IMAGE )
        if(len(corners[1]) == dimensions[0]*dimensions[1]):
            if (subpixel):
                spCorners = cv.FindCornerSubPix(self.getGrayscaleMatrix(), corners[1], (11, 11), (-1, -1), (cv.CV_TERMCRIT_ITER | cv.CV_TERMCRIT_EPS, 10, 0.01))
            else:
                spCorners = corners[1]
            return FeatureSet([ Chessboard(self, dimensions, spCorners) ])
        else:
            return None


    def edges(self, t1=50, t2=100):
        """
        **SUMMARY**

        Finds an edge map Image using the Canny edge detection method.  Edges will be brighter than the surrounding area.

        The t1 parameter is roughly the "strength" of the edge required, and the value between t1 and t2 is used for edge linking.

        For more information:

        * http://opencv.willowgarage.com/documentation/python/imgproc_feature_detection.html

        * http://en.wikipedia.org/wiki/Canny_edge_detector

        **PARAMETERS**

        * *t1* - Int - the lower Canny threshold.
        * *t2* - Int - the upper Canny threshold.

        **RETURNS**

        A SimpleCV image where the edges are white on a black background.

        **EXAMPLE**

        >>> cam = Camera()
        >>> while True:
        >>>    cam.getImage().edges().show()


        **SEE ALSO**

        :py:meth:`findLines`

        """
        return Image(self._getEdgeMap(t1, t2), colorSpace=self._colorSpace)


    def _getEdgeMap(self, t1=50, t2=100):
        """
        Return the binary bitmap which shows where edges are in the image.  The two
        parameters determine how much change in the image determines an edge,
        and how edges are linked together.  For more information refer to:


        http://en.wikipedia.org/wiki/Canny_edge_detector
        http://opencv.willowgarage.com/documentation/python/imgproc_feature_detection.html?highlight=canny#Canny
        """


        if (self._edgeMap and self._cannyparam[0] == t1 and self._cannyparam[1] == t2):
            return self._edgeMap


        self._edgeMap = self.getEmpty(1)
        cv.Canny(self._getGrayscaleBitmap(), self._edgeMap, t1, t2)
        self._cannyparam = (t1, t2)


        return self._edgeMap


    def rotate(self, angle, fixed=True, point=[-1, -1], scale = 1.0):
        """
        **SUMMARY***

        This function rotates an image around a specific point by the given angle
        By default in "fixed" mode, the returned Image is the same dimensions as the original Image, and the contents will be scaled to fit.  In "full" mode the
        contents retain the original size, and the Image object will scale
        by default, the point is the center of the image.
        you can also specify a scaling parameter

        .. Note:
          that when fixed is set to false selecting a rotation point has no effect since the image is move to fit on the screen.

        **PARAMETERS**

        * *angle* - angle in degrees positive is clockwise, negative is counter clockwise
        * *fixed* - if fixed is true,keep the original image dimensions, otherwise scale the image to fit the rotation
        * *point* - the point about which we want to rotate, if none is defined we use the center.
        * *scale* - and optional floating point scale parameter.

        **RETURNS**

        The rotated SimpleCV image.

        **EXAMPLE**

        >>> img = Image('logo')
        >>> img2 = img.rotate( 73.00, point=(img.width/2,img.height/2))
        >>> img3 = img.rotate( 73.00, fixed=False, point=(img.width/2,img.height/2))
        >>> img4 = img2.sideBySide(img3)
        >>> img4.show()

        **SEE ALSO**

        :py:meth:`rotate90`

        """
        if( point[0] == -1 or point[1] == -1 ):
            point[0] = (self.width-1)/2
            point[1] = (self.height-1)/2


        if (fixed):
            retVal = self.getEmpty()
            cv.Zero(retVal)
            rotMat = cv.CreateMat(2, 3, cv.CV_32FC1)
            cv.GetRotationMatrix2D((float(point[0]), float(point[1])), float(angle), float(scale), rotMat)
            cv.WarpAffine(self.getBitmap(), retVal, rotMat)
            return Image(retVal, colorSpace=self._colorSpace)




        #otherwise, we're expanding the matrix to fit the image at original size
        rotMat = cv.CreateMat(2, 3, cv.CV_32FC1)
        # first we create what we thing the rotation matrix should be
        cv.GetRotationMatrix2D((float(point[0]), float(point[1])), float(angle), float(scale), rotMat)
        A = np.array([0, 0, 1])
        B = np.array([self.width, 0, 1])
        C = np.array([self.width, self.height, 1])
        D = np.array([0, self.height, 1])
        #So we have defined our image ABC in homogenous coordinates
        #and apply the rotation so we can figure out the image size
        a = np.dot(rotMat, A)
        b = np.dot(rotMat, B)
        c = np.dot(rotMat, C)
        d = np.dot(rotMat, D)
        #I am not sure about this but I think the a/b/c/d are transposed
        #now we calculate the extents of the rotated components.
        minY = min(a[1], b[1], c[1], d[1])
        minX = min(a[0], b[0], c[0], d[0])
        maxY = max(a[1], b[1], c[1], d[1])
        maxX = max(a[0], b[0], c[0], d[0])
        #from the extents we calculate the new size
        newWidth = np.ceil(maxX-minX)
        newHeight = np.ceil(maxY-minY)
        #now we calculate a new translation
        tX = 0
        tY = 0
        #calculate the translation that will get us centered in the new image
        if( minX < 0 ):
            tX = -1.0*minX
        elif(maxX > newWidth-1 ):
            tX = -1.0*(maxX-newWidth)


        if( minY < 0 ):
            tY = -1.0*minY
        elif(maxY > newHeight-1 ):
            tY = -1.0*(maxY-newHeight)


        #now we construct an affine map that will the rotation and scaling we want with the
        #the corners all lined up nicely with the output image.
        src = ((A[0], A[1]), (B[0], B[1]), (C[0], C[1]))
        dst = ((a[0]+tX, a[1]+tY), (b[0]+tX, b[1]+tY), (c[0]+tX, c[1]+tY))


        cv.GetAffineTransform(src, dst, rotMat)


        #calculate the translation of the corners to center the image
        #use these new corner positions as the input to cvGetAffineTransform
        retVal = cv.CreateImage((int(newWidth), int(newHeight)), 8, int(3))
        cv.Zero(retVal)

        cv.WarpAffine(self.getBitmap(), retVal, rotMat)
        #cv.AddS(retVal,(0,255,0),retVal)
        return Image(retVal, colorSpace=self._colorSpace)


    def transpose(self):
        """
        **SUMMARY**

        Does a fast 90 degree rotation to the right with a flip.

        .. Warning::
          Subsequent calls to this function *WILL NOT* keep rotating it to the right!!!
          This function just does a matrix transpose so following one transpose by another will
          just yield the original image.

        **RETURNS**

        The rotated SimpleCV Image.

        **EXAMPLE**

        >>> img = Image("logo")
        >>> img2 = img.transpose()
        >>> img2.show()

        **SEE ALSO**

        :py:meth:`rotate`


        """
        retVal = cv.CreateImage((self.height, self.width), cv.IPL_DEPTH_8U, 3)
        cv.Transpose(self.getBitmap(), retVal)
        return(Image(retVal, colorSpace=self._colorSpace))


    def shear(self, cornerpoints):
        """
        **SUMMARY**

        Given a set of new corner points in clockwise order, return a shear-ed image
        that transforms the image contents.  The returned image is the same
        dimensions.

        **PARAMETERS**

        * *cornerpoints* - a 2x4 tuple of points. The order is (top_left, top_right, bottom_left, bottom_right)

        **RETURNS**

        A simpleCV image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> points = ((50,0),(img.width+50,0),(img.width,img.height),(0,img.height))
        >>> img.shear(points).show()

        **SEE ALSO**

        :py:meth:`transformAffine`
        :py:meth:`warp`
        :py:meth:`rotate`

        http://en.wikipedia.org/wiki/Transformation_matrix

        """
        src =  ((0, 0), (self.width-1, 0), (self.width-1, self.height-1))
        #set the original points
        aWarp = cv.CreateMat(2, 3, cv.CV_32FC1)
        #create the empty warp matrix
        cv.GetAffineTransform(src, cornerpoints, aWarp)


        return self.transformAffine(aWarp)


    def transformAffine(self, rotMatrix):
        """
        **SUMMARY**

        This helper function for shear performs an affine rotation using the supplied matrix.
        The matrix can be a either an openCV mat or an np.ndarray type.
        The matrix should be a 2x3

        **PARAMETERS**

        * *rotMatrix* - A 2x3 numpy array or CvMat of the affine transform.

        **RETURNS**

        The rotated image. Note that the rotation is done in place, i.e. the image is not enlarged to fit the transofmation.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> points = ((50,0),(img.width+50,0),(img.width,img.height),(0,img.height))
        >>> src =  ((0, 0), (img.width-1, 0), (img.width-1, img.height-1))
        >>> result = cv.createMat(2,3,cv.CV_32FC1)
        >>> cv.GetAffineTransform(src,points,result)
        >>> img.transformAffine(result).show()


        **SEE ALSO**

        :py:meth:`shear`
        :py:meth`warp`
        :py:meth:`transformPerspective`
        :py:meth:`rotate`

        http://en.wikipedia.org/wiki/Transformation_matrix

        """
        retVal = self.getEmpty()
        if(type(rotMatrix) == np.ndarray ):
            rotMatrix = npArray2cvMat(rotMatrix)
        cv.WarpAffine(self.getBitmap(), retVal, rotMatrix)
        return Image(retVal, colorSpace=self._colorSpace)


    def warp(self, cornerpoints):
        """
        **SUMMARY**

        This method performs and arbitrary perspective transform.
        Given a new set of corner points in clockwise order frin top left, return an Image with
        the images contents warped to the new coordinates.  The returned image
        will be the same size as the original image


        **PARAMETERS**

        * *cornerpoints* - A list of four tuples corresponding to the destination corners in the order of (top_left,top_right,bottom_left,bottom_right)

        **RETURNS**

        A simpleCV Image with the warp applied. Note that this operation does not enlarge the image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> points = ((30, 30), (img.width-10, 70), (img.width-1-40, img.height-1+30),(20,img.height+10))
        >>> img.warp(points).show()

        **SEE ALSO**

        :py:meth:`shear`
        :py:meth:`transformAffine`
        :py:meth:`transformPerspective`
        :py:meth:`rotate`

        http://en.wikipedia.org/wiki/Transformation_matrix

        """
        #original coordinates
        src = ((0, 0), (self.width-1, 0), (self.width-1, self.height-1), (0, self.height-1))
        pWarp = cv.CreateMat(3, 3, cv.CV_32FC1) #create an empty 3x3 matrix
        cv.GetPerspectiveTransform(src, cornerpoints, pWarp) #figure out the warp matrix


        return self.transformPerspective(pWarp)


    def transformPerspective(self, rotMatrix):
        """
        **SUMMARY**

        This helper function for warp performs an affine rotation using the supplied matrix.
        The matrix can be a either an openCV mat or an np.ndarray type.
        The matrix should be a 3x3

       **PARAMETERS**
            * *rotMatrix* - Numpy Array or CvMat

        **RETURNS**

        The rotated image. Note that the rotation is done in place, i.e. the image is not enlarged to fit the transofmation.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> points = ((50,0),(img.width+50,0),(img.width,img.height),(0,img.height))
        >>> src = ((30, 30), (img.width-10, 70), (img.width-1-40, img.height-1+30),(20,img.height+10))
        >>> result = cv.CreateMat(3,3,cv.CV_32FC1)
        >>> cv.GetPerspectiveTransform(src,points,result)
        >>> img.transformPerspective(result).show()


        **SEE ALSO**

        :py:meth:`shear`
        :py:meth:`warp`
        :py:meth:`transformPerspective`
        :py:meth:`rotate`

        http://en.wikipedia.org/wiki/Transformation_matrix

        """
        try:
            import cv2
            if( type(rotMatrix) !=  np.ndarray ):
                rotMatrix = np.array(rotMatrix)
            retVal = cv2.warpPerspective(src=np.array(self.getMatrix()), dsize=(self.width,self.height),M=rotMatrix,flags = cv2.INTER_CUBIC)
            return Image(retVal, colorSpace=self._colorSpace, cv2image=True)
        except:            
            retVal = self.getEmpty()
            if(type(rotMatrix) == np.ndarray ):
                rotMatrix = npArray2cvMat(rotMatrix)
            cv.WarpPerspective(self.getBitmap(), retVal, rotMatrix)
            return Image(retVal, colorSpace=self._colorSpace)
            
    def getPixel(self, x, y):
        """
        **SUMMARY**

        This function returns the RGB value for a particular image pixel given a specific row and column.

        .. Warning::
          this function will always return pixels in RGB format even if the image is BGR format.

        **PARAMETERS**

            * *x* - Int the x pixel coordinate.
            * *y* - Int the y pixel coordinate.

        **RETURNS**

        A color value that is a three element integer tuple.

        **EXAMPLE**

        >>> img = Image(logo)
        >>> color = img.getPixel(10,10)


        .. Warning::
          We suggest that this method be used sparingly. For repeated pixel access use python array notation. I.e. img[x][y].

        """
        c = None
        retVal = None
        if( x < 0 or x >= self.width ):
            logger.warning("getRGBPixel: X value is not valid.")
        elif( y < 0 or y >= self.height ):
            logger.warning("getRGBPixel: Y value is not valid.")
        else:
            c = cv.Get2D(self.getBitmap(), y, x)
            if( self._colorSpace == ColorSpace.BGR ):
                retVal = (c[2],c[1],c[0])
            else:
                retVal = (c[0],c[1],c[2])

        return retVal


    def getGrayPixel(self, x, y):
        """
        **SUMMARY**

        This function returns the gray value for a particular image pixel given a specific row and column.


        .. Warning::
          This function will always return pixels in RGB format even if the image is BGR format.

        **PARAMETERS**

        * *x* - Int the x pixel coordinate.
        * *y* - Int the y pixel coordinate.

        **RETURNS**

        A gray value integer between 0 and 255.

        **EXAMPLE**

        >>> img = Image(logo)
        >>> color = img.getGrayPixel(10,10)


        .. Warning::
          We suggest that this method be used sparingly. For repeated pixel access use python array notation. I.e. img[x][y].

        """
        retVal = None
        if( x < 0 or x >= self.width ):
            logger.warning("getGrayPixel: X value is not valid.")
        elif( y < 0 or y >= self.height ):
            logger.warning("getGrayPixel: Y value is not valid.")
        else:
            retVal = cv.Get2D(self._getGrayscaleBitmap(), y, x)
            retVal = retVal[0]
        return retVal


    def getVertScanline(self, column):
        """
        **SUMMARY**

        This function returns a single column of RGB values from the image as a numpy array. This is handy if you
        want to crawl the image looking for an edge.

        **PARAMETERS**

        * *column* - the column number working from left=0 to right=img.width.

        **RETURNS**

        A numpy array of the pixel values. Ususally this is in BGR format.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> myColor = [0,0,0]
        >>> sl = img.getVertScanline(423)
        >>> sll = sl.tolist()
        >>> for p in sll:
        >>>    if( p == myColor ):
        >>>        # do something

        **SEE ALSO**

        :py:meth:`getHorzScanlineGray`
        :py:meth:`getHorzScanline`
        :py:meth:`getVertScanlineGray`
        :py:meth:`getVertScanline`

        """
        retVal = None
        if( column < 0 or column >= self.width ):
            logger.warning("getVertRGBScanline: column value is not valid.")
        else:
            retVal = cv.GetCol(self.getBitmap(), column)
            retVal = np.array(retVal)
            retVal = retVal[:, 0, :]
        return retVal


    def getHorzScanline(self, row):
        """
        **SUMMARY**

        This function returns a single row of RGB values from the image.
        This is handy if you want to crawl the image looking for an edge.

        **PARAMETERS**

        * *row* - the row number working from top=0 to bottom=img.height.

        **RETURNS**

        A a lumpy numpy array of the pixel values. Ususally this is in BGR format.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> myColor = [0,0,0]
        >>> sl = img.getHorzScanline(422)
        >>> sll = sl.tolist()
        >>> for p in sll:
        >>>    if( p == myColor ):
        >>>        # do something

        **SEE ALSO**

        :py:meth:`getHorzScanlineGray`
        :py:meth:`getVertScanlineGray`
        :py:meth:`getVertScanline`

        """
        retVal = None
        if( row < 0 or row >= self.height ):
            logger.warning("getHorzRGBScanline: row value is not valid.")
        else:
            retVal = cv.GetRow(self.getBitmap(), row)
            retVal = np.array(retVal)
            retVal = retVal[0, :, :]
        return retVal


    def getVertScanlineGray(self, column):
        """
        **SUMMARY**

        This function returns a single column of gray values from the image as a numpy array. This is handy if you
        want to crawl the image looking for an edge.

        **PARAMETERS**

        * *column* - the column number working from left=0 to right=img.width.

        **RETURNS**

        A a lumpy numpy array of the pixel values.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> myColor = [255]
        >>> sl = img.getVertScanlineGray(421)
        >>> sll = sl.tolist()
        >>> for p in sll:
        >>>    if( p == myColor ):
        >>>        # do something

        **SEE ALSO**

        :py:meth:`getHorzScanlineGray`
        :py:meth:`getHorzScanline`
        :py:meth:`getVertScanline`

        """
        retVal = None
        if( column < 0 or column >= self.width ):
            logger.warning("getHorzRGBScanline: row value is not valid.")
        else:
            retVal = cv.GetCol(self._getGrayscaleBitmap(), column )
            retVal = np.array(retVal)
            #retVal = retVal.transpose()
        return retVal


    def getHorzScanlineGray(self, row):
        """
        **SUMMARY**

        This function returns a single row of gray values from the image as a numpy array. This is handy if you
        want to crawl the image looking for an edge.

        **PARAMETERS**

        * *row* - the row number working from top=0 to bottom=img.height.

        **RETURNS**

        A a lumpy numpy array of the pixel values.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> myColor = [255]
        >>> sl = img.getHorzScanlineGray(420)
        >>> sll = sl.tolist()
        >>> for p in sll:
        >>>    if( p == myColor ):
        >>>        # do something

        **SEE ALSO**

        :py:meth:`getHorzScanlineGray`
        :py:meth:`getHorzScanline`
        :py:meth:`getVertScanlineGray`
        :py:meth:`getVertScanline`

        """
        retVal = None
        if( row < 0 or row >= self.height ):
            logger.warning("getHorzRGBScanline: row value is not valid.")
        else:
            retVal = cv.GetRow(self._getGrayscaleBitmap(), row )
            retVal = np.array(retVal)
            retVal = retVal.transpose()
        return retVal


    def crop(self, x , y = None, w = None, h = None, centered=False, smart=False):
        """
        
        **SUMMARY**
        
        Consider you want to crop a image with the following dimension::

            (x,y)
            +--------------+
            |              |
            |              |h
            |              |
            +--------------+
                  w      (x1,y1)
        

        Crop attempts to use the x and y position variables and the w and h width
        and height variables to crop the image. When centered is false, x and y
        define the top and left of the cropped rectangle. When centered is true
        the function uses x and y as the centroid of the cropped region.

        You can also pass a feature into crop and have it automatically return
        the cropped image within the bounding outside area of that feature

        Or parameters can be in the form of a
         - tuple or list : (x,y,w,h) or [x,y,w,h]
         - two points : (x,y),(x1,y1) or [(x,y),(x1,y1)]

        **PARAMETERS**

        * *x* - An integer or feature.
              - If it is a feature we crop to the features dimensions.
              - This can be either the top left corner of the image or the center cooridnate of the the crop region.
              - or in the form of tuple/list. i,e (x,y,w,h) or [x,y,w,h]
              - Otherwise in two point form. i,e [(x,y),(x1,y1)] or (x,y)
        * *y* - The y coordinate of the center, or top left corner  of the crop region.
              - Otherwise in two point form. i,e (x1,y1)
        * *w* - Int - the width of the cropped region in pixels.
        * *h* - Int - the height of the cropped region in pixels.
        * *centered*  - Boolean - if True we treat the crop region as being the center
          coordinate and a width and height. If false we treat it as the top left corner of the crop region.
        * *smart* - Will make sure you don't try and crop outside the image size, so if your image is 100x100 and you tried a crop like img.crop(50,50,100,100), it will autoscale the crop to the max width.
        

        **RETURNS**

        A SimpleCV Image cropped to the specified width and height.

        **EXAMPLE**

        >>> img = Image('lenna')
        >>> img.crop(50,40,128,128).show()
        >>> img.crop((50,40,128,128)).show() #roi
        >>> img.crop([50,40,128,128]) #roi
        >>> img.crop((50,40),(178,168)) # two point form
        >>> img.crop([(50,40),(178,168)]) # two point form
        >>> img.crop([x1,x2,x3,x4,x5],[y1,y1,y3,y4,y5]) # list of x's and y's
        >>> img.crop([(x,y),(x,y),(x,y),(x,y),(x,y)] # list of (x,y)
        >>> img.crop(x,y,100,100, smart=True)
        **SEE ALSO**

        :py:meth:`embiggen`
        :py:meth:`regionSelect`
        """

        if smart:
          if x > self.width:
            x = self.width
          elif x < 0:
            x = 0
          elif y > self.height:
            y = self.height
          elif y < 0:
            y = 0
          elif (x + w) > self.width:
            w = self.width - x
          elif (y + h) > self.height:
            h = self.height - y
          
        if(isinstance(x,np.ndarray)):
            x = x.tolist()
        if(isinstance(y,np.ndarray)):
            y = y.tolist()

        #If it's a feature extract what we need
        if(isinstance(x, Feature)):
            theFeature = x
            x = theFeature.points[0][0]
            y = theFeature.points[0][1]
            w = theFeature.width()
            h = theFeature.height()

        elif(isinstance(x, (tuple,list)) and len(x) == 4 and isinstance(x[0],(int, long, float))
             and y == None and w == None and h == None):
                x,y,w,h = x
        # x of the form [(x,y),(x1,y1),(x2,y2),(x3,y3)]
        # x of the form [[x,y],[x1,y1],[x2,y2],[x3,y3]]
        # x of the form ([x,y],[x1,y1],[x2,y2],[x3,y3])
        # x of the form ((x,y),(x1,y1),(x2,y2),(x3,y3))
        # x of the form (x,y,x1,y2) or [x,y,x1,y2]            
        elif( isinstance(x, (list,tuple)) and
              isinstance(x[0],(list,tuple)) and
              (len(x) == 4 and len(x[0]) == 2 ) and
              y == None and w == None and h == None):
            if (len(x[0])==2 and len(x[1])==2 and len(x[2])==2 and len(x[3])==2):
                xmax = np.max([x[0][0],x[1][0],x[2][0],x[3][0]])
                ymax = np.max([x[0][1],x[1][1],x[2][1],x[3][1]])
                xmin = np.min([x[0][0],x[1][0],x[2][0],x[3][0]])
                ymin = np.min([x[0][1],x[1][1],x[2][1],x[3][1]])
                x = xmin
                y = ymin
                w = xmax-xmin
                h = ymax-ymin
            else:
                logger.warning("x should be in the form  ((x,y),(x1,y1),(x2,y2),(x3,y3))")
                return None
 
        # x,y of the form [x1,x2,x3,x4,x5....] and y similar
        elif(isinstance(x, (tuple,list)) and
             isinstance(y, (tuple,list)) and
             len(x) > 4 and len(y) > 4 ):
            if(isinstance(x[0],(int, long, float)) and isinstance(y[0],(int, long, float))):
                xmax = np.max(x)
                ymax = np.max(y)
                xmin = np.min(x)
                ymin = np.min(y)
                x = xmin
                y = ymin
                w = xmax-xmin
                h = ymax-ymin
            else:
                logger.warning("x should be in the form x = [1,2,3,4,5] y =[0,2,4,6,8]")
                return None

        # x of the form [(x,y),(x,y),(x,y),(x,y),(x,y),(x,y)]
        elif(isinstance(x, (list,tuple)) and
             len(x) > 4 and len(x[0]) == 2 and y == None and w == None and h == None):
            if(isinstance(x[0][0],(int, long, float))):
                xs = [pt[0] for pt in x]
                ys = [pt[1] for pt in x]
                xmax = np.max(xs)
                ymax = np.max(ys)
                xmin = np.min(xs)
                ymin = np.min(ys)
                x = xmin
                y = ymin
                w = xmax-xmin
                h = ymax-ymin
            else:
                logger.warning("x should be in the form [(x,y),(x,y),(x,y),(x,y),(x,y),(x,y)]")
                return None

        # x of the form [(x,y),(x1,y1)]
        elif(isinstance(x,(list,tuple)) and len(x) == 2 and isinstance(x[0],(list,tuple)) and isinstance(x[1],(list,tuple)) and y == None and w == None and h == None):
            if (len(x[0])==2 and len(x[1])==2):
                xt = np.min([x[0][0],x[1][0]])
                yt = np.min([x[0][0],x[1][0]])
                w = np.abs(x[0][0]-x[1][0])
                h = np.abs(x[0][1]-x[1][1])
                x = xt
                y = yt
            else:
                logger.warning("x should be in the form [(x1,y1),(x2,y2)]")
                return None

        # x and y of the form (x,y),(x1,y2)
        elif(isinstance(x, (tuple,list)) and isinstance(y,(tuple,list)) and w == None and h == None):
            if (len(x)==2 and len(y)==2):
                xt = np.min([x[0],y[0]])
                yt = np.min([x[1],y[1]])
                w = np.abs(y[0]-x[0])
                h = np.abs(y[1]-x[1])
                x = xt
                y = yt
                
            else:
                logger.warning("if x and y are tuple it should be in the form (x1,y1) and (x2,y2)")
                return None



        if(y == None or w == None or h == None):
            print "Please provide an x, y, width, height to function"

        if( w <= 0 or h <= 0 ):
            logger.warning("Can't do a negative crop!")
            return None

        retVal = cv.CreateImage((int(w),int(h)), cv.IPL_DEPTH_8U, 3)
        if( x < 0 or y < 0 ):
            logger.warning("Crop will try to help you, but you have a negative crop position, your width and height may not be what you want them to be.")


        if( centered ):
            rectangle = (int(x-(w/2)), int(y-(h/2)), int(w), int(h))
        else:
            rectangle = (int(x), int(y), int(w), int(h))

        (topROI, bottomROI) = self._rectOverlapROIs((rectangle[2],rectangle[3]),(self.width,self.height),(rectangle[0],rectangle[1]))

        if( bottomROI is None ):
            logger.warning("Hi, your crop rectangle doesn't even overlap your image. I have no choice but to return None.")
            return None

        retVal = np.zeros((bottomROI[3],bottomROI[2],3),dtype='uint8')

        retVal= self.getNumpyCv2()[bottomROI[1]:bottomROI[1] + bottomROI[3],bottomROI[0]:bottomROI[0] + bottomROI[2],:] 
        
        img = Image(retVal, colorSpace=self._colorSpace,cv2image = True)

        #Buffering the top left point (x, y) in a image.
        img._uncroppedX = self._uncroppedX + int(x)
        img._uncroppedY = self._uncroppedY + int(y)
        return img

    def regionSelect(self, x1, y1, x2, y2 ):
        """
        **SUMMARY**

        Region select is similar to crop, but instead of taking a position and width
        and height values it simply takes two points on the image and returns the selected
        region. This is very helpful for creating interactive scripts that require
        the user to select a region.

        **PARAMETERS**

        * *x1* - Int - Point one x coordinate.
        * *y1* - Int  - Point one y coordinate.
        * *x2* - Int  - Point two x coordinate.
        * *y2* - Int  - Point two y coordinate.

        **RETURNS**

        A cropped SimpleCV Image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> subreg = img.regionSelect(10,10,100,100) # often this comes from a mouse click
        >>> subreg.show()

        **SEE ALSO**

        :py:meth:`crop`

        """
        w = abs(x1-x2)
        h = abs(y1-y2)


        retVal = None
        if( w <= 0 or h <= 0 or w > self.width or h > self.height ):
            logger.warning("regionSelect: the given values will not fit in the image or are too small.")
        else:
            xf = x2
            if( x1 < x2 ):
                xf = x1
            yf = y2
            if( y1 < y2 ):
                yf = y1
            retVal = self.crop(xf, yf, w, h)


        return retVal


    def clear(self):
        """
        **SUMMARY**

        This is a slightly unsafe method that clears out the entire image state
        it is usually used in conjunction with the drawing blobs to fill in draw
        a single large blob in the image.

        .. Warning:
          Do not use this method unless you have a particularly compelling reason.

        """
        cv.SetZero(self._bitmap)
        self._clearBuffers()

    def draw(self, features, color=Color.GREEN, width=1, autocolor=False):
        """
        **SUMMARY**

        This is a method to draw Features on any given image.

        **PARAMETERS**

        * *features* - FeatureSet or any Feature (eg. Line, Circle, Corner, etc)
        * *color*    - Color of the Feature to be drawn
        * *width*    - width of the Feature to be drawn
        * *autocolor*- If true a color is randomly selected for each feature

        **RETURNS**
        None

        **EXAMPLE**

        img = Image("lenna")
        lines = img.equalize().findLines()
        img.draw(lines)
        img.show()
        """
        if type(features) == type(self):
            warnings.warn("You need to pass drawable features.")
            return None
        if hasattr(features, 'draw'):
            from copy import deepcopy
            if isinstance(features, FeatureSet):
                cfeatures = deepcopy(features)
                for cfeat in cfeatures:
                    cfeat.image = self
                cfeatures.draw(color, width, autocolor)
            else:
                cfeatures = deepcopy(features)
                cfeatures.image = self
                cfeatures.draw(color, width)
        else:
            warnings.warn("You need to pass drawable features.")
        return None

    def drawText(self, text = "", x = None, y = None, color = Color.BLUE, fontsize = 16):
        """
        **SUMMARY**

        This function draws the string that is passed on the screen at the specified coordinates.

        The Default Color is blue but you can pass it various colors

        The text will default to the center of the screen if you don't pass it a value


        **PARAMETERS**

        * *text* - String - the text you want to write. ASCII only please.
        * *x* - Int - the x position in pixels.
        * *y* - Int - the y position in pixels.
        * *color* - Color object or Color Tuple
        * *fontsize* - Int - the font size - roughly in points.

        **RETURNS**

        Nothing. This is an in place function. Text is added to the Images drawing layer.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> img.drawText("xamox smells like cool ranch doritos.", 50,50,color=Color.BLACK,fontsize=48)
        >>> img.show()

        **SEE ALSO**

        :py:meth:`dl`
        :py:meth:`drawCircle`
        :py:meth:`drawRectangle`

        """
        if(x == None):
            x = (self.width / 2)
        if(y == None):
            y = (self.height / 2)


        self.getDrawingLayer().setFontSize(fontsize)
        self.getDrawingLayer().text(text, (x, y), color)


    def drawRectangle(self,x,y,w,h,color=Color.RED,width=1,alpha=255):
        """
        **SUMMARY**

        Draw a rectangle on the screen given the upper left corner of the rectangle
        and the width and height.

        **PARAMETERS**

        * *x* - the x position.
        * *y* - the y position.
        * *w* - the width of the rectangle.
        * *h* - the height of the rectangle.
        * *color* - an RGB tuple indicating the desired color.
        * *width* - the width of the rectangle, a value less than or equal to zero means filled in completely.
        * *alpha* - the alpha value on the interval from 255 to 0, 255 is opaque, 0 is completely transparent.

        **RETURNS**

        None - this operation is in place and adds the rectangle to the drawing layer.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> img.drawREctange( 50,50,100,123)
        >>> img.show()

        **SEE ALSO**

        :py:meth:`dl`
        :py:meth:`drawCircle`
        :py:meth:`drawRectangle`
        :py:meth:`applyLayers`
        :py:class:`DrawingLayer`

        """
        if( width < 1 ):
            self.getDrawingLayer().rectangle((x,y),(w,h),color,filled=True,alpha=alpha)
        else:
            self.getDrawingLayer().rectangle((x,y),(w,h),color,width,alpha=alpha)

    def drawRotatedRectangle(self,boundingbox,color=Color.RED,width=1):
        """
        **SUMMARY**

        Draw the minimum bouding rectangle. This rectangle is a series of four points.

        **TODO**

        **KAT FIX THIS**
        """

        cv.EllipseBox(self.getBitmap(),box=boundingbox,color=color,thicness=width)


    def show(self, type = 'window'):
        """
        **SUMMARY**

        This function automatically pops up a window and shows the current image.

        **PARAMETERS**

        * *type* - this string can have one of two values, either 'window', or 'browser'. Window opens
          a display window, while browser opens the default web browser to show an image.

        **RETURNS**

        This method returns the display object. In the case of the window this is a JpegStreamer
        object. In the case of a window a display object is returned.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> img.show()
        >>> img.show('browser')

        **SEE ALSO**

        :py:class:`JpegStreamer`
        :py:class:`Display`

        """

        if(type == 'browser'):
            import webbrowser
            js = JpegStreamer(8080)
            self.save(js)
            webbrowser.open("http://localhost:8080", 2)
            return js
        elif (type == 'window'):
            from SimpleCV.Display import Display
            if init_options_handler.on_notebook:
                d = Display(displaytype='notebook')
            else:
                d = Display(self.size())
            self.save(d)
            return d
        else:
            print "Unknown type to show"

    def _surface2Image(self,surface):
        imgarray = pg.surfarray.array3d(surface)
        retVal = Image(imgarray)
        retVal._colorSpace = ColorSpace.RGB
        return retVal.toBGR().transpose()

    def _image2Surface(self,img):
        return pg.image.fromstring(img.getPIL().tostring(),img.size(), "RGB")
        #return pg.surfarray.make_surface(img.toRGB().getNumpy())

    def toPygameSurface(self):
        """
        **SUMMARY**

        Converts this image to a pygame surface. This is useful if you want
        to treat an image as a sprite to render onto an image. An example
        would be rendering blobs on to an image.

        .. Warning::
          *THIS IS EXPERIMENTAL*. We are plannng to remove this functionality sometime in the near future.

        **RETURNS**

        The image as a pygame surface.

        **SEE ALSO**


        :py:class:`DrawingLayer`
        :py:meth:`insertDrawingLayer`
        :py:meth:`addDrawingLayer`
        :py:meth:`dl`
        :py:meth:`toPygameSurface`
        :py:meth:`getDrawingLayer`
        :py:meth:`removeDrawingLayer`
        :py:meth:`clearLayers`
        :py:meth:`layers`
        :py:meth:`mergedLayers`
        :py:meth:`applyLayers`
        :py:meth:`drawText`
        :py:meth:`drawRectangle`
        :py:meth:`drawCircle`
        :py:meth:`blit`

        """
        return pg.image.fromstring(self.getPIL().tostring(),self.size(), "RGB")


    def addDrawingLayer(self, layer = None):
        """
        **SUMMARY**

        Push a new drawing layer onto the back of the layer stack

        **PARAMETERS**

        * *layer* - The new drawing layer to add.

        **RETURNS**

        The index of the new layer as an integer.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> myLayer = DrawingLayer((img.width,img.height))
        >>> img.addDrawingLayer(myLayer)

        **SEE ALSO**

        :py:class:`DrawingLayer`
        :py:meth:`insertDrawinglayer`
        :py:meth:`addDrawinglayer`
        :py:meth:`dl`
        :py:meth:`toPygameSurface`
        :py:meth:`getDrawingLayer`
        :py:meth:`removeDrawingLayer`
        :py:meth:`clearLayers`
        :py:meth:`layers`
        :py:meth:`mergedLayers`
        :py:meth:`applyLayers`
        :py:meth:`drawText`
        :py:meth:`drawRectangle`
        :py:meth:`drawCircle`
        :py:meth:`blit`

        """

        if not isinstance(layer, DrawingLayer):
            return "Please pass a DrawingLayer object"

        if not layer:
            layer = DrawingLayer(self.size())
        self._mLayers.append(layer)
        return len(self._mLayers)-1


    def insertDrawingLayer(self, layer, index):
        """
        **SUMMARY**

        Insert a new layer into the layer stack at the specified index.

        **PARAMETERS**

        * *layer* - A drawing layer with crap you want to draw.
        * *index* - The index at which to insert the layer.

        **RETURNS**

        None - that's right - nothing.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> myLayer1 = DrawingLayer((img.width,img.height))
        >>> myLayer2 = DrawingLayer((img.width,img.height))
        >>> #Draw on the layers
        >>> img.insertDrawingLayer(myLayer1,1) # on top
        >>> img.insertDrawingLayer(myLayer2,2) # on the bottom


        **SEE ALSO**

        :py:class:`DrawingLayer`
        :py:meth:`addDrawinglayer`
        :py:meth:`dl`
        :py:meth:`toPygameSurface`
        :py:meth:`getDrawingLayer`
        :py:meth:`removeDrawingLayer`
        :py:meth:`clearLayers`
        :py:meth:`layers`
        :py:meth:`mergedLayers`
        :py:meth:`applyLayers`
        :py:meth:`drawText`
        :py:meth:`drawRectangle`
        :py:meth:`drawCircle`
        :py:meth:`blit`

        """
        self._mLayers.insert(index, layer)
        return None


    def removeDrawingLayer(self, index = -1):
        """
        **SUMMARY**

        Remove a layer from the layer stack based on the layer's index.

        **PARAMETERS**

        * *index* - Int - the index of the layer to remove.

        **RETURNS**

        This method returns the removed drawing layer.

        **EXAMPLES**

        >>> img = Image("Lenna")
        >>> img.removeDrawingLayer(1) # removes the layer with index = 1
        >>> img.removeDrawingLayer() # if no index is specified it removes the top layer


        **SEE ALSO**

        :py:class:`DrawingLayer`
        :py:meth:`addDrawinglayer`
        :py:meth:`dl`
        :py:meth:`toPygameSurface`
        :py:meth:`getDrawingLayer`
        :py:meth:`removeDrawingLayer`
        :py:meth:`clearLayers`
        :py:meth:`layers`
        :py:meth:`mergedLayers`
        :py:meth:`applyLayers`
        :py:meth:`drawText`
        :py:meth:`drawRectangle`
        :py:meth:`drawCircle`
        :py:meth:`blit`

        """
        try:
            return self._mLayers.pop(index)
        except IndexError:
            print 'Not a valid index or No layers to remove!'


    def getDrawingLayer(self, index = -1):
        """
        **SUMMARY**

        Return a drawing layer based on the provided index.  If not provided, will
        default to the top layer.  If no layers exist, one will be created

        **PARAMETERS**

        * *index* - returns the drawing layer at the specified index.

        **RETURNS**

        A drawing layer.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> myLayer1 = DrawingLayer((img.width,img.height))
        >>> myLayer2 = DrawingLayer((img.width,img.height))
        >>> #Draw on the layers
        >>> img.insertDrawingLayer(myLayer1,1) # on top
        >>> img.insertDrawingLayer(myLayer2,2) # on the bottom
        >>> layer2 =img.getDrawingLayer(2)

        **SEE ALSO**

        :py:class:`DrawingLayer`
        :py:meth:`addDrawinglayer`
        :py:meth:`dl`
        :py:meth:`toPygameSurface`
        :py:meth:`getDrawingLayer`
        :py:meth:`removeDrawingLayer`
        :py:meth:`clearLayers`
        :py:meth:`layers`
        :py:meth:`mergedLayers`
        :py:meth:`applyLayers`
        :py:meth:`drawText`
        :py:meth:`drawRectangle`
        :py:meth:`drawCircle`
        :py:meth:`blit`

        """
        if not len(self._mLayers):
            layer = DrawingLayer(self.size())
            self.addDrawingLayer(layer)
        try:
            return self._mLayers[index]
        except IndexError:
            print 'Not a valid index'


    def dl(self, index = -1):
        """
        **SUMMARY**

        Alias for :py:meth:`getDrawingLayer`

        """
        return self.getDrawingLayer(index)


    def clearLayers(self):
        """
        **SUMMARY**

        Remove all of the drawing layers.

        **RETURNS**

        None.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> myLayer1 = DrawingLayer((img.width,img.height))
        >>> myLayer2 = DrawingLayer((img.width,img.height))
        >>> img.insertDrawingLayer(myLayer1,1) # on top
        >>> img.insertDrawingLayer(myLayer2,2) # on the bottom
        >>> img.clearLayers()

        **SEE ALSO**

        :py:class:`DrawingLayer`
        :py:meth:`dl`
        :py:meth:`toPygameSurface`
        :py:meth:`getDrawingLayer`
        :py:meth:`removeDrawingLayer`
        :py:meth:`layers`
        :py:meth:`mergedLayers`
        :py:meth:`applyLayers`
        :py:meth:`drawText`
        :py:meth:`drawRectangle`
        :py:meth:`drawCircle`
        :py:meth:`blit`

        """
        for i in self._mLayers:
            self._mLayers.remove(i)


        return None

    def layers(self):
        """
        **SUMMARY**

        Return the array of DrawingLayer objects associated with the image.

        **RETURNS**

        A list of of drawing layers.

        **SEE ALSO**

        :py:class:`DrawingLayer`
        :py:meth:`addDrawingLayer`
        :py:meth:`dl`
        :py:meth:`toPygameSurface`
        :py:meth:`getDrawingLayer`
        :py:meth:`removeDrawingLayer`
        :py:meth:`mergedLayers`
        :py:meth:`applyLayers`
        :py:meth:`drawText`
        :py:meth:`drawRectangle`
        :py:meth:`drawCircle`
        :py:meth:`blit`

        """
        return self._mLayers


        #render the image.

    def _renderImage(self, layer):
        imgSurf = self.getPGSurface(self).copy()
        imgSurf.blit(layer._mSurface, (0, 0))
        return Image(imgSurf)

    def mergedLayers(self):
        """
        **SUMMARY**

        Return all DrawingLayer objects as a single DrawingLayer.

        **RETURNS**

        Returns a drawing layer with all of the drawing layers of this image merged into one.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> myLayer1 = DrawingLayer((img.width,img.height))
        >>> myLayer2 = DrawingLayer((img.width,img.height))
        >>> img.insertDrawingLayer(myLayer1,1) # on top
        >>> img.insertDrawingLayer(myLayer2,2) # on the bottom
        >>> derp = img.mergedLayers()

        **SEE ALSO**

        :py:class:`DrawingLayer`
        :py:meth:`addDrawingLayer`
        :py:meth:`dl`
        :py:meth:`toPygameSurface`
        :py:meth:`getDrawingLayer`
        :py:meth:`removeDrawingLayer`
        :py:meth:`layers`
        :py:meth:`applyLayers`
        :py:meth:`drawText`
        :py:meth:`drawRectangle`
        :py:meth:`drawCircle`
        :py:meth:`blit`

        """
        final = DrawingLayer(self.size())
        for layers in self._mLayers: #compose all the layers
            layers.renderToOtherLayer(final)
        return final

    def applyLayers(self, indicies=-1):
        """
        **SUMMARY**

        Render all of the layers onto the current image and return the result.
        Indicies can be a list of integers specifying the layers to be used.

        **PARAMETERS**

        * *indicies* -  Indicies can be a list of integers specifying the layers to be used.

        **RETURNS**

        The image after applying the drawing layers.

        **EXAMPLE**

        >>> img = Image("Lenna")
        >>> myLayer1 = DrawingLayer((img.width,img.height))
        >>> myLayer2 = DrawingLayer((img.width,img.height))
        >>> #Draw some stuff
        >>> img.insertDrawingLayer(myLayer1,1) # on top
        >>> img.insertDrawingLayer(myLayer2,2) # on the bottom
        >>> derp = img.applyLayers()

        **SEE ALSO**

        :py:class:`DrawingLayer`
        :py:meth:`dl`
        :py:meth:`toPygameSurface`
        :py:meth:`getDrawingLayer`
        :py:meth:`removeDrawingLayer`
        :py:meth:`layers`
        :py:meth:`drawText`
        :py:meth:`drawRectangle`
        :py:meth:`drawCircle`
        :py:meth:`blit`

        """
        if not len(self._mLayers):
            return self

        if(indicies==-1 and len(self._mLayers) > 0 ):
            final = self.mergedLayers()
            imgSurf = self.getPGSurface().copy()
            imgSurf.blit(final._mSurface, (0, 0))
            return Image(imgSurf)
        else:
            final = DrawingLayer((self.width, self.height))
            retVal = self
            indicies.reverse()
            for idx in indicies:
                retVal = self._mLayers[idx].renderToOtherLayer(final)
            imgSurf = self.getPGSurface().copy()
            imgSurf.blit(final._mSurface, (0, 0))
            indicies.reverse()
            return Image(imgSurf)

    def adaptiveScale(self, resolution,fit=True):
        """
        **SUMMARY**

        Adapative Scale is used in the Display to automatically
        adjust image size to match the display size. This method attempts to scale
        an image to the desired resolution while keeping the aspect ratio the same.
        If fit is False we simply crop and center the image to the resolution.
        In general this method should look a lot better than arbitrary cropping and scaling.

        **PARAMETERS**

        * *resolution* - The size of the returned image as a (width,height) tuple.
        * *fit* - If fit is true we try to fit the image while maintaining the aspect ratio.
          If fit is False we crop and center the image to fit the resolution.

        **RETURNS**

        A SimpleCV Image.

        **EXAMPLE**

        This is typically used in this instance:

        >>> d = Display((800,600))
        >>> i = Image((640, 480))
        >>> i.save(d)

        Where this would scale the image to match the display size of 800x600

        """

        wndwAR = float(resolution[0])/float(resolution[1])
        imgAR = float(self.width)/float(self.height)
        img = self
        targetx = 0
        targety = 0
        targetw = resolution[0]
        targeth = resolution[1]
        if( self.size() == resolution): # we have to resize
            retVal = self
        elif( imgAR == wndwAR and fit):
            retVal = img.scale(resolution[0],resolution[1])
            return retVal
        elif(fit):
            #scale factors
            retVal = np.zeros((resolution[1],resolution[0],3),dtype='uint8')
            wscale = (float(self.width)/float(resolution[0]))
            hscale = (float(self.height)/float(resolution[1]))
            if(wscale>1): #we're shrinking what is the percent reduction
                wscale=1-(1.0/wscale)
            else: # we need to grow the image by a percentage
                wscale = 1.0-wscale
            if(hscale>1):
                hscale=1-(1.0/hscale)
            else:
                hscale=1.0-hscale
            if( wscale == 0 ): #if we can get away with not scaling do that
                targetx = 0
                targety = (resolution[1]-self.height)/2
                targetw = img.width
                targeth = img.height
            elif( hscale == 0 ): #if we can get away with not scaling do that
                targetx = (resolution[0]-img.width)/2
                targety = 0
                targetw = img.width
                targeth = img.height
            elif(wscale < hscale): # the width has less distortion
                sfactor = float(resolution[0])/float(self.width)
                targetw = int(float(self.width)*sfactor)
                targeth = int(float(self.height)*sfactor)
                if( targetw > resolution[0] or targeth > resolution[1]):
                    #aw shucks that still didn't work do the other way instead
                    sfactor = float(resolution[1])/float(self.height)
                    targetw = int(float(self.width)*sfactor)
                    targeth = int(float(self.height)*sfactor)
                    targetx = (resolution[0]-targetw)/2
                    targety = 0
                else:
                    targetx = 0
                    targety = (resolution[1]-targeth)/2
                img = img.scale(targetw,targeth)
            else: #the height has more distortion
                sfactor = float(resolution[1])/float(self.height)
                targetw = int(float(self.width)*sfactor)
                targeth = int(float(self.height)*sfactor)
                if( targetw > resolution[0] or targeth > resolution[1]):
                    #aw shucks that still didn't work do the other way instead
                    sfactor = float(resolution[0])/float(self.width)
                    targetw = int(float(self.width)*sfactor)
                    targeth = int(float(self.height)*sfactor)
                    targetx = 0
                    targety = (resolution[1]-targeth)/2
                else:
                    targetx = (resolution[0]-targetw)/2
                    targety = 0
                img = img.scale(targetw,targeth)
        
        else: # we're going to crop instead
            if(self.width <= resolution[0] and self.height <= resolution[1] ): # center a too small image
                #we're too small just center the thing
                retVal = np.zeros((resolution[1],resolution[0],3),dtype='uint8')
                targetx = (resolution[0]/2)-(self.width/2)
                targety = (resolution[1]/2)-(self.height/2)
                targeth = self.height
                targetw = self.width
            elif(self.width > resolution[0] and self.height > resolution[1]): #crop too big on both axes
                targetw = resolution[0]
                targeth = resolution[1]
                targetx = 0
                targety = 0
                x = (self.width-resolution[0])/2
                y = (self.height-resolution[1])/2
                img = img.crop(x,y,targetw,targeth)
                return img
            elif( self.width <= resolution[0] and self.height > resolution[1]): #height too big
                #crop along the y dimension and center along the x dimension
                retVal = np.zeros((resolution[1],resolution[0],3),dtype='uint8')
                targetw = self.width
                targeth = resolution[1]
                targetx = (resolution[0]-self.width)/2
                targety = 0
                x = 0
                y = (self.height-resolution[1])/2
                img = img.crop(x,y,targetw,targeth)

            elif( self.width > resolution[0] and self.height <= resolution[1]): #width too big
                #crop along the y dimension and center along the x dimension
                retVal = np.zeros((resolution[1],resolution[0],3),dtype='uint8')
                targetw = resolution[0]
                targeth = self.height
                targetx = 0
                targety = (resolution[1]-self.height)/2
                x = (self.width-resolution[0])/2
                y = 0
                img = img.crop(x,y,targetw,targeth)
        
        retVal[targety:targety + targeth,targetx:targetx + targetw,:] = img.getNumpyCv2()
        retVal = Image(retVal,cv2image = True)
        return(retVal)


    def blit(self, img, pos=None,alpha=None,mask=None,alphaMask=None):
        """
        **SUMMARY**

        Blit aka bit blit - which in ye olden days was an acronym for bit-block transfer. In other words blit is
        when you want to smash two images together, or add one image to another. This method takes in a second
        SimpleCV image, and then allows you to add to some point on the calling image. A general blit command
        will just copy all of the image. You can also copy the image with an alpha value to the source image
        is semi-transparent. A binary mask can be used to blit non-rectangular image onto the souce image.
        An alpha mask can be used to do and arbitrarily transparent image to this image. Both the mask and
        alpha masks are SimpleCV Images.

        **PARAMETERS**

        * *img* - an image to place ontop of this image.
        * *pos* - an (x,y) position tuple of the top left corner of img on this image. Note that these values
          can be negative.
        * *alpha* - a single floating point alpha value (0=see the bottom image, 1=see just img, 0.5 blend the two 50/50).
        * *mask* - a binary mask the same size as the input image. White areas are blitted, black areas are not blitted.
        * *alphaMask* - an alpha mask where each grayscale value maps how much of each image is shown.

        **RETURNS**

        A SimpleCV Image. The size will remain the same.

        **EXAMPLE**

        >>> topImg = Image("top.png")
        >>> bottomImg = Image("bottom.png")
        >>> mask = Image("mask.png")
        >>> aMask = Image("alpphaMask.png")
        >>> bottomImg.blit(top,pos=(100,100)).show()
        >>> bottomImg.blit(top,alpha=0.5).show()
        >>> bottomImg.blit(top,pos=(100,100),mask=mask).show()
        >>> bottomImg.blit(top,pos=(-10,-10)alphaMask=aMask).show()

        **SEE ALSO**

        :py:meth:`createBinaryMask`
        :py:meth:`createAlphaMask`

        """
        retVal = Image(self.getEmpty())
        cv.Copy(self.getBitmap(),retVal.getBitmap())

        w = img.width
        h = img.height

        if( pos is None ):
            pos = (0,0)

        (topROI, bottomROI) = self._rectOverlapROIs((img.width,img.height),(self.width,self.height),pos)

        if( alpha is not None ):
            cv.SetImageROI(img.getBitmap(),topROI);
            cv.SetImageROI(retVal.getBitmap(),bottomROI);
            a = float(alpha)
            b = float(1.00-a)
            g = float(0.00)
            cv.AddWeighted(img.getBitmap(),a,retVal.getBitmap(),b,g,retVal.getBitmap())
            cv.ResetImageROI(img.getBitmap());
            cv.ResetImageROI(retVal.getBitmap());
        elif( alphaMask is not None ):
            if( alphaMask is not None and (alphaMask.width != img.width or alphaMask.height != img.height ) ):
                logger.warning("Image.blit: your mask and image don't match sizes, if the mask doesn't fit, you can not blit! Try using the scale function.")
                return None

            cImg = img.crop(topROI[0],topROI[1],topROI[2],topROI[3])
            cMask = alphaMask.crop(topROI[0],topROI[1],topROI[2],topROI[3])
            retValC = retVal.crop(bottomROI[0],bottomROI[1],bottomROI[2],bottomROI[3])
            r = cImg.getEmpty(1)
            g = cImg.getEmpty(1)
            b = cImg.getEmpty(1)
            cv.Split(cImg.getBitmap(), b, g, r, None)
            rf=cv.CreateImage((cImg.width,cImg.height),cv.IPL_DEPTH_32F,1)
            gf=cv.CreateImage((cImg.width,cImg.height),cv.IPL_DEPTH_32F,1)
            bf=cv.CreateImage((cImg.width,cImg.height),cv.IPL_DEPTH_32F,1)
            af=cv.CreateImage((cImg.width,cImg.height),cv.IPL_DEPTH_32F,1)
            cv.ConvertScale(r,rf)
            cv.ConvertScale(g,gf)
            cv.ConvertScale(b,bf)
            cv.ConvertScale(cMask._getGrayscaleBitmap(),af)
            cv.ConvertScale(af,af,scale=(1.0/255.0))
            cv.Mul(rf,af,rf)
            cv.Mul(gf,af,gf)
            cv.Mul(bf,af,bf)

            dr = retValC.getEmpty(1)
            dg = retValC.getEmpty(1)
            db = retValC.getEmpty(1)
            cv.Split(retValC.getBitmap(), db, dg, dr, None)
            drf=cv.CreateImage((retValC.width,retValC.height),cv.IPL_DEPTH_32F,1)
            dgf=cv.CreateImage((retValC.width,retValC.height),cv.IPL_DEPTH_32F,1)
            dbf=cv.CreateImage((retValC.width,retValC.height),cv.IPL_DEPTH_32F,1)
            daf=cv.CreateImage((retValC.width,retValC.height),cv.IPL_DEPTH_32F,1)
            cv.ConvertScale(dr,drf)
            cv.ConvertScale(dg,dgf)
            cv.ConvertScale(db,dbf)
            cv.ConvertScale(cMask.invert()._getGrayscaleBitmap(),daf)
            cv.ConvertScale(daf,daf,scale=(1.0/255.0))
            cv.Mul(drf,daf,drf)
            cv.Mul(dgf,daf,dgf)
            cv.Mul(dbf,daf,dbf)

            cv.Add(rf,drf,rf)
            cv.Add(gf,dgf,gf)
            cv.Add(bf,dbf,bf)

            cv.ConvertScaleAbs(rf,r)
            cv.ConvertScaleAbs(gf,g)
            cv.ConvertScaleAbs(bf,b)

            cv.Merge(b,g,r,None,retValC.getBitmap())
            cv.SetImageROI(retVal.getBitmap(),bottomROI)
            cv.Copy(retValC.getBitmap(),retVal.getBitmap())
            cv.ResetImageROI(retVal.getBitmap())

        elif( mask is not None):
            if( mask is not None and (mask.width != img.width or mask.height != img.height ) ):
                logger.warning("Image.blit: your mask and image don't match sizes, if the mask doesn't fit, you can not blit! Try using the scale function. ")
                return None
            cv.SetImageROI(img.getBitmap(),topROI)
            cv.SetImageROI(mask.getBitmap(),topROI)
            cv.SetImageROI(retVal.getBitmap(),bottomROI)
            cv.Copy(img.getBitmap(),retVal.getBitmap(),mask.getBitmap())
            cv.ResetImageROI(img.getBitmap())
            cv.ResetImageROI(mask.getBitmap())
            cv.ResetImageROI(retVal.getBitmap())
        else:  #vanilla blit
            cv.SetImageROI(img.getBitmap(),topROI)
            cv.SetImageROI(retVal.getBitmap(),bottomROI)
            cv.Copy(img.getBitmap(),retVal.getBitmap())
            cv.ResetImageROI(img.getBitmap())
            cv.ResetImageROI(retVal.getBitmap())

        return retVal

    def sideBySide(self, image, side="right", scale=True ):
        """
        **SUMMARY**

        Combine two images as a side by side images. Great for before and after images.

        **PARAMETERS**

        * *side* - what side of this image to place the other image on.
          choices are ('left'/'right'/'top'/'bottom').

        * *scale* - if true scale the smaller of the two sides to match the
          edge touching the other image. If false we center the smaller
          of the two images on the edge touching the larger image.

        **RETURNS**

        A new image that is a combination of the two images.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> img2 = Image("orson_welles.jpg")
        >>> img3 = img.sideBySide(img2)

        **TODO**

        Make this accept a list of images.

        """
        #there is probably a cleaner way to do this, but I know I hit every case when they are enumerated
        retVal = None
        if( side == "top" ):
            #clever
            retVal = image.sideBySide(self,"bottom",scale)
        elif( side == "bottom" ):
            if( self.width > image.width ):
                if( scale ):
                    #scale the other image width to fit
                    resized = image.resize(w=self.width)
                    nW = self.width
                    nH = self.height + resized.height
                    newCanvas = cv.CreateImage((nW,nH), cv.IPL_DEPTH_8U, 3)
                    cv.SetZero(newCanvas)
                    cv.SetImageROI(newCanvas,(0,0,nW,self.height))
                    cv.Copy(self.getBitmap(),newCanvas)
                    cv.ResetImageROI(newCanvas)
                    cv.SetImageROI(newCanvas,(0,self.height,resized.width,resized.height))
                    cv.Copy(resized.getBitmap(),newCanvas)
                    cv.ResetImageROI(newCanvas)
                    retVal = Image(newCanvas,colorSpace=self._colorSpace)
                else:
                    nW = self.width
                    nH = self.height + image.height
                    newCanvas = cv.CreateImage((nW,nH), cv.IPL_DEPTH_8U, 3)
                    cv.SetZero(newCanvas)
                    cv.SetImageROI(newCanvas,(0,0,nW,self.height))
                    cv.Copy(self.getBitmap(),newCanvas)
                    cv.ResetImageROI(newCanvas)
                    xc = (self.width-image.width)/2
                    cv.SetImageROI(newCanvas,(xc,self.height,image.width,image.height))
                    cv.Copy(image.getBitmap(),newCanvas)
                    cv.ResetImageROI(newCanvas)
                    retVal = Image(newCanvas,colorSpace=self._colorSpace)
            else: #our width is smaller than the other image
                if( scale ):
                    #scale the other image width to fit
                    resized = self.resize(w=image.width)
                    nW = image.width
                    nH = resized.height + image.height
                    newCanvas = cv.CreateImage((nW,nH), cv.IPL_DEPTH_8U, 3)
                    cv.SetZero(newCanvas)
                    cv.SetImageROI(newCanvas,(0,0,resized.width,resized.height))
                    cv.Copy(resized.getBitmap(),newCanvas)
                    cv.ResetImageROI(newCanvas)
                    cv.SetImageROI(newCanvas,(0,resized.height,nW,image.height))
                    cv.Copy(image.getBitmap(),newCanvas)
                    cv.ResetImageROI(newCanvas)
                    retVal = Image(newCanvas,colorSpace=self._colorSpace)
                else:
                    nW = image.width
                    nH = self.height + image.height
                    newCanvas = cv.CreateImage((nW,nH), cv.IPL_DEPTH_8U, 3)
                    cv.SetZero(newCanvas)
                    xc = (image.width - self.width)/2
                    cv.SetImageROI(newCanvas,(xc,0,self.width,self.height))
                    cv.Copy(self.getBitmap(),newCanvas)
                    cv.ResetImageROI(newCanvas)
                    cv.SetImageROI(newCanvas,(0,self.height,image.width,image.height))
                    cv.Copy(image.getBitmap(),newCanvas)
                    cv.ResetImageROI(newCanvas)
                    retVal = Image(newCanvas,colorSpace=self._colorSpace)

        elif( side == "right" ):
            retVal = image.sideBySide(self,"left",scale)
        else: #default to left
            if( self.height > image.height ):
                if( scale ):
                    #scale the other image height to fit
                    resized = image.resize(h=self.height)
                    nW = self.width + resized.width
                    nH = self.height
                    newCanvas = cv.CreateImage((nW,nH), cv.IPL_DEPTH_8U, 3)
                    cv.SetZero(newCanvas)
                    cv.SetImageROI(newCanvas,(0,0,resized.width,resized.height))
                    cv.Copy(resized.getBitmap(),newCanvas)
                    cv.ResetImageROI(newCanvas)
                    cv.SetImageROI(newCanvas,(resized.width,0,self.width,self.height))
                    cv.Copy(self.getBitmap(),newCanvas)
                    cv.ResetImageROI(newCanvas)
                    retVal = Image(newCanvas,colorSpace=self._colorSpace)
                else:
                    nW = self.width+image.width
                    nH = self.height
                    newCanvas = cv.CreateImage((nW,nH), cv.IPL_DEPTH_8U, 3)
                    cv.SetZero(newCanvas)
                    yc = (self.height-image.height)/2
                    cv.SetImageROI(newCanvas,(0,yc,image.width,image.height))
                    cv.Copy(image.getBitmap(),newCanvas)
                    cv.ResetImageROI(newCanvas)
                    cv.SetImageROI(newCanvas,(image.width,0,self.width,self.height))
                    cv.Copy(self.getBitmap(),newCanvas)
                    cv.ResetImageROI(newCanvas)
                    retVal = Image(newCanvas,colorSpace=self._colorSpace)
            else: #our height is smaller than the other image
                if( scale ):
                    #scale our height to fit
                    resized = self.resize(h=image.height)
                    nW = image.width + resized.width
                    nH = image.height
                    newCanvas = cv.CreateImage((nW,nH), cv.IPL_DEPTH_8U, 3)
                    cv.SetZero(newCanvas)
                    cv.SetImageROI(newCanvas,(0,0,image.width,image.height))
                    cv.Copy(image.getBitmap(),newCanvas)
                    cv.ResetImageROI(newCanvas)
                    cv.SetImageROI(newCanvas,(image.width,0,resized.width,resized.height))
                    cv.Copy(resized.getBitmap(),newCanvas)
                    cv.ResetImageROI(newCanvas)
                    retVal = Image(newCanvas,colorSpace=self._colorSpace)
                else:
                    nW = image.width + self.width
                    nH = image.height
                    newCanvas = cv.CreateImage((nW,nH), cv.IPL_DEPTH_8U, 3)
                    cv.SetZero(newCanvas)
                    cv.SetImageROI(newCanvas,(0,0,image.width,image.height))
                    cv.Copy(image.getBitmap(),newCanvas)
                    cv.ResetImageROI(newCanvas)
                    yc = (image.height-self.height)/2
                    cv.SetImageROI(newCanvas,(image.width,yc,self.width,self.height))
                    cv.Copy(self.getBitmap(),newCanvas)
                    cv.ResetImageROI(newCanvas)
                    retVal = Image(newCanvas,colorSpace=self._colorSpace)
        return retVal


    def embiggen(self, size=None, color=Color.BLACK, pos=None):
        """
        **SUMMARY**

        Make the canvas larger but keep the image the same size.

        **PARAMETERS**

        * *size* - width and heigt tuple of the new canvas or give a single vaule in which to scale the image size, for instance size=2 would make the image canvas twice the size

        * *color* - the color of the canvas

        * *pos* - the position of the top left corner of image on the new canvas,
          if none the image is centered.

        **RETURNS**

        The enlarged SimpleCV Image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> img = img.embiggen((1024,1024),color=Color.BLUE)
        >>> img.show()

        """

        if not isinstance(size, tuple) and size > 1:
          size = (self.width * size, self.height * size)
        

        if( size == None or size[0] < self.width or size[1] < self.height ):
            logger.warning("image.embiggenCanvas: the size provided is invalid")
            return None

        newCanvas = cv.CreateImage(size, cv.IPL_DEPTH_8U, 3)
        cv.SetZero(newCanvas)
        newColor = cv.RGB(color[0],color[1],color[2])
        cv.AddS(newCanvas,newColor,newCanvas)
        topROI = None
        bottomROI = None
        if( pos is None ):
            pos = (((size[0]-self.width)/2),((size[1]-self.height)/2))

        (topROI, bottomROI) = self._rectOverlapROIs((self.width,self.height),size,pos)
        if( topROI is None or bottomROI is None):
            logger.warning("image.embiggenCanvas: the position of the old image doesn't make sense, there is no overlap")
            return None

        cv.SetImageROI(newCanvas, bottomROI)
        cv.SetImageROI(self.getBitmap(),topROI)
        cv.Copy(self.getBitmap(),newCanvas)
        cv.ResetImageROI(newCanvas)
        cv.ResetImageROI(self.getBitmap())
        return Image(newCanvas)



    def _rectOverlapROIs(self,top, bottom, pos):
        """
        top is a rectangle (w,h)
        bottom is a rectangle (w,h)
        pos is the top left corner of the top rectangle with respect to the bottom rectangle's top left corner
        method returns none if the two rectangles do not overlap. Otherwise returns the top rectangle's ROI (x,y,w,h)
        and the bottom rectangle's ROI (x,y,w,h)
        """
        # the position of the top rect coordinates give bottom top right = (0,0)
        tr = (pos[0]+top[0],pos[1])
        tl = pos
        br = (pos[0]+top[0],pos[1]+top[1])
        bl = (pos[0],pos[1]+top[1])
        # do an overlap test to weed out corner cases and errors
        def inBounds((w,h), (x,y)):
            retVal = True
            if( x < 0 or  y < 0 or x > w or y > h):
                retVal = False
            return retVal

        trc = inBounds(bottom,tr)
        tlc = inBounds(bottom,tl)
        brc = inBounds(bottom,br)
        blc = inBounds(bottom,bl)
        if( not trc and not tlc and not brc and not blc ): # no overlap
            return None,None
        elif( trc and tlc and brc and blc ): # easy case top is fully inside bottom
            tRet = (0,0,top[0],top[1])
            bRet = (pos[0],pos[1],top[0],top[1])
            return tRet,bRet
        # let's figure out where the top rectangle sits on the bottom
        # we clamp the corners of the top rectangle to live inside
        # the bottom rectangle and from that get the x,y,w,h
        tl = (np.clip(tl[0],0,bottom[0]),np.clip(tl[1],0,bottom[1]))
        br = (np.clip(br[0],0,bottom[0]),np.clip(br[1],0,bottom[1]))

        bx = tl[0]
        by = tl[1]
        bw = abs(tl[0]-br[0])
        bh = abs(tl[1]-br[1])
        # now let's figure where the bottom rectangle is in the top rectangle
        # we do the same thing with different coordinates
        pos = (-1*pos[0], -1*pos[1])
        #recalculate the bottoms's corners with respect to the top.
        tr = (pos[0]+bottom[0],pos[1])
        tl = pos
        br = (pos[0]+bottom[0],pos[1]+bottom[1])
        bl = (pos[0],pos[1]+bottom[1])
        tl = (np.clip(tl[0],0,top[0]), np.clip(tl[1],0,top[1]))
        br = (np.clip(br[0],0,top[0]), np.clip(br[1],0,top[1]))
        tx = tl[0]
        ty = tl[1]
        tw = abs(br[0]-tl[0])
        th = abs(br[1]-tl[1])
        return (tx,ty,tw,th),(bx,by,bw,bh)

    def createBinaryMask(self,color1=(0,0,0),color2=(255,255,255)):
        """
        **SUMMARY**

        Generate a binary mask of the image based on a range of rgb values.
        A binary mask is a black and white image where the white area is kept and the
        black area is removed.

        This method is used by specifying two colors as the range between the minimum and maximum
        values that will be masked white.

        **PARAMETERS**

        * *color1* - The starting color range for the mask..
        * *color2* - The end of the color range for the mask.

        **RETURNS**

        A binary (black/white) image mask as a SimpleCV Image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> mask = img.createBinaryMask(color1=(0,128,128),color2=(255,255,255)
        >>> mask.show()

        **SEE ALSO**

        :py:meth:`createBinaryMask`
        :py:meth:`createAlphaMask`
        :py:meth:`blit`
        :py:meth:`threshold`

        """
        if( color1[0]-color2[0] == 0 or
            color1[1]-color2[1] == 0 or
            color1[2]-color2[2] == 0 ):
            logger.warning("No color range selected, the result will be black, returning None instead.")
            return None
        if( color1[0] > 255 or color1[0] < 0 or
            color1[1] > 255 or color1[1] < 0 or
            color1[2] > 255 or color1[2] < 0 or
            color2[0] > 255 or color2[0] < 0 or
            color2[1] > 255 or color2[1] < 0 or
            color2[2] > 255 or color2[2] < 0 ):
            logger.warning("One of the tuple values falls outside of the range of 0 to 255")
            return None

        r = self.getEmpty(1)
        g = self.getEmpty(1)
        b = self.getEmpty(1)

        rl = self.getEmpty(1)
        gl = self.getEmpty(1)
        bl = self.getEmpty(1)

        rh = self.getEmpty(1)
        gh = self.getEmpty(1)
        bh = self.getEmpty(1)

        cv.Split(self.getBitmap(),b,g,r,None);
        #the difference == 255 case is where open CV
        #kinda screws up, this should just be a white image
        if( abs(color1[0]-color2[0]) == 255 ):
            cv.Zero(rl)
            cv.AddS(rl,255,rl)
        #there is a corner case here where difference == 0
        #right now we throw an error on this case.
        #also we use the triplets directly as OpenCV is
        # SUPER FINICKY about the type of the threshold.
        elif( color1[0] < color2[0] ):
            cv.Threshold(r,rl,color1[0],255,cv.CV_THRESH_BINARY)
            cv.Threshold(r,rh,color2[0],255,cv.CV_THRESH_BINARY)
            cv.Sub(rl,rh,rl)
        else:
            cv.Threshold(r,rl,color2[0],255,cv.CV_THRESH_BINARY)
            cv.Threshold(r,rh,color1[0],255,cv.CV_THRESH_BINARY)
            cv.Sub(rl,rh,rl)


        if( abs(color1[1]-color2[1]) == 255 ):
            cv.Zero(gl)
            cv.AddS(gl,255,gl)
        elif( color1[1] < color2[1] ):
            cv.Threshold(g,gl,color1[1],255,cv.CV_THRESH_BINARY)
            cv.Threshold(g,gh,color2[1],255,cv.CV_THRESH_BINARY)
            cv.Sub(gl,gh,gl)
        else:
            cv.Threshold(g,gl,color2[1],255,cv.CV_THRESH_BINARY)
            cv.Threshold(g,gh,color1[1],255,cv.CV_THRESH_BINARY)
            cv.Sub(gl,gh,gl)

        if( abs(color1[2]-color2[2]) == 255 ):
            cv.Zero(bl)
            cv.AddS(bl,255,bl)
        elif( color1[2] < color2[2] ):
            cv.Threshold(b,bl,color1[2],255,cv.CV_THRESH_BINARY)
            cv.Threshold(b,bh,color2[2],255,cv.CV_THRESH_BINARY)
            cv.Sub(bl,bh,bl)
        else:
            cv.Threshold(b,bl,color2[2],255,cv.CV_THRESH_BINARY)
            cv.Threshold(b,bh,color1[2],255,cv.CV_THRESH_BINARY)
            cv.Sub(bl,bh,bl)


        cv.And(rl,gl,rl)
        cv.And(rl,bl,rl)
        return Image(rl)

    def applyBinaryMask(self, mask,bg_color=Color.BLACK):
        """
        **SUMMARY**

        Apply a binary mask to the image. The white areas of the mask will be kept,
        and the black areas removed. The removed areas will be set to the color of
        bg_color.

        **PARAMETERS**

        * *mask* - the binary mask image. White areas are kept, black areas are removed.
        * *bg_color* - the color of the background on the mask.

        **RETURNS**

        A binary (black/white) image mask as a SimpleCV Image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> mask = img.createBinaryMask(color1=(0,128,128),color2=(255,255,255)
        >>> result = img.applyBinaryMask(mask)
        >>> result.show()

        **SEE ALSO**

        :py:meth:`createBinaryMask`
        :py:meth:`createAlphaMask`
        :py:meth:`applyBinaryMask`
        :py:meth:`blit`
        :py:meth:`threshold`

        """
        newCanvas = cv.CreateImage((self.width,self.height), cv.IPL_DEPTH_8U, 3)
        cv.SetZero(newCanvas)
        newBG = cv.RGB(bg_color[0],bg_color[1],bg_color[2])
        cv.AddS(newCanvas,newBG,newCanvas)
        if( mask.width != self.width or mask.height != self.height ):
            logger.warning("Image.applyBinaryMask: your mask and image don't match sizes, if the mask doesn't fit, you can't apply it! Try using the scale function. ")
            return None
        cv.Copy(self.getBitmap(),newCanvas,mask.getBitmap());
        return Image(newCanvas,colorSpace=self._colorSpace);

    def createAlphaMask(self, hue=60, hue_lb=None,hue_ub=None):
        """
        **SUMMARY**

        Generate a grayscale or binary mask image based either on a hue or an RGB triplet that can be used
        like an alpha channel. In the resulting mask, the hue/rgb_color will be treated as transparent (black).

        When a hue is used the mask is treated like an 8bit alpha channel.
        When an RGB triplet is used the result is a binary mask.
        rgb_thresh is a distance measure between a given a pixel and the mask value that we will
        add to the mask. For example, if rgb_color=(0,255,0) and rgb_thresh=5 then any pixel
        winthin five color values of the rgb_color will be added to the mask (e.g. (0,250,0),(5,255,0)....)

        Invert flips the mask values.


        **PARAMETERS**

        * *hue* - a hue used to generate the alpha mask.
        * *hue_lb* - the upper value  of a range of hue values to use.
        * *hue_ub* - the lower value  of a range of hue values to use.

        **RETURNS**

        A grayscale alpha mask as a SimpleCV Image.

        >>> img = Image("lenna")
        >>> mask = img.createAlphaMask(hue_lb=50,hue_ub=70)
        >>> mask.show()

        **SEE ALSO**

        :py:meth:`createBinaryMask`
        :py:meth:`createAlphaMask`
        :py:meth:`applyBinaryMask`
        :py:meth:`blit`
        :py:meth:`threshold`

        """

        if( hue<0 or hue > 180 ):
            logger.warning("Invalid hue color, valid hue range is 0 to 180.")

        if( self._colorSpace != ColorSpace.HSV ):
            hsv = self.toHSV()
        else:
            hsv = self
        h = hsv.getEmpty(1)
        s = hsv.getEmpty(1)
        retVal = hsv.getEmpty(1)
        mask = hsv.getEmpty(1)
        cv.Split(hsv.getBitmap(),h,None,s,None)
        hlut = np.zeros((256,1),dtype=uint8) #thankfully we're not doing a LUT on saturation
        if(hue_lb is not None and hue_ub is not None):
            hlut[hue_lb:hue_ub]=255
        else:
            hlut[hue] = 255
        cv.LUT(h,mask,cv.fromarray(hlut))
        cv.Copy(s,retVal,mask) #we'll save memory using hue
        return Image(retVal)


    def applyPixelFunction(self, theFunc):
        """
        **SUMMARY**

        apply a function to every pixel and return the result
        The function must be of the form int (r,g,b)=func((r,g,b))

        **PARAMETERS**

        * *theFunc* - a function pointer to a function of the form (r,g.b) = theFunc((r,g,b))

        **RETURNS**

        A simpleCV image after mapping the function to the image.

        **EXAMPLE**

        >>> def derp(pixels):
        >>>     return (int(b*.2),int(r*.3),int(g*.5))
        >>>
        >>> img = Image("lenna")
        >>> img2 = img.applyPixelFunction(derp)

        """
        #there should be a way to do this faster using numpy vectorize
        #but I can get vectorize to work with the three channels together... have to split them
        #TODO: benchmark this against vectorize
        pixels = np.array(self.getNumpy()).reshape(-1,3).tolist()
        result = np.array(map(theFunc,pixels),dtype=uint8).reshape(self.width,self.height,3)
        return Image(result)


    def integralImage(self,tilted=False):
        """
        **SUMMARY**

        Calculate the integral image and return it as a numpy array.
        The integral image gives the sum of all of the pixels above and to the
        right of a given pixel location. It is useful for computing Haar cascades.
        The return type is a numpy array the same size of the image. The integral
        image requires 32Bit values which are not easily supported by the SimpleCV
        Image class.

        **PARAMETERS**

        * *tilted*  - if tilted is true we tilt the image 45 degrees and then calculate the results.

        **RETURNS**

        A numpy array of the values.

        **EXAMPLE**

        >>> img = Image("logo")
        >>> derp = img.integralImage()

        **SEE ALSO**

        http://en.wikipedia.org/wiki/Summed_area_table
        """

        if(tilted):
            img2 = cv.CreateImage((self.width+1, self.height+1), cv.IPL_DEPTH_32F, 1)
            img3 = cv.CreateImage((self.width+1, self.height+1), cv.IPL_DEPTH_32F, 1)
            cv.Integral(self._getGrayscaleBitmap(),img3,None,img2)
        else:
            img2 = cv.CreateImage((self.width+1, self.height+1), cv.IPL_DEPTH_32F, 1)
            cv.Integral(self._getGrayscaleBitmap(),img2)
        return np.array(cv.GetMat(img2))


    def convolve(self,kernel = [[1,0,0],[0,1,0],[0,0,1]],center=None):
        """
        **SUMMARY**

        Convolution performs a shape change on an image.  It is similiar to
        something like a dilate.  You pass it a kernel in the form of a list, np.array, or cvMat

        **PARAMETERS**

        * *kernel* - The convolution kernel. As a cvArray, cvMat, or Numpy Array.
        * *center* - If true we use the center of the kernel.

        **RETURNS**

        The image after we apply the convolution.

        **EXAMPLE**

        >>> img = Image("sampleimages/simplecv.png")
        >>> kernel = [[1,0,0],[0,1,0],[0,0,1]]
        >>> conv = img.convolve()

        **SEE ALSO**

        http://en.wikipedia.org/wiki/Convolution

        """
        if(isinstance(kernel, list)):
            kernel = np.array(kernel)

        if(type(kernel)==np.ndarray):
            sz = kernel.shape
            kernel = kernel.astype(np.float32)
            myKernel = cv.CreateMat(sz[0], sz[1], cv.CV_32FC1)
            cv.SetData(myKernel, kernel.tostring(), kernel.dtype.itemsize * kernel.shape[1])
        elif(type(kernel)==cv.mat):
            myKernel = kernel
        else:
            logger.warning("Convolution uses numpy arrays or cv.mat type.")
            return None
        retVal = self.getEmpty(3)
        if(center is None):
            cv.Filter2D(self.getBitmap(),retVal,myKernel)
        else:
            cv.Filter2D(self.getBitmap(),retVal,myKernel,center)
        return Image(retVal)

    def findTemplate(self, template_image = None, threshold = 5, method = "SQR_DIFF_NORM", grayscale=True, rawmatches = False):
        """
        **SUMMARY**

        This function searches an image for a template image.  The template
        image is a smaller image that is searched for in the bigger image.
        This is a basic pattern finder in an image.  This uses the standard
        OpenCV template (pattern) matching and cannot handle scaling or rotation

        Template matching returns a match score for every pixel in the image.
        Often pixels that are near to each other and a close match to the template
        are returned as a match. If the threshold is set too low expect to get
        a huge number of values. The threshold parameter is in terms of the
        number of standard deviations from the mean match value you are looking

        For example, matches that are above three standard deviations will return
        0.1% of the pixels. In a 800x600 image this means there will be
        800*600*0.001 = 480 matches.

        This method returns the locations of wherever it finds a match above a
        threshold. Because of how template matching works, very often multiple
        instances of the template overlap significantly. The best approach is to
        find the centroid of all of these values. We suggest using an iterative
        k-means approach to find the centroids.


        **PARAMETERS**

        * *template_image* - The template image.
        * *threshold* - Int
        * *method* -

          * SQR_DIFF_NORM - Normalized square difference
          * SQR_DIFF      - Square difference
          * CCOEFF        -
          * CCOEFF_NORM   -
          * CCORR         - Cross correlation
          * CCORR_NORM    - Normalize cross correlation
        * *grayscale* - Boolean - If false, template Match is found using BGR image.
        
        **EXAMPLE**

        >>> image = Image("/path/to/img.png")
        >>> pattern_image = image.crop(100,100,100,100)
        >>> found_patterns = image.findTemplate(pattern_image)
        >>> found_patterns.draw()
        >>> image.show()

        **RETURNS**

        This method returns a FeatureSet of TemplateMatch objects.

        """
        if(template_image == None):
            logger.info( "Need image for matching")
            return

        if(template_image.width > self.width):
            #logger.info( "Image too wide")
            return

        if(template_image.height > self.height):
            logger.info("Image too tall")
            return

        check = 0; # if check = 0 we want maximal value, otherwise minimal
        if(method is None or method == "" or method == "SQR_DIFF_NORM"):#minimal
            method = cv.CV_TM_SQDIFF_NORMED
            check = 1;
        elif(method == "SQR_DIFF"): #minimal
            method = cv.CV_TM_SQDIFF
            check = 1
        elif(method == "CCOEFF"): #maximal
            method = cv.CV_TM_CCOEFF
        elif(method == "CCOEFF_NORM"): #maximal
            method = cv.CV_TM_CCOEFF_NORMED
        elif(method == "CCORR"): #maximal
            method = cv.CV_TM_CCORR
        elif(method == "CCORR_NORM"): #maximal
            method = cv.CV_TM_CCORR_NORMED
        else:
            logger.warning("ooops.. I don't know what template matching method you are looking for.")
            return None
        #create new image for template matching computation
        matches = cv.CreateMat( (self.height - template_image.height + 1),
                                (self.width - template_image.width + 1),
                                cv.CV_32FC1)

        #choose template matching method to be used
        if grayscale:
            cv.MatchTemplate( self._getGrayscaleBitmap(), template_image._getGrayscaleBitmap(), matches, method )
        else:
            cv.MatchTemplate( self.getBitmap(), template_image.getBitmap(), matches, method )
        mean = np.mean(matches)
        sd = np.std(matches)
        if(check > 0):
            compute = np.where((matches < mean-threshold*sd) )
        else:
            compute = np.where((matches > mean+threshold*sd) )

        mapped = map(tuple, np.column_stack(compute))
        fs = FeatureSet()
        for location in mapped:
            fs.append(TemplateMatch(self, template_image, (location[1],location[0]), matches[location[0], location[1]]))

        if (rawmatches):
            return fs
        #cluster overlapping template matches
        finalfs = FeatureSet()
        if( len(fs) > 0 ):
            finalfs.append(fs[0])
            for f in fs:
                match = False
                for f2 in finalfs:
                    if( f2._templateOverlaps(f) ): #if they overlap
                        f2.consume(f) #merge them
                        match = True
                        break

                if( not match ):
                    finalfs.append(f)

            for f in finalfs: #rescale the resulting clusters to fit the template size
                f.rescale(template_image.width,template_image.height)

            fs = finalfs

        return fs

    def findTemplateOnce(self, template_image = None, threshold = 0.2, method = "SQR_DIFF_NORM", grayscale=True):
        """
        **SUMMARY**

        This function searches an image for a single template image match.The template
        image is a smaller image that is searched for in the bigger image.
        This is a basic pattern finder in an image.  This uses the standard
        OpenCV template (pattern) matching and cannot handle scaling or rotation

        This method returns the single best match if and only if that
        match less than the threshold (greater than in the case of
        some methods).
        
        **PARAMETERS**

        * *template_image* - The template image.
        * *threshold* - Int
        * *method* -

          * SQR_DIFF_NORM - Normalized square difference
          * SQR_DIFF      - Square difference
          * CCOEFF        -
          * CCOEFF_NORM   -
          * CCORR         - Cross correlation
          * CCORR_NORM    - Normalize cross correlation
        * *grayscale* - Boolean - If false, template Match is found using BGR image.
        
        **EXAMPLE**

        >>> image = Image("/path/to/img.png")
        >>> pattern_image = image.crop(100,100,100,100)
        >>> found_patterns = image.findTemplateOnce(pattern_image)
        >>> found_patterns.draw()
        >>> image.show()

        **RETURNS**

        This method returns a FeatureSet of TemplateMatch objects.

        """
        if(template_image == None):
            logger.info( "Need image for template matching.")
            return

        if(template_image.width > self.width):
            logger.info( "Template image is too wide for the given image.")
            return

        if(template_image.height > self.height):
            logger.info("Template image too tall for the given image.")
            return

        check = 0; # if check = 0 we want maximal value, otherwise minimal
        if(method is None or method == "" or method == "SQR_DIFF_NORM"):#minimal
            method = cv.CV_TM_SQDIFF_NORMED
            check = 1;
        elif(method == "SQR_DIFF"): #minimal
            method = cv.CV_TM_SQDIFF
            check = 1
        elif(method == "CCOEFF"): #maximal
            method = cv.CV_TM_CCOEFF
        elif(method == "CCOEFF_NORM"): #maximal
            method = cv.CV_TM_CCOEFF_NORMED
        elif(method == "CCORR"): #maximal
            method = cv.CV_TM_CCORR
        elif(method == "CCORR_NORM"): #maximal
            method = cv.CV_TM_CCORR_NORMED
        else:
            logger.warning("ooops.. I don't know what template matching method you are looking for.")
            return None
        #create new image for template matching computation
        matches = cv.CreateMat( (self.height - template_image.height + 1),
                                (self.width - template_image.width + 1),
                                cv.CV_32FC1)

        #choose template matching method to be used
        if grayscale:
            cv.MatchTemplate( self._getGrayscaleBitmap(), template_image._getGrayscaleBitmap(), matches, method )
        else:
            cv.MatchTemplate( self.getBitmap(), template_image.getBitmap(), matches, method )
        mean = np.mean(matches)
        sd = np.std(matches)
        if(check > 0):
            if( np.min(matches) <= threshold ):
                compute = np.where( matches == np.min(matches) )
            else:
                return []
        else:
            if( np.max(matches) >= threshold ):
                compute = np.where( matches == np.max(matches) )
            else:
                return []
        mapped = map(tuple, np.column_stack(compute))
        fs = FeatureSet()
        for location in mapped:
            fs.append(TemplateMatch(self, template_image, (location[1],location[0]), matches[location[0], location[1]]))

        return fs


    def readText(self):
        """
        **SUMMARY**

        This function will return any text it can find using OCR on the
        image.

        Please note that it does not handle rotation well, so if you need
        it in your application try to rotate and/or crop the area so that
        the text would be the same way a document is read

        **RETURNS**

        A String

        **EXAMPLE**

        >>> img = Imgae("somethingwithtext.png")
        >>> text = img.readText()
        >>> print text

        **NOTE**

        If you're having run-time problems I feel bad for your son,
        I've got 99 problems but dependencies ain't one:

        http://code.google.com/p/tesseract-ocr/
        http://code.google.com/p/python-tesseract/

        """

        if(not OCR_ENABLED):
            return "Please install the correct OCR library required - http://code.google.com/p/tesseract-ocr/ http://code.google.com/p/python-tesseract/"

        api = tesseract.TessBaseAPI()
        api.SetOutputName("outputName")
        api.Init(".","eng",tesseract.OEM_DEFAULT)
        api.SetPageSegMode(tesseract.PSM_AUTO)


        jpgdata = StringIO()
        self.getPIL().save(jpgdata, "jpeg")
        jpgdata.seek(0)
        stringbuffer = jpgdata.read()
        result = tesseract.ProcessPagesBuffer(stringbuffer,len(stringbuffer),api)
        return result

    def findCircle(self,canny=100,thresh=350,distance=-1):
        """
        **SUMMARY**

        Perform the Hough Circle transform to extract _perfect_ circles from the image
        canny - the upper bound on a canny edge detector used to find circle edges.

        **PARAMETERS**

        * *thresh* - the threshold at which to count a circle. Small parts of a circle get
          added to the accumulator array used internally to the array. This value is the
          minimum threshold. Lower thresholds give more circles, higher thresholds give fewer circles.

        .. ::Warning:
          If this threshold is too high, and no circles are found the underlying OpenCV
          routine fails and causes a segfault.

        * *distance* - the minimum distance between each successive circle in pixels. 10 is a good
          starting value.

        **RETURNS**

        A feature set of Circle objects.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> circs = img.findCircles()
        >>> for c in circs:
        >>>    print c


        """
        storage = cv.CreateMat(self.width, 1, cv.CV_32FC3)
        #a distnace metric for how apart our circles should be - this is sa good bench mark
        if(distance < 0 ):
            distance = 1 + max(self.width,self.height)/50
        cv.HoughCircles(self._getGrayscaleBitmap(),storage, cv.CV_HOUGH_GRADIENT, 2, distance,canny,thresh)
        if storage.rows == 0:
            return None
        circs = np.asarray(storage)
        sz = circs.shape
        circleFS = FeatureSet()
        for i in range(sz[0]):
            circleFS.append(Circle(self,int(circs[i][0][0]),int(circs[i][0][1]),int(circs[i][0][2])))
        return circleFS

    def whiteBalance(self,method="Simple"):
        """
        **SUMMARY**

        Attempts to perform automatic white balancing.
        Gray World see: http://scien.stanford.edu/pages/labsite/2000/psych221/projects/00/trek/GWimages.html
        Robust AWB: http://scien.stanford.edu/pages/labsite/2010/psych221/projects/2010/JasonSu/robustawb.html
        http://scien.stanford.edu/pages/labsite/2010/psych221/projects/2010/JasonSu/Papers/Robust%20Automatic%20White%20Balance%20Algorithm%20using%20Gray%20Color%20Points%20in%20Images.pdf
        Simple AWB:
        http://www.ipol.im/pub/algo/lmps_simplest_color_balance/
        http://scien.stanford.edu/pages/labsite/2010/psych221/projects/2010/JasonSu/simplestcb.html



        **PARAMETERS**

        * *method* - The method to use for white balancing. Can be one of the following:

          * `Gray World <http://scien.stanford.edu/pages/labsite/2000/psych221/projects/00/trek/GWimages.html>`_

          * `Robust AWB <http://scien.stanford.edu/pages/labsite/2010/psych221/projects/2010/JasonSu/robustawb.html>`_

          * `Simple AWB <http://www.ipol.im/pub/algo/lmps_simplest_color_balance/>`_


        **RETURNS**

        A SimpleCV Image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> img2 = img.whiteBalance()

        """
        img = self
        if(method=="GrayWorld"):
            avg = cv.Avg(img.getBitmap());
            bf = float(avg[0])
            gf = float(avg[1])
            rf = float(avg[2])
            af = (bf+gf+rf)/3.0
            if( bf == 0.00 ):
                b_factor = 1.00
            else:
                b_factor = af/bf

            if( gf == 0.00 ):
                g_factor = 1.00
            else:
                g_factor = af/gf

            if( rf == 0.00 ):
                r_factor = 1.00
            else:
                r_factor = af/rf

            b = img.getEmpty(1)
            g = img.getEmpty(1)
            r = img.getEmpty(1)
            cv.Split(self.getBitmap(), b, g, r, None)
            bfloat = cv.CreateImage((img.width, img.height), cv.IPL_DEPTH_32F, 1)
            gfloat = cv.CreateImage((img.width, img.height), cv.IPL_DEPTH_32F, 1)
            rfloat = cv.CreateImage((img.width, img.height), cv.IPL_DEPTH_32F, 1)

            cv.ConvertScale(b,bfloat,b_factor)
            cv.ConvertScale(g,gfloat,g_factor)
            cv.ConvertScale(r,rfloat,r_factor)

            (minB,maxB,minBLoc,maxBLoc) = cv.MinMaxLoc(bfloat)
            (minG,maxG,minGLoc,maxGLoc) = cv.MinMaxLoc(gfloat)
            (minR,maxR,minRLoc,maxRLoc) = cv.MinMaxLoc(rfloat)
            scale = max([maxR,maxG,maxB])
            sfactor = 1.00
            if(scale > 255 ):
                sfactor = 255.00/float(scale)

            cv.ConvertScale(bfloat,b,sfactor);
            cv.ConvertScale(gfloat,g,sfactor);
            cv.ConvertScale(rfloat,r,sfactor);

            retVal = img.getEmpty()
            cv.Merge(b,g,r,None,retVal);
            retVal = Image(retVal)
        elif( method == "Simple" ):
            thresh = 0.003
            sz = img.width*img.height
            tempMat = img.getNumpy()
            bcf = sss.cumfreq(tempMat[:,:,0], numbins=256)
            bcf = bcf[0] # get our cumulative histogram of values for this color

            blb = -1 #our upper bound
            bub = 256 # our lower bound
            lower_thresh = 0.00
            upper_thresh = 0.00
            #now find the upper and lower thresh% of our values live
            while( lower_thresh < thresh ):
                blb = blb+1
                lower_thresh = bcf[blb]/sz
            while( upper_thresh < thresh ):
                bub = bub-1
                upper_thresh = (sz-bcf[bub])/sz


            gcf = sss.cumfreq(tempMat[:,:,1], numbins=256)
            gcf = gcf[0]
            glb = -1 #our upper bound
            gub = 256 # our lower bound
            lower_thresh = 0.00
            upper_thresh = 0.00
            #now find the upper and lower thresh% of our values live
            while( lower_thresh < thresh ):
                glb = glb+1
                lower_thresh = gcf[glb]/sz
            while( upper_thresh < thresh ):
                gub = gub-1
                upper_thresh = (sz-gcf[gub])/sz


            rcf = sss.cumfreq(tempMat[:,:,2], numbins=256)
            rcf = rcf[0]
            rlb = -1 #our upper bound
            rub = 256 # our lower bound
            lower_thresh = 0.00
            upper_thresh = 0.00
            #now find the upper and lower thresh% of our values live
            while( lower_thresh < thresh ):
                rlb = rlb+1
                lower_thresh = rcf[rlb]/sz
            while( upper_thresh < thresh ):
                rub = rub-1
                upper_thresh = (sz-rcf[rub])/sz
            #now we create the scale factors for the remaining pixels
            rlbf = float(rlb)
            rubf = float(rub)
            glbf = float(glb)
            gubf = float(gub)
            blbf = float(blb)
            bubf = float(bub)

            rLUT = np.ones((256,1),dtype=uint8)
            gLUT = np.ones((256,1),dtype=uint8)
            bLUT = np.ones((256,1),dtype=uint8)
            for i in range(256):
                if(i <= rlb):
                    rLUT[i][0] = 0
                elif( i >= rub):
                    rLUT[i][0] = 255
                else:
                    rf = ((float(i)-rlbf)*255.00/(rubf-rlbf))
                    rLUT[i][0] = int(rf)
                if( i <= glb):
                    gLUT[i][0] = 0
                elif( i >= gub):
                    gLUT[i][0] = 255
                else:
                    gf = ((float(i)-glbf)*255.00/(gubf-glbf))
                    gLUT[i][0] = int(gf)
                if( i <= blb):
                    bLUT[i][0] = 0
                elif( i >= bub):
                    bLUT[i][0] = 255
                else:
                    bf = ((float(i)-blbf)*255.00/(bubf-blbf))
                    bLUT[i][0] = int(bf)
            retVal = img.applyLUT(bLUT,rLUT,gLUT)
        return retVal

    def applyLUT(self,rLUT=None,bLUT=None,gLUT=None):
        """
        **SUMMARY**

        Apply LUT allows you to apply a LUT (look up table) to the pixels in a image. Each LUT is just
        an array where each index in the array points to its value in the result image. For example
        rLUT[0]=255 would change all pixels where the red channel is zero to the value 255.

        **PARAMETERS**

        * *rLUT* - a tuple or np.array of size (256x1) with dtype=uint8.
        * *gLUT* - a tuple or np.array of size (256x1) with dtype=uint8.
        * *bLUT* - a tuple or np.array of size (256x1) with dtype=uint8.

        .. warning::
          The dtype is very important. Will throw the following error without it:
          error: dst.size() == src.size() && dst.type() == CV_MAKETYPE(lut.depth(), src.channels())


        **RETURNS**

        The SimpleCV image remapped using the LUT.

        **EXAMPLE**

        This example saturates the red channel:

        >>> rlut = np.ones((256,1),dtype=uint8)*255
        >>> img=img.applyLUT(rLUT=rlut)


        NOTE:

        -==== BUG NOTE ====-
        This method seems to error on the LUT map for some versions of OpenCV.
        I am trying to figure out why. -KAS
        """
        r = self.getEmpty(1)
        g = self.getEmpty(1)
        b = self.getEmpty(1)
        cv.Split(self.getBitmap(),b,g,r,None);
        if(rLUT is not None):
            cv.LUT(r,r,cv.fromarray(rLUT))
        if(gLUT is not None):
            cv.LUT(g,g,cv.fromarray(gLUT))
        if(bLUT is not None):
            cv.LUT(b,b,cv.fromarray(bLUT))
        temp = self.getEmpty()
        cv.Merge(b,g,r,None,temp)
        return Image(temp)


    def _getRawKeypoints(self,thresh=500.00,flavor="SURF", highQuality=1, forceReset=False):
        """
        .. _getRawKeypoints:
        This method finds keypoints in an image and returns them as the raw keypoints
        and keypoint descriptors. When this method is called it caches a the features
        and keypoints locally for quick and easy access.

        Parameters:
        min_quality - The minimum quality metric for SURF descriptors. Good values
                      range between about 300.00 and 600.00

        flavor - a string indicating the method to use to extract features.
                 A good primer on how feature/keypoint extractiors can be found here:

                 http://en.wikipedia.org/wiki/Feature_detection_(computer_vision)
                 http://www.cg.tu-berlin.de/fileadmin/fg144/Courses/07WS/compPhoto/Feature_Detection.pdf


                 "SURF" - extract the SURF features and descriptors. If you don't know
                 what to use, use this.
                 See: http://en.wikipedia.org/wiki/SURF

                 "STAR" - The STAR feature extraction algorithm
                 See: http://pr.willowgarage.com/wiki/Star_Detector

                 "FAST" - The FAST keypoint extraction algorithm
                 See: http://en.wikipedia.org/wiki/Corner_detection#AST_based_feature_detectors

                 All the flavour specified below are for OpenCV versions >= 2.4.0 :

                 "MSER" - Maximally Stable Extremal Regions algorithm

                 See: http://en.wikipedia.org/wiki/Maximally_stable_extremal_regions

                 "Dense" - Dense Scale Invariant Feature Transform.

                 See: http://www.vlfeat.org/api/dsift.html

                 "ORB" - The Oriented FAST and Rotated BRIEF

                 See: http://www.willowgarage.com/sites/default/files/orb_final.pdf

                 "SIFT" - Scale-invariant feature transform

                 See: http://en.wikipedia.org/wiki/Scale-invariant_feature_transform

                 "BRISK" - Binary Robust Invariant Scalable Keypoints

                  See: http://www.asl.ethz.ch/people/lestefan/personal/BRISK

                 "FREAK" - Fast Retina Keypoints

                  See: http://www.ivpe.com/freak.htm
                  Note: It's a keypoint descriptor and not a KeyPoint detector. SIFT KeyPoints
                  are detected and FERAK is used to extract keypoint descriptor.

        highQuality - The SURF descriptor comes in two forms, a vector of 64 descriptor
                      values and a vector of 128 descriptor values. The latter are "high"
                      quality descriptors.

        forceReset - If keypoints have already been calculated for this image those
                     keypoints are returned veresus recalculating the values. If
                     force reset is True we always recalculate the values, otherwise
                     we will used the cached copies.

        Returns:
        A tuple of keypoint objects and optionally a numpy array of the descriptors.

        Example:
        >>> img = Image("aerospace.jpg")
        >>> kp,d = img._getRawKeypoints()

        Notes:
        If you would prefer to work with the raw keypoints and descriptors each image keeps
        a local cache of the raw values. These are named:

        self._mKeyPoints # A tuple of keypoint objects
        See: http://opencv.itseez.com/modules/features2d/doc/common_interfaces_of_feature_detectors.html#keypoint-keypoint
        self._mKPDescriptors # The descriptor as a floating point numpy array
        self._mKPFlavor = "NONE" # The flavor of the keypoints as a string.

        See Also:
         ImageClass._getRawKeypoints(self,thresh=500.00,forceReset=False,flavor="SURF",highQuality=1)
         ImageClass._getFLANNMatches(self,sd,td)
         ImageClass.findKeypointMatch(self,template,quality=500.00,minDist=0.2,minMatch=0.4)
         ImageClass.drawKeypointMatches(self,template,thresh=500.00,minDist=0.15,width=1)

        """
        try:
            import cv2
            ver = cv2.__version__
            new_version = 0
            #For OpenCV versions till 2.4.0,  cv2.__versions__ are of the form "$Rev: 4557 $"
            if not ver.startswith('$Rev:'):
                if int(ver.replace('.','0'))>=20400:
                    new_version = 1
        except:
            warnings.warn("Can't run Keypoints without OpenCV >= 2.3.0")
            return (None, None)

        if( forceReset ):
            self._mKeyPoints = None
            self._mKPDescriptors = None

        _detectors = ["SIFT", "SURF", "FAST", "STAR", "FREAK", "ORB", "BRISK", "MSER", "Dense"]
        _descriptors = ["SIFT", "SURF", "ORB", "FREAK", "BRISK"]
        if flavor not in _detectors:
            warnings.warn("Invalid choice of keypoint detector.")
            return (None, None)

        if self._mKeyPoints != None and self._mKPFlavor == flavor:
            return (self._mKeyPoints, self._mKPDescriptors)

        if hasattr(cv2, flavor):

            if flavor == "SURF":
                # cv2.SURF(hessianThreshold, nOctaves, nOctaveLayers, extended, upright)
                detector = cv2.SURF(thresh, 4, 2, highQuality, 1)
                if new_version == 0:
                    self._mKeyPoints, self._mKPDescriptors = detector.detect(self.getGrayNumpy(), None, False)
                else:
                    self._mKeyPoints, self._mKPDescriptors = detector.detectAndCompute(self.getGrayNumpy(), None, False)
                if len(self._mKeyPoints) == 0:
                    return (None, None)
                if highQuality == 1:
                    self._mKPDescriptors = self._mKPDescriptors.reshape((-1, 128))
                else:
                    self._mKPDescriptors = self._mKPDescriptors.reshape((-1, 64))

            elif flavor in _descriptors:
                detector = getattr(cv2,  flavor)()
                self._mKeyPoints, self._mKPDescriptors = detector.detectAndCompute(self.getGrayNumpy(), None, False)
            elif flavor == "MSER":
                if hasattr(cv2, "FeatureDetector_create"):
                    detector = cv2.FeatureDetector_create("MSER")
                    self._mKeyPoints = detector.detect(self.getGrayNumpy())
        elif flavor == "STAR":
            detector = cv2.StarDetector()
            self._mKeyPoints = detector.detect(self.getGrayNumpy())
        elif flavor == "FAST":
            if not hasattr(cv2, "FastFeatureDetector"):
                warnings.warn("You need OpenCV >= 2.4.0 to support FAST")
                return None, None
            detector = cv2.FastFeatureDetector(int(thresh), True)
            self._mKeyPoints = detector.detect(self.getGrayNumpy(), None)
        elif hasattr(cv2, "FeatureDetector_create"):
            if flavor in _descriptors:
                extractor = cv2.DescriptorExtractor_create(flavor)
                if flavor == "FREAK":
                    if new_version == 0:
                        warnings.warn("You need OpenCV >= 2.4.3 to support FAST")
                    flavor = "SIFT"
                detector = cv2.FeatureDetector_create(flavor)
                self._mKeyPoints = detector.detect(self.getGrayNumpy())
                self._mKeyPoints, self._mKPDescriptors = extractor.compute(self.getGrayNumpy(), self._mKeyPoints)
            else:
                detector = cv2.FeatureDetector_create(flavor)
                self._mKeyPoints = detector.detect(self.getGrayNumpy())
        else:
            warnings.warn("SimpleCV can't seem to find appropriate function with your OpenCV version.")
            return (None, None)
        return (self._mKeyPoints, self._mKPDescriptors)

    def _getFLANNMatches(self,sd,td):
        """
        Summary:
        This method does a fast local approximate nearest neighbors (FLANN) calculation between two sets
        of feature vectors. The result are two numpy arrays the first one is a list of indexes of the
        matches and the second one is the match distance value. For the match indices or idx, the index
        values correspond to the values of td, and the value in the array is the index in td. I.
        I.e. j = idx[i] is where td[i] matches sd[j].
        The second numpy array, at the index i is the match distance between td[i] and sd[j].
        Lower distances mean better matches.

        Parameters:
        sd - A numpy array of feature vectors of any size.
        td - A numpy array of feature vectors of any size, this vector is used for indexing
             and the result arrays will have a length matching this vector.

        Returns:
        Two numpy arrays, the first one, idx, is the idx of the matches of the vector td with sd.
        The second one, dist, is the distance value for the closest match.

        Example:
        >>> kpt,td = img1._getRawKeypoints() # t is template
        >>> kps,sd = img2._getRawKeypoints() # s is source
        >>> idx,dist = img1._getFLANNMatches(sd,td)
        >>> j = idx[42]
        >>> print kps[j] # matches kp 42
        >>> print dist[i] # the match quality.

        Notes:
        If you would prefer to work with the raw keypoints and descriptors each image keeps
        a local cache of the raw values. These are named:

        self._mKeyPoints # A tuple of keypoint objects
        See: http://opencv.itseez.com/modules/features2d/doc/common_interfaces_of_feature_detectors.html#keypoint-keypoint
        self._mKPDescriptors # The descriptor as a floating point numpy array
        self._mKPFlavor = "NONE" # The flavor of the keypoints as a string.

        See:
         ImageClass._getRawKeypoints(self,thresh=500.00,forceReset=False,flavor="SURF",highQuality=1)
         ImageClass._getFLANNMatches(self,sd,td)
         ImageClass.drawKeypointMatches(self,template,thresh=500.00,minDist=0.15,width=1)
         ImageClass.findKeypoints(self,min_quality=300.00,flavor="SURF",highQuality=False )
         ImageClass.findKeypointMatch(self,template,quality=500.00,minDist=0.2,minMatch=0.4)
        """
        try:
            import cv2
        except:
            logger.warning("Can't run FLANN Matches without OpenCV >= 2.3.0")
            return
        FLANN_INDEX_KDTREE = 1  # bug: flann enums are missing
        flann_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 4)
        flann = cv2.flann_Index(sd, flann_params)
        idx, dist = flann.knnSearch(td, 1, params = {}) # bug: need to provide empty dict
        del flann
        return idx,dist

    def drawKeypointMatches(self,template,thresh=500.00,minDist=0.15,width=1):
        """
        **SUMMARY**

        Draw keypoints draws a side by side representation of two images, calculates
        keypoints for both images, determines the keypoint correspondences, and then draws
        the correspondences. This method is helpful for debugging keypoint calculations
        and also looks really cool :) .  The parameters mirror the parameters used
        for findKeypointMatches to assist with debugging

        **PARAMETERS**

        * *template* - A template image.
        * *quality* - The feature quality metric. This can be any value between about 300 and 500. Higher
          values should return fewer, but higher quality features.
        * *minDist* - The value below which the feature correspondence is considered a match. This
          is the distance between two feature vectors. Good values are between 0.05 and 0.3
        * *width* - The width of the drawn line.

        **RETURNS**

        A side by side image of the template and source image with each feature correspondence
        draw in a different color.

        **EXAMPLE**

        >>> img = cam.getImage()
        >>> template = Image("myTemplate.png")
        >>> result = img.drawKeypointMatches(self,template,300.00,0.4):

        **NOTES**

        If you would prefer to work with the raw keypoints and descriptors each image keeps
        a local cache of the raw values. These are named:

        self._mKeyPoints # A tuple of keypoint objects
        See: http://opencv.itseez.com/modules/features2d/doc/common_interfaces_of_feature_detectors.html#keypoint-keypoint
        self._mKPDescriptors # The descriptor as a floating point numpy array
        self._mKPFlavor = "NONE" # The flavor of the keypoints as a string.

        **SEE ALSO**

        :py:meth:`drawKeypointMatches`
        :py:meth:`findKeypoints`
        :py:meth:`findKeypointMatch`

        """
        if template == None:
            return None

        resultImg = template.sideBySide(self,scale=False)
        hdif = (self.height-template.height)/2
        skp,sd = self._getRawKeypoints(thresh)
        tkp,td = template._getRawKeypoints(thresh)
        if( td == None or sd == None ):
            logger.warning("We didn't get any descriptors. Image might be too uniform or blurry." )
            return resultImg
        template_points = float(td.shape[0])
        sample_points = float(sd.shape[0])
        magic_ratio = 1.00
        if( sample_points > template_points ):
            magic_ratio = float(sd.shape[0])/float(td.shape[0])

        idx,dist = self._getFLANNMatches(sd,td) # match our keypoint descriptors
        p = dist[:,0]
        result = p*magic_ratio < minDist #, = np.where( p*magic_ratio < minDist )
        for i in range(0,len(idx)):
            if( result[i] ):
                pt_a = (tkp[i].pt[1], tkp[i].pt[0]+hdif)
                pt_b = (skp[idx[i]].pt[1]+template.width,skp[idx[i]].pt[0])
                resultImg.drawLine(pt_a,pt_b,color=Color.getRandom(),thickness=width)
        return resultImg


    def findKeypointMatch(self,template,quality=500.00,minDist=0.2,minMatch=0.4):
        """
        **SUMMARY**

        findKeypointMatch allows you to match a template image with another image using
        SURF keypoints. The method extracts keypoints from each image, uses the Fast Local
        Approximate Nearest Neighbors algorithm to find correspondences between the feature
        points, filters the correspondences based on quality, and then, attempts to calculate
        a homography between the two images. This homography allows us to draw a matching
        bounding box in the source image that corresponds to the template. This method allows
        you to perform matchs the ordinarily fail when using the findTemplate method.
        This method should be able to handle a reasonable changes in camera orientation and
        illumination. Using a template that is close to the target image will yield much
        better results.

        .. Warning::
          This method is only capable of finding one instance of the template in an image.
          If more than one instance is visible the homography calculation and the method will
          fail.

        **PARAMETERS**

        * *template* - A template image.
        * *quality* - The feature quality metric. This can be any value between about 300 and 500. Higher
          values should return fewer, but higher quality features.
        * *minDist* - The value below which the feature correspondence is considered a match. This
          is the distance between two feature vectors. Good values are between 0.05 and 0.3
        * *minMatch* - The percentage of features which must have matches to proceed with homography calculation.
          A value of 0.4 means 40% of features must match. Higher values mean better matches
          are used. Good values are between about 0.3 and 0.7


        **RETURNS**

        If a homography (match) is found this method returns a feature set with a single
        KeypointMatch feature. If no match is found None is returned.

        **EXAMPLE**

        >>> template = Image("template.png")
        >>> img = camera.getImage()
        >>> fs = img.findKeypointMatch(template)
        >>> if( fs is not None ):
        >>>      fs.draw()
        >>>      img.show()

        **NOTES**

        If you would prefer to work with the raw keypoints and descriptors each image keeps
        a local cache of the raw values. These are named:

        | self._mKeyPoints # A Tuple of keypoint objects
        | self._mKPDescriptors # The descriptor as a floating point numpy array
        | self._mKPFlavor = "NONE" # The flavor of the keypoints as a string.
        | `See Documentation <http://opencv.itseez.com/modules/features2d/doc/common_interfaces_of_feature_detectors.html#keypoint-keypoint>`_

        **SEE ALSO**

        :py:meth:`_getRawKeypoints`
        :py:meth:`_getFLANNMatches`
        :py:meth:`drawKeypointMatches`
        :py:meth:`findKeypoints`

        """
        try:
            import cv2
        except:
            warnings.warn("Can't Match Keypoints without OpenCV >= 2.3.0")
            return
            
        if template == None:
          return None
        fs = FeatureSet()
        skp,sd = self._getRawKeypoints(quality)
        tkp,td = template._getRawKeypoints(quality)
        if( skp == None or tkp == None ):
            warnings.warn("I didn't get any keypoints. Image might be too uniform or blurry." )
            return None

        template_points = float(td.shape[0])
        sample_points = float(sd.shape[0])
        magic_ratio = 1.00
        if( sample_points > template_points ):
            magic_ratio = float(sd.shape[0])/float(td.shape[0])

        idx,dist = self._getFLANNMatches(sd,td) # match our keypoint descriptors
        p = dist[:,0]
        result = p*magic_ratio < minDist #, = np.where( p*magic_ratio < minDist )
        pr = result.shape[0]/float(dist.shape[0])

        if( pr > minMatch and len(result)>4 ): # if more than minMatch % matches we go ahead and get the data
            lhs = []
            rhs = []
            for i in range(0,len(idx)):
                if( result[i] ):
                    lhs.append((tkp[i].pt[1], tkp[i].pt[0]))
                    rhs.append((skp[idx[i]].pt[0], skp[idx[i]].pt[1]))
            
            rhs_pt = np.array(rhs)
            lhs_pt = np.array(lhs)
            if( len(rhs_pt) < 16 or len(lhs_pt) < 16 ):
                return None
            homography = []
            (homography,mask) = cv2.findHomography(lhs_pt,rhs_pt,cv2.RANSAC, ransacReprojThreshold=1.0 )
            w = template.width
            h = template.height
            
            pts = np.array([[0,0],[0,h],[w,h],[w,0]], dtype="float32")
            
            pPts = cv2.perspectiveTransform(np.array([pts]), homography)
            
            pt0i = (pPts[0][0][1], pPts[0][0][0])
            pt1i = (pPts[0][1][1], pPts[0][1][0])
            pt2i = (pPts[0][2][1], pPts[0][2][0])
            pt3i = (pPts[0][3][1], pPts[0][3][0])
            
            #construct the feature set and return it.
            fs = FeatureSet() 
            fs.append(KeypointMatch(self,template,(pt0i,pt1i,pt2i,pt3i),homography))
            #the homography matrix is necessary for many purposes like image stitching.
            #fs.append(homography) # No need to add homography as it is already being
            #added in KeyPointMatch class.
            return fs
        else:
            return None


    def findKeypoints(self,min_quality=300.00,flavor="SURF",highQuality=False ):
        """
        **SUMMARY**

        This method finds keypoints in an image and returns them as a feature set.
        Keypoints are unique regions in an image that demonstrate some degree of
        invariance to changes in camera pose and illumination. They are helpful
        for calculating homographies between camera views, object rotations, and
        multiple view overlaps.

        We support four keypoint detectors and only one form of keypoint descriptors.
        Only the surf flavor of keypoint returns feature and descriptors at this time.

        **PARAMETERS**

        * *min_quality* - The minimum quality metric for SURF descriptors. Good values
          range between about 300.00 and 600.00

        * *flavor* - a string indicating the method to use to extract features.
          A good primer on how feature/keypoint extractiors can be found in
          `feature detection on wikipedia <http://en.wikipedia.org/wiki/Feature_detection_(computer_vision)>`_
          and
          `this tutorial. <http://www.cg.tu-berlin.de/fileadmin/fg144/Courses/07WS/compPhoto/Feature_Detection.pdf>`_


          * "SURF" - extract the SURF features and descriptors. If you don't know
            what to use, use this.

            See: http://en.wikipedia.org/wiki/SURF

          * "STAR" - The STAR feature extraction algorithm

            See: http://pr.willowgarage.com/wiki/Star_Detector

          * "FAST" - The FAST keypoint extraction algorithm

            See: http://en.wikipedia.org/wiki/Corner_detection#AST_based_feature_detectors

          All the flavour specified below are for OpenCV versions >= 2.4.0 :

          * "MSER" - Maximally Stable Extremal Regions algorithm

            See: http://en.wikipedia.org/wiki/Maximally_stable_extremal_regions

          * "Dense" -

          * "ORB" - The Oriented FAST and Rotated BRIEF

            See: http://www.willowgarage.com/sites/default/files/orb_final.pdf

          * "SIFT" - Scale-invariant feature transform

            See: http://en.wikipedia.org/wiki/Scale-invariant_feature_transform

          * "BRISK" - Binary Robust Invariant Scalable Keypoints

            See: http://www.asl.ethz.ch/people/lestefan/personal/BRISK

           * "FREAK" - Fast Retina Keypoints

             See: http://www.ivpe.com/freak.htm
             Note: It's a keypoint descriptor and not a KeyPoint detector. SIFT KeyPoints
             are detected and FERAK is used to extract keypoint descriptor.

        * *highQuality* - The SURF descriptor comes in two forms, a vector of 64 descriptor
          values and a vector of 128 descriptor values. The latter are "high"
          quality descriptors.

        **RETURNS**

        A feature set of KeypointFeatures. These KeypointFeatures let's you draw each
        feature, crop the features, get the feature descriptors, etc.

        **EXAMPLE**

        >>> img = Image("aerospace.jpg")
        >>> fs = img.findKeypoints(flavor="SURF",min_quality=500,highQuality=True)
        >>> fs = fs.sortArea()
        >>> fs[-1].draw()
        >>> img.draw()

        **NOTES**

        If you would prefer to work with the raw keypoints and descriptors each image keeps
        a local cache of the raw values. These are named:

        :py:meth:`_getRawKeypoints`
        :py:meth:`_getFLANNMatches`
        :py:meth:`drawKeypointMatches`
        :py:meth:`findKeypoints`

        """
        try:
            import cv2
        except:
            logger.warning("Can't use Keypoints without OpenCV >= 2.3.0")
            return None

        fs = FeatureSet()
        kp = []
        d = []
        if highQuality:
            kp,d = self._getRawKeypoints(thresh=min_quality,forceReset=True,flavor=flavor,highQuality=1)
        else:
            kp,d = self._getRawKeypoints(thresh=min_quality,forceReset=True,flavor=flavor,highQuality=0)

        if( flavor in ["ORB", "SIFT", "SURF", "BRISK", "FREAK"]  and kp!=None and d !=None ):
            for i in range(0,len(kp)):
                fs.append(KeyPoint(self,kp[i],d[i],flavor))
        elif(flavor in ["FAST", "STAR", "MSER", "Dense"] and kp!=None ):
            for i in range(0,len(kp)):
                fs.append(KeyPoint(self,kp[i],None,flavor))
        else:
            logger.warning("ImageClass.Keypoints: I don't know the method you want to use")
            return None

        return fs

    def findMotion(self, previous_frame, window=11, method='BM', aggregate=True):
        """
        **SUMMARY**

        findMotion performs an optical flow calculation. This method attempts to find
        motion between two subsequent frames of an image. You provide it
        with the previous frame image and it returns a feature set of motion
        fetures that are vectors in the direction of motion.

        **PARAMETERS**

        * *previous_frame* - The last frame as an Image.
        * *window* - The block size for the algorithm. For the the HS and LK methods
          this is the regular sample grid at which we return motion samples.
          For the block matching method this is the matching window size.
        * *method* - The algorithm to use as a string.
          Your choices are:

          * 'BM' - default block matching robust but slow - if you are unsure use this.

          * 'LK' - `Lucas-Kanade method <http://en.wikipedia.org/wiki/Lucas%E2%80%93Kanade_method>`_

          * 'HS' - `Horn-Schunck method <http://en.wikipedia.org/wiki/Horn%E2%80%93Schunck_method>`_

        * *aggregate* - If aggregate is true, each of our motion features is the average of
          motion around the sample grid defined by window. If aggregate is false
          we just return the the value as sampled at the window grid interval. For
          block matching this flag is ignored.

        **RETURNS**

        A featureset of motion objects.

        **EXAMPLES**

        >>> cam = Camera()
        >>> img1 = cam.getImage()
        >>> img2 = cam.getImage()
        >>> motion = img2.findMotion(img1)
        >>> motion.draw()
        >>> img2.show()

        **SEE ALSO**

        :py:class:`Motion`
        :py:class:`FeatureSet`

        """
        try:
            import cv2
            ver = cv2.__version__
            #For OpenCV versions till 2.4.0,  cv2.__versions__ are of the form "$Rev: 4557 $"
            if not ver.startswith('$Rev:') :
                if int(ver.replace('.','0'))>=20400 :
                    FLAG_VER = 1
                    if (window > 9):
                        window = 9
            else :
                FLAG_VER = 0
        except :
            FLAG_VER = 0

        if( self.width != previous_frame.width or self.height != previous_frame.height):
            logger.warning("ImageClass.getMotion: To find motion the current and previous frames must match")
            return None
        fs = FeatureSet()
        max_mag = 0.00

        if( method == "LK" or method == "HS" ):
            # create the result images.
            xf = cv.CreateImage((self.width, self.height), cv.IPL_DEPTH_32F, 1)
            yf = cv.CreateImage((self.width, self.height), cv.IPL_DEPTH_32F, 1)
            win = (window,window)
            if( method == "LK" ):
                cv.CalcOpticalFlowLK(self._getGrayscaleBitmap(),previous_frame._getGrayscaleBitmap(),win,xf,yf)
            else:
                cv.CalcOpticalFlowHS(previous_frame._getGrayscaleBitmap(),self._getGrayscaleBitmap(),0,xf,yf,1.0,(cv.CV_TERMCRIT_ITER | cv.CV_TERMCRIT_EPS, 10, 0.01))

            w = math.floor((float(window))/2.0)
            cx = ((self.width-window)/window)+1 #our sample rate
            cy = ((self.height-window)/window)+1
            vx = 0.00
            vy = 0.00
            for x in range(0,int(cx)): # go through our sample grid
                for y in range(0,int(cy)):
                    xi = (x*window)+w # calculate the sample point
                    yi = (y*window)+w
                    if( aggregate ):
                        lowx = int(xi-w)
                        highx = int(xi+w)
                        lowy = int(yi-w)
                        highy = int(yi+w)
                        xderp = xf[lowy:highy,lowx:highx] # get the average x/y components in the output
                        yderp = yf[lowy:highy,lowx:highx]
                        vx = np.average(xderp)
                        vy = np.average(yderp)
                    else: # other wise just sample
                        vx = xf[yi,xi]
                        vy = yf[yi,xi]

                    mag = (vx*vx)+(vy*vy)
                    if(mag > max_mag): # calculate the max magnitude for normalizing our vectors
                        max_mag = mag
                    fs.append(Motion(self,xi,yi,vx,vy,window)) # add the sample to the feature set

        elif( method == "BM"):
            # In the interest of keep the parameter list short
            # I am pegging these to the window size.
            # For versions with OpenCV 2.4.0 and below.
            if ( FLAG_VER==0):
                block = (window,window) # block size
                shift = (int(window*1.2),int(window*1.2)) # how far to shift the block
                spread = (window*2,window*2) # the search windows.
                wv = (self.width - block[0]) / shift[0] # the result image size
                hv = (self.height - block[1]) / shift[1]
                xf = cv.CreateMat(hv, wv, cv.CV_32FC1)
                yf = cv.CreateMat(hv, wv, cv.CV_32FC1)
                cv.CalcOpticalFlowBM(previous_frame._getGrayscaleBitmap(),self._getGrayscaleBitmap(),block,shift,spread,0,xf,yf)

            #For versions with OpenCV 2.4.0 and above.
            elif ( FLAG_VER==1) :
                block = (window,window) # block size
                shift = (int(window*0.2),int(window*0.2)) # how far to shift the block
                spread = (window,window) # the search windows.
                wv = self.width-block[0]+shift[0]
                hv = self.height-block[1]+shift[1]
                xf = cv.CreateImage((wv,hv), cv.IPL_DEPTH_32F, 1)
                yf = cv.CreateImage((wv,hv), cv.IPL_DEPTH_32F, 1)
                cv.CalcOpticalFlowBM(previous_frame._getGrayscaleBitmap(),self._getGrayscaleBitmap(),block,shift,spread,0,xf,yf)

            for x in range(0,int(wv)): # go through the sample grid
                for y in range(0,int(hv)):
                    xi = (shift[0]*(x))+block[0] #where on the input image the samples live
                    yi = (shift[1]*(y))+block[1]
                    vx = xf[y,x] # the result image values
                    vy = yf[y,x]
                    fs.append(Motion(self,xi,yi,vx,vy,window)) # add the feature
                    mag = (vx*vx)+(vy*vy) # same the magnitude
                    if(mag > max_mag):
                        max_mag = mag
        else:
            logger.warning("ImageClass.findMotion: I don't know what algorithm you want to use. Valid method choices are Block Matching -> \"BM\" Horn-Schunck -> \"HS\" and Lucas-Kanade->\"LK\" ")
            return None

        max_mag = math.sqrt(max_mag) # do the normalization
        for f in fs:
            f.normalizeTo(max_mag)

        return fs



    def _generatePalette(self,bins,hue, centroids = None):
        """
        **SUMMARY**

        This is the main entry point for palette generation. A palette, for our purposes,
        is a list of the main colors in an image. Creating a palette with 10 bins, tries
        to cluster the colors in rgb space into ten distinct groups. In hue space we only
        look at the hue channel. All of the relevant palette data is cached in the image
        class.

        **PARAMETERS**

        * *bins* - an integer number of bins into which to divide the colors in the image.
        * *hue* - if hue is true we do only cluster on the image hue values.
        * *centroids* - A list of tuples that are the initial k-means estimates. This is handy if you want consisten results from the palettize.

        **RETURNS**

        Nothing, but creates the image's cached values for:

        self._mDoHuePalette
        self._mPaletteBins
        self._mPalette
        self._mPaletteMembers
        self._mPalettePercentages


        **EXAMPLE**

        >>> img._generatePalette(bins=42)

        **NOTES**

        The hue calculations should be siginificantly faster than the generic RGB calculation as
        it works in a one dimensional space. Sometimes the underlying scipy method freaks out
        about k-means initialization with the following warning:

        UserWarning: One of the clusters is empty. Re-run kmean with a different initialization.

        This shouldn't be a real problem.

        **SEE ALSO**

        ImageClass.getPalette(self,bins=10,hue=False
        ImageClass.rePalette(self,palette,hue=False):
        ImageClass.drawPaletteColors(self,size=(-1,-1),horizontal=True,bins=10,hue=False)
        ImageClass.palettize(self,bins=10,hue=False)
        ImageClass.binarizeFromPalette(self, palette_selection)
        ImageClass.findBlobsFromPalette(self, palette_selection, dilate = 0, minsize=5, maxsize=0)
        """
        if( self._mPaletteBins != bins or
            self._mDoHuePalette != hue ):
            total = float(self.width*self.height)
            percentages = []
            result = None
            if( not hue ):
                pixels = np.array(self.getNumpy()).reshape(-1, 3)   #reshape our matrix to 1xN
                if( centroids == None ):
                    result = scv.kmeans(pixels,bins)
                else:
                    if(isinstance(centroids,list)):
                        centroids = np.array(centroids,dtype='uint8')
                    result = scv.kmeans(pixels,centroids)

                self._mPaletteMembers = scv.vq(pixels,result[0])[0]

            else:
                hsv = self
                if( self._colorSpace != ColorSpace.HSV ):
                    hsv = self.toHSV()

                h = hsv.getEmpty(1)
                cv.Split(hsv.getBitmap(),None,None,h,None)
                mat =  cv.GetMat(h)
                pixels = np.array(mat).reshape(-1,1)

                if( centroids == None ):
                    result = scv.kmeans(pixels,bins)
                else:
                    if(isinstance( centroids,list)):
                        centroids = np.array( centroids,dtype='uint8')
                        centroids = centroids.reshape(centroids.shape[0],1)
                    result = scv.kmeans(pixels,centroids)

                self._mPaletteMembers = scv.vq(pixels,result[0])[0]


            for i in range(0,bins):
                count = np.where(self._mPaletteMembers==i)
                v = float(count[0].shape[0])/total
                percentages.append(v)

            self._mDoHuePalette = hue
            self._mPaletteBins = bins
            self._mPalette = np.array(result[0],dtype='uint8')
            self._mPalettePercentages = percentages


    def getPalette(self,bins=10,hue=False,centroids=None):
        """
        **SUMMARY**

        This method returns the colors in the palette of the image. A palette is the
        set of the most common colors in an image. This method is helpful for segmentation.

        **PARAMETERS**

        * *bins* - an integer number of bins into which to divide the colors in the image.
        * *hue*  - if hue is true we do only cluster on the image hue values.
        * *centroids* - A list of tuples that are the initial k-means estimates. This is handy if you want consisten results from the palettize.

        **RETURNS**

        A numpy array of the BGR color tuples.

        **EXAMPLE**

        >>> p = img.getPalette(bins=42)
        >>> print p[2]

        **NOTES**

        The hue calculations should be siginificantly faster than the generic RGB calculation as
        it works in a one dimensional space. Sometimes the underlying scipy method freaks out
        about k-means initialization with the following warning:

        .. Warning::
          One of the clusters is empty. Re-run kmean with a different initialization.
          This shouldn't be a real problem.

        **SEE ALSO**

        :py:meth:`rePalette`
        :py:meth:`drawPaletteColors`
        :py:meth:`palettize`
        :py:meth:`getPalette`
        :py:meth:`binarizeFromPalette`
        :py:meth:`findBlobsFromPalette`

        """
        self._generatePalette(bins,hue,centroids)
        return self._mPalette


    def rePalette(self,palette,hue=False):
        """
        **SUMMARY**

        rePalette takes in the palette from another image and attempts to apply it to this image.
        This is helpful if you want to speed up the palette computation for a series of images (like those in a
        video stream.

        **PARAMETERS**

        * *palette* - The pre-computed palette from another image.
        * *hue* - Boolean Hue - if hue is True we use a hue palette, otherwise we use a BGR palette.

        **RETURNS**

        A SimpleCV Image.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> img2 = Image("logo")
        >>> p = img.getPalette()
        >>> result = img2.rePalette(p)
        >>> result.show()

        **SEE ALSO**

        :py:meth:`rePalette`
        :py:meth:`drawPaletteColors`
        :py:meth:`palettize`
        :py:meth:`getPalette`
        :py:meth:`binarizeFromPalette`
        :py:meth:`findBlobsFromPalette`

        """
        retVal = None
        if(hue):
            hsv = self
            if( self._colorSpace != ColorSpace.HSV ):
                hsv = self.toHSV()

            h = hsv.getEmpty(1)
            cv.Split(hsv.getBitmap(),None,None,h,None)
            mat =  cv.GetMat(h)
            pixels = np.array(mat).reshape(-1,1)
            result = scv.vq(pixels,palette)
            derp = palette[result[0]]
            retVal = Image(derp[::-1].reshape(self.height,self.width)[::-1])
            retVal = retVal.rotate(-90,fixed=False)
            retVal._mDoHuePalette = True
            retVal._mPaletteBins = len(palette)
            retVal._mPalette = palette
            retVal._mPaletteMembers = result[0]

        else:
            result = scv.vq(self.getNumpy().reshape(-1,3),palette)
            retVal = Image(palette[result[0]].reshape(self.width,self.height,3))
            retVal._mDoHuePalette = False
            retVal._mPaletteBins = len(palette)
            retVal._mPalette = palette
            pixels = np.array(self.getNumpy()).reshape(-1, 3)
            retVal._mPaletteMembers = scv.vq(pixels,palette)[0]

        percentages = []
        total = self.width*self.height
        for i in range(0,len(palette)):
            count = np.where(self._mPaletteMembers==i)
            v = float(count[0].shape[0])/total
            percentages.append(v)
        self._mPalettePercentages = percentages
        return retVal

    def drawPaletteColors(self,size=(-1,-1),horizontal=True,bins=10,hue=False):
        """
        **SUMMARY**

        This method returns the visual representation (swatches) of the palette in an image. The palette
        is orientated either horizontally or vertically, and each color is given an area
        proportional to the number of pixels that have that color in the image. The palette
        is arranged as it is returned from the clustering algorithm. When size is left
        to its default value, the palette size will match the size of the
        orientation, and then be 10% of the other dimension. E.g. if our image is 640X480 the horizontal
        palette will be (640x48) likewise the vertical palette will be (480x64)

        If a Hue palette is used this method will return a grayscale palette.

        **PARAMETERS**

        * *bins* - an integer number of bins into which to divide the colors in the image.
        * *hue* - if hue is true we do only cluster on the image hue values.
        * *size* - The size of the generated palette as a (width,height) tuple, if left default we select
          a size based on the image so it can be nicely displayed with the
          image.
        * *horizontal* - If true we orientate our palette horizontally, otherwise vertically.

        **RETURNS**

        A palette swatch image.

        **EXAMPLE**

        >>> p = img1.drawPaletteColors()
        >>> img2 = img1.sideBySide(p,side="bottom")
        >>> img2.show()

        **NOTES**

        The hue calculations should be siginificantly faster than the generic RGB calculation as
        it works in a one dimensional space. Sometimes the underlying scipy method freaks out
        about k-means initialization with the following warning:

        .. Warning::
          One of the clusters is empty. Re-run kmean with a different initialization.
          This shouldn't be a real problem.

        **SEE ALSO**

        :py:meth:`rePalette`
        :py:meth:`drawPaletteColors`
        :py:meth:`palettize`
        :py:meth:`getPalette`
        :py:meth:`binarizeFromPalette`
        :py:meth:`findBlobsFromPalette`

        """
        self._generatePalette(bins,hue)
        retVal = None
        if( not hue ):
            if( horizontal ):
                if( size[0] == -1 or size[1] == -1 ):
                    size = (int(self.width),int(self.height*.1))
                pal = cv.CreateImage(size, cv.IPL_DEPTH_8U, 3)
                cv.Zero(pal)
                idxL = 0
                idxH = 0
                for i in range(0,bins):
                    idxH =np.clip(idxH+(self._mPalettePercentages[i]*float(size[0])),0,size[0]-1)
                    roi = (int(idxL),0,int(idxH-idxL),size[1])
                    cv.SetImageROI(pal,roi)
                    color = np.array((float(self._mPalette[i][2]),float(self._mPalette[i][1]),float(self._mPalette[i][0])))
                    cv.AddS(pal,color,pal)
                    cv.ResetImageROI(pal)
                    idxL = idxH
                retVal = Image(pal)
            else:
                if( size[0] == -1 or size[1] == -1 ):
                    size = (int(self.width*.1),int(self.height))
                pal = cv.CreateImage(size, cv.IPL_DEPTH_8U, 3)
                cv.Zero(pal)
                idxL = 0
                idxH = 0
                for i in range(0,bins):
                    idxH =np.clip(idxH+self._mPalettePercentages[i]*size[1],0,size[1]-1)
                    roi = (0,int(idxL),size[0],int(idxH-idxL))
                    cv.SetImageROI(pal,roi)
                    color = np.array((float(self._mPalette[i][2]),float(self._mPalette[i][1]),float(self._mPalette[i][0])))
                    cv.AddS(pal,color,pal)
                    cv.ResetImageROI(pal)
                    idxL = idxH
                retVal = Image(pal)
        else: # do hue
            if( horizontal ):
                if( size[0] == -1 or size[1] == -1 ):
                    size = (int(self.width),int(self.height*.1))
                pal = cv.CreateImage(size, cv.IPL_DEPTH_8U, 1)
                cv.Zero(pal)
                idxL = 0
                idxH = 0
                for i in range(0,bins):
                    idxH =np.clip(idxH+(self._mPalettePercentages[i]*float(size[0])),0,size[0]-1)
                    roi = (int(idxL),0,int(idxH-idxL),size[1])
                    cv.SetImageROI(pal,roi)
                    cv.AddS(pal,float(self._mPalette[i]),pal)
                    cv.ResetImageROI(pal)
                    idxL = idxH
                retVal = Image(pal)
            else:
                if( size[0] == -1 or size[1] == -1 ):
                    size = (int(self.width*.1),int(self.height))
                pal = cv.CreateImage(size, cv.IPL_DEPTH_8U, 1)
                cv.Zero(pal)
                idxL = 0
                idxH = 0
                for i in range(0,bins):
                    idxH =np.clip(idxH+self._mPalettePercentages[i]*size[1],0,size[1]-1)
                    roi = (0,int(idxL),size[0],int(idxH-idxL))
                    cv.SetImageROI(pal,roi)
                    cv.AddS(pal,float(self._mPalette[i]),pal)
                    cv.ResetImageROI(pal)
                    idxL = idxH
                retVal = Image(pal)

        return retVal

    def palettize(self,bins=10,hue=False,centroids=None):
        """
        **SUMMARY**

        This method analyzes an image and determines the most common colors using a k-means algorithm.
        The method then goes through and replaces each pixel with the centroid of the clutsters found
        by k-means. This reduces the number of colors in an image to the number of bins. This can be particularly
        handy for doing segementation based on color.

        **PARAMETERS**

        * *bins* - an integer number of bins into which to divide the colors in the image.
        * *hue* - if hue is true we do only cluster on the image hue values.


        **RETURNS**

        An image matching the original where each color is replaced with its palette value.

        **EXAMPLE**

        >>> img2 = img1.palettize()
        >>> img2.show()

        **NOTES**

        The hue calculations should be siginificantly faster than the generic RGB calculation as
        it works in a one dimensional space. Sometimes the underlying scipy method freaks out
        about k-means initialization with the following warning:

        .. Warning::
          UserWarning: One of the clusters is empty. Re-run kmean with a different initialization.
          This shouldn't be a real problem.

        **SEE ALSO**

        :py:meth:`rePalette`
        :py:meth:`drawPaletteColors`
        :py:meth:`palettize`
        :py:meth:`getPalette`
        :py:meth:`binarizeFromPalette`
        :py:meth:`findBlobsFromPalette`

        """
        retVal = None
        self._generatePalette(bins,hue,centroids)
        if( hue ):
            derp = self._mPalette[self._mPaletteMembers]
            retVal = Image(derp[::-1].reshape(self.height,self.width)[::-1])
            retVal = retVal.rotate(-90,fixed=False)
        else:
            retVal = Image(self._mPalette[self._mPaletteMembers].reshape(self.width,self.height,3))
        return retVal


    def findBlobsFromPalette(self, palette_selection, dilate = 0, minsize=5, maxsize=0,appx_level=3):
        """
        **SUMMARY**

        This method attempts to use palettization to do segmentation and behaves similar to the
        findBlobs blob in that it returs a feature set of blob objects. Once a palette has been
        extracted using getPalette() we can then select colors from that palette to be labeled
        white within our blobs.

        **PARAMETERS**

        * *palette_selection* - color triplets selected from our palette that will serve turned into blobs
          These values can either be a 3xN numpy array, or a list of RGB triplets.
        * *dilate* - the optional number of dilation operations to perform on the binary image
          prior to performing blob extraction.
        * *minsize* - the minimum blob size in pixels
        * *maxsize* - the maximim blob size in pixels.
        * *appx_level* - The blob approximation level - an integer for the maximum distance between the true edge and the
          approximation edge - lower numbers yield better approximation.

        **RETURNS**

        If the method executes successfully a FeatureSet of Blobs is returned from the image. If the method
        fails a value of None is returned.

       **EXAMPLE**

        >>> img = Image("lenna")
        >>> p = img.getPalette()
        >>> blobs = img.findBlobsFromPalette( (p[0],p[1],[6]) )
        >>> blobs.draw()
        >>> img.show()

        **SEE ALSO**

        :py:meth:`rePalette`
        :py:meth:`drawPaletteColors`
        :py:meth:`palettize`
        :py:meth:`getPalette`
        :py:meth:`binarizeFromPalette`
        :py:meth:`findBlobsFromPalette`

        """

        #we get the palette from find palete
        #ASSUME: GET PALLETE WAS CALLED!
        bwimg = self.binarizeFromPalette(palette_selection)
        if( dilate > 0 ):
            bwimg =bwimg.dilate(dilate)

        if (maxsize == 0):
            maxsize = self.width * self.height
        #create a single channel image, thresholded to parameters

        blobmaker = BlobMaker()
        blobs = blobmaker.extractFromBinary(bwimg,
            self, minsize = minsize, maxsize = maxsize,appx_level=appx_level)

        if not len(blobs):
            return None
        return blobs


    def binarizeFromPalette(self, palette_selection):
        """
        **SUMMARY**

        This method uses the color palette to generate a binary (black and white) image. Palaette selection
        is a list of color tuples retrieved from img.getPalette(). The provided values will be drawn white
        while other values will be black.

        **PARAMETERS**

        palette_selection - color triplets selected from our palette that will serve turned into blobs
        These values can either be a 3xN numpy array, or a list of RGB triplets.

        **RETURNS**

        This method returns a black and white images, where colors that are close to the colors
        in palette_selection are set to white

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> p = img.getPalette()
        >>> b = img.binarizeFromPalette( (p[0],p[1],[6]) )
        >>> b.show()

        **SEE ALSO**

        :py:meth:`rePalette`
        :py:meth:`drawPaletteColors`
        :py:meth:`palettize`
        :py:meth:`getPalette`
        :py:meth:`binarizeFromPalette`
        :py:meth:`findBlobsFromPalette`

        """

        #we get the palette from find palete
        #ASSUME: GET PALLETE WAS CALLED!
        if( self._mPalette == None ):
            logger.warning("Image.binarizeFromPalette: No palette exists, call getPalette())")
            return None
        retVal = None
        img = self.palettize(self._mPaletteBins, hue=self._mDoHuePalette)
        if( not self._mDoHuePalette ):
            npimg = img.getNumpy()
            white = np.array([255,255,255])
            black = np.array([0,0,0])

            for p in palette_selection:
                npimg = np.where(npimg != p,npimg,white)

            npimg = np.where(npimg != white,black,white)
            retVal = Image(npimg)
        else:
            npimg = img.getNumpy()[:,:,1]
            white = np.array([255])
            black = np.array([0])

            for p in palette_selection:
                npimg = np.where(npimg != p,npimg,white)

            npimg = np.where(npimg != white,black,white)
            retVal = Image(npimg)

        return retVal

    def skeletonize(self, radius = 5):
        """
        **SUMMARY**

        Skeletonization is the process of taking in a set of blobs (here blobs are white
        on a black background) and finding a squigly line that would be the back bone of
        the blobs were they some sort of vertebrate animal. Another way of thinking about
        skeletonization is that it finds a series of lines that approximates a blob's shape.

        A good summary can be found here:

        http://www.inf.u-szeged.hu/~palagyi/skel/skel.html

        **PARAMETERS**

        * *radius* - an intenger that defines how roughly how wide a blob must be to be added
          to the skeleton, lower values give more skeleton lines, higher values give
          fewer skeleton lines.

        **EXAMPLE**

        >>> cam = Camera()
        >>> while True:
        >>>     img = cam.getImage()
        >>>     b = img.binarize().invert()
        >>>     s = img.skeletonize()
        >>>     r = b-s
        >>>     r.show()


        **NOTES**

        This code was a suggested improvement by Alex Wiltchko, check out his awesome blog here:

        http://alexbw.posterous.com/

        """
        img = self.toGray().getNumpy()[:,:,0]
        distance_img = ndimage.distance_transform_edt(img)
        morph_laplace_img = ndimage.morphological_laplace(distance_img, (radius, radius))
        skeleton = morph_laplace_img < morph_laplace_img.min()/2
        retVal = np.zeros([self.width,self.height])
        retVal[skeleton] = 255
        return Image(retVal)

    def smartThreshold(self, mask=None, rect=None):
        """
        **SUMMARY**

        smartThreshold uses a method called grabCut, also called graph cut, to
        automagically generate a grayscale mask image. The dumb version of threshold
        just uses color, smartThreshold looks at
        both color and edges to find a blob. To work smartThreshold needs either a
        rectangle that bounds the object you want to find, or a mask. If you use
        a rectangle make sure it holds the complete object. In the case of a mask, it
        need not be a normal binary mask, it can have the normal white foreground and black
        background, but also a light and dark gray values that correspond to areas
        that are more likely to be foreground and more likely to be background. These
        values can be found in the color class as Color.BACKGROUND, Color.FOREGROUND,
        Color.MAYBE_BACKGROUND, and Color.MAYBE_FOREGROUND.

        **PARAMETERS**

        * *mask* - A grayscale mask the same size as the image using the 4 mask color values
        * *rect* - A rectangle tuple of the form (x_position,y_position,width,height)

        **RETURNS**

        A grayscale image with the foreground / background values assigned to:

        * BACKGROUND = (0,0,0)

        * MAYBE_BACKGROUND = (64,64,64)

        * MAYBE_FOREGROUND =  (192,192,192)

        * FOREGROUND = (255,255,255)

        **EXAMPLE**

        >>> img = Image("RatTop.png")
        >>> mask = Image((img.width,img.height))
        >>> mask.dl().circle((100,100),80,color=Color.MAYBE_BACKGROUND,filled=True)
        >>> mask.dl().circle((100,100),60,color=Color.MAYBE_FOREGROUND,filled=True)
        >>> mask.dl().circle((100,100),40,color=Color.FOREGROUND,filled=True)
        >>> mask = mask.applyLayers()
        >>> new_mask = img.smartThreshold(mask=mask)
        >>> new_mask.show()

        **NOTES**

        http://en.wikipedia.org/wiki/Graph_cuts_in_computer_vision

        **SEE ALSO**

        :py:meth:`smartFindBlobs`

        """
        try:
            import cv2
        except:
            logger.warning("Can't Do GrabCut without OpenCV >= 2.3.0")
            return
        retVal = []
        if( mask is not None ):
            bmp = mask._getGrayscaleBitmap()
            # translate the human readable images to something opencv wants using a lut
            LUT = np.zeros((256,1),dtype=uint8)
            LUT[255]=1
            LUT[64]=2
            LUT[192]=3
            cv.LUT(bmp,bmp,cv.fromarray(LUT))
            mask_in = np.array(cv.GetMat(bmp))
            # get our image in a flavor grab cut likes
            npimg = np.array(cv.GetMat(self.getBitmap()))
            # require by opencv
            tmp1 = np.zeros((1, 13 * 5))
            tmp2 = np.zeros((1, 13 * 5))
            # do the algorithm
            cv2.grabCut(npimg,mask_in,None,tmp1,tmp2,10,mode=cv2.GC_INIT_WITH_MASK)
            # generate the output image
            output = cv.CreateImageHeader((mask_in.shape[1],mask_in.shape[0]),cv.IPL_DEPTH_8U,1)
            cv.SetData(output,mask_in.tostring(),mask_in.dtype.itemsize*mask_in.shape[1])
            # remap the color space
            LUT = np.zeros((256,1),dtype=uint8)
            LUT[1]=255
            LUT[2]=64
            LUT[3]=192
            cv.LUT(output,output,cv.fromarray(LUT))
            # and create the return value
            mask._graybitmap = None # don't ask me why... but this gets corrupted
            retVal = Image(output)

        elif ( rect is not None ):
            npimg = np.array(cv.GetMat(self.getBitmap()))
            tmp1 = np.zeros((1, 13 * 5))
            tmp2 = np.zeros((1, 13 * 5))
            mask = np.zeros((self.height,self.width),dtype='uint8')
            cv2.grabCut(npimg,mask,rect,tmp1,tmp2,10,mode=cv2.GC_INIT_WITH_RECT)
            bmp = cv.CreateImageHeader((mask.shape[1],mask.shape[0]),cv.IPL_DEPTH_8U,1)
            cv.SetData(bmp,mask.tostring(),mask.dtype.itemsize*mask.shape[1])
            LUT = np.zeros((256,1),dtype=uint8)
            LUT[1]=255
            LUT[2]=64
            LUT[3]=192
            cv.LUT(bmp,bmp,cv.fromarray(LUT))
            retVal = Image(bmp)
        else:
            logger.warning( "ImageClass.findBlobsSmart requires either a mask or a selection rectangle. Failure to provide one of these causes your bytes to splinter and bit shrapnel to hit your pipeline making it asplode in a ball of fire. Okay... not really")
        return retVal

    def smartFindBlobs(self,mask=None,rect=None,thresh_level=2,appx_level=3):
        """
        **SUMMARY**

        smartFindBlobs uses a method called grabCut, also called graph cut, to
        automagically determine the boundary of a blob in the image. The dumb find
        blobs just uses color threshold to find the boundary, smartFindBlobs looks at
        both color and edges to find a blob. To work smartFindBlobs needs either a
        rectangle that bounds the object you want to find, or a mask. If you use
        a rectangle make sure it holds the complete object. In the case of a mask, it
        need not be a normal binary mask, it can have the normal white foreground and black
        background, but also a light and dark gray values that correspond to areas
        that are more likely to be foreground and more likely to be background. These
        values can be found in the color class as Color.BACKGROUND, Color.FOREGROUND,
        Color.MAYBE_BACKGROUND, and Color.MAYBE_FOREGROUND.

        **PARAMETERS**

        * *mask* - A grayscale mask the same size as the image using the 4 mask color values
        * *rect* - A rectangle tuple of the form (x_position,y_position,width,height)
        * *thresh_level* - This represents what grab cut values to use in the mask after the
          graph cut algorithm is run,

          * 1  - means use the foreground, maybe_foreground, and maybe_background values
          * 2  - means use the foreground and maybe_foreground values.
          * 3+ - means use just the foreground

        * *appx_level* - The blob approximation level - an integer for the maximum distance between the true edge and the
          approximation edge - lower numbers yield better approximation.


        **RETURNS**

        A featureset of blobs. If everything went smoothly only a couple of blobs should
        be present.

        **EXAMPLE**

        >>> img = Image("RatTop.png")
        >>> mask = Image((img.width,img.height))
        >>> mask.dl().circle((100,100),80,color=Color.MAYBE_BACKGROUND,filled=True
        >>> mask.dl().circle((100,100),60,color=Color.MAYBE_FOREGROUND,filled=True)
        >>> mask.dl().circle((100,100),40,color=Color.FOREGROUND,filled=True)
        >>> mask = mask.applyLayers()
        >>> blobs = img.smartFindBlobs(mask=mask)
        >>> blobs.draw()
        >>> blobs.show()

        **NOTES**

        http://en.wikipedia.org/wiki/Graph_cuts_in_computer_vision

        **SEE ALSO**

        :py:meth:`smartThreshold`

        """
        result = self.smartThreshold(mask, rect)
        binary = None
        retVal = None

        if result:
            if( thresh_level ==  1 ):
                result = result.threshold(192)
            elif( thresh_level == 2):
                result = result.threshold(128)
            elif( thresh_level > 2 ):
                result = result.threshold(1)
            bm = BlobMaker()
            retVal = bm.extractFromBinary(result,self,appx_level)

        return retVal

    def threshold(self, value):
        """
        **SUMMARY**

        We roll old school with this vanilla threshold function. It takes your image
        converts it to grayscale, and applies a threshold. Values above the threshold
        are white, values below the threshold are black (note this is in contrast to
        binarize... which is a stupid function that drives me up a wall). The resulting
        black and white image is returned.

        **PARAMETERS**

        * *value* - the threshold, goes between 0 and 255.

        **RETURNS**

        A black and white SimpleCV image.

        **EXAMPLE**

        >>> img = Image("purplemonkeydishwasher.png")
        >>> result = img.threshold(42)

        **NOTES**

        THRESHOLD RULES BINARIZE DROOLS!

        **SEE ALSO**

        :py:meth:`binarize`

        """
        gray = self._getGrayscaleBitmap()
        result = self.getEmpty(1)
        cv.Threshold(gray, result, value, 255, cv.CV_THRESH_BINARY)
        retVal = Image(result)
        return retVal


    def floodFill(self,points,tolerance=None,color=Color.WHITE,lower=None,upper=None,fixed_range=True):
        """
        **SUMMARY**

        FloodFill works just like ye olde paint bucket tool in your favorite image manipulation
        program. You select a point (or a list of points), a color, and a tolerance, and floodFill will start at that
        point, looking for pixels within the tolerance from your intial pixel. If the pixel is in
        tolerance, we will convert it to your color, otherwise the method will leave the pixel alone.
        The method accepts both single values, and triplet tuples for the tolerance values. If you
        require more control over your tolerance you can use the upper and lower values. The fixed
        range parameter let's you toggle between setting the tolerance with repect to the seed pixel,
        and using a tolerance that is relative to the adjacent pixels. If fixed_range is true the
        method will set its tolerance with respect to the seed pixel, otherwise the tolerance will
        be with repsect to adjacent pixels.

        **PARAMETERS**

        * *points* - A tuple, list of tuples, or np.array of seed points for flood fill
        * *tolerance* - The color tolerance as a single value or a triplet.
        * *color* - The color to replace the floodFill pixels with
        * *lower* - If tolerance does not provide enough control you can optionally set the upper and lower values
          around the seed pixel. This value can be a single value or a triplet. This will override
          the tolerance variable.
        * *upper* - If tolerance does not provide enough control you can optionally set the upper and lower values
          around the seed pixel. This value can be a single value or a triplet. This will override
          the tolerance variable.
        * *fixed_range* - If fixed_range is true we use the seed_pixel +/- tolerance
          If fixed_range is false, the tolerance is +/- tolerance of the values of
          the adjacent pixels to the pixel under test.

        **RETURNS**

        An Image where the values similar to the seed pixel have been replaced by the input color.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> img2 = img.floodFill(((10,10),(54,32)),tolerance=(10,10,10),color=Color.RED)
        >>> img2.show()

        **SEE ALSO**

        :py:meth:`floodFillToMask`
        :py:meth:`findFloodFillBlobs`

        """
        if( isinstance(color,np.ndarray) ):
            color = color.tolist()
        elif( isinstance(color,dict) ):
            color = (color['R'],color['G'],color['B'])

        if( isinstance(points,tuple) ):
            points = np.array(points)
        # first we guess what the user wants to do
        # if we get and int/float convert it to a tuple
        if( upper is None and lower is None and tolerance is None ):
            upper = (0,0,0)
            lower = (0,0,0)

        if( tolerance is not None and
            (isinstance(tolerance,float) or isinstance(tolerance,int))):
            tolerance = (int(tolerance),int(tolerance),int(tolerance))

        if( lower is not None and
            (isinstance(lower,float) or isinstance(lower, int)) ):
            lower = (int(lower),int(lower),int(lower))
        elif( lower is None ):
            lower = tolerance

        if( upper is not None and
            (isinstance(upper,float) or isinstance(upper, int)) ):
            upper = (int(upper),int(upper),int(upper))
        elif( upper is None ):
            upper = tolerance

        if( isinstance(points,tuple) ):
            points = np.array(points)

        flags = 8
        if( fixed_range ):
            flags = flags+cv.CV_FLOODFILL_FIXED_RANGE

        bmp = self.getEmpty()
        cv.Copy(self.getBitmap(),bmp)

        if( len(points.shape) != 1 ):
            for p in points:
                cv.FloodFill(bmp,tuple(p),color,lower,upper,flags)
        else:
            cv.FloodFill(bmp,tuple(points),color,lower,upper,flags)

        retVal = Image(bmp)

        return retVal

    def floodFillToMask(self, points,tolerance=None,color=Color.WHITE,lower=None,upper=None,fixed_range=True,mask=None):
        """
        **SUMMARY**

        floodFillToMask works sorta paint bucket tool in your favorite image manipulation
        program. You select a point (or a list of points), a color, and a tolerance, and floodFill will start at that
        point, looking for pixels within the tolerance from your intial pixel. If the pixel is in
        tolerance, we will convert it to your color, otherwise the method will leave the pixel alone.
        Unlike regular floodFill, floodFillToMask, will return a binary mask of your flood fill
        operation. This is handy if you want to extract blobs from an area, or create a
        selection from a region. The method takes in an optional mask. Non-zero values of the mask
        act to block the flood fill operations. This is handy if you want to use an edge image
        to "stop" the flood fill operation within a particular region.

        The method accepts both single values, and triplet tuples for the tolerance values. If you
        require more control over your tolerance you can use the upper and lower values. The fixed
        range parameter let's you toggle between setting the tolerance with repect to the seed pixel,
        and using a tolerance that is relative to the adjacent pixels. If fixed_range is true the
        method will set its tolerance with respect to the seed pixel, otherwise the tolerance will
        be with repsect to adjacent pixels.

        **PARAMETERS**

        * *points* - A tuple, list of tuples, or np.array of seed points for flood fill
        * *tolerance* - The color tolerance as a single value or a triplet.
        * *color* - The color to replace the floodFill pixels with
        * *lower* - If tolerance does not provide enough control you can optionally set the upper and lower values
          around the seed pixel. This value can be a single value or a triplet. This will override
          the tolerance variable.
        * *upper* - If tolerance does not provide enough control you can optionally set the upper and lower values
          around the seed pixel. This value can be a single value or a triplet. This will override
          the tolerance variable.
        * *fixed_range* - If fixed_range is true we use the seed_pixel +/- tolerance
          If fixed_range is false, the tolerance is +/- tolerance of the values of
          the adjacent pixels to the pixel under test.
        * *mask* - An optional mask image that can be used to control the flood fill operation.
          the output of this function will include the mask data in the input mask.

        **RETURNS**

        An Image where the values similar to the seed pixel have been replaced by the input color.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> mask = img.edges()
        >>> mask= img.floodFillToMask(((10,10),(54,32)),tolerance=(10,10,10),mask=mask)
        >>> mask.show

        **SEE ALSO**

        :py:meth:`floodFill`
        :py:meth:`findFloodFillBlobs`

        """
        mask_flag = 255 # flag weirdness

        if( isinstance(color,np.ndarray) ):
            color = color.tolist()
        elif( isinstance(color,dict) ):
            color = (color['R'],color['G'],color['B'])
        
        if( isinstance(points,tuple) ):
            points = np.array(points)

        # first we guess what the user wants to do
        # if we get and int/float convert it to a tuple
        if( upper is None and lower is None and tolerance is None ):
            upper = (0,0,0)
            lower = (0,0,0)

        if( tolerance is not None and
            (isinstance(tolerance,float) or isinstance(tolerance,int))):
            tolerance = (int(tolerance),int(tolerance),int(tolerance))

        if( lower is not None and
            (isinstance(lower,float) or isinstance(lower, int)) ):
            lower = (int(lower),int(lower),int(lower))
        elif( lower is None ):
            lower = tolerance

        if( upper is not None and
            (isinstance(upper,float) or isinstance(upper, int)) ):
            upper = (int(upper),int(upper),int(upper))
        elif( upper is None ):
            upper = tolerance

        if( isinstance(points,tuple) ):
            points = np.array(points)

        flags = (mask_flag << 8 )+8
        if( fixed_range ):
            flags = flags + cv.CV_FLOODFILL_FIXED_RANGE

        localMask = None
        #opencv wants a mask that is slightly larger
        if( mask is None ):
            localMask  = cv.CreateImage((self.width+2,self.height+2), cv.IPL_DEPTH_8U, 1)
            cv.Zero(localMask)
        else:
            localMask = mask.embiggen(size=(self.width+2,self.height+2))._getGrayscaleBitmap()

        bmp = self.getEmpty()
        cv.Copy(self.getBitmap(),bmp)
        if( len(points.shape) != 1 ):
            for p in points:
                cv.FloodFill(bmp,tuple(p),color,lower,upper,flags,localMask)
        else:
            cv.FloodFill(bmp,tuple(points),color,lower,upper,flags,localMask)

        retVal = Image(localMask)
        retVal = retVal.crop(1,1,self.width,self.height)
        return retVal

    def findBlobsFromMask(self, mask,threshold=128, minsize=10, maxsize=0,appx_level=3 ):
        """
        **SUMMARY**

        This method acts like findBlobs, but it lets you specifiy blobs directly by
        providing a mask image. The mask image must match the size of this image, and
        the mask should have values > threshold where you want the blobs selected. This
        method can be used with binarize, dialte, erode, floodFill, edges etc to
        get really nice segmentation.

        **PARAMETERS**

        * *mask* - The mask image, areas lighter than threshold will be counted as blobs.
          Mask should be the same size as this image.
        * *threshold* - A single threshold value used when we binarize the mask.
        * *minsize* - The minimum size of the returned blobs.
        * *maxsize*  - The maximum size of the returned blobs, if none is specified we peg
          this to the image size.
        * *appx_level* - The blob approximation level - an integer for the maximum distance between the true edge and the
          approximation edge - lower numbers yield better approximation.


        **RETURNS**

        A featureset of blobs. If no blobs are found None is returned.

        **EXAMPLE**

        >>> img = Image("Foo.png")
        >>> mask = img.binarize().dilate(2)
        >>> blobs = img.findBlobsFromMask(mask)
        >>> blobs.show()

        **SEE ALSO**

        :py:meth:`findBlobs`
        :py:meth:`binarize`
        :py:meth:`threshold`
        :py:meth:`dilate`
        :py:meth:`erode`
        """
        if (maxsize == 0):
            maxsize = self.width * self.height
        #create a single channel image, thresholded to parameters
        if( mask.width != self.width or mask.height != self.height ):
            logger.warning("ImageClass.findBlobsFromMask - your mask does not match the size of your image")
            return None

        blobmaker = BlobMaker()
        gray = mask._getGrayscaleBitmap()
        result = mask.getEmpty(1)
        cv.Threshold(gray, result, threshold, 255, cv.CV_THRESH_BINARY)
        blobs = blobmaker.extractFromBinary(Image(result), self, minsize = minsize, maxsize = maxsize,appx_level=appx_level)

        if not len(blobs):
            return None

        return FeatureSet(blobs).sortArea()


    def findFloodFillBlobs(self,points,tolerance=None,lower=None,upper=None,
                           fixed_range=True,minsize=30,maxsize=-1):
        """

        **SUMMARY**

        This method lets you use a flood fill operation and pipe the results to findBlobs. You provide
        the points to seed floodFill and the rest is taken care of.

        floodFill works just like ye olde paint bucket tool in your favorite image manipulation
        program. You select a point (or a list of points), a color, and a tolerance, and floodFill will start at that
        point, looking for pixels within the tolerance from your intial pixel. If the pixel is in
        tolerance, we will convert it to your color, otherwise the method will leave the pixel alone.
        The method accepts both single values, and triplet tuples for the tolerance values. If you
        require more control over your tolerance you can use the upper and lower values. The fixed
        range parameter let's you toggle between setting the tolerance with repect to the seed pixel,
        and using a tolerance that is relative to the adjacent pixels. If fixed_range is true the
        method will set its tolerance with respect to the seed pixel, otherwise the tolerance will
        be with repsect to adjacent pixels.

        **PARAMETERS**

        * *points* - A tuple, list of tuples, or np.array of seed points for flood fill.
        * *tolerance* - The color tolerance as a single value or a triplet.
        * *color* - The color to replace the floodFill pixels with
        * *lower* - If tolerance does not provide enough control you can optionally set the upper and lower values
          around the seed pixel. This value can be a single value or a triplet. This will override
          the tolerance variable.
        * *upper* - If tolerance does not provide enough control you can optionally set the upper and lower values
          around the seed pixel. This value can be a single value or a triplet. This will override
          the tolerance variable.
        * *fixed_range* - If fixed_range is true we use the seed_pixel +/- tolerance
          If fixed_range is false, the tolerance is +/- tolerance of the values of
          the adjacent pixels to the pixel under test.
        * *minsize* - The minimum size of the returned blobs.
        * *maxsize* - The maximum size of the returned blobs, if none is specified we peg
          this to the image size.

        **RETURNS**

        A featureset of blobs. If no blobs are found None is returned.

        An Image where the values similar to the seed pixel have been replaced by the input color.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> blerbs = img.findFloodFillBlobs(((10,10),(20,20),(30,30)),tolerance=30)
        >>> blerbs.show()

        **SEE ALSO**

        :py:meth:`findBlobs`
        :py:meth:`floodFill`

        """
        mask = self.floodFillToMask(points,tolerance,color=Color.WHITE,lower=lower,upper=upper,fixed_range=fixed_range)
        return self.findBlobsFromMask(mask,minsize,maxsize)

    def _doDFT(self, grayscale=False):
        """
        **SUMMARY**

        This private method peforms the discrete Fourier transform on an input image.
        The transform can be applied to a single channel gray image or to each channel of the
        image. Each channel generates a 64F 2 channel IPL image corresponding to the real
        and imaginary components of the DFT. A list of these IPL images are then cached
        in the private member variable _DFT.


        **PARAMETERS**

        * *grayscale* - If grayscale is True we first covert the image to grayscale, otherwise
          we perform the operation on each channel.

        **RETURNS**

        nothing - but creates a locally cached list of IPL imgaes corresponding to the real
        and imaginary components of each channel.

        **EXAMPLE**

        >>> img = Image('logo.png')
        >>> img._doDFT()
        >>> img._DFT[0] # get the b channel Re/Im components

        **NOTES**

        http://en.wikipedia.org/wiki/Discrete_Fourier_transform
        http://math.stackexchange.com/questions/1002/fourier-transform-for-dummies

        **TO DO**

        This method really needs to convert the image to an optimal DFT size.
        http://opencv.itseez.com/modules/core/doc/operations_on_arrays.html#getoptimaldftsize

        """
        if( grayscale and (len(self._DFT) == 0 or len(self._DFT) == 3)):
            self._DFT = []
            img = self._getGrayscaleBitmap()
            width, height = cv.GetSize(img)
            src = cv.CreateImage((width, height), cv.IPL_DEPTH_64F, 2)
            dst = cv.CreateImage((width, height), cv.IPL_DEPTH_64F, 2)
            data = cv.CreateImage((width, height), cv.IPL_DEPTH_64F, 1)
            blank = cv.CreateImage((width, height), cv.IPL_DEPTH_64F, 1)
            cv.ConvertScale(img,data,1.0)
            cv.Zero(blank)
            cv.Merge(data,blank,None,None,src)
            cv.Merge(data,blank,None,None,dst)
            cv.DFT(src, dst, cv.CV_DXT_FORWARD)
            self._DFT.append(dst)
        elif( not grayscale and (len(self._DFT) < 2 )):
            self._DFT = []
            r = self.getEmpty(1)
            g = self.getEmpty(1)
            b = self.getEmpty(1)
            cv.Split(self.getBitmap(),b,g,r,None)
            chans = [b,g,r]
            width = self.width
            height = self.height
            data = cv.CreateImage((width, height), cv.IPL_DEPTH_64F, 1)
            blank = cv.CreateImage((width, height), cv.IPL_DEPTH_64F, 1)
            src = cv.CreateImage((width, height), cv.IPL_DEPTH_64F, 2)
            for c in chans:
                dst = cv.CreateImage((width, height), cv.IPL_DEPTH_64F, 2)
                cv.ConvertScale(c,data,1.0)
                cv.Zero(blank)
                cv.Merge(data,blank,None,None,src)
                cv.Merge(data,blank,None,None,dst)
                cv.DFT(src, dst, cv.CV_DXT_FORWARD)
                self._DFT.append(dst)

    def _getDFTClone(self,grayscale=False):
        """
        **SUMMARY**

        This method works just like _doDFT but returns a deep copy
        of the resulting array which can be used in destructive operations.

        **PARAMETERS**

        * *grayscale* - If grayscale is True we first covert the image to grayscale, otherwise
          we perform the operation on each channel.

        **RETURNS**

        A deep copy of the cached DFT real/imaginary image list.

        **EXAMPLE**

        >>> img = Image('logo.png')
        >>> myDFT = img._getDFTClone()
        >>> SomeCVFunc(myDFT[0])

        **NOTES**

        http://en.wikipedia.org/wiki/Discrete_Fourier_transform
        http://math.stackexchange.com/questions/1002/fourier-transform-for-dummies

        **SEE ALSO**

        ImageClass._doDFT()

        """
        # this is needs to be switched to the optimal
        # DFT size for faster processing.
        self._doDFT(grayscale)
        retVal = []
        if(grayscale):
            gs = cv.CreateImage((self.width,self.height),cv.IPL_DEPTH_64F,2)
            cv.Copy(self._DFT[0],gs)
            retVal.append(gs)
        else:
            for img in self._DFT:
                temp = cv.CreateImage((self.width,self.height),cv.IPL_DEPTH_64F,2)
                cv.Copy(img,temp)
                retVal.append(temp)
        return retVal

    def rawDFTImage(self,grayscale=False):
        """
        **SUMMARY**

        This method returns the **RAW** DFT transform of an image as a list of IPL Images.
        Each result image is a two channel 64f image where the first channel is the real
        component and the second channel is teh imaginary component. If the operation
        is performed on an RGB image and grayscale is False the result is a list of
        these images of the form [b,g,r].

        **PARAMETERS**

        * *grayscale* - If grayscale is True we first covert the image to grayscale, otherwise
          we perform the operation on each channel.

        **RETURNS**

        A list of the DFT images (see above). Note that this is a shallow copy operation.

        **EXAMPLE**

        >>> img = Image('logo.png')
        >>> myDFT = img.rawDFTImage()
        >>> for c in myDFT:
        >>>    #do some operation on the DFT

        **NOTES**

        http://en.wikipedia.org/wiki/Discrete_Fourier_transform
        http://math.stackexchange.com/questions/1002/fourier-transform-for-dummies

        **SEE ALSO**

        :py:meth:`rawDFTImage`
        :py:meth:`getDFTLogMagnitude`
        :py:meth:`applyDFTFilter`
        :py:meth:`highPassFilter`
        :py:meth:`lowPassFilter`
        :py:meth:`bandPassFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyButterworthFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyGaussianFilter`
        :py:meth:`applyUnsharpMask`

        """
        self._doDFT(grayscale)
        return self._DFT

    def getDFTLogMagnitude(self,grayscale=False):
        """
        **SUMMARY**

        This method returns the log value of the magnitude image of the DFT transform. This
        method is helpful for examining and comparing the results of DFT transforms. The log
        component helps to "squish" the large floating point values into an image that can
        be rendered easily.

        In the image the low frequency components are in the corners of the image and the high
        frequency components are in the center of the image.

        **PARAMETERS**

        * *grayscale* - if grayscale is True we perform the magnitude operation of the grayscale
          image otherwise we perform the operation on each channel.

        **RETURNS**

        Returns a SimpleCV image corresponding to the log magnitude of the input image.

        **EXAMPLE**

        >>> img = Image("RedDog2.jpg")
        >>> img.getDFTLogMagnitude().show()
        >>> lpf = img.lowPassFilter(img.width/10.img.height/10)
        >>> lpf.getDFTLogMagnitude().show()

        **NOTES**

        * http://en.wikipedia.org/wiki/Discrete_Fourier_transform
        * http://math.stackexchange.com/questions/1002/fourier-transform-for-dummies

        **SEE ALSO**

        :py:meth:`rawDFTImage`
        :py:meth:`getDFTLogMagnitude`
        :py:meth:`applyDFTFilter`

        :py:meth:`highPassFilter`
        :py:meth:`lowPassFilter`
        :py:meth:`bandPassFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyButterworthFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyGaussianFilter`
        :py:meth:`applyUnsharpMask`


        """
        dft = self._getDFTClone(grayscale)
        chans = []
        if( grayscale ):
            chans = [self.getEmpty(1)]
        else:
            chans = [self.getEmpty(1),self.getEmpty(1),self.getEmpty(1)]
        data = cv.CreateImage((self.width, self.height), cv.IPL_DEPTH_64F, 1)
        blank = cv.CreateImage((self.width, self.height), cv.IPL_DEPTH_64F, 1)

        for i in range(0,len(chans)):
            cv.Split(dft[i],data,blank,None,None)
            cv.Pow( data, data, 2.0)
            cv.Pow( blank, blank, 2.0)
            cv.Add( data, blank, data, None)
            cv.Pow( data, data, 0.5 )
            cv.AddS( data, cv.ScalarAll(1.0), data, None ) # 1 + Mag
            cv.Log( data, data ) # log(1 + Mag
            min, max, pt1, pt2 = cv.MinMaxLoc(data)
            cv.Scale(data, data, 1.0/(max-min), 1.0*(-min)/(max-min))
            cv.Mul(data,data,data,255.0)
            cv.Convert(data,chans[i])

        retVal = None
        if( grayscale ):
            retVal = Image(chans[0])
        else:
            retVal = self.getEmpty()
            cv.Merge(chans[0],chans[1],chans[2],None,retVal)
            retVal = Image(retVal)
        return retVal

    def _boundsFromPercentage(self, floatVal, bound):
        return np.clip(int(floatVal*bound),0,bound)

    def applyDFTFilter(self,flt,grayscale=False):
        """
        **SUMMARY**

        This function allows you to apply an arbitrary filter to the DFT of an image.
        This filter takes in a gray scale image, whiter values are kept and black values
        are rejected. In the DFT image, the lower frequency values are in the corners
        of the image, while the higher frequency components are in the center. For example,
        a low pass filter has white squares in the corners and is black everywhere else.

        **PARAMETERS**

        * *grayscale* - if this value is True we perfrom the operation on the DFT of the gray
          version of the image and the result is gray image. If grayscale is true
          we perform the operation on each channel and the recombine them to create
          the result.

        * *flt* - A grayscale filter image. The size of the filter must match the size of
          the image.

        **RETURNS**

        A SimpleCV image after applying the filter.

        **EXAMPLE**

        >>>  filter = Image("MyFilter.png")
        >>>  myImage = Image("MyImage.png")
        >>>  result = myImage.applyDFTFilter(filter)
        >>>  result.show()

        **SEE ALSO**

        :py:meth:`rawDFTImage`
        :py:meth:`getDFTLogMagnitude`
        :py:meth:`applyDFTFilter`
        :py:meth:`highPassFilter`
        :py:meth:`lowPassFilter`
        :py:meth:`bandPassFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyButterworthFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyGaussianFilter`
        :py:meth:`applyUnsharpMask`

        **TODO**

        Make this function support a separate filter image for each channel.
        """
        if isinstance(flt, DFT):
            filteredimage = flt.applyFilter(self, grayscale)
            return filteredimage

        if( flt.width != self.width and
            flt.height != self.height ):
            logger.warning("Image.applyDFTFilter - Your filter must match the size of the image")
        dft = []
        if( grayscale ):
            dft = self._getDFTClone(grayscale)
            flt = flt._getGrayscaleBitmap()
            flt64f = cv.CreateImage((flt.width,flt.height),cv.IPL_DEPTH_64F,1)
            cv.ConvertScale(flt,flt64f,1.0)
            finalFilt = cv.CreateImage((flt.width,flt.height),cv.IPL_DEPTH_64F,2)
            cv.Merge(flt64f,flt64f,None,None,finalFilt)
            for d in dft:
                cv.MulSpectrums(d,finalFilt,d,0)
        else: #break down the filter and then do each channel
            dft = self._getDFTClone(grayscale)
            flt = flt.getBitmap()
            b = cv.CreateImage((flt.width,flt.height),cv.IPL_DEPTH_8U,1)
            g = cv.CreateImage((flt.width,flt.height),cv.IPL_DEPTH_8U,1)
            r = cv.CreateImage((flt.width,flt.height),cv.IPL_DEPTH_8U,1)
            cv.Split(flt,b,g,r,None)
            chans = [b,g,r]
            for c in range(0,len(chans)):
                flt64f = cv.CreateImage((chans[c].width,chans[c].height),cv.IPL_DEPTH_64F,1)
                cv.ConvertScale(chans[c],flt64f,1.0)
                finalFilt = cv.CreateImage((chans[c].width,chans[c].height),cv.IPL_DEPTH_64F,2)
                cv.Merge(flt64f,flt64f,None,None,finalFilt)
                cv.MulSpectrums(dft[c],finalFilt,dft[c],0)

        return self._inverseDFT(dft)

    def _boundsFromPercentage(self, floatVal, bound):
        return np.clip(int(floatVal*(bound/2.00)),0,(bound/2))

    def highPassFilter(self, xCutoff,yCutoff=None,grayscale=False):
        """
        **SUMMARY**

        This method applies a high pass DFT filter. This filter enhances
        the high frequencies and removes the low frequency signals. This has
        the effect of enhancing edges. The frequencies are defined as going between
        0.00 and 1.00 and where 0 is the lowest frequency in the image and 1.0 is
        the highest possible frequencies. Each of the frequencies are defined
        with respect to the horizontal and vertical signal. This filter
        isn't perfect and has a harsh cutoff that causes ringing artifacts.

        **PARAMETERS**

        * *xCutoff* - The horizontal frequency at which we perform the cutoff. A separate
          frequency can be used for the b,g, and r signals by providing a
          list of values. The frequency is defined between zero to one,
          where zero is constant component and 1 is the highest possible
          frequency in the image.

        * *yCutoff* - The cutoff frequencies in the y direction. If none are provided
          we use the same values as provided for x.

        * *grayscale* - if this value is True we perfrom the operation on the DFT of the gray
          version of the image and the result is gray image. If grayscale is true
          we perform the operation on each channel and the recombine them to create
          the result.

        **RETURNS**

        A SimpleCV Image after applying the filter.

        **EXAMPLE**

        >>> img = Image("SimpleCV/sampleimages/RedDog2.jpg")
        >>> img.getDFTLogMagnitude().show()
        >>> hpf = img.highPassFilter([0.2,0.1,0.2])
        >>> hpf.show()
        >>> hpf.getDFTLogMagnitude().show()

        **NOTES**

        This filter is far from perfect and will generate a lot of ringing artifacts.
        * See: http://en.wikipedia.org/wiki/Ringing_(signal)
        * See: http://en.wikipedia.org/wiki/High-pass_filter#Image

        **SEE ALSO**

        :py:meth:`rawDFTImage`
        :py:meth:`getDFTLogMagnitude`
        :py:meth:`applyDFTFilter`
        :py:meth:`highPassFilter`
        :py:meth:`lowPassFilter`
        :py:meth:`bandPassFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyButterworthFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyGaussianFilter`
        :py:meth:`applyUnsharpMask`

        """
        if( isinstance(xCutoff,float) ):
            xCutoff = [xCutoff,xCutoff,xCutoff]
        if( isinstance(yCutoff,float) ):
            yCutoff = [yCutoff,yCutoff,yCutoff]
        if(yCutoff is None):
            yCutoff = [xCutoff[0],xCutoff[1],xCutoff[2]]

        for i in range(0,len(xCutoff)):
            xCutoff[i] = self._boundsFromPercentage(xCutoff[i],self.width)
            yCutoff[i] = self._boundsFromPercentage(yCutoff[i],self.height)

        filter = None
        h  = self.height
        w  = self.width

        if( grayscale ):
            filter = cv.CreateImage((self.width,self.height),cv.IPL_DEPTH_8U,1)
            cv.Zero(filter)
            cv.AddS(filter,255,filter) # make everything white
            #now make all of the corners black
            cv.Rectangle(filter,(0,0),(xCutoff[0],yCutoff[0]),(0,0,0),thickness=-1) #TL
            cv.Rectangle(filter,(0,h-yCutoff[0]),(xCutoff[0],h),(0,0,0),thickness=-1) #BL
            cv.Rectangle(filter,(w-xCutoff[0],0),(w,yCutoff[0]),(0,0,0),thickness=-1) #TR
            cv.Rectangle(filter,(w-xCutoff[0],h-yCutoff[0]),(w,h),(0,0,0),thickness=-1) #BR

        else:
            #I need to looking into CVMERGE/SPLIT... I would really need to know
            # how much memory we're allocating here
            filterB = cv.CreateImage((self.width,self.height),cv.IPL_DEPTH_8U,1)
            filterG = cv.CreateImage((self.width,self.height),cv.IPL_DEPTH_8U,1)
            filterR = cv.CreateImage((self.width,self.height),cv.IPL_DEPTH_8U,1)
            cv.Zero(filterB)
            cv.Zero(filterG)
            cv.Zero(filterR)
            cv.AddS(filterB,255,filterB) # make everything white
            cv.AddS(filterG,255,filterG) # make everything whit
            cv.AddS(filterR,255,filterR) # make everything white
            #now make all of the corners black
            temp = [filterB,filterG,filterR]
            i = 0
            for f in temp:
                cv.Rectangle(f,(0,0),(xCutoff[i],yCutoff[i]),0,thickness=-1)
                cv.Rectangle(f,(0,h-yCutoff[i]),(xCutoff[i],h),0,thickness=-1)
                cv.Rectangle(f,(w-xCutoff[i],0),(w,yCutoff[i]),0,thickness=-1)
                cv.Rectangle(f,(w-xCutoff[i],h-yCutoff[i]),(w,h),0,thickness=-1)
                i = i+1

            filter = cv.CreateImage((self.width,self.height),cv.IPL_DEPTH_8U,3)
            cv.Merge(filterB,filterG,filterR,None,filter)

        scvFilt = Image(filter)
        retVal = self.applyDFTFilter(scvFilt,grayscale)
        return retVal

    def lowPassFilter(self, xCutoff,yCutoff=None,grayscale=False):
        """
        **SUMMARY**

        This method applies a low pass DFT filter. This filter enhances
        the low frequencies and removes the high frequency signals. This has
        the effect of reducing noise. The frequencies are defined as going between
        0.00 and 1.00 and where 0 is the lowest frequency in the image and 1.0 is
        the highest possible frequencies. Each of the frequencies are defined
        with respect to the horizontal and vertical signal. This filter
        isn't perfect and has a harsh cutoff that causes ringing artifacts.

        **PARAMETERS**

        * *xCutoff* - The horizontal frequency at which we perform the cutoff. A separate
          frequency can be used for the b,g, and r signals by providing a
          list of values. The frequency is defined between zero to one,
          where zero is constant component and 1 is the highest possible
          frequency in the image.

        * *yCutoff* - The cutoff frequencies in the y direction. If none are provided
          we use the same values as provided for x.

        * *grayscale* - if this value is True we perfrom the operation on the DFT of the gray
          version of the image and the result is gray image. If grayscale is true
          we perform the operation on each channel and the recombine them to create
          the result.

        **RETURNS**

        A SimpleCV Image after applying the filter.

        **EXAMPLE**

        >>> img = Image("SimpleCV/sampleimages/RedDog2.jpg")
        >>> img.getDFTLogMagnitude().show()
        >>> lpf = img.lowPassFilter([0.2,0.2,0.05])
        >>> lpf.show()
        >>> lpf.getDFTLogMagnitude().show()

        **NOTES**

        This filter is far from perfect and will generate a lot of ringing artifacts.
        See: http://en.wikipedia.org/wiki/Ringing_(signal)
        See: http://en.wikipedia.org/wiki/Low-pass_filter

        **SEE ALSO**

        :py:meth:`rawDFTImage`
        :py:meth:`getDFTLogMagnitude`
        :py:meth:`applyDFTFilter`
        :py:meth:`highPassFilter`
        :py:meth:`lowPassFilter`
        :py:meth:`bandPassFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyButterworthFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyGaussianFilter`
        :py:meth:`applyUnsharpMask`

        """
        if( isinstance(xCutoff,float) ):
            xCutoff = [xCutoff,xCutoff,xCutoff]
        if( isinstance(yCutoff,float) ):
            yCutoff = [yCutoff,yCutoff,yCutoff]
        if(yCutoff is None):
            yCutoff = [xCutoff[0],xCutoff[1],xCutoff[2]]

        for i in range(0,len(xCutoff)):
            xCutoff[i] = self._boundsFromPercentage(xCutoff[i],self.width)
            yCutoff[i] = self._boundsFromPercentage(yCutoff[i],self.height)

        filter = None
        h  = self.height
        w  = self.width

        if( grayscale ):
            filter = cv.CreateImage((self.width,self.height),cv.IPL_DEPTH_8U,1)
            cv.Zero(filter)
            #now make all of the corners black

            cv.Rectangle(filter,(0,0),(xCutoff[0],yCutoff[0]),255,thickness=-1) #TL
            cv.Rectangle(filter,(0,h-yCutoff[0]),(xCutoff[0],h),255,thickness=-1) #BL
            cv.Rectangle(filter,(w-xCutoff[0],0),(w,yCutoff[0]),255,thickness=-1) #TR
            cv.Rectangle(filter,(w-xCutoff[0],h-yCutoff[0]),(w,h),255,thickness=-1) #BR

        else:
            #I need to looking into CVMERGE/SPLIT... I would really need to know
            # how much memory we're allocating here
            filterB = cv.CreateImage((self.width,self.height),cv.IPL_DEPTH_8U,1)
            filterG = cv.CreateImage((self.width,self.height),cv.IPL_DEPTH_8U,1)
            filterR = cv.CreateImage((self.width,self.height),cv.IPL_DEPTH_8U,1)
            cv.Zero(filterB)
            cv.Zero(filterG)
            cv.Zero(filterR)
            #now make all of the corners black
            temp = [filterB,filterG,filterR]
            i = 0
            for f in temp:
                cv.Rectangle(f,(0,0),(xCutoff[i],yCutoff[i]),255,thickness=-1)
                cv.Rectangle(f,(0,h-yCutoff[i]),(xCutoff[i],h),255,thickness=-1)
                cv.Rectangle(f,(w-xCutoff[i],0),(w,yCutoff[i]),255,thickness=-1)
                cv.Rectangle(f,(w-xCutoff[i],h-yCutoff[i]),(w,h),255,thickness=-1)
                i = i+1

            filter = cv.CreateImage((self.width,self.height),cv.IPL_DEPTH_8U,3)
            cv.Merge(filterB,filterG,filterR,None,filter)

        scvFilt = Image(filter)
        retVal = self.applyDFTFilter(scvFilt,grayscale)
        return retVal


    #FUCK! need to decide BGR or RGB
    # ((rx_begin,ry_begin)(gx_begin,gy_begin)(bx_begin,by_begin))
    # or (x,y)
    def bandPassFilter(self, xCutoffLow, xCutoffHigh, yCutoffLow=None, yCutoffHigh=None,grayscale=False):
        """
        **SUMMARY**

        This method applies a simple band pass DFT filter. This filter enhances
        the a range of frequencies and removes all of the other frequencies. This allows
        a user to precisely select a set of signals to display . The frequencies are
        defined as going between
        0.00 and 1.00 and where 0 is the lowest frequency in the image and 1.0 is
        the highest possible frequencies. Each of the frequencies are defined
        with respect to the horizontal and vertical signal. This filter
        isn't perfect and has a harsh cutoff that causes ringing artifacts.

        **PARAMETERS**

        * *xCutoffLow*  - The horizontal frequency at which we perform the cutoff of the low
          frequency signals. A separate
          frequency can be used for the b,g, and r signals by providing a
          list of values. The frequency is defined between zero to one,
          where zero is constant component and 1 is the highest possible
          frequency in the image.

        * *xCutoffHigh* - The horizontal frequency at which we perform the cutoff of the high
          frequency signals. Our filter passes signals between xCutoffLow and
          xCutoffHigh. A separate frequency can be used for the b, g, and r
          channels by providing a
          list of values. The frequency is defined between zero to one,
          where zero is constant component and 1 is the highest possible
          frequency in the image.

        * *yCutoffLow* - The low frequency cutoff in the y direction. If none
          are provided we use the same values as provided for x.

        * *yCutoffHigh* - The high frequency cutoff in the y direction. If none
          are provided we use the same values as provided for x.

        * *grayscale* - if this value is True we perfrom the operation on the DFT of the gray
          version of the image and the result is gray image. If grayscale is true
          we perform the operation on each channel and the recombine them to create
          the result.

        **RETURNS**

        A SimpleCV Image after applying the filter.

        **EXAMPLE**

        >>> img = Image("SimpleCV/sampleimages/RedDog2.jpg")
        >>> img.getDFTLogMagnitude().show()
        >>> lpf = img.bandPassFilter([0.2,0.2,0.05],[0.3,0.3,0.2])
        >>> lpf.show()
        >>> lpf.getDFTLogMagnitude().show()

        **NOTES**

        This filter is far from perfect and will generate a lot of ringing artifacts.

        See: http://en.wikipedia.org/wiki/Ringing_(signal)

        **SEE ALSO**

        :py:meth:`rawDFTImage`
        :py:meth:`getDFTLogMagnitude`
        :py:meth:`applyDFTFilter`
        :py:meth:`highPassFilter`
        :py:meth:`lowPassFilter`
        :py:meth:`bandPassFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyButterworthFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyGaussianFilter`
        :py:meth:`applyUnsharpMask`

        """

        if( isinstance(xCutoffLow,float) ):
            xCutoffLow = [xCutoffLow,xCutoffLow,xCutoffLow]
        if( isinstance(yCutoffLow,float) ):
            yCutoffLow = [yCutoffLow,yCutoffLow,yCutoffLow]
        if( isinstance(xCutoffHigh,float) ):
            xCutoffHigh = [xCutoffHigh,xCutoffHigh,xCutoffHigh]
        if( isinstance(yCutoffHigh,float) ):
            yCutoffHigh = [yCutoffHigh,yCutoffHigh,yCutoffHigh]

        if(yCutoffLow is None):
            yCutoffLow = [xCutoffLow[0],xCutoffLow[1],xCutoffLow[2]]
        if(yCutoffHigh is None):
            yCutoffHigh = [xCutoffHigh[0],xCutoffHigh[1],xCutoffHigh[2]]

        for i in range(0,len(xCutoffLow)):
            xCutoffLow[i] = self._boundsFromPercentage(xCutoffLow[i],self.width)
            xCutoffHigh[i] = self._boundsFromPercentage(xCutoffHigh[i],self.width)
            yCutoffHigh[i] = self._boundsFromPercentage(yCutoffHigh[i],self.height)
            yCutoffLow[i] = self._boundsFromPercentage(yCutoffLow[i],self.height)

        filter = None
        h  = self.height
        w  = self.width
        if( grayscale ):
            filter = cv.CreateImage((self.width,self.height),cv.IPL_DEPTH_8U,1)
            cv.Zero(filter)
            #now make all of the corners black
            cv.Rectangle(filter,(0,0),(xCutoffHigh[0],yCutoffHigh[0]),255,thickness=-1) #TL
            cv.Rectangle(filter,(0,h-yCutoffHigh[0]),(xCutoffHigh[0],h),255,thickness=-1) #BL
            cv.Rectangle(filter,(w-xCutoffHigh[0],0),(w,yCutoffHigh[0]),255,thickness=-1) #TR
            cv.Rectangle(filter,(w-xCutoffHigh[0],h-yCutoffHigh[0]),(w,h),255,thickness=-1) #BR
            cv.Rectangle(filter,(0,0),(xCutoffLow[0],yCutoffLow[0]),0,thickness=-1) #TL
            cv.Rectangle(filter,(0,h-yCutoffLow[0]),(xCutoffLow[0],h),0,thickness=-1) #BL
            cv.Rectangle(filter,(w-xCutoffLow[0],0),(w,yCutoffLow[0]),0,thickness=-1) #TR
            cv.Rectangle(filter,(w-xCutoffLow[0],h-yCutoffLow[0]),(w,h),0,thickness=-1) #BR


        else:
            #I need to looking into CVMERGE/SPLIT... I would really need to know
            # how much memory we're allocating here
            filterB = cv.CreateImage((self.width,self.height),cv.IPL_DEPTH_8U,1)
            filterG = cv.CreateImage((self.width,self.height),cv.IPL_DEPTH_8U,1)
            filterR = cv.CreateImage((self.width,self.height),cv.IPL_DEPTH_8U,1)
            cv.Zero(filterB)
            cv.Zero(filterG)
            cv.Zero(filterR)
            #now make all of the corners black
            temp = [filterB,filterG,filterR]
            i = 0
            for f in temp:
                cv.Rectangle(f,(0,0),(xCutoffHigh[i],yCutoffHigh[i]),255,thickness=-1) #TL
                cv.Rectangle(f,(0,h-yCutoffHigh[i]),(xCutoffHigh[i],h),255,thickness=-1) #BL
                cv.Rectangle(f,(w-xCutoffHigh[i],0),(w,yCutoffHigh[i]),255,thickness=-1) #TR
                cv.Rectangle(f,(w-xCutoffHigh[i],h-yCutoffHigh[i]),(w,h),255,thickness=-1) #BR
                cv.Rectangle(f,(0,0),(xCutoffLow[i],yCutoffLow[i]),0,thickness=-1) #TL
                cv.Rectangle(f,(0,h-yCutoffLow[i]),(xCutoffLow[i],h),0,thickness=-1) #BL
                cv.Rectangle(f,(w-xCutoffLow[i],0),(w,yCutoffLow[i]),0,thickness=-1) #TR
                cv.Rectangle(f,(w-xCutoffLow[i],h-yCutoffLow[i]),(w,h),0,thickness=-1) #BR
                i = i+1

            filter = cv.CreateImage((self.width,self.height),cv.IPL_DEPTH_8U,3)
            cv.Merge(filterB,filterG,filterR,None,filter)

        scvFilt = Image(filter)
        retVal = self.applyDFTFilter(scvFilt,grayscale)
        return retVal





    def _inverseDFT(self,input):
        """
        **SUMMARY**
        **PARAMETERS**
        **RETURNS**
        **EXAMPLE**
        NOTES:
        SEE ALSO:
        """
        # a destructive IDFT operation for internal calls
        w = input[0].width
        h = input[0].height
        if( len(input) == 1 ):
            cv.DFT(input[0], input[0], cv.CV_DXT_INV_SCALE)
            result = cv.CreateImage((w,h), cv.IPL_DEPTH_8U, 1)
            data = cv.CreateImage((w,h), cv.IPL_DEPTH_64F, 1)
            blank = cv.CreateImage((w,h), cv.IPL_DEPTH_64F, 1)
            cv.Split(input[0],data,blank,None,None)
            min, max, pt1, pt2 = cv.MinMaxLoc(data)
            denom = max-min
            if(denom == 0):
                denom = 1
            cv.Scale(data, data, 1.0/(denom), 1.0*(-min)/(denom))
            cv.Mul(data,data,data,255.0)
            cv.Convert(data,result)
            retVal = Image(result)
        else: # DO RGB separately
            results = []
            data = cv.CreateImage((w,h), cv.IPL_DEPTH_64F, 1)
            blank = cv.CreateImage((w,h), cv.IPL_DEPTH_64F, 1)
            for i in range(0,len(input)):
                cv.DFT(input[i], input[i], cv.CV_DXT_INV_SCALE)
                result = cv.CreateImage((w,h), cv.IPL_DEPTH_8U, 1)
                cv.Split( input[i],data,blank,None,None)
                min, max, pt1, pt2 = cv.MinMaxLoc(data)
                denom = max-min
                if(denom == 0):
                    denom = 1
                cv.Scale(data, data, 1.0/(denom), 1.0*(-min)/(denom))
                cv.Mul(data,data,data,255.0) # this may not be right
                cv.Convert(data,result)
                results.append(result)

            retVal = cv.CreateImage((w,h),cv.IPL_DEPTH_8U,3)
            cv.Merge(results[0],results[1],results[2],None,retVal)
            retVal = Image(retVal)
        del input
        return retVal

    def InverseDFT(self, raw_dft_image):
        """
        **SUMMARY**

        This method provides a way of performing an inverse discrete Fourier transform
        on a real/imaginary image pair and obtaining the result as a SimpleCV image. This
        method is helpful if you wish to perform custom filter development.

        **PARAMETERS**

        * *raw_dft_image* - A list object with either one or three IPL images. Each image should
          have a 64f depth and contain two channels (the real and the imaginary).

        **RETURNS**

        A simpleCV image.

        **EXAMPLE**

        Note that this is an example, I don't recommend doing this unless you know what
        you are doing.

        >>> raw = img.getRawDFT()
        >>> cv.SomeOperation(raw)
        >>> result = img.InverseDFT(raw)
        >>> result.show()

        **SEE ALSO**

        :py:meth:`rawDFTImage`
        :py:meth:`getDFTLogMagnitude`
        :py:meth:`applyDFTFilter`
        :py:meth:`highPassFilter`
        :py:meth:`lowPassFilter`
        :py:meth:`bandPassFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyButterworthFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyGaussianFilter`
        :py:meth:`applyUnsharpMask`

        """
        input  = []
        w = raw_dft_image[0].width
        h = raw_dft_image[0].height
        if(len(raw_dft_image) == 1):
            gs = cv.CreateImage((w,h),cv.IPL_DEPTH_64F,2)
            cv.Copy(self._DFT[0],gs)
            input.append(gs)
        else:
            for img in raw_dft_image:
                temp = cv.CreateImage((w,h),cv.IPL_DEPTH_64F,2)
                cv.Copy(img,temp)
                input.append(img)

        if( len(input) == 1 ):
            cv.DFT(input[0], input[0], cv.CV_DXT_INV_SCALE)
            result = cv.CreateImage((w,h), cv.IPL_DEPTH_8U, 1)
            data = cv.CreateImage((w,h), cv.IPL_DEPTH_64F, 1)
            blank = cv.CreateImage((w,h), cv.IPL_DEPTH_64F, 1)
            cv.Split(input[0],data,blank,None,None)
            min, max, pt1, pt2 = cv.MinMaxLoc(data)
            denom = max-min
            if(denom == 0):
                denom = 1
            cv.Scale(data, data, 1.0/(denom), 1.0*(-min)/(denom))
            cv.Mul(data,data,data,255.0)
            cv.Convert(data,result)
            retVal = Image(result)
        else: # DO RGB separately
            results = []
            data = cv.CreateImage((w,h), cv.IPL_DEPTH_64F, 1)
            blank = cv.CreateImage((w,h), cv.IPL_DEPTH_64F, 1)
            for i in range(0,len(raw_dft_image)):
                cv.DFT(input[i], input[i], cv.CV_DXT_INV_SCALE)
                result = cv.CreateImage((w,h), cv.IPL_DEPTH_8U, 1)
                cv.Split( input[i],data,blank,None,None)
                min, max, pt1, pt2 = cv.MinMaxLoc(data)
                denom = max-min
                if(denom == 0):
                    denom = 1
                cv.Scale(data, data, 1.0/(denom), 1.0*(-min)/(denom))
                cv.Mul(data,data,data,255.0) # this may not be right
                cv.Convert(data,result)
                results.append(result)

            retVal = cv.CreateImage((w,h),cv.IPL_DEPTH_8U,3)
            cv.Merge(results[0],results[1],results[2],None,retVal)
            retVal = Image(retVal)

        return retVal

    def applyButterworthFilter(self,dia=400,order=2,highpass=False,grayscale=False):
        """
        **SUMMARY**

        Creates a butterworth filter of 64x64 pixels, resizes it to fit
        image, applies DFT on image using the filter.
        Returns image with DFT applied on it

        **PARAMETERS**

        * *dia* - int Diameter of Butterworth low pass filter
        * *order* - int Order of butterworth lowpass filter
        * *highpass*: BOOL True: highpass filterm False: lowpass filter
        * *grayscale*: BOOL

        **EXAMPLE**

        >>> im = Image("lenna")
        >>> img = im.applyButterworth(dia=400,order=2,highpass=True,grayscale=False)

        Output image: http://i.imgur.com/5LS3e.png

        >>> img = im.applyButterworth(dia=400,order=2,highpass=False,grayscale=False)

        Output img: http://i.imgur.com/QlCAY.png

        >>> im = Image("grayscale_lenn.png") #take image from here: http://i.imgur.com/O0gZn.png
        >>> img = im.applyButterworth(dia=400,order=2,highpass=True,grayscale=True)

        Output img: http://i.imgur.com/BYYnp.png

        >>> img = im.applyButterworth(dia=400,order=2,highpass=False,grayscale=True)

        Output img: http://i.imgur.com/BYYnp.png

        **SEE ALSO**

        :py:meth:`rawDFTImage`
        :py:meth:`getDFTLogMagnitude`
        :py:meth:`applyDFTFilter`
        :py:meth:`highPassFilter`
        :py:meth:`lowPassFilter`
        :py:meth:`bandPassFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyButterworthFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyGaussianFilter`
        :py:meth:`applyUnsharpMask`

        """
        #reimplemented with faster, vectorized filter kernel creation
        w,h = self.size()
        intensity_scale = 2**8 - 1 #for now 8-bit
        sz_x            = 64       #for now constant, symmetric
        sz_y            = 64       #for now constant, symmetric
        x0              = sz_x/2.0 #for now, on center 
        y0              = sz_y/2.0 #for now, on center
        #efficient "vectorized" computation
        X, Y = np.meshgrid(np.arange(sz_x), np.arange(sz_y))
        D = np.sqrt((X-x0)**2+(Y-y0)**2)
        flt = intensity_scale/(1.0 + (D/dia)**(order*2)) 
        if highpass:     #then invert the filter
            flt = intensity_scale - flt
        flt    = Image(flt) #numpy arrays are in row-major form...doesn't matter for symmetric filter 
        flt_re = flt.resize(w,h)
        img = self.applyDFTFilter(flt_re,grayscale)
        return img

    def applyGaussianFilter(self, dia=400, highpass=False, grayscale=False):
        """
        **SUMMARY**

        Creates a gaussian filter of 64x64 pixels, resizes it to fit
        image, applies DFT on image using the filter.
        Returns image with DFT applied on it

        **PARAMETERS**

        * *dia* -  int - diameter of Gaussian filter
        * *highpass*: BOOL True: highpass filter False: lowpass filter
        * *grayscale*: BOOL

        **EXAMPLE**

        >>> im = Image("lenna")
        >>> img = im.applyGaussianfilter(dia=400,highpass=True,grayscale=False)

        Output image: http://i.imgur.com/DttJv.png

        >>> img = im.applyGaussianfilter(dia=400,highpass=False,grayscale=False)

        Output img: http://i.imgur.com/PWn4o.png

        >>> im = Image("grayscale_lenn.png") #take image from here: http://i.imgur.com/O0gZn.png
        >>> img = im.applyGaussianfilter(dia=400,highpass=True,grayscale=True)

        Output img: http://i.imgur.com/9hX5J.png

        >>> img = im.applyGaussianfilter(dia=400,highpass=False,grayscale=True)

        Output img: http://i.imgur.com/MXI5T.png

        **SEE ALSO**

        :py:meth:`rawDFTImage`
        :py:meth:`getDFTLogMagnitude`
        :py:meth:`applyDFTFilter`
        :py:meth:`highPassFilter`
        :py:meth:`lowPassFilter`
        :py:meth:`bandPassFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyButterworthFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyGaussianFilter`
        :py:meth:`applyUnsharpMask`

        """
        #reimplemented with faster, vectorized filter kernel creation
        w,h = self.size()
        intensity_scale = 2**8 - 1 #for now 8-bit
        sz_x            = 64       #for now constant, symmetric
        sz_y            = 64       #for now constant, symmetric
        x0              = sz_x/2.0 #for now, on center 
        y0              = sz_y/2.0 #for now, on center
        #efficient "vectorized" computation
        X, Y = np.meshgrid(np.arange(sz_x), np.arange(sz_y))
        D = np.sqrt((X-x0)**2+(Y-y0)**2)
        flt = intensity_scale*np.exp(-0.5*(D/dia)**2) 
        if highpass:     #then invert the filter
            flt = intensity_scale - flt
        flt    = Image(flt) #numpy arrays are in row-major form...doesn't matter for symmetric filter 
        flt_re = flt.resize(w,h)
        img = self.applyDFTFilter(flt_re,grayscale)
        return img

    def applyUnsharpMask(self,boost=1,dia=400,grayscale=False):
        """
        **SUMMARY**

        This method applies unsharp mask or highboost filtering
        on image depending upon the boost value provided.
        DFT is applied on image using gaussian lowpass filter.
        A mask is created subtracting the DFT image from the original
        iamge. And then mask is added in the image to sharpen it.
        unsharp masking => image + mask
        highboost filtering => image + (boost)*mask

        **PARAMETERS**

        * *boost* - int  boost = 1 => unsharp masking, boost > 1 => highboost filtering
        * *dia* - int Diameter of Gaussian low pass filter
        * *grayscale* - BOOL

        **EXAMPLE**

        Gaussian Filters:

        >>> im = Image("lenna")
        >>> img = im.applyUnsharpMask(2,grayscale=False) #highboost filtering

        output image: http://i.imgur.com/A1pZf.png

        >>> img = im.applyUnsharpMask(1,grayscale=False) #unsharp masking

        output image: http://i.imgur.com/smCdL.png

        >>> im = Image("grayscale_lenn.png") #take image from here: http://i.imgur.com/O0gZn.png
        >>> img = im.applyUnsharpMask(2,grayscale=True) #highboost filtering

        output image: http://i.imgur.com/VtGzl.png

        >>> img = im.applyUnsharpMask(1,grayscale=True) #unsharp masking

        output image: http://i.imgur.com/bywny.png

        **SEE ALSO**

        :py:meth:`rawDFTImage`
        :py:meth:`getDFTLogMagnitude`
        :py:meth:`applyDFTFilter`
        :py:meth:`highPassFilter`
        :py:meth:`lowPassFilter`
        :py:meth:`bandPassFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyButterworthFilter`
        :py:meth:`InverseDFT`
        :py:meth:`applyGaussianFilter`
        :py:meth:`applyUnsharpMask`

        """
        if boost < 0:
            print "boost >= 1"
            return None

        lpIm = self.applyGaussianFilter(dia=dia,grayscale=grayscale,highpass=False)
        im = Image(self.getBitmap())
        mask = im - lpIm
        img = im
        for i in range(boost):
            img = img + mask
        return img

    def listHaarFeatures(self):
        '''
        This is used to list the built in features available for HaarCascade feature
        detection.  Just run this function as:

        >>> img.listHaarFeatures()

        Then use one of the file names returned as the input to the findHaarFeature()
        function.  So you should get a list, more than likely you will see face.xml,
        to use it then just

        >>> img.findHaarFeatures('face.xml')
        '''

        features_directory = os.path.join(LAUNCH_PATH, 'Features','HaarCascades')
        features = os.listdir(features_directory)
        print features

    def _CopyAvg(self, src, dst,roi, levels, levels_f, mode):
        '''
        Take the value in an ROI, calculate the average / peak hue
        and then set the output image roi to the value.
        '''

        if( mode ): # get the peak hue for an area
            h = src[roi[0]:roi[0]+roi[2],roi[1]:roi[1]+roi[3]].hueHistogram()
            myHue = np.argmax(h)
            C = (float(myHue),float(255),float(255),float(0))
            cv.SetImageROI(dst,roi)
            cv.AddS(dst,c,dst)
            cv.ResetImageROI(dst)
        else: # get the average value for an area optionally set levels
            cv.SetImageROI(src.getBitmap(),roi)
            cv.SetImageROI(dst,roi)
            avg = cv.Avg(src.getBitmap())
            avg = (float(avg[0]),float(avg[1]),float(avg[2]),0)
            if(levels is not None):
                avg = (int(avg[0]/levels)*levels_f,int(avg[1]/levels)*levels_f,int(avg[2]/levels)*levels_f,0)
            cv.AddS(dst,avg,dst)
            cv.ResetImageROI(src.getBitmap())
            cv.ResetImageROI(dst)

    def pixelize(self, block_size = 10, region = None, levels=None, doHue=False):
        """
        **SUMMARY**

        Pixelation blur, like the kind used to hide naughty bits on your favorite tv show.

        **PARAMETERS**

        * *block_size* - the blur block size in pixels, an integer is an square blur, a tuple is rectangular.
        * *region* - do the blur in a region in format (x_position,y_position,width,height)
        * *levels* - the number of levels per color channel. This makes the image look like an 8-bit video game.
        * *doHue* - If this value is true we calculate the peak hue for the area, not the
          average color for the area.

        **RETURNS**

        Returns the image with the pixelation blur applied.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> result = img.pixelize( 16, (200,180,250,250), levels=4)
        >>> img.show()

        """

        if( isinstance(block_size, int) ):
            block_size = (block_size,block_size)


        retVal = self.getEmpty()


        levels_f = 0.00
        if( levels is not None ):
            levels = 255/int(levels)
            if(levels <= 1 ):
                levels = 2
            levels_f = float(levels)

        if( region is not None ):
            cv.Copy(self.getBitmap(), retVal)
            cv.SetImageROI(retVal,region)
            cv.Zero(retVal)
            cv.ResetImageROI(retVal)
            xs = region[0]
            ys = region[1]
            w = region[2]
            h = region[3]
        else:
            xs = 0
            ys = 0
            w = self.width
            h = self.height

        #if( region is None ):
        hc = w / block_size[0] #number of horizontal blocks
        vc = h / block_size[1] #number of vertical blocks
        #when we fit in the blocks, we're going to spread the round off
        #over the edges 0->x_0, 0->y_0  and x_0+hc*block_size
        x_lhs = int(np.ceil(float(w%block_size[0])/2.0)) # this is the starting point
        y_lhs = int(np.ceil(float(h%block_size[1])/2.0))
        x_rhs = int(np.floor(float(w%block_size[0])/2.0)) # this is the starting point
        y_rhs = int(np.floor(float(h%block_size[1])/2.0))
        x_0 = xs+x_lhs
        y_0 = ys+y_lhs
        x_f = (x_0+(block_size[0]*hc)) #this would be the end point
        y_f = (y_0+(block_size[1]*vc))

        for i in range(0,hc):
            for j in range(0,vc):
                xt = x_0+(block_size[0]*i)
                yt = y_0+(block_size[1]*j)
                roi = (xt,yt,block_size[0],block_size[1])
                self._CopyAvg(self,retVal,roi,levels,levels_f,doHue)


        if( x_lhs > 0 ): # add a left strip
            xt = xs
            wt = x_lhs
            ht = block_size[1]
            for j in range(0,vc):
                yt = y_0+(j*block_size[1])
                roi = (xt,yt,wt,ht)
                self._CopyAvg(self,retVal,roi,levels,levels_f,doHue)


        if( x_rhs > 0 ): # add a right strip
            xt = (x_0+(block_size[0]*hc))
            wt = x_rhs
            ht = block_size[1]
            for j in range(0,vc):
                yt = y_0+(j*block_size[1])
                roi = (xt,yt,wt,ht)
                self._CopyAvg(self,retVal,roi,levels,levels_f,doHue)

        if( y_lhs > 0 ): # add a left strip
            yt = ys
            ht = y_lhs
            wt = block_size[0]
            for i in range(0,hc):
                xt = x_0+(i*block_size[0])
                roi = (xt,yt,wt,ht)
                self._CopyAvg(self,retVal,roi,levels,levels_f,doHue)

        if( y_rhs > 0 ): # add a right strip
            yt = (y_0+(block_size[1]*vc))
            ht = y_rhs
            wt = block_size[0]
            for i in range(0,hc):
                xt = x_0+(i*block_size[0])
                roi = (xt,yt,wt,ht)
                self._CopyAvg(self,retVal,roi,levels,levels_f,doHue)

        #now the corner cases
        if(x_lhs > 0 and y_lhs > 0 ):
            roi = (xs,ys,x_lhs,y_lhs)
            self._CopyAvg(self,retVal,roi,levels,levels_f,doHue)

        if(x_rhs > 0 and y_rhs > 0 ):
            roi = (x_f,y_f,x_rhs,y_rhs)
            self._CopyAvg(self,retVal,roi,levels,levels_f,doHue)

        if(x_lhs > 0 and y_rhs > 0 ):
            roi = (xs,y_f,x_lhs,y_rhs)
            self._CopyAvg(self,retVal,roi,levels,levels_f,doHue)

        if(x_rhs > 0 and y_lhs > 0 ):
            roi = (x_f,ys,x_rhs,y_lhs)
            self._CopyAvg(self,retVal,roi,levels,levels_f,doHue)

        if(doHue):
            cv.CvtColor(retVal,retVal,cv.CV_HSV2BGR)


        return Image(retVal)

    def anonymize(self, block_size=10, features=None, transform=None):
        """
        **SUMMARY**

        Anonymize, for additional privacy to images.

        **PARAMETERS**

        * *features* - A list with the Haar like feature cascades that should be matched.
        * *block_size* - The size of the blocks for the pixelize function.
        * *transform* - A function, to be applied to the regions matched instead of pixelize.
        * This function must take two arguments: the image and the region it'll be applied to,
        * as in region = (x, y, width, height).

        **RETURNS**

        Returns the image with matching regions pixelated.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> anonymous = img.anonymize()
        >>> anonymous.show()

        >>> def my_function(img, region):
        >>>     x, y, width, height = region
        >>>     img = img.crop(x, y, width, height)
        >>>     return img
        >>>
        >>>img = Image("lenna")
        >>>transformed = img.anonymize(transform = my_function)

        """

        regions = []

        if features is None:
            regions.append(self.findHaarFeatures("face"))
            regions.append(self.findHaarFeatures("profile"))
        else:
            for feature in features:
                regions.append(self.findHaarFeatures(feature))

        found = [f for f in regions if f is not None]

        img = self.copy()

        if found:
            for feature_set in found:
                for region in feature_set:
                    rect = (region.topLeftCorner()[0], region.topLeftCorner()[1],
                            region.width(), region.height())
                    if transform is None:
                        img = img.pixelize(block_size=block_size, region=rect)
                    else:
                        img = transform(img, rect)

        return img


    def fillHoles(self):
        """
        **SUMMARY**

        Fill holes on a binary image by closing the contours

        **PARAMETERS**

        * *img* - a binary image
        **RETURNS**

        The image with the holes filled
        **EXAMPLE**

        >>> img = Image("SimpleCV")
        #todo Add noise and showcase the image 

        """
        # kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5))
        # res = cv2.morphologyEx(self.getGrayNumpy(),cv2.MORPH_OPEN,kernel)
        # return res
        des = cv2.bitwise_not(self.getGrayNumpy())
        return cv2.inPaint(des)
        contour,hier = cv2.findContours(des,cv2.RETR_CCOMP,cv2.CHAIN_APPROX_SIMPLE)

        for cnt in contour:
            cv2.drawContours(des,[cnt],0,255,-1)
            print 'yep'

        gray = cv2.bitwise_not(des)
        return gray

    def edgeIntersections(self, pt0, pt1, width=1, canny1=0, canny2=100):
        """
        **SUMMARY**

        Find the outermost intersection of a line segment and the edge image and return
        a list of the intersection points. If no intersections are found the method returns
        an empty list.

        **PARAMETERS**

        * *pt0* - an (x,y) tuple of one point on the intersection line.
        * *pt1* - an (x,y) tuple of the second point on the intersection line.
        * *width* - the width of the line to use. This approach works better when
                    for cases where the edges on an object are not always closed
                    and may have holes.
        * *canny1* - the lower bound of the Canny edge detector parameters.
        * *canny2* - the upper bound of the Canny edge detector parameters.

        **RETURNS**

        A list of two (x,y) tuples or an empty list.

        **EXAMPLE**

        >>> img = Image("SimpleCV")
        >>> a = (25,100)
        >>> b = (225,110)
        >>> pts = img.edgeIntersections(a,b,width=3)
        >>> e = img.edges(0,100)
        >>> e.drawLine(a,b,color=Color.RED)
        >>> e.drawCircle(pts[0],10,color=Color.GREEN)
        >>> e.drawCircle(pts[1],10,color=Color.GREEN)
        >>> e.show()

        img = Image("SimpleCV")
        a = (25,100)
        b = (225,100)
        pts = img.edgeIntersections(a,b,width=3)
        e = img.edges(0,100)
        e.drawLine(a,b,color=Color.RED)
        e.drawCircle(pts[0],10,color=Color.GREEN)
        e.drawCircle(pts[1],10,color=Color.GREEN)
        e.show()


        """
        w = abs(pt0[0]-pt1[0])
        h = abs(pt0[1]-pt1[1])
        x = np.min([pt0[0],pt1[0]])
        y = np.min([pt0[1],pt1[1]])
        if( w <= 0 ):
            w = width
            x = np.clip(x-(width/2),0,x-(width/2))
        if( h <= 0 ):
            h = width
            y = np.clip(y-(width/2),0,y-(width/2))
        #got some corner cases to catch here
        p0p = np.array([(pt0[0]-x,pt0[1]-y)])
        p1p = np.array([(pt1[0]-x,pt1[1]-y)])
        edges = self.crop(x,y,w,h)._getEdgeMap(canny1, canny2)
        line = cv.CreateImage((w,h),cv.IPL_DEPTH_8U,1)
        cv.Zero(line)
        cv.Line(line,((pt0[0]-x),(pt0[1]-y)),((pt1[0]-x),(pt1[1]-y)),cv.Scalar(255.00),width,8)
        cv.Mul(line,edges,line)
        intersections = uint8(np.array(cv.GetMat(line)).transpose())
        (xs,ys) = np.where(intersections==255)
        points = zip(xs,ys)
        if(len(points)==0):
            return [None,None]
        A = np.argmin(spsd.cdist(p0p,points,'cityblock'))
        B = np.argmin(spsd.cdist(p1p,points,'cityblock'))
        ptA = (int(xs[A]+x),int(ys[A]+y))
        ptB = (int(xs[B]+x),int(ys[B]+y))
        # we might actually want this to be list of all the points
        return [ptA, ptB]


    def fitContour(self, initial_curve, window=(11,11), params=(0.1,0.1,0.1),doAppx=True,appx_level=1):
        """
        
        **SUMMARY**

        This method tries to fit a list of points to lines in the image. The list of points
        is a list of (x,y) tuples that are near (i.e. within the window size) of the line
        you want to fit in the image. This method uses a binary such as the result of calling
        edges.

        This method is based on active contours. Please see this reference:
        http://en.wikipedia.org/wiki/Active_contour_model

        **PARAMETERS**

        * *initial_curve* - region of the form [(x0,y0),(x1,y1)...] that are the initial conditions to fit.
        * *window* - the search region around each initial point to look for a solution.
        * *params* - The alpha, beta, and gamma parameters for the active contours
          algorithm as a list [alpha,beta,gamma].
        * *doAppx* - post process the snake into a polynomial approximation. Basically
          this flag will clean up the output of the contour algorithm.
        * *appx_level* - how much to approximate the snake, higher numbers mean more approximation.

        **DISCUSSION**

        THIS SECTION IS QUOTED FROM: http://users.ecs.soton.ac.uk/msn/book/new_demo/Snakes/
        There are three components to the Energy Function:

        * Continuity
        * Curvature
        * Image (Gradient)
        
        Each Weighted by Specified Parameter:

        Total Energy = Alpha*Continuity + Beta*Curvature + Gamma*Image

        Choose different values dependent on Feature to extract:

        * Set alpha high if there is a deceptive Image Gradient
        * Set beta  high if smooth edged Feature, low if sharp edges
        * Set gamma high if contrast between Background and Feature is low


        **RETURNS**

        A list of (x,y) tuples that approximate the curve. If you do not use
        approximation the list should be the same length as the input list length.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> edges = img.edges(t1=120,t2=155)
        >>> guess = [(311,284),(313,270),(320,259),(330,253),(347,245)]
        >>> result = edges.fitContour(guess)
        >>> img.drawPoints(guess,color=Color.RED)
        >>> img.drawPoints(result,color=Color.GREEN)
        >>> img.show()

        """
        alpha = [params[0]]
        beta= [params[1]]
        gamma = [params[2]]
        if( window[0]%2 == 0 ):
            window = (window[0]+1,window[1])
            logger.warn("Yo dawg, just a heads up, snakeFitPoints wants an odd window size. I fixed it for you, but you may want to take a look at your code.")
        if( window[1]%2 == 0 ):
            window = (window[0],window[1]+1)
            logger.warn("Yo dawg, just a heads up, snakeFitPoints wants an odd window size. I fixed it for you, but you may want to take a look at your code.")
        raw = cv.SnakeImage(self._getGrayscaleBitmap(),initial_curve,alpha,beta,gamma,window,(cv.CV_TERMCRIT_ITER,10,0.01))
        if( doAppx ):
            try:
                import cv2
            except:
                logger.warning("Can't Do snakeFitPoints without OpenCV >= 2.3.0")
                return
            appx = cv2.approxPolyDP(np.array([raw],'float32'),appx_level,True)
            retVal = []
            for p in appx:
                retVal.append((int(p[0][0]),int(p[0][1])))
        else:
            retVal = raw

        return retVal

    def fitEdge(self,guess,window=10,threshold=128, measurements=5, darktolight=True, lighttodark=True,departurethreshold=1):
      """
        **SUMMARY**
        
        Fit edge in a binary/gray image using an initial guess and the least squares method.
        The functions returns a single line  

        **PARAMETERS**

        * *guess* - A tuples of the form ((x0,y0),(x1,y1)) which is an approximate guess
        * *window* - A window around the guess to search.
        * *threshold* - the threshold above which we count a pixel as a line
        * *measurements* -the number of line projections to use for fitting the line 
        TODO: Constrict a line to black to white or white to black
        Right vs. Left orientation.

        **RETURNS**

        A a line object 
        **EXAMPLE**
      """
      searchLines = FeatureSet()
      fitPoints = FeatureSet()
      x1 = guess[0][0]
      x2 = guess[1][0]
      y1 = guess[0][1]
      y2 = guess[1][1]
      dx = float((x2-x1))/(measurements-1)
      dy = float((y2-y1))/(measurements-1)
      s = np.zeros((measurements,2))
      lpstartx = np.zeros(measurements)
      lpstarty = np.zeros(measurements)
      lpendx = np.zeros(measurements)
      lpendy = np.zeros(measurements)
      linefitpts = np.zeros((measurements,2))

      #obtain equation for initial guess line
      if( x1==x2): #vertical line must be handled as special case since slope isn't defined
        m=0
        mo = 0
        b = x1
        for i in xrange(0, measurements):
            s[i][0] = x1 
            s[i][1] = y1 + i * dy
            lpstartx[i] = s[i][0] + window
            lpstarty[i] = s[i][1] 
            lpendx[i] = s[i][0] - window
            lpendy[i] = s[i][1] 
            Cur_line = Line(self,((lpstartx[i],lpstarty[i]),(lpendx[i],lpendy[i])))
            ((lpstartx[i],lpstarty[i]),(lpendx[i],lpendy[i])) = Cur_line.cropToImageEdges().end_points

            searchLines.append(Cur_line)
            tmp = self.getThresholdCrossing((int(lpstartx[i]),int(lpstarty[i])),(int(lpendx[i]),int(lpendy[i])),threshold=threshold,lighttodark=lighttodark, darktolight=darktolight, departurethreshold=departurethreshold)
            fitPoints.append(Circle(self,tmp[0],tmp[1],3))
            linefitpts[i] = tmp

      else:
        m = float((y2-y1))/(x2-x1)
        b = y1 - m*x1
        mo = -1/m #slope of orthogonal line segments
       
        #obtain points for measurement along the initial guess line
        for i in xrange(0, measurements):
            s[i][0] = x1 + i * dx
            s[i][1] = y1 + i * dy
            fx = (math.sqrt(math.pow(window,2))/(1+mo))/2
            fy = fx * mo 
            lpstartx[i] = s[i][0] + fx
            lpstarty[i] = s[i][1] + fy
            lpendx[i] = s[i][0] - fx
            lpendy[i] = s[i][1] - fy
            Cur_line = Line(self,((lpstartx[i],lpstarty[i]),(lpendx[i],lpendy[i])))
            ((lpstartx[i],lpstarty[i]),(lpendx[i],lpendy[i])) = Cur_line.cropToImageEdges().end_points
            searchLines.append(Cur_line)
            tmp = self.getThresholdCrossing((int(lpstartx[i]),int(lpstarty[i])),(int(lpendx[i]),int(lpendy[i])),threshold=threshold,lighttodark=lighttodark, darktolight=darktolight,departurethreshold=departurethreshold)
            fitPoints.append((tmp[0],tmp[1]))
            linefitpts[i] = tmp

      badpts = []    
      for j in range(len(linefitpts)):
        if (linefitpts[j,0] == -1) or (linefitpts[j,1] == -1):
            badpts.append(j)
      for pt in badpts:
        linefitpts = np.delete(linefitpts,pt,axis=0)

      x = linefitpts[:,0]
      y = linefitpts[:,1]
      ymin = np.min(y)
      ymax = np.max(y)
      xmax = np.max(x)
      xmin = np.min(x)

      if( (xmax-xmin) > (ymax-ymin) ):
          # do the least squares
          A = np.vstack([x,np.ones(len(x))]).T
          m,c = nla.lstsq(A,y)[0]
          y0 = int(m*xmin+c)
          y1 = int(m*xmax+c)
          finalLine = Line(self,((xmin,y0),(xmax,y1)))
      else:
          # do the least squares
          A = np.vstack([y,np.ones(len(y))]).T
          m,c = nla.lstsq(A,x)[0]
          x0 = int(ymin*m+c)
          x1 = int(ymax*m+c)
          finalLine = Line(self,((x0,ymin),(x1,ymax)))

      return finalLine, searchLines, fitPoints

    def getThresholdCrossing(self, pt1, pt2, threshold=128, darktolight=True, lighttodark=True, departurethreshold=1):
        """
        **SUMMARY**

        This function takes in an image and two points, calculates the intensity 
        profile between the points, and returns the single point at which the profile
        crosses an intensity

        **PARAMETERS**

        * *p1, p2* - the starting and ending points in tuple form e.g. (1,2)
        * *threshold* pixel value of desired threshold crossing
        * *departurethreshold* - noise reduction technique.  requires this many points to be above the threshold to trigger crossing

        **RETURNS**

        A a lumpy numpy array of the pixel values. Ususally this is in BGR format.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> myColor = [0,0,0]
        >>> sl = img.getHorzScanline(422)
        >>> sll = sl.tolist()
        >>> for p in sll:
        >>>    if( p == myColor ):
        >>>        # do something

        **SEE ALSO**

        :py:meth:`getHorzScanlineGray`
        :py:meth:`getVertScanlineGray`
        :py:meth:`getVertScanline`

        """
        linearr = self.getDiagonalScanlineGrey(pt1,pt2)
        ind = 0
        crossing = -1
        if departurethreshold==1:
            while ind < linearr.size-1:
                if darktolight:
                    if linearr[ind] <=threshold and linearr[ind+1] > threshold:
                        crossing = ind
                        break
                if lighttodark:
                    if linearr[ind] >= threshold and linearr[ind+1] < threshold:
                        crossing = ind
                        break
                ind = ind +1
            if crossing != -1:
                xind = pt1[0] + int(round((pt2[0]-pt1[0])*crossing/linearr.size))
                yind = pt1[1] + int(round((pt2[1]-pt1[1])*crossing/linearr.size))
                retVal = (xind,yind)
            else:
                retVal = (-1,-1)
                #print 'Edgepoint not found.'
        else:
            while ind < linearr.size-(departurethreshold+1):
                if darktolight:
                    if linearr[ind] <=threshold and (linearr[ind+1:ind+1+departurethreshold] > threshold).all():
                        crossing = ind
                        break
                if lighttodark:
                    if linearr[ind] >= threshold and (linearr[ind+1:ind+1+departurethreshold] < threshold).all():
                        crossing = ind
                        break
                ind = ind +1
            if crossing != -1:
                xind = pt1[0] + int(round((pt2[0]-pt1[0])*crossing/linearr.size))
                yind = pt1[1] + int(round((pt2[1]-pt1[1])*crossing/linearr.size))
                retVal = (xind,yind)
            else:
                retVal = (-1,-1)
                #print 'Edgepoint not found.'
        return retVal
        

    def getDiagonalScanlineGrey(self, pt1, pt2):
        """
        **SUMMARY**

        This function returns a single line of greyscale values from the image.
        TODO: speed inprovements and RGB tolerance

        **PARAMETERS**

        * *pt1, pt2* - the starting and ending points in tuple form e.g. (1,2)

        **RETURNS**

        An array of the pixel values.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> sl = img.getDiagonalScanlineGrey((100,200),(300,400))
        

        **SEE ALSO**

        :py:meth:`getHorzScanlineGray`
        :py:meth:`getVertScanlineGray`
        :py:meth:`getVertScanline`

        """
        if not self.isGray():
            self = self.toGray()
        #self = self._getGrayscaleBitmap()
        width = round(math.sqrt(math.pow(pt2[0]-pt1[0],2) + math.pow(pt2[1]-pt1[1],2)))
        retVal = np.zeros(width)
        
        for x in range(0, retVal.size):
            xind = pt1[0] + int(round((pt2[0]-pt1[0])*x/retVal.size))
            yind = pt1[1] + int(round((pt2[1]-pt1[1])*x/retVal.size))
            current_pixel = self.getPixel(xind,yind)
            retVal[x] = current_pixel[0]
        return retVal

    def fitLines(self,guesses,window=10,threshold=128):
        """
        **SUMMARY**

        Fit lines in a binary/gray image using an initial guess and the least squares method.
        The lines are returned as a line feature set.

        **PARAMETERS**

        * *guesses* - A list of tuples of the form ((x0,y0),(x1,y1)) where each of the lines
          is an approximate guess.
        * *window* - A window around the guess to search.
        * *threshold* - the threshold above which we count a pixel as a line

        **RETURNS**

        A feature set of line features, one per guess.

        **EXAMPLE**


        >>> img = Image("lsq.png")
        >>> guesses = [((313,150),(312,332)),((62,172),(252,52)),((102,372),(182,182)),((372,62),(572,162)),((542,362),(462,182)),((232,412),(462,423))]
        >>> l = img.fitLines(guesses,window=10)
        >>> l.draw(color=Color.RED,width=3)
        >>> for g in guesses:
        >>>    img.drawLine(g[0],g[1],color=Color.YELLOW)

        >>> img.show()
        """

        retVal = FeatureSet()
        i =0
        for g in guesses:
            # Guess the size of the crop region from the line guess and the window.
            ymin = np.min([g[0][1],g[1][1]])
            ymax = np.max([g[0][1],g[1][1]])
            xmin = np.min([g[0][0],g[1][0]])
            xmax = np.max([g[0][0],g[1][0]])

            xminW = np.clip(xmin-window,0,self.width)
            xmaxW = np.clip(xmax+window,0,self.width)
            yminW = np.clip(ymin-window,0,self.height)
            ymaxW = np.clip(ymax+window,0,self.height)
            temp = self.crop(xminW,yminW,xmaxW-xminW,ymaxW-yminW)
            temp = temp.getGrayNumpy()

            # pick the lines above our threshold
            x,y = np.where(temp>threshold)
            pts = zip(x,y)
            gpv = np.array([float(g[0][0]-xminW),float(g[0][1]-yminW)])
            gpw = np.array([float(g[1][0]-xminW),float(g[1][1]-yminW)])
            def lineSegmentToPoint(p):
                w = gpw
                v = gpv
                #print w,v
                p = np.array([float(p[0]),float(p[1])])
                l2 = np.sum((w-v)**2)
                t = float(np.dot((p-v),(w-v))) / float(l2)
                if( t < 0.00 ):
                    return np.sqrt(np.sum((p-v)**2))
                elif(t > 1.0):
                    return np.sqrt(np.sum((p-w)**2))
                else:
                    project = v + (t*(w-v))
                    return np.sqrt(np.sum((p-project)**2))
            # http://stackoverflow.com/questions/849211/shortest-distance-between-a-point-and-a-line-segment

            distances = np.array(map(lineSegmentToPoint,pts))
            closepoints = np.where(distances<window)[0]

            pts = np.array(pts)

            if( len(closepoints) < 3 ):
                continue

            good_pts = pts[closepoints]
            good_pts = good_pts.astype(float)


            x = good_pts[:,0]
            y = good_pts[:,1]
            # do the shift from our crop
            # generate the line values
            x = x + xminW
            y = y + yminW

            ymin = np.min(y)
            ymax = np.max(y)
            xmax = np.max(x)
            xmin = np.min(x)

            if( (xmax-xmin) > (ymax-ymin) ):
                # do the least squares
                A = np.vstack([x,np.ones(len(x))]).T
                m,c = nla.lstsq(A,y)[0]
                y0 = int(m*xmin+c)
                y1 = int(m*xmax+c)
                retVal.append(Line(self,((xmin,y0),(xmax,y1))))
            else:
                # do the least squares
                A = np.vstack([y,np.ones(len(y))]).T
                m,c = nla.lstsq(A,x)[0]
                x0 = int(ymin*m+c)
                x1 = int(ymax*m+c)
                retVal.append(Line(self,((x0,ymin),(x1,ymax))))

        return retVal

    def fitLinePoints(self,guesses,window=(11,11), samples=20,params=(0.1,0.1,0.1)):
        """
        **DESCRIPTION**

        This method uses the snakes / active contour approach in an attempt to
        fit a series of points to a line that may or may not be exactly linear.

        **PARAMETERS**

        * *guesses* - A set of lines that we wish to fit to. The lines are specified
          as a list of tuples of (x,y) tuples. E.g. [((x0,y0),(x1,y1))....]
        * *window* - The search window in pixels for the active contours approach.
        * *samples* - The number of points to sample along the input line,
          these are the initial conditions for active contours method.
        * *params* - the alpha, beta, and gamma values for the active contours routine.

        **RETURNS**

        A list of fitted contour points. Each contour is a list of (x,y) tuples.

        **EXAMPLE**

        >>> img = Image("lsq.png")
        >>> guesses = [((313,150),(312,332)),((62,172),(252,52)),((102,372),(182,182)),((372,62),(572,162)),((542,362),(462,182)),((232,412),(462,423))]
        >>> r = img.fitLinePoints(guesses)
        >>> for rr in r:
        >>>    img.drawLine(rr[0],rr[1],color=Color.RED,width=3)
        >>> for g in guesses:
        >>>    img.drawLine(g[0],g[1],color=Color.YELLOW)

        >>> img.show()

        """
        pts = []
        for g in guesses:
            #generate the approximation
            bestGuess = []
            dx = float(g[1][0]-g[0][0])
            dy = float(g[1][1]-g[0][1])
            l = np.sqrt((dx*dx)+(dy*dy))
            if( l <= 0 ):
                logger.warning("Can't Do snakeFitPoints without OpenCV >= 2.3.0")
                return

            dx = dx/l
            dy = dy/l
            for i in range(-1,samples+1):
                t = i*(l/samples)
                bestGuess.append((int(g[0][0]+(t*dx)),int(g[0][1]+(t*dy))))
            # do the snake fitting
            appx = self.fitContour(bestGuess,window=window,params=params,doAppx=False)
            pts.append(appx)

        return pts



    def drawPoints(self, pts, color=Color.RED, sz=3, width=-1):
        """
        **DESCRIPTION**

        A quick and dirty points rendering routine.

        **PARAMETERS**

        * *pts* - pts a list of (x,y) points.
        * *color* - a color for our points.
        * *sz* - the circle radius for our points.
        * *width* - if -1 fill the point, otherwise the size of point border

        **RETURNS**

        None - This is an inplace operation.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> img.drawPoints([(10,10),(30,30)])
        >>> img.show()
        """
        for p in pts:
            self.drawCircle(p,sz,color,width)
        return None

    def sobel(self, xorder=1, yorder=1, doGray=True, aperture=5, aperature=None):
        """
        **DESCRIPTION**

        Sobel operator for edge detection

        **PARAMETERS**

        * *xorder* - int - Order of the derivative x.
        * *yorder* - int - Order of the derivative y.
        * *doGray* - Bool - grayscale or not.
        * *aperture* - int - Size of the extended Sobel kernel. It must be 1, 3, 5, or 7.

        **RETURNS**

        Image with sobel opeartor applied on it

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> s = img.sobel()
        >>> s.show()
        """
        aperture = aperature if aperature else aperture
        retVal = None
        try:
            import cv2
        except:
            logger.warning("Can't do Sobel without OpenCV >= 2.3.0")
            return None

        if( aperture != 1 and aperture != 3 and aperture != 5 and aperture != 7 ):
            logger.warning("Bad Sobel Aperture, values are [1,3,5,7].")
            return None

        if( doGray ):
            dst = cv2.Sobel(self.getGrayNumpy(),cv2.cv.CV_32F,xorder,yorder,ksize=aperture)
            minv = np.min(dst)
            maxv = np.max(dst)
            cscale = 255/(maxv-minv)
            shift =  -1*(minv)

            t = np.zeros(self.size(),dtype='uint8')
            t = cv2.convertScaleAbs(dst,t,cscale,shift/255.0)
            retVal = Image(t)

        else:
            layers = self.splitChannels(grayscale=False)
            sobel_layers = []
            for layer in layers:
                dst = cv2.Sobel(layer.getGrayNumpy(),cv2.cv.CV_32F,xorder,yorder,ksize=aperture)

                minv = np.min(dst)
                maxv = np.max(dst)
                cscale = 255/(maxv-minv)
                shift =  -1*(minv)

                t = np.zeros(self.size(),dtype='uint8')
                t = cv2.convertScaleAbs(dst,t,cscale,shift/255.0)
                sobel_layers.append(Image(t))
            b,g,r = sobel_layers

            retVal = self.mergeChannels(b,g,r)
        return retVal

    def track(self, method="CAMShift", ts=None, img=None, bb=None, **kwargs):
        """
        
        **DESCRIPTION**

        Tracking the object surrounded by the bounding box in the given
        image or TrackSet.

        **PARAMETERS**

        * *method* - str - The Tracking Algorithm to be applied
        * *ts* - TrackSet - SimpleCV.Features.TrackSet.
        * *img* - Image - Image to be tracked or list - List of Images to be tracked.
        * *bb* - tuple - Bounding Box tuple (x, y, w, h)
        

        **Optional Parameters**

        *CAMShift*

        CAMShift Tracker is based on mean shift thresholding algorithm which is
        combined with an adaptive region-sizing step. Histogram is calcualted based
        on the mask provided. If mask is not provided, hsv transformed image of the
        provided image is thresholded using inRange function (band thresholding).

        lower HSV and upper HSV values are used inRange function. If the user doesn't 
        provide any range values, default range values are used.

        Histogram is back projected using previous images to get an appropriate image
        and it passed to camshift function to find the object in the image. Users can 
        decide the number of images to be used in back projection by providing num_frames.

        lower - Lower HSV value for inRange thresholding. tuple of (H, S, V). Default : (0, 60, 32)
        upper - Upper HSV value for inRange thresholding. tuple of (H, S, V). Default: (180, 255, 255)
        mask - Mask to calculate Histogram. It's better if you don't provide one. Default: calculated using above thresholding ranges.
        num_frames - number of frames to be backtracked. Default: 40

        *LK*

        LK Tracker is based on Optical Flow method. In brief, optical flow can be
        defined as the apparent motion of objects caused by the relative motion between
        an observer and the scene. (Wikipedia).

        LK Tracker first finds some good feature points in the given bounding box in the image.
        These are the tracker points. In consecutive frames, optical flow of these feature points
        is calculated. Users can limit the number of feature points by provideing maxCorners and 
        qualityLevel. number of features will always be less than maxCorners. These feature points
        are calculated using Harris Corner detector. It returns a matrix with each pixel having
        some quality value. Only good features are used based upon the qualityLevel provided. better
        features have better quality measure and hence are more suitable to track.

        Users can set minimum distance between each features by providing minDistance.

        LK tracker finds optical flow using a number of pyramids and users can set this number by
        providing maxLevel and users can set size of the search window for Optical Flow by setting 
        winSize.

        docs from http://docs.opencv.org/
        maxCorners - Maximum number of corners to return in goodFeaturesToTrack. If there are more corners than are found, the strongest of them is returned. Default: 4000
        qualityLevel - Parameter characterizing the minimal accepted quality of image corners. The parameter value is multiplied by the best corner quality measure, which is the minimal eigenvalue or the Harris function response. The corners with the quality measure less than the product are rejected. For example, if the best corner has the quality measure = 1500,  and the qualityLevel=0.01 , then all the corners with the quality measure less than 15 are rejected. Default: 0.08
        minDistance - Minimum possible Euclidean distance between the returned corners. Default: 2
        blockSize - Size of an average block for computing a derivative covariation matrix over each pixel neighborhood. Default: 3
        winSize - size of the search window at each pyramid level. Default: (10, 10)
        maxLevel - 0-based maximal pyramid level number; if set to 0, pyramids are not used (single level), Default: 10 if set to 1, two levels are used, and so on

        *SURF*

        SURF based tracker finds keypoints in the template and computes the descriptor. The template is
        chosen based on the bounding box provided with the first image. The image is cropped and stored 
        as template. SURF keypoints are found and descriptor is computed for the template and stored.

        SURF keypoints are found in the image and its descriptor is computed. Image keypoints and template
        keypoints are matched using K-nearest neighbor algorithm. Matched keypoints are filtered according
        to the knn distance of the points. Users can set this criteria by setting distance.
        Density Based Clustering algorithm (DBSCAN) is applied on the matched keypoints to filter out points
        that are in background. DBSCAN creates a cluster of object points anc background points. These background
        points are discarded. Users can set certain parameters for DBSCAN which are listed below.

        K-means is applied on matched KeyPoints with k=1 to find the center of the cluster and then bounding
        box is predicted based upon the position of all the object KeyPoints.

        eps_val - eps for DBSCAN. The maximum distance between two samples for them to be considered as in the same neighborhood. default: 0.69
        min_samples - min number of samples in DBSCAN. The number of samples in a neighborhood for a point to be considered as a core point. default: 5
        distance - thresholding KNN distance of each feature. if KNN distance > distance, point is discarded. default: 100

        *MFTrack*

        Median Flow tracker is similar to LK tracker (based on Optical Flow), but it's more advanced, better and
        faster.

        In MFTrack, tracking points are decided based upon the number of horizontal and vertical points and window
        size provided by the user. Unlike LK Tracker, good features are not found which saves a huge amount of time.

        feature points are selected symmetrically in the bounding box.
        Total number of feature points to be tracked = numM * numN.

        If the width and height of bounding box is 200 and 100 respectively, and numM = 10 and numN = 10,
        there will be 10 points in the bounding box equally placed(10 points in 200 pixels) in each row. and 10 equally placed
        points (10 points in 100 pixels) in each column. So total number of tracking points = 100.

        numM > 0
        numN > 0 (both may not be equal)

        users can provide a margin around the bounding box that will be considered to place feature points and
        calculate optical flow.
        Optical flow is calculated from frame1 to frame2 and from frame2 to frame1. There might be some points 
        which give inaccurate optical flow, to eliminate these points the above method is used. It is called
        forward-backward error tracking. Optical Flow seach window size can be set usung winsize_lk.

        For each point, comparision is done based on the quadratic area around it.
        The length of the square window can be set using winsize.

        numM        - Number of points to be tracked in the bounding box
                      in height direction. 
                      default: 10
                
        numN        - Number of points to be tracked in the bounding box
                      in width direction.
                      default: 10 
                  
        margin      - Margin around the bounding box.
                      default: 5

        winsize_lk  - Optical Flow search window size.
                      default: 4

        winsize     - Size of quadratic area around the point which is compared.
                      default: 10


        Available Tracking Methods

         - CamShift
         - LK
         - SURF
         - MFTrack
         

        **RETURNS**

        SimpleCV.Features.TrackSet

        Returns a TrackSet with all the necessary attributes.

        **HOW TO**

        >>> ts = img.track("camshift", img=img1, bb=bb)
        

        Here TrackSet is returned. All the necessary attributes will be included in the trackset.
        After getting the trackset you need not provide the bounding box or image. You provide TrackSet as parameter to track().
        Bounding box and image will be taken from the trackset.
        So. now

        >>> ts = new_img.track("camshift",ts)

        The new Tracking feature will be appended to the given trackset and that will be returned.
        So, to use it in loop::

          img = cam.getImage()
          bb = (img.width/4,img.height/4,img.width/4,img.height/4)
          ts = img.track(img=img, bb=bb)
          while (True):
              img = cam.getImage()
              ts = img.track("camshift", ts=ts)

          ts = []
          while (some_condition_here):
              img = cam.getImage()
              ts = img.track("camshift",ts,img0,bb)

        
        now here in first loop iteration since ts is empty, img0 and bb will be considered.
        New tracking object will be created and added in ts (TrackSet)
        After first iteration, ts is not empty and hence the previous
        image frames and bounding box will be taken from ts and img0
        and bb will be ignored.

        # Instead of loop, give a list of images to be tracked.

        ts = []
        imgs = [img1, img2, img3, ..., imgN]
        ts = img0.track("camshift", ts, imgs, bb)
        ts.drawPath()
        ts[-1].image.show()

        Using Optional Parameters:

        for CAMShift
        
        >>> ts = []
        >>> ts = img.track("camshift", ts, img1, bb, lower=(40, 100, 100), upper=(100, 250, 250))

        You can provide some/all/None of the optional parameters listed for CAMShift.

        for LK
        
        >>> ts = []
        >>> ts = img.track("lk", ts, img1, bb, maxCorners=4000, qualityLevel=0.5, minDistance=3)

        You can provide some/all/None of the optional parameters listed for LK.

        for SURF
        
        >>> ts = []
        >>> ts = img.track("surf", ts, img1, bb, eps_val=0.7, min_samples=8, distance=200)

        You can provide some/all/None of the optional parameters listed for SURF.

        for MFTrack
        >>> ts = []
        >>> ts = img.track("mftrack", ts, img1, bb, numM=12, numN=12, winsize=15)

        You can provide some/all/None of the optional parameters listed for MFTrack.

        Check out Tracking examples provided in the SimpleCV source code.

        READ MORE:

        CAMShift Tracker:
        Uses meanshift based CAMShift thresholding technique. Blobs and objects with
        single tone or tracked very efficiently. CAMshift should be preferred if you 
        are trying to track faces. It is optimized to track faces.

        LK (Lucas Kanade) Tracker:
        It is based on LK Optical Flow. It calculates Optical flow in frame1 to frame2 
        and also in frame2 to frame1 and using back track error, filters out false
        positives.

        SURF based Tracker:
        Matches keypoints from the template image and the current frame.
        flann based matcher is used to match the keypoints.
        Density based clustering is used classify points as in-region (of bounding box)
        and out-region points. Using in-region points, new bounding box is predicted using
        k-means.

        Median Flow Tracker:
    
        Media Flow Tracker is the base tracker that is used in OpenTLD. It is based on
        Optical Flow. It calculates optical flow of the points in the bounding box from
        frame 1 to frame 2 and from frame 2 to frame 1 and using back track error, removes
        false positives. As the name suggests, it takes the median of the flow, and eliminates
        points.
        """
        if not ts and not img:
            print "Invalid Input. Must provide FeatureSet or Image"
            return None

        if not ts and not bb:
            print "Invalid Input. Must provide Bounding Box with Image"
            return None

        if not ts:
            ts = TrackSet()
        else:
            img = ts[-1].image
            bb = ts[-1].bb
        try:
            import cv2
        except ImportError:
            print "Tracking is available for OpenCV >= 2.3"
            return None

        if type(img) == list:
            ts = self.track(method, ts, img[0], bb, **kwargs)
            for i in img:
                ts = i.track(method, ts, **kwargs)
            return ts

        # Issue #256 - (Bug) Memory management issue due to too many number of images.
        nframes = 300
        if 'nframes' in kwargs:
            nframes = kwargs['nframes']

        if len(ts) > nframes:
            ts.trimList(50)

        if method.lower() == "camshift":
            track = camshiftTracker(self, bb, ts, **kwargs)
            ts.append(track)

        elif method.lower() == "lk":
            track = lkTracker(self, bb, ts, img, **kwargs)
            ts.append(track)

        elif method.lower() == "surf":
            try:
                from scipy.spatial import distance as Dis
                from sklearn.cluster import DBSCAN
            except ImportError:
                logger.warning("sklearn required")
                return None
            if not hasattr(cv2, "FeatureDetector_create"):
                warnings.warn("OpenCV >= 2.4.3 required. Returning None.")
                return None
            track = surfTracker(self, bb, ts, **kwargs)
            ts.append(track)

        elif method.lower() == "mftrack":
            track = mfTracker(self, bb, ts, img, **kwargs)
            ts.append(track)

        return ts

    def _to32F(self):
        """
        **SUMMARY**

        Convert this image to a 32bit floating point image.

        """
        retVal = cv.CreateImage((self.width,self.height), cv.IPL_DEPTH_32F, 3)
        cv.Convert(self.getBitmap(),retVal)
        return retVal

    def __getstate__(self):
        return dict( size = self.size(), colorspace = self._colorSpace, image = self.applyLayers().getBitmap().tostring() )

    def __setstate__(self, mydict):
        self._bitmap = cv.CreateImageHeader(mydict['size'], cv.IPL_DEPTH_8U, 3)
        cv.SetData(self._bitmap, mydict['image'])
        self._colorSpace = mydict['colorspace']
        self.width = mydict['size'][0]
        self.height = mydict['size'][1]

    def area(self):
        '''
        Returns the area of the Image.
        '''

        return self.width * self.height

    def _get_header_anim(self):
        """ Animation header. To replace the getheader()[0] """
        bb = "GIF89a"
        bb += int_to_bin(self.size()[0])
        bb += int_to_bin(self.size()[1])
        bb += "\x87\x00\x00"
        return bb

    def rotate270(self):
        """
        **DESCRIPTION**

        Rotate the image 270 degrees to the left, the same as 90 degrees to the right.
        This is the same as rotateRight()

        **RETURNS**

        A SimpleCV image.

        **EXAMPLE**

        >>>> img = Image('lenna')
        >>>> img.rotate270().show()

        """
        retVal = cv.CreateImage((self.height, self.width), cv.IPL_DEPTH_8U, 3)
        cv.Transpose(self.getBitmap(), retVal)
        cv.Flip(retVal, retVal, 1) 
        return(Image(retVal, colorSpace=self._colorSpace))

    def rotate90(self):
        """
        **DESCRIPTION**

        Rotate the image 90 degrees to the left, the same as 270 degrees to the right.
        This is the same as rotateRight()

        **RETURNS**

        A SimpleCV image.

        **EXAMPLE**

        >>>> img = Image('lenna')
        >>>> img.rotate90().show()

        """

        retVal = cv.CreateImage((self.height, self.width), cv.IPL_DEPTH_8U, 3)
        cv.Transpose(self.getBitmap(), retVal)
        cv.Flip(retVal, retVal, 0) # vertical
        return(Image(retVal, colorSpace=self._colorSpace))

    def rotateLeft(self): # same as 90
        """
        **DESCRIPTION**

        Rotate the image 90 degrees to the left.
        This is the same as rotate 90.

        **RETURNS**

        A SimpleCV image.

        **EXAMPLE**

        >>>> img = Image('lenna')
        >>>> img.rotateLeft().show()

        """

        return self.rotate90()

    def rotateRight(self): # same as 270
        """
        **DESCRIPTION**

        Rotate the image 90 degrees to the right.
        This is the same as rotate 270.

        **RETURNS**

        A SimpleCV image.

        **EXAMPLE**

        >>>> img = Image('lenna')
        >>>> img.rotateRight().show()

        """
        
        return self.rotate270()


    def rotate180(self):
        """
        **DESCRIPTION**

        Rotate the image 180 degrees to the left/right.
        This is the same as rotate 90.

        **RETURNS**

        A SimpleCV image.

        **EXAMPLE**

        >>>> img = Image('lenna')
        >>>> img.rotate180().show()
        """
        retVal = cv.CreateImage((self.width, self.height), cv.IPL_DEPTH_8U, 3)
        cv.Flip(self.getBitmap(), retVal, 0) #vertical
        cv.Flip(retVal, retVal, 1)#horizontal
        return(Image(retVal, colorSpace=self._colorSpace))

    def verticalHistogram(self, bins=10, threshold=128,normalize=False,forPlot=False):
        """
        
        **DESCRIPTION**

        This method generates histogram of the number of grayscale pixels
        greater than the provided threshold. The method divides the image
        into a number evenly spaced vertical bins and then counts the number
        of pixels where the pixel is greater than the threshold. This method
        is helpful for doing basic morphological analysis.

        **PARAMETERS**
        
        * *bins* - The number of bins to use.
        * *threshold* - The grayscale threshold. We count pixels greater than this value.
        * *normalize* - If normalize is true we normalize the bin countsto sum to one. Otherwise we return the number of pixels.
        * *forPlot* - If this is true we return the bin indicies, the bin counts, and the bin widths as a tuple. We can use these values in pyplot.bar to quickly plot the histogram.
        

        **RETURNS**

        The default settings return the raw bin counts moving from left to
        right on the image. If forPlot is true we return a tuple that
        contains a list of bin labels, the bin counts, and the bin widths.
        This tuple can be used to plot the histogram using
        matplotlib.pyplot.bar function.
        
        
        **EXAMPLE**
        
          >>> import matplotlib.pyplot as plt
          >>> img = Image('lenna')
          >>> plt.bar(*img.verticalHistogram(threshold=128,bins=10,normalize=False,forPlot=True),color='y')
          >>> plt.show()
        
        
        **NOTES**

        See: http://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html
        See: http://matplotlib.org/api/pyplot_api.html?highlight=hist#matplotlib.pyplot.hist

        """
        if( bins <= 0 ):
            raise Exception("Not enough bins")

        img = self.getGrayNumpy()
        pts = np.where(img>threshold)
        y = pts[1]
        hist = np.histogram(y,bins=bins,range=(0,self.height),normed=normalize)
        retVal = None
        if( forPlot ):
            # for using matplotlib bar command
            # bin labels, bin values, bin width
            retVal=(hist[1][0:-1],hist[0],self.height/bins)
        else:
            retVal = hist[0]
        return retVal

    def horizontalHistogram(self, bins=10, threshold=128,normalize=False,forPlot=False):
        """
        
        **DESCRIPTION**

        This method generates histogram of the number of grayscale pixels
        greater than the provided threshold. The method divides the image
        into a number evenly spaced horizontal bins and then counts the number
        of pixels where the pixel is greater than the threshold. This method
        is helpful for doing basic morphological analysis.

        **PARAMETERS**
        
        * *bins* - The number of bins to use.
        * *threshold* - The grayscale threshold. We count pixels greater than this value.
        * *normalize* - If normalize is true we normalize the bin counts to sum to one. Otherwise we return the number of pixels.
        * *forPlot* - If this is true we return the bin indicies, the bin counts, and the bin widths as a tuple. We can use these values in pyplot.bar to quickly plot the histogram.
        
        
        **RETURNS**

        The default settings return the raw bin counts moving from top to
        bottom on the image. If forPlot is true we return a tuple that
        contains a list of bin labels, the bin counts, and the bin widths.
        This tuple can be used to plot the histogram using
        matplotlib.pyplot.bar function.

        **EXAMPLE**

        >>>> import matplotlib.pyplot as plt
        >>>> img = Image('lenna')
        >>>> plt.bar(img.horizontalHistogram(threshold=128,bins=10,normalize=False,forPlot=True),color='y')
        >>>> plt.show())

        **NOTES**

        See: http://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html
        See: http://matplotlib.org/api/pyplot_api.html?highlight=hist#matplotlib.pyplot.hist

        """
        if( bins <= 0 ):
            raise Exception("Not enough bins")

        img = self.getGrayNumpy()
        pts = np.where(img>threshold)
        x = pts[0]
        hist = np.histogram(x,bins=bins,range=(0,self.width),normed=normalize)
        retVal = None
        if( forPlot ):
            # for using matplotlib bar command
            # bin labels, bin values, bin width
            retVal=(hist[1][0:-1],hist[0],self.width/bins)
        else:
            retVal = hist[0]
        return retVal

    def getLineScan(self,x=None,y=None,pt1=None,pt2=None,channel = -1):
        """
        **SUMMARY**

        This function takes in a channel of an image or grayscale by default
        and then pulls out a series of pixel values as a linescan object
        than can be manipulated further.

        **PARAMETERS**
        
        * *x* - Take a vertical line scan at the column x.
        * *y* - Take a horizontal line scan at the row y.
        * *pt1* - Take a line scan between two points on the line the line scan values always go in the +x direction
        * *pt2* - Second parameter for a non-vertical or horizontal line scan.
        * *channel* - To select a channel. eg: selecting a channel RED,GREEN or BLUE. If set to -1 it operates with gray scale values
        
        
        **RETURNS**

        A SimpleCV.LineScan object or None if the method fails.

        **EXAMPLE**

        >>>> import matplotlib.pyplot as plt
        >>>> img = Image('lenna')
        >>>> a = img.getLineScan(x=10)
        >>>> b = img.getLineScan(y=10)
        >>>> c = img.getLineScan(pt1 = (10,10), pt2 = (500,500) )
        >>>> plt.plot(a)
        >>>> plt.plot(b)
        >>>> plt.plot(c)
        >>>> plt.show()

        """

        if channel == -1:
            img = self.getGrayNumpy()
        else:
            try:
                img = self.getNumpy()[:,:,channel]
            except IndexError:
                print 'Channel missing!'
                return None

        retVal = None
        if( x is not None and y is None and pt1 is None and pt2 is None):
            if( x >= 0 and x < self.width):
                retVal = LineScan(img[x,:])
                retVal.image = self
                retVal.pt1 = (x,0)
                retVal.pt2 = (x,self.height)
                retVal.col = x
                x = np.ones((1,self.height))[0]*x
                y = range(0,self.height,1)
                pts = zip(x,y)
                retVal.pointLoc = pts
            else:
                warnings.warn("ImageClass.getLineScan - that is not valid scanline.")
                return None

        elif( x is None and y is not None and pt1 is None and pt2 is None):
            if( y >= 0 and y < self.height):
                retVal = LineScan(img[:,y])
                retVal.image = self
                retVal.pt1 = (0,y)
                retVal.pt2 = (self.width,y)
                retVal.row = y
                y = np.ones((1,self.width))[0]*y
                x = range(0,self.width,1)
                pts = zip(x,y)
                retVal.pointLoc = pts

            else:
                warnings.warn("ImageClass.getLineScan - that is not valid scanline.")
                return None

            pass
        elif( (isinstance(pt1,tuple) or isinstance(pt1,list)) and
              (isinstance(pt2,tuple) or isinstance(pt2,list)) and
              len(pt1) == 2 and len(pt2) == 2 and
              x is None and y is None):

            pts = self.bresenham_line(pt1,pt2)
            retVal = LineScan([img[p[0],p[1]] for p in pts])
            retVal.pointLoc = pts
            retVal.image = self
            retVal.pt1 = pt1
            retVal.pt2 = pt2

        else:
            # an invalid combination - warn
            warnings.warn("ImageClass.getLineScan - that is not valid scanline.")
            return None
        retVal.channel = channel
        return retVal

    def setLineScan(self, linescan,x=None,y=None,pt1=None,pt2=None,channel = -1):
        """
        **SUMMARY**

        This function helps you put back the linescan in the image.

        **PARAMETERS**
        
        * *linescan* - LineScan object
        * *x* - put  line scan at the column x.
        * *y* - put line scan at the row y.
        * *pt1* - put line scan between two points on the line the line scan values always go in the +x direction
        * *pt2* - Second parameter for a non-vertical or horizontal line scan.
        * *channel* - To select a channel. eg: selecting a channel RED,GREEN or BLUE. If set to -1 it operates with gray scale values
        
        
        **RETURNS**

        A SimpleCV.Image 

        **EXAMPLE**

        >>> img = Image('lenna')
        >>> a = img.getLineScan(x=10)
        >>> for index in range(len(a)):
            ... a[index] = 0
        >>> newimg = img.putLineScan(a, x=50)
        >>> newimg.show()
        # This will show you a black line in column 50.
        
        """
        #retVal = self.toGray()
        if channel == -1:
            img = np.copy(self.getGrayNumpy())
        else:
            try:
                img = np.copy(self.getNumpy()[:,:,channel])
            except IndexError:
                print 'Channel missing!'
                return None

        if( x is None and y is None and pt1 is None and pt2 is None):
            if(linescan.pt1 is None or linescan.pt2 is None):
                warnings.warn("ImageClass.setLineScan: No coordinates to re-insert linescan.")
                return None
            else:
                pt1 = linescan.pt1
                pt2 = linescan.pt2
                if( pt1[0] == pt2[0] and np.abs(pt1[1]-pt2[1])==self.height):
                    x = pt1[0] # vertical line
                    pt1=None
                    pt2=None

                elif( pt1[1] == pt2[1] and np.abs(pt1[0]-pt2[0])==self.width):
                    y = pt1[1] # horizontal line
                    pt1=None
                    pt2=None


        retVal = None
        if( x is not None and y is None and pt1 is None and pt2 is None):
            if( x >= 0 and x < self.width):
                if( len(linescan) != self.height ):
                    linescan = linescan.resample(self.height)
                #check for number of points
                #linescan = np.array(linescan)
                img[x,:] = np.clip(linescan[:], 0, 255)
            else:
                warnings.warn("ImageClass.setLineScan: No coordinates to re-insert linescan.")
                return None
        elif( x is None and y is not None and pt1 is None and pt2 is None):
            if( y >= 0 and y < self.height):
                if( len(linescan) != self.width ):
                    linescan = linescan.resample(self.width)
                #check for number of points
                #linescan = np.array(linescan)
                img[:,y] = np.clip(linescan[:], 0, 255)
            else:
                warnings.warn("ImageClass.setLineScan: No coordinates to re-insert linescan.")
                return None
        elif( (isinstance(pt1,tuple) or isinstance(pt1,list)) and
              (isinstance(pt2,tuple) or isinstance(pt2,list)) and
              len(pt1) == 2 and len(pt2) == 2 and
              x is None and y is None):

            pts = self.bresenham_line(pt1,pt2)
            if( len(linescan) != len(pts) ):
                linescan = linescan.resample(len(pts))
            #linescan = np.array(linescan)
            linescan = np.clip(linescan[:], 0, 255)
            idx = 0
            for pt in pts:
                img[pt[0],pt[1]]=linescan[idx]
                idx = idx+1
        else:
            warnings.warn("ImageClass.setLineScan: No coordinates to re-insert linescan.")
            return None
        if channel == -1:
            retVal = Image(img)
        else:
            temp = np.copy(self.getNumpy())
            temp[:,:,channel] = img
            retVal = Image(temp)
        return retVal

    def replaceLineScan(self, linescan, x=None, y=None, pt1=None, pt2=None, channel = None):
        """
        
        **SUMMARY**

        This function easily lets you replace the linescan in the image.
        Once you get the LineScan object, you might want to edit it. Perform
        some task, apply some filter etc and now you want to put it back where
        you took it from. By using this function, it is not necessary to specify
        where to put the data. It will automatically replace where you took the 
        LineScan from.

        **PARAMETERS**
        
        * *linescan* - LineScan object
        * *x* - put  line scan at the column x.
        * *y* - put line scan at the row y.
        * *pt1* - put line scan between two points on the line the line scan values always go in the +x direction
        * *pt2* - Second parameter for a non-vertical or horizontal line scan.
        * *channel* - To select a channel. eg: selecting a channel RED,GREEN or BLUE. If set to -1 it operates with gray scale values

        
        **RETURNS**

        A SimpleCV.Image 

        **EXAMPLE**

        >>> img = Image('lenna')
        >>> a = img.getLineScan(x=10)
        >>> for index in range(len(a)):
            ... a[index] = 0
        >>> newimg = img.replaceLineScan(a)
        >>> newimg.show()
        # This will show you a black line in column 10.
        
        """
        

        if x is None and y is None and pt1 is None and pt2 is None and channel is None:
            
            if linescan.channel == -1:
                img = np.copy(self.getGrayNumpy())
            else:
                try:
                    img = np.copy(self.getNumpy()[:,:,linescan.channel])
                except IndexError:
                    print 'Channel missing!'
                    return None

            if linescan.row is not None:
                if len(linescan) == self.width:
                    ls = np.clip(linescan, 0, 255)
                    img[:,linescan.row] = ls[:]
                else:
                    warnings.warn("LineScan Size and Image size do not match")
                    return None

            elif linescan.col is not None:
                if len(linescan) == self.height:
                    ls = np.clip(linescan, 0, 255)
                    img[linescan.col,:] = ls[:]
                else:
                    warnings.warn("LineScan Size and Image size do not match")
                    return None
            elif linescan.pt1 and linescan.pt2:
                pts = self.bresenham_line(linescan.pt1, linescan.pt2)
                if( len(linescan) != len(pts) ):
                    linescan = linescan.resample(len(pts))
                ls = np.clip(linescan[:], 0, 255)
                idx = 0
                for pt in pts:
                    img[pt[0],pt[1]]=ls[idx]
                    idx = idx+1
            
            if linescan.channel == -1:
                retVal = Image(img)
            else:
                temp = np.copy(self.getNumpy())
                temp[:,:,linescan.channel] = img
                retVal = Image(temp)

        else:
            if channel is None:
                retVal = self.setLineScan(linescan , x, y, pt1, pt2, linescan.channel)
            else:
                retVal = self.setLineScan(linescan , x, y, pt1, pt2, channel)
        return retVal


    def getPixelsOnLine(self,pt1,pt2):
        """
        **SUMMARY**

        Return all of the pixels on an arbitrary line.

        **PARAMETERS**

        * *pt1* - The first pixel coordinate as an (x,y) tuple or list.
        * *pt2* - The second pixel coordinate as an (x,y) tuple or list.

        **RETURNS**

        Returns a list of RGB pixels values.

        **EXAMPLE**

        >>>> img = Image('something.png')
        >>>> img.getPixelsOnLine( (0,0), (img.width/2,img.height/2) )
        """
        retVal = None
        if( (isinstance(pt1,tuple) or isinstance(pt1,list)) and
            (isinstance(pt2,tuple) or isinstance(pt2,list)) and
            len(pt1) == 2 and len(pt2) == 2 ):
            pts = self.bresenham_line(pt1,pt2)
            retVal = [self.getPixel(p[0],p[1]) for p in pts]
        else:
            warnings.warn("ImageClass.getPixelsOnLine - The line you provided is not valid")

        return retVal

    def bresenham_line(self, (x,y), (x2,y2)):
        """
        Brensenham line algorithm

        cribbed from: http://snipplr.com/view.php?codeview&id=22482

        This is just a helper method
        """
        if (not 0 <= x <= self.width-1 or not 0 <= y <= self.height-1 or
            not 0 <= x2 <= self.width-1 or not 0 <= y2 <= self.height-1):
            l = Line(self, ((x, y), (x2, y2))).cropToImageEdges()
            if l:
                ep = list(l.end_points)
                ep.sort()
                x, y = ep[0]
                x2, y2 = ep[1]
            else:
                return []

        steep = 0
        coords = []
        dx = abs(x2 - x)
        if (x2 - x) > 0:
            sx = 1
        else:
            sx = -1
        dy = abs(y2 - y)
        if (y2 - y) > 0:
            sy = 1
        else:
            sy = -1
        if dy > dx:
            steep = 1
            x,y = y,x
            dx,dy = dy,dx
            sx,sy = sy,sx
        d = (2 * dy) - dx
        for i in range(0,dx):
            if steep:
                coords.append((y,x))
            else:
                coords.append((x,y))
            while d >= 0:
                y = y + sy
                d = d - (2 * dx)
            x = x + sx
            d = d + (2 * dy)
        coords.append((x2,y2))
        return coords

    def uncrop(self, ListofPts): #(x,y),(x2,y2)):
        """
        **SUMMARY**

        This function allows us to translate a set of points from the crop window back to the coordinate of the source window.

        **PARAMETERS**

        * *ListofPts* - set of points from cropped image.

        **RETURNS**

        Returns a list of coordinates in the source image.

        **EXAMPLE**

        >> img = Image('lenna')
        >> croppedImg = img.crop(10,20,250,500)
        >> sourcePts = croppedImg.uncrop([(2,3),(56,23),(24,87)])
        """
        return [(i[0]+self._uncroppedX,i[1]+self._uncroppedY)for i in ListofPts]

    def grid(self,dimensions=(10,10), color=(0, 0, 0), width=1, antialias=True, alpha=-1):


        """
        **SUMMARY**

        Draw a grid on the image

        **PARAMETERS**

        * *dimensions* - No of rows and cols as an (rows,xols) tuple or list.
        * *color* - Grid's color as a tuple or list.
        * *width* - The grid line width in pixels.
        * *antialias* - Draw an antialiased object
        * *aplha* - The alpha blending for the object. If this value is -1 then the
                            layer default value is used. A value of 255 means opaque, while 0 means transparent.

        **RETURNS**

        Returns the index of the drawing layer of the grid

        **EXAMPLE**

        >>>> img = Image('something.png')
        >>>> img.grid([20,20],(255,0,0))
        >>>> img.grid((20,20),(255,0,0),1,True,0)
        """
        retVal = self.copy()
        try:
            step_row = self.size()[1]/dimensions[0]
            step_col = self.size()[0]/dimensions[1]
        except ZeroDivisionError:
            return imgTemp

        i = 1
        j = 1

        grid = DrawingLayer(self.size()) #add a new layer for grid
        while( (i < dimensions[0]) and (j < dimensions[1]) ):
            if( i < dimensions[0] ):
                grid.line((0,step_row*i), (self.size()[0],step_row*i), color, width, antialias, alpha)
                i = i + 1
            if( j < dimensions[1] ):
                grid.line((step_col*j,0), (step_col*j,self.size()[1]), color, width, antialias, alpha)
                j = j + 1
        retVal._gridLayer[0] = retVal.addDrawingLayer(grid) # store grid layer index
        retVal._gridLayer[1] = dimensions
        return retVal

    def removeGrid(self):

        """
        **SUMMARY**

                Remove Grid Layer from the Image.

        **PARAMETERS**

                None

        **RETURNS**

                Drawing Layer corresponding to the Grid Layer

        **EXAMPLE**

        >>>> img = Image('something.png')
        >>>> img.grid([20,20],(255,0,0))
        >>>> gridLayer = img.removeGrid()

        """

        if self._gridLayer[0] is not None:
            grid = self.removeDrawingLayer(self._gridLayer[0])
            self._gridLayer=[None,[0, 0]]
            return grid
        else:
            return None


    def findGridLines(self):

        """
        **SUMMARY**

        Return Grid Lines as a Line Feature Set

        **PARAMETERS**

        None

        **RETURNS**

        Grid Lines as a Feature Set

        **EXAMPLE**

        >>>> img = Image('something.png')
        >>>> img.grid([20,20],(255,0,0))
        >>>> lines = img.findGridLines()

        """

        gridIndex = self.getDrawingLayer(self._gridLayer[0])
        if self._gridLayer[0]==-1:
            print "Cannot find grid on the image, Try adding a grid first"

        lineFS = FeatureSet()
        try:
            step_row = self.size()[1]/self._gridLayer[1][0]
            step_col = self.size()[0]/self._gridLayer[1][1]
        except ZeroDivisionError:
            return None

        i = 1
        j = 1

        while( i < self._gridLayer[1][0] ):
            lineFS.append(Line(self,((0,step_row*i), (self.size()[0],step_row*i))))
            i = i + 1
        while( j < self._gridLayer[1][1] ):
            lineFS.append(Line(self,((step_col*j,0), (step_col*j,self.size()[1]))))
            j = j + 1

        return lineFS

    def logicalAND(self, img, grayscale=True):
        """
        **SUMMARY**

        Perform bitwise AND operation on images

        **PARAMETERS**

        img - the bitwise operation to be performed with
        grayscale

        **RETURNS**

        SimpleCV.ImageClass.Image

        **EXAMPLE**

        >>> img = Image("something.png")
        >>> img1 = Image("something_else.png")
        >>> img.logicalAND(img1, grayscale=False)
        >>> img.logicalAND(img1)

        """
        if not self.size() == img.size():
            print "Both images must have same sizes"
            return None
        try:
            import cv2
        except ImportError:
            print "This function is available for OpenCV >= 2.3"
        if grayscale:
            retval = cv2.bitwise_and(self.getGrayNumpyCv2(), img.getGrayNumpyCv2())
        else:
            retval = cv2.bitwise_and(self.getNumpyCv2(), img.getNumpyCv2())
        return Image(retval, cv2image=True)

    def logicalNAND(self, img, grayscale=True):
        """
        **SUMMARY**

        Perform bitwise NAND operation on images

        **PARAMETERS**

        img - the bitwise operation to be performed with
        grayscale

        **RETURNS**

        SimpleCV.ImageClass.Image

        **EXAMPLE**

        >>> img = Image("something.png")
        >>> img1 = Image("something_else.png")
        >>> img.logicalNAND(img1, grayscale=False)
        >>> img.logicalNAND(img1)

        """
        if not self.size() == img.size():
            print "Both images must have same sizes"
            return None
        try:
            import cv2
        except ImportError:
            print "This function is available for OpenCV >= 2.3"
        if grayscale:
            retval = cv2.bitwise_and(self.getGrayNumpyCv2(), img.getGrayNumpyCv2())
        else:
            retval = cv2.bitwise_and(self.getNumpyCv2(), img.getNumpyCv2())
        retval = cv2.bitwise_not(retval)
        return Image(retval, cv2image=True)

    def logicalOR(self, img, grayscale=True):
        """
        **SUMMARY**

        Perform bitwise OR operation on images

        **PARAMETERS**

        img - the bitwise operation to be performed with
        grayscale

        **RETURNS**

        SimpleCV.ImageClass.Image

        **EXAMPLE**

        >>> img = Image("something.png")
        >>> img1 = Image("something_else.png")
        >>> img.logicalOR(img1, grayscale=False)
        >>> img.logicalOR(img1)

        """
        if not self.size() == img.size():
            print "Both images must have same sizes"
            return None
        try:
            import cv2
        except ImportError:
            print "This function is available for OpenCV >= 2.3"
        if grayscale:
            retval = cv2.bitwise_or(self.getGrayNumpyCv2(), img.getGrayNumpyCv2())
        else:
            retval = cv2.bitwise_or(self.getNumpyCv2(), img.getNumpyCv2())
        return Image(retval, cv2image=True)

    def logicalXOR(self, img, grayscale=True):
        """
        **SUMMARY**

        Perform bitwise XOR operation on images

        **PARAMETERS**

        img - the bitwise operation to be performed with
        grayscale

        **RETURNS**

        SimpleCV.ImageClass.Image

        **EXAMPLE**

        >>> img = Image("something.png")
        >>> img1 = Image("something_else.png")
        >>> img.logicalXOR(img1, grayscale=False)
        >>> img.logicalXOR(img1)

        """
        if not self.size() == img.size():
            print "Both images must have same sizes"
            return None
        try:
            import cv2
        except ImportError:
            print "This function is available for OpenCV >= 2.3"
        if grayscale:
            retval = cv2.bitwise_xor(self.getGrayNumpyCv2(), img.getGrayNumpyCv2())
        else:
            retval = cv2.bitwise_xor(self.getNumpyCv2(), img.getNumpyCv2())
        return Image(retval, cv2image=True)

    def matchSIFTKeyPoints(self, template, quality=200):
        """
        **SUMMARY**

        matchSIFTKeypoint allows you to match a template image with another image using
        SIFT keypoints. The method extracts keypoints from each image, uses the Fast Local
        Approximate Nearest Neighbors algorithm to find correspondences between the feature
        points, filters the correspondences based on quality.
        This method should be able to handle a reasonable changes in camera orientation and
        illumination. Using a template that is close to the target image will yield much
        better results.

        **PARAMETERS**

        * *template* - A template image.
        * *quality* - The feature quality metric. This can be any value between about 100 and 500. Lower
          values should return fewer, but higher quality features.

        **RETURNS**

        A Tuple of lists consisting of matched KeyPoints found on the image and matched
        keypoints found on the template. keypoints are sorted according to lowest distance.

        **EXAMPLE**

        >>> template = Image("template.png")
        >>> img = camera.getImage()
        >>> fs = img.macthSIFTKeyPoints(template)

        **SEE ALSO**

        :py:meth:`_getRawKeypoints`
        :py:meth:`_getFLANNMatches`
        :py:meth:`drawKeypointMatches`
        :py:meth:`findKeypoints`

        """
        try:
            import cv2
        except ImportError:
            warnings.warn("OpenCV >= 2.4.3 required")
            return None
        if not hasattr(cv2, "FeatureDetector_create"):
            warnings.warn("OpenCV >= 2.4.3 required")
            return None
        if template == None:
            return None    
        detector = cv2.FeatureDetector_create("SIFT")
        descriptor = cv2.DescriptorExtractor_create("SIFT")
        img = self.getNumpyCv2()
        template_img = template.getNumpyCv2()

        skp = detector.detect(img)
        skp, sd = descriptor.compute(img, skp)

        tkp = detector.detect(template_img)
        tkp, td = descriptor.compute(template_img, tkp)

        idx, dist = self._getFLANNMatches(sd, td)
        dist = dist[:,0]/2500.0
        dist = dist.reshape(-1,).tolist()
        idx = idx.reshape(-1).tolist()
        indices = range(len(dist))
        indices.sort(key=lambda i: dist[i])
        dist = [dist[i] for i in indices]
        idx = [idx[i] for i in indices]
        sfs = []
        for i, dis in itertools.izip(idx, dist):
            if dis < quality:
                sfs.append(KeyPoint(template, skp[i], sd, "SIFT"))
            else:
                break #since sorted

        idx, dist = self._getFLANNMatches(td, sd)
        dist = dist[:,0]/2500.0
        dist = dist.reshape(-1,).tolist()
        idx = idx.reshape(-1).tolist()
        indices = range(len(dist))
        indices.sort(key=lambda i: dist[i])
        dist = [dist[i] for i in indices]
        idx = [idx[i] for i in indices]
        tfs = []
        for i, dis in itertools.izip(idx, dist):
            if dis < quality:
                tfs.append(KeyPoint(template, tkp[i], td, "SIFT"))
            else:
                break

        return sfs, tfs

    def drawSIFTKeyPointMatch(self, template, distance=200, num=-1, width=1):
        """
        **SUMMARY**

        Draw SIFT keypoints draws a side by side representation of two images, calculates
        keypoints for both images, determines the keypoint correspondences, and then draws
        the correspondences. This method is helpful for debugging keypoint calculations
        and also looks really cool :) .  The parameters mirror the parameters used
        for findKeypointMatches to assist with debugging

        **PARAMETERS**

        * *template* - A template image.
        * *distance* - This can be any value between about 100 and 500. Lower value should
                        return less number of features but higher quality features.
        * *num* -   Number of features you want to draw. Features are sorted according to the
                    dist from min to max.
        * *width* - The width of the drawn line.

        **RETURNS**

        A side by side image of the template and source image with each feature correspondence
        draw in a different color.

        **EXAMPLE**

        >>> img = cam.getImage()
        >>> template = Image("myTemplate.png")
        >>> result = img.drawSIFTKeypointMatch(self,template,300.00):

        **SEE ALSO**

        :py:meth:`drawKeypointMatches`
        :py:meth:`findKeypoints`
        :py:meth:`findKeypointMatch`

        """
        if template == None:
            return
        resultImg = template.sideBySide(self,scale=False)
        hdif = (self.height-template.height)/2
        sfs, tfs = self.matchSIFTKeyPoints(template, distance)
        maxlen = min(len(sfs), len(tfs))
        if num < 0 or num > maxlen:
            num = maxlen
        for i in range(num):
            skp = sfs[i]
            tkp = tfs[i]
            pt_a = (int(tkp.y), int(tkp.x)+hdif)
            pt_b = (int(skp.y)+template.width, int(skp.x))
            resultImg.drawLine(pt_a, pt_b, color=Color.getRandom(),thickness=width)
        return resultImg

    def stegaEncode(self,message):
        """
        **SUMMARY**

        A simple steganography tool for hidding messages in images.
        **PARAMETERS**

        * *message* -A message string that you would like to encode.

        **RETURNS**

        Your message encoded in the returning image.

        **EXAMPLE**

        >>>> img = Image('lenna')
        >>>> img2 = img.stegaEncode("HELLO WORLD!")
        >>>> img2.save("TopSecretImg.png")
        >>>> img3 = Image("TopSecretImg.png")
        >>>> img3.stegaDecode()

        **NOTES**

        More here:
        http://en.wikipedia.org/wiki/Steganography
        You will need to install stepic:
        http://domnit.org/stepic/doc/pydoc/stepic.html

        You may need to monkey with jpeg compression
        as it seems to degrade the encoded message.

        PNG sees to work quite well.

        """

        try:
            import stepic
        except ImportError:
            logger.warning("stepic library required")
            return None
        warnings.simplefilter("ignore")
        pilImg = pil.frombuffer("RGB",self.size(),self.toString())
        stepic.encode_inplace(pilImg,message)
        retVal = Image(pilImg)
        return retVal.flipVertical()

    def stegaDecode(self):
        """
        **SUMMARY**

        A simple steganography tool for hidding and finding
        messages in images.

        **RETURNS**

        Your message decoded in the image.

        **EXAMPLE**

        >>>> img = Image('lenna')
        >>>> img2 = img.stegaEncode("HELLO WORLD!")
        >>>> img2.save("TopSecretImg.png")
        >>>> img3 = Image("TopSecretImg.png")
        >>>> img3.stegaDecode()

        **NOTES**

        More here:
        http://en.wikipedia.org/wiki/Steganography
        You will need to install stepic:
        http://domnit.org/stepic/doc/pydoc/stepic.html

        You may need to monkey with jpeg compression
        as it seems to degrade the encoded message.

        PNG sees to work quite well.

        """
        try:
            import stepic
        except ImportError:
            logger.warning("stepic library required")
            return None
        warnings.simplefilter("ignore")
        pilImg = pil.frombuffer("RGB",self.size(),self.toString())
        result = stepic.decode(pilImg)
        return result

    def findFeatures(self, method="szeliski", threshold=1000):
        """
        **SUMMARY**

        Find szeilski or Harris features in the image.
        Harris features correspond to Harris corner detection in the image.

        Read more:

        Harris Features: http://en.wikipedia.org/wiki/Corner_detection
        szeliski Features: http://research.microsoft.com/en-us/um/people/szeliski/publications.htm

        **PARAMETERS**

        * *method* - Features type
        * *threshold* - threshold val

        **RETURNS**

        A list of Feature objects corrseponding to the feature points.

        **EXAMPLE**

        >>> img = Image("corner_sample.png")
        >>> fpoints = img.findFeatures("harris", 2000)
        >>> for f in fpoints:
            ... f.draw()
        >>> img.show()

        **SEE ALSO**

        :py:meth:`drawKeypointMatches`
        :py:meth:`findKeypoints`
        :py:meth:`findKeypointMatch`

        """
        try:
            import cv2
        except ImportError:
            logger.warning("OpenCV >= 2.3.0 required")
            return None
        img = self.getGrayNumpyCv2()
        blur = cv2.GaussianBlur(img, (3, 3), 0)

        Ix = cv2.Sobel(blur, cv2.CV_32F, 1, 0)
        Iy = cv2.Sobel(blur, cv2.CV_32F, 0, 1)

        Ix_Ix = np.multiply(Ix, Ix)
        Iy_Iy = np.multiply(Iy, Iy)
        Ix_Iy = np.multiply(Ix, Iy)

        Ix_Ix_blur = cv2.GaussianBlur(Ix_Ix, (5, 5), 0)
        Iy_Iy_blur = cv2.GaussianBlur(Iy_Iy, (5, 5), 0)
        Ix_Iy_blur = cv2.GaussianBlur(Ix_Iy, (5, 5), 0)

        harris_thresh = threshold*5000
        alpha = 0.06
        detA = Ix_Ix_blur * Iy_Iy_blur - Ix_Iy_blur**2
        traceA = Ix_Ix_blur + Iy_Iy_blur
        feature_list = []
        if method == "szeliski":
            harmonic_mean = detA / traceA
            for j, i in np.argwhere(harmonic_mean > threshold):
                feature_list.append(Feature(self, i, j, ((i, j), (i, j), (i, j), (i, j))))

        elif method == "harris":
            harris_function = detA - (alpha*traceA*traceA)
            for j,i in np.argwhere(harris_function > harris_thresh):
                feature_list.append(Feature(self, i, j, ((i, j), (i, j), (i, j), (i, j))))
        else:
            logger.warning("Invalid method.")
            return None
        return feature_list

    def watershed(self, mask=None, erode=2,dilate=2, useMyMask=False):
        """
        **SUMMARY**

        Implements the Watershed algorithm on the input image.

        Read more:

        Watershed: "http://en.wikipedia.org/wiki/Watershed_(image_processing)"

        **PARAMETERS**

        * *mask* - an optional binary mask. If none is provided we do a binarize and invert.
        * *erode* - the number of times to erode the mask to find the foreground.
        * *dilate* - the number of times to dilate the mask to find possible background.
        * *useMyMask* - if this is true we do not modify the mask.

        **RETURNS**

        The Watershed image

        **EXAMPLE**

        >>> img = Image("/sampleimages/wshed.jpg")
        >>> img1 = img.watershed()
        >>> img1.show()

        # here is an example of how to create your own mask

        >>> img = Image('lenna')
        >>> myMask = Image((img.width,img.height))
        >>> myMask = myMask.floodFill((0,0),color=Color.WATERSHED_BG)
        >>> mask = img.threshold(128)
        >>> myMask = (myMask-mask.dilate(2)+mask.erode(2))
        >>> result = img.watershed(mask=myMask,useMyMask=True)

        **SEE ALSO**
        Color.WATERSHED_FG - The watershed foreground color
        Color.WATERSHED_BG - The watershed background color
        Color.WATERSHED_UNSURE - The watershed not sure if fg or bg color.

        TODO: Allow the user to pass in a function that defines the watershed mask.
        """

        try:
            import cv2
        except ImportError:
            logger.warning("OpenCV >= 2.3.0 required")
            return None
        output = self.getEmpty(3)
        if mask is None:
            mask = self.binarize().invert()
        newmask = None 
        if( not useMyMask ):
            newmask = Image((self.width,self.height))
            newmask = newmask.floodFill((0,0),color=Color.WATERSHED_BG)            
            newmask = (newmask-mask.dilate(dilate)+mask.erode(erode))
        else:
            newmask = mask            
        m = np.int32(newmask.getGrayNumpyCv2())
        cv2.watershed(self.getNumpyCv2(),m)
        m = cv2.convertScaleAbs(m)
        ret,thresh = cv2.threshold(m,0,255,cv2.cv.CV_THRESH_OTSU)
        retVal = Image(thresh,cv2image=True)
        return retVal

    def findBlobsFromWatershed(self,mask=None,erode=2,dilate=2,useMyMask=False,invert=False,minsize=20,maxsize=None):
        """
        **SUMMARY**

        Implements the watershed algorithm on the input image with an optional mask and t
        hen uses the mask to find blobs.

        Read more:

        Watershed: "http://en.wikipedia.org/wiki/Watershed_(image_processing)"

        **PARAMETERS**

        * *mask* - an optional binary mask. If none is provided we do a binarize and invert.
        * *erode* - the number of times to erode the mask to find the foreground.
        * *dilate* - the number of times to dilate the mask to find possible background.
        * *useMyMask* - if this is true we do not modify the mask.
        * *invert* - invert the resulting mask before finding blobs.
        * *minsize* - minimum blob size in pixels.
        * *maxsize* - the maximum blob size in pixels.

        **RETURNS**

        A feature set of blob features. 

        **EXAMPLE**

        >>> img = Image("/sampleimages/wshed.jpg")
        >>> mask = img.threshold(100).dilate(3)
        >>> blobs = img.findBlobsFromWatershed(mask)
        >>> blobs.show()

        **SEE ALSO**
        Color.WATERSHED_FG - The watershed foreground color
        Color.WATERSHED_BG - The watershed background color
        Color.WATERSHED_UNSURE - The watershed not sure if fg or bg color.
        
        """        
        newmask = self.watershed(mask,erode,dilate,useMyMask)
        if( invert ):
            newmask = mask.invert()
        return self.findBlobsFromMask(newmask,minsize=minsize,maxsize=maxsize)

    def maxValue(self,locations=False):
        """
        **SUMMARY**
        Returns the brightest/maximum pixel value in the
        grayscale image. This method can also return the
        locations of pixels with this value.

        **PARAMETERS**

        * *locations* - If true return the location of pixels
           that have this value.
        
        **RETURNS**

        The maximum value and optionally the list of points as
        a list of (x,y) tuples.
        
        **EXAMPLE**

        >>> img = Image("lenna")
        >>> max = img.maxValue()
        >>> min, pts = img.minValue(locations=True)
        >>> img2 = img.stretch(min,max)

        """
        if(locations):
            val = np.max(self.getGrayNumpy())
            x,y = np.where(self.getGrayNumpy()==val)
            locs = zip(x.tolist(),y.tolist())
            return int(val),locs
        else:
            val = np.max(self.getGrayNumpy())
            return int(val)
                
    def minValue(self,locations=False):
        """
        **SUMMARY**
        Returns the darkest/minimum pixel value in the
        grayscale image. This method can also return the
        locations of pixels with this value.

        **PARAMETERS**

        * *locations* - If true return the location of pixels
           that have this value.
        
        **RETURNS**

        The minimum value and optionally the list of points as
        a list of (x,y) tuples.
        
        **EXAMPLE**

        >>> img = Image("lenna")
        >>> max = img.maxValue()
        >>> min, pts = img.minValue(locations=True)
        >>> img2 = img.stretch(min,max)

        """
        if(locations):
            val = np.min(self.getGrayNumpy())
            x,y = np.where(self.getGrayNumpy()==val)
            locs = zip(x.tolist(),y.tolist())
            return int(val),locs
        else:
            val = np.min(self.getGrayNumpy())
            return int(val)

    
    def findKeypointClusters(self, num_of_clusters = 5, order='dsc', flavor='surf'):
        '''
        This function is meant to try and find interesting areas of an
        image. It does this by finding keypoint clusters in an image.
        It uses keypoint (ORB) detection to locate points of interest
        and then uses kmeans clustering to get the X,Y coordinates of
        those clusters of keypoints. You provide the expected number
        of clusters and you will get back a list of the X,Y coordinates
        and rank order of the number of Keypoints around those clusters

        **PARAMETERS**
        * num_of_clusters - The number of clusters you are looking for (default: 5)
        * order - The rank order you would like the points returned in, dsc or asc, (default: dsc)
        * flavor - The keypoint type, or 'corner' for just corners


        **EXAMPLE**
        
        >>> img = Image('simplecv')
        >>> clusters = img.findKeypointClusters()
        >>> clusters.draw()
        >>> img.show()

        **RETURNS**
        
        FeatureSet
        '''
        if flavor.lower() == 'corner':
          keypoints = self.findCorners() #fallback to corners
        else:
          keypoints = self.findKeypoints(flavor=flavor.upper()) #find the keypoints
        if keypoints == None or keypoints <= 0:
          return None
          
        xypoints = np.array([(f.x,f.y) for f in keypoints])
        xycentroids, xylabels = scv.kmeans2(xypoints, num_of_clusters) # find the clusters of keypoints
        xycounts = np.array([])
        
        for i in range(num_of_clusters ): #count the frequency of occurences for sorting
            xycounts = np.append(xycounts, len(np.where(xylabels == i)[-1]))
            
        merged = np.msort(np.hstack((np.vstack(xycounts), xycentroids))) #sort based on occurence
        clusters = [c[1:] for c in merged] # strip out just the values ascending
        if order.lower() == 'dsc':
            clusters = clusters[::-1] #reverse if descending

        fs = FeatureSet()
        for x,y in clusters: #map the values to a feature set
            f = Corner(self, x, y)
            fs.append(f)
  
        return fs

    def getFREAKDescriptor(self, flavor="SURF"):
        """
        **SUMMARY**

        Compute FREAK Descriptor of given keypoints.
        FREAK - Fast Retina Keypoints.
        Read more: http://www.ivpe.com/freak.htm

        Keypoints can be extracted using following detectors.

        - SURF
        - SIFT
        - BRISK
        - ORB
        - STAR
        - MSER
        - FAST
        - Dense

        **PARAMETERS**

        * *flavor* - Detector (see above list of detectors) - string

        **RETURNS**

        * FeatureSet* - A feature set of KeyPoint Features.
        * Descriptor* - FREAK Descriptor

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> fs, des = img.getFREAKDescriptor("ORB")

        """
        try:
            import cv2
        except ImportError:
            warnings.warn("OpenCV version >= 2.4.2 requierd")
            return None

        if cv2.__version__.startswith('$Rev:'):
            warnings.warn("OpenCV version >= 2.4.2 requierd")
            return None

        if int(cv2.__version__.replace('.','0'))<20402:
            warnings.warn("OpenCV version >= 2.4.2 requierd")
            return None
            
        flavors = ["SIFT", "SURF", "BRISK", "ORB", "STAR", "MSER", "FAST", "Dense"]
        if flavor not in flavors:
            warnings.warn("Unkown Keypoints detector. Returning None.")
            return None
        detector = cv2.FeatureDetector_create(flavor)
        extractor = cv2.DescriptorExtractor_create("FREAK")
        self._mKeyPoints = detector.detect(self.getGrayNumpyCv2())
        self._mKeyPoints, self._mKPDescriptors = extractor.compute(self.getGrayNumpyCv2(), 
                                                                   self._mKeyPoints)
        fs = FeatureSet()
        for i in range(len(self._mKeyPoints)):
            fs.append(KeyPoint(self, self._mKeyPoints[i], self._mKPDescriptors[i], flavor))

        return fs, self._mKPDescriptors

    def getGrayHistogramCounts(self, bins = 255, limit=-1):
        '''
        This function returns a list of tuples of greyscale pixel counts
        by frequency.  This would be useful in determining the dominate
        pixels (peaks) of the greyscale image.
  
        **PARAMETERS**
  
        * *bins* - The number of bins for the hisogram, defaults to 255 (greyscale)
        * *limit* - The number of counts to return, default is all

        **RETURNS**

        * List * - A list of tuples of (frequency, value)

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> counts = img.getGrayHistogramCounts()
        >>> counts[0] #the most dominate pixel color tuple of frequency and value
        >>> counts[1][1] #the second most dominate pixel color value
        '''

        hist = self.histogram(bins)
        vals = [(e,h) for h,e in enumerate(hist)]
        vals.sort()
        vals.reverse()

        if limit == -1:
            limit = bins

        return vals[:limit]

    def grayPeaks(self, bins = 255, delta = 0, lookahead = 15):
        """
        **SUMMARY**

        Takes the histogram of a grayscale image, and returns the peak
        grayscale intensity values.

        The bins parameter can be used to lump grays together, by default it is 
        set to 255

        Returns a list of tuples, each tuple contains the grayscale intensity,
        and the fraction of the image that has it.

        **PARAMETERS**

        * *bins* - the integer number of bins, between 1 and 255.

        * *delta* - the minimum difference betweena peak and the following points,
                    before a peak may be considered a peak.Useful to hinder the 
                    algorithm from picking up false peaks towards to end of
                    the signal.

        * *lookahead* - the distance to lookahead from a peakto determine if it is
                    an actual peak, should be an integer greater than 0. 

        **RETURNS**

        A list of (grays,fraction) tuples.

        **NOTE**

        Implemented using the techniques used in huetab()

        """

        # The bins are the no of edges bounding an histogram. 
        # Thus bins= Number of bars in histogram+1 
        # As range() function is exclusive, 
        # hence bins+2 is passed as parameter.

        y_axis, x_axis = np.histogram(self.getGrayNumpy(), bins = range(bins+2))
        x_axis = x_axis[0:bins+1]
        maxtab = []
        mintab = []
        length = len(y_axis)
        if x_axis is None:
            x_axis = range(length)

        #perform some checks
        if length != len(x_axis):
            raise ValueError, "Input vectors y_axis and x_axis must have same length"
        if lookahead < 1:
            raise ValueError, "Lookahead must be above '1' in value"
        if not (np.isscalar(delta) and delta >= 0):
            raise ValueError, "delta must be a positive number"

        #needs to be a numpy array
        y_axis = np.asarray(y_axis)

        #maxima and minima candidates are temporarily stored in
        #mx and mn respectively
        mn, mx = np.Inf, -np.Inf

        #Only detect peak if there is 'lookahead' amount of points after it
        for index, (x, y) in enumerate(zip(x_axis[:-lookahead], y_axis[:-lookahead])):
            if y > mx:
                mx = y
                mxpos = x
            if y < mn:
                mn = y
                mnpos = x

            ####look for max####
            if y < mx-delta and mx != np.Inf:
                #Maxima peak candidate found
                #look ahead in signal to ensure that this is a peak and not jitter
                if y_axis[index:index+lookahead].max() < mx:
                    maxtab.append((mxpos, mx))
                    #set algorithm to only find minima now
                    mx = np.Inf
                    mn = np.Inf

            if y > mn+delta and mn != -np.Inf:
                #Minima peak candidate found
                #look ahead in signal to ensure that this is a peak and not jitter
                if y_axis[index:index+lookahead].min() > mn:
                    mintab.append((mnpos, mn))
                    #set algorithm to only find maxima now
                    mn = -np.Inf
                    mx = -np.Inf
                
        retVal = []
        for intensity, pixelcount in maxtab:
            retVal.append((intensity, pixelcount / float(self.width * self.height)))
        return retVal

    def tvDenoising(self, gray=False, weight=50, eps=0.0002, max_iter=200, resize=1):
        """
        **SUMMARY**

        Performs Total Variation Denoising, this filter tries to minimize the
        total-variation of the image. 

        see : http://en.wikipedia.org/wiki/Total_variation_denoising

        **Parameters**

        * *gray* - Boolean value which identifies the colorspace of
            the input image. If set to True, filter uses gray scale values,
            otherwise colorspace is used.

        * *weight* - Denoising weight, it controls the extent of denoising.

        * *eps* - Stopping criteria for the algorithm. If the relative difference
            of the cost function becomes less than this value, the algorithm stops.

        * *max_iter* - Determines the maximum number of iterations the algorithm
            goes through for optimizing.

        * *resize* - Parameter to scale up/down the image. If set to
            1 filter is applied on the original image. This parameter is
            mostly to speed up the filter.

        **NOTE**
        
        This function requires Scikit-image library to be installed!
        To install scikit-image library run::
         
            sudo pip install -U scikit-image

        Read More: http://scikit-image.org/
        
        """

        try:
            from skimage.filter import denoise_tv_chambolle
        except ImportError:
            logger.warn('Scikit-image Library not installed!')
            return None
        
        img = self.copy()
        
        if resize <= 0:
            print 'Enter a valid resize value'
            return None

        if resize != 1:
            img = img.resize(int(img.width*resize),int(img.height*resize))

        if gray is True:
            img = img.getGrayNumpy()
            multichannel = False
        elif gray is False:
            img = img.getNumpy()
            multichannel = True
        else:
            warnings.warn('gray value not valid')
            return None

        denoise_mat = denoise_tv_chambolle(img,weight,eps,max_iter,multichannel)
        retVal = img * denoise_mat

        retVal = Image(retVal)
        if resize != 1:
            return retVal.resize(int(retVal.width/resize),int(retVal.width/resize))
        else:
            return retVal
      
    def motionBlur(self,intensity=15, direction='NW'):
        """
        **SUMMARY**

        Performs the motion blur of an Image. Uses different filters to find out
        the motion blur in different directions.

        see : https://en.wikipedia.org/wiki/Motion_blur

        **Parameters**

        * *intensity* - The intensity of the motion blur effect. Basically defines
            the size of the filter used in the process. It has to be an integer.
            0 intensity implies no blurring.

        * *direction* - The direction of the motion. It is a string taking values
            left, right, up, down as well as N, S, E, W for north, south, east, west 
            and NW, NE, SW, SE for northwest and so on. 
            default is NW

        **RETURNS**

        An image with the specified motion blur filter applied.

        **EXAMPLE**
        >>> i = Image ('lenna')
        >>> mb = i.motionBlur()
        >>> mb.show()
        
        """
        mid = int(intensity/2)
        tmp = np.identity(intensity)
        
        if intensity == 0:
            warnings.warn("0 intensity means no blurring")
            return self

        elif intensity % 2 is 0:
            div=mid
            for i in range(mid, intensity-1):
                tmp[i][i] = 0
        else:
            div=mid+1
            for i in range(mid+1, intensity-1):
                tmp[i][i]=0

        if direction == 'right' or direction.upper() == 'E':
            kernel = np.concatenate((np.zeros((1,mid)),np.ones((1,mid+1))),axis=1)
        elif direction == 'left' or direction.upper() == 'W':
            kernel = np.concatenate((np.ones((1,mid+1)),np.zeros((1,mid))),axis=1)
        elif direction == 'up' or direction.upper() == 'N':
            kernel = np.concatenate((np.ones((1+mid,1)),np.zeros((mid,1))),axis=0)
        elif direction == 'down' or direction.upper() == 'S':
            kernel = np.concatenate((np.zeros((mid,1)),np.ones((mid+1,1))),axis=0)
        elif direction.upper() == 'NW':
            kernel = tmp
        elif direction.upper() == 'NE':
            kernel = np.fliplr(tmp)
        elif direction.upper() == 'SW':
            kernel = np.flipud(tmp)
        elif direction.upper() == 'SE':
            kernel = np.flipud(np.fliplr(tmp))
        else:
            warnings.warn("Please enter a proper direction")
            return None
        
        retval=self.convolve(kernel=kernel/div)
        return retval
        
    def recognizeFace(self, recognizer=None):
        """
        **SUMMARY**

        Find faces in the image using FaceRecognizer and predict their class.

        **PARAMETERS**

        * *recognizer*   - Trained FaceRecognizer object

        **EXAMPLES**

        >>> cam = Camera()
        >>> img = cam.getImage()
        >>> recognizer = FaceRecognizer()
        >>> recognizer.load("training.xml")
        >>> print img.recognizeFace(recognizer)
        """
        try:
            import cv2
            if not hasattr(cv2, "createFisherFaceRecognizer"):
                warnings.warn("OpenCV >= 2.4.4 required to use this.")
                return None
        except ImportError:
            warnings.warn("OpenCV >= 2.4.4 required to use this.")
            return None

        if not isinstance(recognizer, FaceRecognizer):
            warnings.warn("SimpleCV.Features.FaceRecognizer object required.")
            return None

        w, h = recognizer.imageSize
        label = recognizer.predict(self.resize(w, h))
        return label

    def findAndRecognizeFaces(self, recognizer, cascade=None):
        """
        **SUMMARY**

        Predict the class of the face in the image using FaceRecognizer.

        **PARAMETERS**

        * *recognizer*  - Trained FaceRecognizer object

        * *cascade*     -haarcascade which would identify the face
                         in the image.

        **EXAMPLES**

        >>> cam = Camera()
        >>> img = cam.getImage()
        >>> recognizer = FaceRecognizer()
        >>> recognizer.load("training.xml")
        >>> feat = img.findAndRecognizeFaces(recognizer, "face.xml")
        >>> for feature, label, confidence in feat:
            ... i = feature.crop()
            ... i.drawText(str(label))
            ... i.show()
        """
        try:
            import cv2
            if not hasattr(cv2, "createFisherFaceRecognizer"):
                warnings.warn("OpenCV >= 2.4.4 required to use this.")
                return None
        except ImportError:
            warnings.warn("OpenCV >= 2.4.4 required to use this.")
            return None

        if not isinstance(recognizer, FaceRecognizer):
            warnings.warn("SimpleCV.Features.FaceRecognizer object required.")
            return None

        if not cascade:
            cascade = "/".join([LAUNCH_PATH,"/Features/HaarCascades/face.xml"])

        faces = self.findHaarFeatures(cascade)
        if not faces:
            warnings.warn("Faces not found in the image.")
            return None

        retVal = []
        for face in faces:
            label, confidence = face.crop().recognizeFace(recognizer)
            retVal.append([face, label, confidence])
        return retVal

    def channelMixer(self, channel = 'r', weight = (100,100,100)):
        """
        **SUMMARY**

        Mixes channel of an RGB image based on the weights provided. The output is given at the 
        channel provided in the parameters. Basically alters the value of one channelg of an RGB
        image based in the values of other channels and itself. If the image is not RGB then first
        converts the image to RGB and then mixes channel

        **PARAMETERS**

        * *channel* - The output channel in which the values are to be replaced. 
        It can have either 'r' or 'g' or 'b'

        * *weight* - The weight of each channel in calculation of the mixed channel.
        It is a tuple having 3 values mentioning the percentage of the value of the 
        channels, from -200% to 200%

        **RETURNS**

        A SimpleCV RGB Image with the provided channel replaced with the mixed channel.

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> img2 = img.channelMixer()
        >>> Img3 = img.channelMixer(channel = 'g', weights = (3,2,1))

        **NOTE**

        Read more at http://docs.gimp.org/en/plug-in-colors-channel-mixer.html

        """
        r, g, b = self.splitChannels()
        if weight[0] > 200 or weight[1] > 200 or weight[2] >= 200:
            if weight[0] <-200 or weight[1] < -200 or weight[2] < -200:
                warnings.warn('Value of weights can be from -200 to 200%')
                return None

        weight = map(float,weight)
        channel = channel.lower()
        if channel == 'r':
            r = r*(weight[0]/100.0) + g*(weight[1]/100.0) + b*(weight[2]/100.0)
        elif channel == 'g':
            g = r*(weight[0]/100.0) + g*(weight[1]/100.0) + b*(weight[2]/100.0)
        elif channel == 'b':
            b = r*(weight[0]/100.0) + g*(weight[1]/100.0) + b*(weight[2]/100.0)
        else:
            warnings.warn('Please enter a valid channel(r/g/b)')
            return None

        retVal = self.mergeChannels(r = r, g = g, b = b)
        return retVal

    def prewitt(self):
        """
        **SUMMARY**

        Prewitt operator for edge detection

        **PARAMETERS**

        None

        **RETURNS**

        Image with prewitt opeartor applied on it

        **EXAMPLE**

        >>> img = Image("lenna")
        >>> p = img.prewitt()
        >>> p.show()

        **NOTES**

        Read more at: http://en.wikipedia.org/wiki/Prewitt_operator
        
        """
        img = self.copy()
        grayimg = img.grayscale()
        gx = [[1,1,1],[0,0,0],[-1,-1,-1]]
        gy = [[-1,0,1],[-1,0,1],[-1,0,1]]
        grayx = grayimg.convolve(gx)
        grayy = grayimg.convolve(gy)
        grayxnp = np.uint64(grayx.getGrayNumpy())
        grayynp = np.uint64(grayy.getGrayNumpy())
        retVal = Image(np.sqrt(grayxnp**2+grayynp**2))
        return retVal

    def edgeSnap(self,pointList,step = 1):
        """
        **SUMMARY**

        Given a List of points finds edges closet to the line joining two 
        successive points, edges are returned as a FeatureSet of
        Lines.

        Note : Image must be binary, it is assumed that prior conversion is done

        **Parameters**

       * *pointList* - List of points to be checked for nearby edges.

        * *step* - Number of points to skip if no edge is found in vicinity.
                   Keep this small if you want to sharply follow a curve

        **RETURNS**

        * FeatureSet * - A FeatureSet of Lines
        
        **EXAMPLE**

        >>> image = Image("logo").edges()
        >>> edgeLines = image.edgeSnap([(50,50),(230,200)])
        >>> edgeLines.draw(color = Color.YELLOW,width = 3)
        """

        imgArray = self.getGrayNumpy()
        c1 = np.count_nonzero(imgArray )
        c2 = np.count_nonzero(imgArray - 255)
        
        #checking that all values are 0 and 255
        if( c1 + c2 != imgArray.size):
            raise ValueError,"Image must be binary"

        if(len(pointList) < 2 ):
            return None

        finalList = [pointList[0]]
        featureSet  = FeatureSet()
        last = pointList[0]
        for point in pointList[1:None]:
            finalList += self._edgeSnap2(last,point,step)
            last = point
            
        last = finalList[0]
        for point in finalList:
            featureSet.append(Line(self,(last,point)))
            last = point
        return featureSet

    def _edgeSnap2(self,start,end,step):
        """
        **SUMMARY**

        Given a two points returns a list of edge points closet to the line joining the points 
        Point is a tuple of two numbers

        Note : Image must be binary

        **Parameters**

        * *start* - First Point

        * *end* - Second Point

        * *step* - Number of points to skip if no edge is found in vicinity
                   Keep this low to detect sharp curves

        **RETURNS**

        * List * - A list of tuples , each tuple contains (x,y) values
        
        """


        edgeMap = np.copy(self.getGrayNumpy())

        #Size of the box around a point which is checked for edges.
        box = step*4

        xmin = min(start[0],end[0])
        xmax = max(start[0],end[0])
        ymin = min(start[1],end[1])
        ymax = max(start[1],end[1])

        line = self.bresenham_line(start,end)

        #List of Edge Points.
        finalList = []
        i = 0
        
        #Closest any point has ever come to the end point
        overallMinDist = None

        while  i < len(line) :
            
            x,y = line[i]
            
            #Get the matrix of points fromx around current point.
            region = edgeMap[x-box:x+box,y-box:y+box]

            #Condition at the boundary of the image
            if(region.shape[0] == 0 or region.shape[1] == 0):
                i += step
                continue

            #Index of all Edge points
            indexList = np.argwhere(region>0)
            if (indexList.size > 0):
                
                #Center the coordinates around the point
                indexList -= box
                minDist = None

                # Incase multiple edge points exist, choose the one closest
                # to the end point
                for ix,iy in indexList:
                    dist = math.hypot(x+ix-end[0],iy+y-end[1])
                    if(minDist ==None or dist < minDist ):
                        dx,dy = ix,iy
                        minDist = dist

                # The distance of the new point is compared with the least 
                # distance computed till now, the point is rejected if it's
                # comparitively more. This is done so that edge points don't
                # wrap around a curve instead of heading towards the end point
                if(overallMinDist!= None and minDist > overallMinDist*1.1):
                    i+=step
                    continue

                if( overallMinDist == None or minDist < overallMinDist ):
                    overallMinDist = minDist

                # Reset the points in the box so that they are not detected
                # during the next iteration.
                edgeMap[x-box:x+box,y-box:y+box] = 0

                # Keep all the points in the bounding box
                if( xmin <= x+dx <= xmax and ymin <= y+dx <=ymax):
                    #Add the point to list and redefine the line
                    line =[(x+dx,y+dy)] + self.bresenham_line((x+dx, y+dy), end)
                    finalList += [(x+dx,y+dy)]
                
                    i = 0
            
            i += step 
        finalList += [end]
        return finalList

    def motionBlur(self,intensity=15, angle = 0):
        """
        **SUMMARY**

        Performs the motion blur of an Image given the intensity and angle

        see : https://en.wikipedia.org/wiki/Motion_blur

        **Parameters**

        * *intensity* - The intensity of the motion blur effect. Governs the 
            size of the kernel used in convolution

        * *angle* - Angle in degrees at which motion blur will occur. Positive
            is Clockwise and negative is Anti-Clockwise. 0 blurs from left to 
            right
            

        **RETURNS**

        An image with the specified motion blur applied.

        **EXAMPLE**
        >>> img = Image ('lenna')
        >>> blur = img.motionBlur(40,45)
        >>> blur.show()
        
        """
        
        intensity = int(intensity)

        if(intensity <= 1):
            logger.warning('power less than 1 will result in no change')
            return self
        
        kernel = np.zeros((intensity,intensity))
        
        rad = math.radians(angle)
        x1,y1 = intensity/2,intensity/2
        
        x2 = int(x1-(intensity-1)/2*math.sin(rad))
        y2 = int(y1 -(intensity-1)/2*math.cos(rad))
        
        line = self.bresenham_line((x1,y1),(x2,y2))
        
        x = [p[0] for p in line]
        y = [p[1] for p in line]
        
        kernel[x,y] = 1
        kernel = kernel/len(line)
        return self.convolve(kernel = kernel)

    def getLightness(self):
        """
        **SUMMARY**

        This method converts the given RGB image to grayscale using the
        Lightness method.

        **Parameters**
        
        None

        **RETURNS**

        A GrayScale image with values according to the Lightness method

        **EXAMPLE**
        >>> img = Image ('lenna')
        >>> out = img.getLightness()
        >>> out.show()
        
        **NOTES**

        Algorithm used: value = (MAX(R,G,B) + MIN(R,G,B))/2

        """
        if( self._colorSpace == ColorSpace.BGR or
                self._colorSpace == ColorSpace.UNKNOWN ):
            imgMat = np.array(self.getNumpyCv2(),dtype=np.int)
            retVal = np.array((np.max(imgMat,2) + np.min(imgMat,2))/2,dtype=np.uint8)
        else:
            logger.warnings('Input a RGB image')
            return None

        return Image(retVal,cv2image=True)

    def getLuminosity(self):
        """
        **SUMMARY**

        This method converts the given RGB image to grayscale using the
        Luminosity method.

        **Parameters**
        
        None

        **RETURNS**

        A GrayScale image with values according to the Luminosity method

        **EXAMPLE**
        >>> img = Image ('lenna')
        >>> out = img.getLuminosity()
        >>> out.show()
        
        **NOTES**

        Algorithm used: value =  0.21 R + 0.71 G + 0.07 B

        """
        if( self._colorSpace == ColorSpace.BGR or
                self._colorSpace == ColorSpace.UNKNOWN ):
            imgMat = np.array(self.getNumpyCv2(),dtype=np.int)
            retVal = np.array(np.average(imgMat,2,(0.07,0.71,0.21)),dtype=np.uint8)
        else:
            logger.warnings('Input a RGB image')
            return None

        return Image(retVal,cv2image=True)

    def getAverage(self):
        """
        **SUMMARY**

        This method converts the given RGB image to grayscale by averaging out
        the R,G,B values.

        **Parameters**
        
        None

        **RETURNS**

        A GrayScale image with values according to the Average method

        **EXAMPLE**
        >>> img = Image ('lenna')
        >>> out = img.getAverage()
        >>> out.show()
        
        **NOTES**

        Algorithm used: value =  (R+G+B)/3

        """
        if( self._colorSpace == ColorSpace.BGR or
                self._colorSpace == ColorSpace.UNKNOWN ):
            imgMat = np.array(self.getNumpyCv2(),dtype=np.int)
            retVal = np.array(imgMat.mean(2),dtype=np.uint8)
        else:
            logger.warnings('Input a RGB image')
            return None

        return Image(retVal,cv2image=True)
    
    def smartRotate(self,bins=18,point = [-1,-1],auto = True,threshold=80,minLength=30,maxGap=10,t1=150,t2=200,fixed = True):
        """
        **SUMMARY**

        Attempts to rotate the image so that the most significant lines are 
        approximately parellel to horizontal or vertical edges.

        **Parameters**
        
        
        * *bins* - The number of bins the lines will be grouped into.
        
        * *point* - the point about which to rotate, refer :py:meth:`rotate`
        
        * *auto* - If true point will be computed to the mean of centers of all
            the lines in the selected bin. If auto is True, value of point is
            ignored
            
        * *threshold* - which determines the minimum "strength" of the line
            refer :py:meth:`findLines` for details.
            
        * *minLength* - how many pixels long the line must be to be returned,
            refer :py:meth:`findLines` for details.
            
        * *maxGap* - how much gap is allowed between line segments to consider 
            them the same line .refer to :py:meth:`findLines` for details.
            
        * *t1* - thresholds used in the edge detection step, 
            refer to :py:meth:`_getEdgeMap` for details.
            
        * *t2* - thresholds used in the edge detection step, 
            refer to :py:meth:`_getEdgeMap` for details.
            
        * *fixed* - if fixed is true,keep the original image dimensions, 
            otherwise scale the image to fit the rotation , refer to 
            :py:meth:`rotate`

        **RETURNS**

        A rotated image

        **EXAMPLE**
        >>> i = Image ('image.jpg')
        >>> i.smartRotate().show()

        """
        lines = self.findLines(threshold, minLength, maxGap, t1,t2)
        
        if(len(lines) == 0):
            logger.warning("No lines found in the image")
            return self

        # Initialize empty bins
        binn = [[] for i in range(bins)]
        
        #Convert angle to bin number
        conv = lambda x:int(x+90)/bins

        #Adding lines to bins
        [ binn[conv(line.angle())].append(line) for line in lines ]

        #computing histogram, value of each column is total length of all lines
        #in the bin
        hist = [ sum([line.length() for line in lines]) for lines in binn]
        
        #The maximum histogram
        index = np.argmax(np.array(hist))
        
        #Good ol weighted mean, for the selected bin
        avg = sum([line.angle()*line.length() for line in binn[index]])/sum([line.length() for line in binn[index] ])

        #Mean of centers of all lines in selected bin
        if(auto ):
            x = sum([line.end_points[0][0] + line.end_points[1][0] for line in binn[index]])/2/len(binn[index])
            y = sum([line.end_points[0][1] + line.end_points[1][1] for line in binn[index]])/2/len(binn[index])
            point = [x,y]

        #Determine whether to rotate the lines to vertical or horizontal 
        if (-45 <= avg <= 45):
            return self.rotate(avg,fixed = fixed,point = point)
        elif (avg > 45):
            return self.rotate(avg-90,fixed = fixed,point = point)
        else:
            return self.rotate(avg+90,fixed = fixed,point = point)
        #Congratulations !! You did a smart thing

    def normalize(self, newMin = 0, newMax = 255, minCut = 2, maxCut = 98):
        """
        **SUMMARY**

        Performs image normalization and yeilds a linearly normalized gray image.
        Also known as contrast strestching.

        see : http://en.wikipedia.org/wiki/Normalization_(image_processing)

        **Parameters**

        * *newMin* - The minimum of the new range over which the image is normalized

        * *newMax* - The maximum of the new range over which the image is normalized

        * *minCut* - A number between 0 to 100. The threshold percentage for the 
        current minimum value selection. This helps us to avoid the effect of outlying
        pixel with either very low value

        * *maxCut* - A number between 0 to 100. The threshold percentage for the 
        current minimum value selection. This helps us to avoid the effect of outlying
        pixel with either very low value          

        **RETURNS**

        A normalized grayscale image.

        **EXAMPLE**
        >>> img = Image ('lenna')
        >>> norm = i.normalize()
        >>> norm.show()

        """
        if newMin < 0 or newMax >255:
            warnings.warn("newMin and newMax can vary from 0-255")
            return None
        if newMax < newMin:
            warnings.warn("newMin should be less than newMax")
            return None
        if minCut > 100 or maxCut > 100:
            warnings.warn("minCut and maxCut")
            return None
        #avoiding the effect of odd pixels
        try:
            hist = self.getGrayHistogramCounts()
            freq, val = zip(*hist)
            maxfreq = (freq[0]-freq[-1])* maxCut/100.0
            minfreq = (freq[0]-freq[-1])* minCut/100.0
            closestMatch = lambda a,l:min(l, key=lambda x:abs(x-a))
            maxval = closestMatch(maxfreq, val)
            minval = closestMatch(minfreq, val)
            retVal = (self.grayscale()-minval)*((newMax-newMin)/float(maxval-minval))+ newMin
        #catching zero division in case there are very less intensities present
        #Normalizing based on absolute max and min intensities present
        except ZeroDivisionError:
            maxval = self.maxValue()
            minval = self.minValue()
            retVal = (self.grayscale()-minval)*((newMax-newMin)/float(maxval-minval))+ newMin
        #catching the case where there is only one intensity throughout
        except:
            warnings.warn("All pixels of the image have only one intensity value")
            return None
        return retVal

    def getNormalizedHueHistogram(self,roi=None):
        """
        **SUMMARY**

        This method generates a normalized hue histogram for the image
        or the ROI within the image. The hue histogram is a 2D hue/saturation
        numpy array histogram with a shape of 180x256. This histogram can
        be used for histogram back projection. 

        **PARAMETERS**

        * *roi* - Anything that can be cajoled into being an ROI feature
          including a tuple of (x,y,w,h), a list of points, or another feature.

        **RETURNS**

        A normalized 180x256 numpy array that is the hue histogram.

        **EXAMPLE**

        >>> img = Image('lenna')
        >>> roi = (0,0,100,100)
        >>> hist = img.getNormalizedHueHistogram(roi)

        **SEE ALSO**

        ImageClass.backProjectHueHistogram()
        ImageClass.findBlobsFromHueHistogram()
        
        """
        try:
            import cv2
        except ImportError:
            warnings.warn("OpenCV >= 2.3 required to use this.")
            return None

        from SimpleCV.Features import ROI
        if( roi ): # roi is anything that can be taken to be an roi
            roi = ROI(roi,self)
            hsv = roi.crop().toHSV().getNumpyCv2()
        else: 
            hsv = self.toHSV().getNumpyCv2()
        hist = cv2.calcHist([hsv],[0,1],None,[180,256],[0,180,0,256])
        cv2.normalize(hist,hist,0,255,cv2.NORM_MINMAX)
        return hist

    def backProjectHueHistogram(self,model,smooth=True,fullColor=False,threshold=None):
        """
        **SUMMARY**

        This method performs hue histogram back projection on the image. This is a very
        quick and easy way of matching objects based on color. Given a hue histogram
        taken from another image or an roi within the image we attempt to find all
        pixels that are similar to the colors inside the histogram. The result can
        either be a grayscale image that shows the matches or a color image.


        **PARAMETERS**

        * *model* - The histogram to use for pack projection. This can either be
          a histogram, anything that can be converted into an ROI for the image (like
          an x,y,w,h tuple or a feature, or another image.
        * *smooth* - A bool, True means apply a smoothing operation after doing the
          back project to improve the results.
        * *fullColor* - return the results as a color image where pixels included
          in the back projection are rendered as their source colro.
        * *threshold* - If this value is not None, we apply a threshold to the
          result of back projection to yield a binary image. Valid values are from
          1 to 255.

        **RETURNS**

        A SimpleCV Image rendered according to the parameters provided.

        **EXAMPLE**

        >>>> img = Image('lenna')
        >>>> hist = img.getNormalizedHueHistogram((0,0,50,50)) # generate a hist
        >>>> a = img.backProjectHueHistogram(hist)
        >>>> b = img.backProjectHueHistogram((0,0,50,50) # same result
        >>>> c = img.backProjectHueHistogram(Image('lyle'))

        **SEE ALSO**
        ImageClass.getNormalizedHueHistogram()
        ImageClass.findBlobsFromHueHistogram()
        
        """
        try:
            import cv2
        except ImportError:
            warnings.warn("OpenCV >= 2.3 required to use this.")
            return None
        
        if( model is None ):
            warnings.warn('Backproject requires a model')
            return None
        # this is the easier test, try to cajole model into ROI
        if( isinstance(model,Image) ):
            model = model.getNormalizedHueHistogram()
        if(not isinstance(model,np.ndarray) or  model.shape != (180,256) ):
            model = self.getNormalizedHueHistogram(model)
        if( isinstance(model,np.ndarray) and model.shape == (180,256) ):
            hsv = self.toHSV().getNumpyCv2()
            dst = cv2.calcBackProject([hsv],[0,1],model,[0,180,0,256],1)
            if smooth:
                disc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5))
                cv2.filter2D(dst,-1,disc,dst)
            result = Image(dst,cv2image=True)
            result = result.toBGR()
            if( threshold ):
                result = result.threshold(threshold)
            if( fullColor ):
                temp = Image((self.width,self.height))
                result = temp.blit(self,alphaMask=result)
            return result
        else:
            warnings.warn('Backproject model does not appear to be valid')
            return None

        
    def findBlobsFromHueHistogram(self,model,threshold=1,smooth=True,minsize=10,maxsize=None):
        """
        **SUMMARY**

        This method performs hue histogram back projection on the image and uses
        the results to generate a FeatureSet of blob objects. This is a very
        quick and easy way of matching objects based on color. Given a hue histogram
        taken from another image or an roi within the image we attempt to find all
        pixels that are similar to the colors inside the histogram. 


        **PARAMETERS**

        * *model* - The histogram to use for pack projection. This can either be
          a histogram, anything that can be converted into an ROI for the image (like
          an x,y,w,h tuple or a feature, or another image.
        * *smooth* - A bool, True means apply a smoothing operation after doing the
          back project to improve the results.
        * *threshold* - If this value is not None, we apply a threshold to the
          result of back projection to yield a binary image. Valid values are from
          1 to 255.
        * *minsize* - the minimum blob size in pixels.
        * *maxsize* - the maximum blob size in pixels.

        **RETURNS**

        A FeatureSet of blob objects or None if no blobs are found.

        **EXAMPLE**

        >>>> img = Image('lenna')
        >>>> hist = img.getNormalizedHueHistogram((0,0,50,50)) # generate a hist
        >>>> blobs = img.findBlobsFromHueHistogram(hist)
        >>>> blobs.show()

        **SEE ALSO**
        
        ImageClass.getNormalizedHueHistogram()
        ImageClass.backProjectHueHistogram()
        
        """
        newMask = self.backProjectHueHistogram(model,smooth,fullColor=False,threshold=threshold)
        return self.findBlobsFromMask(newMask,minsize=minsize,maxsize=maxsize)        

    def filter(self, flt, grayscale=False):
        """
        **SUMMARY**

        This function allows you to apply an arbitrary filter to the DFT of an image.
        This filter takes in a gray scale image, whiter values are kept and black values
        are rejected. In the DFT image, the lower frequency values are in the corners
        of the image, while the higher frequency components are in the center. For example,
        a low pass filter has white squares in the corners and is black everywhere else.

        **PARAMETERS**

        * *flt* - A DFT filter

        * *grayscale* - if this value is True we perfrom the operation on the DFT of the gray
          version of the image and the result is gray image. If grayscale is true
          we perform the operation on each channel and the recombine them to create
          the result.

        **RETURNS**

        A SimpleCV image after applying the filter.

        **EXAMPLE**

        >>>  filter = DFT.createGaussianFilter()
        >>>  myImage = Image("MyImage.png")
        >>>  result = myImage.filter(filter)
        >>>  result.show()
        """
        filteredimage = flt.applyFilter(self, grayscale)
        return filteredimage

from SimpleCV.Features import FeatureSet, Feature, Barcode, Corner, HaarFeature, Line, Chessboard, TemplateMatch, BlobMaker, Circle, KeyPoint, Motion, KeypointMatch, FaceRecognizer
from SimpleCV.Tracking import camshiftTracker, lkTracker, surfTracker, mfTracker, TrackSet
from SimpleCV.Stream import JpegStreamer
from SimpleCV.Font import *
from SimpleCV.DrawingLayer import *
from SimpleCV.DFT import DFT

########NEW FILE########
__FILENAME__ = LineScan

from SimpleCV.base import *
import scipy.signal as sps
import scipy.optimize as spo
import numpy as np
import copy, operator


class LineScan(list):
    """
    **SUMMARY**

    A line scan is a one dimensional signal pulled from the intensity
    of a series of a pixels in an image. LineScan allows you to do a series
    of operations just like on an image class object. You can also treat the
    line scan as a python list object. A linescan object is automatically
    generated by calling ImageClass.getLineScan on an image. You can also
    roll your own by declaring a LineScan object and passing the constructor
    a 1xN list of values.

    **EXAMPLE**

    >>>> import matplotlib.pyplot as plt
    >>>> img = Image('lenna')
    >>>> s = img.getLineScan(y=128)
    >>>> ss = s.smooth()
    >>>> plt.plot(s)
    >>>> plt.plot(ss)
    >>>> plt.show()
    """
    pointLoc = None
    image = None

    def __init__(self, args, **kwargs):
        if isinstance(args, np.ndarray):
            args = args.tolist()
        list.__init__(self,args)
        self.image = None
        self.pt1 = None
        self.pt2 = None
        self.row = None
        self.col = None
        self.channel = -1
        for key in kwargs:
            if key == 'pointLocs':
                if kwargs[key] is not None:
                    self.pointLoc = kwargs[key]
            if key == 'image':
                if kwargs[key] is not None:
                    self.img = kwargs[key]
            if key == 'pt1':
                if kwargs[key] is not None:
                    self.pt1 = kwargs[key]
            if key == 'pt2':
                if kwargs[key] is not None:
                    self.pt2 = kwargs[key]
            if key == "x":
                if kwargs[key] is not None:
                    self.col = kwargs[key]
            if key == "y":
                if kwargs[key] is not None:
                    self.row = kwargs[key]
            if key == "channel":
                if kwargs[key] is not None:
                    self.channel = kwargs[key]
                    
        if(self.pointLoc is None):
            self.pointLoc = zip(range(0,len(self)),range(0,len(self)))

    def __getitem__(self,key):
        """
        **SUMMARY**

        Returns a LineScan when sliced. Previously used to
        return list. Now it is possible to use LineScanm member
        functions on sub-lists

        """
        if type(key) is types.SliceType: #Or can use 'try:' for speed
            return LineScan(list.__getitem__(self, key))
        else:
            return list.__getitem__(self,key)

    def __getslice__(self, i, j):
        """
        Deprecated since python 2.0, now using __getitem__
        """
        return self.__getitem__(slice(i,j))

    def __sub__(self,other):
        
        if len(self) == len(other):
            retVal = LineScan(map(operator.sub,self,other))
        else:
            print 'Size mismatch'
            return None
        retVal._update(self)
        return retVal

    def __add__(self,other):
        
        if len(self) == len(other):
            retVal = LineScan(map(operator.add,self,other))
        else:
            print 'Size mismatch'
            return None
        retVal._update(self)
        return retVal

    def __mul__(self,other):

        if len(self) == len(other):
            retVal = LineScan(map(operator.mul,self,other))
        else:
            print 'Size mismatch'
            return None

        retVal._update(self)
        return retVal

    def __div__(self,other):

        if len(self) == len(other):
            try:
                retVal = LineScan(map(operator.div,self,other))
            except ZeroDivisionError:
                print 'Second LineScan contains zeros'
                return None
        else:
            print 'Size mismatch'
            return None

        retVal._update(self)
        return retVal

    def _update(self, linescan):
        """
        ** SUMMARY**

        Updates LineScan's Instance Objects.

        """
        self.image = linescan.image
        self.pt1 = linescan.pt1
        self.pt2 = linescan.pt2
        self.row = linescan.row
        self.col = linescan.col
        self.channel = linescan.channel
        self.pointLoc = linescan.pointLoc


    def smooth(self,degree=3):
        """
        **SUMMARY**

        Perform a Gasusian simple smoothing operation on the signal.

        **PARAMETERS**

        * *degree* - The degree of the fitting function. Higher degree means more smoothing.

        **RETURNS**

        A smoothed LineScan object.

        **EXAMPLE**

        >>>> import matplotlib.pyplot as plt
        >>>> img = Image('lenna')
        >>>> sl = img.getLineScan(y=128)
        >>>> plt.plot(sl)
        >>>> plt.plot(sl.smooth(7))
        >>>> plt.show()

        **NOTES**
        Cribbed from http://www.swharden.com/blog/2008-11-17-linear-data-smoothing-in-python/
        """
        window=degree*2-1
        weight=np.array([1.0]*window)
        weightGauss=[]
        for i in range(window):
            i=i-degree+1
            frac=i/float(window)
            gauss=1/(np.exp((4*(frac))**2))
            weightGauss.append(gauss)
        weight=np.array(weightGauss)*weight
        smoothed=[0.0]*(len(self)-window)
        for i in range(len(smoothed)):
            smoothed[i]=sum(np.array(self[i:i+window])*weight)/sum(weight)
        # recenter the signal so it sits nicely on top of the old
        front = self[0:(degree-1)]
        front += smoothed
        front += self[-1*degree:]
        retVal = LineScan(front,image=self.image,pointLoc=self.pointLoc,pt1=self.pt1,pt2=self.pt2)
        retVal._update(self)
        return retVal

    def normalize(self):
        """
        **SUMMARY**

        Normalize the signal so the maximum value is scaled to one.

        **RETURNS**

        A normalized scanline object.

        **EXAMPLE**

        >>>> import matplotlib.pyplot as plt
        >>>> img = Image('lenna')
        >>>> sl = img.getLineScan(y=128)
        >>>> plt.plot(sl)
        >>>> plt.plot(sl.normalize())
        >>>> plt.show()

        """
        temp = np.array(self, dtype='float32')
        temp = temp / np.max(temp)
        retVal = LineScan(list(temp[:]),image=self.image,pointLoc=self.pointLoc,pt1=self.pt1,pt2=self.pt2)
        retVal._update(self)
        return retVal

    def scale(self,value_range=(0,1)):
        """
        **SUMMARY**

        Scale the signal so the maximum and minimum values are
        all scaled to the values in value_range. This is handy
        if you want to compare the shape of two signals that
        are scaled to different ranges.

        **PARAMETERS**

        * *value_range* - A tuple that provides the lower and upper bounds
                          for the output signal.

        **RETURNS**

        A scaled LineScan object.

        **EXAMPLE**

        >>>> import matplotlib.pyplot as plt
        >>>> img = Image('lenna')
        >>>> sl = img.getLineScan(y=128)
        >>>> plt.plot(sl)
        >>>> plt.plot(sl.scale(value_range(0,255)))
        >>>> plt.show()
    
        **SEE ALSO**

        """
        temp = np.array(self, dtype='float32')
        vmax = np.max(temp)
        vmin = np.min(temp)
        a = np.min(value_range)
        b = np.max(value_range)
        temp = (((b-a)/(vmax-vmin))*(temp-vmin))+a
        retVal = LineScan(list(temp[:]),image=self.image,pointLoc=self.pointLoc,pt1=self.pt1,pt2=self.pt2)
        retVal._update(self)
        return retVal

    def minima(self):
        """
        **SUMMARY**

        The function the global minima in the line scan.

        **RETURNS**

        Returns a list of tuples of the format:
        (LineScanIndex,MinimaValue,(image_position_x,image_position_y))

        **EXAMPLE**

        >>>> import matplotlib.pyplot as plt
        >>>> img = Image('lenna')
        >>>> sl = img.getLineScan(y=128)
        >>>> minima = sl.smooth().minima()
        >>>> plt.plot(sl)
        >>>> for m in minima:
        >>>>    plt.plot(m[0],m[1],'ro')
        >>>> plt.show()

        """
        # all of these functions should return
        # value, index, pixel coordinate
        # [(index,value,(pix_x,pix_y))...]
        minvalue = np.min(self)
        idxs = np.where(np.array(self)==minvalue)[0]
        minvalue = np.ones((1,len(idxs)))*minvalue # make zipable
        minvalue = minvalue[0]
        pts = np.array(self.pointLoc)
        pts = pts[idxs]
        pts = [(p[0],p[1]) for p in pts] # un numpy this
        return zip(idxs,minvalue,pts)

    def maxima(self):
        """
        **SUMMARY**

        The function finds the global maxima in the line scan.

        **RETURNS**

        Returns a list of tuples of the format:
        (LineScanIndex,MaximaValue,(image_position_x,image_position_y))

        **EXAMPLE**

        >>>> import matplotlib.pyplot as plt
        >>>> img = Image('lenna')
        >>>> sl = img.getLineScan(y=128)
        >>>> maxima = sl.smooth().maxima()
        >>>> plt.plot(sl)
        >>>> for m in maxima:
        >>>>    plt.plot(m[0],m[1],'ro')
        >>>> plt.show()

        """

        # all of these functions should return
        # value, index, pixel coordinate
        # [(index,value,(pix_x,pix_y))...]
        maxvalue = np.max(self)
        idxs = np.where(np.array(self)==maxvalue)[0]
        maxvalue = np.ones((1,len(idxs)))*maxvalue # make zipable
        maxvalue = maxvalue[0]
        pts = np.array(self.pointLoc)
        pts = pts[idxs]
        pts = [(p[0],p[1]) for p in pts] # un numpy
        return zip(idxs,maxvalue,pts)

    def derivative(self):
        """
        **SUMMARY**

        This function finds the discrete derivative of the signal.
        The discrete derivative is simply the difference between each
        succesive samples. A good use of this function is edge detection

        **RETURNS**

        Returns the discrete derivative function as a LineScan object.

        **EXAMPLE**

        >>>> import matplotlib.pyplot as plt
        >>>> img = Image('lenna')
        >>>> sl = img.getLineScan(y=128)
        >>>> plt.plot(sl)
        >>>> plt.plot(sl.derivative())
        >>>> plt.show()

        """
        temp = np.array(self,dtype='float32')
        d = [0]
        d += list(temp[1:]-temp[0:-1])
        retVal = LineScan(d,image=self.image,pointLoc=self.pointLoc,pt1=self.pt1,pt2=self.pt2)
        #retVal.image = self.image
        #retVal.pointLoc = self.pointLoc
        return retVal

    def localMaxima(self):
        """
        **SUMMARY**

        The function finds local maxima in the line scan. Local maxima
        are defined as points that are greater than their neighbors to
        the left and to the right.

        **RETURNS**

        Returns a list of tuples of the format:
        (LineScanIndex,MaximaValue,(image_position_x,image_position_y))

        **EXAMPLE**

        >>>> import matplotlib.pyplot as plt
        >>>> img = Image('lenna')
        >>>> sl = img.getLineScan(y=128)
        >>>> maxima = sl.smooth().maxima()
        >>>> plt.plot(sl)
        >>>> for m in maxima:
        >>>>    plt.plot(m[0],m[1],'ro')
        >>>> plt.show()

        """
        temp = np.array(self)
        idx = np.r_[True, temp[1:] > temp[:-1]] & np.r_[temp[:-1] > temp[1:], True]
        idx = np.where(idx==True)[0]
        values = temp[idx]
        pts = np.array(self.pointLoc)
        pts = pts[idx]
        pts = [(p[0],p[1]) for p in pts] # un numpy
        return zip(idx,values,pts)


    def localMinima(self):
        """""
        **SUMMARY**

        The function the local minima in the line scan. Local minima
        are defined as points that are less than their neighbors to
        the left and to the right.

        **RETURNS**

        Returns a list of tuples of the format:
        (LineScanIndex,MinimaValue,(image_position_x,image_position_y))

        **EXAMPLE**

        >>>> import matplotlib.pyplot as plt
        >>>> img = Image('lenna')
        >>>> sl = img.getLineScan(y=128)
        >>>> minima = sl.smooth().minima()
        >>>> plt.plot(sl)
        >>>> for m in minima:
        >>>>    plt.plot(m[0],m[1],'ro')
        >>>> plt.show()

        """
        temp = np.array(self)
        idx = np.r_[True, temp[1:] < temp[:-1]] & np.r_[temp[:-1] < temp[1:], True]
        idx = np.where(idx==True)[0]
        values = temp[idx]
        pts = np.array(self.pointLoc)
        pts = pts[idx]
        pts = [(p[0],p[1]) for p in pts] # un numpy
        return zip(idx,values,pts)

    def resample(self,n=100):
        """
        **SUMMARY**

        Resample the signal to fit into n samples. This method is
        handy if you would like to resize multiple signals so that
        they fit together nice. Note that using n < len(LineScan)
        can cause data loss.

        **PARAMETERS**

        * *n* - The number of samples to resample to.

        **RETURNS**

        A LineScan object of length n.

        **EXAMPLE**

        >>>> import matplotlib.pyplot as plt
        >>>> img = Image('lenna')
        >>>> sl = img.getLineScan(y=128)
        >>>> plt.plot(sl)
        >>>> plt.plot(sl.resample(100))
        >>>> plt.show()

        """
        signal = sps.resample(self,n)
        pts = np.array(self.pointLoc)
        # we assume the pixel points are linear
        # so we can totally do this better manually
        x = linspace(pts[0,0],pts[-1,0],n)
        y = linspace(pts[0,1],pts[-1,1],n)
        pts = zip(x,y)
        retVal = LineScan(list(signal),image=self.image,pointLoc=self.pointLoc,pt1=self.pt1,pt2=self.pt2)
        retVal._update(self)
        return retVal


    # this needs to be moved out to a cookbook or something
    #def linear(xdata,m,b):
    #    return m*xdata+b

    # need to add polyfit too
    #http://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html
    def fitToModel(self,f,p0=None):
        """
        **SUMMARY**

        Fit the data to the provided model. This can be any arbitrary
        2D signal. Return the data of the model scaled to the data.


        **PARAMETERS**

        * *f* - a function of the form f(x_values, p0,p1, ... pn) where
                p is parameter for the model.

        * *p0* - a list of the initial guess for the model parameters.

        **RETURNS**

        A LineScan object where the fitted model data replaces the
        actual data.


        **EXAMPLE**

        >>>> def aLine(x,m,b):
        >>>>     return m*x+b
        >>>> import matplotlib.pyplot as plt
        >>>> img = Image('lenna')
        >>>> sl = img.getLineScan(y=128)
        >>>> fit = sl.fitToModel(aLine)
        >>>> plt.plot(sl)
        >>>> plt.plot(fit)
        >>>> plt.show()

        """
        yvals = np.array(self,dtype='float32')
        xvals = range(0,len(yvals),1)
        popt,pcov = spo.curve_fit(f,xvals,yvals,p0=p0)
        yvals = f(xvals,*popt)
        retVal = LineScan(list(yvals),image=self.image,pointLoc=self.pointLoc,pt1=self.pt1,pt2=self.pt2)
        retVal._update(self)
        return retVal


    def getModelParameters(self,f,p0=None):
        """
        **SUMMARY**

        Fit a model to the data and then return

        **PARAMETERS**

        * *f* - a function of the form f(x_values, p0,p1, ... pn) where
                p is parameter for the model.

        * *p0* - a list of the initial guess for the model parameters.

        **RETURNS**

        The model parameters as a list. For example if you use a line
        model y=mx+b the function returns the m and b values that fit
        the data.

        **EXAMPLE**

        >>>> def aLine(x,m,b):
        >>>>     return m*x+b
        >>>> import matplotlib.pyplot as plt
        >>>> img = Image('lenna')
        >>>> sl = img.getLineScan(y=128)
        >>>> p = sl.getModelParameters(aLine)
        >>>> print p

        """
        yvals = np.array(self,dtype='float32')
        xvals = range(0,len(yvals),1)
        popt,pcov = spo.curve_fit(f,xvals,yvals,p0=p0)
        return popt

    def convolve(self,kernel):
        """
        **SUMMARY**

        Convolve the line scan with a one dimenisional kernel stored as
        a list. This allows you to create an arbitrary filter for the signal.

        **PARAMETERS**

        * *kernel* - An Nx1 list or np.array that defines the kernel.

        **RETURNS**

        A LineScan feature with the kernel applied. We crop off
        the fiddly bits at the end and the begining of the kernel
        so everything lines up nicely.

        **EXAMPLE**

        >>>> import matplotlib.pyplot as plt
        >>>> smooth_kernel = [0.1,0.2,0.4,0.2,0.1]
        >>>> img = Image('lenna')
        >>>> sl = img.getLineScan(y=128)
        >>>> out = sl.convolve(smooth_kernel)
        >>>> plt.plot(sl)
        >>>> plt.plot(out)
        >>>> plt.show()

        **SEE ALSO**

        """
        out = np.convolve(self,np.array(kernel,dtype='float32'),'same')
        retVal = LineScan(out,image=self.image,pointLoc=self.pointLoc,pt1=self.pt1,pt2=self.pt2,channel=self.channel)
        return retVal

    def fft(self):
        """
        **SUMMARY**

        Perform a Fast Fourier Transform on the line scan and return
        the FFT output and the frequency of each value.


        **RETURNS**

        The FFT as a numpy array of irrational numbers and a one dimensional
        list of frequency values.

        **EXAMPLE**

        >>>> import matplotlib.pyplot as plt
        >>>> img = Image('lenna')
        >>>> sl = img.getLineScan(y=128)
        >>>> fft,freq = sl.fft()
        >>>> plt.plot(freq,fft.real,freq,fft.imag)
        >>>> plt.show()

        """
        signal = np.array(self,dtype='float32')
        fft = np.fft.fft(signal)
        freq = np.fft.fftfreq(len(signal))
        return (fft,freq)


    def ifft(self,fft):
        """
        **SUMMARY**

        Perform an inverse fast Fourier transform on the provided
        irrationally valued signal and return the results as a
        LineScan.


        **PARAMETERS**

        * *fft* - A one dimensional numpy array of irrational values
                  upon which we will perform the IFFT.

        **RETURNS**

        A LineScan object of the reconstructed signal.

        **EXAMPLE**

        >>>> img = Image('lenna')
        >>>> sl = img.getLineScan(pt1=(0,0),pt2=(300,200))
        >>>> fft,frq = sl.fft()
        >>>> fft[30:] = 0 # low pass filter
        >>>> sl2 = sl.ifft(fft)
        >>>> import matplotlib.pyplot as plt
        >>>> plt.plot(sl)
        >>>> plt.plot(sl2)
        """
        signal = np.fft.ifft(fft)
        retVal = LineScan(signal.real)
        retVal.image = self.image
        retVal.pointLoc = self.pointLoc
        return retVal

    def createEmptyLUT(self,defaultVal=-1):
        """
        **SUMMARY**

        Create an empty look up table (LUT).

        If default value is what the lut is intially filled with
        if defaultVal == 0
            the array is all zeros.
        if defaultVal > 0
            the array is set to default value. Clipped to 255.
        if defaultVal < 0
            the array is set to the range [0,255]
        if defaultVal is a tuple of two values:
            we set stretch the range of 0 to 255 to match
            the range provided.


        **PARAMETERS**

        * *defaultVal* - See above.

        **RETURNS**

        A LUT.

        **EXAMPLE**

        >>>> ls = img.getLineScan(x=10)
        >>>> lut = ls.createEmptyLUT()
        >>>> ls2 = ls.applyLUT(lut)
        >>>> plt.plot(ls)
        >>>> plt.plot(ls2)
        >>>> plt.show()

        """
        lut = None
        if( isinstance(defaultVal,list) or
            isinstance(defaultVal,tuple)):
            start = np.clip(defaultVal[0],0,255)
            stop = np.clip(defaultVal[1],0,255)
            lut = np.around(np.linspace(start,stop,256),0)
            lut = np.array(lut,dtype='uint8')
            lut = lut.tolist()
        elif( defaultVal == 0 ):
            lut = np.zeros([1,256]).tolist()[0]
        elif( defaultVal > 0 ):
            defaultVal = np.clip(defaultVal,1,255)
            lut = np.ones([1,256])*defaultVal
            lut = np.array(lut,dtype='uint8')
            lut = lut.tolist()[0]
        elif( defaultVal < 0 ):
            lut = np.linspace(0,256,256)
            lut = np.array(lut,dtype='uint8')
            lut = lut.tolist()
        return lut

    def fillLUT(self,lut,idxs,value=255):
        """
        **SUMMARY**

        Fill up an existing LUT (look up table) at the indexes specified
        by idxs with the value specified by value. This is useful for picking
        out specific values.

        **PARAMETERS**

        * *lut* - An existing LUT (just a list of 255 values).
        * *idxs* -  The indexes of the LUT to fill with the value.
                    This can also be a sample swatch of an image.
        * *value* - the value to set the LUT[idx] to


        **RETURNS**

        An updated LUT.

        **EXAMPLE**

        >>>> ls = img.getLineScan(x=10)
        >>>> lut = ls.createEmptyLUT()
        >>>> swatch = img.crop(0,0,10,10)
        >>>> ls.fillLUT(lut,swatch,255)
        >>>> ls2 = ls.applyLUT(lut)
        >>>> plt.plot(ls)
        >>>> plt.plot(ls2)
        >>>> plt.show()

        """
        # for the love of god keep this small
        # for some reason isInstance is being persnickety
        if(idxs.__class__.__name__  == 'Image' ):
            npg = idxs.getGrayNumpy()
            npg = npg.reshape([npg.shape[0]*npg.shape[1]])
            idxs = npg.tolist()
        value = np.clip(value,0,255)
        for idx in idxs:
            if(idx >= 0 and idx < len(lut)):
                lut[idx]=value
        return lut

    def threshold(self,threshold=128,invert=False):
        """
        **SUMMARY**

        Do a 1D threshold operation. Values about the threshold
        will be set to 255, values below the threshold will be
        set to 0. If invert is true we do the opposite.

        **PARAMETERS**

        * *threshold* - The cutoff value for our threshold.
        * *invert* - if invert is false values above the threshold
                     are set to 255, if invert is True the are set to 0.

        **RETURNS**

        The thresholded linescan operation.

        **EXAMPLE**

        >>>> ls = img.getLineScan(x=10)
        >>>> ls2 = ls.threshold()
        >>>> plt.plot(ls)
        >>>> plt.plot(ls2)
        >>>> plt.show()

        """
        out = []
        high = 255
        low = 0
        if( invert ):
            high = 0
            low = 255
        for pt in self:
            if( pt < threshold ):
                out.append(low)
            else:
                out.append(high)
        retVal = LineScan(out,image=self.image,pointLoc=self.pointLoc,pt1=self.pt1,pt2=self.pt2)
        retVal._update(self)
        return retVal

    def invert(self,max=255):
        """
        **SUMMARY**

        Do an 8bit invert of the signal. What was black is now
        white, what was 255 is now zero.

        **PARAMETERS**

        * *max* - The maximum value of a pixel in the image, usually 255.

        **RETURNS**

        The inverted LineScan object.

        **EXAMPLE**

        >>>> ls = img.getLineScan(x=10)
        >>>> ls2 = ls.invert()
        >>>> plt.plot(ls)
        >>>> plt.plot(ls2)
        >>>> plt.show()

        """

        out = []
        for pt in self:
            out.append(255-pt)
        retVal = LineScan(out,image=self.image,pointLoc=self.pointLoc,pt1=self.pt1,pt2=self.pt2)
        retVal._update(self)
        return retVal

    def mean(self):
        """
        **SUMMARY**

        Computes the statistical mean of the signal.

        **RETURNS**

        The mean of the LineScan object.

        **EXAMPLE**

        >>>> ls = img.getLineScan(x=10)
        >>>> avg = ls.mean()
        >>>> plt.plot(ls)
        >>>> plt.axhline(y = avg)
        >>>> plt.show()

        """
        return float(sum(self))/len(self)

    def variance(self):
        """
        **SUMMARY**

        Computes the variance of the signal.

        **RETURNS**

        The variance of the LineScan object.

        **EXAMPLE**

        >>>> ls = img.getLineScan(x=10)
        >>>> var = ls.variance()
        >>>> var

        """
        mean = float(sum(self))/len(self)
        summation = 0
        for num in self:
            summation += (num - mean)**2
        return summation/len(self)

    def std(self):
        """
        **SUMMARY**

        Computes the standard deviation of the signal.

        **RETURNS**

        The standard deviation of the LineScan object.

        **EXAMPLE**

        >>>> ls = img.getLineScan(x=10)
        >>>> avg = ls.mean()
        >>>> std = ls.std()
        >>>> plt.plot(ls)
        >>>> plt.axhline(y = avg)
        >>>> plt.axhline(y = avg - std, color ='r')
        >>>> plt.axhline(y = avg + std, color ='r')
        >>>> plt.show()

        """
        mean = float(sum(self))/len(self)
        summation = 0
        for num in self:
            summation += (num - mean)**2
        return np.sqrt(summation/len(self))

    def median(self,sz=5):
        """
        **SUMMARY**

        Do a sliding median filter with a window size equal to size.


        **PARAMETERS**

        * *sz* - the size of the median filter.

        **RETURNS**

        The linescan after being passed through the median filter.
        The last index where the value occurs or None if none is found.


        **EXAMPLE**

        >>>> ls = img.getLineScan(x=10)
        >>>> ls2 = ls.median(7)
        >>>> plt.plot(ls)
        >>>> plt.plot(ls2)
        >>>> plt.show()

        """
        if( sz%2==0 ):
            sz = sz+1
        skip = int(np.floor(sz/2))
        out = self[0:skip]
        vsz = len(self)
        for idx in range(skip,vsz-skip):
            val = np.median(self[(idx-skip):(idx+skip)])
            out.append(val)
        for pt in self[-1*skip:]:
            out.append(pt)
        retVal = LineScan(out,image=self.image,pointLoc=self.pointLoc,pt1=self.pt1,pt2=self.pt2)
        retVal._update(self)
        return retVal

    def findFirstIdxEqualTo(self,value=255):
        """
        **SUMMARY**

        Find the index of the first element of the linescan that has
        a value equal to value. If nothing is found None is returned.

        **PARAMETERS**

        * *value* - the value to look for.

        **RETURNS**

        The first index where the value occurs or None if none is found.


        **EXAMPLE**

        >>>> ls = img.getLineScan(x=10)
        >>>> idx = ls.findFIRSTIDXEqualTo()

        """
        vals = np.where(np.array(self)==value)[0]
        retVal = None
        if( len(vals) > 0 ):
            retVal = vals[0]
        return retVal

    def findLastIdxEqualTo(self,value=255):
        """
        **SUMMARY**

        Find the index of the last element of the linescan that has
        a value equal to value. If nothing is found None is returned.

        **PARAMETERS**

        * *value* - the value to look for.

        **RETURNS**

        The last index where the value occurs or None if none is found.


        **EXAMPLE**

        >>>> ls = img.getLineScan(x=10)
        >>>> idx = ls.findLastIDXEqualTo()

        """

        vals = np.where(np.array(self)==value)[0]
        retVal = None
        if( len(vals) > 0 ):
            retVal = vals[-1]
        return retVal

    def findFirstIdxGreaterThan(self,value=255):
        """
        **SUMMARY**

        Find the index of the first element of the linescan that has
        a value equal to value. If nothing is found None is returned.

        **PARAMETERS**

        * *value* - the value to look for.

        **RETURNS**

        The first index where the value occurs or None if none is found.


        **EXAMPLE**

        >>>> ls = img.getLineScan(x=10)
        >>>> idx = ls.findFIRSTIDXEqualTo()

        """
        vals = np.where(np.array(self)>=value)[0]
        retVal = None
        if( len(vals) > 0 ):
            retVal = vals[0]
        return retVal
    def applyLUT(self,lut):
        """
        **SUMMARY**

        Apply a look up table to the signal.

        **PARAMETERS**

        * *lut* an array of of length 256, the array elements are the values
          that are replaced via the lut

        **RETURNS**

        A LineScan object with the LUT applied to the values.

        **EXAMPLE**

        >>>> ls = img.getLineScan(x=10)
        >>>> lut = ls.createEmptyLUT()
        >>>> ls2 = ls.applyLUT(lut)
        >>>> plt.plot(ls)
        >>>> plt.plot(ls2)

        """
        out = []
        for pt in self:
            out.append(lut[pt])
        retVal = LineScan(out,image=self.image,pointLoc=self.pointLoc,pt1=self.pt1,pt2=self.pt2)
        retVal._update(self)
        return retVal

    def medianFilter(self, kernel_size=5):
        """
        **SUMMARY**

        Apply median filter on the data

        **PARAMETERS**

        * *kernel_size* - Size of the filter (should be odd int) - int

        **RETURNS**

        A LineScan object with the median filter applied to the values.

        **EXAMPLE**

        >>> ls = img.getLineScan(x=10)
        >>> mf = ls.medianFilter()
        >>> plt.plot(ls)
        >>> plt.plot(mf)
        """
        try:
            from scipy.signal import medfilt
        except ImportError:
            warnings.warn("Scipy vesion >= 0.11 requierd.")
            return None
        if kernel_size % 2 == 0:
            kernel_size-=1
            print "Kernel Size should be odd. New kernel size =" , (kernel_size)
        
        medfilt_array = medfilt(np.asarray(self[:]), kernel_size)
        retVal = LineScan(medfilt_array.astype("uint8").tolist(), image=self.image,pointLoc=self.pointLoc,pt1=self.pt1,pt2=self.pt2, x=self.col, y=self.row)
        retVal._update(self)
        return retVal

    def detrend(self):
        """
        **SUMMARY**

        Detren the data

        **PARAMETERS**

        **RETURNS**

        A LineScan object with detrened data.

        **EXAMPLE**

        >>> ls = img.getLineScan(x=10)
        >>> dt = ls.detrend()
        >>> plt.plot(ls)
        >>> plt.plot(dt)
        """
        try:
            from scipy.signal import detrend as sdetrend
        except ImportError:
            warnings.warn("Scipy vesion >= 0.11 requierd.")
            return None
        detrend_arr = sdetrend(np.asarray(self[:]))
        retVal = LineScan(detrend_arr.astype("uint8").tolist(), image=self.image,pointLoc=self.pointLoc,pt1=self.pt1,pt2=self.pt2, x=self.col, y=self.row)
        retVal._update(self)
        return retVal

    def runningAverage(self, diameter=3, algo="uniform"):
        """
        **SUMMARY**

        Finds the running average by either using a uniform kernel or using a gaussian kernel.
        The gaussian kernelis calculated from the standard normal distribution formulae.

        **PARAMETERS**

        * *diameter* - Size of the window (should be odd int) - int

        * *algo* - "uniform" (default) / "gaussian" - used to decide the kernel - string

        **RETURNS**

        A LineScan object with the kernel of the provided algorithm applied.

        **EXAMPLE**

        >>> ls = img.getLineScan(x=10)
        >>> ra = ls.runningAverage()
        >>> rag = ls.runningAverage(15,algo="gaussian")
        >>> plt.plot(ls)
        >>> plt.plot(ra)
        >>> plt.plot(rag)
        >>> plt.show()
        
        """

        if diameter%2 == 0:
            warnings.warn("Diameter must be an odd integer")
            return None
        if algo=="uniform":
            kernel=list(1/float(diameter)*np.ones(diameter))
        elif algo=="gaussian":
            kernel=list()
            r=float(diameter)/2
            for i in range(-int(r),int(r)+1):
                kernel.append(np.exp(-i**2/(2*(r/3)**2))/(np.sqrt(2*np.pi)*(r/3)))
        retVal = LineScan(map(int,self.convolve(kernel)))
        retVal._update(self)
        return retVal

    def findPeaks(self, window = 30, delta = 3):
        """
        **SUMMARY**

        Finds the peaks in a LineScan.

        **PARAMETERS**

        * *window* - the size of the window in which the peak
         should have the highest value to be considered as a peak.
         By default this is 15 as it gives appropriate results.
         The lower this value the more the peaks are returned

        * *delta* - the minimum difference between the peak and 
        all elements in the window

        **RETURNS**

        A list of (peak position, peak value) tuples.

        **EXAMPLE**

        >>> ls = img.getLineScan(x=10)
        >>> peaks = ls.findPeaks()
        >>> print peaks
        >>> peaks10 = ls.findPeaks(window=10)
        >>> print peaks10
        
        """

        maximum = -np.Inf
        width = int(window/2.0)
        peaks = []

        for index,val in enumerate(self):
            #peak found
            if val > maximum:
                maximum = val
                maxpos = index
            #checking whether peak satisfies window and delta conditions
            if max( self[max(0, index-width):index+width])+delta< maximum:
                peaks.append((maxpos, maximum))
                maximum = -np.Inf
        return peaks

    def findValleys(self,window = 30, delta = 3 ):
        """
        **SUMMARY**

        Finds the valleys in a LineScan.

        **PARAMETERS**

        * *window* - the size of the window in which the valley
         should have the highest value to be considered as a valley.
         By default this is 15 as it gives appropriate results.
         The lower this value the more the valleys are returned

        * *delta* - the minimum difference between the valley and 
        all elements in the window

        **RETURNS**

        A list of (peak position, peak value) tuples.

        **EXAMPLE**

        >>> ls = img.getLineScan(x=10)
        >>> valleys = ls.findValleys()
        >>> print valleys
        >>> valleys10 = ls.findValleys(window=10)
        >>> print valleys10
        
        """
        minimum = np.Inf
        width = int(window/2.0)
        peaks = []

        for index,val in enumerate(self):
            #peak found
            if val < minimum:
                minimum = val
                minpos = index
            #checking whether peak satisfies window and delta conditions
            if min( self[max(0, index-width):index+width])-delta > minimum:
                peaks.append((minpos, minimum))
                minimum = np.Inf
        return peaks
        

    def fitSpline(self,degree=2):
        """
        **SUMMARY**

        A function to generate a spline curve fitting over the points in LineScan with
        order of precision given by the parameter degree

        **PARAMETERS**

        * *degree* - the precision of the generated spline 

        **RETURNS**

        The spline as a LineScan fitting over the initial values of LineScan

        **EXAMPLE**

        >>> import matplotlib.pyplot as plt
        >>> img = Image("lenna")
        >>> ls = img.getLineScan(pt1=(10,10)),pt2=(20,20)).normalize()
        >>> spline = ls.fitSpline()
        >>> plt.plot(ls)
        >>> plt.show()
        >>> plt.plot(spline)
        >>> plt.show()
        
        **NOTES**

        Implementation taken from http://www.scipy.org/Cookbook/Interpolation  

        """
        if degree > 4:
            degree = 4  # No significant improvement with respect to time usage
        if degree < 1:
            warnings.warn('LineScan.fitSpline - degree needs to be >= 1')
            return None
        retVal = None
        y = np.array(self)
        x = np.arange(0,len(y),1)
        dx = 1
        newx = np.arange(0,len(y)-1,pow(0.1,degree))
        cj = sps.cspline1d(y)
        retVal = sps.cspline1d_eval(cj,newx,dx=dx,x0=x[0])
        return retVal


########NEW FILE########
__FILENAME__ = BOFTester
from SimpleCV import *
from numpy import *
from SimpleCV.Features import BOFFeatureExtractor
import os
import glob
import time
cat_path = "./data/cat/truth/"
cheeseburger_path = "./data/cheeseburger/truth/"
paths = [cat_path,cheeseburger_path]
bof = BOFFeatureExtractor()
#bof.load('cbdata.txt')
bof.generate(paths,imgs_per_dir=200,numcodes=256,sz=(11,11),img_layout=(16,16),padding=4 )
bof.save("codebook.png","cbdata.txt")
#count = 0
#test = bof.reconstruct(Image("codebook.png"))
#test.save("anticodebook.jpg")
#for infile in glob.glob( os.path.join(cat_path, '*.jpg') ):
#    print "opening file: " + infile
#    img = Image(infile)
#    oimg = bof.reconstruct(img)
#    fname = infile[0:-4]+"_reconstruction.png"
#    oimg.save(fname)

########NEW FILE########
__FILENAME__ = ConfusionMatrix
from SimpleCV.base import *
from SimpleCV.ImageClass import Image

class ConfusionMatrix():
    def __init__(self,classList):
        self.classList = classList
        self.classCount = len(classList)
        self.confusionMatrix = np.zeros([self.classCount,self.classCount])
        self.correctCount = 0
        self.incorrectCount = 0
        self.totalCount = 0
        self.nameMap = {}
        idx = 0
        for obj in classList:
            self.nameMap[obj] = idx
            idx = idx + 1

    def addDataPoint(self,truth_name,test_name):
        self.confusionMatrix[self.nameMap[truth_name]][self.nameMap[test_name]] += 1
        if( truth_name == test_name ):
            self.correctCount += 1
        else:
            self.incorrectCount +=1

        self.totalCount += 1

    def getCorrectPercent(self):
        if( self.totalCount > 0 and self.correctCount ):
            return np.around(float(self.correctCount)/float(self.totalCount),4)
        else:
            return 0.00

    def getIncorrectPercent(self):
        if( self.totalCount > 0 and self.correctCount ):
            return np.around(float(self.incorrectCount)/float(self.totalCount),4)
        else:
            return 0.00

    def getClassCorrectPercent(self, className):
        total = float(np.sum(self.confusionMatrix[:,self.nameMap[className]]))
        correct = float(self.confusionMatrix[self.nameMap[className],self.nameMap[className]])
        if( correct == 0 or total == 0 ):
            return 0
        else:
            return np.around(correct/total,2)

    def getClassIncorrectPercent(self, className):
        total = float(np.sum(self.confusionMatrix[:,self.nameMap[className]]))
        correct = float(self.confusionMatrix[self.nameMap[className],self.nameMap[className]])
        incorrect = total-correct
        if( incorrect == 0 or total == 0 ):
            return 0
        else:
            return np.around(incorrect/total,2)

    def getClassCorrect(self, className):
        correct = self.confusionMatrix[self.nameMap[className],self.nameMap[className]]
        return correct

    def getClassIncorrect(self, className):
        total = np.sum(self.confusionMatrix[:,self.nameMap[className]])
        correct = self.confusionMatrix[self.nameMap[className],self.nameMap[className]]
        incorrect = total-correct
        return incorrect


    def getClassCount(self,className):
        return np.sum(self.confusionMatrix[:,self.nameMap[className]])

    def getMisclassifiedCount(self,className):
        # if we're class A, this returns the number of class B, C ...
        # that were classified as A
        count = np.sum(self.confusionMatrix[self.nameMap[className],:])
        correct = self.confusionMatrix[[self.nameMap[className]],self.nameMap[className]]
        total = count - correct
        return int(total[0])

    def toString(self,pad_sz=7):
        retVal = 50*'#'
        retVal += "\n"
        retVal += "Total Data Points " + str(self.totalCount) + "\n"
        retVal += "Correct Data Points " + str(self.correctCount) + "\n"
        retVal += "Incorrect Data Points " + str(self.incorrectCount) + "\n"
        retVal += "\n"
        retVal += "Correct " + str(100.00*self.getCorrectPercent()) + "%\n"
        retVal += "Incorrect " + str(100.00*self.getIncorrectPercent()) + "% \n"
        retVal += 50*'#'
        retVal += '\n'
        wrdLen = 0
        sz = pad_sz
        for c in self.classList:
            if( len(c) > wrdLen ):
                wrdLen = len(c)

        top=(wrdLen+1)* " "
        for c in self.classList:
            top = top + c[0:np.min([len(c),sz])].rjust(sz," ")+"|"
        retVal += top+"\n"
        for i in range(0,len(self.classList)):
            line = self.classList[i].rjust(wrdLen," ")+"|"
            nums = self.confusionMatrix[i]
            for n in nums:
                line += str(n).rjust(sz," ") + "|"
            retVal += line+"\n"
        retVal += 50*'#'
        retVal += "\n"
        return retVal

########NEW FILE########
__FILENAME__ = KNNClassifier
from SimpleCV.base import *
from SimpleCV.ImageClass import Image, ImageSet
from SimpleCV.DrawingLayer import *
from SimpleCV.Features import FeatureExtractorBase
"""
This class is encapsulates almost everything needed to train, test, and deploy a
multiclass k-nearest neighbors image classifier. Training data should
be stored in separate directories for each class. This class uses the feature
extractor base class to  convert images into a feature vector. The basic workflow
is as follows.
1. Get data.
2. Setup Feature Extractors (roll your own or use the ones I have written).
3. Train the classifier.
4. Test the classifier.
5. Tweak parameters as necessary.
6. Repeat until you reach the desired accuracy.
7. Save the classifier.
8. Deploy using the classify method.
"""
class KNNClassifier:
    """
    This class encapsulates a K- Nearest Neighbor Classifier.

    See http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm
    """
    mClassNames = []
    mDataSetRaw = []
    mK=1
    mDistType = None
    mDataSetOrange = []
    mClassifier = None
    mLearner = None
    mFeatureExtractors = None
    mOrangeDomain = None

    mDistDict = {}

    def __init__(self,featureExtractors,k=1,dist=None):
        """
        dist = distance algorithm
        k = number of nearest neighbors
        """
        if not ORANGE_ENABLED:
            logger.warning("The required orange machine learning library is not installed")
            return None

        self.mDistDict = {
            'Hamming': orange.ExamplesDistanceConstructor_Hamming(), # Hamming - only good for discrete variables
            'Maximal': orange.ExamplesDistance_Maximal(),
            'Manhattan':orange.ExamplesDistanceConstructor_Manhattan(),
            'Euclidean':orange.ExamplesDistanceConstructor_Euclidean(),
            'Normalized':None
        }


        self.mFeatureExtractors =  featureExtractors
        if( dist is not None ):
            self.mDistType = self.mDistDict[dist];
        self.mK = k;
        self.mLearner = None
        self.mClassNames = []
        self.mDataSetRaw = []
        self.mDataSetOrange = []
        self.mClassifier = None
        self.mOrangeDomain = None

    def setK(self,k):
        """
        Note that training and testing will need to be redone.
        """
        self.mK = k

    def setDistanceMetric(self,dist):
        """
        Note that training and testing will need to be redone.
        """
        self.mDistType = self.mDistDict[dist]

    def load(cls, fname):
        """
        Load the classifier from file
        """
        return pickle.load(file(fname))
    load = classmethod(load)


    def save(self, fname):
        """
        Save the classifier to file
        """
        output = open(fname, 'wb')
        pickle.dump(self,output,2) # use two otherwise it w
        output.close()

    def __getstate__(self):
        mydict = self.__dict__.copy()
        self.mDataSetOrange = None
        del mydict['mDataSetOrange']
        self.mOrangeDomain = None
        del mydict['mOrangeDomain']
        return mydict

    def __setstate__(self, mydict):
        self.__dict__ = mydict
        colNames = []
        for extractor in self.mFeatureExtractors:
            colNames.extend(extractor.getFieldNames())
        self.mOrangeDomain = orange.Domain(map(orange.FloatVariable,colNames),orange.EnumVariable("type",values=self.mClassNames))
        self.mDataSetOrange = orange.ExampleTable(self.mOrangeDomain,self.mDataSetRaw)


    def classify(self, image):
        """
        Classify a single image. Takes in an image and returns the string
        of the classification.

        Make sure you haved loaded the feauture extractors and the training data.

        """
        featureVector = []
        for extractor in self.mFeatureExtractors: #get the features
            feats = extractor.extract(image)
            if( feats is not None ):
                featureVector.extend(feats)
        featureVector.extend([self.mClassNames[0]])
        test = orange.ExampleTable(self.mOrangeDomain,[featureVector])
        c = self.mClassifier(test[0]) #classify
        return str(c) #return to class name


    def setFeatureExtractors(self, extractors):
        """
        Add a list of feature extractors to the classifier. These feature extractors
        must match the ones used to train the classifier. If the classifier is already
        trained then this method will require that you retrain the data.
        """
        self.mFeatureExtractors = extractors
        return None

    def _trainPath(self,path,className,subset,disp,verbose):
        count = 0
        files = []
        for ext in IMAGE_FORMATS:
            files.extend(glob.glob( os.path.join(path, ext)))
        if(subset > 0):
            nfiles = min(subset,len(files))
        else:
            nfiles = len(files)
        badFeat = False
        for i in range(nfiles):
            infile = files[i]
            if verbose:
                print "Opening file: " + infile
            img = Image(infile)
            featureVector = []
            for extractor in self.mFeatureExtractors:
                feats = extractor.extract(img)
                if( feats is not None ):
                    featureVector.extend(feats)
                else:
                    badFeat = True

            if(badFeat):
                badFeat = False
                continue

            featureVector.extend([className])
            self.mDataSetRaw.append(featureVector)
            text = 'Training: ' + className
            self._WriteText(disp,img,text,Color.WHITE)
            count = count + 1
            del img
        return count

    def _trainImageSet(self,imageset,className,subset,disp,verbose):
        count = 0
        badFeat = False
        if (subset>0):
            imageset = imageset[0:subset]   
        for img in imageset:
            if verbose:
                print "Opening file: " + img.filename
            featureVector = []
            for extractor in self.mFeatureExtractors:
                feats = extractor.extract(img)
                if( feats is not None ):
                    featureVector.extend(feats)
                else:
                    badFeat = True
                    
            if(badFeat):
                badFeat = False
                continue
            
            featureVector.extend([className])
            self.mDataSetRaw.append(featureVector)
            text = 'Training: ' + className
            self._WriteText(disp,img,text,Color.WHITE)
            count = count + 1
            del img
        return count

    def train(self,images,classNames,disp=None,subset=-1,savedata=None,verbose=True):
        """
        Train the classifier.
        images paramater can take in a list of paths or a list of imagesets
        images - the order of the paths or imagesets must be in the same order as the class type

        - Note all image classes must be in seperate directories
        - The class names must also align to the directories

        disp - if display is a display we show images and class label,
        otherwise nothing is done.

        subset - if subset = -1 we use the whole dataset. If subset = # then we
        use min(#images,subset)

        savedata - if save data is None nothing is saved. If savedata is a file
        name we save the data to a tab delimited file.

        verbose - print confusion matrix and file names
        returns [%Correct %Incorrect Confusion_Matrix]
        """
        count = 0
        self.mClassNames = classNames
        # fore each class, get all of the data in the path and train
        for i in range(len(classNames)):
            if ( isinstance(images[i], str) ):
                count = count + self._trainPath(images[i],classNames[i],subset,disp,verbose)
            else:
                count = count + self._trainImageSet(images[i],classNames[i],subset,disp,verbose)

        colNames = []
        for extractor in self.mFeatureExtractors:
            colNames.extend(extractor.getFieldNames())

        if(count <= 0):
            logger.warning("No features extracted - bailing")
            return None

        # push our data into an orange example table
        self.mOrangeDomain = orange.Domain(map(orange.FloatVariable,colNames),orange.EnumVariable("type",values=self.mClassNames))
        self.mDataSetOrange = orange.ExampleTable(self.mOrangeDomain,self.mDataSetRaw)
        if(savedata is not None):
            orange.saveTabDelimited (savedata, self.mDataSetOrange)

        self.mLearner =  orange.kNNLearner()
        self.mLearner.k = self.mK
        if(self.mDistType is not None):
            self.mClassifier.distanceConstructor = self.mDistType
        self.mClassifier = self.mLearner(self.mDataSetOrange)
        correct = 0
        incorrect = 0
        for i in range(count):
            c = self.mClassifier(self.mDataSetOrange[i])
            test = self.mDataSetOrange[i].getclass()
            if verbose:
                print "original", test, "classified as", c
            if(test==c):
                correct = correct + 1
            else:
                incorrect = incorrect + 1

        good = 100*(float(correct)/float(count))
        bad = 100*(float(incorrect)/float(count))

        confusion = 0
        if( len(self.mClassNames) > 2 ):
            crossValidator = orngTest.learnAndTestOnLearnData([self.mLearner],self. mDataSetOrange)
            confusion = orngStat.confusionMatrices(crossValidator)[0]

        if verbose:
            print("Correct: "+str(good))
            print("Incorrect: "+str(bad))
            classes = self.mDataSetOrange.domain.classVar.values
            print "\t"+"\t".join(classes)
            for className, classConfusions in zip(classes, confusion):
                print ("%s" + ("\t%i" * len(classes))) % ((className, ) + tuple(classConfusions))
        return [good, bad, confusion]




    def test(self,images,classNames,disp=None,subset=-1,savedata=None,verbose=True):
        """
        Test the classifier.
        images paramater can take in a list of paths or a list of imagesets
        images - the order of the paths or imagesets must be in the same order as the class type

        - Note all image classes must be in seperate directories
        - The class names must also align to the directories

        disp - if display is a display we show images and class label,
        otherwise nothing is done.

        subset - if subset = -1 we use the whole dataset. If subset = # then we
        use min(#images,subset)

        savedata - if save data is None nothing is saved. If savedata is a file
        name we save the data to a tab delimited file.

        verbose - print confusion matrix and file names
        returns [%Correct %Incorrect Confusion_Matrix]
        """
        count = 0
        correct = 0
        self.mClassNames = classNames
        colNames = []
        for extractor in self.mFeatureExtractors:
            colNames.extend(extractor.getFieldNames())
            self.mOrangeDomain = orange.Domain(map(orange.FloatVariable,colNames),orange.EnumVariable("type",values=self.mClassNames))

        dataset = []
        for i in range(len(classNames)):
            if ( isinstance(images[i],str) ):
                [dataset,cnt,crct] =self._testPath(images[i],classNames[i],dataset,subset,disp,verbose)
                count = count + cnt
                correct = correct + crct
            else:
                [dataset,cnt,crct] =self._testImageSet(images[i],classNames[i],dataset,subset,disp,verbose)
                count = count + cnt
                correct = correct + crct


        testData = orange.ExampleTable(self.mOrangeDomain,dataset)

        if savedata is not None:
            orange.saveTabDelimited (savedata, testData)

        confusion = 0
        if( len(self.mClassNames) > 2 ):
            crossValidator = orngTest.learnAndTestOnTestData([self.mLearner],self.mDataSetOrange,testData)
            confusion = orngStat.confusionMatrices(crossValidator)[0]

        good = 100*(float(correct)/float(count))
        bad = 100*(float(count-correct)/float(count))
        if verbose:
            print("Correct: "+str(good))
            print("Incorrect: "+str(bad))
            classes = self.mDataSetOrange.domain.classVar.values
            print "\t"+"\t".join(classes)
            for className, classConfusions in zip(classes, confusion):
                print ("%s" + ("\t%i" * len(classes))) % ((className, ) + tuple(classConfusions))

        return [good, bad, confusion]

    def _testPath(self,path,className,dataset,subset,disp,verbose):
        count = 0
        correct = 0
        badFeat = False
        files = []
        for ext in IMAGE_FORMATS:
            files.extend(glob.glob( os.path.join(path, ext)))
        if(subset > 0):
            nfiles = min(subset,len(files))
        else:
            nfiles = len(files)
        for i in range(nfiles):
            infile = files[i]
            if verbose:
                print "Opening file: " + infile
            img = Image(infile)
            featureVector = []
            for extractor in self.mFeatureExtractors:
                feats = extractor.extract(img)
                if( feats is not None ):
                    featureVector.extend(feats)
                else:
                    badFeat = True
            if( badFeat ):
                del img
                badFeat = False
                continue
            featureVector.extend([className])
            dataset.append(featureVector)
            test = orange.ExampleTable(self.mOrangeDomain,[featureVector])
            c = self.mClassifier(test[0])
            testClass = test[0].getclass()
            if(testClass==c):
                text =  "Classified as " + str(c)
                self._WriteText(disp,img,text, Color.GREEN)
                correct = correct + 1
            else:
                text =  "Mislassified as " + str(c)
                self._WriteText(disp,img,text, Color.RED)
            count = count + 1
            del img

        return([dataset,count,correct])

    def _testImageSet(self,imageset,className,dataset,subset,disp,verbose):
        count = 0
        correct = 0
        badFeat = False
        if(subset > 0):
            imageset = imageset[0:subset]
        for img in imageset:
            if verbose:
                print "Opening file: " + img.filename
            featureVector = []
            for extractor in self.mFeatureExtractors:
                feats = extractor.extract(img)
                if( feats is not None ):
                    featureVector.extend(feats)
                else:
                    badFeat = True
            if( badFeat ):
                del img
                badFeat = False
                continue 
            featureVector.extend([className])
            dataset.append(featureVector)
            test = orange.ExampleTable(self.mOrangeDomain,[featureVector])
            c = self.mClassifier(test[0])
            testClass = test[0].getclass()
            if(testClass==c):
                text =  "Classified as " + str(c)
                self._WriteText(disp,img,text, Color.GREEN)
                correct = correct + 1
            else:   
                text =  "Mislassified as " + str(c)
                self._WriteText(disp,img,text, Color.RED)
            count = count + 1
            del img
            
        return([dataset,count,correct])

    def _WriteText(self, disp, img, txt,color):
        if(disp is not None):
            txt = ' ' + txt + ' '
            img = img.adaptiveScale(disp.resolution)
            layer = DrawingLayer((img.width,img.height))
            layer.setFontSize(60)
            layer.ezViewText(txt,(20,20),fgcolor=color)
            img.addDrawingLayer(layer)
            img.applyLayers()
            img.save(disp)

########NEW FILE########
__FILENAME__ = MLTestSuite
from SimpleCV import *

print ""
print "This program runs a list of test for machine learning on"
print "the SimpleCV library. Not all scores will be high, this"
print "is just to ensure that the libraries are functioning correctly"
print "on your system"
print ""
print "***** WARNING *****"
print "This program is about to download a large data set to run it's test"


inp = raw_input("Do you want to continue [Y/n]")
if not (inp == "" or inp.lower() == "y"):
    print "Exiting the program"
    sys.exit()


machine_learning_data_set = "https://github.com/downloads/sightmachine/SimpleCV/machine_learning_dataset.zip"
data_path = download_and_extract(machine_learning_data_set)

w = 800
h = 600
n=50

display = Display(resolution = (w,h))

hue = HueHistogramFeatureExtractor(mNBins=16)
edge = EdgeHistogramFeatureExtractor()
bof = BOFFeatureExtractor()
bof.load('../Features/cbdata.txt')
haar = HaarLikeFeatureExtractor(fname="../Features/haar.txt")
morph = MorphologyFeatureExtractor()

spath = data_path + "/data/structured/"
upath = data_path + "/data/unstructured/"
ball_path = spath+"ball/"
basket_path = spath+"basket/"
boat_path = spath+"boat/"
cactus_path = spath +"cactus/"
cup_path = spath+"cup/"
duck_path = spath+"duck/"
gb_path = spath+"greenblock/"
match_path = spath+"matches/"
rb_path = spath+"redblock/"
s1_path = spath+"stuffed/"
s2_path = spath+"stuffed2/"
s3_path = spath+"stuffed3/"

arbor_path = upath+"arborgreens/"
football_path = upath+"football/"
sanjuan_path = upath+"sanjuans/"



print('SVMPoly')
#Set up am SVM with a poly kernel
extractors = [hue]
path = [cactus_path,cup_path,basket_path]
classes = ['cactus','cup','basket']
props ={
        'KernelType':'Poly', #default is a RBF Kernel
        'SVMType':'C',     #default is C
        'nu':None,          # NU for SVM NU
        'c':None,           #C for SVM C - the slack variable
        'degree':3,      #degree for poly kernels - defaults to 3
        'coef':None,        #coef for Poly/Sigmoid defaults to 0
        'gamma':None,       #kernel param for poly/rbf/sigma - default is 1/#samples
    }
print('Train')
classifierSVMP = SVMClassifier(extractors,props)
data = []
for p in path:
    data.append(ImageSet(p))
classifierSVMP.train(data,classes,disp=display,subset=n) #train
print('Test')
[pos,neg,confuse] = classifierSVMP.test(data,classes,disp=display,subset=n)
files = []
for ext in IMAGE_FORMATS:
    files.extend(glob.glob( os.path.join(path[0], ext)))
for i in range(10):
    img = Image(files[i])
    cname = classifierSVMP.classify(img)
    print(files[i]+' -> '+cname)
classifierSVMP.save('PolySVM.pkl')
print('Reloading from file')
testSVM = SVMClassifier.load('PolySVM.pkl')
#testSVM.setFeatureExtractors(extractors)
files = glob.glob( os.path.join(path[0], '*.jpg'))
for i in range(10):
    img = Image(files[i])
    cname = testSVM.classify(img)
    print(files[i]+' -> '+cname)

print('###############################################################################')
print('SVMRBF   ')
# now try an RBF kernel
extractors = [hue,edge]
path = [cactus_path,cup_path,basket_path]
classes = ['cactus','cup','basket']
props ={
        'KernelType':'RBF', #default is a RBF Kernel
        'SVMType':'NU',     #default is C
        'nu':None,          # NU for SVM NU
        'c':None,           #C for SVM C - the slack variable
        'degree':None,      #degree for poly kernels - defaults to 3
        'coef':None,        #coef for Poly/Sigmoid defaults to 0
        'gamma':None,       #kernel param for poly/rbf/sigma
    }
print('Train')
classifierSVMRBF = SVMClassifier(extractors,props)
data = []
for p in path:
    data.append(ImageSet(p))

classifierSVMRBF.train(data,classes,disp=display,subset=n) #train
print('Test')
[pos,neg,confuse] = classifierSVMRBF.test(data,classes,disp=display,subset=n)
files = glob.glob( os.path.join(path[0], '*.jpg'))
for i in range(10):
    img = Image(files[i])
    cname = classifierSVMRBF.classify(img)
    print(files[i]+' -> '+cname)
classifierSVMRBF.save('RBFSVM.pkl')
print('Reloading from file')
testSVMRBF = SVMClassifier.load('RBFSVM.pkl')
#testSVMRBF.setFeatureExtractors(extractors)
files = glob.glob( os.path.join(path[0], '*.jpg'))
for i in range(10):
    img = Image(files[i])
    cname = testSVMRBF.classify(img)
    print(files[i]+' -> '+cname)


print('###############################################################################')
print('Bayes')
extractors = [haar]
classifierBayes = NaiveBayesClassifier(extractors)#
print('Train')
path = [arbor_path,football_path,sanjuan_path]
classes = ['arbor','football','sanjuan']
classifierBayes.train(path,classes,disp=display,subset=n) #train
print('Test')
[pos,neg,confuse] = classifierBayes.test(path,classes,disp=display,subset=n)
files = glob.glob( os.path.join(path[0], '*.jpg'))
for i in range(10):
    img = Image(files[i])
    cname = classifierBayes.classify(img)
    print(files[i]+' -> '+cname)
classifierBayes.save('Bayes.pkl')
print('Reloading from file')
testBayes = NaiveBayesClassifier.load('Bayes.pkl')
testBayes.setFeatureExtractors(extractors)
files = glob.glob( os.path.join(path[0], '*.jpg'))
for i in range(10):
    img = Image(files[i])
    cname = testBayes.classify(img)
    print(files[i]+' -> '+cname)

print('###############################################################################')


print('###############################################################################')
print('Forest')
extractors = [morph]
classifierForest = TreeClassifier(extractors,flavor='Forest')#
print('Train')
path = [s1_path,s2_path,s3_path]
classes = ['s1','s2','s3']
classifierForest.train(path,classes,disp=display,subset=n) #train
print('Test')
[pos,neg,confuse] = classifierForest.test(path,classes,disp=display,subset=n)
files = glob.glob( os.path.join(path[0], '*.jpg'))
for i in range(10):
    img = Image(files[i])
    cname = classifierForest.classify(img)
    print(files[i]+' -> '+cname)

classifierForest.save('forest.pkl')
print('Reloading from file')
testForest = TreeClassifier.load('forest.pkl')
testForest.setFeatureExtractors(extractors)
files = glob.glob( os.path.join(path[0], '*.jpg'))
for i in range(10):
    img = Image(files[i])
    cname = testForest.classify(img)
    print(files[i]+' -> '+cname)

print('###############################################################################')
print('Bagged Tree')
extractors = [haar]
classifierBagTree = TreeClassifier(extractors,flavor='Bagged')#
print('Train')
path = [s1_path,s2_path,s3_path]
classes = ['s1','s2','s3']
classifierBagTree.train(path,classes,disp=display,subset=n) #train
print('Test')
[pos,neg,confuse] = classifierBagTree.test(path,classes,disp=display,subset=n)
files = glob.glob( os.path.join(path[0], '*.jpg'))
for i in range(10):
    img = Image(files[i])
    cname = classifierBagTree.classify(img)
    print(files[i]+' -> '+cname)

classifierBagTree.save('bagtree.pkl')
print('Reloading from file')
testBagTree = TreeClassifier.load('bagtree.pkl')
testBagTree.setFeatureExtractors(extractors)
files = glob.glob( os.path.join(path[0], '*.jpg'))
for i in range(10):
    img = Image(files[i])
    cname = testBagTree.classify(img)
    print(files[i]+' -> '+cname)



print('###############################################################################')
print('Vanilla Tree')
extractors = [haar]
classifierTree = TreeClassifier(featureExtractors=extractors)
print('Train')
path = [s1_path,s2_path,s3_path]
classes = ['s1','s2','s3']
classifierTree.train(path,classes,disp=display,subset=n) #train
print('Test')
[pos,neg,confuse] = classifierTree.test(path,classes,disp=display,subset=n)
files = glob.glob( os.path.join(path[0], '*.jpg'))
for i in range(10):
    img = Image(files[i])
    cname = classifierTree.classify(img)
    print(files[i]+' -> '+cname)
print('Reloading from file')
classifierTree.save('tree.pkl')
testTree = TreeClassifier.load('tree.pkl')
testTree.setFeatureExtractors(extractors)
for i in range(10):
    img = Image(files[i])
    cname = testTree.classify(img)
    print(files[i]+' -> '+cname)

print('###############################################################################')
print('Boosted Tree')
extractors = [haar]
classifierBTree = TreeClassifier(extractors,flavor='Boosted')#
print('Train')
path = [s1_path,s2_path,s3_path]
classes = ['s1','s2','s3']
classifierBTree.train(path,classes,disp=display,subset=n) #train
print('Test')
[pos,neg,confuse] = classifierBTree.test(path,classes,disp=display,subset=n)
files = glob.glob( os.path.join(path[0], '*.jpg'))
for i in range(10):
    img = Image(files[i])
    cname = classifierBTree.classify(img)
    print(files[i]+' -> '+cname)

classifierBTree.save('btree.pkl')
print('Reloading from file')

testBoostTree = TreeClassifier.load('btree.pkl')
testBoostTree.setFeatureExtractors(extractors)
files = glob.glob( os.path.join(path[0], '*.jpg'))
for i in range(10):
    img = Image(files[i])
    cname = testBoostTree.classify(img)
    print(files[i]+' -> '+cname)




print('###############################################################################')
print('KNN')
extractors = [hue,edge]
classifierKNN = KNNClassifier(extractors)#
print('Train')
path = [s1_path,s2_path,s3_path]
classes = ['s1','s2','s3']
classifierKNN.train(path,classes,disp=display,subset=n) #train
print('Test')
[pos,neg,confuse] = classifierKNN.test(path,classes,disp=display,subset=n)
files = glob.glob( os.path.join(path[0], '*.jpg'))
for i in range(10):
    img = Image(files[i])
    cname = classifierKNN.classify(img)
    print(files[i]+' -> '+cname)

classifierKNN.save('knn.pkl')
print('Reloading from file')
testKNN = KNNClassifier.load('knn.pkl')
testKNN.setFeatureExtractors(extractors)
files = glob.glob( os.path.join(path[0], '*.jpg'))
for i in range(10):
    img = Image(files[i])
    cname = testKNN.classify(img)
    print(files[i]+' -> '+cname)


print ""
print "All the machine learning test have ran correctly"

########NEW FILE########
__FILENAME__ = NaiveBayesClassifier
from SimpleCV.base import *
from SimpleCV.ImageClass import Image, ImageSet
from SimpleCV.DrawingLayer import *
from SimpleCV.Features import FeatureExtractorBase
"""
This class is encapsulates almost everything needed to train, test, and deploy a
multiclass support vector machine for an image classifier. Training data should
be stored in separate directories for each class. This class uses the feature
extractor base class to  convert images into a feature vector. The basic workflow
is as follows.
1. Get data.
2. Setup Feature Extractors (roll your own or use the ones I have written).
3. Train the classifier.
4. Test the classifier.
5. Tweak parameters as necessary.
6. Repeat until you reach the desired accuracy.
7. Save the classifier.
8. Deploy using the classify method.
"""
class NaiveBayesClassifier:
    """
    This class encapsulates a Naive Bayes Classifier.
    See:
    http://en.wikipedia.org/wiki/Naive_bayes
    """
    mClassNames = []
    mDataSetRaw = []
    mDataSetOrange = []
    mClassifier = None
    mFeatureExtractors = None
    mOrangeDomain = None

    def __init__(self,featureExtractors):

        if not ORANGE_ENABLED:
            logger.warning("The required orange machine learning library is not installed")
            return None

        self.mFeatureExtractors =  featureExtractors
        self.mClassNames = []
        self.mDataSetRaw = []
        self.mDataSetOrange = []
        self.mClassifier = None
        self.mOrangeDomain = None

    def load(cls, fname):
        """
        Load the classifier from file
        """
        return pickle.load(file(fname))
    load = classmethod(load)


    def save(self, fname):
        """
        Save the classifier to file
        """
        output = open(fname, 'wb')
        pickle.dump(self,output,2) # use two otherwise it w
        output.close()

    def __getstate__(self):
        mydict = self.__dict__.copy()
        self.mDataSetOrange = None
        del mydict['mDataSetOrange']
        self.mOrangeDomain = None
        del mydict['mOrangeDomain']
        return mydict

    def __setstate__(self, mydict):
        self.__dict__ = mydict
        colNames = []
        for extractor in self.mFeatureExtractors:
            colNames.extend(extractor.getFieldNames())
        self.mOrangeDomain = orange.Domain(map(orange.FloatVariable,colNames),orange.EnumVariable("type",values=self.mClassNames))
        self.mDataSetOrange = orange.ExampleTable(self.mOrangeDomain,self.mDataSetRaw)


    def classify(self, image):
        """
        Classify a single image. Takes in an image and returns the string
        of the classification.

        Make sure you haved loaded the feauture extractors and the training data.

        """
        featureVector = []
        for extractor in self.mFeatureExtractors: #get the features
            feats = extractor.extract(image)
            if( feats is not None ):
                featureVector.extend(feats)
        featureVector.extend([self.mClassNames[0]])
        test = orange.ExampleTable(self.mOrangeDomain,[featureVector])
        c = self.mClassifier(test[0]) #classify
        return str(c) #return to class name


    def setFeatureExtractors(self, extractors):
        """
        Add a list of feature extractors to the classifier. These feature extractors
        must match the ones used to train the classifier. If the classifier is already
        trained then this method will require that you retrain the data.
        """
        self.mFeatureExtractors = extractors
        return None

    def _trainPath(self,path,className,subset,disp,verbose):
        count = 0
        files = []
        for ext in IMAGE_FORMATS:
            files.extend(glob.glob( os.path.join(path, ext)))
        if(subset > 0):
            nfiles = min(subset,len(files))
        else:
            nfiles = len(files)
        badFeat = False
        for i in range(nfiles):
            infile = files[i]
            if verbose:
                print "Opening file: " + infile
            img = Image(infile)
            featureVector = []
            for extractor in self.mFeatureExtractors:
                feats = extractor.extract(img)
                if( feats is not None ):
                    featureVector.extend(feats)
                else:
                    badFeat = True

            if(badFeat):
                badFeat = False
                continue

            featureVector.extend([className])
            self.mDataSetRaw.append(featureVector)
            text = 'Training: ' + className
            self._WriteText(disp,img,text,Color.WHITE)
            count = count + 1
            del img
        return count

    def _trainImageSet(self,imageset,className,subset,disp,verbose):
        count = 0
        badFeat = False
        if (subset>0):
            imageset = imageset[0:subset]   
        for img in imageset:
            if verbose:
                print "Opening file: " + img.filename
            featureVector = []
            for extractor in self.mFeatureExtractors:
                feats = extractor.extract(img)
                if( feats is not None ):
                    featureVector.extend(feats)
                else:
                    badFeat = True
                    
            if(badFeat):
                badFeat = False
                continue
            
            featureVector.extend([className])
            self.mDataSetRaw.append(featureVector)
            text = 'Training: ' + className
            self._WriteText(disp,img,text,Color.WHITE)
            count = count + 1
            del img
        return count

    def train(self,images,classNames,disp=None,subset=-1,savedata=None,verbose=True):
        """
        Train the classifier.
        images paramater can take in a list of paths or a list of imagesets
        images - the order of the paths or imagesets must be in the same order as the class type

        - Note all image classes must be in seperate directories
        - The class names must also align to the directories

        disp - if display is a display we show images and class label,
        otherwise nothing is done.

        subset - if subset = -1 we use the whole dataset. If subset = # then we
        use min(#images,subset)

        savedata - if save data is None nothing is saved. If savedata is a file
        name we save the data to a tab delimited file.

        verbose - print confusion matrix and file names
        returns [%Correct %Incorrect Confusion_Matrix]
        """
        count = 0
        self.mClassNames = classNames
        # fore each class, get all of the data in the path and train
        for i in range(len(classNames)):
            if ( isinstance(images[i], str) ):
                count = count + self._trainPath(images[i],classNames[i],subset,disp,verbose)
            else:
                count = count + self._trainImageSet(images[i],classNames[i],subset,disp,verbose)

        colNames = []
        for extractor in self.mFeatureExtractors:
            colNames.extend(extractor.getFieldNames())

        if(count <= 0):
            logger.warning("No features extracted - bailing")
            return None

        # push our data into an orange example table
        self.mOrangeDomain = orange.Domain(map(orange.FloatVariable,colNames),orange.EnumVariable("type",values=self.mClassNames))
        self.mDataSetOrange = orange.ExampleTable(self.mOrangeDomain,self.mDataSetRaw)
        if(savedata is not None):
            orange.saveTabDelimited (savedata, self.mDataSetOrange)

        self.mClassifier = orange.BayesLearner(self.mDataSetOrange)
        correct = 0
        incorrect = 0
        for i in range(count):
            c = self.mClassifier(self.mDataSetOrange[i])
            test = self.mDataSetOrange[i].getclass()
            if verbose:
                print "original", test, "classified as", c
            if(test==c):
                correct = correct + 1
            else:
                incorrect = incorrect + 1

        good = 100*(float(correct)/float(count))
        bad = 100*(float(incorrect)/float(count))

        confusion = 0
        if( len(self.mClassNames) > 2 ):
            crossValidator = orngTest.learnAndTestOnLearnData([orange.BayesLearner],self.mDataSetOrange)
            confusion = orngStat.confusionMatrices(crossValidator)[0]

        if verbose:
            print("Correct: "+str(good))
            print("Incorrect: "+str(bad))
            classes = self.mDataSetOrange.domain.classVar.values
            print "\t"+"\t".join(classes)
            for className, classConfusions in zip(classes, confusion):
                print ("%s" + ("\t%i" * len(classes))) % ((className, ) + tuple(classConfusions))

        return [good, bad, confusion]




    def test(self,images,classNames,disp=None,subset=-1,savedata=None,verbose=True):
        """
        Test the classifier.
        images paramater can take in a list of paths or a list of imagesets
        images - the order of the paths or imagesets must be in the same order as the class type

        - Note all image classes must be in seperate directories
        - The class names must also align to the directories

        disp - if display is a display we show images and class label,
        otherwise nothing is done.

        subset - if subset = -1 we use the whole dataset. If subset = # then we
        use min(#images,subset)

        savedata - if save data is None nothing is saved. If savedata is a file
        name we save the data to a tab delimited file.

        verbose - print confusion matrix and file names
        returns [%Correct %Incorrect Confusion_Matrix]
        """
        count = 0
        correct = 0
        self.mClassNames = classNames
        colNames = []
        for extractor in self.mFeatureExtractors:
            colNames.extend(extractor.getFieldNames())
            self.mOrangeDomain = orange.Domain(map(orange.FloatVariable,colNames),orange.EnumVariable("type",values=self.mClassNames))

        dataset = []
        for i in range(len(classNames)):
            if ( isinstance(images[i],str) ):
                [dataset,cnt,crct] =self._testPath(images[i],classNames[i],dataset,subset,disp,verbose)
                count = count + cnt
                correct = correct + crct
            else:
                [dataset,cnt,crct] =self._testImageSet(images[i],classNames[i],dataset,subset,disp,verbose)
                count = count + cnt
                correct = correct + crct


        testData = orange.ExampleTable(self.mOrangeDomain,dataset)

        if savedata is not None:
            orange.saveTabDelimited (savedata, testData)

        confusion = 0
        if( len(self.mClassNames) > 2 ):
            crossValidator = orngTest.learnAndTestOnTestData([orange.BayesLearner()],self.mDataSetOrange,testData)
            confusion = orngStat.confusionMatrices(crossValidator)[0]

        good = 100*(float(correct)/float(count))
        bad = 100*(float(count-correct)/float(count))
        if verbose:
            print("Correct: "+str(good))
            print("Incorrect: "+str(bad))
            classes = self.mDataSetOrange.domain.classVar.values
            print "\t"+"\t".join(classes)
            for className, classConfusions in zip(classes, confusion):
                print ("%s" + ("\t%i" * len(classes))) % ((className, ) + tuple(classConfusions))

        return [good, bad, confusion]

    def _testPath(self,path,className,dataset,subset,disp,verbose):
        count = 0
        correct = 0
        badFeat = False
        files = []
        for ext in IMAGE_FORMATS:
            files.extend(glob.glob( os.path.join(path, ext)))
        if(subset > 0):
            nfiles = min(subset,len(files))
        else:
            nfiles = len(files)
        for i in range(nfiles):
            infile = files[i]
            if verbose:
                print "Opening file: " + infile
            img = Image(infile)
            featureVector = []
            for extractor in self.mFeatureExtractors:
                feats = extractor.extract(img)
                if( feats is not None ):
                    featureVector.extend(feats)
                else:
                    badFeat = True
            if( badFeat ):
                del img
                badFeat = False
                continue
            featureVector.extend([className])
            dataset.append(featureVector)
            test = orange.ExampleTable(self.mOrangeDomain,[featureVector])
            c = self.mClassifier(test[0])
            testClass = test[0].getclass()
            if(testClass==c):
                text =  "Classified as " + str(c)
                self._WriteText(disp,img,text, Color.GREEN)
                correct = correct + 1
            else:
                text =  "Mislassified as " + str(c)
                self._WriteText(disp,img,text, Color.RED)
            count = count + 1
            del img

        return([dataset,count,correct])

    def _testImageSet(self,imageset,className,dataset,subset,disp,verbose):
        count = 0
        correct = 0
        badFeat = False
        if(subset > 0):
            imageset = imageset[0:subset]
        for img in imageset:
            if verbose:
                print "Opening file: " + img.filename
            featureVector = []
            for extractor in self.mFeatureExtractors:
                feats = extractor.extract(img)
                if( feats is not None ):
                    featureVector.extend(feats)
                else:
                    badFeat = True
            if( badFeat ):
                del img
                badFeat = False
                continue 
            featureVector.extend([className])
            dataset.append(featureVector)
            test = orange.ExampleTable(self.mOrangeDomain,[featureVector])
            c = self.mClassifier(test[0])
            testClass = test[0].getclass()
            if(testClass==c):
                text =  "Classified as " + str(c)
                self._WriteText(disp,img,text, Color.GREEN)
                correct = correct + 1
            else:   
                text =  "Mislassified as " + str(c)
                self._WriteText(disp,img,text, Color.RED)
            count = count + 1
            del img
            
        return([dataset,count,correct])

    def _WriteText(self, disp, img, txt,color):
        if(disp is not None):
            txt = ' ' + txt + ' '
            img = img.adaptiveScale(disp.resolution)
            layer = DrawingLayer((img.width,img.height))
            layer.setFontSize(60)
            layer.ezViewText(txt,(20,20),fgcolor=color)
            img.addDrawingLayer(layer)
            img.applyLayers()
            img.save(disp)

########NEW FILE########
__FILENAME__ = flickrapi2
#!/usr/bin/python
#
# Flickr API implementation
#
# Inspired largely by Michele Campeotto's flickrclient and Aaron Swartz'
# xmltramp... but I wanted to get a better idea of how python worked in
# those regards, so I mostly worked those components out for myself.
#
# http://micampe.it/things/flickrclient
# http://www.aaronsw.com/2002/xmltramp/
#
# Release 1: initial release
# Release 2: added upload functionality
# Release 3: code cleanup, convert to doc strings
# Release 4: better permission support
# Release 5: converted into fuller-featured "flickrapi"
# Release 6: fix upload sig bug (thanks Deepak Jois), encode test output
# Release 7: fix path construction, Manish Rai Jain's improvements, exceptions
# Release 8: change API endpoint to "api.flickr.com"
#
# Work by (or inspired by) Manish Rai Jain <manishrjain@gmail.com>:
#
#    improved error reporting, proper multipart MIME boundary creation,
#    use of urllib2 to allow uploads through a proxy, upload accepts
#    raw data as well as a filename
#
# Copyright 2005 Brian "Beej Jorgensen" Hall <beej@beej.us>
#
#    This work is licensed under the Creative Commons
#    Attribution License.  To view a copy of this license,
#    visit http://creativecommons.org/licenses/by/2.5/ or send
#    a letter to Creative Commons, 543 Howard Street, 5th
#    Floor, San Francisco, California, 94105, USA.
#
# This license says that I must be credited for any derivative works.
# You do not need to credit me to simply use the FlickrAPI classes in
# your Python scripts--you only need to credit me if you're taking this
# FlickrAPI class and modifying it or redistributing it.
#
# Previous versions of this API were granted to the public domain.
# You're free to use those as you please.
#
# Beej Jorgensen, Maintainer, November 2005
# beej@beej.us
#

import sys
import md5
import string
import urllib
import urllib2
import mimetools
import httplib
import os.path
import xml.dom.minidom

########################################################################
# Exceptions
########################################################################

class UploadException(Exception):
    pass

########################################################################
# XML functionality
########################################################################

#-----------------------------------------------------------------------
class XMLNode:
    """XMLNode -- generic class for holding an XML node

    xmlStr = \"\"\"<xml foo="32">
    <name bar="10">Name0</name>
    <name bar="11" baz="12">Name1</name>
    </xml>\"\"\"

    f = XMLNode.parseXML(xmlStr)

    print f.elementName              # xml
    print f['foo']                   # 32
    print f.name                     # [<name XMLNode>, <name XMLNode>]
    print f.name[0].elementName      # name
    print f.name[0]["bar"]           # 10
    print f.name[0].elementText      # Name0
    print f.name[1].elementName      # name
    print f.name[1]["bar"]           # 11
    print f.name[1]["baz"]           # 12

    """

    def __init__(self):
        """Construct an empty XML node."""
        self.elementName=""
        self.elementText=""
        self.attrib={}
        self.xml=""

    def __setitem__(self, key, item):
        """Store a node's attribute in the attrib hash."""
        self.attrib[key] = item

    def __getitem__(self, key):
        """Retrieve a node's attribute from the attrib hash."""
        try:
            return self.attrib[key]
        except:
            return "null"
    #-----------------------------------------------------------------------
    #@classmethod
    def parseXML(cls, xmlStr, storeXML=False):
        """Convert an XML string into a nice instance tree of XMLNodes.

        xmlStr -- the XML to parse
        storeXML -- if True, stores the XML string in the root XMLNode.xml

        """

        def __parseXMLElement(element, thisNode):
            """Recursive call to process this XMLNode."""
            thisNode.elementName = element.nodeName

            #print element.nodeName

            # add element attributes as attributes to this node
            for i in range(element.attributes.length):
                an = element.attributes.item(i)
                thisNode[an.name] = an.nodeValue

            for a in element.childNodes:
                if a.nodeType == xml.dom.Node.ELEMENT_NODE:

                    child = XMLNode()
                    try:
                        list = getattr(thisNode, a.nodeName)
                    except AttributeError:
                        setattr(thisNode, a.nodeName, [])

                    # add the child node as an attrib to this node
                    list = getattr(thisNode, a.nodeName);
                    #print "appending child: %s to %s" % (a.nodeName, thisNode.elementName)
                    list.append(child);

                    __parseXMLElement(a, child)

                elif a.nodeType == xml.dom.Node.TEXT_NODE:
                    thisNode.elementText += a.nodeValue
    
            return thisNode

        dom = xml.dom.minidom.parseString(xmlStr)

        # get the root
        rootNode = XMLNode()
        if storeXML: rootNode.xml = xmlStr

        return __parseXMLElement(dom.firstChild, rootNode)
    parseXML = classmethod(parseXML)

########################################################################
# Flickr functionality
########################################################################

#-----------------------------------------------------------------------
class FlickrAPI:
    """Encapsulated flickr functionality.

    Example usage:

      flickr = FlickrAPI(flickrAPIKey, flickrSecret)
      rsp = flickr.auth_checkToken(api_key=flickrAPIKey, auth_token=token)

    """
    flickrHost = "api.flickr.com"
    flickrRESTForm = "/services/rest/"
    flickrAuthForm = "/services/auth/"
    flickrUploadForm = "/services/upload/"

    #-------------------------------------------------------------------
    def __init__(self, apiKey, secret):
        """Construct a new FlickrAPI instance for a given API key and secret."""
        self.apiKey = apiKey
        self.secret = secret

        self.__handlerCache={}

    #-------------------------------------------------------------------
    def __sign(self, data):
        """Calculate the flickr signature for a set of params.

        data -- a hash of all the params and values to be hashed, e.g.
                {"api_key":"AAAA", "auth_token":"TTTT"}

        """
        dataName = self.secret
        keys = data.keys()
        keys.sort()
        for a in keys: dataName += (a + data[a])
        #print dataName
        hash = md5.new()
        hash.update(dataName)
        return hash.hexdigest()

    #-------------------------------------------------------------------
    def __getattr__(self, method, **arg):
        """Handle all the flickr API calls.
        
        This is Michele Campeotto's cleverness, wherein he writes a
        general handler for methods not defined, and assumes they are
        flickr methods.  He then converts them to a form to be passed as
        the method= parameter, and goes from there.

        http://micampe.it/things/flickrclient

        My variant is the same basic thing, except it tracks if it has
        already created a handler for a specific call or not.

        example usage:

                flickr.auth_getFrob(api_key="AAAAAA")
                rsp = flickr.favorites_getList(api_key=flickrAPIKey, \\
                        auth_token=token)

        """

        if not self.__handlerCache.has_key(method):
            def handler(_self = self, _method = method, **arg):
                _method = "flickr." + _method.replace("_", ".")
                url = "http://" + FlickrAPI.flickrHost + \
                        FlickrAPI.flickrRESTForm
                arg["method"] = _method
                postData = urllib.urlencode(arg) + "&api_sig=" + \
                        _self.__sign(arg)
                #print "--url---------------------------------------------"
                #print url
                #print "--postData----------------------------------------"
                #print postData
                f = urllib.urlopen(url, postData)
                data = f.read()
                #print "--response----------------------------------------"
                #print data
                f.close()
                return XMLNode.parseXML(data, True)

            self.__handlerCache[method] = handler;

        return self.__handlerCache[method]

    #-------------------------------------------------------------------
    def __getAuthURL(self, perms, frob):
        """Return the authorization URL to get a token.

        This is the URL the app will launch a browser toward if it
        needs a new token.
                
        perms -- "read", "write", or "delete"
        frob -- picked up from an earlier call to FlickrAPI.auth_getFrob()

        """

        data = {"api_key": self.apiKey, "frob": frob, "perms": perms}
        data["api_sig"] = self.__sign(data)
        return "http://%s%s?%s" % (FlickrAPI.flickrHost, \
                FlickrAPI.flickrAuthForm, urllib.urlencode(data))

    #-------------------------------------------------------------------
    def upload(self, filename=None, jpegData=None, **arg):
        """Upload a file to flickr.

        Be extra careful you spell the parameters correctly, or you will
        get a rather cryptic "Invalid Signature" error on the upload!

        Supported parameters:

        One of filename or jpegData must be specified by name when 
        calling this method:

        filename -- name of a file to upload
        jpegData -- array of jpeg data to upload

        api_key
        auth_token
        title
        description
        tags -- space-delimited list of tags, "tag1 tag2 tag3"
        is_public -- "1" or "0"
        is_friend -- "1" or "0"
        is_family -- "1" or "0"

        """

        if filename == None and jpegData == None or \
                filename != None and jpegData != None:

            raise UploadException("filename OR jpegData must be specified")

        # verify key names
        for a in arg.keys():
            if a != "api_key" and a != "auth_token" and a != "title" and \
                    a != "description" and a != "tags" and a != "is_public" and \
                    a != "is_friend" and a != "is_family":

                sys.stderr.write("FlickrAPI: warning: unknown parameter " \
                        "\"%s\" sent to FlickrAPI.upload\n" % (a))

        arg["api_sig"] = self.__sign(arg)
        url = "http://" + FlickrAPI.flickrHost + FlickrAPI.flickrUploadForm

        # construct POST data
        boundary = mimetools.choose_boundary()
        body = ""

        # required params
        for a in ('api_key', 'auth_token', 'api_sig'):
            body += "--%s\r\n" % (boundary)
            body += "Content-Disposition: form-data; name=\""+a+"\"\r\n\r\n"
            body += "%s\r\n" % (arg[a])

        # optional params
        for a in ('title', 'description', 'tags', 'is_public', \
                'is_friend', 'is_family'):

            if arg.has_key(a):
                body += "--%s\r\n" % (boundary)
                body += "Content-Disposition: form-data; name=\""+a+"\"\r\n\r\n"
                body += "%s\r\n" % (arg[a])

        body += "--%s\r\n" % (boundary)
        body += "Content-Disposition: form-data; name=\"photo\";"
        body += " filename=\"%s\"\r\n" % filename
        body += "Content-Type: image/jpeg\r\n\r\n"

        #print body

        if filename != None:
            fp = file(filename, "rb")
            data = fp.read()
            fp.close()
        else:
            data = jpegData

        postData = body.encode("utf_8") + data + \
                ("--%s--" % (boundary)).encode("utf_8")

        request = urllib2.Request(url)
        request.add_data(postData)
        request.add_header("Content-Type", \
                "multipart/form-data; boundary=%s" % boundary)
        response = urllib2.urlopen(request)
        rspXML = response.read()

        return XMLNode.parseXML(rspXML)


    #-----------------------------------------------------------------------
    #@classmethod
    def testFailure(cls, rsp, exit=True):
        """Exit app if the rsp XMLNode indicates failure."""
        if rsp['stat'] == "fail":
            sys.stderr.write("%s\n" % (cls.getPrintableError(rsp)))
            if exit: sys.exit(1)
    testFailure = classmethod(testFailure)

    #-----------------------------------------------------------------------
    #@classmethod
    def getPrintableError(cls, rsp):
        """Return a printed error message string."""
        return "%s: error %s: %s" % (rsp.elementName, \
                cls.getRspErrorCode(rsp), cls.getRspErrorMsg(rsp))
    getPrintableError = classmethod(getPrintableError)

    #-----------------------------------------------------------------------
    #@classmethod
    def getRspErrorCode(cls, rsp):
        """Return the error code of a response, or 0 if no error."""
        if rsp['stat'] == "fail":
            return rsp.err[0]['code']

        return 0
    getRspErrorCode = classmethod(getRspErrorCode)

    #-----------------------------------------------------------------------
    #@classmethod
    def getRspErrorMsg(cls, rsp):
        """Return the error message of a response, or "Success" if no error."""
        if rsp['stat'] == "fail":
            return rsp.err[0]['msg']

        return "Success"
    getRspErrorMsg = classmethod(getRspErrorMsg)

    #-----------------------------------------------------------------------
    def __getCachedTokenPath(self):
        """Return the directory holding the app data."""
        return os.path.expanduser(os.path.sep.join(["~", ".flickr", \
                self.apiKey]))

    #-----------------------------------------------------------------------
    def __getCachedTokenFilename(self):
        """Return the full pathname of the cached token file."""
        return os.path.sep.join([self.__getCachedTokenPath(), "auth.xml"])

    #-----------------------------------------------------------------------
    def __getCachedToken(self):
        """Read and return a cached token, or None if not found.

        The token is read from the cached token file, which is basically the
        entire RSP response containing the auth element.
        """

        try:
            f = file(self.__getCachedTokenFilename(), "r")
            
            data = f.read()
            f.close()

            rsp = XMLNode.parseXML(data)

            return rsp.auth[0].token[0].elementText

        except IOError:
            return None

    #-----------------------------------------------------------------------
    def __setCachedToken(self, xml):
        """Cache a token for later use.

        The cached tag is stored by simply saving the entire RSP response
        containing the auth element.

        """

        path = self.__getCachedTokenPath()
        if not os.path.exists(path):
            os.makedirs(path)

        f = file(self.__getCachedTokenFilename(), "w")
        f.write(xml)
        f.close()


    #-----------------------------------------------------------------------
    def getToken(self, perms="read", browser="lynx"):
        """Get a token either from the cache, or make a new one from the
        frob.

        This first attempts to find a token in the user's token cache on
        disk.
        
        If that fails (or if the token is no longer valid based on
        flickr.auth.checkToken) a new frob is acquired.  The frob is
        validated by having the user log into flickr (with lynx), and
        subsequently a valid token is retrieved.

        The newly minted token is then cached locally for the next run.

        perms--"read", "write", or "delete"
        browser--whatever browser should be used in the system() call

        """
        
        # see if we have a saved token
        token = self.__getCachedToken()

        # see if it's valid
        if token != None:
            rsp = self.auth_checkToken(api_key=self.apiKey, auth_token=token)
            if rsp['stat'] != "ok":
                token = None
            else:
                # see if we have enough permissions
                tokenPerms = rsp.auth[0].perms[0].elementText
                if tokenPerms == "read" and perms != "read": token = None
                elif tokenPerms == "write" and perms == "delete": token = None

        # get a new token if we need one
        if token == None:
            # get the frob
            rsp = self.auth_getFrob(api_key=self.apiKey)
            self.testFailure(rsp)

            frob = rsp.frob[0].elementText

            # validate online
            os.system("%s '%s'" % (browser, self.__getAuthURL(perms, frob)))

            # get a token
            rsp = self.auth_getToken(api_key=self.apiKey, frob=frob)
            self.testFailure(rsp)

            token = rsp.auth[0].token[0].elementText

            # store the auth info for next time
            self.__setCachedToken(rsp.xml)

        return token

########################################################################
# App functionality
########################################################################

def main(argv):
    # flickr auth information:
    flickrAPIKey = "fa33550d413b36b3fddc473a931a3b3b"  # API key
    flickrSecret = "7fd481bff0916055"                  # shared "secret"

    # make a new FlickrAPI instance
    fapi = FlickrAPI(flickrAPIKey, flickrSecret)

    # do the whole whatever-it-takes to get a valid token:
    token = fapi.getToken(browser="chrome")

    # get my favorites
    rsp = fapi.favorites_getList(api_key=flickrAPIKey,auth_token=token)
    fapi.testFailure(rsp)

    # and print them
    for a in rsp.photos[0].photo:
        print "%10s: %s" % (a['id'], a['title'].encode("ascii", "replace"))

    # upload the file foo.jpg
    #rsp = fapi.upload(filename="foo.jpg", \
    #       api_key=flickrAPIKey, auth_token=token, \
    #       title="This is the title", description="This is the description", \
    #       tags="tag1 tag2 tag3", is_public="1")
    #if rsp == None:
    #       sys.stderr.write("can't find file\n")
    #else:
    #       fapi.testFailure(rsp)

    return 0

# run the main if we're not being imported:
if __name__ == "__main__": sys.exit(main(sys.argv))


########NEW FILE########
__FILENAME__ = get_imgs_geo_gps_search
#!/usr/bin/python
#
# So this script is in a bit of a hack state right now.
# This script reads
#
#
#


# Graciously copied and modified from:
# http://graphics.cs.cmu.edu/projects/im2gps/flickr_code.html
#Image querying script written by Tamara Berg,
#and extended heavily James Hays

#9/26/2007 added dynamic timeslices to query more efficiently.
#8/18/2008 added new fields and set maximum time slice.
#8/19/2008 this is a much simpler function which gets ALL geotagged photos of
# sufficient accuracy.  No queries, no negative constraints.
# divides up the query results into multiple files
# 1/5/2009
# now uses date_taken instead of date_upload to get more diverse blocks of images
# 1/13/2009 - uses the original im2gps keywords, not as negative constraints though

import sys, string, math, time, socket
from flickrapi2 import FlickrAPI
from datetime import datetime
import pycurl
import os
import shutil
socket.setdefaulttimeout(30)  #30 second time out on sockets before they throw
#an exception.  I've been having trouble with urllib.urlopen hanging in the
#flickr API.  This will show up as exceptions.IOError.

#the time out needs to be pretty long, it seems, because the flickr servers can be slow
#to respond to our big searches.

#returns a query and the search times to attempt to get a desired number of photos
#this needs serious refactoring -KAS
def DoSearch(fapi,query_string,desired_photos):
    # number of seconds to skip per query
    #timeskip = 62899200 #two years
    #timeskip = 604800  #one week
    timeskip = 172800  #two days
    #timeskip = 86400 #one day
    #timeskip = 3600 #one hour
    #timeskip = 2257 #for resuming previous query

    #mintime = 1121832000 #from im2gps
    #mintime = 1167407788 # resume crash england
    #mintime = 1177828976 #resume crash japan
    #mintime = 1187753798 #resume crash greece
    mintime = 1171416400 #resume crash WashingtonDC
    maxtime = mintime+timeskip
    endtime = 1192165200  #10/12/2007, at the end of im2gps queries



    print datetime.fromtimestamp(mintime)
    print datetime.fromtimestamp(endtime)

    while (maxtime < endtime):

        #new approach - adjust maxtime until we get the desired number of images
        #within a block. We'll need to keep upper bounds and lower
        #lower bound is well defined (mintime), but upper bound is not. We can't
        #search all the way from endtime.

        lower_bound = mintime + 900 #lower bound OF the upper time limit. must be at least 15 minutes or zero results
        upper_bound = mintime + timeskip * 20 #upper bound of the upper time limit
        maxtime     = .95 * lower_bound + .05 * upper_bound

        print '\nBinary search on time range upper bound'
        print 'Lower bound is ' + str(datetime.fromtimestamp(lower_bound))
        print 'Upper bound is ' + str(datetime.fromtimestamp(upper_bound))

        keep_going = 6 #search stops after a fixed number of iterations
        while( keep_going > 0 and maxtime < endtime):
            try:
                rsp = fapi.photos_search(api_key=flickrAPIKey,
                                        ispublic="1",
                                        media="photos",
                                        per_page="250",
                                        page="1",
                                        has_geo = "0", #bbox="-180, -90, 180, 90",
                                        text=query_string,
                                        accuracy="6", #6 is region level.
                                        min_upload_date=str(mintime),
                                        max_upload_date=str(maxtime))

                #we want to catch these failures somehow and keep going.
                time.sleep(1)
                fapi.testFailure(rsp)
                total_images = rsp.photos[0]['total'];
                null_test = int(total_images); #want to make sure this won't crash later on for some reason
                null_test = float(total_images);

                print '\nnumimgs: ' + total_images
                print 'mintime: ' + str(mintime) + ' maxtime: ' + str(maxtime) + ' timeskip:  ' + str(maxtime - mintime)

                if( int(total_images) > desired_photos ):
                    print 'too many photos in block, reducing maxtime'
                    upper_bound = maxtime
                    maxtime = (lower_bound + maxtime) / 2 #midpoint between current value and lower bound.

                if( int(total_images) < desired_photos):
                    print 'too few photos in block, increasing maxtime'
                    lower_bound = maxtime
                    maxtime = (upper_bound + maxtime) / 2

                print 'Lower bound is ' + str(datetime.fromtimestamp(lower_bound))
                print 'Upper bound is ' + str(datetime.fromtimestamp(upper_bound))

                if( int(total_images) > 0): #only if we're not in a degenerate case
                    keep_going = keep_going - 1
                else:
                    upper_bound = upper_bound + timeskip;

            except KeyboardInterrupt:
                print('Keyboard exception while querying for images, exiting\n')
                raise
            except:
                print sys.exc_info()[0]
                #print type(inst)     # the exception instance
                #print inst.args      # arguments stored in .args
                #print inst           # __str__ allows args to printed directly
                print ('Exception encountered while querying for images\n')

        #end of while binary search
        print 'finished binary search'
        return([mintime,maxtime,total_images,rsp])



###########################################################################
# Modify this section to reflect your data and specific search
###########################################################################
# flickr auth information:
# change these to your flickr api keys and secret
flickrAPIKey = "fa33550d413b36b3fddc473a931a3b3b"  # API key
flickrSecret = "7fd481bff0916055"                  # shared "secret"
rootpath = "../data/" #where do you want the data
desired_photos = 1000 #how many photos do you want to try and get
query_file_name = 'query.dat' #The file to get the queries from



#query_file_name = 'place_rec_queries_fall08.txt'
query_file = open(query_file_name, 'r')

#aggregate all of the positive and negative queries together.
pos_queries = []  #an empty list
neg_queries = ''  #a string
num_queries = 0

for line in query_file:
    if line[0] != '#' and len(line) > 1:  #line end character is 2 long?
        print line[0:len(line)-1]
        if line[0] != '-':
            pos_queries = pos_queries + [line[0:len(line)-1]]
            num_queries = num_queries + 1
        if line[0] == '-':
            neg_queries = neg_queries + ' ' + line[0:len(line)-1]

query_file.close()
print 'positive queries:  '
print pos_queries
print 'negative queries:  ' + neg_queries
print 'num_queries = ' + str(num_queries)
#this is the desired number of photos in each block


# make a new FlickrAPI instance
fapi = FlickrAPI(flickrAPIKey, flickrSecret)

for current_tag in range(0, num_queries):

    print('TOP OF LOOP')
    # change this to the location where you want to put your output file
    try:
        stats = os.stat(rootpath)
    except OSError:
        os.mkdir(rootpath)

    outpath = rootpath+pos_queries[current_tag]+'/'
    try:
        os.mkdir(outpath)
    except OSError:
        shutil.rmtree(outpath,True)
        os.mkdir(outpath)

    out_file = open(rootpath + pos_queries[current_tag] + '.txt','w')
    ###########################################################################

    #form the query string.
    query_string = pos_queries[current_tag] + ' ' + neg_queries
    print '\n\nquery_string is ' + query_string

    total_images_queried = 0;
    [mintime,maxtime,total_images,rsp] = DoSearch(fapi,query_string,desired_photos)

    print('GETTING TOTATL IMAGES:'+str(total_images))
    s = '\nmintime: ' + str(mintime) + ' maxtime: ' + str(maxtime)
    print s
    out_file.write(s + '\n')
    i = getattr(rsp,'photos',None)
    if i:

        s = 'numimgs: ' + total_images
        print s
        out_file.write(s + '\n')

        current_image_num = 1;

        num = 4 # CHANGE THIS BACK int(rsp.photos[0]['pages'])
        s =  'total pages: ' + str(num)
        print s
        out_file.write(s + '\n')

        #only visit 16 pages max, to try and avoid the dreaded duplicate bug
        #16 pages = 4000 images, should be duplicate safe.  Most interesting pictures will be taken.

        num_visit_pages = min(16,num)

        s = 'visiting only ' + str(num_visit_pages) + ' pages ( up to ' + str(num_visit_pages * 250) + ' images)'
        print s
        out_file.write(s + '\n')

        total_images_queried = total_images_queried + min((num_visit_pages * 250), int(total_images))

        #print 'stopping before page ' + str(int(math.ceil(num/3) + 1)) + '\n'

        pagenum = 1;

        counter = -1
        while( pagenum <= num_visit_pages ):
        #for pagenum in range(1, num_visit_pages + 1):  #page one is searched twice

            print '  page number ' + str(pagenum)

            try:
                print("PAGE")
                print(pagenum)
                # WARNING THIS QUERY HAS TO MATCH THE SEARCH QUERY!!!!
                rsp = fapi.photos_search(api_key=flickrAPIKey,
                                        ispublic="1",
                                        media="photos",
                                        per_page="250",
                                        page=str(pagenum),
                                        has_geo = "0",
                                        text=query_string,
                                        #extras = "tags, original_format, license, geo, date_taken, date_upload, o_dims, views",
                                        #accuracy="6", #6 is region level.
                                        min_upload_date=str(1121832000),#mintime),
                                        max_upload_date=str(1192165200))#maxtime))

                #rsp = fapi.photos_search(api_key=flickrAPIKey,
                 #                   ispublic="1",
                  #                  media="photos",
                   #                 per_page="250",
                    #                page='0', #str(pagenum),
                     #               sort="interestingness-desc",
                      #              has_geo = "0", #bbox="-180, -90, 180, 90",
                       #             text=query_string,
                        #            #accuracy="6", #6 is region level.  most things seem 10 or better.
                         #           extras = "tags, original_format, license, geo, date_taken, date_upload, o_dims, views",
                          #          min_upload_date=str(mintime),
                           #         max_upload_date=str(maxtime))
                                    ##min_taken_date=str(datetime.fromtimestamp(mintime)),
                                    ##max_taken_date=str(datetime.fromtimestamp(maxtime)))
                time.sleep(1)
                fapi.testFailure(rsp)
            except KeyboardInterrupt:
                print('Keyboard exception while querying for images, exiting\n')
                raise
            except:
                print sys.exc_info()[0]
                #print type(inst)     # the exception instance
                #print inst.args      # arguments stored in .args
                #print inst           # __str__ allows args to printed directly
                print ('Exception encountered while querying for images\n')
            else:
                print('got a response')
                # and print them
                k = getattr(rsp,'photos',None)
                if k:
                    print('In K')
                    m = getattr(rsp.photos[0],'photo',None)
                    if m:
                        print('In    M')
                        for b in rsp.photos[0].photo:
                            print('In b')
                            if b!=None:
                                counter = counter + 1
                                ##print(http://farm{farm-id}.static.flickr.com/{server-id}/{id}_{secret}.jpg)

                                myurl = 'http://farm'+b['farm']+".static.flickr.com/"+b['server']+"/"+b['id']+"_"+b['secret']+'.jpg'
                                fname = outpath+pos_queries[current_tag]+str(counter)+'.jpg' #b['id']+"_"+b['secret']+'.jpg'
                                print(myurl)
                                print(fname)
                                mycurl = pycurl.Curl()
                                mycurl.setopt(pycurl.URL, str(myurl))
                                myfile = open(fname,"wb")
                                mycurl.setopt(pycurl.WRITEDATA, myfile)
                                mycurl.setopt(pycurl.FOLLOWLOCATION, 1)
                                mycurl.setopt(pycurl.MAXREDIRS, 5)
                                mycurl.setopt(pycurl.NOSIGNAL, 1)
                                mycurl.perform()
                                mycurl.close()
                                myfile.close()
                                out_file.write('URL: '+myurl+'\n')
                                out_file.write('File: '+ fname+'\n')
                                out_file.write('photo: ' + b['id'] + ' ' + b['secret'] + ' ' + b['server'] + '\n')
                                out_file.write('owner: ' + b['owner'] + '\n')
                                out_file.write('title: ' + b['title'].encode("ascii","replace") + '\n')

                                out_file.write('originalsecret: ' + b['originalsecret'] + '\n')
                                out_file.write('originalformat: ' + b['originalformat'] + '\n')
                                out_file.write('o_height: ' + b['o_height'] + '\n')
                                out_file.write('o_width: ' + b['o_width'] + '\n')
                                out_file.write('datetaken: ' + b['datetaken'].encode("ascii","replace") + '\n')
                                out_file.write('dateupload: ' + b['dateupload'].encode("ascii","replace") + '\n')

                                out_file.write('tags: ' + b['tags'].encode("ascii","replace") + '\n')

                                out_file.write('license: ' + b['license'].encode("ascii","replace") + '\n')
                                out_file.write('latitude: '  + b['latitude'].encode("ascii","replace") + '\n')
                                out_file.write('longitude: ' + b['longitude'].encode("ascii","replace") + '\n')
                                out_file.write('accuracy: '  + b['accuracy'].encode("ascii","replace") + '\n')

                                out_file.write('views: ' + b['views'] + '\n')
                                out_file.write('interestingness: ' + str(current_image_num) + ' out of ' + str(total_images) + '\n');
                                out_file.write('\n')
                                current_image_num = current_image_num + 1;

            print('')
            pagenum = pagenum + 1;  #this is in the else exception block.  Itwon't increment for a failure.
            #this block is indented such that it will only run if there are no exceptions
            #in the original query.  That means if there are exceptions, mintime won't be incremented
            #and it will try again
            timeskip = maxtime - mintime #used for initializing next binary search
            mintime  = maxtime

    out_file.write('Total images queried: ' + str(total_images_queried) + '\n')
    out_file.close

########NEW FILE########
__FILENAME__ = ShapeContextClassifier
from SimpleCV.base import *
from SimpleCV.Features.Features import Feature, FeatureSet
from SimpleCV.Color import Color
from SimpleCV.ImageClass import Image
from SimpleCV.Features.Detection import ShapeContextDescriptor
import math
import scipy.stats as sps


"""
Classify an object based on shape context
"""
class ShapeContextClassifier():

    def  __init__(self,images,labels):
        """
        Create a shape context classifier.

        * *images* - a list of input binary images where the things
          to be detected are white.

        * *labels* - the names of each class of objects.
        """
        # The below import has been done in init since it throws "Need scikits learn installed" for $import SimpleCV
        try:
            from sklearn import neighbors
        except:
            print "Need scikits learn installed"

        self.imgMap = {}
        self.ptMap = {}
        self.descMap = {}
        self.knnMap = {}
        self.blobCount = {}
        self.labels = labels
        self.images = images
        import warnings
        warnings.simplefilter("ignore")
        for i in range(0,len(images)):
            print "precomputing " + images[i].filename
            self.imgMap[labels[i]] = images[i]

            pts,desc,count  = self._image2FeatureVector(images[i])
            self.blobCount[labels[i]] = count
            self.ptMap[labels[i]] = pts
            self.descMap[labels[i]] = desc
            knn = neighbors.KNeighborsClassifier()
            knn.fit(desc,range(0,len(pts)))
            self.knnMap[labels[i]] = knn

    def _image2FeatureVector(self,img):
        """
        generate a list of points, SC descriptors, and the count of points
        """
        #IMAGES MUST BE WHITE ON BLACK!
        fulllist = []
        raw_descriptors = []
        blobs = img.findBlobs(minsize=50)
        count = 0
        if( blobs is not None ):
            count = len(blobs)
            for b in blobs:
                fulllist += b._filterSCPoints()
                raw_descriptors = blobs[0]._generateSC(fulllist)
        return fulllist,raw_descriptors,count


    def _getMatch(self,model_scd,test_scd):
        correspondence,distance = self._doMatching(model_scd,test_scd)
        return self._matchQuality(distances)

    def _doMatching(self,model_name,test_scd):
        myPts = len(test_scd)
        otPts = len(self.ptMap[model_name])
        # some magic metric that keeps features
        # with a lot of points from dominating
        #metric = 1.0 + np.log10( np.max([myPts,otPts])/np.min([myPts,otPts])) # <-- this could be moved to after the sum
        otherIdx = []
        distance = []
        import warnings
        warnings.simplefilter("ignore")
        results = []
        for sample in test_scd:
            best = self.knnMap[model_name].predict(sample)
            idx = best[0] # this is where we can play with k
            scd = self.descMap[model_name][idx]
            temp = np.sqrt(np.sum(((sample-scd)**2)))
            #temp = 0.5*np.sum((sample-scd)**2)/np.sum((sample+scd))
            if( math.isnan(temp) ):
                temp = sys.maxint
            distance.append(temp)
        return [otherIdx,distance]

    def _matchQuality(self,distances):
        #distances = np.array(distances)
        #sd = np.std(distances)
        #x = np.mean(distances)
        #min = np.min(distances)
        # not sure trimmed mean is perfect
        # realistically we should have some bimodal dist
        # and we want to throw away stuff with awful matches
        # so long as the number of points is not a huge
        # chunk of our points.
        #tmean = sps.tmean(distances,(min,x+sd))
        tmean = np.mean(distances)
        std = np.std(distances)
        return tmean,std


    def _buildMatchDict(self,image, countBlobs):
        # we may want to base the count on the num best_matchesber of large blobs
        points,descriptors,count = self._image2FeatureVector(image)
        matchDict = {}
        matchStd = {}
        for key,value in self.descMap.items():
            if( countBlobs and self.blobCount[key] == count ): # only do matching for similar number of blobs
                #need to hold on to correspondences
                correspondence, distances = self._doMatching(key,descriptors)
                result,std = self._matchQuality(distances)
                matchDict[key] = result
                matchStd[key] = std
            elif( not countBlobs ):
                correspondence, distances = self._doMatching(key,descriptors)
                result,std = self._matchQuality(distances)
                matchDict[key] = result
                matchStd[key] = std

        return points,descriptors,count,matchDict, matchStd

    def classify(self,image, blobFilter=True):
        """
        Classify an input image.

        * *image* - the input binary image.
        * *blobFilter* - Do a first pass where you only match objects
          that have the same number of blobs - speeds up computation
          and match quality.
        """
        points,descriptors,count,matchDict,matchStd = self._buildMatchDict(image, blobFilter)
        best = sys.maxint
        best_name = "No Match"
        for k,v in matchDict.items():
            if ( v < best ):
                best = v
                best_name = k

        return best_name, best, matchDict, matchStd

    def getTopNMatches(self,image,n=3, blobFilter = True):
        """
        Classify an input image and return the top n results.

        * *image* - the input binary image.
        * *n* - the number of results to return.
        * *blobFilter* - Do a first pass where you only match objects
          that have the same number of blobs - speeds up computation
          and match quality.
        """
        n = np.clip(n,1,len(self.labels))
        points,descriptors,count,matchDict,matchStd = self._buildMatchDict(image,blobFilter)
        best_matches = list(sorted(matchDict, key=matchDict.__getitem__))
        retList = []
        for k in best_matches:
            retList.append((k,matchDict[k]))
        return retList[0:n], matchDict, matchStd

########NEW FILE########
__FILENAME__ = SVMClassifier
from SimpleCV.base import *
from SimpleCV.ImageClass import Image, ImageSet
from SimpleCV.DrawingLayer import *
from SimpleCV.Features import FeatureExtractorBase
"""
This class is encapsulates almost everything needed to train, test, and deploy a
multiclass support vector machine for an image classifier. Training data should
be stored in separate directories for each class. This class uses the feature
extractor base class to  convert images into a feature vector. The basic workflow
is as follows.
1. Get data.
2. Setup Feature Extractors (roll your own or use the ones I have written).
3. Train the classifier.
4. Test the classifier.
5. Tweak parameters as necessary.
6. Repeat until you reach the desired accuracy.
7. Save the classifier.
8. Deploy using the classify method.
"""
class SVMClassifier:
    """
    This class encapsulates a Naive Bayes Classifier.
    See:
    http://en.wikipedia.org/wiki/Support_vector_machine
    """
    mClassNames = []
    mDataSetRaw = []
    mDataSetOrange = []
    mClassifier = None
    mFeatureExtractors = None
    mOrangeDomain = None
    mSVMPrototype = None

    mKernelType = {}

    mSVMType = {}

    mSVMProperties = {
        'KernelType':'RBF', #default is a RBF Kernel
        'SVMType':'NU',     #default is C
        'nu':None,          # NU for SVM NU
        'c':None,           #C for SVM C - the slack variable
        'degree':None,      #degree for poly kernels - defaults to 3
        'coef':None,        #coef for Poly/Sigmoid defaults to 0
        'gamma':None,       #kernel param for poly/rbf/sigma - default is 1/#samples
    }
    #human readable to CV constant property mapping

    def __init__(self,featureExtractors,properties=None):

        if not ORANGE_ENABLED:
            logger.warning("The required orange machine learning library is not installed")
            return None

        self.mKernelType = {
            'RBF':orange.SVMLearner.RBF, #Radial basis kernel
            'Linear':orange.SVMLearner.Linear, #Linear basis kernel
            'Poly':orange.SVMLearner.Polynomial, #Polynomial kernel
            'Sigmoid':orange.SVMLearner.Sigmoid #Sigmoid Kernel
        }

        self.mSVMType = {
            'NU':orange.SVMLearner.Nu_SVC,
            'C':orange.SVMLearner.C_SVC
        }

        self.mFeatureExtractors =  featureExtractors
        if(properties is not None):
            self.mSVMProperties = properties
        self._parameterizeKernel()
        self.mClassNames = []
        self.mDataSetRaw = []
        self.mDataSetOrange = []
        self.mClassifier = None
        self.mOrangeDomain = None

    def setProperties(self, properties):
        """
        Note that resetting the properties will reset the SVM and you will need
        to retrain.
        """
        if(properties is not None):
            self.mSVMProperties = properties
        self._parameterizeKernel()


    def _parameterizeKernel(self):
        #Set the parameters for our SVM
        self.mSVMPrototype = orange.SVMLearner()
        self.mSVMPrototype.svm_type = self.mSVMType[self.mSVMProperties["SVMType"]]
        self.mSVMPrototype.kernel_type = self.mKernelType[self.mSVMProperties["KernelType"]]
        if(self.mSVMProperties["nu"] is not None):
            self.mSVMPrototype.nu = self.mSVMProperties["nu"]
        if(self.mSVMProperties["c"] is not None):
            self.mSVMPrototype.C = self.mSVMProperties["c"]
        if(self.mSVMProperties["degree"]  is not None):
            self.mSVMPrototype.degree = self.mSVMProperties["degree"]
        if(self.mSVMProperties["coef"] is not None):
            self.mSVMPrototype.coef0 = self.mSVMProperties["coef"]
        if(self.mSVMProperties["gamma"] is not None):
            self.mSVMPrototype.gamma = self.mSVMProperties["gamma"]


    def load(cls, fname):
        """
        Load the classifier from file
        """
        return pickle.load(file(fname, 'rb'))
    load = classmethod(load)


    def save(self, fname):
        """
        Save the classifier to file
        """
        output = open(fname, 'wb')
        pickle.dump(self,output,2) # use two otherwise it w
        output.close()

    def __getstate__(self):
        mydict = self.__dict__.copy()
        self.mDataSetOrange = None
        del mydict['mDataSetOrange']
        self.mOrangeDomain = None
        del mydict['mOrangeDomain']
        self.mClassifier = None
        del mydict['mClassifier']
        return mydict

    def __setstate__(self, mydict):
        self.__dict__ = mydict
        colNames = []
        for extractor in self.mFeatureExtractors:
            colNames.extend(extractor.getFieldNames())
        self.mOrangeDomain = orange.Domain(map(orange.FloatVariable,colNames),orange.EnumVariable("type",values=self.mClassNames))
        self.mDataSetOrange = orange.ExampleTable(self.mOrangeDomain,self.mDataSetRaw)
        self.mClassifier = self.mSVMPrototype(self.mDataSetOrange)


    def classify(self, image):
        """
        Classify a single image. Takes in an image and returns the string
        of the classification.

        Make sure you haved loaded the feauture extractors and the training data.

        """
        featureVector = []
        for extractor in self.mFeatureExtractors: #get the features
            feats = extractor.extract(image)
            if( feats is not None ):
                featureVector.extend(feats)
        featureVector.extend([self.mClassNames[0]])
        test = orange.ExampleTable(self.mOrangeDomain,[featureVector])
        c = self.mClassifier(test[0]) #classify
        return str(c) #return to class name


    def setFeatureExtractors(self, extractors):
        """
        Add a list of feature extractors to the classifier. These feature extractors
        must match the ones used to train the classifier. If the classifier is already
        trained then this method will require that you retrain the data.
        """
        self.mFeatureExtractors = extractors
        return None

    def _trainPath(self,path,className,subset,disp,verbose):
        count = 0
        files = []
        for ext in IMAGE_FORMATS:
            files.extend(glob.glob( os.path.join(path, ext)))
        if(subset > 0):
            nfiles = min(subset,len(files))
        else:
            nfiles = len(files)
        badFeat = False
        for i in range(nfiles):
            infile = files[i]
            if verbose:
                print "Opening file: " + infile
            img = Image(infile)
            featureVector = []
            for extractor in self.mFeatureExtractors:
                feats = extractor.extract(img)

                if( feats is not None ):
                    featureVector.extend(feats)
                else:
                    badFeat = True

            if(badFeat):
                badFeat = False
                continue
            featureVector.extend([className])
            self.mDataSetRaw.append(featureVector)
            text = 'Training: ' + className
            self._WriteText(disp,img,text,Color.WHITE)
            count = count + 1
            del img
        return count

    def _trainImageSet(self,imageset,className,subset,disp,verbose):
        count = 0
        badFeat = False
        if (subset>0):
            imageset = imageset[0:subset]   
        for img in imageset:
            if verbose:
                print "Opening file: " + img.filename
            featureVector = []
            for extractor in self.mFeatureExtractors:
                feats = extractor.extract(img)
                if( feats is not None ):
                    featureVector.extend(feats)
                else:
                    badFeat = True
                    
            if(badFeat):
                badFeat = False
                continue
            
            featureVector.extend([className])
            self.mDataSetRaw.append(featureVector)
            text = 'Training: ' + className
            self._WriteText(disp,img,text,Color.WHITE)
            count = count + 1
            del img
        return count

    def train(self,images,classNames,disp=None,subset=-1,savedata=None,verbose=True):
        """
        Train the classifier.
        images paramater can take in a list of paths or a list of imagesets
        images - the order of the paths or imagesets must be in the same order as the class type

        - Note all image classes must be in seperate directories
        - The class names must also align to the directories

        disp - if display is a display we show images and class label,
        otherwise nothing is done.

        subset - if subset = -1 we use the whole dataset. If subset = # then we
        use min(#images,subset)

        savedata - if save data is None nothing is saved. If savedata is a file
        name we save the data to a tab delimited file.

        verbose - print confusion matrix and file names
        returns [%Correct %Incorrect Confusion_Matrix]
        """
        count = 0
        self.mClassNames = classNames
        # fore each class, get all of the data in the path and train
        for i in range(len(classNames)):
            if ( isinstance(images[i], str) ):
                count = count + self._trainPath(images[i],classNames[i],subset,disp,verbose)
            else:
                count = count + self._trainImageSet(images[i],classNames[i],subset,disp,verbose)

        colNames = []
        for extractor in self.mFeatureExtractors:
            colNames.extend(extractor.getFieldNames())

        if(count <= 0):
            logger.warning("No features extracted - bailing")
            return None

        # push our data into an orange example table
        self.mOrangeDomain = orange.Domain(map(orange.FloatVariable,colNames),orange.EnumVariable("type",values=self.mClassNames))
        self.mDataSetOrange = orange.ExampleTable(self.mOrangeDomain,self.mDataSetRaw)
        if(savedata is not None):
            orange.saveTabDelimited (savedata, self.mDataSetOrange)

        self.mClassifier = self.mSVMPrototype(self.mDataSetOrange)
        correct = 0
        incorrect = 0
        for i in range(count):
            c = self.mClassifier(self.mDataSetOrange[i])
            test = self.mDataSetOrange[i].getclass()
            if verbose:
                print "original", test, "classified as", c
            if(test==c):
                correct = correct + 1
            else:
                incorrect = incorrect + 1

        good = 100*(float(correct)/float(count))
        bad = 100*(float(incorrect)/float(count))

        confusion = 0
        if( len(self.mClassNames) > 2 ):
            crossValidator = orngTest.learnAndTestOnLearnData([self.mSVMPrototype],self.mDataSetOrange)
            confusion = orngStat.confusionMatrices(crossValidator)[0]

        if verbose:
            print("Correct: "+str(good))
            print("Incorrect: "+str(bad))
            classes = self.mDataSetOrange.domain.classVar.values
            print confusion
            #print "\t"+"\t".join(classes)
            #for className, classConfusions in zip(classes, confusion):
            #    print ("%s" + ("\t%i" * len(classes))) % ((className, ) + tuple(classConfusions))

        return [good, bad, confusion]




    def test(self,images,classNames,disp=None,subset=-1,savedata=None,verbose=True):
        """
        Test the classifier.
        images paramater can take in a list of paths or a list of imagesets
        images - the order of the paths or imagesets must be in the same order as the class type

        - Note all image classes must be in seperate directories
        - The class names must also align to the directories

        disp - if display is a display we show images and class label,
        otherwise nothing is done.

        subset - if subset = -1 we use the whole dataset. If subset = # then we
        use min(#images,subset)

        savedata - if save data is None nothing is saved. If savedata is a file
        name we save the data to a tab delimited file.

        verbose - print confusion matrix and file names
        returns [%Correct %Incorrect Confusion_Matrix]
        """
        count = 0
        correct = 0
        self.mClassNames = classNames
        colNames = []
        for extractor in self.mFeatureExtractors:
            colNames.extend(extractor.getFieldNames())
            self.mOrangeDomain = orange.Domain(map(orange.FloatVariable,colNames),orange.EnumVariable("type",values=self.mClassNames))

        dataset = []
        for i in range(len(classNames)):
            if ( isinstance(images[i],str) ):
                [dataset,cnt,crct] =self._testPath(images[i],classNames[i],dataset,subset,disp,verbose)
                count = count + cnt
                correct = correct + crct
            else:
                [dataset,cnt,crct] =self._testImageSet(images[i],classNames[i],dataset,subset,disp,verbose)
                count = count + cnt
                correct = correct + crct


        testData = orange.ExampleTable(self.mOrangeDomain,dataset)

        if savedata is not None:
            orange.saveTabDelimited (savedata, testdata)

        confusion = 0
        if( len(self.mClassNames) > 2 ):
            crossValidator = orngTest.learnAndTestOnTestData([self.mSVMPrototype],self.mDataSetOrange,testData)
            confusion = orngStat.confusionMatrices(crossValidator)[0]

        good = 100*(float(correct)/float(count))
        bad = 100*(float(count-correct)/float(count))
        if verbose:
            print("Correct: "+str(good))
            print("Incorrect: "+str(bad))
            classes = self.mDataSetOrange.domain.classVar.values
            print "\t"+"\t".join(classes)
            for className, classConfusions in zip(classes, confusion):
                print ("%s" + ("\t%i" * len(classes))) % ((className, ) + tuple(classConfusions))

        return [good, bad, confusion]

    def _testPath(self,path,className,dataset,subset,disp,verbose):
        count = 0
        correct = 0
        badFeat = False
        files = []
        for ext in IMAGE_FORMATS:
            files.extend(glob.glob( os.path.join(path, ext)))
        if(subset > 0):
            nfiles = min(subset,len(files))
        else:
            nfiles = len(files)
        for i in range(nfiles):
            infile = files[i]
            if verbose:
                print "Opening file: " + infile
            img = Image(infile)
            featureVector = []
            for extractor in self.mFeatureExtractors:
                feats = extractor.extract(img)
                if( feats is not None ):
                    featureVector.extend(feats)
                else:
                    badFeat = True
            if( badFeat ):
                del img
                badFeat = False
                continue
            featureVector.extend([className])
            dataset.append(featureVector)
            test = orange.ExampleTable(self.mOrangeDomain,[featureVector])
            c = self.mClassifier(test[0])
            testClass = test[0].getclass()
            if(testClass==c):
                text =  "Classified as " + str(c)
                self._WriteText(disp,img,text, Color.GREEN)
                correct = correct + 1
            else:
                text =  "Mislassified as " + str(c)
                self._WriteText(disp,img,text, Color.RED)
            count = count + 1
            del img

        return([dataset,count,correct])

    def _testImageSet(self,imageset,className,dataset,subset,disp,verbose):
        count = 0
        correct = 0
        badFeat = False
        if(subset > 0):
            imageset = imageset[0:subset]
        for img in imageset:
            if verbose:
                print "Opening file: " + img.filename
            featureVector = []
            for extractor in self.mFeatureExtractors:
                feats = extractor.extract(img)
                if( feats is not None ):
                    featureVector.extend(feats)
                else:
                    badFeat = True
            if( badFeat ):
                del img
                badFeat = False
                continue 
            featureVector.extend([className])
            dataset.append(featureVector)
            test = orange.ExampleTable(self.mOrangeDomain,[featureVector])
            c = self.mClassifier(test[0])
            testClass = test[0].getclass()
            if(testClass==c):
                text =  "Classified as " + str(c)
                self._WriteText(disp,img,text, Color.GREEN)
                correct = correct + 1
            else:   
                text =  "Mislassified as " + str(c)
                self._WriteText(disp,img,text, Color.RED)
            count = count + 1
            del img
            
        return([dataset,count,correct])

    def _WriteText(self, disp, img, txt,color):
        if(disp is not None):
            txt = ' ' + txt + ' '
            img = img.adaptiveScale(disp.resolution)
            layer = DrawingLayer((img.width,img.height))
            layer.setFontSize(60)
            layer.ezViewText(txt,(20,20),fgcolor=color)
            img.addDrawingLayer(layer)
            img.applyLayers()
            img.save(disp)

########NEW FILE########
__FILENAME__ = TemporalColorTracker
from SimpleCV import Image, ImageSet, Camera, VirtualCamera, ROI, Color, LineScan
import numpy as np
import scipy.signal as sps
import warnings
import time as time
class TemporalColorTracker:
    """
    **SUMMARY**

    The temporal color tracker attempts to find and periodic color
    signal in an roi or arbitrary function. Once the temporal tracker is
    trained it will return a count object every time the signal is detected.
    This class is usefull for counting periodically occuring events, for example,
    waves on a beach or the second hand on a clock.
    
    """
    def __init__(self):
        self._rtData = LineScan([]) # the deployed data
        self._steadyState = None # mu/signal for the ss behavior
        self._extractor = None
        self._roi = None
        self._window = None
        self._template = None
        self._cutoff = None
        self._bestKey = None
        self._isPeak = False
        self.corrTemplates = None
        self.peaks = {}
        self.valleys = {}
        self.doPeaks = {}
        self.corrStdMult = 3.0
        self.count = 0

    def train(self,src,roi=None, extractor=None, doCorr=False, maxFrames=1000,
              ssWndw=0.05, pkWndw=30, pkDelta=3, corrStdMult=2.0, forceChannel=None, verbose=True):
        """
        **SUMMARY**

        To train the TemporalColorTracker you provide it with a video, camera, or ImageSet and either
        an region of interest (ROI) or a function of the form:

        (R,G,B) = MyFunction(Image)

        This function takes in an image and returns a tuple of RGB balues for the frame.
        The TemoralColroTracker will then attempt to find the maximum peaks in the data
        and create a model for the peaks.

        **PARAMETERS**

        * *src* - An image source, either a camera, a virtual camera (like a video) or
          an ImageSet.
        * *roi* - An ROI object that tells the tracker where to look in the frame.
        * *extractor* - A function with the following signature:
          (R,G,B) = Extract(Image)
        * *doCorr* - Do correlation use correlation to confirm that the signal is present.
        * *maxFrames* - The maximum number of frames to use for training.
        * *ssWndw* - SteadyState window, this is the size of the window to look for a steady
          state, i.e a region where the signal is not changing.
        * *pkWndw* - The window size to look for peaks/valleys in the signal. This is roughly
          the period of the signal.
        * *pkDelta* - The minimum difference between the steady state to look for peaks.
        * *corrStdMult* - The maximum correlation standard deviation of the training set
          to use when looking for a signal. This is the knob to dial in when using correlation
          to confirm the event happened.
        * *forceChannel* - A string that is the channel to use. Options are:
           * 'r' - Red Channel
           * 'g' - Green Channel
           * 'b' - Blue Channel 
           * 'h' - Hue Channel
           * 'i' - Intensity Channel
          By default this module will look at the signal with the highest peak/valley swings.
          You can manually overide this behavior.
        * *verbose* - Print debug info after training. 
        
        **RETURNS**

        Nothing, will raise an exception if no signal is found.

        **EXAMPLE**

        A really simple example

        >>>> cam = Camera(1)
        >>>> tct = TemporalColorTracker()
        >>>> img = cam.getImage()
        >>>> roi = ROI(img.width*0.45,img.height*0.45,img.width*0.1,img.height*0.1,img)
        >>>> tct.train(cam,roi=roi,maxFrames=250)
        >>>> disp = Display((800,600))
        >>>> while disp.isNotDone():
        >>>>     img = cam.getImage()
        >>>>     result = tct.recognize(img)
        >>>>     roi = ROI(img.width*0.45,img.height*0.45,img.width*0.1,img.height*0.1,img)
        >>>>     roi.draw(width=3)
        >>>>     img.drawText(str(result),20,20,color=Color.RED,fontsize=32)
        >>>>     img = img.applyLayers()
        >>>>     img.save(disp)

        """
        if( roi is None and extractor is None ):
            raise Exception('Need to provide an ROI or an extractor')
        self.doCorr = doCorr
        self.corrStdMult = corrStdMult
        self._extractor = extractor #function that returns a RGB values
        self._roi = roi
        self._extract(src,maxFrames,verbose)
        self._findSteadyState(windowSzPrct=ssWndw)
        self._findPeaks(pkWndw,pkDelta)
        self._extractSignalInfo(forceChannel)
        self._buildSignalProfile()
        if verbose:
            for key in self.data.keys():
                print 30*'-'
                print "Channel: {0}".format(key)
                print "Data Points: {0}".format(len(self.data[key]))
                print "Steady State: {0}+/-{1}".format(self._steadyState[key][0],self._steadyState[key][1])
                print "Peaks: {0}".format(self.peaks[key])
                print "Valleys: {0}".format(self.valleys[key])
                print "Use Peaks: {0}".format(self.doPeaks[key])
            print 30*'-'
            print "BEST SIGNAL: {0}".format(self._bestKey)
            print "BEST WINDOW: {0}".format(self._window)
            print "BEST CUTOFF: {0}".format(self._cutoff)
                
    def _getDataFromImg(self,img):
        """
        Get the data from the image 
        """
        mc = None
        if( self._extractor ):
            mc = self._extractor(img)
        else:
            temp = self._roi.reassign(img)    
            mc = temp.meanColor()
        self.data['r'].append(mc[0])
        self.data['g'].append(mc[1])
        self.data['b'].append(mc[2])
        # NEED TO CHECK THAT THIS REALLY RGB
        self.data['i'].append(Color.getLightness(mc))
        self.data['h'].append(Color.getHueFromRGB(mc))
        #return [mc[0],mc[1],mc[2],gray,Color.rgbToHue(mc)]

    def _extract(self,src,maxFrames,verbose):
        # get the full dataset and append it to the data vector dictionary.
        self.data = {'r':[],'g':[],'b':[],'i':[],'h':[]}
        if( isinstance(src,ImageSet) ):
            src = VirtualCamera(src,st='imageset') # this could cause a bug
        elif( isinstance(src,(VirtualCamera,Camera))):
            count = 0
            for i in range(0,maxFrames):
                img = src.getImage()
                count = count + 1
                if( verbose ):
                    print "Got Frame {0}".format(count)
                if( isinstance(src,Camera) ):
                    time.sleep(0.05) # let the camera sleep
                if( img is None ):
                    break
                else:
                    self._getDataFromImg(img)
                
        else:
            raise Exception('Not a valid training source')
            return None
    
    def _findSteadyState(self,windowSzPrct=0.05):
        # slide a window across each of the signals
        # find where the std dev of the window is minimal
        # this is the steady state (e.g. where the
        # assembly line has nothing moving)
        # save the mean and sd of this value
        # as a tuple in the steadyStateDict
        self._steadyState = {}
        for key in self.data.keys():
            wndwSz = int(np.floor(windowSzPrct*len(self.data[key])))
            signal = self.data[key]
            # slide the window and get the std
            data = [np.std(signal[i:i+wndwSz]) for i in range(0,len(signal)-wndwSz)]
            # find the first spot where sd is minimal
            index = np.where(data==np.min(data))[0][0]
            # find the mean for the window
            mean = np.mean(signal[index:index+wndwSz])
            self._steadyState[key]=(mean,data[index])


    def _findPeaks(self,pkWndw,pkDelta):
        """
        Find the peaks and valleys in the data
        """
        self.peaks = {}
        self.valleys = {}
        for key in self.data.keys():
            ls = LineScan(self.data[key])
            # need to automagically adjust the window
            # to make sure we get a minimum number of
            # of peaks, maybe let the user guess a min?
            self.peaks[key]=ls.findPeaks(pkWndw,pkDelta)
            self.valleys[key]=ls.findValleys(pkWndw,pkDelta)

    def _extractSignalInfo(self,forceChannel):
        """
        Find the difference between the peaks and valleys
        """
        self.pD = {}
        self.vD = {}
        self.doPeaks = {}
        bestSpread = 0.00
        bestDoPeaks = None
        bestKey = None
        for key in self.data.keys():
            #Look at which signal has a bigger distance from
            #the steady state behavior
            if( len(self.peaks[key]) > 0 ):
                peakMean = np.mean(np.array(self.peaks[key])[:,1])
                self.pD[key] =  np.abs(self._steadyState[key][0]-peakMean)
            else:
                self.pD[key] = 0.00

            if( len(self.valleys[key]) > 0 ):
                valleyMean = np.mean(np.array(self.valleys[key])[:,1])
                self.vD[key] =  np.abs(self._steadyState[key][0]-valleyMean)
            else:
                self.vD[key] = 0.00
                
            self.doPeaks[key]=False
            best = self.vD[key]
            if( self.pD[key] > self.vD[key] ):
                best = self.pD[key]
                self.doPeaks[key] = True
            if( best > bestSpread ):
                bestSpread = best
                bestDoPeaks = self.doPeaks[key]
                bestKey = key
        # Now we know which signal has the most spread
        # and what direction we are looking for.
        if( forceChannel is not None ):
            if(self.data.has_key(forceChannel)):
                self._bestKey = forceChannel
            else:
                raise Exception('That is not a valid data channel')
        else:
            self._bestKey = bestKey

        
    def _buildSignalProfile(self):
        key = self._bestKey
        self._window = None
        peaks = None
        if( self.doPeaks[key] ):
            self._isPeak = True
            peaks = self.peaks[key]
            # We're just going to do halfway
            self._cutoff = self._steadyState[key][0]+(self.pD[key]/2.0)            
        else:
            self._isPeak = False
            peaks = self.valleys[key]
            self._cutoff = self._steadyState[key][0]-(self.vD[key]/2.0)
        if( len(peaks) > 1 ):
            p2p = np.array(peaks[1:])-np.array(peaks[:-1])
            p2pMean = int(np.mean(p2p))
            p2pS = int(np.std(p2p))
            p2pMean = p2pMean + 2*p2pS
            # constrain it to be an od window
            if int(p2pMean) % 2 == 1:
                p2pMean = p2pMean+1 
            self._window = p2pMean
        else:
            raise Exception("Can't find enough peaks")
        if( self.doCorr and self._window is not None ):
            self._doCorr()

        #NEED TO ERROR OUT ON NOT ENOUGH POINTS

    def _doCorr(self):
        key = self._bestKey
        # build an average signal for the peaks and valleys
        # centered at the peak. The go and find the correlation
        # value of each peak/valley with the average signal
        self.corrTemplates = []
        halfWndw = self._window/2
        pList = None 
        if( self._isPeak ):
            pList = self.peaks[key]
        else:
            pList = self.valleys[key]

        for peak in pList:
            center = peak[0]
            lb = center-halfWndw
            ub = center+halfWndw
            # ignore signals that fall of the end of the data
            if( lb > 0 and ub < len(self.data[key]) ):
                self.corrTemplates.append(np.array(self.data[key][lb:ub]))
        if( len(self.corrTemplates) < 1 ):
            raise Exception('Could not find a coherrent signal for correlation.')
        
        sig = np.copy(self.corrTemplates[0]) # little np gotcha
        for peak in self.corrTemplates[1:]:
            sig += peak
        self._template = sig / len(self.corrTemplates)
        self._template /= np.max(self._template)
        corrVals = [np.correlate(peak/np.max(peak),self._template) for peak in self.corrTemplates] 
        print corrVals
        self.corrThresh = (np.mean(corrVals),np.std(corrVals))
        
    def _getBestValue(self,img):
        """
        Extract the data from the live signal
        """
        if( self._extractor ):
            mc = self._extractor(img)
        else:
            temp = self._roi.reassign(img)    
            mc = temp.meanColor()
        if( self._bestKey == 'r' ):
            return mc[0]
        elif( self._bestKey == 'g' ):
            return mc[1]
        elif( self._bestKey == 'b' ):
            return mc[2]
        elif( self._bestKey == 'i' ):
            return Color.getLightness(mc)
        elif( self._bestKey == 'h' ):
            return Color.getHueFromRGB(mc)
        
    def _updateBuffer(self,v):
        """
        Keep a buffer of the running data and process it to determine if there is
        a peak. 
        """
        self._rtData.append(v)
        wndwCenter = int(np.floor(self._window/2.0))
        # pop the end of the buffer
        if( len(self._rtData) > self._window):
            self._rtData = self._rtData[1:]
            if( self._isPeak ):
                lm = self._rtData.findPeaks()
                for l in lm:
                    if( l[0] == wndwCenter and l[1] > self._cutoff ):
                        if( self.doCorr ):
                            corrVal = np.correlate(self._rtData.normalize(),self._template)
                            thresh = self.corrThresh[0]-self.corrStdMult*self.corrThresh[1]
                            if( corrVal[0] > thresh ):
                                self.count += 1
                        else:
                            self.count += 1
            else:
                lm = self._rtData.findValleys()
                for l in lm:
                    if( l[0] == wndwCenter and l[1] < self._cutoff ):
                        if( self.doCorr ):
                            corrVal = np.correlate(self._rtData.normalize(),self._template)
                            thresh = self.corrThresh[0]-self.corrStdMult*self.corrThresh[1]
                            if( corrVal[0] > thresh ):
                                self.count += 1
                        else:
                            self.count += 1
        return self.count
        
    def recognize(self,img):
        """

        **SUMMARY***

        This method is used to do the real time signal analysis. Pass the method
        an image from the stream and it will return the event count. Note that
        due to buffering the signal may lag the actual video by up to a few seconds.

        **PARAMETERS**

        * *img* - The image in the stream to test. 

        **RETURNS**

        Returns an int that is the count of the number of times the event has occurred.

        **EXAMPLE**

        >>>> cam = Camera(1)
        >>>> tct = TemporalColorTracker()
        >>>> img = cam.getImage()
        >>>> roi = ROI(img.width*0.45,img.height*0.45,img.width*0.1,img.height*0.1,img)
        >>>> tct.train(cam,roi=roi,maxFrames=250)
        >>>> disp = Display((800,600))
        >>>> while disp.isNotDone():
        >>>>     img = cam.getImage()
        >>>>     result = tct.recognize(img)
        >>>>     roi = ROI(img.width*0.45,img.height*0.45,img.width*0.1,img.height*0.1,img)
        >>>>     roi.draw(width=3)
        >>>>     img.drawText(str(result),20,20,color=Color.RED,fontsize=32)
        >>>>     img = img.applyLayers()
        >>>>     img.save(disp)

        **TODO**

        Return True/False if the event occurs.
        """
        if( self._bestKey is None ):
            raise Exception('The TemporalColorTracker has not been trained.')
        v = self._getBestValue(img)
        return self._updateBuffer(v)

########NEW FILE########
__FILENAME__ = TestTemporalColorTracker
from SimpleCV import Camera, Image, Color, TemporalColorTracker, ROI, Display
import matplotlib.pyplot as plt

cam = Camera(1)
tct = TemporalColorTracker()
img = cam.getImage()
roi = ROI(img.width*0.45,img.height*0.45,img.width*0.1,img.height*0.1,img)
tct.train(cam,roi=roi,maxFrames=250,pkWndw=20)

# Matplot Lib example plotting
plotc = {'r':'r','g':'g','b':'b','i':'m','h':'y'}
for key in tct.data.keys():
    plt.plot(tct.data[key],plotc[key])
    for pt in tct.peaks[key]:
        plt.plot(pt[0],pt[1],'r*')
    for pt in tct.valleys[key]:
        plt.plot(pt[0],pt[1],'b*')
    plt.grid()
plt.show()

disp = Display((800,600))
while disp.isNotDone():
    img = cam.getImage()
    result = tct.recognize(img)
    plt.plot(tct._rtData,'r-')
    plt.grid()
    plt.savefig('temp.png')
    plt.clf()
    plotImg = Image('temp.png')    
    
    roi = ROI(img.width*0.45,img.height*0.45,img.width*0.1,img.height*0.1,img)
    roi.draw(width=3)
    img.drawText(str(result),20,20,color=Color.RED,fontsize=32)
    img = img.applyLayers()
    img = img.blit(plotImg.resize(w=img.width,h=img.height),pos=(0,0),alpha=0.5)
    img.save(disp)

########NEW FILE########
__FILENAME__ = TreeClassifier
from SimpleCV.base import *
from SimpleCV.ImageClass import Image, ImageSet
from SimpleCV.DrawingLayer import *
from SimpleCV.Features import FeatureExtractorBase


"""
This class is encapsulates almost everything needed to train, test, and deploy a
multiclass decision tree / forest image classifier. Training data should
be stored in separate directories for each class. This class uses the feature
extractor base class to  convert images into a feature vector. The basic workflow
is as follows.
1. Get data.
2. Setup Feature Extractors (roll your own or use the ones I have written).
3. Train the classifier.
4. Test the classifier.
5. Tweak parameters as necessary.
6. Repeat until you reach the desired accuracy.
7. Save the classifier.
8. Deploy using the classify method.
"""
class TreeClassifier:
    """
    This method encapsulates a number of tree-based machine learning approaches
    and associated meta algorithms.

    Decision trees:
    http://en.wikipedia.org/wiki/Decision_trees

    boosted adpative decision trees
    http://en.wikipedia.org/wiki/Adaboost

    random forrests
    http://en.wikipedia.org/wiki/Random_forest

    bagging (bootstrap aggregating)
    http://en.wikipedia.org/wiki/Bootstrap_aggregating
    """
    mClassNames = []
    mDataSetRaw = []
    mDataSetOrange = []
    mClassifier = None
    mLearner = None
    mTree = None
    mFeatureExtractors = None
    mOrangeDomain = None
    mFlavorParams = None

    mTreeTypeDict = {
        "Tree":0,    # A vanilla classification tree
        "Bagged":1, # Bootstrap aggregating aka bagging - make new data sets and test on them
        "Forest":2, # Lots of little trees
        "Boosted":3 # Highly optimized trees.
    }

    mforestFlavorDict = {
        "NTrees":100, #number of trees in our forest
        "NAttributes":None # number of attributes per split sqrt(features) is default
     }
    mBoostedFlavorDict = {
        "NClassifiers":10, #number of learners
    }
    mBaggedFlavorDict = {
        "NClassifiers":10, #numbers of classifiers / tree splits
    }

    def __init__(self,featureExtractors=[],flavor='Tree',flavorDict=None):
        """
        dist = distance algorithm
        k = number of nearest neighbors
        """
        if not ORANGE_ENABLED:
            logger.warning("I'm sorry, but you need the orange machine learning library installed to use this")
            return None

        self.mClassNames = []
        self.mDataSetRaw = []
        self.mDataSetOrange = []
        self.mClassifier = None
        self.mLearner = None
        self.mTree = None
        self.mFeatureExtractors = None
        self.mOrangeDomain = None
        self.mFlavorParams = None
        self.mFlavor = self.mTreeTypeDict[flavor]
        if(flavorDict is None):
            if(self.mFlavor == self.mTreeTypeDict["Bagged"]):
                self.mFlavorParams = self.mBaggedFlavorDict
            elif(self.mFlavor == self.mTreeTypeDict["Forest"]):
                self.mFlavorParams = self.mforestFlavorDict #mmmm tastes like   pinecones and squirrels
            elif(self.mFlavor == self.mTreeTypeDict["Boosted"]):
                self.mFlavorParams = self.mBoostedFlavorDict
        else:
            self.mFlavorParams = flavorDict
        self.mFeatureExtractors =  featureExtractors

    def load(cls, fname):
        """
        Load the classifier from file
        """
        return pickle.load(file(fname))
    load = classmethod(load)


    def save(self, fname):
        """
        Save the classifier to file
        """
        output = open(fname, 'wb')
        pickle.dump(self,output,2) # use two otherwise it w
        output.close()

    def __getstate__(self):
        mydict = self.__dict__.copy()
        self.mDataSetOrange = None
        del mydict['mDataSetOrange']
        self.mOrangeDomain = None
        del mydict['mOrangeDomain']
        self.mLearner = None
        del mydict['mLearner']
        self.mTree = None
        del mydict['mTree']
        return mydict

    def __setstate__(self, mydict):
        self.__dict__ = mydict
        colNames = []
        for extractor in self.mFeatureExtractors:
            colNames.extend(extractor.getFieldNames())
        self.mOrangeDomain = orange.Domain(map(orange.FloatVariable,colNames),orange.EnumVariable("type",values=self.mClassNames))
        self.mDataSetOrange = orange.ExampleTable(self.mOrangeDomain,self.mDataSetRaw)
        if(self.mFlavor == 0):
            self.mLearner =  orange.TreeLearner()
            self.mClassifier = self.mLearner(self.mDataSetOrange)
        elif(self.mFlavor == 1): #bagged
            self.mTree =  orange.TreeLearner()
            self.mLearner = orngEnsemble.BaggedLearner(self.mTree,t=self.mFlavorParams["NClassifiers"])
            self.mClassifier = self.mLearner(self.mDataSetOrange)
        elif(self.mFlavor == 2):#forest
            self.mTree =  orange.TreeLearner()
            self.mLearner =  orngEnsemble.RandomForestLearner(trees=self.mFlavorParams["NTrees"], attributes=self.mFlavorParams["NAttributes"])
            self.mClassifier = self.mLearner(self.mDataSetOrange)
        elif(self.mFlavor == 3):#boosted
            self.mTree =  orange.TreeLearner()
            self.mLearner = orngEnsemble.BoostedLearner(self.mTree,t=self.mFlavorParams["NClassifiers"])
            self.mClassifier = self.mLearner(self.mDataSetOrange)


    def classify(self, image):
        """
        Classify a single image. Takes in an image and returns the string
        of the classification.

        Make sure you haved loaded the feauture extractors and the training data.

        """
        featureVector = []
        for extractor in self.mFeatureExtractors: #get the features
            feats = extractor.extract(image)
            if( feats is not None ):
                featureVector.extend(feats)
        featureVector.extend([self.mClassNames[0]])
        test = orange.ExampleTable(self.mOrangeDomain,[featureVector])
        c = self.mClassifier(test[0]) #classify
        return str(c) #return to class name


    def setFeatureExtractors(self, extractors):
        """
        Add a list of feature extractors to the classifier. These feature extractors
        must match the ones used to train the classifier. If the classifier is already
        trained then this method will require that you retrain the data.
        """
        self.mFeatureExtractors = extractors
        return None

    def _trainPath(self,path,className,subset,disp,verbose):
        count = 0
        files = []
        for ext in IMAGE_FORMATS:
            files.extend(glob.glob( os.path.join(path, ext)))
        if(subset > 0):
            nfiles = min(subset,len(files))
        else:
            nfiles = len(files)
        badFeat = False
        for i in range(nfiles):
            infile = files[i]
            if verbose:
                print "Opening file: " + infile
            img = Image(infile)
            featureVector = []
            for extractor in self.mFeatureExtractors:
                feats = extractor.extract(img)
                if( feats is not None ):
                    featureVector.extend(feats)
                else:
                    badFeat = True

            if(badFeat):
                badFeat = False
                continue

            featureVector.extend([className])
            self.mDataSetRaw.append(featureVector)
            text = 'Training: ' + className
            self._WriteText(disp,img,text,Color.WHITE)
            count = count + 1
            del img
        return count

    def _trainImageSet(self,imageset,className,subset,disp,verbose):
        count = 0
        badFeat = False
        if (subset>0):
            imageset = imageset[0:subset]   
        for img in imageset:
            if verbose:
                print "Opening file: " + img.filename
            featureVector = []
            for extractor in self.mFeatureExtractors:
                feats = extractor.extract(img)
                if( feats is not None ):
                    featureVector.extend(feats)
                else:
                    badFeat = True
                    
            if(badFeat):
                badFeat = False
                continue
            
            featureVector.extend([className])
            self.mDataSetRaw.append(featureVector)
            text = 'Training: ' + className
            self._WriteText(disp,img,text,Color.WHITE)
            count = count + 1
            del img
        return count

    def train(self,images,classNames,disp=None,subset=-1,savedata=None,verbose=True):
        """
        Train the classifier.
        images paramater can take in a list of paths or a list of imagesets
        images - the order of the paths or imagesets must be in the same order as the class type

        - Note all image classes must be in seperate directories
        - The class names must also align to the directories or imagesets

        disp - if display is a display we show images and class label,
        otherwise nothing is done.

        subset - if subset = -1 we use the whole dataset. If subset = # then we
        use min(#images,subset)

        savedata - if save data is None nothing is saved. If savedata is a file
        name we save the data to a tab delimited file.

        verbose - print confusion matrix and file names
        returns [%Correct %Incorrect Confusion_Matrix]
        """
        #if( (self.mFlavor == 1 or self.mFlavor == 3) and len(classNames) > 2):
        #    logger.warning("Boosting / Bagging only works for binary classification tasks!!!")

        count = 0
        self.mClassNames = classNames
        # for each class, get all of the data in the path and train
        for i in range(len(classNames)):
            if ( isinstance(images[i], str) ):
                count = count + self._trainPath(images[i],classNames[i],subset,disp,verbose)
            else:
                count = count + self._trainImageSet(images[i],classNames[i],subset,disp,verbose)

        colNames = []
        for extractor in self.mFeatureExtractors:
            colNames.extend(extractor.getFieldNames())

        if(count <= 0):
            logger.warning("No features extracted - bailing")
            return None

        self.mOrangeDomain = orange.Domain(map(orange.FloatVariable,colNames),orange.EnumVariable("type",values=self.mClassNames))
        self.mDataSetOrange = orange.ExampleTable(self.mOrangeDomain,self.mDataSetRaw)
        if(savedata is not None):
            orange.saveTabDelimited (savedata, self.mDataSetOrange)

        if(self.mFlavor == 0):
            self.mLearner =  orange.TreeLearner()
            self.mClassifier = self.mLearner(self.mDataSetOrange)
        elif(self.mFlavor == 1): #bagged
            self.mTree =  orange.TreeLearner()
            self.mLearner = orngEnsemble.BaggedLearner(self.mTree,t=self.mFlavorParams["NClassifiers"])
            self.mClassifier = self.mLearner(self.mDataSetOrange)
        elif(self.mFlavor == 2):#forest
            self.mTree =  orange.TreeLearner()
            self.mLearner =  orngEnsemble.RandomForestLearner(trees=self.mFlavorParams["NTrees"], attributes=self.mFlavorParams["NAttributes"])
            self.mClassifier = self.mLearner(self.mDataSetOrange)
        elif(self.mFlavor == 3):#boosted
            self.mTree =  orange.TreeLearner()
            self.mLearner = orngEnsemble.BoostedLearner(self.mTree,t=self.mFlavorParams["NClassifiers"])
            self.mClassifier = self.mLearner(self.mDataSetOrange)

        correct = 0
        incorrect = 0
        for i in range(count):
            c = self.mClassifier(self.mDataSetOrange[i])
            test = self.mDataSetOrange[i].getclass()
            if verbose:
                print "original", test, "classified as", c
            if(test==c):
                correct = correct + 1
            else:
                incorrect = incorrect + 1

        good = 100*(float(correct)/float(count))
        bad = 100*(float(incorrect)/float(count))

        confusion = 0
        if( len(self.mClassNames) > 2 ):
            crossValidator = orngTest.learnAndTestOnLearnData([self.mLearner],self.mDataSetOrange)
            confusion = orngStat.confusionMatrices(crossValidator)[0]

        if verbose:
            print("Correct: "+str(good))
            print("Incorrect: "+str(bad))
            if( confusion != 0 ):
                classes = self.mDataSetOrange.domain.classVar.values
                print "\t"+"\t".join(classes)
                for className, classConfusions in zip(classes, confusion):
                    print ("%s" + ("\t%i" * len(classes))) % ((className, ) + tuple(    classConfusions))

        if(self.mFlavor == 0):
            self._PrintTree(self.mClassifier)

        return [good, bad, confusion]




    def test(self,images,classNames,disp=None,subset=-1,savedata=None,verbose=True):
        """
        Test the classifier.
        images paramater can take in a list of paths or a list of imagesets
        images - the order of the paths or imagesets must be in the same order as the class type

        - Note all image classes must be in seperate directories
        - The class names must also align to the directories or imagesets

        disp - if display is a display we show images and class label,
        otherwise nothing is done.

        subset - if subset = -1 we use the whole dataset. If subset = # then we
        use min(#images,subset)

        savedata - if save data is None nothing is saved. If savedata is a file
        name we save the data to a tab delimited file.

        verbose - print confusion matrix and file names
        returns [%Correct %Incorrect Confusion_Matrix]
        """
        count = 0
        correct = 0
        self.mClassNames = classNames
        colNames = []
        for extractor in self.mFeatureExtractors:
            colNames.extend(extractor.getFieldNames())
            if(self.mOrangeDomain is None):
                self.mOrangeDomain = orange.Domain(map(orange.FloatVariable,colNames),orange.EnumVariable("type",values=self.mClassNames))

        dataset = []
        for i in range(len(classNames)):
            if ( isinstance(images[i],str) ):
                [dataset,cnt,crct] =self._testPath(images[i],classNames[i],dataset,subset,disp,verbose)
                count = count + cnt
                correct = correct + crct
            else:
                [dataset,cnt,crct] =self._testImageSet(images[i],classNames[i],dataset,subset,disp,verbose)
                count = count + cnt
                correct = correct + crct


        testData = orange.ExampleTable(self.mOrangeDomain,dataset)

        if savedata is not None:
            orange.saveTabDelimited (savedata, testData)

        confusion = 0
        if( len(self.mClassNames) > 2 ):
            crossValidator = orngTest.learnAndTestOnTestData([self.mLearner],self.mDataSetOrange,testData)
            confusion = orngStat.confusionMatrices(crossValidator)[0]

        good = 100*(float(correct)/float(count))
        bad = 100*(float(count-correct)/float(count))
        if verbose:
            print("Correct: "+str(good))
            print("Incorrect: "+str(bad))
            if( confusion != 0 ):
                classes = self.mDataSetOrange.domain.classVar.values
                print "\t"+"\t".join(classes)
                for className, classConfusions in zip(classes, confusion):
                    print ("%s" + ("\t%i" * len(classes))) % ((className, ) + tuple(    classConfusions))
        return [good, bad, confusion]

    def _testPath(self,path,className,dataset,subset,disp,verbose):
        count = 0
        correct = 0
        badFeat = False
        files = []
        for ext in IMAGE_FORMATS:
            files.extend(glob.glob( os.path.join(path, ext)))
        if(subset > 0):
            nfiles = min(subset,len(files))
        else:
            nfiles = len(files)
        for i in range(nfiles):
            infile = files[i]
            if verbose:
                print "Opening file: " + infile
            img = Image(infile)
            featureVector = []
            for extractor in self.mFeatureExtractors:
                feats = extractor.extract(img)
                if( feats is not None ):
                    featureVector.extend(feats)
                else:
                    badFeat = True
            if( badFeat ):
                del img
                badFeat = False
                continue
            featureVector.extend([className])
            dataset.append(featureVector)
            test = orange.ExampleTable(self.mOrangeDomain,[featureVector])
            c = self.mClassifier(test[0])
            testClass = test[0].getclass()
            if(testClass==c):
                text =  "Classified as " + str(c)
                self._WriteText(disp,img,text, Color.GREEN)
                correct = correct + 1
            else:
                text =  "Mislassified as " + str(c)
                self._WriteText(disp,img,text, Color.RED)
            count = count + 1
            del img

        return([dataset,count,correct])

    def _testImageSet(self,imageset,className,dataset,subset,disp,verbose):
        count = 0
        correct = 0
        badFeat = False
        if(subset > 0):
            imageset = imageset[0:subset]
        for img in imageset:
            if verbose:
                print "Opening file: " + img.filename
            featureVector = []
            for extractor in self.mFeatureExtractors:
                feats = extractor.extract(img)
                if( feats is not None ):
                    featureVector.extend(feats)
                else:
                    badFeat = True
            if( badFeat ):
                del img
                badFeat = False
                continue 
            featureVector.extend([className])
            dataset.append(featureVector)
            test = orange.ExampleTable(self.mOrangeDomain,[featureVector])
            c = self.mClassifier(test[0])
            testClass = test[0].getclass()
            if(testClass==c):
                text =  "Classified as " + str(c)
                self._WriteText(disp,img,text, Color.GREEN)
                correct = correct + 1
            else:   
                text =  "Mislassified as " + str(c)
                self._WriteText(disp,img,text, Color.RED)
            count = count + 1
            del img
            
        return([dataset,count,correct])

    def _WriteText(self, disp, img, txt,color):
        if(disp is not None):
            txt = ' ' + txt + ' '
            img = img.adaptiveScale(disp.resolution)
            layer = DrawingLayer((img.width,img.height))
            layer.setFontSize(60)
            layer.ezViewText(txt,(20,20),fgcolor=color)
            img.addDrawingLayer(layer)
            img.applyLayers()
            img.save(disp)

    def _PrintTree(self,x):
        #adapted from the orange documentation
        if type(x) == orange.TreeClassifier:
            self._PrintTree0(x.tree, 0)
        elif type(x) == orange.TreeNode:
            self._PrintTree0(x, 0)
        else:
            raise TypeError, "invalid parameter"

    def _PrintTree0(self,node,level):
        #adapted from the orange documentation
        if not node:
            print " "*level + "<null node>"
            return

        if node.branchSelector:
            nodeDesc = node.branchSelector.classVar.name
            nodeCont = node.distribution
            print "\n" + "   "*level + "%s (%s)" % (nodeDesc, nodeCont),
            for i in range(len(node.branches)):
                print "\n" + "   "*level + ": %s" % node.branchDescriptions[i],
                self._PrintTree0(node.branches[i], level+1)
        else:
            nodeCont = node.distribution
            majorClass = node.nodeClassifier.defaultValue
            print "--> %s (%s) " % (majorClass, nodeCont)

########NEW FILE########
__FILENAME__ = TurkingModule
from SimpleCV import Display, Image, Color, ImageSet
import os
import os.path as osp
import time
import glob
import pickle
"""
This class is a helper utility for automatically turking
image data for supervsed learning. This class helps you
run through a bunch of images and sort them into a bunch
of classes or categories.

You provide a path to
the images that need to be turked/sorted, your class labels, and
what keys you want to bind to the classes. The turker will
load all the images, optionally process them, and then display
them. To sort the images you just push a key mapped to your class
and the class tosses them into a directory and labels them.
The class can optionally pickle your data for you to use later.
"""
class TurkingModule:

    """
    **SUMMARY**

    Init sets up the turking module.

    **PARAMETERS**

    * *source_path* - A list of the path(s) with the images to be turked.
    * *out_path* - the output path, a directory will be created for each class.
    * *classes* - the names of the classes you are turking as a list of strings.
    * *key_bindings* - the keys to bind to each class when turking.
    * *preprocess* - a preprocess function. It should take in an image and return a list of images.
    * *postprocess* a post-process step. The signature should be image in and image out.


    **EXAMPLE**
    >>>> def GetBlobs(img):
    >>>>   blobs = img.findBlobs()
    >>>>    return [b.mMask for b in blobs]

    >>>> def ScaleIng(img):
    >>>>    return img.resize(100,100).invert()

    >>>> turker = TurkingModule(['./data/'],['./turked/'],['apple','banana','cherry'],['a','b','c'],preProcess=GetBlobs,postProcess=ScaleInv]
    >>>> turker.turk()
    >>>> # ~~~ stuff ~~~
    >>>> turker.save('./derp.pkl')

    ** TODO **

    TODO: Make it so you just pickle the data and don't have to save each file

    """
    def __init__(self,source_paths,out_path,classList,key_bindings,preprocess=None, postprocess=None):

        #if( not os.access(out_path,os.W_OK) ):
        #    print "Output path is not writeable."
        #    raise Exception("Output path is not writeable.")

        self.keyBindings = key_bindings
        self.classes = classList
        self.countMap = {}
        self.classMap = {}
        self.directoryMap = {}
        self.out_path = out_path
        self.keyMap = {}
        if( len(classList)!=len(key_bindings)):
            print "Must have a key for each class."
            raise Exception("Must have a key for each class.")
        for key,cls in zip(key_bindings,classList):
            self.keyMap[key] = cls
        # this should work

        if( preprocess is None ):
            def fakeProcess(img):
                return [img]
            preprocess = fakeProcess
        self.preProcess = preprocess

        if( postprocess is None ):
            def fakePostProcess(img):
                return img
            postprocess = fakePostProcess

        self.postProcess = postprocess

        self.srcImgs = ImageSet()

        if( isinstance(source_paths,ImageSet) ):
            self.srcImgs = source_path
        else:
            for sp in source_paths:
                print "Loading " + sp
                imgSet = ImageSet(sp)
                print "Loaded " + str(len(imgSet))
                self.srcImgs += imgSet

        if( not osp.exists(out_path) ):
            os.mkdir(out_path)
        for c in classList:
            outdir = out_path+c+'/'
            self.directoryMap[c] = outdir
            if( not osp.exists(outdir) ):
                os.mkdir(outdir)

        for c in classList:
            searchstr = self.directoryMap[c]+'*.png'
            fc = glob.glob(searchstr)
            self.countMap[c] = len(fc)
            self.classMap[c] = ImageSet(self.directoryMap[c])

    def _saveIt(self,img,classType):
        img.clearLayers()
        path = self.out_path + classType + "/" + classType+str(self.countMap[classType])+".png"
        print "Saving: " + path
        img = self.postProcess(img)
        self.classMap[classType].append(img)
        img.save(path)
        self.countMap[classType] = self.countMap[classType] + 1

    def getClass(self,className):
        """
        **SUMMARY**

        Returns the image set that has been turked for the given class.

        **PARAMETERS**

        * *className* - the class name as a string.

        **RETURNS**

        An image set on success, None on failure.

        **EXAMPLE**

        >>>> # Do turking
        >>>> iset = turkModule.getClass('cats')
        >>>> iset.show()
        """
        if(className in self.classMap):
            return self.classMap[className]
        else:
            return None

    def _drawControls(self,img,font_size,color,spacing ):
        img.drawText("space - skip",10,spacing,fontsize=font_size,color=color)
        img.drawText("esc - exit",10,2*spacing,fontsize=font_size,color=color)
        y = 3*spacing
        for k,cls in self.keyMap.items():
            str = k + " - " + cls
            img.drawText(str,10,y,fontsize=font_size,color=color)
            y = y + spacing
        return img

    def turk(self,saveOriginal=False,disp_size=(800,600),showKeys=True,font_size=16,color=Color.RED,spacing=10 ):
        """
        **SUMMARY**

        This function does the turning of the data. The method goes through each image,
        applies the preprocessing (which can return multiple images), displays each image
        with an optional display of the key mapping. The user than selects the key that describes
        the class of the image. The image is then post processed and saved to the directory.
        The escape key kills the turking, the space key skips an image.

        **PARAMETERS**

        * *saveOriginal* - if true save the original image versus the preprocessed image.
        * *disp_size* - size of the display to create.
        * *showKeys* - Show the key mapping for the turking. Note that on small images this may not render correctly.
        * *font_size* - the font size for the turking display.
        * *color* - the font color.
        * *spacing* - the spacing between each line of text on the display.

        **RETURNS**

        Nothing but stores each image in the directory. The image sets are also available
        via the getClass method.

        **EXAMPLE**

        >>>> def GetBlobs(img):
        >>>>     blobs = img.findBlobs()
        >>>>     return [b.mMask for b in blobs]

        >>>> def ScaleIng(img):
        >>>>     return img.resize(100,100).invert()

        >>>> turker = TurkingModule(['./data/'],['./turked/'],['apple','banana','cherry'],['a','b','c'],preProcess=GetBlobs,postProcess=ScaleInv]
        >>>> turker.turk()
        >>>> # ~~~ stuff ~~~
        >>>> turker.save('./derp.pkl')

        ** TODO **
        TODO: fix the display so that it renders correctly no matter what the image size.
        TODO: Make it so you can stop and start turking at any given spot in the process
        """
        disp = Display(disp_size)
        bail = False
        for img in self.srcImgs:
            print img.filename
            samples = self.preProcess(img)
            for sample in samples:
                if( showKeys ):
                    sample = self._drawControls(sample,font_size,color,spacing )

                sample.save(disp)
                gotKey = False
                while( not gotKey ):
                    keys = disp.checkEvents(True)
                    for k in keys:
                        if k in self.keyMap:
                            if saveOriginal:
                                self._saveIt(img,self.keyMap[k])
                            else:
                                self._saveIt(sample,self.keyMap[k])
                            gotKey = True
                        if k == 'space':
                            gotKey = True # skip
                        if k == 'escape':
                            return

    def save(self,fname):
        """
        **SUMMARY**

        Pickle the relevant data from the turking.

        ** PARAMETERS **

        * *fname* - the file fame.
        """
        saveThis = [self.classes,self.directoryMap,self.classMap,self.countMap]
        pickle.dump( saveThis, open( fname, "wb" ) )

    # todo: eventually we should allow the user to randomly
    # split up the data set and then save it.
    # def splitTruthTest(self)

########NEW FILE########
__FILENAME__ = TurkingModuleExample
from SimpleCV import Image, Features, Color, ImageSet, Display
from SimpleCV.MachineLearning import TurkingModule
# This example shows how to use the turking module.
# by turking we mean manually sorting and classifying images
# ostensibly for supervised learning

# return images of all the big blobs in our source images
def preprocess(img):
    blobs = img.findBlobs(minsize=200)
    blobs = blobs.sortArea()
    return [b.mImg for b in blobs]

# once we are done invert them
def postprocess(img):
    return img.invert()

# we are going to turk two things junk and stuff
classes = ['junk','stuff']
# press j for junk and s for stuff
key_bind = ['j','s']
# put stuff right here
outdir = './'
# use the sample images directory for our sources
input = ['../sampleimages/']

# set everything up
turker = TurkingModule(input,outdir,classes,key_bind,preprocess,postprocess)

# run the turking, the display window must have focus
turker.turk(font_size=16, color = Color.BLUE, spacing=18)

# show what we got
print "="*30
print "TURKING DONE!"
for c in classes:
    print "="*30
    print "Showing " + c
    iset = turker.getClass(c)
    iset.show(0.1)

# save the results
turker.save('junkAndStuff.pkl')

########NEW FILE########
__FILENAME__ = ColorSegmentation
from SimpleCV.base import *
from SimpleCV.Features import Feature, FeatureSet, BlobMaker
from SimpleCV.ColorModel import ColorModel
from SimpleCV.Color import Color
from SimpleCV.ImageClass import Image
from SimpleCV.Segmentation.SegmentationBase import SegmentationBase

class ColorSegmentation(SegmentationBase):
    """
    Perform color segmentation based on a color model or color provided. This class
    uses ColorModel.py to create a color model.
    """
    mColorModel = []
    mError = False
    mCurImg = []
    mTruthImg = []
    mBlobMaker = []

    def __init__(self):
        self.mColorModel = ColorModel()
        self.mError = False
        self.mCurImg = Image()
        self.mTruthImg = Image()
        self.mBlobMaker = BlobMaker()


    def addImage(self, img):
        """
        Add a single image to the segmentation algorithm
        """
        self.mTruthImg = img
        self.mCurImg = self.mColorModel.threshold(img)
        return


    def isReady(self):
        """
        Returns true if the camera has a segmented image ready.
        """
        return True;

    def isError(self):
        """
        Returns true if the segmentation system has detected an error.
        Eventually we'll consruct a syntax of errors so this becomes
        more expressive
        """
        return self.mError #need to make a generic error checker

    def resetError(self):
        """
        Clear the previous error.
        """
        self.mError = false
        return

    def reset(self):
        """
        Perform a reset of the segmentation systems underlying data.
        """
        self.mColorModel.reset()

    def getRawImage(self):
        """
        Return the segmented image with white representing the foreground
        and black the background.
        """
        return self.mCurImg

    def getSegmentedImage(self, whiteFG=True):
        """
        Return the segmented image with white representing the foreground
        and black the background.
        """
        return self.mCurImg

    def getSegmentedBlobs(self):
        """
        return the segmented blobs from the fg/bg image
        """
        return self.mBlobMaker.extractFromBinary(self.mCurImg,self.mTruthImg)

    # The following are class specific methods

    def addToModel(self, data):
        self.mColorModel.add(data)

    def subtractModel(self, data):
        self.mColorModel.remove(data)

    def __getstate__(self):
        mydict = self.__dict__.copy()
        self.mBlobMaker = None
        del mydict['mBlobMaker']
        return mydict

    def __setstate__(self, mydict):
        self.__dict__ = mydict
        self.mBlobMaker = BlobMaker()

########NEW FILE########
__FILENAME__ = DiffSegmentation
from SimpleCV.base import *
from SimpleCV.Features import Feature, FeatureSet, BlobMaker
from SimpleCV.ImageClass import Image
from SimpleCV.Segmentation.SegmentationBase import SegmentationBase


class DiffSegmentation(SegmentationBase):
    """
    This method will do image segmentation by looking at the difference between
    two frames.

    grayOnly - use only gray images.
    threshold - The value at which we consider the color difference to
    be significant enough to be foreground imagery.

    The general usage is

    >>> segmentor = DiffSegmentation()
    >>> cam = Camera()
    >>> while(1):
    >>>    segmentor.addImage(cam.getImage())
    >>>    if(segmentor.isReady()):
    >>>        img = segmentor.getSegmentedImage()

    """
    mError = False
    mLastImg = None
    mCurrImg = None
    mDiffImg = None
    mColorImg = None
    mGrayOnlyMode = True
    mThreshold = 10
    mBlobMaker = None

    def __init__(self, grayOnly=False, threshold = (10,10,10) ):
        self.mGrayOnlyMode = grayOnly
        self.mThreshold = threshold
        self.mError = False
        self.mCurrImg = None
        self.mLastImg = None
        self.mDiffImg = None
        self.mColorImg = None
        self.mBlobMaker = BlobMaker()

    def addImage(self, img):
        """
        Add a single image to the segmentation algorithm
        """
        if( img is None ):
            return
        if( self.mLastImg == None ):
            if( self.mGrayOnlyMode ):
                self.mLastImg = img.toGray()
                self.mDiffImg = Image(self.mLastImg.getEmpty(1))
                self.mCurrImg = None
            else:
                self.mLastImg = img
                self.mDiffImg = Image(self.mLastImg.getEmpty(3))
                self.mCurrImg = None
        else:
            if( self.mCurrImg is not None ): #catch the first step
                self.mLastImg = self.mCurrImg

            if( self.mGrayOnlyMode ):
                self.mColorImg = img
                self.mCurrImg = img.toGray()
            else:
                self.mColorImg = img
                self.mCurrImg = img


            cv.AbsDiff(self.mCurrImg.getBitmap(),self.mLastImg.getBitmap(),self.mDiffImg.getBitmap())

        return


    def isReady(self):
        """
        Returns true if the camera has a segmented image ready.
        """
        if( self.mDiffImg is None ):
            return False
        else:
            return True


    def isError(self):
        """
        Returns true if the segmentation system has detected an error.
        Eventually we'll consruct a syntax of errors so this becomes
        more expressive
        """
        return self.mError #need to make a generic error checker

    def resetError(self):
        """
        Clear the previous error.
        """
        self.mError = False
        return

    def reset(self):
        """
        Perform a reset of the segmentation systems underlying data.
        """
        self.mCurrImg = None
        self.mLastImg = None
        self.mDiffImg = None

    def getRawImage(self):
        """
        Return the segmented image with white representing the foreground
        and black the background.
        """
        return self.mDiffImg

    def getSegmentedImage(self, whiteFG=True):
        """
        Return the segmented image with white representing the foreground
        and black the background.
        """
        retVal = None
        if( whiteFG ):
            retVal = self.mDiffImg.binarize(thresh=self.mThreshold)
        else:
            retVal = self.mDiffImg.binarize(thresh=self.mThreshold).invert()
        return retVal

    def getSegmentedBlobs(self):
        """
        return the segmented blobs from the fg/bg image
        """
        retVal = []
        if( self.mColorImg is not None and self.mDiffImg is not None ):
            retVal = self.mBlobMaker.extractFromBinary(self.mDiffImg.binarize(thresh=self.mThreshold),self.mColorImg)
        return retVal

    def __getstate__(self):
        mydict = self.__dict__.copy()
        self.mBlobMaker = None
        del mydict['mBlobMaker']
        return mydict

    def __setstate__(self, mydict):
        self.__dict__ = mydict
        self.mBlobMaker = BlobMaker()

########NEW FILE########
__FILENAME__ = MOGSegmentation
from SimpleCV.Features import Feature, FeatureSet, BlobMaker
from SimpleCV.ImageClass import Image
from SimpleCV.Segmentation.SegmentationBase import SegmentationBase

class MOGSegmentation(SegmentationBase):
    """
    Background subtraction using mixture of gausians.
    For each pixel store a set of gaussian distributions and try to fit new pixels
    into those distributions. One of the distributions will represent the background.
    
    history - length of the pixel history to be stored
    nMixtures - number of gaussian distributions to be stored per pixel
    backgroundRatio - chance of a pixel being included into the background model
    noiseSigma - noise amount
    learning rate - higher learning rate means the system will adapt faster to new backgrounds
    """

    mError = False
    mDiffImg = None
    mColorImg = None
    mReady = False
    
    # OpenCV default parameters
    history = 200
    nMixtures = 5
    backgroundRatio = 0.7
    noiseSigma = 15
    learningRate = 0.7
    bsMOG = None

    def __init__(self, history = 200, nMixtures = 5, backgroundRatio = 0.7, noiseSigma = 15, learningRate = 0.7):
        
        try:
            import cv2            
        except ImportError:
            raise ImportError("Cannot load OpenCV library which is required by SimpleCV")
            return    
        if not hasattr(cv2, 'BackgroundSubtractorMOG'):
            raise ImportError("A newer version of OpenCV is needed")
            return            
        
        self.mError = False
        self.mReady = False        
        self.mDiffImg = None
        self.mColorImg = None
        self.mBlobMaker = BlobMaker()
        
        self.history = history
        self.nMixtures = nMixtures
        self.backgroundRatio = backgroundRatio
        self.noiseSigma = noiseSigma
        self.learningRate = learningRate
        
        self.mBSMOG = cv2.BackgroundSubtractorMOG(history, nMixtures, backgroundRatio, noiseSigma)
        
        
        

    def addImage(self, img):
        """
        Add a single image to the segmentation algorithm
        """
        if( img is None ):
            return

        self.mColorImg = img
        self.mDiffImg = Image(self.mBSMOG.apply(img.getNumpyCv2(), None, self.learningRate), cv2image=True) 
        self.mReady = True
        return


    def isReady(self):
        """
        Returns true if the camera has a segmented image ready.
        """
        return self.mReady


    def isError(self):
        """
        Returns true if the segmentation system has detected an error.
        Eventually we'll consruct a syntax of errors so this becomes
        more expressive
        """
        return self.mError #need to make a generic error checker

    def resetError(self):
        """
        Clear the previous error.
        """
        self.mError = false
        return

    def reset(self):
        """
        Perform a reset of the segmentation systems underlying data.
        """
        self.mModelImg = None
        self.mDiffImg = None

    def getRawImage(self):
        """
        Return the segmented image with white representing the foreground
        and black the background.
        """
        return self.mDiffImg

    def getSegmentedImage(self, whiteFG=True):
        """
        Return the segmented image with white representing the foreground
        and black the background.
        """        
        return self.mDiffImg

    def getSegmentedBlobs(self):
        """
        return the segmented blobs from the fg/bg image
        """
        retVal = []
        if( self.mColorImg is not None and self.mDiffImg is not None ):
            retVal = self.mBlobMaker.extractFromBinary(self.mDiffImg ,self.mColorImg)
        return retVal


    def __getstate__(self):
        mydict = self.__dict__.copy()
        self.mBlobMaker = None
        self.mDiffImg = None
        del mydict['mBlobMaker']
        del mydict['mDiffImg']
        return mydict

    def __setstate__(self, mydict):
        self.__dict__ = mydict
        self.mBlobMaker = BlobMaker()
        

########NEW FILE########
__FILENAME__ = RunningSegmentation
from SimpleCV.base import *
from SimpleCV.Features import Feature, FeatureSet, BlobMaker
from SimpleCV.ImageClass import Image
from SimpleCV.Segmentation.SegmentationBase import SegmentationBase

class RunningSegmentation(SegmentationBase):
    """
    RunningSegmentation performs segmentation using a running background model.
    This model uses an accumulator which performs a running average of previous frames
    where:
    accumulator = ((1-alpha)input_image)+((alpha)accumulator)
    """

    mError = False
    mAlpha = 0.1
    mThresh = 10
    mModelImg = None
    mDiffImg = None
    mCurrImg = None
    mBlobMaker = None
    mGrayOnly = True
    mReady = False

    def __init__(self, alpha=0.7, thresh=(20,20,20)):
        """
        Create an running background difference.
        alpha - the update weighting where:
        accumulator = ((1-alpha)input_image)+((alpha)accumulator)

        threshold - the foreground background difference threshold.
        """
        self.mError = False
        self.mReady = False
        self.mAlpha = alpha
        self.mThresh = thresh
        self.mModelImg = None
        self.mDiffImg = None
        self.mColorImg = None
        self.mBlobMaker = BlobMaker()

    def addImage(self, img):
        """
        Add a single image to the segmentation algorithm
        """
        if( img is None ):
            return

        self.mColorImg = img
        if( self.mModelImg == None ):
            self.mModelImg = Image(cv.CreateImage((img.width,img.height), cv.IPL_DEPTH_32F, 3))
            self.mDiffImg = Image(cv.CreateImage((img.width,img.height), cv.IPL_DEPTH_32F, 3))
        else:
            # do the difference
            cv.AbsDiff(self.mModelImg.getBitmap(),img.getFPMatrix(),self.mDiffImg.getBitmap())
            #update the model
            cv.RunningAvg(img.getFPMatrix(),self.mModelImg.getBitmap(),self.mAlpha)
            self.mReady = True
        return


    def isReady(self):
        """
        Returns true if the camera has a segmented image ready.
        """
        return self.mReady


    def isError(self):
        """
        Returns true if the segmentation system has detected an error.
        Eventually we'll consruct a syntax of errors so this becomes
        more expressive
        """
        return self.mError #need to make a generic error checker

    def resetError(self):
        """
        Clear the previous error.
        """
        self.mError = false
        return

    def reset(self):
        """
        Perform a reset of the segmentation systems underlying data.
        """
        self.mModelImg = None
        self.mDiffImg = None

    def getRawImage(self):
        """
        Return the segmented image with white representing the foreground
        and black the background.
        """
        return self._floatToInt(self.mDiffImg)

    def getSegmentedImage(self, whiteFG=True):
        """
        Return the segmented image with white representing the foreground
        and black the background.
        """
        retVal = None
        img = self._floatToInt(self.mDiffImg)
        if( whiteFG ):
            retVal = img.binarize(thresh=self.mThresh)
        else:
            retVal = img.binarize(thresh=self.mThresh).invert()
        return retVal

    def getSegmentedBlobs(self):
        """
        return the segmented blobs from the fg/bg image
        """
        retVal = []
        if( self.mColorImg is not None and self.mDiffImg is not None ):

            eightBit = self._floatToInt(self.mDiffImg)
            retVal = self.mBlobMaker.extractFromBinary(eightBit.binarize(thresh=self.mThresh),self.mColorImg)

        return retVal


    def _floatToInt(self,input):
        """
        convert a 32bit floating point cv array to an int array
        """
        temp = cv.CreateImage((input.width,input.height), cv.IPL_DEPTH_8U, 3)
        cv.Convert(input.getBitmap(),temp)

        return Image(temp)

    def __getstate__(self):
        mydict = self.__dict__.copy()
        self.mBlobMaker = None
        self.mModelImg = None
        self.mDiffImg = None
        del mydict['mBlobMaker']
        del mydict['mModelImg']
        del mydict['mDiffImg']
        return mydict

    def __setstate__(self, mydict):
        self.__dict__ = mydict
        self.mBlobMaker = BlobMaker()

########NEW FILE########
__FILENAME__ = SegmentationBase
from SimpleCV.base import *
from SimpleCV.Features import Feature, FeatureSet
from SimpleCV.Color import Color
from SimpleCV.ImageClass import Image

class SegmentationBase(object):
    """
    Right now I am going to keep this class as brain dead and single threaded as
    possible just so I can get the hang of abc in python. The idea behind a segmentation
    object is that you pass it frames, it does some sort of operations and you
    get a foreground / background segemnted image. Eventually I would like
    these processes to by asynchronous and multithreaded so that they can raise
    specific image processing events.
    """

    __metaclass__ = abc.ABCMeta

    def load(cls, fname):
        """
        load segmentation settings to file.
        """
        return pickle.load(file(fname))
    load = classmethod(load)


    def save(self, fname):
        """
        Save segmentation settings to file.
        """
        output = open(fname, 'wb')
        pickle.dump(self,output,2) # use two otherwise it borks the system
        output.close()

    @abc.abstractmethod
    def addImage(self, img):
        """
        Add a single image to the segmentation algorithm
        """
        return

    @abc.abstractmethod
    def isReady(self):
        """
        Returns true if the camera has a segmented image ready.
        """
        return False

    @abc.abstractmethod
    def isError(self):
        """
        Returns true if the segmentation system has detected an error.
        Eventually we'll consruct a syntax of errors so this becomes
        more expressive
        """
        return False

    @abc.abstractmethod
    def resetError(self):
        """
        Clear the previous error.
        """
        return False

    @abc.abstractmethod
    def reset(self):
        """
        Perform a reset of the segmentation systems underlying data.
        """

    @abc.abstractmethod
    def getRawImage(self):
        """
        Return the segmented image with white representing the foreground
        and black the background.
        """

    @abc.abstractmethod
    def getSegmentedImage(self, whiteFG=True):
        """
        Return the segmented image with white representing the foreground
        and black the background.
        """

    @abc.abstractmethod
    def getSegmentedBlobs(self):
        """
        return the segmented blobs from the fg/bg image
        """

########NEW FILE########
__FILENAME__ = Example
#!/usr/bin/python
import os
import glob
from subprocess import call
from SimpleCV import *
import sys


def listFiles(directory):
    for path, dirs, files in os.walk(directory):
        for f in files:
            yield os.path.join(path, f)


def magic_examples(self, arg):
    DIR = os.path.join(LAUNCH_PATH, 'examples')
    files = [f for f in listFiles(DIR) if f.endswith('.py')]
    file_names = [file.split('/')[-1] for file in files]
    iarg = None
    arg = str(arg)

    try:
        iarg = int(arg)
    except:
        pass

    if isinstance(arg, str) and arg == "":
        counter = 0
        print "Available Examples:"
        print "--------------------------------------------"
        for file in file_names:
            print "[",counter,"]:",file
            counter += 1

        print "Just type example #, to run the example on the list"
        print "for instance: example 1"
        print ""
        print "Close the window or press ctrl+c to stop the example"

    elif isinstance(iarg, int):
        print "running example:", files[iarg]
        try:
            call([sys.executable, files[iarg]])
        except:
            print "Couldn't run example:", files[iarg]


    elif isinstance(arg, str) and arg.lower() == "joshua":
        print "GREETINGS PROFESSOR FALKEN"
        print ""
        print "HELLO"
        print ""
        print "A STRANGE GAME."
        print "THE ONLY WINNING MOVE IS"
        print "NOT TO PLAY."
        print ""
        print "HOW ABOUT A NICE GAME OF CHESS?"
        print ""


    else:
        print "Example: " + arg + " does not exist, or an error occurred"

########NEW FILE########
__FILENAME__ = Shell
#!/usr/bin/python

#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# SimpleCV
# a kinder, gentler machine vision python library
#-----------------------------------------------------------------------
# SimpleCV is an interface for Open Source machine
# vision libraries in Python.
# It provides a concise, readable interface for cameras,
# image manipulation, feature extraction, and format conversion.
# Our mission is to give casual users a comprehensive interface
# for basic machine vision functions and an
# elegant programming interface for advanced users.
#
# more info:
# http://www.simplecv.org
#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


#load system libraries
from subprocess import call
import platform
import webbrowser
import sys

from SimpleCV.__init__ import *

#Load simpleCV libraries
from SimpleCV.Shell.Tutorial import *
from SimpleCV.Shell.Example import *

try:
    from SimpleCV import __version__ as SIMPLECV_VERSION
except ImportError:
    SIMPLECV_VERSION = ''


#Command to clear the shell screen
def shellclear():
    if platform.system() == "Windows":
        return
    call("clear")

#method to get magic_* methods working in bpython
def make_magic(method):
    def wrapper(*args, **kwargs):
        if not args:
            return method('', '')
        return method('', *args, **kwargs)

    return wrapper


def plot(arg):
    try:
        import matplotlib.pyplot as plt
    except ImportError:
        logger.warning("Matplotlib is not installed and required")
        return

    print "args", arg
    print "type", type(arg)
    plt.plot(arg)
    plt.show()


def hist(arg):
    try:
        import pylab
    except ImportError:
        logger.warning("pylab is not installed and required")
        return
    plot(pylab.hist(arg)[1])


def magic_clear(self, arg):
    shellclear()

def magic_forums(self, arg):
    webbrowser.open('http://help.simplecv.org/questions/')

def magic_walkthrough(self, arg):
    webbrowser.open('http://examples.simplecv.org/en/latest/')

def magic_docs(self, arg):
    webbrowser.open('http://www.simplecv.org/docs/')

banner = '+-----------------------------------------------------------+\n'
banner += ' SimpleCV '
banner += SIMPLECV_VERSION
banner += ' [interactive shell] - http://simplecv.org\n'
banner += '+-----------------------------------------------------------+\n'
banner += '\n'
banner += 'Commands: \n'
banner += '\t"exit()" or press "Ctrl+ D" to exit the shell\n'
banner += '\t"clear()" to clear the shell screen\n'
banner += '\t"tutorial()" to begin the SimpleCV interactive tutorial\n'
banner += '\t"example()" gives a list of examples you can run\n'
banner += '\t"forums()" will launch a web browser for the help forums\n'
banner += '\t"walkthrough()" will launch a web browser with a walkthrough\n'
banner += '\n'
banner += 'Usage:\n'
banner += '\tdot complete works to show library\n'
banner += '\tfor example: Image().save("/tmp/test.jpg") will dot complete\n'
banner += '\tjust by touching TAB after typing Image().\n'
banner += '\n'
banner += 'Documentation:\n'
banner += '\thelp(Image), ?Image, Image?, or Image()? all do the same\n'
banner += '\t"docs()" will launch webbrowser showing documentation'
banner += '\n'
exit_msg = '\n... [Exiting the SimpleCV interactive shell] ...\n'


def setup_ipython():
    try:
        import IPython
        from IPython.config.loader import Config
        from IPython.frontend.terminal.embed import InteractiveShellEmbed

        cfg = Config()
        cfg.PromptManager.in_template = "SimpleCV:\\#> "
        cfg.PromptManager.out_template = "SimpleCV:\\#: "
        #~ cfg.InteractiveShellEmbed.prompt_in1 = "SimpleCV:\\#> "
        #~ cfg.InteractiveShellEmbed.prompt_out="SimpleCV:\\#: "
        scvShell = InteractiveShellEmbed(config=cfg, banner1=banner,
                                         exit_msg=exit_msg)
        scvShell.define_magic("tutorial", magic_tutorial)
        scvShell.define_magic("clear", magic_clear)
        scvShell.define_magic("example", magic_examples)
        scvShell.define_magic("forums", magic_forums)
        scvShell.define_magic("walkthrough", magic_walkthrough)
        scvShell.define_magic("docs", magic_docs)
    except ImportError:
        try:
            from IPython.Shell import IPShellEmbed

            argsv = ['-pi1', 'SimpleCV:\\#>', '-pi2', '   .\\D.:', '-po',
                     'SimpleCV:\\#>', '-nosep']
            scvShell = IPShellEmbed(argsv)
            scvShell.set_banner(banner)
            scvShell.set_exit_msg(exit_msg)
            scvShell.IP.api.expose_magic("tutorial", magic_tutorial)
            scvShell.IP.api.expose_magic("clear", magic_clear)
            scvShell.IP.api.expose_magic("example", magic_examples)
            scvShell.IP.api.expose_magic("forums", magic_forums)
            scvShell.IP.api.expose_magic("walkthrough", magic_walkthrough)
            scvShell.IP.api.expose_magic("docs", magic_docs)
        except ImportError:
            raise

    return scvShell()


def setup_bpython():
    import bpython
    example = make_magic(magic_examples)
    clear = make_magic(magic_clear)
    docs = make_magic(magic_docs)
    tutorial = make_magic(magic_tutorial)
    walkthrough = make_magic(magic_walkthrough)
    forums = make_magic(magic_forums)
    temp = locals().copy()
    temp.update(globals())
    return bpython.embed(locals_=temp, banner=banner)


def setup_plain():
    import code

    return code.interact(banner=banner, local=globals())


def run_notebook(mainArgs):

    if IPython.__version__.startswith('1.'):
        """Run the ipython notebook server"""
        from IPython.html import notebookapp
        from IPython.html.services.kernels import kernelmanager
    else:
        from IPython.frontend.html.notebook import notebookapp
        from IPython.frontend.html.notebook import kernelmanager

    code = ""
    code += "from SimpleCV import *;"
    code += "init_options_handler.enable_notebook();"

    kernelmanager.MappingKernelManager.first_beat = 30.0
    app = notebookapp.NotebookApp.instance()
    mainArgs += [
        '--port', '5050',
        '--c', code,
    ]
    app.initialize(mainArgs)
    app.start()
    sys.exit()


def self_update():
    URL = "https://github.com/sightmachine/SimpleCV/zipball/master"
    command = "pip install -U %s" % URL

    if os.getuid() == 0:
        command = "sudo " + command

    returncode = call(command, shell=True)
    sys.exit()


def run_shell(shell=None):
    shells = ['setup_ipython', 'setup_bpython', 'setup_plain']
    available_shells = [shell] if shell else shells

    for shell in available_shells:
        try:
            return globals()[shell]()
        except ImportError:
            pass
    raise ImportError


def main(*args):
    log_level = logging.WARNING
    interface = None

    if len(sys.argv) > 1 and len(sys.argv[1]) > 1:
        flag = sys.argv[1]

        if flag == 'notebook':
            run_notebook(sys.argv[1:])
            sys.exit()

        elif flag == 'update':
            print "Updating SimpleCV....."
            self_update()

        if flag in ['--headless', 'headless']:
            # set SDL to use the dummy NULL video driver,
            #   so it doesn't need a windowing system.
            os.environ["SDL_VIDEODRIVER"] = "dummy"

        elif flag in ['--nowarnings', 'nowarnings']:
            log_level = logging.INFO

        elif flag in ['--debug', 'debug']:
            log_level = logging.DEBUG

        if flag in ['--ipython', 'ipython']:
            interface = 'setup_ipython'

        elif flag in ['--bpython', 'bpython']:
            interface = 'setup_bpython'
        else:
            interface = 'setup_plain'

    init_logging(log_level)
    shellclear()
    scvShell = run_shell(interface)

########NEW FILE########
__FILENAME__ = Tutorial
#!/usr/bin/python

# A Basic SimpleCV interactive shell tutorial

#load required libraries
from SimpleCV import *

from subprocess import call
from code import InteractiveInterpreter
import platform

lb = "\n" #linebreak
tb = "\t" #tab
tutorial_interpreter = InteractiveInterpreter(globals())
logo = None
img = None
clone = None
thumb = None
eroded = None
cropped = None

#Command to clear the shell screen
def shellclear():
    if platform.system() == "Windows":
        return
    call("clear")

def attempt(variable_name, desired_class):
    prompt_and_run()
    variable = globals().get(variable_name)

    if isinstance(variable,desired_class):
        if desired_class == Image:
            if variable.isEmpty():
                print lb
                print "Although you can create empty Images on SimpleCV, let's not"
                print "play with that now!"
                print lb
                return False

        return True

    return False

def prompt_and_run():

    command = raw_input("SimpleCV:> ")
    tutorial_interpreter.runsource(command)
    return command

def request_show_command():
    while True:
        if prompt_and_run().endswith('.show()'):
            return

def end_tutorial():
    print lb
    print "Type 'quit' to leave the tutorials, or press Enter to move on!"
    command = raw_input("SimpleCV:> ")
    return command.lower() == 'quit'

def end_of_tutorial():
    print lb
    print "This is the end of our tutorial!"
    print lb
    print "For more help, go to www.simplecv.org, and don't forget about the"
    print "help function!"
    print lb

def command_loop(command, desired_tuple):
    while True:
        print command
        print lb

        if attempt(desired_tuple[0], desired_tuple[1]):
            return

        print lb
        print "Oops! %s is still not %s" % (desired_tuple[0], str(desired_tuple[1]))

def tutorial_image():
    shellclear()
    print "SimpleCV Image tutorial"
    print "-----------------------"
    print lb
    print "Using images is simple, in SimpleCV."
    print lb
    print "First thing we are going to do is load an image. Try it yourself:"
    print lb

    cmd = "logo = Image(\"simplecv\")"
    desired_tuple = ('logo', Image)
    command_loop(cmd, desired_tuple)

    print lb
    print "Correct! You just loaded SimpleCV logo into memory."
    print "Let's try it to use one of your images. There are different ways to"
    print "do that. You can try, for example:"
    print lb
    print "img = Image(URL_TO_MY_PICTURE) or img = Image(PATH_TO_MY_PICTURE)"
    print lb
    cmd =  "Example: img = Image('http://simplecv.org/logo.jpg')"

    desired_tuple = ('img', Image)
    command_loop(cmd, desired_tuple)

    print lb
    print "Perfect! Now we want to see it:"
    print lb
    cmd = "img.show()"
    print cmd
    print lb

    request_show_command()

    print lb
    print "Alright! This was tutorial 1/6."
    print "Next tutorial: Saving Images"
    if not end_tutorial():
        tutorial_save()
    return

def tutorial_save():
    shellclear()
    print "Saving Images"
    print lb
    print "Once you have an Image Object loaded in memory you can"
    print "now save it to disk."
    print lb
    raw_input("[Press enter to continue]")
    print lb
    print "Saving an image is very simple, pardon the pun. Once it's loaded"
    print "into memory, it's literally just:"
    print "img.save()"
    print lb
    print "This will save the image back to the location it was loaded from"
    print "so if you did img = Image('/tmp/test.jpg'), then it would save"
    print "it back there, otherwise you can do:"
    print "img.save('/any/path/you/want')"
    print lb
    print "So try it now and save an image somewhere on your system"
    print lb

    if platform.system() == "Windows":
        print "img.save('C:/myimg.jpg')"
    else:
        print "img.save('/tmp/new.jpg')"

    print lb

    while True:
        if prompt_and_run().startswith('img.save'):
            break
        print "Please try to save img!"
        print lb

    print "Correct, you just saved a new copy of your image!"
    print "As you can see in SimpleCV most of the functions are intuitive."

    print lb
    print "Alright! This was tutorial 2/6."
    print "Next tutorial: Camera"
    if not end_tutorial():
        tutorial_camera()
    return


def tutorial_camera():
    shellclear()
    print "Camera"
    print lb
    print "As long as your camera driver is supported then you shouldn't have a"
    print "problem. Type 'skip' to skip the camera tutorial, or press Enter to"
    print "continue."
    print lb

    command = raw_input("SimpleCV:> ")
    if command.lower() != 'skip':
        print lb
        print "To load the camera, just type:"
        print lb

        cmd = "cam = Camera()"
        desired_tuple = ('cam', Camera)
        command_loop(cmd, desired_tuple)

        print lb
        print "Next, to grab an image from the Camera we type:"
        cmd = "img = cam.getImage()"
        tutorial_interpreter.runsource("del(img)")
        desired_tuple = ('img', Image)
        command_loop(cmd, desired_tuple)

        print "Just as before, if we want to display it, we just type:"
        print lb
        print "img.show()"
        print lb

        request_show_command()

    print lb
    print "Alright! This was tutorial 3/6."
    print "Next tutorial: Copying Images"
    if not end_tutorial():
        tutorial_copy()
    return

def tutorial_copy():
    shellclear()
    print "Copying Images"
    print lb
    print "If you need a copy of an image, this is also very simple:"
    print "Let's try to clone img, which we already have."

    global img
    if not img:
        img = Image("lenna")

    print lb
    cmd = "clone = img.copy()"
    desired_tuple = ('clone', Image)

    while True:
        command_loop(cmd, desired_tuple)
        if clone != img: #Returns False if they have different addresses.
            break

        print "You have to use the copy() function!"

    print lb
    print "Correct, you just cloned an image into memory."
    print "You need to be careful when using this method though as using as a"
    print "reference vs. a copy.  For instance, if you just typed:"
    print lb
    print "clone = img"
    print lb
    print "clone would actually point at the same thing in memory as img."

    print lb
    print "Alright! This was tutorial 4/6."
    print "Next tutorial: Manipulating Images"
    if not end_tutorial():
        tutorial_manipulation()
    return

def tutorial_manipulation():
    shellclear()
    print "Manipulating Images"
    print lb
    print "Now we can easily load and save images. It's time to start doing some"
    print "image processing with them. Let's make img, which we already have, a"
    print "90x90 thumbnail:"

    global img
    if not img:
        img = Image("lenna")

    print lb
    cmd = "thumb = img.scale(90,90)"
    desired_tuple = ('thumb', Image)

    while True:
        command_loop(cmd, desired_tuple)
        if thumb.size() == (90,90):
            break

        print "Your thumbnail's size isn't 90x90! Try again!"

    print lb
    print "Now display it with thumb.show()"
    print lb
    request_show_command()

    print lb
    print "Now let's erode the picture some:"
    print lb
    cmd = "eroded = img.erode()"
    desired_tuple = ('eroded', Image)
    command_loop(cmd, desired_tuple)

    print lb
    print "Display it with eroded.show(). It should look almost as if the image"
    print "was made if ink and had water spoiled on it."
    print lb
    request_show_command()

    print lb
    print "Last but not least, let's crop a section of the image out:"
    print lb
    cmd = "cropped = img.crop(100, 100, 50, 50)"
    desired_tuple = ('cropped', Image)
    command_loop(cmd, desired_tuple)

    print lb
    print "Use cropped.show() to display it."
    print lb
    request_show_command()
    print lb
    print "That went from the coordinate in (X,Y), which is (0,0) and is the"
    print "top left corner of the picture, to coordinates (100,100) in the"
    print "(X,Y) and cropped a picture from that which is 50 pixels by 50 pixels."

    print lb
    print "Alright! This was tutorial 5/6."
    print "Next tutorial: Features"
    if not end_tutorial():
        tutorial_features()
    return



def tutorial_slicing():
    shellclear()
    print "Slicing Images"
    print lb
    print "Slicing is sort of a new paradigm to access parts of an image."
    print "Typically in vision a region of interest (ROI) is given.  "
    print "In this case, slicing is a very powerful way to access parts"
    print "of an image, or basically any matrix in SimpleCV in general."
    print lb
    print "This is done by using:"
    print "section = img[1:10,1:10]"
    print lb
    print "What is returned is an image object with that window."
    print "the slicing basically acts like a ROI but returns an image"
    print "so if you wanted to say run edge detection on a 20x20 box"
    print "in the picture that started at x=5,y=10 you use:"
    print "foundedges = img[5:25,10:30].edges()"
    print lb
    raw_input("[Press enter to continue]")
    shellclear()
    in_text = ""
    shouldbe = "ROI = img[1:6,1:6]"
    print "Please type this now:"
    print shouldbe
    print lb
    while (in_text != shouldbe):
        in_text = raw_input("SimpleCV:>")
        if(in_text != shouldbe):
            print "sorry, that is incorrect"
            print "please type:"
            print shouldbe
    shellclear()
    print "Correct, you just returned a 5 pixel by 5 pixel image object"
    print lb
    return

def tutorial_features():
    shellclear()
    print "Features"
    print lb
    print "Features are things you are looking for in the picture. They can be"
    print "blobs, corners, lines, etc. Features are sometimes referred to as a"
    print "fiducial in computer vision. These features are something that is"
    print "measurable, and something that makes images unique. Features are"
    print "something like when comparing things like fruit. In this case the"
    print "features could be the shape and the color, amongst others."
    print lb
    print "What features are in SimpleCV is an abstract representation of that."
    print "You take your image, then perform a function on it, and get back"
    print "features or another image with them applied. The crop example is"
    print "a case where an image is returned after we perform something."
    print lb
    print "In a simple example we will use the famous 'lenna' image, and find"
    print "corners in the picture."
    print lb
    tutorial_interpreter.runsource("img = Image('lenna')")
    print "img = Image('lenna') (already done for you)"
    print lb
    print "Try it yourself:"
    print lb

    cmd = "corners = img.findCorners()"
    desired_tuple = ('corners', FeatureSet)
    command_loop(cmd, desired_tuple)

    print lb
    print "Correct, you just got a featureset object which contains"
    print "feature objects.  These feature objects contain data from the"
    print "found corners"
    print lb

    print "Tip: If your are unsure what parameters to pass, you can always use"
    print "the built in help support by typing help(Image.findCorners). Keep in"
    print "mind that this help works for all of the functions available in"
    print "SimpleCV"
    print lb

    print "We can also do that with blobs. Try it:"
    print lb

    cmd = "blobs = img.findBlobs()"
    desired_tuple = ('blobs', FeatureSet)
    command_loop(cmd, desired_tuple)

    print lb
    print "Great, but..."
    print "When we show the image we won't notice anything different. This"
    print "is because we have to actually tell the blobs to draw themselves"
    print "on the image:"
    print lb
    print "blobs.draw()"
    print lb

    while True:
        if prompt_and_run().endswith('.draw()'):
            break
        print "No blobs have been drawn!"
        print lb

    print "Now use img.show() to see the changes!"
    print lb
    request_show_command()
    print lb
    raw_input("[Press enter to continue]")

    print lb
    print lb
    print "There's also a small trick built into SimpleCV to do this even faster"
    print lb
    tutorial_interpreter.runsource("img = Image('lenna')")
    print "img = Image('lenna') (already done for you)"
    print lb

    while True:
        print "img.findBlobs().show()"
        print lb
        if prompt_and_run().endswith('.show()'):
            break
        print "Nothing has been shown!"
        print lb

    print lb
    print "Alright! This was tutorial 6/6."
    #print "Next tutorial: ..."
    return

def magic_tutorial(self,arg):
    tutorials_dict = {'image': tutorial_image, 'save': tutorial_save,
                     'camera': tutorial_camera, 'manipulation': tutorial_manipulation,
                     'copy': tutorial_copy, 'features': tutorial_features}


    if (arg == ""):
        shellclear()
        print "+--------------------------------+"
        print " Welcome to the SimpleCV tutorial "
        print "+--------------------------------+"
        print lb
        print "At anytime on the SimpleCV Interactive Shell you can type tutorial,"
        print "then press the tab key and it will autocomplete any tutorial that"
        print "is currently available."
        print lb
        print "Let's start off with Loading and Saving images!"
        print lb
        print lb
        raw_input("[Press enter to continue]")
        tutorial_image()
        end_of_tutorial()
        return
    else:
        if arg in tutorials_dict:
            tutorials_dict[arg]()
        else:
            print "%s is not a tutorial!" % arg

########NEW FILE########
__FILENAME__ = Stream
from SimpleCV.base import *


_jpegstreamers = {}
class JpegStreamHandler(SimpleHTTPRequestHandler):
    """
    The JpegStreamHandler handles requests to the threaded HTTP server.
    Once initialized, any request to this port will receive a multipart/replace
    jpeg.
    """


    def do_GET(self):
        global _jpegstreamers


        if (self.path == "/" or not self.path):


            self.send_response(200)
            self.send_header('Content-type', 'text/html')
            self.end_headers()
            self.wfile.write("""
<html>
<head>
<style type=text/css>
    body { background-image: url(/stream); background-repeat:no-repeat; background-position:center top; background-attachment:fixed; height:100% }
</style>
</head>
<body>
&nbsp;
</body>
</html>
            """)
            return


        elif (self.path == "/stream"):
            self.send_response(200)
            self.send_header("Connection", "close")
            self.send_header("Max-Age", "0")
            self.send_header("Expires", "0")
            self.send_header("Cache-Control", "no-cache, private")
            self.send_header("Pragma", "no-cache")
            self.send_header("Content-Type", "multipart/x-mixed-replace; boundary=--BOUNDARYSTRING")
            self.end_headers()
            (host, port) = self.server.socket.getsockname()[:2]


            count = 0
            timeout = 0.75
            lasttimeserved = 0
            while (1):
                if (_jpegstreamers[port].refreshtime > lasttimeserved or time.time() - timeout > lasttimeserved):
                    try:
                        self.wfile.write("--BOUNDARYSTRING\r\n")
                        self.send_header("Content-type", "image/jpeg")
                        self.send_header("Content-Length", str(len(_jpegstreamers[port].jpgdata.getvalue())))
                        self.end_headers()
                        self.wfile.write(_jpegstreamers[port].jpgdata.getvalue() + "\r\n")
                        lasttimeserved = time.time()
                    except socket.error, e:
                        return
                    except IOError, e:
                        return
                    count = count + 1


                time.sleep(_jpegstreamers[port].sleeptime)




class JpegTCPServer(SocketServer.ThreadingMixIn, SocketServer.TCPServer):
    allow_reuse_address = True
    daemon_threads = True


#factory class for jpegtcpservers
class JpegStreamer():
    """
    The JpegStreamer class allows the user to stream a jpeg encoded file
    to a HTTP port.  Any updates to the jpg file will automatically be pushed
    to the browser via multipart/replace content type.


    To initialize:
    js = JpegStreamer()


    to update:
    img.save(js)


    to open a browser and display:
    import webbrowser
    webbrowser.open(js.url)


    Note 3 optional parameters on the constructor:
    - port (default 8080) which sets the TCP port you need to connect to
    - sleep time (default 0.1) how often to update.  Above 1 second seems to cause dropped connections in Google chrome


    Once initialized, the buffer and sleeptime can be modified and will function properly -- port will not.
    """
    server = ""
    host = ""
    port = ""
    sleeptime = ""
    framebuffer = ""
    counter = 0
    refreshtime = 0


    def __init__(self, hostandport = 8080, st=0.1 ):
        global _jpegstreamers
        if (type(hostandport) == int):
            self.port = hostandport
            self.host = "localhost"
        elif (isinstance(hostandport, basestring) and re.search(":", hostandport)):
            (self.host, self.port) = hostandport.split(":")
            self.port = int(self.port)
        elif (type(hostandport) == tuple):
            (self.host, self.port) = hostandport


        self.sleeptime = st
        self.server = JpegTCPServer((self.host, self.port), JpegStreamHandler)
        self.server_thread = threading.Thread(target = self.server.serve_forever)
        _jpegstreamers[self.port] = self
        self.server_thread.daemon = True
        self.server_thread.start()
        self.framebuffer = self #self referential, ugh.  but gives some bkcompat


    def url(self):
        """
        Returns the JpegStreams Webbrowser-appropriate URL, if not provided in the constructor, it defaults to "http://localhost:8080"
        """
        return "http://" + self.host + ":" + str(self.port) + "/"


    def streamUrl(self):
        """
        Returns the URL of the MJPEG stream. If host and port are not set in the constructor, defaults to "http://localhost:8080/stream/"
        """
        return self.url() + "stream"




class VideoStream():
    """
    The VideoStream lets you save video files in a number of different formats.


    You can initialize it by specifying the file you want to output::


        vs = VideoStream("hello.avi")


    You can also specify a framerate, and if you want to "fill" in missed frames.
    So if you want to record a realtime video you may want to do this::


        vs = VideoStream("myvideo.avi", 25, True) #note these are default values


    Where if you want to do a stop-motion animation, you would want to turn fill off::


        vs_animation = VideoStream("cartoon.avi", 15, False)


    If you select a fill, the VideoStream will do its best to stay close to "real time" by duplicating frames or dropping frames when the clock doesn't sync up
    with the file writes.


    You can save a frame to the video by using the Image.save() function::


        my_camera.getImage().save(vs)
    """


    fps = 25
    filename = ""
    writer = ""
    fourcc = ""
    framefill = True
    videotime = 0.0
    starttime = 0.0
    framecount = 0


    def __init__(self, filename, fps = 25, framefill = True):
        (revextension, revname) = filename[::-1].split(".")
        extension = revextension[::-1]
        self.filename = filename
        self.fps = fps
        self.framefill = framefill
        #if extension == "mpg":
        self.fourcc = cv.CV_FOURCC('I', 'Y', 'U', 'V')
            #self.fourcc = 0
        #else:
        #  logger.warning(extension + " is not supported for video writing on this platform, sorry");
        #  return False


    def initializeWriter(self, size):
        self.writer = cv.CreateVideoWriter(self.filename, self.fourcc, self.fps, size, 1)
        self.videotime = 0.0
        self.starttime = time.time()


    def writeFrame(self, img):
        """
        This writes a frame to the display object
        this is automatically called by image.save() but you can
        use this function to save just the bitmap as well so
        image markup is not implicit,typically you use image.save() but
        this allows for more finer control
        """
        if not self.writer:
            self.initializeWriter(img.size())
            self.lastframe = img


        frametime = 1.0 / float(self.fps)
        targettime = self.starttime + frametime * self.framecount
        realtime = time.time()
        if self.framefill:
            #see if we need to do anything to adjust to real time
            if (targettime > realtime + frametime):
                #if we're more than one frame ahead
                #save the lastframe, but don't write to videoout
                self.lastframe = img
                return


            elif (targettime < realtime - frametime):
                #we're at least one frame behind
                framesbehind = int((realtime - targettime) * self.fps) + 1
                #figure out how many frames behind we are


                lastframes = framesbehind / 2
                for i in range(0, lastframes):
                    self.framecount += 1
                    cv.WriteFrame(self.writer, self.lastframe.getBitmap())


                theseframes = framesbehind - lastframes
                for i in range(0, theseframes):
                    self.framecount += 1
                    cv.WriteFrame(self.writer, img.getBitmap())
                #split missing frames evenly between the prior and current frame
            else: #we are on track
                self.framecount += 1
                cv.WriteFrame(self.writer, img.getBitmap())
        else:
            cv.WriteFrame(self.writer, img.getBitmap())
            self.framecount += 1


        self.lastframe = img

########NEW FILE########
__FILENAME__ = test_c
from SimpleCV import *

color = Color()
img = Image('JeepGood.png')
img = img.invert()
img2 = Image('JeepGood.png')
img2 = img2.invert()
#img2 = img2.resize(img.width,img.height)

blobs = img.findBlobs()
blobs2 = img2.findBlobs()



confuse = []

# fs = blobs[0].getShapeContext()
# fs.draw()
# img.show()
# time.sleep(20)
i = 0
for b in blobs:
    for d in blobs2:
        metric =  b.getMatchMetric(d)
        result = b.showCorrespondence(d,'bottom')
        title = "Match Quality: " + str(metric)
        result.drawText(title,20,20,color=Color.RED,fontsize=42)
        result.show()
        fname = "SanityCheckExample"+str(i)+".png"
        i = i+ 1
        result.save(fname)
        print "------------------------------"
        print metric
        confuse.append(metric)

print confuse

confuse = np.array(confuse)


print confuse.reshape(4,4)

time.sleep(10)

########NEW FILE########
__FILENAME__ = test_multi
from SimpleCV import *
import string
import pickle
color = Color()

subset = 5
iset = ImageSet()
iset.load("./dataset/model/",sort_by="name")
iset = iset[0:subset]
testset = ImageSet()
testset.load("./dataset/test/",sort_by="name")
testset = testset[0:subset]
# fix featureset standardize to include fname
#iset = iset.standardize(400,400) # this should deal with some of the point issues
#testset = testset.standardize(400,400)
names = []
for i in iset:
    names.append(i.filename)

print names
names = names[0:subset]

scc = None
fname = 'classifier.pkl'
load_from_file = False
if( load_from_file ):
    scc = pickle.load( open( fname, "rb" ) )
else:
    scc = ShapeContextClassifier(iset,names) #this needs to be pickled.
    pickle.dump(scc, open( fname, "wb" ) )

print "--------------------------"
print "--------------------------"
print "Performing Analysis!"
print "--------------------------"
print "--------------------------"
classifications = []

i = 0
for test in testset:
    print "--------------------------"
    best, value, result = scc.classify(test)
    print "Total points in result " + str(len(scc.ptMap[best]))
    print "Testing: " + test.filename
    print "Best Result: " + best
    words = string.split(best,'/')
    words2 = string.split(test.filename,'/')
    test = test.resize(h=400)
    true_match = iset[i].resize(h=400)
    # test image, actual match, truth match
    test2 = scc.imgMap[best].resize(h=400)
    matchImg = test.sideBySide(test2)
    matchImg = matchImg.sideBySide(true_match)
    matchImg = matchImg.resize(w=1200)
    label = "Matched " + words2[-1] + " with " +  words[-1]
    matchImg.drawText(label,10,10,color=Color.RED,fontsize=30)
    label = "MatchVal: " + str(np.around(value,4))
    matchImg.drawText(label,10,45,color=Color.RED,fontsize=30)
    matchImg.show()
    fname = "match"+str(i)+".png"
    i = i + 1
    matchImg.save(fname)
    classifications.append((test.filename,result))
    print result
pickle.dump(classifications, open( "classifications.pkl", "wb" ) )

########NEW FILE########
__FILENAME__ = test_sc
from SimpleCV import *

color = Color()
img = Image('badge.png')
img = img.invert()
blobs = img.findBlobs()
img2 = Image('deformed.png')
img2 = img2.invert()
blobs2 = img2.findBlobs()

for j in range(0,len(blobs)):
    data = blobs[j].ShapeContextMatch(blobs2[j])
    mapvals = data[0]
    fs1 = blobs[j].getShapeContext()
    fs1.draw()
    fs2 = blobs2[j].getShapeContext()
    fs2.draw()

img2 = img2.applyLayers()
img = img.applyLayers()
img3 = img.sideBySide(img2,'bottom')

for j in range(0,3):
    data = blobs[j].ShapeContextMatch(blobs2[j])
    mapvals = data[0]
    for i in range(0,len(blobs[j]._completeContour)):
    #img3.clearLayers()
        lhs = blobs[j]._completeContour[i]
        idx = mapvals[i];
        rhs = blobs2[j]._completeContour[idx[0]]
        rhsShift = (rhs[0],rhs[1]+img.height)
        img3.drawLine(lhs,rhsShift,color=color.getRandom(),thickness=1)
        img3.show()

########NEW FILE########
__FILENAME__ = tests
# /usr/bin/python
# To run this test you need python nose tools installed
# Run test just use:
#   nosetest tests.py
#
# *Note: If you add additional test, please prefix the function name
# to the type of operation being performed.  For instance modifying an
# image, test_image_erode().  If you are looking for lines, then
# test_detection_lines().  This makes it easier to verify visually
# that all the correct test per operation exist

import os, sys, pickle, operator
from SimpleCV import *
from nose.tools import with_setup, nottest

VISUAL_TEST = False  # if TRUE we save the images - otherwise we DIFF against them - the default is False
SHOW_WARNING_TESTS = False  # show that warnings are working - tests will pass but warnings are generated.

#colors
black = Color.BLACK
white = Color.WHITE
red = Color.RED
green = Color.GREEN
blue = Color.BLUE

###############
# TODO -
# Examples of how to do profiling
# Examples of how to do a single test -
# UPDATE THE VISUAL TESTS WITH EXAMPLES.
# Fix exif data
# Turn off test warnings using decorators.
# Write a use the tests doc.

#images
barcode = "../sampleimages/barcode.png"
testimage = "../sampleimages/9dots4lines.png"
testimage2 = "../sampleimages/aerospace.jpg"
whiteimage = "../sampleimages/white.png"
blackimage = "../sampleimages/black.png"
testimageclr = "../sampleimages/statue_liberty.jpg"
testbarcode = "../sampleimages/barcode.png"
testoutput = "../sampleimages/9d4l.jpg"
tmpimg = "../sampleimages/tmpimg.jpg"
greyscaleimage = "../sampleimages/greyscale.jpg"
logo = "../sampleimages/simplecv.png"
logo_inverted = "../sampleimages/simplecv_inverted.png"
ocrimage = "../sampleimages/ocr-test.png"
circles = "../sampleimages/circles.png"
webp = "../sampleimages/simplecv.webp"

#alpha masking images
topImg = "../sampleimages/RatTop.png"
bottomImg = "../sampleimages/RatBottom.png"
maskImg = "../sampleimages/RatMask.png"
alphaMaskImg = "../sampleimages/RatAlphaMask.png"
alphaSrcImg = "../sampleimages/GreenMaskSource.png"

#standards path
standard_path = "./standard/"


#Given a set of images, a path, and a tolerance do the image diff.
def imgDiffs(test_imgs,name_stem,tolerance,path):
    count = len(test_imgs)
    for idx in range(0,count):
        lhs = test_imgs[idx].applyLayers() # this catches drawing methods
        fname = standard_path+name_stem+str(idx)+".jpg"
        rhs = Image(fname)
        if( lhs.width == rhs.width and lhs.height == rhs.height ):
            diff = (lhs-rhs)
            val = np.average(diff.getNumpy())
            if( val > tolerance ):
                print val
                return True
    return False

#Save a list of images to a standard path.
def imgSaves(test_imgs, name_stem, path=standard_path):
    count = len(test_imgs)
    for idx in range(0,count):
        fname = standard_path+name_stem+str(idx)+".jpg"
        test_imgs[idx].save(fname)#,quality=95)

#perform the actual image save and image diffs.
def perform_diff(result,name_stem,tolerance=3.0,path=standard_path):
    if(VISUAL_TEST): # save the correct images for a visual test
        imgSaves(result,name_stem,path)
    else: # otherwise we test our output against the visual test
        if( imgDiffs(result,name_stem,tolerance,path) ):
            assert False
        else:
            pass

def test_image_stretch():
    img = Image(greyscaleimage)
    stretched = img.stretch(100,200)
    if(stretched == None):
        assert False

    result = [stretched]
    name_stem = "test_stretch"
    perform_diff(result,name_stem)


#These function names are required by nose test, please leave them as is
def setup_context():
    img = Image(testimage)

def destroy_context():
    img = ""

@with_setup(setup_context, destroy_context)
def test_image_loadsave():
    img = Image(testimage)
    img.save(testoutput)
    if (os.path.isfile(testoutput)):
        os.remove(testoutput)
        pass
    else:
        assert False

def test_image_numpy_constructor():
    img = Image(testimage)
    grayimg = img.grayscale()

    chan3_array = np.array(img.getMatrix())
    chan1_array = np.array(img.getGrayscaleMatrix())

    img2 = Image(chan3_array)
    grayimg2 = Image(chan1_array)

    if (img2[0,0] == img[0,0] and grayimg2[0,0] == grayimg[0,0]):
        pass
    else:
        assert False

def test_image_bitmap():
    img1 = Image("lenna")
    img2 = Image("lenna")
    img2 = img2.smooth()
    result = [img1,img2]
    name_stem = "test_image_bitmap"
    perform_diff(result,name_stem)

# # Image Class Test

def test_image_scale():
    img = Image(testimage)
    thumb = img.scale(30,30)
    if(thumb == None):
        assert False
    result = [thumb]
    name_stem = "test_image_scale"
    perform_diff(result,name_stem)

def test_image_copy():
    img = Image(testimage2)
    copy = img.copy()

    if (img[1,1] != copy[1,1] or img.size() != copy.size()):
        assert False

    result = [copy]
    name_stem = "test_image_copy"
    perform_diff(result,name_stem)

    pass


def test_image_getitem():
    img = Image(testimage)
    colors = img[1,1]
    if (colors[0] == 255 and colors[1] == 255 and colors[2] == 255):
        pass
    else:
        assert False

def test_image_getslice():
    img = Image(testimage)
    section = img[1:10,1:10]
    if(section == None):
        assert False


def test_image_setitem():
    img = Image(testimage)
    img[1,1] = (0, 0, 0)
    newimg = Image(img.getBitmap())
    colors = newimg[1,1]
    if (colors[0] == 0 and colors[1] == 0 and colors[2] == 0):
        pass
    else:
        assert False

    result = [newimg]
    name_stem = "test_image_setitem"
    perform_diff(result,name_stem)


def test_image_setslice():
    img = Image(testimage)
    img[1:10,1:10] = (0,0,0) #make a black box
    newimg = Image(img.getBitmap())
    section = newimg[1:10,1:10]
    for i in range(5):
        colors = section[i,0]
        if (colors[0] != 0 or colors[1] != 0 or colors[2] != 0):
            assert False
    pass
    result = [newimg]
    name_stem = "test_image_setslice"
    perform_diff(result,name_stem)


def test_detection_findCorners():
    img = Image(testimage2)
    corners = img.findCorners(25)
    corners.draw()
    if (len(corners) == 0):
        assert False
    result = [img]
    name_stem = "test_detection_findCorners"
    perform_diff(result,name_stem)


def test_color_meancolor():
    a = np.arange(0, 256)
    b = a[::-1]
    c = np.copy(a)/2
    a = a.reshape(16, 16)
    b = b.reshape(16, 16)
    c = c.reshape(16, 16)
    imgarr = np.dstack((a, b, c))
    img = Image(imgarr)

    b, g, r = img.meanColor('BGR')
    if not (127 < r < 128 and 127 < g < 128 and 63 < b < 64):
        assert False

    r, g, b = img.meanColor('RGB')
    if not (127 < r < 128 and 127 < g < 128 and 63 < b < 64):
        assert False

    h, s, v = img.meanColor('HSV')
    if not (83 < h < 84 and 191 < s < 192 and 191 < v < 192):
        assert False

    x, y, z = img.meanColor('XYZ')
    if not (109 < x < 110 and 122 < y < 123 and 77 < z < 79):
        assert False

    gray = img.meanColor('Gray')
    if not (120 < gray < 121):
        assert False

    y, cr, cb = img.meanColor('YCrCb')
    if not (120 < y < 121 and 133 < cr < 134 and 96 < cb < 97):
        assert False

    h, l, s = img.meanColor('HLS')
    if not (84 < h < 85 and 117 < l < 118 and 160 < s < 161):
        assert False
    pass

def test_image_smooth():
    img = Image(testimage2)
    result = []
    result.append(img.smooth())
    result.append(img.smooth('bilateral', (3,3), 4, 1))
    result.append(img.smooth('blur', (3, 3)))
    result.append(img.smooth('median', (3, 3)))
    result.append(img.smooth('gaussian', (5,5), 0))
    result.append(img.smooth('bilateral', (3,3), 4, 1,grayscale=False))
    result.append(img.smooth('blur', (3, 3),grayscale=True))
    result.append(img.smooth('median', (3, 3),grayscale=True))
    result.append(img.smooth('gaussian', (5,5), 0,grayscale=True))
    name_stem = "test_image_smooth"
    perform_diff(result,name_stem)
    pass

def test_image_gammaCorrect():
    img = Image(topImg)
    img2 = img.gammaCorrect(1)
    img3 = img.gammaCorrect(0.5)
    img4 = img.gammaCorrect(2)
    result = []
    result.append(img3)
    result.append(img4)
    name_stem = "test_image_gammaCorrect"
    perform_diff(result, name_stem)
    if ((img3.meanColor() >= img2.meanColor()) and (img4.meanColor() <= img2.meanColor())):
        pass
    else:
        assert False

def test_image_binarize():
    img =  Image(testimage2)
    binary = img.binarize()
    binary2 = img.binarize((60, 100, 200))
    hist = binary.histogram(20)
    hist2 = binary2.histogram(20)

    result = [binary,binary2]
    name_stem = "test_image_binarize"
    perform_diff(result,name_stem)

    if (hist[0] + hist[-1] == np.sum(hist) and hist2[0] + hist2[-1] == np.sum(hist2)):
        pass
    else:
        assert False

def test_image_binarize_adaptive():
    img =  Image(testimage2)
    binary = img.binarize(-1)
    hist = binary.histogram(20)

    result = [binary]
    name_stem = "test_image_binarize_adaptive"
    perform_diff(result,name_stem)

    if (hist[0] + hist[-1] == np.sum(hist)):
        pass
    else:
        assert False

def test_image_invert():
    img = Image(testimage2)
    clr = img[1,1]
    img = img.invert()

    result = [img]
    name_stem = "test_image_invert"
    perform_diff(result,name_stem)

    if (clr[0] == (255 - img[1,1][0])):
        pass
    else:
        assert False


def test_image_size():
    img = Image(testimage2)
    (width, height) = img.size()
    if type(width) == int and type(height) == int and width > 0 and height > 0:
        pass
    else:
        assert False

def test_image_drawing():
    img = Image(testimageclr)
    img.drawCircle((img.width/2, img.height/2), 10,thickness=3)
    img.drawCircle((img.width/2, img.height/2), 15,thickness=5,color=Color.RED)
    img.drawCircle((img.width/2, img.height/2), 20)
    img.drawLine((5, 5), (5, 8))
    img.drawLine((5, 5), (10, 10),thickness=3)
    img.drawLine((0, 0), (img.width, img.height),thickness=3,color=Color.BLUE)
    img.drawRectangle(20,20,10,5)
    img.drawRectangle(22,22,10,5,alpha=128)
    img.drawRectangle(24,24,10,15,width=-1,alpha=128)
    img.drawRectangle(28,28,10,15,width=3,alpha=128)
    result = [img]
    name_stem = "test_image_drawing"
    perform_diff(result,name_stem)

def test_image_draw():
    img = Image("lenna")
    newimg = Image("simplecv")
    lines = img.findLines()
    newimg.draw(lines)
    lines.draw()
    result = [newimg, img]
    name_stem = "test_image_draw"
    perform_diff(result, name_stem, 5)


def test_image_splitchannels():
    img = Image(testimageclr)
    (r, g, b) = img.splitChannels(True)
    (red, green, blue) = img.splitChannels()
    result = [r,g,b,red,green,blue]
    name_stem = "test_image_splitchannels"
    perform_diff(result,name_stem)
    pass

def test_image_histogram():
    img = Image(testimage2)
    h = img.histogram(25)

    for i in h:
        if type(i) != int:
            assert False

    pass

def test_detection_lines():
    img = Image(testimage2)
    lines = img.findLines()
    lines.draw()
    result = [img]
    name_stem = "test_detection_lines"
    perform_diff(result,name_stem)

    if(lines == 0 or lines == None):
        assert False

def test_detection_lines_standard():
    img = Image(testimage2)
    lines = img.findLines(useStandard=True)
    lines.draw()
    result = [img]
    name_stem = "test_detection_lines_standard"
    perform_diff(result,name_stem)

    if(lines == 0 or lines == None):
        assert False

def test_detection_feature_measures():
    img = Image(testimage2)

    fs = FeatureSet()
    fs.append(Corner(img, 5, 5))
    fs.append(Line(img, ((2, 2), (3,3))))
    bm = BlobMaker()
    result = bm.extract(img)
    fs.extend(result)

    for f in fs:
        a = f.area()
        l = f.length()
        c = f.meanColor()
        d = f.colorDistance()
        th = f.angle()
        pts = f.coordinates()
        dist = f.distanceFrom() #distance from center of image

    fs2 = fs.sortAngle()
    fs3 = fs.sortLength()
    fs4 = fs.sortColorDistance()
    fs5 = fs.sortArea()
    fs1 = fs.sortDistance()
    pass

def test_detection_blobs_appx():
    img = Image("lenna")
    blobs = img.findBlobs()
    blobs[-1].draw(color=Color.RED)
    blobs[-1].drawAppx(color=Color.BLUE)
    result = [img]

    img2 = Image("lenna")
    blobs = img2.findBlobs(appx_level=11)
    blobs[-1].draw(color=Color.RED)
    blobs[-1].drawAppx(color=Color.BLUE)
    result.append(img2)

    name_stem = "test_detection_blobs_appx"
    perform_diff(result,name_stem,5.00)
    if blobs == None:
        assert False

def test_detection_blobs():
    img = Image(testbarcode)
    blobs = img.findBlobs()
    blobs.draw(color=Color.RED)
    result = [img]
    #TODO - WE NEED BETTER COVERAGE HERE
    name_stem = "test_detection_blobs"
    perform_diff(result,name_stem,5.00)

    if blobs == None:
        assert False

def test_detection_blobs_lazy():

    img = Image("lenna")
    b = img.findBlobs()
    result = []

    s = pickle.dumps(b[-1]) # use two otherwise it w
    b2 =  pickle.loads(s)

    result.append(b[-1].mImg)
    result.append(b[-1].mMask)
    result.append(b[-1].mHullImg)
    result.append(b[-1].mHullMask)

    result.append(b2.mImg)
    result.append(b2.mMask)
    result.append(b2.mHullImg)
    result.append(b2.mHullMask)

    #TODO - WE NEED BETTER COVERAGE HERE
    name_stem = "test_detection_blobs_lazy"
    perform_diff(result,name_stem,6.00)


def test_detection_blobs_adaptive():
    img = Image(testimage)
    blobs = img.findBlobs(-1, threshblocksize=99)
    blobs.draw(color=Color.RED)
    result = [img]
    name_stem = "test_detection_blobs_adaptive"
    perform_diff(result,name_stem,5.00)

    if blobs == None:
        assert False

def test_detection_blobs_smallimages():
    # Check if segfault occurs or not
    img = Image("../sampleimages/blobsegfaultimage.png")
    blobs = img.findBlobs()
    # if no segfault, pass

def test_detection_blobs_convexity_defects():
    img = Image('lenna')
    blobs = img.findBlobs()
    b = blobs[-1]
    feat = b.getConvexityDefects()
    points = b.getConvexityDefects(returnPoints=True)
    if len(feat) <= 0 or len(points) <= 0:
        assert False
    pass

def test_detection_barcode():
    try:
        import zbar
    except:
        return None

    img1 = Image(testimage)
    img2 = Image(testbarcode)

    if( SHOW_WARNING_TESTS ):
        nocode = img1.findBarcode()
        if nocode: #we should find no barcode in our test image
            assert False
        code = img2.findBarcode()
        code.draw()
        if code.points:
            pass
        result = [img1,img2]
        name_stem = "test_detection_barcode"
        perform_diff(result,name_stem)


    else:
        pass

def test_detection_x():
    tmpX = Image(testimage).findLines().x()[0]

    if (tmpX > 0 and Image(testimage).size()[0]):
        pass
    else:
        assert False

def test_detection_y():
    tmpY = Image(testimage).findLines().y()[0]

    if (tmpY > 0 and Image(testimage).size()[0]):
        pass
    else:
        assert False

def test_detection_area():
    img = Image(testimage2)
    bm = BlobMaker()
    result = bm.extract(img)
    area_val = result[0].area()

    if(area_val > 0):
        pass
    else:
        assert False

def test_detection_angle():
    angle_val = Image(testimage).findLines().angle()[0]

def test_image():
    img = Image(testimage)
    if(isinstance(img, Image)):
        pass
    else:
        assert False

def test_color_colordistance():
    img = Image(blackimage)
    (r,g,b) = img.splitChannels()
    avg = img.meanColor()

    c1 = Corner(img, 1, 1)
    c2 = Corner(img, 1, 2)
    if (c1.colorDistance(c2.meanColor()) != 0):
        assert False

    if (c1.colorDistance((0,0,0)) != 0):
        assert False

    if (c1.colorDistance((0,0,255)) != 255):
        assert False

    if (c1.colorDistance((255,255,255)) != sqrt(255**2 * 3)):
        assert False

    pass

def test_detection_length():
    img = Image(testimage)
    val = img.findLines().length()

    if (val == None):
        assert False
    if (not isinstance(val, np.ndarray)):
        assert False
    if (len(val) < 0):
        assert False

    pass

def test_detection_sortangle():
    img = Image(testimage)
    val = img.findLines().sortAngle()

    if(val[0].x < val[1].x):
        pass
    else:
        assert False

def test_detection_sortarea():
    img = Image(testimage)
    bm = BlobMaker()
    result = bm.extract(img)
    val = result.sortArea()
    #FIXME: Find blobs may appear to be broken. Returning type none

def test_detection_sortLength():
    img = Image(testimage)
    val = img.findLines().sortLength()
    #FIXME: Length is being returned as euclidean type, believe we need a universal type, either Int or scvINT or something.

#def test_distanceFrom():
#def test_sortColorDistance():
#def test_sortDistance():

def test_image_add():
    imgA = Image(blackimage)
    imgB = Image(whiteimage)

    imgC = imgA + imgB

def test_color_curve_HSL():
    y = np.array([[0,0],[64,128],[192,128],[255,255]])  #These are the weights
    curve = ColorCurve(y)
    img = Image(testimage)
    img2 = img.applyHLSCurve(curve,curve,curve)
    img3 = img-img2

    result = [img2,img3]
    name_stem = "test_color_curve_HLS"
    perform_diff(result,name_stem)

    c = img3.meanColor()
    if( c[0] > 2.0 or c[1] > 2.0 or c[2] > 2.0 ): #there may be a bit of roundoff error
        assert False

def test_color_curve_RGB():
    y = np.array([[0,0],[64,128],[192,128],[255,255]])  #These are the weights
    curve = ColorCurve(y)
    img = Image(testimage)
    img2 = img.applyRGBCurve(curve,curve,curve)
    img3 = img-img2

    result = [img2,img3]
    name_stem = "test_color_curve_RGB"
    perform_diff(result,name_stem)

    c = img3.meanColor()
    if( c[0] > 1.0 or c[1] > 1.0 or c[2] > 1.0 ): #there may be a bit of roundoff error
        assert False

def test_color_curve_GRAY():
    y = np.array([[0,0],[64,128],[192,128],[255,255]])  #These are the weights
    curve = ColorCurve(y)
    img = Image(testimage)
    gray = img.grayscale()
    img2 = img.applyIntensityCurve(curve)

    result = [img2]
    name_stem = "test_color_curve_GRAY"
    perform_diff(result,name_stem)

    g=gray.meanColor()
    i2=img2.meanColor()
    if( g[0]-i2[0] > 1 ): #there may be a bit of roundoff error
        assert False

def test_image_dilate():
    img=Image(barcode)
    img2 = img.dilate(20)

    result = [img2]
    name_stem = "test_image_dilate"
    perform_diff(result,name_stem)
    c=img2.meanColor()

    if( c[0] < 254 or c[1] < 254 or c[2] < 254 ):
        assert False;

def test_image_erode():
    img=Image(barcode)
    img2 = img.erode(100)

    result = [img2]
    name_stem = "test_image_erode"
    perform_diff(result,name_stem)

    c=img2.meanColor()
    print(c)
    if( c[0] > 0 or c[1] > 0 or c[2] > 0 ):
        assert False;

def test_image_morph_open():
    img = Image(barcode);
    erode= img.erode()
    dilate = erode.dilate()
    result = img.morphOpen()
    test = result-dilate
    c=test.meanColor()
    results = [result]
    name_stem = "test_image_morph_open"
    perform_diff(results,name_stem)

    if( c[0] > 1 or c[1] > 1 or c[2] > 1 ):
        assert False;

def test_image_morph_close():
    img = Image(barcode)
    dilate = img.dilate()
    erode = dilate.erode()
    result = img.morphClose()
    test = result-erode
    c=test.meanColor()

    results = [result]
    name_stem = "test_image_morph_close"
    perform_diff(results,name_stem)


    if( c[0] > 1 or c[1] > 1 or c[2] > 1 ):
        assert False;

def test_image_morph_grad():
    img = Image(barcode)
    dilate = img.dilate()
    erode = img.erode()
    dif = dilate-erode
    result = img.morphGradient()
    test = result-dif
    c=test.meanColor()

    results = [result]
    name_stem = "test_image_morph_grad"
    perform_diff(results,name_stem)


    if( c[0] > 1 or c[1] > 1 or c[2] > 1 ):
        assert False

def test_image_rotate_fixed():
    img = Image(testimage2)
    img2=img.rotate(180, scale = 1)
    img3=img.flipVertical()
    img4=img3.flipHorizontal()
    img5 = img.rotate(70)
    img6 = img.rotate(70,scale=0.5)

    results = [img2,img3,img4,img5,img6]
    name_stem = "test_image_rotate_fixed"
    perform_diff(results,name_stem)

    test = img4-img2
    c=test.meanColor()
    print(c)
    if( c[0] > 5 or c[1] > 5 or c[2] > 5 ):
        assert False


def test_image_rotate_full():
    img = Image(testimage2)
    img2=img.rotate(180,"full",scale = 1)

    results = [img2]
    name_stem = "test_image_rotate_full"
    perform_diff(results,name_stem)

    c1=img.meanColor()
    c2=img2.meanColor()
    if( abs(c1[0]-c2[0]) > 5 or abs(c1[1]-c2[1]) > 5 or abs(c1[2]-c2[2]) > 5 ):
        assert False

def test_image_shear_warp():
    img = Image(testimage2)
    dst =  ((img.width/2,0),(img.width-1,img.height/2),(img.width/2,img.height-1))
    s = img.shear(dst)


    color = s[0,0]
    if (color != (0,0,0)):
        assert False

    dst = ((img.width*0.05,img.height*0.03),(img.width*0.9,img.height*0.1),(img.width*0.8,img.height*0.7),(img.width*0.2,img.height*0.9))
    w = img.warp(dst)

    results = [s,w]
    name_stem = "test_image_shear_warp"
    perform_diff(results,name_stem)

    color = s[0,0]
    if (color != (0,0,0)):
        assert False

    pass

def test_image_affine():
    img = Image(testimage2)
    src =  ((0,0),(img.width-1,0),(img.width-1,img.height-1))
    dst =  ((img.width/2,0),(img.width-1,img.height/2),(img.width/2,img.height-1))
    aWarp = cv.CreateMat(2,3,cv.CV_32FC1)
    cv.GetAffineTransform(src,dst,aWarp)
    atrans = img.transformAffine(aWarp)

    aWarp2 = np.array(aWarp)
    atrans2 = img.transformAffine(aWarp2)

    test = atrans-atrans2
    c=test.meanColor()

    results = [atrans,atrans2]

    name_stem = "test_image_affine"
    perform_diff(results,name_stem)

    if( c[0] > 1 or c[1] > 1 or c[2] > 1 ):
        assert False

def test_image_perspective():
    img = Image(testimage2)
    src = ((0,0),(img.width-1,0),(img.width-1,img.height-1),(0,img.height-1))
    dst = ((img.width*0.05,img.height*0.03),(img.width*0.9,img.height*0.1),(img.width*0.8,img.height*0.7),(img.width*0.2,img.height*0.9))
    pWarp = cv.CreateMat(3,3,cv.CV_32FC1)
    cv.GetPerspectiveTransform(src,dst,pWarp)
    ptrans = img.transformPerspective(pWarp)
    pWarp2 = np.array(pWarp)
    ptrans2 = img.transformPerspective(pWarp2)
    test = ptrans-ptrans2
    np_test = test.getNumpy()
    mc=test.meanColor()
    results = [ptrans,ptrans2]
    name_stem = "test_image_perspective"
    # Threshold kept high, otherwise test will fail
    # difference between original image warped image will be always huge
    perform_diff(results,name_stem, 100)

    if( mc[0] > 100 or mc[1] > 100 or mc[2] > 100 ):
        assert False

def test_image_horz_scanline():
    img = Image(logo)
    sl = img.getHorzScanline(10)
    if( sl.shape[0]!=img.width or sl.shape[1]!=3 ):
        assert False

def test_image_vert_scanline():
    img = Image(logo)
    sl = img.getVertScanline(10)
    if( sl.shape[0]!=img.height or sl.shape[1]!=3 ):
        assert False

def test_image_horz_scanline_gray():
    img = Image(logo)
    sl = img.getHorzScanlineGray(10)
    if( sl.shape[0]!=img.width or sl.shape[1]!=1 ):
        assert False

def test_image_vert_scanline_gray():
    img = Image(logo)
    sl = img.getVertScanlineGray(10)
    if( sl.shape[0]!=img.height or sl.shape[1]!=1 ):
        assert False

def test_image_get_pixel():
    img = Image(logo)
    px = img.getPixel(0,0)
    print(px)
    if(px[0] != 0 or px[1] != 0 or px[2] != 0 ):
        assert False

def test_image_get_gray_pixel():
    img = Image(logo)
    px = img.getGrayPixel(0,0)
    if(px != 0):
        assert False


def test_camera_calibration():
    fakeCamera = FrameSource()
    path = "../sampleimages/CalibImage"
    ext = ".png"
    imgs = []
    for i in range(0,10):
        fname = path+str(i)+ext
        img = Image(fname)
        imgs.append(img)

    fakeCamera.calibrate(imgs)
    #we're just going to check that the function doesn't puke
    mat = fakeCamera.getCameraMatrix()
    if( type(mat) != cv.cvmat ):
        assert False
    #we're also going to test load in save in the same pass
    matname = "TestCalibration"
    if( False == fakeCamera.saveCalibration(matname)):
        assert False
    if( False == fakeCamera.loadCalibration(matname)):
        assert False

def test_camera_undistort():
    fakeCamera = FrameSource()
    fakeCamera.loadCalibration("./StereoVision/Default")
    img = Image("../sampleimages/CalibImage0.png")
    img2 = fakeCamera.undistort(img)

    results = [img2]
    name_stem = "test_camera_undistort"
    perform_diff(results,name_stem,tolerance=12)

    if (not img2): #right now just wait for this to return
        assert False

def test_image_crop():
    img = Image(logo)
    x = 5
    y = 6
    w = 10
    h = 20
    crop = img.crop(x,y,w,h)
    crop2 = img[x:(x+w),y:(y+h)]
    crop6 = img.crop(0,0,10,10)
    # if( SHOW_WARNING_TESTS ):
    #     crop7 = img.crop(0,0,-10,10)
    #     crop8 = img.crop(-50,-50,10,10)
    #     crop3 = img.crop(-3,-3,10,20)
    #     crop4 = img.crop(-10,10,20,20,centered=True)
    #     crop5 = img.crop(-10,-10,20,20)

    tests = []
    tests.append(img.crop((50,50),(10,10))) # 0
    tests.append(img.crop([10,10,40,40])) # 1
    tests.append(img.crop((10,10,40,40))) # 2
    tests.append(img.crop([50,50],[10,10])) # 3
    tests.append(img.crop([10,10],[50,50])) # 4

    roi = np.array([10,10,40,40])
    pts1 = np.array([[50,50],[10,10]])
    pts2 = np.array([[10,10],[50,50]])
    pt1 = np.array([10,10])
    pt2 = np.array([50,50])

    tests.append(img.crop(roi)) # 5
    tests.append(img.crop(pts1)) # 6
    tests.append(img.crop(pts2)) # 7
    tests.append(img.crop(pt1,pt2)) # 8
    tests.append(img.crop(pt2,pt1)) # 9

    xs = [10,10,10,20,20,20,30,30,40,40,40,50,50,50]
    ys = [10,20,50,20,30,40,30,10,40,50,10,50,10,42]
    lots = zip(xs,ys)

    tests.append(img.crop(xs,ys)) # 10
    tests.append(img.crop(lots)) # 11
    tests.append(img.crop(np.array(xs),np.array(ys))) # 12
    tests.append(img.crop(np.array(lots))) # 14

    i = 0
    failed = False
    for img in tests:
        if( img is None or img.width != 40 and img.height != 40 ):
            print "FAILED CROP TEST " + str(i) + " " + str(img)
            failed = True
        i = i + 1

    if(failed):
        assert False
    results = [crop,crop2,crop6]
    name_stem = "test_image_crop"
    perform_diff(results,name_stem)

    diff = crop-crop2;
    c=diff.meanColor()
    if( c[0] > 0 or c[1] > 0 or c[2] > 0 ):
        assert False

def test_image_region_select():
    img = Image(logo)
    x1 = 0
    y1 = 0
    x2 = img.width
    y2 = img.height
    crop = img.regionSelect(x1,y1,x2,y2)

    results = [crop]
    name_stem = "test_image_region_select"
    perform_diff(results,name_stem)

    diff = crop-img;
    c=diff.meanColor()
    if( c[0] > 0 or c[1] > 0 or c[2] > 0 ):
        assert False

def test_image_subtract():
    imgA = Image(logo)
    imgB = Image(logo_inverted)
    imgC = imgA - imgB
    results = [imgC]
    name_stem = "test_image_subtract"
    perform_diff(results,name_stem)

def test_image_negative():
    imgA = Image(logo)
    imgB = -imgA
    results = [imgB]
    name_stem = "test_image_negative"
    perform_diff(results,name_stem)


def test_image_divide():
    imgA = Image(logo)
    imgB = Image(logo_inverted)

    imgC = imgA / imgB

    results = [imgC]
    name_stem = "test_image_divide"
    perform_diff(results,name_stem)

def test_image_and():
    imgA = Image(barcode)
    imgB = imgA.invert()


    imgC = imgA & imgB # should be all black

    results = [imgC]
    name_stem = "test_image_and"
    perform_diff(results,name_stem)


def test_image_or():
    imgA = Image(barcode)
    imgB = imgA.invert()

    imgC = imgA | imgB #should be all white

    results = [imgC]
    name_stem = "test_image_or"
    perform_diff(results,name_stem)


def test_image_edgemap():
    imgA = Image(logo)
    imgB = imgA._getEdgeMap()
    #results = [imgB]
    #name_stem = "test_image_edgemap"
    #perform_diff(results,name_stem)


def test_color_colormap_build():
    cm = ColorModel()
    #cm.add(Image(logo))
    cm.add((127,127,127))
    if(cm.contains((127,127,127))):
        cm.remove((127,127,127))
    else:
        assert False

    cm.remove((0,0,0))
    cm.remove((255,255,255))
    cm.add((0,0,0))
    cm.add([(0,0,0),(255,255,255)])
    cm.add([(255,0,0),(0,255,0)])
    img = cm.threshold(Image(testimage))
    c=img.meanColor()

    #if( c[0] > 1 or c[1] > 1 or c[2] > 1 ):
    #  assert False

    cm.save("temp.txt")
    cm2 = ColorModel()
    cm2.load("temp.txt")
    img = Image("logo")
    img2 = cm2.threshold(img)
    cm2.add((0,0,255))
    img3 = cm2.threshold(img)
    cm2.add((255,255,0))
    cm2.add((0,255,255))
    cm2.add((255,0,255))
    img4 = cm2.threshold(img)
    cm2.add(img)
    img5 = cm2.threshold(img)

    results = [img,img2,img3,img4,img5]
    name_stem = "test_color_colormap_build"
    perform_diff(results,name_stem)

    #c=img.meanColor()
    #if( c[0] > 1 or c[1] > 1 or c[2] > 1 ):
    #  assert False


def test_feature_height():
    imgA = Image(logo)
    lines = imgA.findLines(1)
    heights = lines.height()

    if(len(heights) <= 0 ):
        assert False
    else:
        pass

def test_feature_width():
    imgA = Image(logo)
    lines = imgA.findLines(1)
    widths = lines.width()

    if(len(widths) <= 0):
        assert False
    else:
        pass

def test_feature_crop():
    imgA = Image(logo)

    lines = imgA.findLines()

    croppedImages = lines.crop()

    if(len(croppedImages) <= 0):
        assert False
    else:
        pass


def test_color_conversion_func_BGR():
    #we'll just go through the space to make sure nothing blows up
    img = Image(testimage)
    results = []
    results.append(img.toBGR())
    results.append(img.toRGB())
    results.append(img.toHLS())
    results.append(img.toHSV())
    results.append(img.toXYZ())

    bgr = img.toBGR()

    results.append(bgr.toBGR())
    results.append(bgr.toRGB())
    results.append(bgr.toHLS())
    results.append(bgr.toHSV())
    results.append(bgr.toXYZ())

    name_stem = "test_color_conversion_func_BGR"
    perform_diff(results,name_stem,tolerance=4.0)


def test_color_conversion_func_RGB():
    img = Image(testimage)
    if( not img.isBGR() ):
        assert False
    rgb = img.toRGB()

    foo = rgb.toBGR()
    if( not foo.isBGR() ):
        assert False

    foo = rgb.toRGB()
    if( not foo.isRGB() ):
        assert False

    foo = rgb.toHLS()
    if( not foo.isHLS() ):
        assert False

    foo = rgb.toHSV()
    if( not foo.isHSV() ):
        assert False

    foo = rgb.toXYZ()
    if( not foo.isXYZ() ):
        assert False

def test_color_conversion_func_HSV():
    img = Image(testimage)
    hsv = img.toHSV()
    results = [hsv]
    results.append(hsv.toBGR())
    results.append(hsv.toRGB())
    results.append(hsv.toHLS())
    results.append(hsv.toHSV())
    results.append(hsv.toXYZ())
    name_stem = "test_color_conversion_func_HSV"
    perform_diff(results,name_stem,tolerance=4.0 )


def test_color_conversion_func_HLS():
    img = Image(testimage)

    hls = img.toHLS()
    results = [hls]

    results.append(hls.toBGR())
    results.append(hls.toRGB())
    results.append(hls.toHLS())
    results.append(hls.toHSV())
    results.append(hls.toXYZ())

    name_stem = "test_color_conversion_func_HLS"
    perform_diff(results,name_stem,tolerance=4.0)


def test_color_conversion_func_XYZ():
    img = Image(testimage)

    xyz = img.toXYZ()
    results = [xyz]
    results.append(xyz.toBGR())
    results.append(xyz.toRGB())
    results.append(xyz.toHLS())
    results.append(xyz.toHSV())
    results.append(xyz.toXYZ())

    name_stem = "test_color_conversion_func_XYZ"
    perform_diff(results,name_stem,tolerance=8.0)


def test_blob_maker():
    img = Image("../sampleimages/blockhead.png")
    blobber = BlobMaker()
    results = blobber.extract(img)
    print(len(results))
    if( len(results) != 7 ):
        assert False

def test_blob_holes():
    img = Image("../sampleimages/blockhead.png")
    blobber = BlobMaker()
    blobs = blobber.extract(img)
    count = 0
    blobs.draw()
    results = [img]
    name_stem = "test_blob_holes"
    perform_diff(results,name_stem,tolerance=3.0)


    for b in blobs:
        if( b.mHoleContour is not None ):
            count = count + len(b.mHoleContour)
    if( count != 7 ):
        assert False

def test_blob_hull():
    img = Image("../sampleimages/blockhead.png")
    blobber = BlobMaker()
    blobs = blobber.extract(img)
    blobs.draw()

    results = [img]
    name_stem = "test_blob_holes"
    perform_diff(results,name_stem,tolerance=3.0)

    for b in blobs:
        if( len(b.mConvexHull) < 3 ):
            assert False

def test_blob_data():
    img = Image("../sampleimages/blockhead.png")
    blobber = BlobMaker()
    blobs = blobber.extract(img)
    for b in blobs:
        if(b.mArea > 0):
            pass
        if(b.perimeter() > 0):
            pass
        if(sum(b.mAvgColor) > 0 ):
            pass
        if(sum(b.mBoundingBox) > 0 ):
            pass
        if(b.m00 is not 0 and
           b.m01 is not 0 and
           b.m10 is not 0 and
           b.m11 is not 0 and
           b.m20 is not 0 and
           b.m02 is not 0 and
           b.m21 is not 0 and
           b.m12 is not 0 ):
            pass
        if(sum(b.mHu) > 0):
            pass

def test_blob_render():
    img = Image("../sampleimages/blockhead.png")
    blobber = BlobMaker()
    blobs = blobber.extract(img)
    dl = DrawingLayer((img.width,img.height))
    reimg = DrawingLayer((img.width,img.height))
    for b in blobs:
        b.draw(color=Color.RED, alpha=128)
        b.drawHoles(width=2,color=Color.BLUE)
        b.drawHull(color=Color.ORANGE,width=2)
        b.draw(color=Color.RED, alpha=128,layer=dl)
        b.drawHoles(width=2,color=Color.BLUE,layer=dl)
        b.drawHull(color=Color.ORANGE,width=2,layer=dl)
        b.drawMaskToLayer(reimg)

    img.addDrawingLayer(dl)
    results = [img]
    name_stem = "test_blob_render"
    perform_diff(results,name_stem,tolerance=5.0)

    pass

def test_blob_methods():
    img = Image("../sampleimages/blockhead.png")
    blobber = BlobMaker()
    blobs = blobber.extract(img)
    BL = (img.width,img.height)
    first = blobs[0]
    for b in blobs:
        b.width()
        b.height()
        b.area()
        b.maxX()
        b.minX()
        b.maxY()
        b.minY()
        b.minRectWidth()
        b.minRectHeight()
        b.minRectX()
        b.minRectY()
        b.contour()
        b.aspectRatio()
        b.blobImage()
        b.blobMask()
        b.hullImage()
        b.hullMask()
        b.rectifyMajorAxis()
        b.blobImage()
        b.blobMask()
        b.hullImage()
        b.hullMask()
        b.angle()
        b.above(first)
        b.below(first)
        b.left(first)
        b.right(first)
        #b.contains(first)
        #b.overlaps(first)

def test_image_convolve():
    img = Image(testimageclr)
    kernel = np.array([[0,0,0],[0,1,0],[0,0,0]])
    img2 = img.convolve(kernel,center=(2,2))

    results = [img2]
    name_stem = "test_image_convolve"
    perform_diff(results,name_stem)

    c=img.meanColor()
    d=img2.meanColor()
    e0 = abs(c[0]-d[0])
    e1 = abs(c[1]-d[1])
    e2 = abs(c[2]-d[2])
    if( e0 > 1 or e1 > 1 or e2 > 1 ):
        assert False


def test_detection_ocr():
    img = Image(ocrimage)

    foundtext = img.readText()
    print foundtext
    if(len(foundtext) <= 1):
        assert False
    else:
        pass

def test_template_match():
    source = Image("../sampleimages/templatetest.png")
    template = Image("../sampleimages/template.png")
    t = 2
    fs = source.findTemplate(template,threshold=t)
    fs.draw()
    results = [source]
    name_stem = "test_template_match"
    perform_diff(results,name_stem)

    pass

def test_template_match_once():
    source = Image("../sampleimages/templatetest.png")
    template = Image("../sampleimages/template.png")
    t = 2
    fs = source.findTemplateOnce(template,threshold=t)
    if( len(fs) ==  0 ):
        assert False

    fs = source.findTemplateOnce(template,threshold=t,grayscale=False)
    if( len(fs) ==  0 ):
        assert False

    fs = source.findTemplateOnce(template,method='CCORR_NORM')
    if( len(fs) ==  0 ):
        assert False

    pass

def test_template_match_RGB():
    source = Image("../sampleimages/templatetest.png")
    template = Image("../sampleimages/template.png")
    t = 2
    fs = source.findTemplate(template,threshold=t, grayscale=False)
    fs.draw()
    results = [source]
    name_stem = "test_template_match"
    perform_diff(results,name_stem)

    pass


def test_image_intergralimage():
    img = Image(logo)
    ii = img.integralImage()

    if len(ii) == 0:
        assert False


def test_segmentation_diff():
    segmentor = DiffSegmentation()
    i1 = Image("logo")
    i2 = Image("logo_inverted")
    segmentor.addImage(i1)
    segmentor.addImage(i2)
    blobs = segmentor.getSegmentedBlobs()
    if(blobs == None):
        assert False
    else:
        pass

def test_segmentation_running():
    segmentor = RunningSegmentation()
    i1 = Image("logo")
    i2 = Image("logo_inverted")
    segmentor.addImage(i1)
    segmentor.addImage(i2)
    blobs = segmentor.getSegmentedBlobs()
    if(blobs == None):
        assert False
    else:
        pass

def test_segmentation_color():
    segmentor = ColorSegmentation()
    i1 = Image("logo")
    i2 = Image("logo_inverted")
    segmentor.addImage(i1)
    segmentor.addImage(i2)
    blobs = segmentor.getSegmentedBlobs()
    if(blobs == None):
        assert False
    else:
        pass

def test_embiggen():
    img = Image(logo)

    results = []
    w = int(img.width*1.2)
    h = int(img.height*1.2)

    results.append(img.embiggen(size=(w,h),color=Color.RED))
    results.append(img.embiggen(size=(w,h),color=Color.RED,pos=(30,30)))

    results.append(img.embiggen(size=(w,h),color=Color.RED,pos=(-20,-20)))
    results.append(img.embiggen(size=(w,h),color=Color.RED,pos=(30,-20)))
    results.append(img.embiggen(size=(w,h),color=Color.RED,pos=(60,-20)))
    results.append(img.embiggen(size=(w,h),color=Color.RED,pos=(60,30)))

    results.append(img.embiggen(size=(w,h),color=Color.RED,pos=(80,80)))
    results.append(img.embiggen(size=(w,h),color=Color.RED,pos=(30,80)))
    results.append(img.embiggen(size=(w,h),color=Color.RED,pos=(-20,80)))
    results.append(img.embiggen(size=(w,h),color=Color.RED,pos=(-20,30)))

    name_stem = "test_embiggen"
    perform_diff(results,name_stem)

    pass

def test_createBinaryMask():
    img2 = Image(logo)
    results = []
    results.append(img2.createBinaryMask(color1=(0,100,100),color2=(255,200,200)))
    results.append(img2.createBinaryMask(color1=(0,0,0),color2=(128,128,128)))
    results.append(img2.createBinaryMask(color1=(0,0,128),color2=(255,255,255)))

    name_stem = "test_createBinaryMask"
    perform_diff(results,name_stem)

    pass

def test_applyBinaryMask():
    img = Image(logo)
    mask = img.createBinaryMask(color1=(0,128,128),color2=(255,255,255))
    results = []
    results.append(img.applyBinaryMask(mask))
    results.append(img.applyBinaryMask(mask,bg_color=Color.RED))

    name_stem = "test_applyBinaryMask"
    perform_diff(results,name_stem,tolerance=3.0)

    pass

def test_applyPixelFunc():
    img = Image(logo)
    def myFunc((r,g,b)):
        return( (b,g,r) )

    img = img.applyPixelFunction(myFunc)
    name_stem = "test_applyPixelFunc"
    results = [img]
    perform_diff(results,name_stem)
    pass

def test_applySideBySide():
    img = Image(logo)
    img3 = Image(testimage2)

    #LB = little image big image
    #BL = big image little image  -> this is important to test all the possible cases.
    results = []

    results.append(img3.sideBySide(img,side='right',scale=False))
    results.append(img3.sideBySide(img,side='left',scale=False))
    results.append(img3.sideBySide(img,side='top',scale=False))
    results.append(img3.sideBySide(img,side='bottom',scale=False))

    results.append(img.sideBySide(img3,side='right',scale=False))
    results.append(img.sideBySide(img3,side='left',scale=False))
    results.append(img.sideBySide(img3,side='top',scale=False))
    results.append(img.sideBySide(img3,side='bottom',scale=False))

    results.append(img3.sideBySide(img,side='right',scale=True))
    results.append(img3.sideBySide(img,side='left',scale=True))
    results.append(img3.sideBySide(img,side='top',scale=True))
    results.append(img3.sideBySide(img,side='bottom',scale=True))

    results.append(img.sideBySide(img3,side='right',scale=True))
    results.append(img.sideBySide(img3,side='left',scale=True))
    results.append(img.sideBySide(img3,side='top',scale=True))
    results.append(img.sideBySide(img3,side='bottom',scale=True))

    name_stem = "test_applySideBySide"
    perform_diff(results,name_stem)

    pass

def test_resize():
    img = Image(logo)
    w = img.width
    h = img.height
    img2 = img.resize(w*2,None)
    if( img2.width != w*2 or img2.height != h*2):
        assert False

    img3 = img.resize(h=h*2)

    if( img3.width != w*2 or img3.height != h*2):
        assert False

    img4 = img.resize(h=h*2,w=w*2)

    if( img4.width != w*2 or img4.height != h*2):
        assert False

        pass

    results = [img2,img3,img4]
    name_stem = "test_resize"
    perform_diff(results,name_stem)


def test_createAlphaMask():
    alphaMask = Image(alphaSrcImg)
    mask = alphaMask.createAlphaMask(hue=60)
    mask2 = alphaMask.createAlphaMask(hue_lb=59,hue_ub=61)
    top = Image(topImg)
    bottom = Image(bottomImg)
    bottom = bottom.blit(top,alphaMask=mask2)
    results = [mask,mask2,bottom]
    name_stem = "test_createAlphaMask"
    perform_diff(results,name_stem)


def test_blit_regular():
    top = Image(topImg)
    bottom = Image(bottomImg)
    results = []
    results.append(bottom.blit(top))
    results.append(bottom.blit(top,pos=(-10,-10)))
    results.append(bottom.blit(top,pos=(-10,10)))
    results.append(bottom.blit(top,pos=(10,-10)))
    results.append(bottom.blit(top,pos=(10,10)))

    name_stem = "test_blit_regular"
    perform_diff(results,name_stem)

    pass

def test_blit_mask():
    top = Image(topImg)
    bottom = Image(bottomImg)
    mask = Image(maskImg)
    results = []
    results.append(bottom.blit(top,mask=mask))
    results.append(bottom.blit(top,mask=mask,pos=(-50,-50)))
    results.append(bottom.blit(top,mask=mask,pos=(-50,50)))
    results.append(bottom.blit(top,mask=mask,pos=(50,-50)))
    results.append(bottom.blit(top,mask=mask,pos=(50,50)))

    name_stem = "test_blit_mask"
    perform_diff(results,name_stem)

    pass


def test_blit_alpha():
    top = Image(topImg)
    bottom = Image(bottomImg)
    a = 0.5
    results = []
    results.append(bottom.blit(top,alpha=a))
    results.append(bottom.blit(top,alpha=a,pos=(-50,-50)))
    results.append(bottom.blit(top,alpha=a,pos=(-50,50)))
    results.append(bottom.blit(top,alpha=a,pos=(50,-50)))
    results.append(bottom.blit(top,alpha=a,pos=(50,50)))
    name_stem = "test_blit_alpha"
    perform_diff(results,name_stem)

    pass


def test_blit_alpha_mask():
    top = Image(topImg)
    bottom = Image(bottomImg)
    aMask = Image(alphaMaskImg)
    results = []

    results.append(bottom.blit(top,alphaMask=aMask))
    results.append(bottom.blit(top,alphaMask=aMask,pos=(-10,-10)))
    results.append(bottom.blit(top,alphaMask=aMask,pos=(-10,10)))
    results.append(bottom.blit(top,alphaMask=aMask,pos=(10,-10)))
    results.append(bottom.blit(top,alphaMask=aMask,pos=(10,10)))

    name_stem = "test_blit_alpha_mask"
    perform_diff(results,name_stem)

    pass


def test_imageset():
    imgs = ImageSet()

    if(isinstance(imgs, ImageSet)):
        pass
    else:
        assert False

def test_hsv_conversion():
    px = Image((1,1))
    px[0,0] = Color.GREEN
    if (Color.hsv(Color.GREEN) == px.toHSV()[0,0]):
        pass
    else:
        assert False


def test_whiteBalance():
    img = Image("../sampleimages/BadWB2.jpg")
    output = img.whiteBalance()
    output2 = img.whiteBalance(method="GrayWorld")
    results = [output,output2]
    name_stem = "test_whiteBalance"
    perform_diff(results,name_stem)

def test_hough_circles():
    img = Image(circles)
    circs = img.findCircle(thresh=100)
    circs.draw()
    if( circs[0] < 1 ):
        assert False
    circs[0].coordinates()
    circs[0].width()
    circs[0].area()
    circs[0].perimeter()
    circs[0].height()
    circs[0].radius()
    circs[0].diameter()
    circs[0].colorDistance()
    circs[0].meanColor()
    circs[0].distanceFrom(point=(0,0))
    circs[0].draw()
    img2 = circs[0].crop()
    img3 = circs[0].crop(noMask=True)

    results = [img,img2,img3]
    name_stem = "test_hough_circle"
    perform_diff(results,name_stem)


    if( img2 is not None and img3 is not None ):
        pass
    else:
        assert False

def test_drawRectangle():
    img = Image(testimage2)
    img.drawRectangle(0,0,100,100,color=Color.BLUE,width=0,alpha=0)
    img.drawRectangle(1,1,100,100,color=Color.BLUE,width=2,alpha=128)
    img.drawRectangle(1,1,100,100,color=Color.BLUE,width=1,alpha=128)
    img.drawRectangle(2,2,100,100,color=Color.BLUE,width=1,alpha=255)
    img.drawRectangle(3,3,100,100,color=Color.BLUE)
    img.drawRectangle(4,4,100,100,color=Color.BLUE,width=12)
    img.drawRectangle(5,5,100,100,color=Color.BLUE,width=-1)

    results = [img]
    name_stem = "test_drawRectangle"
    perform_diff(results,name_stem)

    pass


def test_BlobMinRect():
    img = Image(testimageclr)
    blobs = img.findBlobs()
    for b in blobs:
        b.drawMinRect(color=Color.BLUE,width=3,alpha=123)
    results = [img]
    name_stem = "test_BlobMinRect"
    perform_diff(results,name_stem)
    pass

def test_BlobRect():
    img = Image(testimageclr)
    blobs = img.findBlobs()
    for b in blobs:
        b.drawRect(color=Color.BLUE,width=3,alpha=123)

    results = [img]
    name_stem = "test_BlobRect"
    perform_diff(results,name_stem)
    pass


def test_BlobPickle():
    img = Image(testimageclr)
    blobs = img.findBlobs()
    for b in blobs:
        p = pickle.dumps(b)
        ub = pickle.loads(p)
        if (ub.mMask - b.mMask).meanColor() != Color.BLACK:
            assert False

    pass

def test_blob_isa_methods():
    img1 = Image(circles)
    img2 = Image("../sampleimages/blockhead.png")
    blobs = img1.findBlobs().sortArea()
    t1 = blobs[-1].isCircle()
    f1 = blobs[-1].isRectangle()
    blobs = img2.findBlobs().sortArea()
    f2 = blobs[-1].isCircle()
    t2 = blobs[-1].isRectangle()
    if( t1 and t2 and not f1 and not f2):
        pass
    else:
        assert False

def test_findKeypoints():
    try:
        import cv2
    except:
        pass
        return
    img = Image(testimage2)
    if cv2.__version__.startswith('$Rev:'):
        flavors = ['SURF','STAR','SIFT'] # supported in 2.3.1
    elif cv2.__version__ == '2.4.0' or cv2.__version__ == '2.4.1':
        flavors = ['SURF','STAR','FAST','MSER','ORB','BRISK','SIFT','Dense']
    else:
        flavors = ['SURF','STAR','FAST','MSER','ORB','BRISK','FREAK','SIFT','Dense']
    for flavor in flavors:
        try:
            print "trying to find " + flavor + " keypoints."
            kp = img.findKeypoints(flavor=flavor)
        except:
            continue
        if( kp is not None ):
            print "Found: " + str(len(kp))
            for k in kp:
                k.getObject()
                k.descriptor()
                k.quality()
                k.octave()
                k.flavor()
                k.angle()
                k.coordinates()
                k.draw()
                k.distanceFrom()
                k.meanColor()
                k.area()
                k.perimeter()
                k.width()
                k.height()
                k.radius()
                k.crop()
            kp.draw()
        else:
            print "Found None."
    results = [img]
    name_stem = "test_findKeypoints"
    #~ perform_diff(results,name_stem)

    pass

def test_movement_feature():
    current1 = Image("../sampleimages/flow_simple1.png")
    prev = Image("../sampleimages/flow_simple2.png")

    fs = current1.findMotion(prev, window=7)
    if( len(fs) > 0 ):
        fs.draw(color=Color.RED)
        img = fs[0].crop()
        color = fs[1].meanColor()
        wndw = fs[1].windowSz()
        for f in fs:
            f.vector()
            f.magnitude()
    else:
        assert False


    current2 = Image("../sampleimages/flow_simple1.png")
    fs = current2.findMotion(prev, window=7,method='HS')
    if( len(fs) > 0 ):
        fs.draw(color=Color.RED)
        img = fs[0].crop()
        color = fs[1].meanColor()
        wndw = fs[1].windowSz()
        for f in fs:
            f.vector()
            f.magnitude()
    else:
        assert False

    current3 = Image("../sampleimages/flow_simple1.png")
    fs = current3.findMotion(prev, window=7,method='LK',aggregate=False)
    if( len(fs) > 0 ):
        fs.draw(color=Color.RED)
        img = fs[0].crop()
        color = fs[1].meanColor()
        wndw = fs[1].windowSz()
        for f in fs:
            f.vector()
            f.magnitude()
    else:
        assert False

    results = [current1,current2,current3]
    name_stem = "test_movement_feature"
    #~ perform_diff(results,name_stem,tolerance=4.0)

    pass

def test_keypoint_extraction():
    try:
        import cv2
    except:
        pass
        return

    img1 = Image("../sampleimages/KeypointTemplate2.png")
    img2 = Image("../sampleimages/KeypointTemplate2.png")
    img3 = Image("../sampleimages/KeypointTemplate2.png")
    img4 = Image("../sampleimages/KeypointTemplate2.png")

    kp1 = img1.findKeypoints()
    kp2 = img2.findKeypoints(highQuality=True)
    kp3 = img3.findKeypoints(flavor="STAR")
    if not cv2.__version__.startswith("$Rev:"):
        kp4 = img4.findKeypoints(flavor="BRISK")
        kp4.draw()
        if len(kp4) == 0:
            assert False
    kp1.draw()
    kp2.draw()
    kp3.draw()



    #TODO: Fix FAST binding
    #~ kp4 = img.findKeypoints(flavor="FAST",min_quality=10)
    if( len(kp1)==190 and
        len(kp2)==190 and
        len(kp3)==37
        #~ and len(kp4)==521
      ):
        pass
    else:
        assert False
    results = [img1,img2,img3]
    name_stem = "test_keypoint_extraction"
    perform_diff(results,name_stem,tolerance=4.0)


def test_keypoint_match():
    try:
        import cv2
    except:
        pass
        return

    template = Image("../sampleimages/KeypointTemplate2.png")
    match0 = Image("../sampleimages/kptest0.png")
    match1 = Image("../sampleimages/kptest1.png")
    match3 = Image("../sampleimages/kptest2.png")
    match2 = Image("../sampleimages/aerospace.jpg")# should be none

    fs0 = match0.findKeypointMatch(template)#test zero
    fs1 = match1.findKeypointMatch(template,quality=300.00,minDist=0.5,minMatch=0.2)
    fs3 = match3.findKeypointMatch(template,quality=300.00,minDist=0.5,minMatch=0.2)
    print "This should fail"
    fs2 = match2.findKeypointMatch(template,quality=500.00,minDist=0.2,minMatch=0.1)
    if( fs0 is not None and fs1 is not None and fs2 is None and  fs3 is not None):
        fs0.draw()
        fs1.draw()
        fs3.draw()
        f = fs0[0]
        f.drawRect()
        f.draw()
        f.getHomography()
        f.getMinRect()
        f.x
        f.y
        f.coordinates()
    else:
        assert False

    results = [match0,match1,match2,match3]
    name_stem = "test_find_keypoint_match"
    perform_diff(results,name_stem)


def test_draw_keypoint_matches():
    try:
        import cv2
    except:
        pass
        return
    template = Image("../sampleimages/KeypointTemplate2.png")
    match0 = Image("../sampleimages/kptest0.png")
    result = match0.drawKeypointMatches(template,thresh=500.00,minDist=0.15,width=1)

    results = [result]
    name_stem = "test_draw_keypoint_matches"
    perform_diff(results,name_stem,tolerance=4.0)


    pass


def test_basic_palette():
    img = Image(testimageclr)
    img._generatePalette(10,False)
    if( img._mPalette is not None and
        img._mPaletteMembers is not None and
        img._mPalettePercentages is not None and
        img._mPaletteBins == 10
        ):
        img._generatePalette(20,True)
        if( img._mPalette is not None and
            img._mPaletteMembers is not None and
            img._mPalettePercentages is not None and
            img._mPaletteBins == 20
            ):
            pass

def test_palettize():
    img = Image(testimageclr)
    img2 = img.palettize(bins=20,hue=False)
    img3 = img.palettize(bins=3,hue=True)
    img4 = img.palettize(centroids=[Color.WHITE,Color.RED,Color.BLUE,Color.GREEN,Color.BLACK])
    img4 = img.palettize(hue=True,centroids=[(0),(30),(60),(180)])
    #UHG@! can't diff because of the kmeans initial conditions causes
    # things to bounce around... otherwise we need to set a friggin huge tolerance

    #results = [img2,img3]
    #name_stem = "test_palettize"
    #perform_diff(results,name_stem)
    pass

def test_repalette():
    img = Image(testimageclr)
    img2 = Image(bottomImg)
    p = img.getPalette()
    img3 = img2.rePalette(p)
    p = img.getPalette(hue=True)
    img4 = img2.rePalette(p,hue=True)

    #results = [img3,img4]
    #name_stem = "test_repalette"
    #perform_diff(results,name_stem)

    pass

def test_drawPalette():
    img = Image(testimageclr)
    img1 = img.drawPaletteColors()
    img2 = img.drawPaletteColors(horizontal=False)
    img3 = img.drawPaletteColors(size=(69,420) )
    img4 = img.drawPaletteColors(size=(69,420),horizontal=False)
    img5 = img.drawPaletteColors(hue=True)
    img6 = img.drawPaletteColors(horizontal=False,hue=True)
    img7 = img.drawPaletteColors(size=(69,420),hue=True )
    img8 = img.drawPaletteColors(size=(69,420),horizontal=False,hue=True)

def test_palette_binarize():
    img = Image(testimageclr)
    p = img.getPalette()
    img2 = img.binarizeFromPalette(p[0:5])
    p = img.getPalette(hue=True)
    img2 = img.binarizeFromPalette(p[0:5])

    pass

def test_palette_blobs():
    img = Image(testimageclr)
    p = img.getPalette()
    b1 = img.findBlobsFromPalette(p[0:5])
    b1.draw()


    p = img.getPalette(hue=True)
    b2 = img.findBlobsFromPalette(p[0:5])
    b2.draw()

    if( len(b1) > 0 and len(b2) > 0 ):
        pass
    else:
        assert False



def test_skeletonize():
    img = Image(logo)
    s = img.skeletonize()
    s2 = img.skeletonize(10)

    results = [s,s2]
    name_stem = "test_skelotinze"
    perform_diff(results,name_stem)

    pass


def test_threshold():
    img = Image(logo)
    for t in range(0,255):
        img.threshold(t)
    pass

def test_smartThreshold():
    img = Image("../sampleimages/RatTop.png")
    mask = Image((img.width,img.height))
    mask.dl().circle((100,100),80,color=Color.MAYBE_BACKGROUND,filled=True)
    mask.dl().circle((100,100),60,color=Color.MAYBE_FOREGROUND,filled=True)
    mask.dl().circle((100,100),40,color=Color.FOREGROUND,filled=True)
    mask = mask.applyLayers()
    new_mask1 = img.smartThreshold(mask=mask)
    new_mask2 = img.smartThreshold(rect=(30,30,150,185))


    results = [new_mask1,new_mask2]
    name_stem = "test_smartThreshold"
    perform_diff(results,name_stem)

    pass

def test_smartFindBlobs():
    img = Image("../sampleimages/RatTop.png")
    mask = Image((img.width,img.height))
    mask.dl().circle((100,100),80,color=Color.MAYBE_BACKGROUND,filled=True)
    mask.dl().circle((100,100),60,color=Color.MAYBE_FOREGROUND,filled=True)
    mask.dl().circle((100,100),40,color=Color.FOREGROUND,filled=True)
    mask = mask.applyLayers()
    blobs = img.smartFindBlobs(mask=mask)
    blobs.draw()
    results = [img]

    if( len(blobs) < 1 ):
        assert False

    for t in range(2,3):
        img = Image("../sampleimages/RatTop.png")
        blobs2 = img.smartFindBlobs(rect=(30,30,150,185),thresh_level=t)
        if(blobs2 is not None):
            blobs2.draw()
            results.append(img)

    name_stem = "test_smartFindBlobs"
    perform_diff(results,name_stem)

    pass


def test_image_webp_load():
    #only run if webm suppport exist on system
    try:
        import webm
    except:
        if( SHOW_WARNING_TESTS ):
            logger.warning("Couldn't run the webp test as optional webm library required")
        pass

    else:
        img = Image(webp)

        if len(img.toString()) <= 1:
            assert False

        else:
            pass

def test_image_webp_save():
    #only run if webm suppport exist on system
    try:
        import webm
    except:
        if( SHOW_WARNING_TESTS ):
            logger.warning("Couldn't run the webp test as optional webm library required")
        pass

    else:
        img = Image('simplecv')
        tf = tempfile.NamedTemporaryFile(suffix=".webp")
        if img.save(tf.name):
            pass
        else:
            assert False

def test_detection_spatial_relationships():
    img = Image(testimageclr)
    template = img.crop(200,200,50,50)
    motion = img.embiggen((img.width+10,img.height+10),pos=(10,10))
    motion = motion.crop(0,0,img.width,img.height)
    blobFS = img.findBlobs()
    lineFS = img.findLines()
    cornFS = img.findCorners()
    moveFS = img.findMotion(motion)
    moveFS = FeatureSet(moveFS[42:52]) # l337 s5p33d h4ck - okay not really
    tempFS = img.findTemplate(template,threshold=1)
    aCirc = (img.width/2,img.height/2,np.min([img.width/2,img.height/2]))
    aRect = (50,50,200,200)
    aPoint = (img.width/2,img.height/2)
    aPoly =  [(0,0),(img.width/2,0),(0,img.height/2)] # a triangle


    feats  = [blobFS,lineFS,cornFS,tempFS,moveFS]

    for f in feats:
        print str(len(f))

    for f in feats:
        for g in feats:

            sample = f[0]
            sample2 = f[1]
            print type(f[0])
            print type(g[0])

            g.above(sample)
            g.below(sample)
            g.left(sample)
            g.right(sample)
            g.overlaps(sample)
            g.inside(sample)
            g.outside(sample)

            g.inside(aRect)
            g.outside(aRect)

            g.inside(aCirc)
            g.outside(aCirc)

            g.inside(aPoly)
            g.outside(aPoly)

            g.above(aPoint)
            g.below(aPoint)
            g.left(aPoint)
            g.right(aPoint)


    pass

def test_getEXIFData():
    img = Image("../sampleimages/cat.jpg")
    img2 = Image(testimage)
    d1 = img.getEXIFData()
    d2 = img2.getEXIFData()
    if( len(d1) > 0 and len(d2) == 0 ):
        pass
    else:
        assert False

def test_get_raw_dft():
    img = Image("../sampleimages/RedDog2.jpg")
    raw3 = img.rawDFTImage()
    raw1 = img.rawDFTImage(grayscale=True)
    if( len(raw3) != 3 or
        len(raw1) != 1 or
        raw1[0].width != img.width or
        raw1[0].height != img.height or
        raw3[0].height != img.height or
        raw3[0].width != img.width or
        raw1[0].depth != 64L or
        raw3[0].depth != 64L or
        raw3[0].channels != 2 or
        raw3[0].channels != 2 ):
        assert False
    else:
        pass

def test_getDFTLogMagnitude():
    img = Image("../sampleimages/RedDog2.jpg")
    lm3 = img.getDFTLogMagnitude()
    lm1 = img.getDFTLogMagnitude(grayscale=True)

    results = [lm3,lm1]
    name_stem = "test_getDFTLogMagnitude"
    perform_diff(results,name_stem,tolerance=6.0)

    pass


def test_applyDFTFilter():
    img = Image("../sampleimages/RedDog2.jpg")
    flt = Image("../sampleimages/RedDogFlt.png")
    f1 = img.applyDFTFilter(flt)
    f2 = img.applyDFTFilter(flt,grayscale=True)
    results = [f1,f2]
    name_stem = "test_applyDFTFilter"
    perform_diff(results,name_stem)
    pass

def test_highPassFilter():
    img = Image("../sampleimages/RedDog2.jpg")
    a = img.highPassFilter(0.5)
    b = img.highPassFilter(0.5,grayscale=True)
    c = img.highPassFilter(0.5,yCutoff=0.4)
    d = img.highPassFilter(0.5,yCutoff=0.4,grayscale=True)
    e = img.highPassFilter([0.5,0.4,0.3])
    f = img.highPassFilter([0.5,0.4,0.3],yCutoff=[0.5,0.4,0.3])

    results = [a,b,c,d,e,f]
    name_stem = "test_HighPassFilter"
    perform_diff(results,name_stem)
    pass

def test_lowPassFilter():
    img = Image("../sampleimages/RedDog2.jpg")
    a = img.lowPassFilter(0.5)
    b = img.lowPassFilter(0.5,grayscale=True)
    c = img.lowPassFilter(0.5,yCutoff=0.4)
    d = img.lowPassFilter(0.5,yCutoff=0.4,grayscale=True)
    e = img.lowPassFilter([0.5,0.4,0.3])
    f = img.lowPassFilter([0.5,0.4,0.3],yCutoff=[0.5,0.4,0.3])

    results = [a,b,c,d,e,f]
    name_stem = "test_LowPassFilter"
    perform_diff(results,name_stem)

    pass

def test_DFT_gaussian():
    img = Image("../sampleimages/RedDog2.jpg")
    flt = DFT.createGaussianFilter(dia=300, size=(300, 300), highpass=False)
    fltimg = img.filter(flt)
    fltimggray = img.filter(flt, grayscale=True)
    flt = DFT.createGaussianFilter(dia=300, size=(300, 300), highpass=True)
    fltimg1 = img.filter(flt)
    fltimggray1 = img.filter(flt, grayscale=True)
    results = [fltimg, fltimggray, fltimg1, fltimggray1]
    name_stem = "test_DFT_gaussian"
    perform_diff(results, name_stem)
    pass

def test_DFT_butterworth():
    img = Image("../sampleimages/RedDog2.jpg")
    flt = DFT.createButterworthFilter(dia=300, size=(300, 300), order=3, highpass=False)
    fltimg = img.filter(flt)
    fltimggray = img.filter(flt, grayscale=True)
    flt = DFT.createButterworthFilter(dia=100, size=(300, 300), order=3, highpass=True)
    fltimg1 = img.filter(flt)
    fltimggray1 = img.filter(flt, grayscale=True)
    results = [fltimg, fltimggray, fltimg1, fltimggray1]
    name_stem = "test_DFT_butterworth"
    perform_diff(results, name_stem)
    pass

def test_DFT_lowpass():
    img = Image("../sampleimages/RedDog2.jpg")
    flt = DFT.createLowpassFilter(xCutoff=150, size=(600, 600))
    fltimg = img.filter(flt)
    fltimggray = img.filter(flt, grayscale=True)
    results = [fltimg, fltimggray]
    name_stem = "test_DFT_lowpass"
    perform_diff(results, name_stem, 20)
    pass

def test_DFT_highpass():
    img = Image("../sampleimages/RedDog2.jpg")
    flt = DFT.createLowpassFilter(xCutoff=10, size=(600, 600))
    fltimg = img.filter(flt)
    fltimggray = img.filter(flt, grayscale=True)
    results = [fltimg, fltimggray]
    name_stem = "test_DFT_highpass"
    perform_diff(results, name_stem, 20)
    pass

def test_DFT_notch():
    img = Image("../sampleimages/RedDog2.jpg")
    flt = DFT.createNotchFilter(dia1=500, size=(512, 512), type="lowpass")
    fltimg = img.filter(flt)
    fltimggray = img.filter(flt, grayscale=True)
    flt = DFT.createNotchFilter(dia1=300, size=(512, 512), type="highpass")
    fltimg1 = img.filter(flt)
    fltimggray1 = img.filter(flt, grayscale=True)
    results = [fltimg, fltimggray, fltimg1, fltimggray1]
    name_stem = "test_DFT_notch"
    perform_diff(results, name_stem, 20)

def test_findHaarFeatures():
    img = Image("../sampleimages/orson_welles.jpg")
    face = HaarCascade("face.xml") #old HaarCascade
    f = img.findHaarFeatures(face)
    f2 = img.findHaarFeatures("face_cv2.xml") #new cv2 HaarCascade
    if( len(f) > 0 and len(f2) > 0 ):
        f.draw()
        f2.draw()
        f[0].width()
        f[0].height()
        f[0].draw()
        f[0].x
        f[0].y
        f[0].length()
        f[0].area()
        pass
    else:
        assert False

    results = [img]
    name_stem = "test_findHaarFeatures"
    perform_diff(results,name_stem)


def test_biblical_flood_fill():
    img = Image(testimage2)
    b = img.findBlobs()
    img.floodFill(b.coordinates(),tolerance=3,color=Color.RED)
    img.floodFill(b.coordinates(),tolerance=(3,3,3),color=Color.BLUE)
    img.floodFill(b.coordinates(),tolerance=(3,3,3),color=Color.GREEN,fixed_range=False)
    img.floodFill((30,30),lower=3,upper=5,color=Color.ORANGE)
    img.floodFill((30,30),lower=3,upper=(5,5,5),color=Color.ORANGE)
    img.floodFill((30,30),lower=(3,3,3),upper=5,color=Color.ORANGE)
    img.floodFill((30,30),lower=(3,3,3),upper=(5,5,5))
    img.floodFill((30,30),lower=(3,3,3),upper=(5,5,5),color=np.array([255,0,0]))
    img.floodFill((30,30),lower=(3,3,3),upper=(5,5,5),color=[255,0,0])

    results = [img]
    name_stem = "test_biblical_flood_fill"
    perform_diff(results,name_stem)

    pass

def test_flood_fill_to_mask():
    img = Image(testimage2)
    b = img.findBlobs()
    imask = img.edges()
    omask = img.floodFillToMask(b.coordinates(),tolerance=10)
    omask2 = img.floodFillToMask(b.coordinates(),tolerance=(3,3,3),mask=imask)
    omask3 = img.floodFillToMask(b.coordinates(),tolerance=(3,3,3),mask=imask,fixed_range=False)

    results = [omask,omask2,omask3]
    name_stem = "test_flood_fill_to_mask"
    perform_diff(results,name_stem)

    pass

def test_findBlobsFromMask():
    img = Image(testimage2)
    mask = img.binarize().invert()
    b1 = img.findBlobsFromMask(mask)
    b2 = img.findBlobs()
    b1.draw()
    b2.draw()

    results = [img]
    name_stem = "test_findBlobsFromMask"
    perform_diff(results,name_stem)


    if(len(b1) == len(b2) ):
        pass
    else:
        assert False



def test_bandPassFilter():
    img = Image("../sampleimages/RedDog2.jpg")
    a = img.bandPassFilter(0.1,0.3)
    b = img.bandPassFilter(0.1,0.3,grayscale=True)
    c = img.bandPassFilter(0.1,0.3,yCutoffLow=0.1,yCutoffHigh=0.3)
    d = img.bandPassFilter(0.1,0.3,yCutoffLow=0.1,yCutoffHigh=0.3,grayscale=True)
    e = img.bandPassFilter([0.1,0.2,0.3],[0.5,0.5,0.5])
    f = img.bandPassFilter([0.1,0.2,0.3],[0.5,0.5,0.5],yCutoffLow=[0.1,0.2,0.3],yCutoffHigh=[0.6,0.6,0.6])
    results = [a,b,c,d,e,f]
    name_stem = "test_bandPassFilter"
    perform_diff(results,name_stem)


def test_image_slice():
    img = Image("../sampleimages/blockhead.png")
    I = img.findLines()
    I2 = I[0:10]
    if type(I2) == list:
        assert False
    else:
        pass

def test_blob_spatial_relationships():
    img = Image("../sampleimages/spatial_relationships.png")
    #please see the image
    blobs = img.findBlobs(threshval=1)
    blobs = blobs.sortArea()
    print(len(blobs))

    center = blobs[-1]
    top = blobs[-2]
    right = blobs[-3]
    bottom = blobs[-4]
    left = blobs[-5]
    inside = blobs[-7]
    overlap = blobs[-6]

    if( not top.above(center) ):
        assert False
    if( not bottom.below(center)):
        assert False
    if( not right.right(center)):
        assert False
    if( not left.left(center)):
        assert False

    if( not center.contains(inside)):
        assert False

    if( center.contains(left) ):
        assert False

    if( not center.overlaps(overlap) ):
        assert False

    if( not overlap.overlaps(center) ):
        assert False

    myTuple = (img.width/2,img.height/2)

    if( not top.above(myTuple) ):
        assert False
    if( not bottom.below(myTuple)):
        assert False
    if( not right.right(myTuple)):
        assert False
    if( not left.left(myTuple)):
        assert False

    if( not top.above(myTuple) ):
        assert False
    if( not bottom.below(myTuple)):
        assert False
    if( not right.right(myTuple)):
        assert False
    if( not left.left(myTuple)):
        assert False
    if( not center.contains(myTuple)):
        assert False

    myNPA = np.array([img.width/2,img.height/2])
    if( not top.above(myNPA) ):
        assert False
    if( not bottom.below(myNPA)):
        assert False
    if( not right.right(myNPA)):
        assert False
    if( not left.left(myNPA)):
        assert False
    if( not center.contains(myNPA)):
        assert False

    if( not center.contains(inside) ):
        assert False

def test_get_aspectratio():
    img = Image("../sampleimages/EdgeTest1.png")
    img2 = Image("../sampleimages/EdgeTest2.png")
    b = img.findBlobs()
    l = img2.findLines()
    c = img2.findCircle(thresh=200)
    c2 = img2.findCorners()
    kp = img2.findKeypoints()
    bb = b.aspectRatios()
    ll = l.aspectRatios()
    cc = c.aspectRatios()
    c22 = c2.aspectRatios()
    kp2 = kp.aspectRatios()

    if( len(bb) > 0 and
        len(ll) > 0 and
        len(cc) > 0 and
        len(c22) > 0 and
        len(kp2) > 0 ):
        pass
    else:
        assert False

def test_line_crop():
    img = Image("../sampleimages/EdgeTest2.png")
    l = img.findLines().sortArea()
    l = l[-5:-1]
    results = []
    for ls in l:
        results.append( ls.crop() )
    name_stem = "test_lineCrop"
    perform_diff(results,name_stem,tolerance=3.0)
    pass

def test_get_corners():
    img = Image("../sampleimages/EdgeTest1.png")
    img2 = Image("../sampleimages/EdgeTest2.png")
    b = img.findBlobs()
    tl = b.topLeftCorners()
    tr = b.topRightCorners()
    bl = b.bottomLeftCorners()
    br = b.bottomRightCorners()

    l = img2.findLines()
    tl2 = l.topLeftCorners()
    tr2 = l.topRightCorners()
    bl2 = l.bottomLeftCorners()
    br2 = l.bottomRightCorners()

    if( tl is not None and
        tr is not None and
        bl is not None and
        br is not None and
        tl2 is not None and
        tr2 is not None and
        bl2 is not None and
        br2 is not None ):
        pass
    else:
        assert False

def test_save_kwargs():
    img = Image("lenna")
    l95 = "l95.jpg"
    l90 = "l90.jpg"
    l80 ="l80.jpg"
    l70="l70.jpg"

    img.save(l95,quality=95)
    img.save(l90,quality=90)
    img.save(l80,quality=80)
    img.save(l70,quality=75)

    s95 = os.stat(l95).st_size
    s90 = os.stat(l90).st_size
    s80 = os.stat(l80).st_size
    s70 = os.stat(l70).st_size

    if( s70 < s80 and s80 < s90 and s90 < s95 ):
        pass
    else:
        assert False

    s95 = os.remove(l95)
    s90 = os.remove(l90)
    s80 = os.remove(l80)
    s70 = os.remove(l70)

def test_on_edge():
    img1 = "./../sampleimages/EdgeTest1.png"
    img2 = "./../sampleimages/EdgeTest2.png"
    imgA = Image(img1)
    imgB = Image(img2)
    imgC = Image(img2)
    imgD = Image(img2)
    imgE = Image(img2)

    blobs = imgA.findBlobs()
    circs = imgB.findCircle(thresh=200)
    corners = imgC.findCorners()
    kp = imgD.findKeypoints()
    lines = imgE.findLines()

    rim =  blobs.onImageEdge()
    inside = blobs.notOnImageEdge()
    rim.draw(color=Color.RED)
    inside.draw(color=Color.BLUE)

    rim =  circs.onImageEdge()
    inside = circs.notOnImageEdge()
    rim.draw(color=Color.RED)
    inside.draw(color=Color.BLUE)

    #rim =  corners.onImageEdge()
    inside = corners.notOnImageEdge()
    #rim.draw(color=Color.RED)
    inside.draw(color=Color.BLUE)

    #rim =  kp.onImageEdge()
    inside = kp.notOnImageEdge()
    #rim.draw(color=Color.RED)
    inside.draw(color=Color.BLUE)

    rim =  lines.onImageEdge()
    inside = lines.notOnImageEdge()
    rim.draw(color=Color.RED)
    inside.draw(color=Color.BLUE)

    results = [imgA,imgB,imgC,imgD,imgE]
    name_stem = "test_onEdge_Features"
    #~ perform_diff(results,name_stem,tolerance=8.0)

def test_feature_angles():
    img = Image("../sampleimages/rotation2.png")
    img2 = Image("../sampleimages/rotation.jpg")
    img3 = Image("../sampleimages/rotation.jpg")
    b = img.findBlobs()
    l = img2.findLines()
    k = img3.findKeypoints()

    for bs in b:
        tl = bs.topLeftCorner()
        img.drawText(str(bs.angle()),tl[0],tl[1],color=Color.RED)

    for ls in l:
        tl = ls.topLeftCorner()
        img2.drawText(str(ls.angle()),tl[0],tl[1],color=Color.GREEN)

    for ks in k:
        tl = ks.topLeftCorner()
        img3.drawText(str(ks.angle()),tl[0],tl[1],color=Color.BLUE)

    results = [img,img2,img3]
    name_stem = "test_feature_angles"
    perform_diff(results,name_stem,tolerance=11.0)

def test_feature_angles_rotate():
    img = Image("../sampleimages/rotation2.png")
    b = img.findBlobs()
    results = []

    for bs in b:
        temp = bs.crop()
        derp = temp.rotate(bs.angle(),fixed=False)
        derp.drawText(str(bs.angle()),10,10,color=Color.RED)
        results.append(derp)
        bs.rectifyMajorAxis()
        results.append(bs.blobImage())

    name_stem = "test_feature_angles_rotate"
    perform_diff(results,name_stem,tolerance=7.0)


def test_nparray2cvmat():
    img = Image('logo')
    gray = img.getGrayNumpy()
    gf32 = np.array(gray,dtype='float32')
    gf64 = np.array(gray,dtype='float64')

    a = npArray2cvMat(gray)
    b = npArray2cvMat(gf32)
    c = npArray2cvMat(gf64)

    a = npArray2cvMat(gray,cv.CV_8UC1)
    b = npArray2cvMat(gf32,cv.CV_8UC1)
    c = npArray2cvMat(gf64,cv.CV_8UC1)

def test_minrect_blobs():
    img = Image("../sampleimages/bolt.png")
    img = img.invert()
    results = []
    for i in range(-10,10):
        ang = float(i*18.00)
        print ang
        t = img.rotate(ang)
        b = t.findBlobs(threshval=128)
        b[-1].drawMinRect(color=Color.RED,width=5)
        results.append(t)

    name_stem = "test_minrect_blobs"
    perform_diff(results,name_stem,tolerance=11.0)

def test_pixelize():
    img = Image("../sampleimages/The1970s.png")
    img1 = img.pixelize(4)
    img2 = img.pixelize((5,13))
    img3 = img.pixelize((img.width/10,img.height))
    img4 = img.pixelize((img.width,img.height/10))
    img5 = img.pixelize((12,12),(200,180,250,250))
    img6 = img.pixelize((12,12),(600,80,250,250))
    img7 = img.pixelize((12,12),(600,80,250,250),levels=4)
    img8 = img.pixelize((12,12),levels=6)
    #img9 = img.pixelize(4, )
    #img10 = img.pixelize((5,13))
    #img11 = img.pixelize((img.width/10,img.height), mode=True)
    #img12 = img.pixelize((img.width,img.height/10), mode=True)
    #img13 = img.pixelize((12,12),(200,180,250,250), mode=True)
    #img14 = img.pixelize((12,12),(600,80,250,250), mode=True)
    #img15 = img.pixelize((12,12),(600,80,250,250),levels=4, mode=True)
    #img16 = img.pixelize((12,12),levels=6, mode=True)

    results = [img1,img2,img3,img4,img5,img6,img7,img8] #img9,img10,img11,img12,img13,img14,img15,img16]
    name_stem = "test_pixelize"
    perform_diff(results,name_stem,tolerance=6.0)

def test_hueFromRGB():
    img = Image("lenna")
    img_hsv = img.toHSV()
    h,s,r = img_hsv[100,300]
    err = 2
    hue = Color.getHueFromRGB(img[100,300])
    if hue > h - err and hue < h + err:
        pass
    else:
        assert False

def test_hueFromBGR():
    img = Image("lenna")
    img_hsv = img.toHSV()
    h,s,r = img_hsv[150,400]
    err = 2
    color_tuple = tuple(reversed(img[150,400]))
    hue = Color.getHueFromBGR(color_tuple)
    if hue > h - err and hue < h + err:
        pass
    else:
        assert False

def test_hueToRGB():
    r,g,b = Color.hueToRGB(0)
    if (r,g,b)== (255,0,0):
        pass
    else:
        assert False
    r,g,b = Color.hueToRGB(15)
    if (r,g,b) == (255,128,0):
        pass
    else:
        assert False
    r,g,b = Color.hueToRGB(30)
    if (r,g,b) == (255,255,0):
        pass
    else:
        assert False
    r,g,b = Color.hueToRGB(45)
    if (r,g,b) == (128,255,0):
        pass
    else:
        assert False
    r,g,b = Color.hueToRGB(60)
    if (r,g,b) == (0,255,0):
        pass
    else:
        assert False
    r,g,b = Color.hueToRGB(75)
    if (r,g,b) == (0,255,128):
        pass
    else:
        assert False
    r,g,b = Color.hueToRGB(90)
    if (r,g,b) == (0,255,255):
        pass
    else:
        assert False
    r,g,b = Color.hueToRGB(105)
    if (r,g,b) == (0,128,255):
        pass
    else:
        assert False
    r,g,b = Color.hueToRGB(120)
    if (r,g,b) == (0,0,255):
        pass
    else:
        assert False
    r,g,b = Color.hueToRGB(135)
    if (r,g,b) == (128,0,255):
        pass
    else:
        assert False
    r,g,b = Color.hueToRGB(150)
    if (r,g,b) == (255,0,255):
        pass
    else:
        assert False
    r,g,b = Color.hueToRGB(165)
    if (r,g,b) == (255,0,128):
        pass
    else:
        assert False

def test_hueToBGR():
    b,g,r = Color.hueToBGR(0)
    if (r,g,b)== (255,0,0):
        pass
    else:
        assert False
    b,g,r= Color.hueToBGR(15)
    if (r,g,b) == (255,128,0):
        pass
    else:
        assert False
    b,g,r= Color.hueToBGR(30)
    if (r,g,b) == (255,255,0):
        pass
    else:
        assert False
    b,g,r= Color.hueToBGR(45)
    if (r,g,b) == (128,255,0):
        pass
    else:
        assert False
    b,g,r= Color.hueToBGR(60)
    if (r,g,b) == (0,255,0):
        pass
    else:
        assert False
    b,g,r= Color.hueToBGR(75)
    if (r,g,b) == (0,255,128):
        pass
    else:
        assert False
    b,g,r= Color.hueToBGR(90)
    if (r,g,b) == (0,255,255):
        pass
    else:
        assert False
    b,g,r= Color.hueToBGR(105)
    if (r,g,b) == (0,128,255):
        pass
    else:
        assert False
    b,g,r= Color.hueToBGR(120)
    if (r,g,b) == (0,0,255):
        pass
    else:
        assert False
    b,g,r= Color.hueToBGR(135)
    if (r,g,b) == (128,0,255):
        pass
    else:
        assert False
    b,g,r= Color.hueToBGR(150)
    if (r,g,b) == (255,0,255):
        pass
    else:
        assert False
    b,g,r= Color.hueToBGR(165)
    if (r,g,b) == (255,0,128):
        pass
    else:
        assert False



def test_point_intersection():
    img = Image("simplecv")
    e = img.edges(0,100)
    for x in range(25,225,25):
        a = (x,25)
        b = (125,125)
        pts = img.edgeIntersections(a,b,width=1)
        e.drawLine(a,b,color=Color.RED)
        e.drawCircle(pts[0],10,color=Color.GREEN)

    for x in range(25,225,25):
        a = (25,x)
        b = (125,125)
        pts = img.edgeIntersections(a,b,width=1)
        e.drawLine(a,b,color=Color.RED)
        e.drawCircle(pts[0],10,color=Color.GREEN)

    for x in range(25,225,25):
        a = (x,225)
        b = (125,125)
        pts = img.edgeIntersections(a,b,width=1)
        e.drawLine(a,b,color=Color.RED)
        e.drawCircle(pts[0],10,color=Color.GREEN)

    for x in range(25,225,25):
        a = (225,x)
        b = (125,125)
        pts = img.edgeIntersections(a,b,width=1)
        e.drawLine(a,b,color=Color.RED)
        e.drawCircle(pts[0],10,color=Color.GREEN)

    results = [e]
    name_stem = "test_point_intersection"
    perform_diff(results,name_stem,tolerance=6.0)

def test_findSkintoneBlobs():
    img = Image('../sampleimages/04000.jpg')

    blobs = img.findSkintoneBlobs()
    for b in blobs:
        if(b.mArea > 0):
            pass
        if(b.perimeter() > 0):
            pass
        if(b.mAvgColor[0] > 5 and b.mAvgColor[1]>140 and b.mAvgColor[1]<180 and b.mAvgColor[2]>77 and b.mAvgColor[2]<135):
            pass


def test_getSkintoneMask():
    imgSet = []
    imgSet.append(Image('../sampleimages/040000.jpg'))
    imgSet.append(Image('../sampleimages/040001.jpg'))
    imgSet.append(Image('../sampleimages/040002.jpg'))
    imgSet.append(Image('../sampleimages/040003.jpg'))
    imgSet.append(Image('../sampleimages/040004.jpg'))
    imgSet.append(Image('../sampleimages/040005.jpg'))
    imgSet.append(Image('../sampleimages/040006.jpg'))
    imgSet.append(Image('../sampleimages/040007.jpg'))
    masks = [img.getSkintoneMask() for img in imgSet]
    VISUAL_TEST = True
    name_stem = 'test_skintone'
    perform_diff(masks,name_stem,tolerance=17)

def test_findKeypoints_all():
    try:
        import cv2
    except:
        pass
        return
    img = Image(testimage2)
    methods = ["ORB", "SIFT", "SURF","FAST", "STAR", "MSER", "Dense"]
    for i in methods :
        print i
        try:
            kp = img.findKeypoints(flavor = i)
        except:
            continue
        if kp!=None :
            for k in kp:
                k.getObject()
                k.descriptor()
                k.quality()
                k.octave()
                k.flavor()
                k.angle()
                k.coordinates()
                k.draw()
                k.distanceFrom()
                k.meanColor()
                k.area()
                k.perimeter()
                k.width()
                k.height()
                k.radius()
                k.crop()
            kp.draw()
        results = [img]
        name_stem = "test_findKeypoints"
        #~ perform_diff(results,name_stem,tolerance=8)
    pass


def test_upload_flickr():
    try:
        import flickrapi
    except:
        if( SHOW_WARNING_TESTS ):
            logger.warning("Couldn't run the upload test as optional flickr library required")
        pass
    else:
        img = Image('simplecv')
        api_key = None
        api_secret = None
        if api_key==None or api_secret==None :
            pass
        else :
            try:
                ret=img.upload('flickr',api_key,api_secret)
                if ret :
                    pass
                else :
                    assert False
            except: # we will chock this up to key errors
                pass

def test_image_new_crop():
    img = Image(logo)
    x = 5
    y = 6
    w = 10
    h = 20
    crop = img.crop((x,y,w,h))
    crop1 = img.crop([x,y,w,h])
    crop2 = img.crop((x,y),(x+w,y+h))
    crop3 = img.crop([(x,y),(x+w,y+h)])
    if( SHOW_WARNING_TESTS ):
        crop7 = img.crop((0,0,-10,10))
        crop8 = img.crop((-50,-50),(10,10))
        crop3 = img.crop([(-3,-3),(10,20)])
        crop4 = img.crop((-10,10,20,20),centered=True)
        crop5 = img.crop([-10,-10,20,20])

    results = [crop,crop1,crop2,crop3]
    name_stem = "test_image_new_crop"
    perform_diff(results,name_stem)

    diff = crop-crop1;
    c=diff.meanColor()
    if( c[0] > 0 or c[1] > 0 or c[2] > 0 ):
        assert False

def test_image_temp_save():
    img1 = Image("lenna")
    img2 = Image(logo)
    path = []
    path.append(img1.save(temp=True))
    path.append(img2.save(temp=True))
    for i in path :
        if i==None :
            assert False

    assert True



def test_image_set_average():
    iset = ImageSet()
    iset.append(Image("./../sampleimages/tracktest0.jpg"))
    iset.append(Image("./../sampleimages/tracktest1.jpg"))
    iset.append(Image("./../sampleimages/tracktest2.jpg"))
    iset.append(Image("./../sampleimages/tracktest3.jpg"))
    iset.append(Image("./../sampleimages/tracktest4.jpg"))
    iset.append(Image("./../sampleimages/tracktest5.jpg"))
    iset.append(Image("./../sampleimages/tracktest6.jpg"))
    iset.append(Image("./../sampleimages/tracktest7.jpg"))
    iset.append(Image("./../sampleimages/tracktest8.jpg"))
    iset.append(Image("./../sampleimages/tracktest9.jpg"))
    avg = iset.average()
    result = [avg]
    name_stem = "test_image_set_average"
    perform_diff(result,name_stem)



def test_save_to_gif():
    imgs = ImageSet()
    imgs.append(Image('../sampleimages/tracktest0.jpg'))
    imgs.append(Image('../sampleimages/tracktest1.jpg'))
    imgs.append(Image('../sampleimages/tracktest2.jpg'))
    imgs.append(Image('../sampleimages/tracktest3.jpg'))
    imgs.append(Image('../sampleimages/tracktest4.jpg'))
    imgs.append(Image('../sampleimages/tracktest5.jpg'))
    imgs.append(Image('../sampleimages/tracktest6.jpg'))
    imgs.append(Image('../sampleimages/tracktest7.jpg'))
    imgs.append(Image('../sampleimages/tracktest8.jpg'))
    imgs.append(Image('../sampleimages/tracktest9.jpg'))

    filename = "test_save_to_gif.gif"
    saved = imgs.save(filename)

    os.remove(filename)

    assert saved == len(imgs)


def sliceinImageSet():
    imgset = ImageSet("../sampleimages/")
    imgset = imgset[8::-2]
    if isinstance(imgset,ImageSet):
        assert True
    else :
        assert False


def test_upload_dropbox():
    try:
        import dropbox
    except:
        if( SHOW_WARNING_TESTS ):
            logger.warning("Couldn't run the upload test as optional dropbox library required")
        pass
    else:
        img = Image('simplecv')
        api_key = ''
        api_secret = ''
        if api_key==None or api_secret==None :
            pass
        else :
            ret=img.upload('dropbox',api_key,api_secret)
            if ret :
                pass
            else :
                assert False

def test_builtin_rotations():
    img = Image('lenna')
    r1 = img - img.rotate180().rotate180()
    r2 = img - img.rotate90().rotate90().rotate90().rotate90()
    r3 = img - img.rotateLeft().rotateLeft().rotateLeft().rotateLeft()
    r4 = img - img.rotateRight().rotateRight().rotateRight().rotateRight()
    r5 = img - img.rotate270().rotate270().rotate270().rotate270()
    if( r1.meanColor() == Color.BLACK and
        r2.meanColor() == Color.BLACK and
        r3.meanColor() == Color.BLACK and
        r4.meanColor() == Color.BLACK and
        r5.meanColor() == Color.BLACK ):
        pass
    else:
        assert False

def test_histograms():
    img = Image('lenna')
    img.verticalHistogram()
    img.horizontalHistogram()

    img.verticalHistogram(bins=3)
    img.horizontalHistogram(bins=3)

    img.verticalHistogram(threshold=10)
    img.horizontalHistogram(threshold=255)

    img.verticalHistogram(normalize=True)
    img.horizontalHistogram(normalize=True)

    img.verticalHistogram(forPlot=True,normalize=True)
    img.horizontalHistogram(forPlot=True,normalize=True)

    pass

def test_blob_full_masks():
    img = Image('lenna')
    b = img.findBlobs()
    m1 = b[-1].getFullMaskedImage()
    m2 = b[-1].getFullHullMaskedImage()
    m3 = b[-1].getFullMask()
    m4 = b[-1].getFullHullMask()
    if(  m1.width == img.width and
         m2.width == img.width and
         m3.width == img.width and
         m4.width == img.width and
         m1.height == img.height and
         m2.height == img.height and
         m3.height == img.height and
         m4.height == img.height ):
        pass
    else:
        assert False


def test_blob_edge_images():
    img = Image('lenna')
    b = img.findBlobs()
    m1 = b[-1].getEdgeImage()
    m2 = b[-1].getHullEdgeImage()
    m3 = b[-1].getFullEdgeImage()
    m4 = b[-1].getFullHullEdgeImage()
    pass

def test_LineScan():
    def lsstuff(ls):
        def aLine(x,m,b):
            return m*x+b
        ls2 = ls.smooth(degree=4).normalize().scale(value_range=[-1,1]).derivative().resample(100).convolve([.25,0.25,0.25,0.25])
        ls2.minima()
        ls2.maxima()
        ls2.localMinima()
        ls2.localMaxima()
        fft,f = ls2.fft()
        ls3 = ls2.ifft(fft)
        ls4 = ls3.fitToModel(aLine)
        ls4.getModelParameters(aLine)
    img = Image("lenna")
    ls = img.getLineScan(x=128,channel=1)
    lsstuff(ls)
    ls = img.getLineScan(y=128)
    lsstuff(ls)
    ls = img.getLineScan(pt1 = (0,0), pt2=(128,128),channel=2)
    lsstuff(ls)
    pass

def test_uncrop():
    img = Image('lenna')
    croppedImg = img.crop(10,20,250,500)
    sourcePts = croppedImg.uncrop([(2,3),(56,23),(24,87)])
    if sourcePts:
        pass

def test_grid():
    img = Image("simplecv")
    img1 = img.grid((10,10),(0,255,0),1)
    img2 = img.grid((20,20),(255,0,255),1)
    img3 = img.grid((20,20),(255,0,255),2)
    result = [img1,img2,img3]
    name_stem = "test_image_grid"
    perform_diff(result,name_stem,12.0)

def test_removeGrid():
    img = Image("lenna")
    gridImage = img.grid()
    dlayer = gridImage.removeGrid()
    if dlayer is None:
        assert False
    dlayer1 = gridImage.removeGrid()
    if dlayer1 is not None:
        assert False
    pass

def test_cluster():
    img = Image("lenna")
    blobs = img.findBlobs()
    clusters1 = blobs.cluster(method="kmeans",k=5,properties=["color"])
    clusters2 = blobs.cluster(method="hierarchical")
    if clusters1 and clusters2:
        pass

def test_line_parallel():
    img = Image("lenna")
    l1 = Line(img, ((100,200), (300,400)))
    l2 = Line(img, ((200,300), (400,500)))
    if l1.isParallel(l2):
        pass
    else:
        assert False

def test_line_perp():
    img = Image("lenna")
    l1 = Line(img, ((100,200), (100,400)))
    l2 = Line(img, ((200,300), (400,300)))
    if l1.isPerpendicular(l2):
        pass
    else:
        assert False

def test_line_imgIntersection():
    img = Image((512, 512))
    for x in range(200, 400):
        img[x, 200] = (255.0, 255.0, 255.0)
    l = Line(img, ((300, 100),(300, 500)))
    if l.imgIntersections(img) == [(300, 200)]:
        pass
    else:
        assert False

def test_line_cropToEdges():
    img = Image((512, 512))
    l = Line(img, ((-10, -5), (400, 400)))
    l_cr = l.cropToImageEdges()
    if l_cr.end_points == ((0, 5), (400, 400)):
        pass
    else:
        assert False

def test_line_extendToEdges():
    img = Image((512, 512))
    l = Line(img, ((10, 10), (30, 30)))
    l_ext = l.extendToImageEdges()
    if l_ext.end_points == [(0, 0), (511, 511)]:
        pass
    else:
        assert False

def test_findGridLines():
    img = Image("simplecv")
    img = img.grid((10,10),(0,255,255))
    lines = img.findGridLines()
    lines.draw()
    result = [img]
    name_stem = "test_image_gridLines"
    perform_diff(result,name_stem,5)

    if(lines == 0 or lines == None):
        assert False

def test_logicalAND():
    img = Image("lenna")
    img1 = img.logicalAND(img.invert())
    if not img1.getNumpy().all():
        pass
    else:
        assert False

def test_logicalOR():
    img = Image("lenna")
    img1 = img.logicalOR(img.invert())
    if img1.getNumpy().all():
        pass
    else:
        assert False

def test_logicalNAND():
    img = Image("lenna")
    img1 = img.logicalNAND(img.invert())
    if img1.getNumpy().all():
        pass
    else:
        assert False

def test_logicalXOR():
    img = Image("lenna")
    img1 = img.logicalXOR(img.invert())
    if img1.getNumpy().all():
        pass
    else:
        assert False

def test_matchSIFTKeyPoints():
    try:
        import cv2
    except ImportError:
        pass
        return
    if not "2.4.3" in cv2.__version__:
        pass
        return
    img = Image("lenna")
    skp, tkp =  img.matchSIFTKeyPoints(img)
    if len(skp) == len(tkp):
        for i in range(len(skp)):
            if (skp[i].x == tkp[i].x and skp[i].y == tkp[i].y):
                pass
            else:
                assert False
    else:
        assert False

def test_findFeatures():
    img = Image('../sampleimages/mtest.png')
    h_features = img.findFeatures("harris", threshold=500)
    s_features = img.findFeatures("szeliski", threshold=500)
    if h_features and s_features:
        pass
    else:
        assert False

def test_ColorMap():
    img = Image('../sampleimages/mtest.png')
    blobs = img.findBlobs()
    cm = ColorMap((Color.RED,Color.YELLOW,Color.BLUE),min(blobs.area()),max(blobs.area()))
    for b in blobs:
        b.draw(cm[b.area()])
    result = [img]
    name_stem = "test_color_map"
    perform_diff(result,name_stem,1.0)


def test_Steganograpy():
    img = Image(logo)
    msg = 'How do I SimpleCV?'
    img.stegaEncode(msg)
    img.save(logo)
    img2 = Image(logo)
    msg2 = img2.stegaDecode()
    pass

def test_watershed():
    img = Image('../sampleimages/wshed.jpg')
    img1 = img.watershed()
    img2 = img.watershed(dilate=3,erode=2)
    img3 = img.watershed(mask=img.threshold(128),erode=1,dilate=1)
    myMask = Image((img.width,img.height))
    myMask = myMask.floodFill((0,0),color=Color.WATERSHED_BG)
    mask = img.threshold(128)
    myMask = (myMask-mask.dilate(2)+mask.erode(2))
    img4 = img.watershed(mask=myMask,useMyMask=True)
    blobs = img.findBlobsFromWatershed(dilate=3,erode=2)
    blobs = img.findBlobsFromWatershed()
    blobs = img.findBlobsFromWatershed(mask=img.threshold(128),erode=1,dilate=1)
    blobs = img.findBlobsFromWatershed(mask=img.threshold(128),erode=1,dilate=1,invert=True)
    blobs = img.findBlobsFromWatershed(mask=myMask,useMyMask=True)
    result = [img1,img2,img3,img4]
    name_stem = "test_watershed"
    perform_diff(result,name_stem,3.0)

def test_minmax():
    img = Image('../sampleimages/wshed.jpg')
    min = img.minValue()
    min,pts = img.minValue(locations=True)
    max = img.maxValue()
    max,pts = img.maxValue(locations=True)
    pass

def testROIFeature():
    img = Image(testimageclr)
    mask = img.threshold(248).dilate(5)
    blobs = img.findBlobsFromMask(mask,minsize=1)
    x,y = np.where(mask.getGrayNumpy()>0)
    xmin = np.min(x)
    xmax = np.max(x)
    ymin = np.min(y)
    ymax = np.max(y)
    w = xmax-xmin
    h = ymax-ymin
    roiList = []

    def subtest(data,effect):
        broke = False
        first = effect(data[0])
        i = 0
        for d in data:
            e = effect(d)
            print (i,e)
            i = i + 1
            if( first != e ):
                broke = True
        return broke

    broi = ROI(blobs)
    broi2 = ROI(blobs,image=img)

    roiList.append(ROI(x=x,y=y,image=img))
    roiList.append(ROI(x=list(x),y=list(y),image=img))
    roiList.append(ROI(x=tuple(x),y=tuple(y),image=img))
    roiList.append(ROI(zip(x,y),image=img))
    roiList.append(ROI((xmin,ymin),(xmax,ymax),image=img))
    roiList.append(ROI(xmin,ymin,w,h,image=img))
    roiList.append(ROI([(xmin,ymin),(xmax,ymin),(xmax,ymax),(xmin,ymax)],image=img))
    roiList.append(ROI(roiList[0]))

    # test the basics
    def toXYWH( roi ):
        return roi.toXYWH()

    if( subtest(roiList,toXYWH) ):
        assert False
    broi.translate(10,10)
    broi.translate(-10)
    broi.translate(y=-10)
    broi.toTLAndBR()
    broi.toPoints()
    broi.toUnitXYWH()
    broi.toUnitTLAndBR()
    broi.toUnitPoints()
    roiList[0].crop()
    newROI=ROI(zip(x,y),image=mask)
    test = newROI.crop()
    xroi,yroi = np.where(test.getGrayNumpy()>128)
    roiPts = zip(xroi,yroi)
    realPts = newROI.CoordTransformPts(roiPts)
    unitROI = newROI.CoordTransformPts(roiPts,output="ROI_UNIT")
    unitSRC = newROI.CoordTransformPts(roiPts,output="SRC_UNIT")
    src1 = newROI.CoordTransformPts(roiPts,intype="SRC_UNIT",output='SRC')
    src2 = newROI.CoordTransformPts(roiPts,intype="ROI_UNIT",output='SRC')
    src3 = newROI.CoordTransformPts(roiPts,intype="SRC_UNIT",output='ROI')
    src4 = newROI.CoordTransformPts(roiPts,intype="ROI_UNIT",output='ROI')
    fs = newROI.splitX(10)
    fs = newROI.splitX(.5,unitVals=True)
    for f in fs:
        f.draw(color=Color.BLUE)
    fs = newROI.splitX(newROI.xtl+10,srcVals=True)
    xs = newROI.xtl
    fs = newROI.splitX([10,20])
    fs = newROI.splitX([xs+10,xs+20,xs+30],srcVals=True)
    fs = newROI.splitX([0.3,0.6,0.9],unitVals=True)
    fs = newROI.splitY(10)
    fs = newROI.splitY(.5,unitVals=True)
    for f in fs:
        f.draw(color=Color.BLUE)
    fs = newROI.splitY(newROI.ytl+30,srcVals=True)
    testROI = ROI(blobs[0],mask)
    for b in blobs[1:]:
        testROI.merge(b)

def test_findKeypointClusters():
    img = Image('simplecv')
    kpc = img.findKeypointClusters()
    if len(kpc) <= 0:
      assert False
    else:
      pass

def test_replaceLineScan():
    img = Image("lenna")
    ls = img.getLineScan(x=100)
    ls[50] = 0
    newimg = img.replaceLineScan(ls)
    if newimg[100, 50][0] == 0:
        pass
    else:
        assert False
    ls = img.getLineScan(x=100,channel=1)
    ls[50] = 0
    newImg = img.replaceLineScan(ls)
    if newImg[100,50][1] == 0:
        pass
    else:
        assert False

def test_runningAverage():
    img = Image('lenna')
    ls = img.getLineScan(y=120)
    ra=ls.runningAverage(5)
    if ra[50] == sum(ls[48:53])/5:
        pass
    else:
        assert False

def lineScan_perform_diff(oLineScan, pLineScan, func, **kwargs):
    nLineScan = func(oLineScan, **kwargs)
    diff = sum([(i - j) for i, j in zip(pLineScan, nLineScan)])
    if diff > 10 or diff < -10:
        return False
    return True

def test_linescan_smooth():
    img = Image("lenna")
    l1 = img.getLineScan(x=60)
    l2 = l1.smooth(degree=7)
    if lineScan_perform_diff(l1, l2, LineScan.smooth, degree=7):
        pass
    else:
        assert False

def test_linescan_normalize():
    img = Image("lenna")
    l1 = img.getLineScan(x=90)
    l2 = l1.normalize()
    if lineScan_perform_diff(l1, l2, LineScan.normalize):
        pass
    else:
        assert False

def test_linescan_scale():
    img = Image("lenna")
    l1 = img.getLineScan(y=90)
    l2 = l1.scale()
    if lineScan_perform_diff(l1, l2, LineScan.scale):
        pass
    else:
        assert False

def test_linescan_derivative():
    img = Image("lenna")
    l1 = img.getLineScan(y=140)
    l2 = l1.derivative()
    if lineScan_perform_diff(l1, l2, LineScan.derivative):
        pass
    else:
        assert False

def test_linescan_resample():
    img = Image("lenna")
    l1 = img.getLineScan(pt1=(300, 300), pt2=(450, 500))
    l2 = l1.resample(n=50)
    if lineScan_perform_diff(l1, l2, LineScan.resample, n=50):
        pass
    else:
        assert False

def test_linescan_fitToModel():
    def aLine(x, m, b):
        return x*m+b
    img = Image("lenna")
    l1 = img.getLineScan(y=200)
    l2 = l1.fitToModel(aLine)
    if lineScan_perform_diff(l1, l2, LineScan.fitToModel, f=aLine):
        pass
    else:
        assert False

def test_linescan_convolve():
    kernel = [0, 2, 0, 4, 0, 2, 0]
    img = Image("lenna")
    l1 = img.getLineScan(x=400)
    l2 = l1.convolve(kernel)
    if lineScan_perform_diff(l1, l2, LineScan.convolve, kernel=kernel):
        pass
    else:
        assert False

def test_linescan_threshold():
    img = Image("lenna")
    l1 = img.getLineScan(x=350)
    l2 = l1.threshold(threshold=200, invert=True)
    if lineScan_perform_diff(l1, l2, LineScan.threshold, threshold=200, invert=True):
        pass
    else:
        assert False

def test_linescan_invert():
    img = Image("lenna")
    l1 = img.getLineScan(y=200)
    l2 = l1.invert(max=40)
    if lineScan_perform_diff(l1, l2, LineScan.invert, max=40):
        pass
    else:
        assert False

def test_linescan_median():
    img = Image("lenna")
    l1 = img.getLineScan(x=120)
    l2 = l1.median(sz=9)
    if lineScan_perform_diff(l1, l2, LineScan.median, sz=9):
        pass
    else:
        assert False

def test_linescan_medianFilter():
    img = Image("lenna")
    l1 = img.getLineScan(y=250)
    l2 = l1.medianFilter(kernel_size=7)
    if lineScan_perform_diff(l1, l2, LineScan.medianFilter, kernel_size=7):
        pass
    else:
        assert False

def test_linescan_detrend():
    img = Image("lenna")
    l1 = img.getLineScan(y=90)
    l2 = l1.detrend()
    if lineScan_perform_diff(l1, l2, LineScan.detrend):
        pass
    else:
        assert False

def test_getFREAKDescriptor():
    try:
        import cv2
    except ImportError:
        pass
    if '$Rev' in cv2.__version__:
        pass
    else:
        if int(cv2.__version__.replace('.','0'))>=20402:
            img = Image("lenna")
            flavors = ["SIFT", "SURF", "BRISK", "ORB", "STAR", "MSER", "FAST", "Dense"]
            for flavor in flavors:
                f, d = img.getFREAKDescriptor(flavor)
                if len(f) == 0:
                    assert False
                if d.shape[0] != len(f) and d.shape[1] != 64:
                    assert False
        else:
            pass
    pass

def test_grayPeaks():
    i = Image('lenna')
    peaks = i.grayPeaks()
    if peaks == None:
        assert False
    else:
        pass

def test_findPeaks():
    img = Image('lenna')
    ls = img.getLineScan(x=150)
    peaks = ls.findPeaks()
    if peaks == None:
        assert False
    else:
        pass

def test_LineScan_sub():
    img = Image('lenna')
    ls = img.getLineScan(x=200)
    ls1 = ls - ls
    if ls1[23] == 0:
        pass
    else:
        assert False

def test_LineScan_add():
    img = Image('lenna')
    ls = img.getLineScan(x=20)
    l = ls + ls
    a = int(ls[20]) + int(ls[20])
    if a == l[20]:
        pass
    else:
        assert False

def test_LineScan_mul():
    img = Image('lenna')
    ls = img.getLineScan(x=20)
    l = ls * ls
    a = int(ls[20]) * int(ls[20])
    if a == l[20]:
        pass
    else:
        assert False

def test_LineScan_div():
    img = Image('lenna')
    ls = img.getLineScan(x=20)
    l = ls / ls
    a = int(ls[20]) / int(ls[20])
    if a == l[20]:
        pass
    else:
        assert False

def test_tvDenoising():
    return # this is way too slow.
    try:
        from skimage.filter import denoise_tv_chambolle
        img = Image('lenna')
        img1 = img.tvDenoising(gray=False,weight=20)
        img2 = img.tvDenoising(weight=50,max_iter=250)
        img3 = img.toGray()
        img3 = img3.tvDenoising(gray=True,weight=20)
        img4 = img.tvDenoising(resize=0.5)
        result = [img1,img2,img3,img4]
        name_stem = "test_tvDenoising"
        perform_diff(result,name_stem,3)
    except ImportError:
        pass

def test_motionBlur():
    i = Image('lenna')
    d = ('n', 's', 'e', 'w', 'ne', 'nw', 'se', 'sw')
    i0 = i.motionBlur(intensity = 20, direction = d[0])
    i1 = i.motionBlur(intensity = 20, direction = d[1])
    i2 = i.motionBlur(intensity = 20, direction = d[2])
    i3 = i.motionBlur(intensity = 20, direction = d[3])
    i4 = i.motionBlur(intensity = 10, direction = d[4])
    i5 = i.motionBlur(intensity = 10, direction = d[5])
    i6 = i.motionBlur(intensity = 10, direction = d[6])
    i7 = i.motionBlur(intensity = 10, direction = d[7])
    a = i.motionBlur(intensity = 0)
    c = 0
    img = (i0, i1, i2, i3, i4, i5, i6, i7)
    for im in img:
        if im is not i:
            c += 1

    if c == 8 and a is i:
        pass
    else:
        assert False

def test_faceRecognize():
    try:
        import cv2
        if hasattr(cv2, "createFisherFaceRecognizer"):
            f = FaceRecognizer()
            images1 = ["../sampleimages/ff1.jpg",
                       "../sampleimages/ff2.jpg",
                       "../sampleimages/ff3.jpg",
                       "../sampleimages/ff4.jpg",
                        "../sampleimages/ff5.jpg"]

            images2 = ["../sampleimages/fm1.jpg",
                       "../sampleimages/fm2.jpg",
                       "../sampleimages/fm3.jpg",
                       "../sampleimages/fm4.jpg",
                       "../sampleimages/fm5.jpg"]

            images3 = ["../sampleimages/fi1.jpg",
                       "../sampleimages/fi2.jpg",
                       "../sampleimages/fi3.jpg",
                       "../sampleimages/fi4.jpg"]

            imgset1 = []
            imgset2 = []
            imgset3 = []

            for img in images1:
                imgset1.append(Image(img))
            label1 = ["female"]*len(imgset1)

            for img in images2:
                imgset2.append(Image(img))
            label2 = ["male"]*len(imgset2)

            imgset = imgset1 + imgset2
            labels = label1 + label2
            imgset[4] = imgset[4].resize(400,400)
            f.train(imgset, labels)

            for img in images3:
                imgset3.append(Image(img))
            imgset[2].resize(300, 300)
            label = []
            for img in imgset3:
                name, confidence = f.predict(img)
                label.append(name)

            if label == ["male", "male", "female", "female"]:
                pass
            else:
                assert False
        else:
            pass
    except ImportError:
        pass

def test_channelMixer():
    i = Image('lenna')
    r = i.channelMixer()
    g = i.channelMixer(channel='g', weight = (100,20,30))
    b = i.channelMixer(channel='b', weight = (30,200,10))
    if i != r and i != g and i != b:
        pass
    else:
        assert False

def test_prewitt():
    i = Image('lenna')
    p = i.prewitt()
    if i != p :
        pass
    else:
        assert False

def test_edgeSnap():
    img = Image('shapes.png',sample=True).edges()

    list1 = [(129,32),(19,88),(124,135)]
    list2 = [(484,294),(297,437)]
    list3 = [(158,357),(339,82)]

    for list_ in list1,list2,list3:
        edgeLines = img.edgeSnap(list_)
        edgeLines.draw(color = Color.YELLOW,width = 4)

    name_stem = "test_edgeSnap"
    result = [img]
    perform_diff(result,name_stem,0.7)

def test_motionBlur():
    image = Image('lenna')
    d = (-70, -45, -30, -10, 100, 150, 235, 420)
    p = ( 10,20,30,40,50,60,70,80)
    img = []

    a = image.motionBlur(0)
    for i in range(8):
        img += [image.motionBlur(p[i],d[i])]
    c = 0
    for im in img:
        if im is not i:
            c += 1

    if c == 8 and a is image:
        pass
    else:
        assert False

def test_grayscalmatrix():
    img = Image("lenna")
    graymat = img.getGrayscaleMatrix()
    newimg = Image(graymat, colorSpace=ColorSpace.GRAY)
    from numpy import array_equal
    if not array_equal(img.getGrayNumpy(), newimg.getGrayNumpy()):
        assert False
    pass

def test_getLightness():
    img = Image('lenna')
    i = img.getLightness()
    if int(i[27,42][0]) == int((max(img[27,42])+min(img[27,42]))/2):
        pass
    else:
        assert False

def test_getLuminosity():
    img = Image('lenna')
    i = img.getLuminosity()
    a = np.array(img[27,42],dtype=np.int)
    if int(i[27,42][0]) == int(np.average(a,0,(0.21,0.71,0.07))):
        pass
    else:
        assert False

def test_getAverage():
    img = Image('lenna')
    i = img.getAverage()
    if int(i[0,0][0]) == int((img[0,0][0]+img[0,0][1]+img[0,0][2])/3):
        pass
    else:
        assert False

def test_smartRotate():
    import time
    img = Image('kptest2.png',sample = True)

    st1 = img.smartRotate(auto = False,fixed = False).resize(500,500)
    st2 = img.rotate(27,fixed = False).resize(500,500)
    diff = np.average((st1-st2).getNumpy())
    if (diff > 1.7):
        print diff
        assert False
    else:
        assert True

def test_normalize():
    img = Image("lenna")
    img1 = img.normalize()
    img2 = img.normalize(minCut = 0,maxCut = 0)
    result = [img1,img2]
    name_stem = "test_image_normalize"
    perform_diff(result,name_stem,5)
    pass

def test_getNormalizedHueHistogram():
    img = Image('lenna')
    a = img.getNormalizedHueHistogram((0,0,100,100))
    b = img.getNormalizedHueHistogram()
    blobs = img.findBlobs()
    c = img.getNormalizedHueHistogram(blobs[-1])
    if( a.shape == (180,256) and b.shape == (180,256)
        and c.shape == (180,256) ):
        pass
    else:
        assert False

def test_backProjecHueHistogram():
    img = Image('lenna')
    img2 = Image('lyle')
    a = img2.getNormalizedHueHistogram()
    imgA = img.backProjectHueHistogram(a)
    imgB = img.backProjectHueHistogram((10,10,50,50),smooth=False,fullColor=True)
    imgC = img.backProjectHueHistogram(img2,threshold=1)
    result = [imgA,imgB,imgC]
    name_stem = "test_image_histBackProj"
    perform_diff(result,name_stem,5)

def test_findBlobsFromHueHistogram():
    img = Image('lenna')
    img2 = Image('lyle')
    a = img2.getNormalizedHueHistogram()
    A = img.findBlobsFromHueHistogram(a)
    B = img.findBlobsFromHueHistogram((10,10,50,50),smooth=False)
    C = img.findBlobsFromHueHistogram(img2,threshold=1)
    pass

def test_drawingLayerToSVG():
    img = Image('lenna')
    dl = img.dl()
    dl.line((0, 0), (100, 100))
    svg = dl.getSVG()
    if svg == '<svg baseProfile="full" height="512" version="1.1" width="512" xmlns="http://www.w3.org/2000/svg" xmlns:ev="http://www.w3.org/2001/xml-events" xmlns:xlink="http://www.w3.org/1999/xlink"><defs /><line x1="0" x2="100" y1="0" y2="100" /></svg>':
        pass
    else:
        assert False

########NEW FILE########
__FILENAME__ = test_cameras
#!/usr/bin/python

import os, sys
from SimpleCV import *
from nose.tools import with_setup


testoutput = "sampleimages/cam.jpg"


def test_virtual_camera_constructor():
    mycam = VirtualCamera(testoutput, 'image')

    props = mycam.getAllProperties()

    for i in props.keys():
        print str(i) + ": " + str(props[i]) + "\n"


    pass

def test_camera_image():
    mycam = Camera(0)

    img = mycam.getImage()
    img.save(testoutput)
    pass

def test_camera_multiple_instances():
    cam1 = Camera()
    img1 = cam1.getImage()
    cam2 = Camera()
    img2 = cam2.getImage()

    if not cam1 or not cam2 or not img1 or not img2:
        assert False

    cam3 = Camera(0) # assuming the default camera index is 0
    img3 = cam3.getImage()

    if not cam3 or not img3:
        assert False
    pass

########NEW FILE########
__FILENAME__ = test_display
# /usr/bin/python
# To run this test you need python nose tools installed
# Run test just use:
#   nosetest test_display.py
#

import os, sys, pickle
from SimpleCV import *
from nose.tools import with_setup, nottest

VISUAL_TEST = True  # if TRUE we save the images - otherwise we DIFF against them - the default is False
SHOW_WARNING_TESTS = False  # show that warnings are working - tests will pass but warnings are generated.

#colors
black = Color.BLACK
white = Color.WHITE
red = Color.RED
green = Color.GREEN
blue = Color.BLUE

###############
# TODO -
# Examples of how to do profiling
# Examples of how to do a single test -
# UPDATE THE VISUAL TESTS WITH EXAMPLES.
# Fix exif data
# Turn off test warnings using decorators.
# Write a use the tests doc.

#images
barcode = "../sampleimages/barcode.png"
testimage = "../sampleimages/9dots4lines.png"
testimage2 = "../sampleimages/aerospace.jpg"
whiteimage = "../sampleimages/white.png"
blackimage = "../sampleimages/black.png"
testimageclr = "../sampleimages/statue_liberty.jpg"
testbarcode = "../sampleimages/barcode.png"
testoutput = "../sampleimages/9d4l.jpg"
tmpimg = "../sampleimages/tmpimg.jpg"
greyscaleimage = "../sampleimages/greyscale.jpg"
logo = "../sampleimages/simplecv.png"
logo_inverted = "../sampleimages/simplecv_inverted.png"
ocrimage = "../sampleimages/ocr-test.png"
circles = "../sampleimages/circles.png"
webp = "../sampleimages/simplecv.webp"

#alpha masking images
topImg = "../sampleimages/RatTop.png"
bottomImg = "../sampleimages/RatBottom.png"
maskImg = "../sampleimages/RatMask.png"
alphaMaskImg = "../sampleimages/RatAlphaMask.png"
alphaSrcImg = "../sampleimages/GreenMaskSource.png"

#standards path
standard_path = "./standard/"

#track images
trackimgs = ["../sampleimages/tracktest0.jpg",
        "../sampleimages/tracktest1.jpg",
        "../sampleimages/tracktest2.jpg",
        "../sampleimages/tracktest3.jpg",
        "../sampleimages/tracktest4.jpg",
        "../sampleimages/tracktest5.jpg",
        "../sampleimages/tracktest6.jpg",
        "../sampleimages/tracktest7.jpg",
        "../sampleimages/tracktest8.jpg",
        "../sampleimages/tracktest9.jpg",]

#Given a set of images, a path, and a tolerance do the image diff.
def imgDiffs(test_imgs,name_stem,tolerance,path):
    count = len(test_imgs)
    for idx in range(0,count):
        lhs = test_imgs[idx].applyLayers() # this catches drawing methods
        fname = standard_path+name_stem+str(idx)+".jpg"
        rhs = Image(fname)
        if( lhs.width == rhs.width and lhs.height == rhs.height ):
            diff = (lhs-rhs)
            val = np.average(diff.getNumpy())
            if( val > tolerance ):
                print val
                return True
    return False

#Save a list of images to a standard path.
def imgSaves(test_imgs, name_stem, path=standard_path):
    count = len(test_imgs)
    for idx in range(0,count):
        fname = standard_path+name_stem+str(idx)+".jpg"
        test_imgs[idx].save(fname)#,quality=95)

#perform the actual image save and image diffs.
def perform_diff(result,name_stem,tolerance=2.0,path=standard_path):
    if(VISUAL_TEST): # save the correct images for a visual test
        imgSaves(result,name_stem,path)
    else: # otherwise we test our output against the visual test
        if( imgDiffs(result,name_stem,tolerance,path) ):
            assert False
        else:
            pass

#These function names are required by nose test, please leave them as is
def setup_context():
    img = Image(testimage)

def destroy_context():
    img = ""

@with_setup(setup_context, destroy_context)
def test_image_stretch():
    img = Image(greyscaleimage)
    stretched = img.stretch(100,200)
    if(stretched == None):
        assert False

    result = [stretched]
    name_stem = "test_stretch"
    perform_diff(result,name_stem)

def test_image_bitmap():
    img1 = Image("lenna")
    img2 = Image("lenna")
    img2 = img2.smooth()
    result = [img1,img2]
    name_stem = "test_image_bitmap"
    perform_diff(result,name_stem)

# # Image Class Test

def test_image_scale():
    img = Image(testimage)
    thumb = img.scale(30,30)
    if(thumb == None):
        assert False
    result = [thumb]
    name_stem = "test_image_scale"
    perform_diff(result,name_stem)

def test_image_copy():
    img = Image(testimage2)
    copy = img.copy()

    if (img[1,1] != copy[1,1] or img.size() != copy.size()):
        assert False

    result = [copy]
    name_stem = "test_image_copy"
    perform_diff(result,name_stem)
    pass

def test_image_setitem():
    img = Image(testimage)
    img[1,1] = (0, 0, 0)
    newimg = Image(img.getBitmap())
    colors = newimg[1,1]
    if (colors[0] == 0 and colors[1] == 0 and colors[2] == 0):
        pass
    else:
        assert False

    result = [newimg]
    name_stem = "test_image_setitem"
    perform_diff(result,name_stem)


def test_image_setslice():
    img = Image(testimage)
    img[1:10,1:10] = (0,0,0) #make a black box
    newimg = Image(img.getBitmap())
    section = newimg[1:10,1:10]
    for i in range(5):
        colors = section[i,0]
        if (colors[0] != 0 or colors[1] != 0 or colors[2] != 0):
            assert False
    pass
    result = [newimg]
    name_stem = "test_image_setslice"
    perform_diff(result,name_stem)


def test_detection_findCorners():
    img = Image(testimage2)
    corners = img.findCorners(25)
    corners.draw()
    if (len(corners) == 0):
        assert False
    result = [img]
    name_stem = "test_detection_findCorners"
    perform_diff(result,name_stem)

def test_image_smooth():
    img = Image(testimage2)
    result = []
    result.append(img.smooth())
    result.append(img.smooth('bilateral', (3,3), 4, 1))
    result.append(img.smooth('blur', (3, 3)))
    result.append(img.smooth('median', (3, 3)))
    result.append(img.smooth('gaussian', (5,5), 0))
    result.append(img.smooth('bilateral', (3,3), 4, 1,grayscale=False))
    result.append(img.smooth('blur', (3, 3),grayscale=True))
    result.append(img.smooth('median', (3, 3),grayscale=True))
    result.append(img.smooth('gaussian', (5,5), 0,grayscale=True))
    name_stem = "test_image_smooth"
    perform_diff(result,name_stem)
    pass

def test_image_binarize():
    img =  Image(testimage2)
    binary = img.binarize()
    binary2 = img.binarize((60, 100, 200))
    hist = binary.histogram(20)
    hist2 = binary2.histogram(20)

    result = [binary,binary2]
    name_stem = "test_image_binarize"
    perform_diff(result,name_stem)

    if (hist[0] + hist[-1] == np.sum(hist) and hist2[0] + hist2[-1] == np.sum(hist2)):
        pass
    else:
        assert False

def test_image_binarize_adaptive():
    img =  Image(testimage2)
    binary = img.binarize(-1)
    hist = binary.histogram(20)

    result = [binary]
    name_stem = "test_image_binarize_adaptive"
    perform_diff(result,name_stem)

    if (hist[0] + hist[-1] == np.sum(hist)):
        pass
    else:
        assert False

def test_image_invert():
    img = Image(testimage2)
    clr = img[1,1]
    img = img.invert()

    result = [img]
    name_stem = "test_image_invert"
    perform_diff(result,name_stem)

    if (clr[0] == (255 - img[1,1][0])):
        pass
    else:
        assert False

def test_image_drawing():
    img = Image(testimageclr)
    img.drawCircle((img.width/2, img.height/2), 10,thickness=3)
    img.drawCircle((img.width/2, img.height/2), 15,thickness=5,color=Color.RED)
    img.drawCircle((img.width/2, img.height/2), 20)
    img.drawLine((5, 5), (5, 8))
    img.drawLine((5, 5), (10, 10),thickness=3)
    img.drawLine((0, 0), (img.width, img.height),thickness=3,color=Color.BLUE)
    img.drawRectangle(20,20,10,5)
    img.drawRectangle(22,22,10,5,alpha=128)
    img.drawRectangle(24,24,10,15,width=-1,alpha=128)
    img.drawRectangle(28,28,10,15,width=3,alpha=128)
    result = [img]
    name_stem = "test_image_drawing"
    perform_diff(result,name_stem)


def test_image_splitchannels():
    img = Image(testimageclr)
    (r, g, b) = img.splitChannels(True)
    (red, green, blue) = img.splitChannels()
    result = [r,g,b,red,green,blue]
    name_stem = "test_image_splitchannels"
    perform_diff(result,name_stem)
    pass

def test_detection_lines():
    img = Image(testimage2)
    lines = img.findLines()
    lines.draw()
    result = [img]
    name_stem = "test_detection_lines"
    perform_diff(result,name_stem)

    if(lines == 0 or lines == None):
        assert False

def test_detection_blobs_appx():
    img = Image("lenna")
    blobs = img.findBlobs()
    blobs[-1].draw(color=Color.RED)
    blobs[-1].drawAppx(color=Color.BLUE)
    result = [img]

    img2 = Image("lenna")
    blobs = img2.findBlobs(appx_level=11)
    blobs[-1].draw(color=Color.RED)
    blobs[-1].drawAppx(color=Color.BLUE)
    result.append(img2)

    name_stem = "test_detection_blobs_appx"
    perform_diff(result,name_stem,5.00)
    if blobs == None:
        assert False

def test_detection_blobs():
    img = Image(testbarcode)
    blobs = img.findBlobs()
    blobs.draw(color=Color.RED)
    result = [img]
    #TODO - WE NEED BETTER COVERAGE HERE
    name_stem = "test_detection_blobs"
    perform_diff(result,name_stem,5.00)

    if blobs == None:
        assert False

def test_detection_blobs_lazy():

    img = Image("lenna")
    b = img.findBlobs()
    result = []

    s = pickle.dumps(b[-1]) # use two otherwise it w
    b2 =  pickle.loads(s)

    result.append(b[-1].mImg)
    result.append(b[-1].mMask)
    result.append(b[-1].mHullImg)
    result.append(b[-1].mHullMask)

    result.append(b2.mImg)
    result.append(b2.mMask)
    result.append(b2.mHullImg)
    result.append(b2.mHullMask)

    #TODO - WE NEED BETTER COVERAGE HERE
    name_stem = "test_detection_blobs_lazy"
    perform_diff(result,name_stem,6.00)


def test_detection_blobs_adaptive():
    img = Image(testimage)
    blobs = img.findBlobs(-1, threshblocksize=99)
    blobs.draw(color=Color.RED)
    result = [img]
    name_stem = "test_detection_blobs_adaptive"
    perform_diff(result,name_stem,5.00)
    if blobs == None:
        assert False


def test_color_curve_HSL():
    y = np.array([[0,0],[64,128],[192,128],[255,255]])  #These are the weights
    curve = ColorCurve(y)
    img = Image(testimage)
    img2 = img.applyHLSCurve(curve,curve,curve)
    img3 = img-img2

    result = [img2,img3]
    name_stem = "test_color_curve_HLS"
    perform_diff(result,name_stem)

    c = img3.meanColor()
    if( c[0] > 2.0 or c[1] > 2.0 or c[2] > 2.0 ): #there may be a bit of roundoff error
        assert False

def test_color_curve_RGB():
    y = np.array([[0,0],[64,128],[192,128],[255,255]])  #These are the weights
    curve = ColorCurve(y)
    img = Image(testimage)
    img2 = img.applyRGBCurve(curve,curve,curve)
    img3 = img-img2

    result = [img2,img3]
    name_stem = "test_color_curve_RGB"
    perform_diff(result,name_stem)

    c = img3.meanColor()
    if( c[0] > 1.0 or c[1] > 1.0 or c[2] > 1.0 ): #there may be a bit of roundoff error
        assert False

def test_color_curve_GRAY():
    y = np.array([[0,0],[64,128],[192,128],[255,255]])  #These are the weights
    curve = ColorCurve(y)
    img = Image(testimage)
    gray = img.grayscale()
    img2 = img.applyIntensityCurve(curve)

    result = [img2]
    name_stem = "test_color_curve_GRAY"
    perform_diff(result,name_stem)

    g=gray.meanColor()
    i2=img2.meanColor()
    if( g[0]-i2[0] > 1 ): #there may be a bit of roundoff error
        assert False

def test_image_dilate():
    img=Image(barcode)
    img2 = img.dilate(20)

    result = [img2]
    name_stem = "test_image_dilate"
    perform_diff(result,name_stem)
    c=img2.meanColor()

    if( c[0] < 254 or c[1] < 254 or c[2] < 254 ):
        assert False;

def test_image_erode():
    img=Image(barcode)
    img2 = img.erode(100)

    result = [img2]
    name_stem = "test_image_erode"
    perform_diff(result,name_stem)

    c=img2.meanColor()
    print(c)
    if( c[0] > 0 or c[1] > 0 or c[2] > 0 ):
        assert False;

def test_image_morph_open():
    img = Image(barcode);
    erode= img.erode()
    dilate = erode.dilate()
    result = img.morphOpen()
    test = result-dilate
    c=test.meanColor()
    results = [result]
    name_stem = "test_image_morph_open"
    perform_diff(results,name_stem)

    if( c[0] > 1 or c[1] > 1 or c[2] > 1 ):
        assert False;

def test_image_morph_close():
    img = Image(barcode)
    dilate = img.dilate()
    erode = dilate.erode()
    result = img.morphClose()
    test = result-erode
    c=test.meanColor()

    results = [result]
    name_stem = "test_image_morph_close"
    perform_diff(results,name_stem)


    if( c[0] > 1 or c[1] > 1 or c[2] > 1 ):
        assert False;

def test_image_morph_grad():
    img = Image(barcode)
    dilate = img.dilate()
    erode = img.erode()
    dif = dilate-erode
    result = img.morphGradient()
    test = result-dif
    c=test.meanColor()

    results = [result]
    name_stem = "test_image_morph_grad"
    perform_diff(results,name_stem)


    if( c[0] > 1 or c[1] > 1 or c[2] > 1 ):
        assert False

def test_image_rotate_fixed():
    img = Image(testimage2)
    img2=img.rotate(180, scale = 1)
    img3=img.flipVertical()
    img4=img3.flipHorizontal()
    img5 = img.rotate(70)
    img6 = img.rotate(70,scale=0.5)

    results = [img2,img3,img4,img5,img6]
    name_stem = "test_image_rotate_fixed"
    perform_diff(results,name_stem)

    test = img4-img2
    c=test.meanColor()
    print(c)
    if( c[0] > 5 or c[1] > 5 or c[2] > 5 ):
        assert False


def test_image_rotate_full():
    img = Image(testimage2)
    img2=img.rotate(180,"full",scale = 1)

    results = [img2]
    name_stem = "test_image_rotate_full"
    perform_diff(results,name_stem)

    c1=img.meanColor()
    c2=img2.meanColor()
    if( abs(c1[0]-c2[0]) > 5 or abs(c1[1]-c2[1]) > 5 or abs(c1[2]-c2[2]) > 5 ):
        assert False

def test_image_shear_warp():
    img = Image(testimage2)
    dst =  ((img.width/2,0),(img.width-1,img.height/2),(img.width/2,img.height-1))
    s = img.shear(dst)


    color = s[0,0]
    if (color != (0,0,0)):
        assert False

    dst = ((img.width*0.05,img.height*0.03),(img.width*0.9,img.height*0.1),(img.width*0.8,img.height*0.7),(img.width*0.2,img.height*0.9))
    w = img.warp(dst)

    results = [s,w]
    name_stem = "test_image_shear_warp"
    perform_diff(results,name_stem)

    color = s[0,0]
    if (color != (0,0,0)):
        assert False

    pass

def test_image_affine():
    img = Image(testimage2)
    src =  ((0,0),(img.width-1,0),(img.width-1,img.height-1))
    dst =  ((img.width/2,0),(img.width-1,img.height/2),(img.width/2,img.height-1))
    aWarp = cv.CreateMat(2,3,cv.CV_32FC1)
    cv.GetAffineTransform(src,dst,aWarp)
    atrans = img.transformAffine(aWarp)

    aWarp2 = np.array(aWarp)
    atrans2 = img.transformAffine(aWarp2)

    test = atrans-atrans2
    c=test.meanColor()

    results = [atrans,atrans2]

    name_stem = "test_image_affine"
    perform_diff(results,name_stem)

    if( c[0] > 1 or c[1] > 1 or c[2] > 1 ):
        assert False

def test_image_perspective():
    img = Image(testimage2)
    src = ((0,0),(img.width-1,0),(img.width-1,img.height-1),(0,img.height-1))
    dst = ((img.width*0.05,img.height*0.03),(img.width*0.9,img.height*0.1),(img.width*0.8,img.height*0.7),(img.width*0.2,img.height*0.9))
    pWarp = cv.CreateMat(3,3,cv.CV_32FC1)
    cv.GetPerspectiveTransform(src,dst,pWarp)
    ptrans = img.transformPerspective(pWarp)

    pWarp2 = np.array(pWarp)
    ptrans2 = img.transformPerspective(pWarp2)


    test = ptrans-ptrans2
    c=test.meanColor()

    results = [ptrans,ptrans2]
    name_stem = "test_image_perspective"
    perform_diff(results,name_stem)
    if( c[0] > 1 or c[1] > 1 or c[2] > 1 ):
        assert False

def test_camera_undistort():
    
    fakeCamera = FrameSource()
    fakeCamera.loadCalibration("TestCalibration")
    img = Image("../sampleimages/CalibImage0.png")
    img2 = fakeCamera.undistort(img)

    results = [img2]
    name_stem = "test_camera_undistort"
    perform_diff(results,name_stem)

    if( not img2 ): #right now just wait for this to return
        assert False

def test_image_crop():
    img = Image(logo)
    x = 5
    y = 6
    w = 10
    h = 20
    crop = img.crop(x,y,w,h)
    crop2 = img[x:(x+w),y:(y+h)]
    crop6 = img.crop(0,0,10,10)
    if( SHOW_WARNING_TESTS ):
        crop7 = img.crop(0,0,-10,10)
        crop8 = img.crop(-50,-50,10,10)
        crop3 = img.crop(-3,-3,10,20)
        crop4 = img.crop(-10,10,20,20,centered=True)
        crop5 = img.crop(-10,-10,20,20)

    results = [crop,crop2,crop6]
    name_stem = "test_image_crop"
    perform_diff(results,name_stem)

    diff = crop-crop2;
    c=diff.meanColor()
    if( c[0] > 0 or c[1] > 0 or c[2] > 0 ):
        assert False

def test_image_region_select():
    img = Image(logo)
    x1 = 0
    y1 = 0
    x2 = img.width
    y2 = img.height
    crop = img.regionSelect(x1,y1,x2,y2)

    results = [crop]
    name_stem = "test_image_region_select"
    perform_diff(results,name_stem)

    diff = crop-img;
    c=diff.meanColor()
    if( c[0] > 0 or c[1] > 0 or c[2] > 0 ):
        assert False

def test_image_subtract():
    imgA = Image(logo)
    imgB = Image(logo_inverted)
    imgC = imgA - imgB
    results = [imgC]
    name_stem = "test_image_subtract"
    perform_diff(results,name_stem)

def test_image_negative():
    imgA = Image(logo)
    imgB = -imgA
    results = [imgB]
    name_stem = "test_image_negative"
    perform_diff(results,name_stem)


def test_image_divide():
    imgA = Image(logo)
    imgB = Image(logo_inverted)

    imgC = imgA / imgB

    results = [imgC]
    name_stem = "test_image_divide"
    perform_diff(results,name_stem)

def test_image_and():
    imgA = Image(barcode)
    imgB = imgA.invert()


    imgC = imgA & imgB # should be all black

    results = [imgC]
    name_stem = "test_image_and"
    perform_diff(results,name_stem)


def test_image_or():
    imgA = Image(barcode)
    imgB = imgA.invert()

    imgC = imgA | imgB #should be all white

    results = [imgC]
    name_stem = "test_image_or"
    perform_diff(results,name_stem)



def test_color_colormap_build():
    cm = ColorModel()
    #cm.add(Image(logo))
    cm.add((127,127,127))
    if(cm.contains((127,127,127))):
        cm.remove((127,127,127))
    else:
        assert False

    cm.remove((0,0,0))
    cm.remove((255,255,255))
    cm.add((0,0,0))
    cm.add([(0,0,0),(255,255,255)])
    cm.add([(255,0,0),(0,255,0)])
    img = cm.threshold(Image(testimage))
    c=img.meanColor()

    #if( c[0] > 1 or c[1] > 1 or c[2] > 1 ):
    #  assert False

    cm.save("temp.txt")
    cm2 = ColorModel()
    cm2.load("temp.txt")
    img = Image("logo")
    img2 = cm2.threshold(img)
    cm2.add((0,0,255))
    img3 = cm2.threshold(img)
    cm2.add((255,255,0))
    cm2.add((0,255,255))
    cm2.add((255,0,255))
    img4 = cm2.threshold(img)
    cm2.add(img)
    img5 = cm2.threshold(img)

    results = [img,img2,img3,img4,img5]
    name_stem = "test_color_colormap_build"
    perform_diff(results,name_stem)

    #c=img.meanColor()
    #if( c[0] > 1 or c[1] > 1 or c[2] > 1 ):
    #  assert False

def test_color_conversion_func_BGR():
    #we'll just go through the space to make sure nothing blows up
    img = Image(testimage)
    results = []
    results.append(img.toBGR())
    results.append(img.toRGB())
    results.append(img.toHLS())
    results.append(img.toHSV())
    results.append(img.toXYZ())

    bgr = img.toBGR()

    results.append(bgr.toBGR())
    results.append(bgr.toRGB())
    results.append(bgr.toHLS())
    results.append(bgr.toHSV())
    results.append(bgr.toXYZ())

    name_stem = "test_color_conversion_func_BGR"
    perform_diff(results,name_stem,tolerance=4.0)


def test_color_conversion_func_HSV():
    img = Image(testimage)
    hsv = img.toHSV()
    results = [hsv]
    results.append(hsv.toBGR())
    results.append(hsv.toRGB())
    results.append(hsv.toHLS())
    results.append(hsv.toHSV())
    results.append(hsv.toXYZ())
    name_stem = "test_color_conversion_func_HSV"
    perform_diff(results,name_stem,tolerance=4.0 )


def test_color_conversion_func_HLS():
    img = Image(testimage)

    hls = img.toHLS()
    results = [hls]

    results.append(hls.toBGR())
    results.append(hls.toRGB())
    results.append(hls.toHLS())
    results.append(hls.toHSV())
    results.append(hls.toXYZ())

    name_stem = "test_color_conversion_func_HLS"
    perform_diff(results,name_stem,tolerance=4.0)


def test_color_conversion_func_XYZ():
    img = Image(testimage)

    xyz = img.toXYZ()
    results = [xyz]
    results.append(xyz.toBGR())
    results.append(xyz.toRGB())
    results.append(xyz.toHLS())
    results.append(xyz.toHSV())
    results.append(xyz.toXYZ())

    name_stem = "test_color_conversion_func_XYZ"
    perform_diff(results,name_stem,tolerance=8.0)


def test_blob_maker():
    img = Image("../sampleimages/blockhead.png")
    blobber = BlobMaker()
    results = blobber.extract(img)
    print(len(results))
    if( len(results) != 7 ):
        assert False

def test_blob_holes():
    img = Image("../sampleimages/blockhead.png")
    blobber = BlobMaker()
    blobs = blobber.extract(img)
    count = 0
    blobs.draw()
    results = [img]
    name_stem = "test_blob_holes"
    perform_diff(results,name_stem,tolerance=3.0)


    for b in blobs:
        if( b.mHoleContour is not None ):
            count = count + len(b.mHoleContour)
    if( count != 7 ):
        assert False

def test_blob_hull():
    img = Image("../sampleimages/blockhead.png")
    blobber = BlobMaker()
    blobs = blobber.extract(img)
    blobs.draw()

    results = [img]
    name_stem = "test_blob_holes"
    perform_diff(results,name_stem,tolerance=3.0)

    for b in blobs:
        if( len(b.mConvexHull) < 3 ):
            assert False

def test_blob_render():
    img = Image("../sampleimages/blockhead.png")
    blobber = BlobMaker()
    blobs = blobber.extract(img)
    dl = DrawingLayer((img.width,img.height))
    reimg = DrawingLayer((img.width,img.height))
    for b in blobs:
        b.draw(color=Color.RED, alpha=128)
        b.drawHoles(width=2,color=Color.BLUE)
        b.drawHull(color=Color.ORANGE,width=2)
        b.draw(color=Color.RED, alpha=128,layer=dl)
        b.drawHoles(width=2,color=Color.BLUE,layer=dl)
        b.drawHull(color=Color.ORANGE,width=2,layer=dl)
        b.drawMaskToLayer(reimg)

    img.addDrawingLayer(dl)
    results = [img]
    name_stem = "test_blob_render"
    perform_diff(results,name_stem,tolerance=5.0)

    pass

def test_image_convolve():
    img = Image(testimageclr)
    kernel = np.array([[0,0,0],[0,1,0],[0,0,0]])
    img2 = img.convolve(kernel,center=(2,2))

    results = [img2]
    name_stem = "test_image_convolve"
    perform_diff(results,name_stem)

    c=img.meanColor()
    d=img2.meanColor()
    e0 = abs(c[0]-d[0])
    e1 = abs(c[1]-d[1])
    e2 = abs(c[2]-d[2])
    if( e0 > 1 or e1 > 1 or e2 > 1 ):
        assert False

def test_template_match():
    source = Image("../sampleimages/templatetest.png")
    template = Image("../sampleimages/template.png")
    t = 2
    fs = source.findTemplate(template,threshold=t)
    fs.draw()
    results = [source]
    name_stem = "test_template_match"
    perform_diff(results,name_stem)

    pass

def test_embiggen():
    img = Image(logo)

    results = []
    w = int(img.width*1.2)
    h = int(img.height*1.2)

    results.append(img.embiggen(size=(w,h),color=Color.RED))
    results.append(img.embiggen(size=(w,h),color=Color.RED,pos=(30,30)))

    results.append(img.embiggen(size=(w,h),color=Color.RED,pos=(-20,-20)))
    results.append(img.embiggen(size=(w,h),color=Color.RED,pos=(30,-20)))
    results.append(img.embiggen(size=(w,h),color=Color.RED,pos=(60,-20)))
    results.append(img.embiggen(size=(w,h),color=Color.RED,pos=(60,30)))

    results.append(img.embiggen(size=(w,h),color=Color.RED,pos=(80,80)))
    results.append(img.embiggen(size=(w,h),color=Color.RED,pos=(30,80)))
    results.append(img.embiggen(size=(w,h),color=Color.RED,pos=(-20,80)))
    results.append(img.embiggen(size=(w,h),color=Color.RED,pos=(-20,30)))

    name_stem = "test_embiggen"
    perform_diff(results,name_stem)

    pass

def test_createBinaryMask():
    img2 = Image(logo)
    results = []
    results.append(img2.createBinaryMask(color1=(0,100,100),color2=(255,200,200)))
    results.append(img2.createBinaryMask(color1=(0,0,0),color2=(128,128,128)))
    results.append(img2.createBinaryMask(color1=(0,0,128),color2=(255,255,255)))

    name_stem = "test_createBinaryMask"
    perform_diff(results,name_stem)

    pass

def test_applyBinaryMask():
    img = Image(logo)
    mask = img.createBinaryMask(color1=(0,128,128),color2=(255,255,255))
    results = []
    results.append(img.applyBinaryMask(mask))
    results.append(img.applyBinaryMask(mask,bg_color=Color.RED))

    name_stem = "test_applyBinaryMask"
    perform_diff(results,name_stem,tolerance=3.0)

    pass

def test_applyPixelFunc():
    img = Image(logo)
    def myFunc((r,g,b)):
        return( (b,g,r) )

    img = img.applyPixelFunction(myFunc)
    name_stem = "test_applyPixelFunc"
    results = [img]
    perform_diff(results,name_stem)
    pass

def test_applySideBySide():
    img = Image(logo)
    img3 = Image(testimage2)

    #LB = little image big image
    #BL = big image little image  -> this is important to test all the possible cases.
    results = []

    results.append(img3.sideBySide(img,side='right',scale=False))
    results.append(img3.sideBySide(img,side='left',scale=False))
    results.append(img3.sideBySide(img,side='top',scale=False))
    results.append(img3.sideBySide(img,side='bottom',scale=False))

    results.append(img.sideBySide(img3,side='right',scale=False))
    results.append(img.sideBySide(img3,side='left',scale=False))
    results.append(img.sideBySide(img3,side='top',scale=False))
    results.append(img.sideBySide(img3,side='bottom',scale=False))

    results.append(img3.sideBySide(img,side='right',scale=True))
    results.append(img3.sideBySide(img,side='left',scale=True))
    results.append(img3.sideBySide(img,side='top',scale=True))
    results.append(img3.sideBySide(img,side='bottom',scale=True))

    results.append(img.sideBySide(img3,side='right',scale=True))
    results.append(img.sideBySide(img3,side='left',scale=True))
    results.append(img.sideBySide(img3,side='top',scale=True))
    results.append(img.sideBySide(img3,side='bottom',scale=True))

    name_stem = "test_applySideBySide"
    perform_diff(results,name_stem)

    pass

def test_resize():
    img = Image(logo)
    w = img.width
    h = img.height
    img2 = img.resize(w*2,None)
    if( img2.width != w*2 or img2.height != h*2):
        assert False

    img3 = img.resize(h=h*2)

    if( img3.width != w*2 or img3.height != h*2):
        assert False

    img4 = img.resize(h=h*2,w=w*2)

    if( img4.width != w*2 or img4.height != h*2):
        assert False

        pass

    results = [img2,img3,img4]
    name_stem = "test_resize"
    perform_diff(results,name_stem)


def test_createAlphaMask():
    alphaMask = Image(alphaSrcImg)
    mask = alphaMask.createAlphaMask(hue=60)
    mask2 = alphaMask.createAlphaMask(hue_lb=59,hue_ub=61)
    top = Image(topImg)
    bottom = Image(bottomImg)
    bottom = bottom.blit(top,alphaMask=mask2)
    results = [mask,mask2,bottom]
    name_stem = "test_createAlphaMask"
    perform_diff(results,name_stem)


def test_blit_regular():
    top = Image(topImg)
    bottom = Image(bottomImg)
    results = []
    results.append(bottom.blit(top))
    results.append(bottom.blit(top,pos=(-10,-10)))
    results.append(bottom.blit(top,pos=(-10,10)))
    results.append(bottom.blit(top,pos=(10,-10)))
    results.append(bottom.blit(top,pos=(10,10)))

    name_stem = "test_blit_regular"
    perform_diff(results,name_stem)

    pass

def test_blit_mask():
    top = Image(topImg)
    bottom = Image(bottomImg)
    mask = Image(maskImg)
    results = []
    results.append(bottom.blit(top,mask=mask))
    results.append(bottom.blit(top,mask=mask,pos=(-50,-50)))
    results.append(bottom.blit(top,mask=mask,pos=(-50,50)))
    results.append(bottom.blit(top,mask=mask,pos=(50,-50)))
    results.append(bottom.blit(top,mask=mask,pos=(50,50)))

    name_stem = "test_blit_mask"
    perform_diff(results,name_stem)

    pass


def test_blit_alpha():
    top = Image(topImg)
    bottom = Image(bottomImg)
    a = 0.5
    results = []
    results.append(bottom.blit(top,alpha=a))
    results.append(bottom.blit(top,alpha=a,pos=(-50,-50)))
    results.append(bottom.blit(top,alpha=a,pos=(-50,50)))
    results.append(bottom.blit(top,alpha=a,pos=(50,-50)))
    results.append(bottom.blit(top,alpha=a,pos=(50,50)))
    name_stem = "test_blit_alpha"
    perform_diff(results,name_stem)

    pass


def test_blit_alpha_mask():
    top = Image(topImg)
    bottom = Image(bottomImg)
    aMask = Image(alphaMaskImg)
    results = []

    results.append(bottom.blit(top,alphaMask=aMask))
    results.append(bottom.blit(top,alphaMask=aMask,pos=(-10,-10)))
    results.append(bottom.blit(top,alphaMask=aMask,pos=(-10,10)))
    results.append(bottom.blit(top,alphaMask=aMask,pos=(10,-10)))
    results.append(bottom.blit(top,alphaMask=aMask,pos=(10,10)))

    name_stem = "test_blit_alpha_mask"
    perform_diff(results,name_stem)

    pass

def test_whiteBalance():
    img = Image("../sampleimages/BadWB2.jpg")
    output = img.whiteBalance()
    output2 = img.whiteBalance(method="GrayWorld")
    results = [output,output2]
    name_stem = "test_whiteBalance"
    perform_diff(results,name_stem)

def test_hough_circles():
    img = Image(circles)
    circs = img.findCircle(thresh=100)
    circs.draw()
    if( circs[0] < 1 ):
        assert False
    circs[0].coordinates()
    circs[0].width()
    circs[0].area()
    circs[0].perimeter()
    circs[0].height()
    circs[0].radius()
    circs[0].diameter()
    circs[0].colorDistance()
    circs[0].meanColor()
    circs[0].distanceFrom(point=(0,0))
    circs[0].draw()
    img2 = circs[0].crop()
    img3 = circs[0].crop(noMask=True)

    results = [img,img2,img3]
    name_stem = "test_hough_circle"
    perform_diff(results,name_stem)


    if( img2 is not None and img3 is not None ):
        pass
    else:
        assert False

def test_drawRectangle():
    img = Image(testimage2)
    img.drawRectangle(0,0,100,100,color=Color.BLUE,width=0,alpha=0)
    img.drawRectangle(1,1,100,100,color=Color.BLUE,width=2,alpha=128)
    img.drawRectangle(1,1,100,100,color=Color.BLUE,width=1,alpha=128)
    img.drawRectangle(2,2,100,100,color=Color.BLUE,width=1,alpha=255)
    img.drawRectangle(3,3,100,100,color=Color.BLUE)
    img.drawRectangle(4,4,100,100,color=Color.BLUE,width=12)
    img.drawRectangle(5,5,100,100,color=Color.BLUE,width=-1)

    results = [img]
    name_stem = "test_drawRectangle"
    perform_diff(results,name_stem)

    pass


def test_BlobMinRect():
    img = Image(testimageclr)
    blobs = img.findBlobs()
    for b in blobs:
        b.drawMinRect(color=Color.BLUE,width=3,alpha=123)
    results = [img]
    name_stem = "test_BlobMinRect"
    perform_diff(results,name_stem)
    pass

def test_BlobRect():
    img = Image(testimageclr)
    blobs = img.findBlobs()
    for b in blobs:
        b.drawRect(color=Color.BLUE,width=3,alpha=123)

    results = [img]
    name_stem = "test_BlobRect"
    perform_diff(results,name_stem)
    pass

def test_blob_isa_methods():
    img1 = Image(circles)
    img2 = Image("../sampleimages/blockhead.png")
    blobs = img1.findBlobs().sortArea()
    t1 = blobs[-1].isCircle()
    f1 = blobs[-1].isRectangle()
    blobs = img2.findBlobs().sortArea()
    f2 = blobs[-1].isCircle()
    t2 = blobs[-1].isRectangle()
    if( t1 and t2 and not f1 and not f2):
        pass
    else:
        assert False

def test_findKeypoints():
    try:
        import cv2
    except:
        pass
        return
    img = Image(testimage2)
    kp = img.findKeypoints()
    for k in kp:
        k.getObject()
        k.descriptor()
        k.quality()
        k.octave()
        k.flavor()
        k.angle()
        k.coordinates()
        k.draw()
        k.distanceFrom()
        k.meanColor()
        k.area()
        k.perimeter()
        k.width()
        k.height()
        k.radius()
        k.crop()

    kp.draw()
    results = [img]
    name_stem = "test_findKeypoints"
    perform_diff(results,name_stem)

    pass

def test_movement_feature():
    current1 = Image("../sampleimages/flow_simple1.png")
    prev = Image("../sampleimages/flow_simple2.png")

    fs = current1.findMotion(prev, window=7)
    if( len(fs) > 0 ):
        fs.draw(color=Color.RED)
        img = fs[0].crop()
        color = fs[1].meanColor()
        wndw = fs[1].windowSz()
        for f in fs:
            f.vector()
            f.magnitude()
    else:
        assert False


    current2 = Image("../sampleimages/flow_simple1.png")
    fs = current2.findMotion(prev, window=7,method='HS')
    if( len(fs) > 0 ):
        fs.draw(color=Color.RED)
        img = fs[0].crop()
        color = fs[1].meanColor()
        wndw = fs[1].windowSz()
        for f in fs:
            f.vector()
            f.magnitude()
    else:
        assert False

    current3 = Image("../sampleimages/flow_simple1.png")
    fs = current3.findMotion(prev, window=7,method='LK',aggregate=False)
    if( len(fs) > 0 ):
        fs.draw(color=Color.RED)
        img = fs[0].crop()
        color = fs[1].meanColor()
        wndw = fs[1].windowSz()
        for f in fs:
            f.vector()
            f.magnitude()
    else:
        assert False

    results = [current1,current2,current3]
    name_stem = "test_movement_feature"
    perform_diff(results,name_stem,tolerance=4.0)

    pass

def test_keypoint_extraction():
    try:
        import cv2
    except:
        pass
        return

    img1 = Image("../sampleimages/KeypointTemplate2.png")
    img2 = Image("../sampleimages/KeypointTemplate2.png")
    img3 = Image("../sampleimages/KeypointTemplate2.png")

    kp1 = img1.findKeypoints()
    kp2 = img2.findKeypoints(highQuality=True)
    kp3 = img3.findKeypoints(flavor="STAR")
    kp1.draw()
    kp2.draw()
    kp3.draw()
    #TODO: Fix FAST binding
    #~ kp4 = img.findKeypoints(flavor="FAST",min_quality=10)
    if( len(kp1)==190 and
        len(kp2)==190 and
        len(kp3)==37
        #~ and len(kp4)==521
      ):
        pass
    else:
        assert False
    results = [img1,img2,img3]
    name_stem = "test_keypoint_extraction"
    perform_diff(results,name_stem,tolerance=3.0)


def test_keypoint_match():
    try:
        import cv2
    except:
        pass
        return

    template = Image("../sampleimages/KeypointTemplate2.png")
    match0 = Image("../sampleimages/kptest0.png")
    match1 = Image("../sampleimages/kptest1.png")
    match3 = Image("../sampleimages/kptest2.png")
    match2 = Image("../sampleimages/aerospace.jpg")# should be none

    fs0 = match0.findKeypointMatch(template)#test zero
    fs1 = match1.findKeypointMatch(template,quality=300.00,minDist=0.5,minMatch=0.2)
    fs3 = match3.findKeypointMatch(template,quality=300.00,minDist=0.5,minMatch=0.2)
    print "This should fail"
    fs2 = match2.findKeypointMatch(template,quality=500.00,minDist=0.2,minMatch=0.1)
    if( fs0 is not None and fs1 is not None and fs2 is None and  fs3 is not None):
        fs0.draw()
        fs1.draw()
        fs3.draw()
        f = fs0[0]
        f.drawRect()
        f.draw()
        f.getHomography()
        f.getMinRect()
        #f.meanColor()
        f.crop()
        f.x
        f.y
        f.coordinates()
    else:
        assert False

    results = [match0,match1,match2,match3]
    name_stem = "test_find_keypoint_match"
    perform_diff(results,name_stem)


def test_draw_keypoint_matches():
    try:
        import cv2
    except:
        pass
        return
    template = Image("../sampleimages/KeypointTemplate2.png")
    match0 = Image("../sampleimages/kptest0.png")
    result = match0.drawKeypointMatches(template,thresh=500.00,minDist=0.15,width=1)

    results = [result]
    name_stem = "test_draw_keypoint_matches"
    perform_diff(results,name_stem,tolerance=4.0)
    pass

def test_skeletonize():
    img = Image(logo)
    s = img.skeletonize()
    s2 = img.skeletonize(10)

    results = [s,s2]
    name_stem = "test_skelotinze"
    perform_diff(results,name_stem)

    pass

def test_smartThreshold():
    img = Image("../sampleimages/RatTop.png")
    mask = Image((img.width,img.height))
    mask.dl().circle((100,100),80,color=Color.MAYBE_BACKGROUND,filled=True)
    mask.dl().circle((100,100),60,color=Color.MAYBE_FOREGROUND,filled=True)
    mask.dl().circle((100,100),40,color=Color.FOREGROUND,filled=True)
    mask = mask.applyLayers()
    new_mask1 = img.smartThreshold(mask=mask)
    new_mask2 = img.smartThreshold(rect=(30,30,150,185))


    results = [new_mask1,new_mask2]
    name_stem = "test_smartThreshold"
    perform_diff(results,name_stem)

    pass

def test_smartFindBlobs():
    img = Image("../sampleimages/RatTop.png")
    mask = Image((img.width,img.height))
    mask.dl().circle((100,100),80,color=Color.MAYBE_BACKGROUND,filled=True)
    mask.dl().circle((100,100),60,color=Color.MAYBE_FOREGROUND,filled=True)
    mask.dl().circle((100,100),40,color=Color.FOREGROUND,filled=True)
    mask = mask.applyLayers()
    blobs = img.smartFindBlobs(mask=mask)
    blobs.draw()
    results = [img]

    if( len(blobs) < 1 ):
        assert False

    for t in range(2,3):
        img = Image("../sampleimages/RatTop.png")
        blobs2 = img.smartFindBlobs(rect=(30,30,150,185),thresh_level=t)
        if(blobs2 is not None):
            blobs2.draw()
            results.append(img)

    name_stem = "test_smartFindBlobs"
    perform_diff(results,name_stem)

    pass


def test_getDFTLogMagnitude():
    img = Image("../sampleimages/RedDog2.jpg")
    lm3 = img.getDFTLogMagnitude()
    lm1 = img.getDFTLogMagnitude(grayscale=True)

    results = [lm3,lm1]
    name_stem = "test_getDFTLogMagnitude"
    perform_diff(results,name_stem)

    pass


def test_applyDFTFilter():
    img = Image("../sampleimages/RedDog2.jpg")
    flt = Image("../sampleimages/RedDogFlt.png")
    f1 = img.applyDFTFilter(flt)
    f2 = img.applyDFTFilter(flt,grayscale=True)
    results = [f1,f2]
    name_stem = "test_applyDFTFilter"
    perform_diff(results,name_stem)
    pass

def test_highPassFilter():
    img = Image("../sampleimages/RedDog2.jpg")
    a = img.highPassFilter(0.5)
    b = img.highPassFilter(0.5,grayscale=True)
    c = img.highPassFilter(0.5,yCutoff=0.4)
    d = img.highPassFilter(0.5,yCutoff=0.4,grayscale=True)
    e = img.highPassFilter([0.5,0.4,0.3])
    f = img.highPassFilter([0.5,0.4,0.3],yCutoff=[0.5,0.4,0.3])

    results = [a,b,c,d,e,f]
    name_stem = "test_HighPassFilter"
    perform_diff(results,name_stem)
    pass

def test_lowPassFilter():
    img = Image("../sampleimages/RedDog2.jpg")
    a = img.lowPassFilter(0.5)
    b = img.lowPassFilter(0.5,grayscale=True)
    c = img.lowPassFilter(0.5,yCutoff=0.4)
    d = img.lowPassFilter(0.5,yCutoff=0.4,grayscale=True)
    e = img.lowPassFilter([0.5,0.4,0.3])
    f = img.lowPassFilter([0.5,0.4,0.3],yCutoff=[0.5,0.4,0.3])

    results = [a,b,c,d,e,f]
    name_stem = "test_LowPassFilter"
    perform_diff(results,name_stem)

    pass

def test_findHaarFeatures():
    img = Image("../sampleimages/orson_welles.jpg")
    face = HaarCascade("face.xml")
    f = img.findHaarFeatures(face)
    f2 = img.findHaarFeatures("face.xml")
    if( len(f) > 0 and len(f2) > 0 ):
        f.draw()
        f2.draw()
        f[0].width()
        f[0].height()
        f[0].draw()
        f[0].x
        f[0].y
        f[0].length()
        f[0].area()
        pass
    else:
        assert False

    results = [img]
    name_stem = "test_findHaarFeatures"
    perform_diff(results,name_stem)



def test_biblical_flood_fill():
    img = Image(testimage2)
    b = img.findBlobs()
    img.floodFill(b.coordinates(),tolerance=3,color=Color.RED)
    img.floodFill(b.coordinates(),tolerance=(3,3,3),color=Color.BLUE)
    img.floodFill(b.coordinates(),tolerance=(3,3,3),color=Color.GREEN,fixed_range=False)
    img.floodFill((30,30),lower=3,upper=5,color=Color.ORANGE)
    img.floodFill((30,30),lower=3,upper=(5,5,5),color=Color.ORANGE)
    img.floodFill((30,30),lower=(3,3,3),upper=5,color=Color.ORANGE)
    img.floodFill((30,30),lower=(3,3,3),upper=(5,5,5))

    results = [img]
    name_stem = "test_biblical_flood_fill"
    perform_diff(results,name_stem)

    pass

def test_flood_fill_to_mask():
    img = Image(testimage2)
    b = img.findBlobs()
    imask = img.edges()
    omask = img.floodFillToMask(b.coordinates(),tolerance=10)
    omask2 = img.floodFillToMask(b.coordinates(),tolerance=(3,3,3),mask=imask)
    omask3 = img.floodFillToMask(b.coordinates(),tolerance=(3,3,3),mask=imask,fixed_range=False)

    results = [omask,omask2,omask3]
    name_stem = "test_flood_fill_to_mask"
    perform_diff(results,name_stem)

    pass

def test_findBlobsFromMask():
    img = Image(testimage2)
    mask = img.binarize().invert()
    b1 = img.findBlobsFromMask(mask)
    b2 = img.findBlobs()
    b1.draw()
    b2.draw()

    results = [img]
    name_stem = "test_findBlobsFromMask"
    perform_diff(results,name_stem)


    if(len(b1) == len(b2) ):
        pass
    else:
        assert False



def test_bandPassFilter():
    img = Image("../sampleimages/RedDog2.jpg")
    a = img.bandPassFilter(0.1,0.3)
    b = img.bandPassFilter(0.1,0.3,grayscale=True)
    c = img.bandPassFilter(0.1,0.3,yCutoffLow=0.1,yCutoffHigh=0.3)
    d = img.bandPassFilter(0.1,0.3,yCutoffLow=0.1,yCutoffHigh=0.3,grayscale=True)
    e = img.bandPassFilter([0.1,0.2,0.3],[0.5,0.5,0.5])
    f = img.bandPassFilter([0.1,0.2,0.3],[0.5,0.5,0.5],yCutoffLow=[0.1,0.2,0.3],yCutoffHigh=[0.6,0.6,0.6])
    results = [a,b,c,d,e,f]
    name_stem = "test_bandPassFilter"
    perform_diff(results,name_stem)


def test_line_crop():
    img = Image("../sampleimages/EdgeTest2.png")
    l = img.findLines().sortArea()
    l = l[-5:-1]
    results = []
    for ls in l:
        results.append( ls.crop() )
    name_stem = "test_lineCrop"
    perform_diff(results,name_stem,tolerance=3.0)
    pass

def test_on_edge():
    img1 = "./../sampleimages/EdgeTest1.png"
    img2 = "./../sampleimages/EdgeTest2.png"
    imgA = Image(img1)
    imgB = Image(img2)
    imgC = Image(img2)
    imgD = Image(img2)
    imgE = Image(img2)

    blobs = imgA.findBlobs()
    circs = imgB.findCircle(thresh=200)
    corners = imgC.findCorners()
    kp = imgD.findKeypoints()
    lines = imgE.findLines()

    rim =  blobs.onImageEdge()
    inside = blobs.notOnImageEdge()
    rim.draw(color=Color.RED)
    inside.draw(color=Color.BLUE)

    rim =  circs.onImageEdge()
    inside = circs.notOnImageEdge()
    rim.draw(color=Color.RED)
    inside.draw(color=Color.BLUE)

    #rim =  corners.onImageEdge()
    inside = corners.notOnImageEdge()
    #rim.draw(color=Color.RED)
    inside.draw(color=Color.BLUE)

    #rim =  kp.onImageEdge()
    inside = kp.notOnImageEdge()
    #rim.draw(color=Color.RED)
    inside.draw(color=Color.BLUE)

    rim =  lines.onImageEdge()
    inside = lines.notOnImageEdge()
    rim.draw(color=Color.RED)
    inside.draw(color=Color.BLUE)

    results = [imgA,imgB,imgC,imgD,imgE]
    name_stem = "test_onEdge_Features"
    perform_diff(results,name_stem,tolerance=7.0)

def test_feature_angles():
    img = Image("../sampleimages/rotation2.png")
    img2 = Image("../sampleimages/rotation.jpg")
    img3 = Image("../sampleimages/rotation.jpg")
    b = img.findBlobs()
    l = img2.findLines()
    k = img3.findKeypoints()

    for bs in b:
        tl = bs.topLeftCorner()
        img.drawText(str(bs.angle()),tl[0],tl[1],color=Color.RED)

    for ls in l:
        tl = ls.topLeftCorner()
        img2.drawText(str(ls.angle()),tl[0],tl[1],color=Color.GREEN)

    for ks in k:
        tl = ks.topLeftCorner()
        img3.drawText(str(ks.angle()),tl[0],tl[1],color=Color.BLUE)

    results = [img,img2,img3]
    name_stem = "test_feature_angles"
    perform_diff(results,name_stem,tolerance=9.0)

def test_feature_angles_rotate():
    img = Image("../sampleimages/rotation2.png")
    b = img.findBlobs()
    results = []

    for bs in b:
        temp = bs.crop()
        derp = temp.rotate(bs.angle(),fixed=False)
        derp.drawText(str(bs.angle()),10,10,color=Color.RED)
        results.append(derp)
        bs.rectifyMajorAxis()
        results.append(bs.blobImage())

    name_stem = "test_feature_angles_rotate"
    perform_diff(results,name_stem,tolerance=7.0)

def test_minrect_blobs():
    img = Image("../sampleimages/bolt.png")
    img = img.invert()
    results = []
    for i in range(-10,10):
        ang = float(i*18.00)
        print ang
        t = img.rotate(ang)
        b = t.findBlobs(threshval=128)
        b[-1].drawMinRect(color=Color.RED,width=5)
        results.append(t)

    name_stem = "test_minrect_blobs"
    perform_diff(results,name_stem,tolerance=11.0)

def test_pixelize():
    img = Image("../sampleimages/The1970s.png")
    img1 = img.pixelize(4)
    img2 = img.pixelize((5,13))
    img3 = img.pixelize((img.width/10,img.height))
    img4 = img.pixelize((img.width,img.height/10))
    img5 = img.pixelize((12,12),(200,180,250,250))
    img6 = img.pixelize((12,12),(600,80,250,250))
    img7 = img.pixelize((12,12),(600,80,250,250),levels=4)
    img8 = img.pixelize((12,12),levels=6)
    #img9 = img.pixelize(4, )
    #img10 = img.pixelize((5,13))
    #img11 = img.pixelize((img.width/10,img.height), mode=True)
    #img12 = img.pixelize((img.width,img.height/10), mode=True)
    #img13 = img.pixelize((12,12),(200,180,250,250), mode=True)
    #img14 = img.pixelize((12,12),(600,80,250,250), mode=True)
    #img15 = img.pixelize((12,12),(600,80,250,250),levels=4, mode=True)
    #img16 = img.pixelize((12,12),levels=6, mode=True)

    results = [img1,img2,img3,img4,img5,img6,img7,img8] #img9,img10,img11,img12,img13,img14,img15,img16]
    name_stem = "test_pixelize"
    perform_diff(results,name_stem,tolerance=6.0)


def test_point_intersection():
    img = Image("simplecv")
    e = img.edges(0,100)
    for x in range(25,225,25):
        a = (x,25)
        b = (125,125)
        pts = img.edgeIntersections(a,b,width=1)
        e.drawLine(a,b,color=Color.RED)
        e.drawCircle(pts[0],10,color=Color.GREEN)

    for x in range(25,225,25):
        a = (25,x)
        b = (125,125)
        pts = img.edgeIntersections(a,b,width=1)
        e.drawLine(a,b,color=Color.RED)
        e.drawCircle(pts[0],10,color=Color.GREEN)

    for x in range(25,225,25):
        a = (x,225)
        b = (125,125)
        pts = img.edgeIntersections(a,b,width=1)
        e.drawLine(a,b,color=Color.RED)
        e.drawCircle(pts[0],10,color=Color.GREEN)

    for x in range(25,225,25):
        a = (225,x)
        b = (125,125)
        pts = img.edgeIntersections(a,b,width=1)
        e.drawLine(a,b,color=Color.RED)
        e.drawCircle(pts[0],10,color=Color.GREEN)

    results = [e]
    name_stem = "test_point_intersection"
    perform_diff(results,name_stem,tolerance=6.0)

def test_getSkintoneMask():
    imgSet = []
    imgSet.append(Image('../sampleimages/040000.jpg'))
    imgSet.append(Image('../sampleimages/040001.jpg'))
    imgSet.append(Image('../sampleimages/040002.jpg'))
    imgSet.append(Image('../sampleimages/040003.jpg'))
    imgSet.append(Image('../sampleimages/040004.jpg'))
    imgSet.append(Image('../sampleimages/040005.jpg'))
    imgSet.append(Image('../sampleimages/040006.jpg'))
    imgSet.append(Image('../sampleimages/040007.jpg'))
    masks = [img.getSkintoneMask() for img in imgSet]
    VISUAL_TEST = True
    name_stem = 'test_skintone'
    perform_diff(masks,name_stem,tolerance=17)

def test_sobel():
    img = Image("lenna")
    s = img.sobel()
    name_stem = "test_sobel"
    s = [s]
    perform_diff(s,name_stem)

def test_image_new_smooth():
    img = Image(testimage2)
    result = []
    result.append(img.medianFilter())
    result.append(img.medianFilter((3,3)))
    result.append(img.medianFilter((5,5),grayscale=True))
    result.append(img.bilateralFilter())
    result.append(img.bilateralFilter(diameter=14,sigmaColor=20, sigmaSpace=34))
    result.append(img.bilateralFilter(grayscale=True))
    result.append(img.blur())
    result.append(img.blur((5,5)))
    result.append(img.blur((3,5),grayscale=True))
    result.append(img.gaussianBlur())
    result.append(img.gaussianBlur((3,7), sigmaX=10 , sigmaY=12))
    result.append(img.gaussianBlur((7,9), sigmaX=10 , sigmaY=12, grayscale=True))
    name_stem = "test_image_new_smooth"
    perform_diff(result,name_stem)
    pass

def test_camshift():
    ts = []
    bb = (195, 160, 49, 46)
    imgs = [Image(img) for img in trackimgs]
    ts = imgs[0].track("camshift", ts, imgs[1:], bb)
    if ts:
        pass
    else:
        assert False

def test_lk():
    ts = []
    bb = (195, 160, 49, 46)
    imgs = [Image(img) for img in trackimgs]
    ts = imgs[0].track("LK", ts, imgs[1:], bb)
    if ts:
        pass
    else:
        assert False

########NEW FILE########
__FILENAME__ = test_haar_cascade
#!/usr/bin/python

from SimpleCV import *

FACECASCADE = 'face.xml'

testimage = "sampleimages/orson_welles.jpg"
testoutput = "sampleimages/orson_welles_face.jpg"

testneighbor_in = "sampleimages/04000.jpg"
testneighbor_out = "sampleimages/04000_face.jpg"

def test_haarcascade():
    img = Image(testimage)
    faces = img.findHaarFeatures(FACECASCADE)

    if (faces):
        faces.draw()
        img.save(testoutput)
    else:
        assert False

def test_minneighbors(img_in=testneighbor_in, img_out=testneighbor_out):
    img = Image(img_in)
    faces = img.findHaarFeatures(FACECASCADE, min_neighbors=20)
    if faces:
        faces.draw()
        img.save(img_out)
    # if len(faces) > 1
    assert len(faces) <= 1, "Haar Cascade is potentially ignoring the 'HIGH' min_neighbors of 20"

########NEW FILE########
__FILENAME__ = test_optional
# /usr/bin/python
# To run this test you need python nose tools installed
# Run test just use:
#   nosetest test_optional.py
#

import os, sys, pickle
from SimpleCV import *
from nose.tools import with_setup, nottest

SHOW_WARNING_TESTS = False  # show that warnings are working - tests will pass but warnings are generated.

#colors
black = Color.BLACK
white = Color.WHITE
red = Color.RED
green = Color.GREEN
blue = Color.BLUE

###############
# TODO -
# Examples of how to do profiling
# Examples of how to do a single test -
# UPDATE THE VISUAL TESTS WITH EXAMPLES.
# Fix exif data
# Turn off test warnings using decorators.
# Write a use the tests doc.

#images
barcode = "../sampleimages/barcode.png"
testimage = "../sampleimages/9dots4lines.png"
testimage2 = "../sampleimages/aerospace.jpg"
whiteimage = "../sampleimages/white.png"
blackimage = "../sampleimages/black.png"
testimageclr = "../sampleimages/statue_liberty.jpg"
testbarcode = "../sampleimages/barcode.png"
testoutput = "../sampleimages/9d4l.jpg"
tmpimg = "../sampleimages/tmpimg.jpg"
greyscaleimage = "../sampleimages/greyscale.jpg"
logo = "../sampleimages/simplecv.png"
logo_inverted = "../sampleimages/simplecv_inverted.png"
ocrimage = "../sampleimages/ocr-test.png"
circles = "../sampleimages/circles.png"
webp = "../sampleimages/simplecv.webp"

#alpha masking images
topImg = "../sampleimages/RatTop.png"
bottomImg = "../sampleimages/RatBottom.png"
maskImg = "../sampleimages/RatMask.png"
alphaMaskImg = "../sampleimages/RatAlphaMask.png"
alphaSrcImg = "../sampleimages/GreenMaskSource.png"

#standards path
standard_path = "./standard/"

#These function names are required by nose test, please leave them as is
def setup_context():
    img = Image(testimage)

def destroy_context():
    img = ""

@with_setup(setup_context, destroy_context)
def test_detection_barcode():
    try:
        import zbar
    except:
        return None

    img1 = Image(testimage)
    img2 = Image(testbarcode)

    if( SHOW_WARNING_TESTS ):
        nocode = img1.findBarcode()
        if nocode: #we should find no barcode in our test image
            assert False
        code = img2.findBarcode()
        code.draw()
        if code.points:
            pass
        result = [img1,img2]
        name_stem = "test_detection_barcode"
        perform_diff(result,name_stem)
    else:
        pass

def test_detection_ocr():
    img = Image(ocrimage)

    foundtext = img.readText()
    print foundtext
    if(len(foundtext) <= 1):
        assert False
    else:
        pass

def test_image_webp_load():
    #only run if webm suppport exist on system
    try:
        import webm
    except:
        if( SHOW_WARNING_TESTS ):
            logger.warning("Couldn't run the webp test as optional webm library required")
        pass

    else:
        img = Image(webp)

        if len(img.toString()) <= 1:
            assert False

        else:
            pass

def test_image_webp_save():
    #only run if webm suppport exist on system
    try:
        import webm
    except:
        if( SHOW_WARNING_TESTS ):
            logger.warning("Couldn't run the webp test as optional webm library required")
        pass

    else:
        img = Image('simplecv')
        tf = tempfile.NamedTemporaryFile(suffix=".webp")
        if img.save(tf.name):
            pass
        else:
            assert False

def test_screenshot():
    try:
        import pyscreenshot
    except:
        if( SHOW_WARNING_TESTS ):
            logger.warning("Couldn't run the pyscreenshot test. Install pyscreenshot library")
        pass
    sc = ScreenCamera()
    res = sc.getResolution()
    img = sc.getImage()
    crop = (res[0]/4,res[1]/4,res[0]/2,res[1]/2)
    sc.setROI(crop)
    cropImg = sc.getImage()
    if img and cropImg :
        assert True
    else:
        assert False

########NEW FILE########
__FILENAME__ = test_stereovision
# /usr/bin/python
# To run this test you need python nose tools installed
# Run test just use:
#   nosetest test_stereovision.py
#

import os, sys, pickle
from SimpleCV import *
from nose.tools import with_setup, nottest

VISUAL_TEST = True  # if TRUE we save the images - otherwise we DIFF against them - the default is False
SHOW_WARNING_TESTS = False  # show that warnings are working - tests will pass but warnings are generated.

#colors
black = Color.BLACK
white = Color.WHITE
red = Color.RED
green = Color.GREEN
blue = Color.BLUE

###############
# TODO -
# Examples of how to do profiling
# Examples of how to do a single test -
# UPDATE THE VISUAL TESTS WITH EXAMPLES.
# Fix exif data
# Turn off test warnings using decorators.
# Write a use the tests doc.

#images
pair1 = ("../sampleimages/stereo1_left.png" , "../sampleimages/stereo1_right.png")
pair2 = ("../sampleimages/stereo2_left.png" , "../sampleimages/stereo2_right.png")
pair3 = ("../sampleimages/stereo1_real_left.png" , "../sampleimages/stereo1_real_right.png")
pair4 = ("../sampleimages/stereo2_real_left.png" , "../sampleimages/stereo2_real_right.png")
pair5 = ("../sampleimages/stereo3_real_left.png" , "../sampleimages/stereo3_real_right.png")

correct_pairs = [pair1,pair2,pair3,pair4,pair5]

#standards path
standard_path = "./standard/"


#Given a set of images, a path, and a tolerance do the image diff.
def imgDiffs(test_imgs,name_stem,tolerance,path):
    count = len(test_imgs)
    for idx in range(0,count):
        lhs = test_imgs[idx].applyLayers() # this catches drawing methods
        fname = standard_path+name_stem+str(idx)+".jpg"
        rhs = Image(fname)
        if( lhs.width == rhs.width and lhs.height == rhs.height ):
            diff = (lhs-rhs)
            val = np.average(diff.getNumpy())
            if( val > tolerance ):
                print val
                return True
    return False

#Save a list of images to a standard path.
def imgSaves(test_imgs, name_stem, path=standard_path):
    count = len(test_imgs)
    for idx in range(0,count):
        fname = standard_path+name_stem+str(idx)+".jpg"
        test_imgs[idx].save(fname)#,quality=95)

#perform the actual image save and image diffs.
def perform_diff(result,name_stem,tolerance=2.0,path=standard_path):
    if(VISUAL_TEST): # save the correct images for a visual test
        imgSaves(result,name_stem,path)
    else: # otherwise we test our output against the visual test
        if( imgDiffs(result,name_stem,tolerance,path) ):
            assert False
        else:
            pass

#These function names are required by nose test, please leave them as is
def setup_context():
    img = Image(pair1[0])

def destroy_context():
    img = ""

@with_setup(setup_context, destroy_context)
def test_findFundamentalMat():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        if ( not StereoImg.findFundamentalMat()):
            assert False

def test_findHomography():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        if (not StereoImg.findHomography()):
            assert False

def test_findDisparityMap():
    dips = []
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        dips.append(StereoImg.findDisparityMap(method="BM"))
    name_stem = "test_disparitymapBM"
    perform_diff(dips,name_stem)

    dips = []
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        dips.append(StereoImg.findDisparityMap(method="SGBM"))
    name_stem = "test_disparitymapSGBM"
    perform_diff(dips,name_stem)

def test_eline():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        F,ptsLeft,ptsRight = StereoImg.findFundamentalMat()
        for pts in ptsLeft :
            line = StereoImg.Eline(pts,F,2)
            if (line == None):
                assert False


def test_projectPoint():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        H,ptsLeft,ptsRight = StereoImg.findHomography()
        for pts in ptsLeft :
            line = StereoImg.projectPoint(pts,H,2)
            if (line == None):
                assert False


def test_StereoCalibration():
    cam = StereoCamera()
    try :
        cam1 = Camera(0)
        cam2 = Camera(1)
        cam1.getImage()
        cam2.getImage()
        try :
            cam = StereoCamera()
            calib = cam.StereoCalibration(0,1,nboards=1)
            if (calib):
                assert True
            else :
                assert False
        except:
            assert False
    except :
        assert True

def test_loadCalibration():
    cam = StereoCamera()
    calbib =  cam.loadCalibration("Stereo","./StereoVision/")
    if (calbib) :
        assert True
    else :
        assert False

def test_StereoRectify():
    cam = StereoCamera()
    calib = cam.loadCalibration("Stereo","./StereoVision/")
    rectify = cam.stereoRectify(calib)
    if rectify :
        assert True
    else :
        assert False

def test_getImagesUndistort():
    img1 = Image(correct_pairs[0][0]).resize(352,288)
    img2 = Image(correct_pairs[0][1]).resize(352,288)
    cam = StereoCamera()
    calib = cam.loadCalibration("Stereo","./StereoVision/")
    rectify = cam.stereoRectify(calib)
    rectLeft,rectRight = cam.getImagesUndistort(img1,img2,calib,rectify)
    if rectLeft and rectRight :
        assert True
    else :
        assert False

########NEW FILE########
__FILENAME__ = test_vimba
import numpy as np
import cv2
import time
from SimpleCV.Camera import VimbaCamera


def printPrettyHeader(msg):
    print "*"*80 + "\n* %s *\n" % msg + "*"*80

def _getProperty(c):
    printPrettyHeader("Test getProperty")

    prop = "ExposureMode"
    print "%s=%s" % (prop, c.getProperty(prop))

def _getAllProperties(c):
    printPrettyHeader("Test getAllProperties")

    allprops = c.getAllProperties()
    for k in sorted(allprops.iterkeys()) :
        print "%s=%s" % (k,allprops[k])

def _setProperty(c):
    printPrettyHeader("Test setProperty (toggle AcquisitionMode)")

    prop = "AcquisitionMode"
    val = c.getProperty(prop)
    print "BEFORE: %s=%s" % (prop, val)
    newval = "Continuous" if val == "SingleFrame" else "SingleFrame"
    print "newval=%s" % newval
    c.setProperty(prop, "Continuous")
    time.sleep(0.2)
    val = c.getProperty(prop)
    print "AFTER: %s=%s" % (prop, val)

def _setupASyncMode(c):
    printPrettyHeader("Test setupASyncMode (toggle TriggerSource)")

    prop1 = 'AcquisitionMode'
    prop2 = 'TriggerSource'    
    print 'BEFORE: %s=%s, %s=%s' % (prop1, c.getProperty(prop1), prop2, c.getProperty(prop2))
    c.setupASyncMode()
    print 'AFTER: %s=%s, %s=%s' % (prop1, c.getProperty(prop1), prop2, c.getProperty(prop2))

def _setupSyncMode(c):
    printPrettyHeader("Test setupSyncMode (toggle TriggerSource)")

    prop1 = 'AcquisitionMode'
    prop2 = 'TriggerSource'    
    print 'BEFORE: %s=%s, %s=%s' % (prop1, c.getProperty(prop1), prop2, c.getProperty(prop2))
    c.setupSyncMode()
    print 'AFTER: %s=%s, %s=%s' % (prop1, c.getProperty(prop1), prop2, c.getProperty(prop2))

def _getImage(c):
    printPrettyHeader("Test getImage")

    img = c.getImage()
    img.save("test_getImage_scv.png")
    print "test_getImage_scv.png saved"

def _runCommand(c):
    printPrettyHeader("Test runCommand")
    vimbacam = c._camera
    f = vimbacam.getFrame()    # creates a frame
    f.announceFrame()
        
    vimbacam.startCapture()
    f.queueFrameCapture()
    c.runCommand("AcquisitionStart")
    c.runCommand("AcquisitionStop")
    f.waitFrameCapture(1000)
    moreUsefulImgData = np.ndarray(buffer = f.getBufferByteData(),
             dtype = np.uint8,
             shape = (f.height, f.width, 1))
    rgb = cv2.cvtColor(moreUsefulImgData, cv2.COLOR_BAYER_RG2RGB)
    vimbacam.endCapture()
    cv2.imwrite('test_runCommand.png', rgb)
    print "test_runCommand.png saved"

def _listAllCameras(c):
    printPrettyHeader("Test listAllCameras")

    l = c.listAllCameras()
    for i in l:
        print 'Camera Id=%s' % i.cameraIdString

def test_all():
    c = VimbaCamera()
    _getProperty(c)
    _getAllProperties(c)
    _setProperty(c)
    _setupASyncMode(c)
    _setupSyncMode(c)

    _getImage(c)
    _runCommand(c)
    _listAllCameras(c)


########NEW FILE########
__FILENAME__ = test_vimba_async
import numpy as np
import cv2
import time
from SimpleCV.Camera import VimbaCamera, AVTCamera
from SimpleCV import Display
from pymba import Vimba

def printPrettyHeader(msg):
    print "*"*80 + "\n* %s *\n" % msg + "*"*80

def _takeShots(cam, numPics, filename):
    start = time.time()
    #print "Taking %d photos..." % numPics
    for i in range(numPics):
        img = cam.getImage()
        img.save("%s_%d.png" % (filename, i))
    end = time.time()
    elapsed = end - start
    print "Took %f seconds" % elapsed

'''
def test_AVT_threaded_getImage():
    printPrettyHeader("Test AVT_threaded_getImage")
    c = AVTCamera(threaded=True)
    time.sleep(0.2)
    img = c.getImage()
    img.save("avt.png")
'''

def test_Vimba_threaded_getImage():
    printPrettyHeader("Test Vimba_threaded_getImage")
    c = VimbaCamera(threaded=True)
    time.sleep(0.2)
    img = c.getImage()
    img.save("vimba.png")

    time.sleep(0.2)

    c.shutdown()

########NEW FILE########
__FILENAME__ = test_vimba_manyshots
import numpy as np
import cv2
import time
from SimpleCV.Camera import VimbaCamera, AVTCamera
from SimpleCV import Display

#c = VimbaCamera(0, threaded=True) # async
#time.sleep(0.2)

def printPrettyHeader(msg):
    print "*"*80 + "\n* %s *\n" % msg + "*"*80

def _takeShots(cam, numPics, filename):
    start = time.time()
    #print "Taking %d photos..." % numPics
    for i in range(numPics):
        img = cam.getImage()
        img.save("%s_%d.png" % (filename, i))
    end = time.time()
    elapsed = end - start
    print "Took %f seconds" % elapsed

def _takeManyVimbaShots(idx):
    c = VimbaCamera()
    print "_takeManyVimbaShots %d" % idx

    _takeShots(c, 10, "cam_vimba%d" % idx)

def _takeAVTManyShots(idx):
    c = AVTCamera()
    print "_takeAVTManyShots %d" % idx

    _takeShots(c, 10, "cam_avtnative%d" % idx)

#_takeAVTManyShots(1)
#_takeAVTManyShots(2)
#_takeManyVimbaShots(1)
#_takeManyVimbaShots(2)

"""
def test_getImageDisplay():
    c = VimbaCamera()
    printPrettyHeader("Test getImage")

    img = c.getImage()
    display = Display()
    img.save(display)
    img.save("test_getImage_scv_display2.png")
    print "test_getImage_scv_display2.png saved"
"""


def test_createManyCameras():
    printPrettyHeader("Test createManyCameras")
    numIter = 10 #1000
    for i in range(numIter):
        _takeManyVimbaShots(i)

def test_oneGrayShot():
    c = VimbaCamera(properties={"mode":"gray"})
    printPrettyHeader("Test oneGrayShot")

    img = c.getImage()
    img.save("test_oneGrayShot.png")

def test_oneShot():
    c = VimbaCamera()
    printPrettyHeader("Test oneShot")

    img = c.getImage()
    img.save("test_oneShot.png")

def test_takeManyShots():
    c = VimbaCamera()
    printPrettyHeader("Test takeManyShots")

    _takeShots(c, 5, "vimba")

def test_AVT_takeManyShots():
    c = AVTCamera()
    printPrettyHeader("Test AVT_takeManyShots")

    _takeShots(c, 5, "avtnative")

"""
def test_makeLotsOfCamera():
    numIter = 1000
    print "Creating %d cameras..." % numIter
    start = time.time()
    for i in range(numIter):
        print "At camera=%d" % i
        cam = VimbaCamera()
        img = cam.getImage()
    end = time.time()
    elapsed = end - start
    print "Took %f seconds" % elapsed

"""

########NEW FILE########
__FILENAME__ = vcamera_tests
#!/usr/bin/python

import os, sys
from SimpleCV import *
from nose.tools import with_setup


testimage = "../sampleimages/9dots4lines.png"
testvideo = "../sampleimages/ball.mov"
testoutput = "standard/vc.jpg"

def doFullVCamCoverageTest(vcam):
    run = True
    count = 0
    maxf = 1000
    while run:
        img = vcam.getImage()
        vcam.getFrameNumber()
        count = count + 1
        if( img is None or count > maxf ):
            run = False
    vcam.rewind()
    run = True
    while run:
        vcam.skipFrames(2)
        img = vcam.getImage()  
        count = count + 1
        if( img is None or count > maxf ):
            run = False
    return True
    

def test_camera_constructor():
    mycam = VirtualCamera(testimage, "image")
    props = mycam.getAllProperties()

    for i in props.keys():
        print str(i) + ": " + str(props[i]) + "\n"
        
    pass

def test_camera_image():
    mycam = VirtualCamera(testimage, "image")
    if(doFullVCamCoverageTest(mycam)):
        pass
    else:
        assert False

def test_camera_video():
    mycam = VirtualCamera(testvideo, "video")
    img = mycam.getImage()
    img.save(testoutput)
    assert img.size() == (320, 240)
    if(doFullVCamCoverageTest(mycam)):
        pass
    else:
        assert False

def test_camera_iset():
    iset = ImageSet('./standard/')
    mycam = VirtualCamera(iset, "imageset")
    img = mycam.getImage()
    if(doFullVCamCoverageTest(mycam)):
        pass
    else:
        assert False

def test_camera_iset_directory():
    iset = './standard/'
    mycam = VirtualCamera(iset, "imageset")
    img = mycam.getImage()
    if(doFullVCamCoverageTest(mycam)):
        pass
    else:
        assert False

########NEW FILE########
__FILENAME__ = YCrCbtests
from SimpleCV import *
img = Image('lenna')

img1 = img.toYCrCb()
if (img1.isYCrCb()):
    print "Converted to YCrCb\n"

img1 = img.toBGR()
img2 = img1.toYCrCb()
if (img2.isYCrCb()):
    print "Converted BGR to YCrCb\n"

img1 = img.toHLS()
img2 = img1.toYCrCb()
if (img2.isYCrCb()):
    print "Converted HLS to YCrCb\n"

img1 = img.toHSV()
img2 = img1.toYCrCb()
if (img2.isYCrCb()):
    print "Converted HSV to YCrCb\n"

img1 = img.toXYZ()
img2 = img1.toYCrCb()
if (img2.isYCrCb()):
    print "Converted XYZ to YCrCb\n"

img1 = img.toYCrCb()
img2 = img1.toRGB()
if (img2.isYCrCb()):
    print "Converted from YCrCb to RGB\n"

img1 = img.toYCrCb()
img2 = img1.toBGR()
if (img2.isRGB()):
    print "Converted from YCrCb to RGB\n"

img1 = img.toYCrCb()
img2 = img1.toHLS()
if (img2.isHLS()):
    print "Converted from YCrCb to HLS\n"

img1 = img.toYCrCb()
img2 = img1.toHSV()
if (img2.isHSV()):
    print "Converted from YCrCb to HSV\n"

img1 = img.toYCrCb()
img2 = img1.toXYZ()
if (img2.isXYZ()):
    print "Converted from YCrCb to XYZ\n"

img1 = img.toGray()
img2 = img1.toGray()
if (img2.isGray()):
    print "Converted from Gray to Gray\n"

########NEW FILE########
__FILENAME__ = Calibrate
#!/usr/bin/python

import webbrowser, sys, time, random
from SimpleCV import Camera, Image, JpegStreamer, Color
from SimpleCV.Display import Display
from scipy.spatial.distance import euclidean as distance
"""
This script can be used to quickly calibrate a camera


You will need to print out the accompanying calibration grid, or provide your
own.  The default arguments are for the provided grid.

By default, the camera calibrates to maximize the amount of coverage of different
planes and distances.  If you want higher accuracy for a single plane in a locked
distance specify "planemode".

"""

def showText(img, text):
    img.dl().setFontSize(25)
    width, height = img.dl().textDimensions(text)
    #img.dl().text(str(width) + 'x' + str(height), (100, 100))
    img.dl().text(text, ((img.width / 2) - (width / 2), 100 - height / 2), color = (0, 120, 0))

def drawline(img, pt1, pt2):
    img.dl().line(pt1, pt2, Color.GREEN, 5, antialias = False)

def drawrect(img, pt1, pt2):
    drawline(img, pt1, (pt1[0],pt2[1]))
    drawline(img, (pt1[0], pt2[1]), pt2)
    drawline(img, pt2, (pt2[0], pt1[1]))
    drawline(img, pt1, (pt2[0], pt1[1]))

def inrect(rect, pts):
    for p in pts:
        if p[0] < rect[0][0] or p[1] < rect[0][1] or p[0] > rect[1][0] or p[1] > rect[1][1]:
            return False

    return True

def saveCalibrationImage(i, imgset, dims):
    """
    Save our image in the calibration set
    """
    if (len(imgset)):
        lastcb = imgset[-1].findChessboard(dims, subpixel = False)
        thiscb = i.findChessboard(dims, subpixel = False)

        cbmid = len(lastcb.coordinates()) / 2
        if distance(lastcb.coordinates()[cbmid], thiscb.coordinates()[cbmid]) < 30:
            showText(i, "Move the chessboard around inside the green rectangle")
            return

    imgset.append(i.copy())
    global save_location
    if not save_location:
        return

    filename = save_location + "_image" + str(len(imgset)) + ".png"
    imgset[-1].save(filename)

def testrect(img, cb, calibration_set, dims, rect):
    drawrect(img, rect[0], rect[1])
    if inrect(rect, cb.points):
        saveCalibrationImage(img, calibration_set, dims)
        return True
    return False

def relativeSize(cb, i):
    return cb.area() / float(i.width * i.height)

def relPercent(cb, i):
    return str(int(relativeSize(cb,i) * 100)) + "%"

def horizontalTilt(cb):  #ratio between the 0,3 and 1,2 point pairs
    distance_ratio = distance(cb.points[0], cb.points[3]) / distance(cb.points[1], cb.points[2])
    if distance_ratio > 1:
        return 1.0 / distance_ratio
    return distance_ratio

def verticalTilt(cb):  #radio between the 0, 1 and 2,3 point pairs
    distance_ratio = distance(cb.points[0], cb.points[1]) / distance(cb.points[2], cb.points[3])
    if distance_ratio > 1:
        return 1.0 / distance_ratio
    return distance_ratio

def introMessage():
    print """
  This tool will help you calibrate your camera to help remove the effects of
  lens distortion and give you more accurate measurement.  You will need:

  - a printed 5x8 chessboard from our provided PDF, taped to a hard flat surface
  - a room with good lighting and plenty of space for moving around
  - a few minutes to begin calibration

  To begin, please put your chessboard close to the camera so the long side is
  horizontal and it fill most of the screen.  Keep it parallel to the camera so it
  appears within the rectangle.
    """

def findLargeFlat(cb, i, calibration_set, dims):
    drawline(i,  (10, 10), (i.width - 10, 10))
    drawline(i,  (i.width - 10, 10), (i.width - 10, i.height - 10))
    drawline(i,  (10, i.height - 10), (i.width - 10, i.height - 10))
    drawline(i,  (10, 10), (10, i.height - 10))


    if not cb:
        return

    if (relativeSize(cb, i) > 0.7):
        saveCalibrationImage(i, calibration_set, dims)
    else:
        showText(i,  "Chessboard is " + str(int(relativeSize(cb,i) * 100)) + " < 70% of view area, bring it closer")

    cb.draw()



def findSmallFlat(cb, i, calibration_set, dims):
    lcs = len(calibration_set)
    if (lcs % 5 == 0): #outline the top left corner
        drawline(i,  (i.width / 2, 0), (i.width / 2, i.height / 2))
        drawline(i,  (0, i.height / 2), (i.width / 2, i.height / 2))
    if (lcs % 5 == 1): #outline top right corner
        drawline(i,  (i.width / 2, 0), (i.width / 2, i.height / 2))
        drawline(i,  (i.width, i.height/2), (i.width / 2, i.height / 2))
    if (lcs % 5 == 2): #outline bottom right corner
        drawline(i,  (i.width, i.height/2), (i.width / 2, i.height / 2))
        drawline(i,  (i.width / 2, i.height), (i.width / 2, i.height / 2))
    if (lcs % 5 == 3): #outline bottom left corner
        drawline(i,  (i.width / 2, i.height), (i.width / 2, i.height / 2))
        drawline(i,  (0, i.height / 2), (i.width / 2, i.height / 2))
    if (lcs % 5 == 4): #outline center
        drawline(i,  (i.width / 4, i.height / 4), (3 * i.width / 4, i.height / 4))
        drawline(i,  (3 * i.width / 4, i.height / 4), (3 * i.width / 4, 3 * i.height / 4))
        drawline(i,  (3 * i.width / 4, 3 * i.height / 4), (i.width / 4, 3 * i.height / 4))
        drawline(i,  (i.width / 4, 3 * i.height / 4), (i.width / 4, i.height / 4))

    if not cb:
        return

    if (relativeSize(cb, i) < 0.13):
        showText(i,  "Chessboard is " + relPercent(cb, i) + " < 13% bring it closer")
    elif (relativeSize(cb, i) > 0.25):
        showText(i,  "Chessboard is " + relPercent(cb, i) + " > 25% move it back")
    elif (horizontalTilt(cb) < 0.9 or verticalTilt < 0.9):
        showText(i,  "Chessboard is tilted, try to keep it flat")
    elif (lcs % 5 == 0): #top left corner
        if (cb.points[2][0] < i.width / 2 and cb.points[2][1] < i.height / 2):
            saveCalibrationImage(i, calibration_set, dims)
        else:
            showText(i, "Put the chessboard within the green rectangle")
    elif (lcs % 5 == 1): #top right corner
        if (cb.points[3][0] > i.width / 2 and cb.points[3][1] < i.height / 2):
            saveCalibrationImage(i, calibration_set, dims)
        else:
            showText(i, "Put the chessboard within the green rectangle")
    elif (lcs % 5 == 2): #bottom right corner
        if (cb.points[0][0] > i.width / 2 and cb.points[0][1] > i.height / 2):
            saveCalibrationImage(i, calibration_set, dims)
        else:
            showText(i, "Put the chessboard within the green rectangle")
    elif (lcs % 5 == 3): #bottom left corner
        if (cb.points[1][0] < i.width / 2 and cb.points[1][1] > i.height / 2):
            saveCalibrationImage(i, calibration_set, dims)
        else:
            showText(i, "Put the chessboard within the green rectangle")
    elif (lcs % 5 == 4): #center
        if (abs(cb.x - i.width / 2) < i.width/8.0 and abs(cb.y - i.height / 2) < i.height/8.0):
            saveCalibrationImage(i, calibration_set, dims)


def findHorizTilted(cb, i, calibration_set, dims):
    drawline(i,  (10, i.height / 8), (i.width - 10, 10))
    drawline(i,  (i.width - 10, 10), (i.width - 10, i.height - 10))
    drawline(i,  (10, i.height - i.height / 8), (i.width - 10, i.height - 10))
    drawline(i,  (10, i.height / 8), (10, i.height - i.height / 8))
    if not cb:
        return

    if relativeSize(cb, i) < 0.4:
        showText(i,  "Chessboard is " + relPercent(cb, i) + " / 40%, bring it closer")
    elif horizontalTilt(cb) > 0.9:
        showText(i,  "Tip the right or left side of the chessboard towards the camera")
    else:
        saveCalibrationImage(i, calibration_set, dims)


def findVertTilted(cb, i, calibration_set, dims):
    drawline(i,  (i.width / 8, 10), (i.width - i.width / 8, 10))
    drawline(i,  (i.width - i.width / 8, 10), (i.width - 10, i.height - 10))
    drawline(i,  (10, i.height - 10), (i.width - 10, i.height - 10))
    drawline(i,  (i.width / 8, 10), (10, i.height - 10))
    if not cb:
        return

    if relativeSize(cb, i) < 0.4:
        showText(i,  "Chessboard is " + relPercent(cb, i) + " / 40%, bring it closer")
    elif verticalTilt(cb) > 0.9:
        showText(i,  "Tip the top or bottom of the chessboard towards the camera")
    else:
        saveCalibrationImage(i, calibration_set, dims)

def findCornerTilted(cb, i, calibration_set, dims):
    drawline(i,  (i.width / 8, 10), (i.width - i.width / 8, 10))
    drawline(i,  (i.width - i.width / 8, 10), (i.width - 10, i.height - i.height / 8))
    drawline(i,  (10, i.height - 10), (i.width - 10, i.height - i.height / 8))
    drawline(i,  (i.width / 8, 10), (10, i.height - 10))

    if not cb:
        return

    if relativeSize(cb, i) < 0.4:
        showText(i,  "Chessboard is " + relPercent(cb, i) + " / 40%, bring it closer")
    elif verticalTilt(cb) > 0.9 or horizontalTilt(cb) > 0.9:
        showText(i,  "Tip the corner of the chessboard more towards the camera")
    else:
        saveCalibrationImage(i, calibration_set, dims)


def findPlane(cb, i, calibration_set, dims):
    if not cb:
        drawrect(i, (10,10), (i.width - 10, i.height - 10))
        return

    cbwidth = cb.width()
    cbheight = cb.height()
    lcs = len(calibration_set)

    tolerance = 1.2 #20% tolerance

    min_x = 5
    left_x = i.width - cbwidth * tolerance
    mid_min_x = i.width / 2 - cbwidth * tolerance / 2
    mid_max_x = i.width / 2 + cbwidth * tolerance / 2
    right_x = cbwidth * tolerance
    max_x = i.width - 5

    min_y = 5
    top_y = i.height - cbheight * tolerance
    mid_min_y = i.height / 2 - cbheight * tolerance / 2
    mid_max_y = i.height / 2 + cbheight * tolerance / 2
    bottom_y = cbheight * tolerance
    max_y = i.height - 5

    result = False

    grid = (
      #outline the top left corner
      ((min_x, min_y), (right_x, bottom_y)),
      #top middle
      ((mid_min_x, min_y), (mid_max_x, bottom_y)),
      #top right
      ((left_x, min_y), (max_x, bottom_y)),
      #right side middle
      ((left_x, mid_min_y), (max_x, mid_max_y)),
      # center
      ((mid_min_x, mid_min_y), (mid_max_x, mid_max_y)),
      #left middle
      ((min_x, mid_min_y), (right_x, mid_max_y)),
      #left bottom corner
      ((min_x, top_y), (right_x, max_y)),
      #bottom middle,
      ((mid_min_x, top_y), (mid_max_x, max_y)),
      #right bottom
      ((left_x, top_y), (max_x, max_y)) )

    testrect(i, cb, calibration_set, dims, grid[lcs % len(grid)])




def main(camindex = 0, capture_width = 800, capture_height = 600, chessboard_width = 8, chessboard_height = 5, planemode = False, gridsize = 0.029, calibrationFile = "default"):
    global save_location

    if planemode:
        mode = 7
    else:
        mode = 0

    dims = (chessboard_width, chessboard_height)

    cam = Camera(camindex, prop_set = { "width": capture_width, "height": capture_height })
    d = Display((capture_width, capture_height))

    save_location = "" #change this if you want to save your calibration images

    calibration_set = [] #calibration images
    fc_set = []

    introMessage()


    while not d.isDone():
        time.sleep(0.01)
        i = cam.getImage().flipHorizontal()
        cb = i.findChessboard(dims, subpixel = False)


        if cb:
            cb = cb[0]
        elif mode != 6:
            showText(i, "Put a chessboard in the green outline")

        if mode == 0:  #10 pictures, chessboard filling 80% of the view space
            findLargeFlat(cb, i, calibration_set, dims)
            if (len(calibration_set) == 10):
                mode = 1
        elif mode == 1:  #5 pictures, chessboard filling 60% of screen, at 45 deg horiz
            findHorizTilted(cb, i, calibration_set, dims)
            if (len(calibration_set) == 15):
                mode = 2
        elif mode == 2:  #5 pictures, chessboard filling 60% of screen, at 45 deg vert
            findVertTilted(cb, i, calibration_set, dims)
            if (len(calibration_set) == 20):
                mode = 3
        elif mode == 3:  #5 pictures, chessboard filling 40% of screen, corners at 45
            findCornerTilted(cb, i, calibration_set, dims)
            if (len(calibration_set) == 25):
                mode = 4
        elif mode == 4:  #10 pictures, chessboard filling 12% - 25% of view space
            findSmallFlat(cb, i, calibration_set, dims)
            if (len(calibration_set) == 35):
                mode = 5
        elif mode == 5:
            cam.calibrate(calibration_set, gridsize, dims)
            cam.saveCalibration(calibrationFile)
            mode = 6
        elif mode == 6:
            showText(i,  "Saved calibration to " + calibrationFile)
        elif mode == 7:
            findPlane(cb, i, calibration_set, dims)
            if (len(calibration_set) == 25):
                mode = 5

        if cb:
            cb.draw()

        i.save(d)

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description = "Create calibration files for your camera")

    parser.add_argument("--camera", type=int, help="id of the camera", default = 0)
    parser.add_argument("--capturewidth",  type=int, help="width of image to capture", default = 800)
    parser.add_argument("--captureheight",  type=int, help="height of image to capture", default = 600)
    parser.add_argument("--width",  type=int, help="number of chessboard squares wide", default = 8)
    parser.add_argument("--height",  type=int, help="number of chessboard squares high", default = 5)
    parser.add_argument("--planemode", action="store_true", help="calibrate on a single 2D plane", default = False)
    parser.add_argument("--gridsize",  type=float, help="chessboard grid size in real units", default = 0.029)
    parser.add_argument("--calibrationfile", type=str, help="filename to output calibration", default = "default")

    args = parser.parse_args()
    main(args.camera, args.capturewidth, args.captureheight, args.width, args.height, args.planemode, args.gridsize, args.calibrationfile)

########NEW FILE########
__FILENAME__ = CAMShiftTracker
from SimpleCV.base import np
try:
    import cv2
except ImportError:
    pass

def camshiftTracker(img, bb, ts, **kwargs):
    """
    **DESCRIPTION**
    
    (Dev Zone)

    Tracking the object surrounded by the bounding box in the given
    image using CAMshift method.

    Warning: Use this if you know what you are doing. Better have a 
    look at Image.track()

    **PARAMETERS**

    * *img* - Image - Image to be tracked.
    * *bb*  - tuple - Bounding Box tuple (x, y, w, h)
    * *ts*  - TrackSet - SimpleCV.Features.TrackSet.

    Optional PARAMETERS:

    lower      - Lower HSV value for inRange thresholding
                 tuple of (H, S, V)
                
    upper      - Upper HSV value for inRange thresholding
                 tuple of (H, S, V)

    mask       - Mask to calculate Histogram. It's better 
                 if you don't provide one.

    num_frames - number of frames to be backtracked.

    **RETURNS**

    SimpleCV.Features.Tracking.CAMShift

    **HOW TO USE**

    >>> cam = Camera()
    >>> ts = []
    >>> img = cam.getImage()
    >>> bb = (100, 100, 300, 300) # get BB from somewhere
    >>> ts = CAMShiftTracker(img, bb, ts, lower=(40, 120, 120), upper=(80, 200, 200), num_frames=30)
    >>> while (some_condition_here):
        ... img = cam.getImage()
        ... bb = ts[-1].bb
        ... ts = CAMShiftTracker(img, bb, ts, lower=(40, 120, 120), upper=(80, 200, 200), num_frames=30)
        ... ts[-1].drawBB()
        ... img.show()

    This is too much confusing. Better use
    Image.track() method.

    READ MORE:

    CAMShift Tracker:
    Uses meanshift based CAMShift thresholding technique. Blobs and objects with
    single tone or tracked very efficiently. CAMshift should be preferred if you 
    are trying to track faces. It is optimized to track faces.
    """

    lower = np.array((0., 60., 32.))
    upper = np.array((180., 255., 255.))
    mask = None
    num_frames = 40

    if not isinstance(bb, tuple):
        bb = tuple(bb)
    bb = (int(bb[0]), int(bb[1]), int(bb[2]), int(bb[3]))

    for key in kwargs:
        if key == 'lower':
            lower = np.array(tuple(kwargs[key]))
        elif key == 'upper':
            upper = np.array(tuple(kwargs[key]))
        elif key == 'mask':
            mask = kwargs[key]
            mask = mask.getNumpyCv2()
        elif key == 'num_frames':
            num_frames = kwargs[key]

    hsv = cv2.cvtColor(img.getNumpyCv2(), cv2.cv.CV_BGR2HSV)
    if mask is None:
        mask = cv2.inRange(hsv, lower, upper)

    x0, y0, w, h = bb
    x1 = x0 + w -1
    y1 = y0 + h -1
    hsv_roi = hsv[y0:y1, x0:x1]
    mask_roi = mask[y0:y1, x0:x1]

    hist = cv2.calcHist( [hsv_roi], [0], mask_roi, [16], [0, 180] )
    cv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX);
    hist_flat = hist.reshape(-1)
    imgs = [hsv]
    if len(ts) > num_frames and num_frames > 1:
        for feat in ts[-num_frames:]:
            imgs.append(feat.image.toHSV().getNumpyCv2())
    elif len(ts) < num_frames and num_frames > 1:
        for feat in ts:
            imgs.append(feat.image.toHSV().getNumpyCv2())

    prob = cv2.calcBackProject(imgs, [0], hist_flat, [0, 180], 1)
    prob &= mask
    term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )
    new_ellipse, track_window = cv2.CamShift(prob, bb, term_crit)
    if track_window[2] == 0 or track_window[3] == 0:
        track_window = bb
    track = CAMShiftTrack(img, track_window, new_ellipse)

    return track

from SimpleCV.Tracking import CAMShiftTrack

########NEW FILE########
__FILENAME__ = LKTracker
from SimpleCV.base import np, itertools
try:
    import cv2
except ImportError:
    pass

def lkTracker(img, bb, ts, oldimg, **kwargs):
    """
    **DESCRIPTION**
    
    (Dev Zone)

    Tracking the object surrounded by the bounding box in the given
    image using Lucas Kanade based Optical Flow method.

    Warning: Use this if you know what you are doing. Better have a 
    look at Image.track()

    **PARAMETERS**

    * *img* - Image - Image to be tracked.
    * *bb*  - tuple - Bounding Box tuple (x, y, w, h)
    * *ts*  - TrackSet - SimpleCV.Features.TrackSet.
    * *oldimg* - Image - Previous Image.

    Optional PARAMETERS:
    (docs from http://docs.opencv.org/)

    maxCorners    - Maximum number of corners to return in goodFeaturesToTrack. 
                    If there are more corners than are found, the strongest of 
                    them is returned.
                
    qualityLevel  - Parameter characterizing the minimal accepted quality of image corners. 
                    The parameter value is multiplied by the best corner quality measure, 
                    which is the minimal eigenvalue or the Harris function response. 
                    The corners with the quality measure less than the product are rejected.
                    For example, if the best corner has the quality measure = 1500, 
                    and the qualityLevel=0.01 , then all the corners with the quality measure 
                    less than 15 are rejected. 
                  
    minDistance   - Minimum possible Euclidean distance between the returned corners.

    blockSize     - Size of an average block for computing a derivative covariation matrix over each pixel neighborhood.

    winSize       - size of the search window at each pyramid level.

    maxLevel      - 0-based maximal pyramid level number; if set to 0, pyramids are not used (single level), 
                    if set to 1, two levels are used, and so on

    **RETURNS**

    SimpleCV.Features.Tracking.LKTracker

    **HOW TO USE**

    >>> cam = Camera()
    >>> ts = []
    >>> img = cam.getImage()
    >>> bb = (100, 100, 300, 300) # get BB from somewhere
    >>> ts = lkTracker(img, bb, ts, img, maxCorners=4000, qualityLevel=0.5, winSize=(12,12))
    >>> while (some_condition_here):
        ... img = cam.getImage()
        ... bb = ts[-1].bb
        ... prevImg = img
        ... ts = lkTracker(img, bb, ts, prevImg, maxCorners=4000, qualityLevel=0.5, winSize=(12,12))
        ... ts[-1].drawBB()
        ... img.show()

    This is too much confusing. Better use
    Image.track() method.

    READ MORE:

    LK (Lucas Kanade) Tracker:
    It is based on LK Optical Flow. It calculates Optical flow in frame1 to frame2 
    and also in frame2 to frame1 and using back track error, filters out false
    positives.
    """
    maxCorners = 4000
    qualityLevel = 0.08
    minDistance = 2
    blockSize = 3
    winSize = (10, 10)
    maxLevel = 10
    for key in kwargs:
        if key == 'maxCorners':
            maxCorners = kwargs[key]
        elif key == 'quality':
            qualityLevel = kwargs[key]
        elif key == 'minDistance':
            minDistance = kwargs[key]
        elif key == 'blockSize':
            blockSize = kwargs[key]
        elif key == 'winSize':
            winSize = kwargs[key]
        elif key == maxLevel:
            maxLevel = kwargs[key]

    bb = (int(bb[0]), int(bb[1]), int(bb[2]), int(bb[3]))
    img1 = img.crop(bb[0],bb[1],bb[2],bb[3])
    g = img1.getGrayNumpyCv2()
    pt = cv2.goodFeaturesToTrack(g, maxCorners = maxCorners, qualityLevel = qualityLevel,
                                minDistance = minDistance, blockSize = blockSize)
    if type(pt) == type(None):
        print "no points"
        track = LK(img, bb, pt)
        return track

    for i in xrange(len(pt)):
        pt[i][0][0] = pt[i][0][0]+bb[0]
        pt[i][0][1] = pt[i][0][1]+bb[1]

    p0 = np.float32(pt).reshape(-1, 1, 2)
    oldg = oldimg.getGrayNumpyCv2()
    newg = img.getGrayNumpyCv2()
    p1, st, err = cv2.calcOpticalFlowPyrLK(oldg, newg, p0, None, winSize  = winSize,
                                           maxLevel = maxLevel,
                                           criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))
    p0r, st, err = cv2.calcOpticalFlowPyrLK(newg, oldg, p1, None, winSize  = winSize,
                                            maxLevel = maxLevel,
                                            criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))

    d = abs(p0-p0r).reshape(-1, 2).max(-1)
    good = d < 1
    new_pts=[]
    for pts, val in itertools.izip(p1, good):
        if val:
            new_pts.append([pts[0][0], pts[0][1]])
    if ts[-1:]:
        old_pts = ts[-1].pts
        if type(old_pts) == type(None):
            old_pts = new_pts
    else:
        old_pts = new_pts
    dx=[]
    dy=[]
    for p1, p2 in itertools.izip(old_pts, new_pts):
        dx.append(p2[0]-p1[0])
        dy.append(p2[1]-p1[1])

    if not dx or not dy:
        track = LK(img, bb, new_pts)
        return track

    cen_dx = round(sum(dx)/len(dx))/3
    cen_dy = round(sum(dy)/len(dy))/3

    bb1 = [bb[0]+cen_dx, bb[1]+cen_dy, bb[2], bb[3]]
    if bb1[0] <= 0:
        bb1[0] = 1
    if bb1[0]+bb1[2] >= img.width:
        bb1[0] = img.width - bb1[2] - 1
    if bb1[1]+bb1[3] >= img.height:
        bb1[1] = img.height - bb1[3] - 1
    if bb1[1] <= 0:
        bb1[1] = 1

    track = LKTrack(img, bb1, new_pts)    
    return track

from SimpleCV.Tracking import LKTrack

########NEW FILE########
__FILENAME__ = MFTracker
from SimpleCV.base import np, cv, math, time, spsd
from copy import copy
try:
    import cv2
except ImportError:
    pass

def mfTracker(img, bb, ts, oldimg, **kwargs):
    """
    **DESCRIPTION**
    
    (Dev Zone)

    Tracking the object surrounded by the bounding box in the given
    image using SURF keypoints.

    Warning: Use this if you know what you are doing. Better have a 
    look at Image.track()

    **PARAMETERS**

    * *img* - Image - Image to be tracked.
    * *bb*  - tuple - Bounding Box tuple (x, y, w, h)
    * *oldimg* - Image - Previous Image
    * *ts*  - TrackSet - SimpleCV.Features.TrackSet.

    Optional PARAMETERS:

    numM     - Number of points to be tracked in the bounding box
               in height direction. 
                
    numN     - Number of points to be tracked in the bounding box
               in width direction. 
                  
    margin   - Margin around the bounding box.

    winsize_lk  - Optical Flow search window size.

    winsize - Size of quadratic area around the point which is compared.

    **RETURNS**

    SimpleCV.Features.Tracking.MFTracker

    **HOW TO USE**

    >>> cam = Camera()
    >>> ts = []
    >>> img = cam.getImage()
    >>> bb = (100, 100, 300, 300) # get BB from somewhere
    >>> ts = MFTracker(img, bb, ts, img, numM=15, numN=15, winsize=12)
    >>> while (some_condition_here):
        ... img = cam.getImage()
        ... bb = ts[-1].bb
        ... prevImg = img
        ... ts = MFTracker(img, bb, ts, img, numM=15, numN=15, winsize=12)
        ... ts[-1].drawBB()
        ... img.show()

    This is too much confusing. Better use
    Image.track() method.

    READ MORE:

    Median Flow Tracker:
    
    Media Flow Tracker is the base tracker that is used in OpenTLD. It is based on
    Optical Flow. It calculates optical flow of the points in the bounding box from
    frame 1 to frame 2 and from frame 2 to frame 1 and using back track error, removes
    false positives. As the name suggests, it takes the median of the flow, and eliminates
    points.
    """
    numM = 10
    numN = 10
    margin = 5
    winsize_ncc = 10
    winsize_lk = 4

    for key in kwargs:
        if key == 'numM':
            numM = kwargs[key]
        elif key == 'numN':
            numN = kwargs[key]
        elif key == 'margin':
            margin = kwargs[key]
        elif key == 'winsize':
            winsize_ncc = kwargs[key]
        elif key == 'winsize_lk':
            winsize_lk = kwargs[key]

    oldg = oldimg.getGrayNumpyCv2()
    newg = img.getGrayNumpyCv2()
    bb = [bb[0], bb[1], bb[0]+bb[2], bb[1]+bb[3]]
    bb, shift = fbtrack(oldg, newg, bb, numM, numN, margin, winsize_ncc, winsize_lk)
    bb = [bb[0], bb[1], bb[2]-bb[0], bb[3]-bb[1]]
    track = MFTrack(img, bb, shift)
    return track

def fbtrack(imgI, imgJ, bb, numM=10, numN=10,margin=5,winsize_ncc=10, winsize_lk=4):
    """
    **SUMMARY**
    (Dev Zone)
    Forward-Backward tracking using Lucas-Kanade Tracker
    
    **PARAMETERS**
    
    imgI - Image contain Object with known BoundingBox (Numpy array)
    imgJ - Following image (Numpy array)
    bb - Bounding box represented through 2 points (x1,y1,x2,y2)
    numM - Number of points in height direction.
    numN - Number of points in width direction.
    margin - margin (in pixel)
    winsize_ncc - Size of quadratic area around the point which is compared.
    
    **RETURNS**
    
    newbb - Bounding box of object in track in imgJ
    scaleshift - relative scale change of bb
    
    """

    nPoints = numM*numN
    sizePointsArray = nPoints*2
    #print bb, "passed in fbtrack"
    pt = getFilledBBPoints(bb, numM, numN, margin)
    fb, ncc, status, ptTracked = lktrack(imgI, imgJ, pt, nPoints, winsize_ncc, winsize_lk)

    nlkPoints = sum(status)[0]
    
    startPoints = []
    targetPoints = []
    fbLKCleaned = [0.0]*nlkPoints
    nccLKCleaned = [0.0]*nlkPoints
    M = 2
    nRealPoints = 0
    
    for i in range(nPoints):
        if ptTracked[M*i] is not -1:
            startPoints.append((pt[2 * i],pt[2*i+1]))
            targetPoints.append((ptTracked[2 * i], ptTracked[2 * i + 1]))
            fbLKCleaned[nRealPoints]=fb[i]
            nccLKCleaned[nRealPoints]=ncc[i]
            nRealPoints+=1
            
    medFb = getMedian(fbLKCleaned)
    medNcc = getMedian(nccLKCleaned)
    
    nAfterFbUsage = 0
    for i in range(nlkPoints):
        if fbLKCleaned[i] <= medFb and nccLKCleaned[i] >= medNcc:
            startPoints[nAfterFbUsage] = startPoints[i]
            targetPoints[nAfterFbUsage] = targetPoints[i]
            nAfterFbUsage+=1

    newBB, scaleshift = predictBB(bb, startPoints, targetPoints, nAfterFbUsage)
    #print newBB, "fbtrack passing newBB"
    return (newBB, scaleshift)

def lktrack(img1, img2, ptsI, nPtsI, winsize_ncc=10, win_size_lk=4, method=cv2.cv.CV_TM_CCOEFF_NORMED):
    """
    **SUMMARY**
    (Dev Zone)
    Lucas-Kanede Tracker with pyramids
    
    **PARAMETERS**
    
    img1 - Previous image or image containing the known bounding box (Numpy array)
    img2 - Current image
    ptsI - Points to track from the first image
           Format ptsI[0] - x1, ptsI[1] - y1, ptsI[2] - x2, ..
    nPtsI - Number of points to track from the first image
    winsize_ncc - size of the search window at each pyramid level in LK tracker (in int)
    method - Paramete specifying the comparison method for normalized cross correlation 
             (see http://opencv.itseez.com/modules/imgproc/doc/object_detection.html?highlight=matchtemplate#cv2.matchTemplate)
    
    **RETURNS**
    
    fb - forward-backward confidence value. (corresponds to euclidean distance between).
    ncc - normCrossCorrelation values
    status - Indicates positive tracks. 1 = PosTrack 0 = NegTrack
    ptsJ - Calculated Points of second image
    
    """ 
    template_pt = []
    target_pt = []
    fb_pt = []
    ptsJ = [-1]*len(ptsI)
    
    for i in range(nPtsI):
        template_pt.append((ptsI[2*i],ptsI[2*i+1]))
        target_pt.append((ptsI[2*i],ptsI[2*i+1]))
        fb_pt.append((ptsI[2*i],ptsI[2*i+1]))
    
    template_pt = np.asarray(template_pt,dtype="float32")
    target_pt = np.asarray(target_pt,dtype="float32")
    fb_pt = np.asarray(fb_pt,dtype="float32")
    
    target_pt, status, track_error = cv2.calcOpticalFlowPyrLK(img1, img2, template_pt, target_pt, 
                                     winSize=(win_size_lk, win_size_lk), flags = cv2.OPTFLOW_USE_INITIAL_FLOW,
                                     criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))
                                     
    fb_pt, status_bt, track_error_bt = cv2.calcOpticalFlowPyrLK(img2,img1, target_pt,fb_pt, 
                                       winSize = (win_size_lk,win_size_lk),flags = cv2.OPTFLOW_USE_INITIAL_FLOW,
                                       criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))
    
    status = status & status_bt
    ncc = normCrossCorrelation(img1, img2, template_pt, target_pt, status, winsize_ncc, method)
    fb = euclideanDistance(template_pt, target_pt)
    
    newfb = -1*np.ones(len(fb))
    newncc = -1*np.ones(len(ncc))
    for i in np.argwhere(status):
        i = i[0]
        ptsJ[2 * i] = target_pt[i][0]
        ptsJ[2 * i + 1] = target_pt[i][1]
        newfb[i] = fb[i]
        newncc[i] = ncc[i]

    return newfb, newncc, status, ptsJ

def getMedianUnmanaged(a):
    if not a:
        return None
    newl = copy(a)
    newl.sort()
    while True:
        try:
            newl.remove(0)
        except ValueError:
            if newl:
                return newl[len(newl)/2]
            return 0

def getMedian(a):
    median = getMedianUnmanaged(a)
    return median

def calculateBBCenter(bb):
    """
    
    **SUMMARY**
    (Dev Zone)
    Calculates the center of the given bounding box
    
    **PARAMETERS**
    
    bb - Bounding Box represented through 2 points (x1,y1,x2,y2)
    
    **RETURNS**
    
    center - A tuple of two floating points
    
    """
    center = (0.5*(bb[0] + bb[2]),0.5*(bb[1]+bb[3]))
    return center
    
def getFilledBBPoints(bb, numM, numN, margin):
    """
    
    **SUMMARY**
    (Dev Zone)
    Creates numM x numN points grid on Bounding Box
    
    **PARAMETERS**
    
    bb - Bounding Box represented through 2 points (x1,y1,x2,y2)
    numM - Number of points in height direction.
    numN - Number of points in width direction.
    margin - margin (in pixel)
    
    **RETURNS**
    
    pt - A list of points (pt[0] - x1, pt[1] - y1, pt[2] - x2, ..)
    
    """
    pointDim = 2
    bb_local = (bb[0] + margin, bb[1] + margin, bb[2] - margin, bb[3] - margin)
    if numM == 1 and numN == 1 :
        pts = calculateBBCenter(bb_local)
        return pts
    
    elif numM > 1 and numN == 1:
        divM = numM - 1
        divN = 2
        spaceM = (bb_local[3]-bb_local[1])/divM
        center = calculateBBCenter(bb_local)
        pt = [0.0]*(2*numM*numN)
        for i in range(numN):
            for j in range(numM):
                pt[i * numM * pointDim + j * pointDim + 0] = center[0]
                pt[i * numM * pointDim + j * pointDim + 1] = bb_local[1] + j * spaceM
                
        return pt
        
    elif numM == 1 and numN > 1:
        divM = 2
        divN = numN - 1
        spaceN = (bb_local[2] - bb_local[0]) / divN
        center = calculateBBCenter(bb_local)
        pt = [0.0]*((numN-1)*numM*pointDim+numN*pointDim)
        for i in range(numN):
            for j in range(numN):
                pt[i * numM * pointDim + j * pointDim + 0] = bb_local[0] + i * spaceN
                pt[i * numM * pointDim + j * pointDim + 1] = center[1]
        return pt
        
    elif numM > 1 and numN > 1:
        divM = numM - 1
        divN = numN - 1
    
    spaceN = (bb_local[2] - bb_local[0]) / divN
    spaceM = (bb_local[3] - bb_local[1]) / divM

    pt = [0.0]*((numN-1)*numM*pointDim+numM*pointDim)
    
    for i in range(numN):
        for j in range(numM):
            pt[i * numM * pointDim + j * pointDim + 0] = float(bb_local[0] + i * spaceN)
            pt[i * numM * pointDim + j * pointDim + 1] = float(bb_local[1] + j * spaceM)
    return pt

def getBBWidth(bb):
    """
    
    **SUMMARY**
    (Dev Zone)
    Get width of the bounding box
    
    **PARAMETERS**
    
    bb - Bounding Box represented through 2 points (x1,y1,x2,y2)
    
    **RETURNS**
    
    width of the bounding box
    
    """
    return bb[2]-bb[0]+1
    
def getBBHeight(bb):
    """
    
    **SUMMARY**
    (Dev Zone)
    Get height of the bounding box
    
    **PARAMETERS**
    
    bb - Bounding Box represented through 2 points (x1,y1,x2,y2)
    
    **RETURNS**
    
    height of the bounding box
    """
    return bb[3]-bb[1]+1
    
def predictBB(bb0, pt0, pt1, nPts):
    """
    
    **SUMMARY**
    (Dev Zone)
    Calculates the new (moved and resized) Bounding box.
    Calculation based on all relative distance changes of all points
    to every point. Then the Median of the relative Values is used.
    
    **PARAMETERS**
    
    bb0 - Bounding Box represented through 2 points (x1,y1,x2,y2)
    pt0 - Starting Points
    pt1 - Target Points
    nPts - Total number of points (eg. len(pt0))
    
    **RETURNS**
    
    bb1 - new bounding box
    shift - relative scale change of bb0
    
    """
    ofx = []
    ofy = []
    for i in range(nPts):
        ofx.append(pt1[i][0]-pt0[i][0])
        ofy.append(pt1[i][1]-pt0[i][1])
    
    dx = getMedianUnmanaged(ofx)
    dy = getMedianUnmanaged(ofy)
    ofx=ofy=0
    
    lenPdist = nPts * (nPts - 1) / 2
    dist0=[]
    for i in range(nPts):
        for j in range(i+1,nPts):
            temp0 = ((pt0[i][0] - pt0[j][0])**2 + (pt0[i][1] - pt0[j][1])**2)**0.5
            temp1 = ((pt1[i][0] - pt1[j][0])**2 + (pt1[i][1] - pt1[j][1])**2)**0.5
            if temp0 != 0:
                dist0.append(float(temp1)/temp0)
            else:
                dist0.append(1.0)
            
    shift = getMedianUnmanaged(dist0)
    if shift is None:
        return(bb0, 1.0)

    # too much variation in shift is due to some errors
    if shift > 1.1 or shift < 0.9:
        shift = 1
    s0 = 0.5 * (shift - 1) * getBBWidth(bb0)
    s1 = 0.5 * (shift - 1) * getBBHeight(bb0)
    
    
    x1 = bb0[0] - s0 + dx
    y1 = bb0[1] - s1 + dy
    x2 = bb0[2] + s0 + dx
    y2 = bb0[3] + s1 + dy
    w = x2-x1
    h = y2-y1
    #print x1,x2,y1,y2,w,h
    if x1 <= 0 or x2 <=0 or y1<=0 or y2 <=0 or w <=20 or h <=20:
        x1 = bb0[0]
        y1 = bb0[1]
        x2 = bb0[2]
        y2 = bb0[3]
        
    bb1 = (int(x1),int(y1),int(x2),int(y2))
              
    return (bb1, shift)
    
def getBB(pt0,pt1):
    xmax = np.max((pt0[0],pt1[0]))
    xmin = np.min((pt0[0],pt1[0]))
    ymax = np.max((pt0[1],pt1[1]))
    ymin = np.min((pt0[1],pt1[1]))
    return xmin,ymin,xmax,ymax
    
def getRectFromBB(bb):
    return bb[0],bb[1],bb[2]-bb[0],bb[3]-bb[1]
    
def euclideanDistance(point1,point2):
    """
    (Dev Zone)
    **SUMMARY**
    
    Calculates eculidean distance between two points
    
    **PARAMETERS**
    
    point1 - vector of points
    point2 - vector of points with same length
    
    **RETURNS**
    
    match = returns a vector of eculidean distance
    """
    match = ((point1[:,0]-point2[:,0])**2+(point1[:,1]-point2[:,1])**2)**0.5
    return match

def normCrossCorrelation(img1, img2, pt0, pt1, status, winsize, method=cv2.cv.CV_TM_CCOEFF_NORMED):
    """
    **SUMMARY**
    (Dev Zone)
    Calculates normalized cross correlation for every point.
    
    **PARAMETERS**
    
    img1 - Image 1.
    img2 - Image 2.
    pt0 - vector of points of img1
    pt1 - vector of points of img2
    status - Switch which point pairs should be calculated.
             if status[i] == 1 => match[i] is calculated.
             else match[i] = 0.0
    winsize- Size of quadratic area around the point
             which is compared.
    method - Specifies the way how image regions are compared. see cv2.matchTemplate
    
    **RETURNS**
    
    match - Output: Array will contain ncc values.
            0.0 if not calculated.
 
    """
    nPts = len(pt0)
    match = np.zeros(nPts)
    for i in np.argwhere(status):
        i = i[0]
        patch1 = cv2.getRectSubPix(img1,(winsize,winsize),tuple(pt0[i]))
        patch2 = cv2.getRectSubPix(img2,(winsize,winsize),tuple(pt1[i]))
        match[i] = cv2.matchTemplate(patch1,patch2,method)
    return match

from SimpleCV.Tracking import MFTrack

########NEW FILE########
__FILENAME__ = SURFTracker
from SimpleCV.base import np, itertools
try:
    import cv2
except ImportError:
    pass

def surfTracker(img, bb, ts, **kwargs):
    """
    **DESCRIPTION**
    
    (Dev Zone)

    Tracking the object surrounded by the bounding box in the given
    image using SURF keypoints.

    Warning: Use this if you know what you are doing. Better have a 
    look at Image.track()

    **PARAMETERS**

    * *img* - Image - Image to be tracked.
    * *bb*  - tuple - Bounding Box tuple (x, y, w, h)
    * *ts*  - TrackSet - SimpleCV.Features.TrackSet.

    Optional PARAMETERS:

    eps_val     - eps for DBSCAN
                  The maximum distance between two samples for them 
                  to be considered as in the same neighborhood. 
                
    min_samples - min number of samples in DBSCAN
                  The number of samples in a neighborhood for a point 
                  to be considered as a core point. 
                  
    distance    - thresholding KNN distance of each feature
                  if KNN distance > distance, point is discarded.

    **RETURNS**

    SimpleCV.Features.Tracking.SURFTracker

    **HOW TO USE**

    >>> cam = Camera()
    >>> ts = []
    >>> img = cam.getImage()
    >>> bb = (100, 100, 300, 300) # get BB from somewhere
    >>> ts = surfTracker(img, bb, ts, eps_val=0.7, distance=150)
    >>> while (some_condition_here):
        ... img = cam.getImage()
        ... bb = ts[-1].bb
        ... ts = surfTracker(img, bb, ts, eps_val=0.7, distance=150)
        ... ts[-1].drawBB()
        ... img.show()

    This is too much confusing. Better use
    Image.track() method.

    READ MORE:

    SURF based Tracker:
    Matches keypoints from the template image and the current frame.
    flann based matcher is used to match the keypoints.
    Density based clustering is used classify points as in-region (of bounding box)
    and out-region points. Using in-region points, new bounding box is predicted using
    k-means.
    """
    eps_val = 0.69
    min_samples = 5
    distance = 100

    for key in kwargs:
        if key == 'eps_val':
            eps_val = kwargs[key]
        elif key == 'min_samples':
            min_samples = kwargs[key]
        elif key == 'dist':
            distance = kwargs[key]

    from scipy.spatial import distance as Dis
    from sklearn.cluster import DBSCAN

    if len(ts) == 0:
        # Get template keypoints
        bb = (int(bb[0]), int(bb[1]), int(bb[2]), int(bb[3]))
        templateImg = img
        detector = cv2.FeatureDetector_create("SURF")
        descriptor = cv2.DescriptorExtractor_create("SURF")

        templateImg_cv2 = templateImg.getNumpyCv2()[bb[1]:bb[1]+bb[3], bb[0]:bb[0]+bb[2]]
        tkp = detector.detect(templateImg_cv2)
        tkp, td = descriptor.compute(templateImg_cv2, tkp)

    else:
        templateImg = ts[-1].templateImg
        tkp = ts[-1].tkp
        td = ts[-1].td
        detector = ts[-1].detector
        descriptor = ts[-1].descriptor

    newimg = img.getNumpyCv2()

    # Get image keypoints
    skp = detector.detect(newimg)
    skp, sd = descriptor.compute(newimg, skp)

    if td is None:
        print "Descriptors are Empty"
        return None

    if sd is None:
        track = SURFTracker(img, skp, detector, descriptor, templateImg, skp, sd, tkp, td)
        return track

    # flann based matcher
    flann_params = dict(algorithm=1, trees=4)
    flann = cv2.flann_Index(sd, flann_params)
    idx, dist = flann.knnSearch(td, 1, params={})
    del flann

    # filter points using distnace criteria
    dist = (dist[:,0]/2500.0).reshape(-1,).tolist()
    idx = idx.reshape(-1).tolist()
    indices = sorted(range(len(dist)), key=lambda i: dist[i])

    dist = [dist[i] for i in indices]
    idx = [idx[i] for i in indices]
    skp_final = []
    skp_final_labelled=[]
    data_cluster=[]
    
    for i, dis in itertools.izip(idx, dist):
        if dis < distance:
            skp_final.append(skp[i])
            data_cluster.append((skp[i].pt[0], skp[i].pt[1]))

    #Use Denstiy based clustering to further fitler out keypoints
    n_data = np.asarray(data_cluster)
    D = Dis.squareform(Dis.pdist(n_data))
    S = 1 - (D/np.max(D))
    
    db = DBSCAN(eps=eps_val, min_samples=min_samples).fit(S)
    core_samples = db.core_sample_indices_
    labels = db.labels_
    for label, i in zip(labels, range(len(labels))):
        if label==0:
            skp_final_labelled.append(skp_final[i])

    track = SURFTrack(img, skp_final_labelled, detector, descriptor, templateImg, skp, sd, tkp, td)

    return track

from SimpleCV.Tracking import SURFTrack

########NEW FILE########
__FILENAME__ = TrackClass
from SimpleCV.Color import Color
from SimpleCV.base import time, np
from SimpleCV.Features.Features import Feature, FeatureSet
try:
    import cv2
except ImportError:
    pass

class Track(Feature):
    """
    **SUMMARY**

    Track class is the base of tracking. All different tracking algorithm
    return different classes but they all belong to Track class. All the
    common attributes are kept in this class

    """
    def __init__(self, img, bb):
        """
        **SUMMARY**

        Initializes all the required parameters and attributes of the class.

        **PARAMETERS**

        * *img* - SimpleCV.ImageClass.Image
        * *bb* - A tuple consisting of (x, y, w, h) of the bounding box

        **RETURNS**

        SimpleCV.Tracking.TrackClass.Track object

        **EXAMPLE**

        >>> track = Track(image, bb)
        """
        self.bb = bb
        self.image = img
        self.bb_x, self.bb_y, self.w, self.h = self.bb
        self.x, self.y = self.center = self.getCenter()
        self.sizeRatio = 1
        self.vel = (0,0)
        self.rt_vel = (0,0)
        self.area = self.getArea()
        self.time = time.time()
        self.cv2numpy = self.image.getNumpyCv2()
        return self

    def getCenter(self):
        """
        **SUMMARY**

        Get the center of the bounding box

        **RETURNS**

        * *tuple* - center of the bounding box (x, y)

        **EXAMPLE**

        >>> track = Track(img, bb)
        >>> cen = track.getCenter()
        """
        return (self.bb_x+self.w/2,self.bb_y+self.h/2)

    def getArea(self):
        """
        **SUMMARY**

        Get the area of the bounding box

        **RETURNS**

        Area of the bounding box

        **EXAMPLE**

        >>> track = Track(img, bb)
        >>> area = track.getArea()
        """
        return self.w*self.h

    def getImage(self):
        """
        **SUMMARY**

        Get the Image

        **RETURNS**

        SimpleCV.ImageClass.Image

        **EXAMPLE**

        >>> track = Track(img, bb)
        >>> i = track.getImage()
        """
        return self.image

    def getBB(self):
        """
        **SUMMARY**

        Get the bounding box

        **RETURNS**

        A tuple  - (x, y, w, h)

        **EXAMPLE**

        >>> track = Track(img, bb)
        >>> print track.getBB()
        """
        return self.bb

    def draw(self, color=Color.GREEN, rad=1, thickness=1):
        """
        **SUMMARY**

        Draw the center of the object on the image.

        **PARAMETERS**

        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *rad* - Radius of the circle to be plotted on the center of the object.
        * *thickness* - Thickness of the boundary of the center circle.

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> track = Track(img, bb)
        >>> track.draw()
        >>> img.show()
        """
        f = self
        f.image.drawCircle(f.center, rad, color, thickness)

    def drawBB(self, color=Color.GREEN, thickness=3):
        """
        **SUMMARY**

        Draw the bounding box over the object on the image.

        **PARAMETERS**

        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *thickness* - Thickness of the boundary of the bounding box.

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> track = Track(img, bb)
        >>> track.drawBB()
        >>> img.show()
        """
        f = self
        f.image.drawRectangle(f.bb_x, f.bb_y, f.w, f.h, color, thickness)

    def showCoordinates(self, pos=None, color=Color.GREEN, size=None):
        """
        **SUMMARY**

        Show the co-ordinates of the object in text on the Image.

        **PARAMETERS**
        * *pos* - A tuple consisting of x, y values. where to put to the text
        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *size* - Fontsize of the text

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> track = Track(img, bb)
        >>> track.showCoordinates()
        >>> img.show()
        """
        f = self
        img = f.image
        if not pos:
            imgsize = img.size()
            pos = (imgsize[0]-120, 10)
        if not size:
            size = 16
        text = "x = %d  y = %d" % (f.x, f.y)
        img.drawText(text, pos[0], pos[1], color, size)

    def showSizeRatio(self, pos=None, color=Color.GREEN, size=None):
        """
        **SUMMARY**

        Show the sizeRatio of the object in text on the image.

        **PARAMETERS**
        * *pos* - A tuple consisting of x, y values. where to put to the text
        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *size* - Fontsize of the text

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... ts[-1].showSizeRatio() # For continuous bounding box
            ... img = img1
        """
        f = self
        img = f.image
        if not pos:
            imgsize = img.size()
            pos = (imgsize[0]-120, 30)
        if not size:
            size = 16
        text = "size = %f" % (f.sizeRatio)
        img.drawText(text, pos[0], pos[1], color, size)

    def showPixelVelocity(self, pos=None, color=Color.GREEN, size=None):
        """
        **SUMMARY**

        Show the Pixel Veloctiy (pixel/frame) of the object in text on the image.

        **PARAMETERS**
        * *pos* - A tuple consisting of x, y values. where to put to the text
        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *size* - Fontsize of the text

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... ts[-1].showPixelVelocity() # For continuous bounding box
            ... img = img1
        """
        f = self
        img = f.image
        vel = f.vel
        if not pos:
            imgsize = img.size()
            pos = (imgsize[0]-120, 90)
        if not size:
            size = 16
        text = "Vx = %.2f Vy = %.2f" % (vel[0], vel[1])
        img.drawText(text, pos[0], pos[1], color, size)
        img.drawText("in pixels/frame", pos[0], pos[1]+size, color, size)

    def showPixelVelocityRT(self, pos=None, color=Color.GREEN, size=None):
        """
        **SUMMARY**

        Show the Pixel Veloctiy (pixels/second) of the object in text on the image.

        **PARAMETERS**
        * *pos* - A tuple consisting of x, y values. where to put to the text
        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *size* - Fontsize of the text

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... ts[-1].showPixelVelocityRT() # For continuous bounding box
            ... img = img1
        """
        f = self
        img = f.image
        vel_rt = f.vel_rt
        if not pos:
            imgsize = img.size()
            pos = (imgsize[0]-120, 50)
        if not size:
            size = 16
        text = "Vx = %.2f Vy = %.2f" % (vel_rt[0], vel_rt[1])
        img.drawText(text, pos[0], pos[1], color, size)
        img.drawText("in pixels/second", pos[0], pos[1]+size, color, size)

    def processTrack(self, func):
        """
        **SUMMARY**

        This method lets you use your own function on the current image.

        **PARAMETERS**
        * *func* - some user defined function for SimpleCV.ImageClass.Image object

        **RETURNS**

        the value returned by the user defined function

        **EXAMPLE**

        >>> def foo(img):
            ... return img.meanColor()
        >>> mean_color = ts[-1].processTrack(foo)
        """
        return func(self.image)

    def getPredictionPoints(self):
        """
        **SUMMARY**

        get predicted Co-ordinates of the center of the object

        **PARAMETERS**
        None

        **RETURNS**

        * *tuple*

        **EXAMPLE**

        >>> track = Track(img, bb)
        >>> track.getPredictedCoordinates()
        """
        return self.predict_pt

    def drawPredicted(self, color=Color.GREEN, rad=1, thickness=1):
        """
        **SUMMARY**

        Draw the center of the object on the image.

        **PARAMETERS**

        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *rad* - Radius of the circle to be plotted on the center of the object.
        * *thickness* - Thickness of the boundary of the center circle.

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> track = Track(img, bb)
        >>> track.drawPredicted()
        >>> img.show()
        """
        f = self
        f.image.drawCircle(f.predict_pt, rad, color, thickness)

    def showPredictedCoordinates(self, pos=None, color=Color.GREEN, size=None):
        """
        **SUMMARY**

        Show the co-ordinates of the object in text on the Image.

        **PARAMETERS**
        * *pos* - A tuple consisting of x, y values. where to put to the text
        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *size* - Fontsize of the text

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> track = Track(img, bb)
        >>> track.showPredictedCoordinates()
        >>> img.show()
        """
        f = self
        img = f.image
        if not pos:
            imgsize = img.size()
            pos = (5, 10)
        if not size:
            size = 16
        text = "Predicted: x = %d  y = %d" % (f.predict_pt[0], f.predict_pt[1])
        img.drawText(text, pos[0], pos[1], color, size)

    def getCorrectedPoints(self):
        """
        **SUMMARY**

        Corrected Co-ordinates of the center of the object

        **PARAMETERS**
        None

        **RETURNS**

        * *tuple*

        **EXAMPLE**

        >>> track = Track(img, bb)
        >>> track.getCorrectedCoordinates()
        """
        return self.state_pt

    def showCorrectedCoordinates(self, pos=None, color=Color.GREEN, size=None):
        """
        **SUMMARY**

        Show the co-ordinates of the object in text on the Image.

        **PARAMETERS**
        * *pos* - A tuple consisting of x, y values. where to put to the text
        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *size* - Fontsize of the text

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> track = Track(img, bb)
        >>> track.showCorrectedCoordinates()
        >>> img.show()
        """
        f = self
        img = f.image
        if not pos:
            imgsize = img.size()
            pos = (5, 40)
        if not size:
            size = 16
        text = "Corrected: x = %d  y = %d" % (f.state_pt[0], f.state_pt[1])
        img.drawText(text, pos[0], pos[1], color, size)

    def drawCorrected(self, color=Color.GREEN, rad=1, thickness=1):
        """
        **SUMMARY**

        Draw the center of the object on the image.

        **PARAMETERS**

        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *rad* - Radius of the circle to be plotted on the center of the object.
        * *thickness* - Thickness of the boundary of the center circle.

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> track = Track(img, bb)
        >>> track.drawCorrected()
        >>> img.show()
        """
        f = self
        f.image.drawCircle(f.state_pt, rad, color, thickness)

class CAMShiftTrack(Track):
    """
    **SUMMARY**

    CAMShift Class is returned by track when CAMShift tracking is required.
    This class is a superset of Track Class. And all of Track class'
    attributes can be accessed.

    CAMShift class has "ellipse" attribute which is not present in Track
    """
    def __init__(self, img, bb, ellipse):
        """
        **SUMMARY**

        Initializes all the required parameters and attributes of the CAMShift class.

        **PARAMETERS**

        * *img* - SimpleCV.ImageClass.Image
        * *bb* - A tuple consisting of (x, y, w, h) of the bounding box
        * ellipse* - A tuple

        **RETURNS**

        SimpleCV.Tracking.TrackClass.CAMShiftTrack object

        **EXAMPLE**

        >>> track = CAMShiftTrack(image, bb, ellipse)
        """
        self = Track.__init__(self, img, bb)
        self.ellipse = ellipse

    def getEllipse(self):
        """
        **SUMMARY**

        Returns the ellipse.

        **RETURNS**

        A tuple

        **EXAMPLE**

        >>> track = CAMShiftTrack(image, bb, ellipse)
        >>> e = track.getEllipse()
        """
        return self.ellipse

class LKTrack(Track):
    """
    **SUMMARY**

    LK Tracking class is used for Lucas-Kanade Track algorithm. It's
    derived from Track Class. Apart from all the properties of Track class,
    LK has few other properties. Since in LK tracking method, we obtain tracking
    points, we have functionalities to draw those points on the image.

    """

    def __init__(self, img, bb, pts):
        """
        **SUMMARY**

        Initializes all the required parameters and attributes of the class.

        **PARAMETERS**

        * *img* - SimpleCV.ImageClass.Image
        * *bb* - A tuple consisting of (x, y, w, h) of the bounding box
        * *pts* - List of all the tracking points

        **RETURNS**

        SimpleCV.Tracking.TrackClass.LKTrack object

        **EXAMPLE**

        >>> track = LKTrack(image, bb, pts)
        """

        self = Track.__init__(self, img, bb)
        self.pts = pts

    def getTrackedPoints(self):
        """
        **SUMMARY**

        Returns all the points which are being tracked.

        **RETURNS**

        A list

        **EXAMPLE**

        >>> track = LKTrack(image, bb, pts)
        >>> pts = track.getTrackedPoints()
        """
        return self.pts

    def drawTrackerPoints(self, color=Color.GREEN, radius=1, thickness=1):
        """
        **SUMMARY**

        Draw all the points which are being tracked.

        **PARAMETERS**
        * *color* - Color of the point
        * *radius* - Radius of the point
        *thickness* - thickness of the circle point

        **RETURNS**

        Nothing

        **EXAMPLE**

        >>> track = LKTrack(image, bb, pts)
        >>> track.drawTrackerPoints()
        """
        if type(self.pts) is not type(None):
            for pt in self.pts:
                self.image.drawCircle(ctr=pt, rad=radius, thickness=thickness, color=color)

class SURFTrack(Track):
    """
    **SUMMARY**

    SURFTracker class is used for SURF Based keypoints matching tracking algorithm.
    It's derived from Track Class. Apart from all the properties of Track class SURFTracker
    has few other properties.

    Matches keypoints from the template image and the current frame.
    flann based matcher is used to match the keypoints.
    Density based clustering is used classify points as in-region (of bounding box)
    and out-region points. Using in-region points, new bounding box is predicted using
    k-means.
    """
    def __init__(self, img, new_pts, detector, descriptor, templateImg, skp, sd, tkp, td):
        """
        **SUMMARY**

        Initializes all the required parameters and attributes of the class.

        **PARAMETERS**

        * *img* - SimpleCV.Image
        * *new_pts* - List of all the tracking points found in the image. - list of cv2.KeyPoint
        * *detector* - SURF detector - cv2.FeatureDetector
        * *descriptor* - SURF descriptor - cv2.DescriptorExtractor
        * *templateImg* - Template Image (First image) - SimpleCV.Image
        * *skp* - image keypoints - list of cv2.KeyPoint
        * *sd* - image descriptor - numpy.ndarray
        * *tkp* - Template Imaeg keypoints - list of cv2.KeyPoint
        * *td* - Template image descriptor - numpy.ndarray

        **RETURNS**

        SimpleCV.Tracking.TrackClass.SURFTrack object

        **EXAMPLE**
        >>> track = SURFTracker(image, pts, detector, descriptor, temp, skp, sd, tkp, td)
        """
        if td is None:
            bb = (1, 1, 1, 1)
            self = Track.__init__(self, img, bb)
            return
        if len(new_pts) < 1:
            bb = (1, 1, 1, 1)
            self = Track.__init__(self, img, bb)
            self.pts = None
            self.templateImg = templateImg
            self.skp = skp
            self.sd = sd
            self.tkp = tkp
            self.td = td
            self.detector = detector
            self.descriptor = descriptor
            return
        if sd is None:
            bb = (1, 1, 1, 1)
            self = Track.__init__(self, img, bb)
            self.pts = None
            self.templateImg = templateImg
            self.skp = skp
            self.sd = sd
            self.tkp = tkp
            self.td = td
            self.detector = detector
            self.descriptor = descriptor
            return

        np_pts = np.asarray([kp.pt for kp in new_pts])
        t, pts, center = cv2.kmeans(np.asarray(np_pts, dtype=np.float32), K=1, bestLabels=None,
                            criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_MAX_ITER, 1, 10), attempts=1,
                            flags=cv2.KMEANS_RANDOM_CENTERS)
        max_x = int(max(np_pts[:, 0]))
        min_x = int(min(np_pts[:, 0]))
        max_y = int(max(np_pts[:, 1]))
        min_y = int(min(np_pts[:, 1]))

        bb =  (min_x-5, min_y-5, max_x-min_x+5, max_y-min_y+5)

        self = Track.__init__(self, img, bb)
        self.templateImg = templateImg
        self.skp = skp
        self.sd = sd
        self.tkp = tkp
        self.td = td
        self.pts = np_pts
        self.detector = detector
        self.descriptor = descriptor

    def getTrackedPoints(self):
        """
        **SUMMARY**

        Returns all the points which are being tracked.

        **RETURNS**

        A list of points.

        **EXAMPLE**

        >>> track = SURFTrack(image, pts, detector, descriptor, temp, skp, sd, tkp, td)
        >>> pts = track.getTrackedPoints()
        """
        return self.pts

    def drawTrackerPoints(self, color=Color.GREEN, radius=1, thickness=1):
        """
        **SUMMARY**

        Draw all the points which are being tracked.

        **PARAMETERS**
        * *color* - Color of the point
        * *radius* - Radius of the point
        *thickness* - thickness of the circle point

        **RETURNS**

        Nothing

        **EXAMPLE**

        >>> track = SURFTrack(image, pts, detector, descriptor, temp, skp, sd, tkp, td)
        >>> track.drawTrackerPoints()
        """
        if type(self.pts) is not type(None):
            for pt in self.pts:
                self.image.drawCircle(ctr=pt, rad=radius, thickness=thickness, color=color)

    def getDetector(self):
        """
        **SUMMARY**

        Returns SURF detector which is being used.

        **RETURNS**

        detector - cv2.Detctor

        **EXAMPLE**

        >>> track = SURFTrack(image, pts, detector, descriptor, temp, skp, sd, tkp, td)
        >>> detector = track.getDetector()
        """
        return self.detector

    def getDescriptor(self):
        """
        **SUMMARY**

        Returns SURF descriptor extractor which is being used.

        **RETURNS**

        detector - cv2.DescriptorExtractor

        **EXAMPLE**

        >>> track = SURFTrack(image, pts, detector, descriptor, temp, skp, sd, tkp, td)
        >>> descriptor= track.getDescriptor()
        """
        return self.descriptor

    def getImageKeyPoints(self):
        """
        **SUMMARY**

        Returns all the keypoints which are found on the image.

        **RETURNS**

        A list of points.

        **EXAMPLE**

        >>> track = SURFTrack(image, pts, detector, descriptor, temp, skp, sd, tkp, td)
        >>> skp = track.getImageKeyPoints()
        """
        return self.skp

    def getImageDescriptor(self):
        """
        **SUMMARY**

        Returns the image descriptor.

        **RETURNS**

        Image descriptor - numpy.ndarray

        **EXAMPLE**

        >>> track = SURFTrack(image, pts, detector, descriptor, temp, skp, sd, tkp, td)
        >>> sd = track.getImageDescriptor()
        """
        return self.sd

    def getTemplateKeyPoints(self):
        """
        **SUMMARY**

        Returns all the keypoints which are found on the template Image.

        **RETURNS**

        A list of points.

        **EXAMPLE**

        >>> track = SURFTrack(image, pts, detector, descriptor, temp, skp, sd, tkp, td)
        >>> tkp = track.getTemplateKeyPoints()
        """
        return self.tkp

    def getTemplateDescriptor(self):
        """
        **SUMMARY**

        Returns the template image descriptor.

        **RETURNS**

        Image descriptor - numpy.ndarray

        **EXAMPLE**

        >>> track = SURFTrack(image, pts, detector, descriptor, temp, skp, sd, tkp, td)
        >>> td = track.getTemplateDescriptor()
        """
        return self.td

    def getTemplateImage(self):
        """
        **SUMMARY**

        Returns Template Image.

        **RETURNS**

        Template Image - SimpleCV.Image

        **EXAMPLE**

        >>> track = SURFTrack(image, pts, detector, descriptor, temp, skp, sd, tkp, td)
        >>> templateImg = track.getTemplateImage()
        """
        return self.templateImg

class MFTrack(Track):
    """
    **SUMMARY**

    MFTracker class is used for Median Flow Tracking algorithm. It's
    derived from Track Class. Apart from all the properties of Track class,
    MFTracker has few other properties.

    Media Flow Tracker is the base tracker that is used in OpenTLD. It is based on
    Optical Flow. It calculates optical flow of the points in the bounding box from
    frame 1 to frame 2 and from frame 2 to frame 1 and using back track error, removes
    false positives. As the name suggests, it takes the median of the flow, and eliminates
    points.


    """
    def __init__(self, img, bb, shift):
        """
        **SUMMARY**

        Initializes all the required parameters and attributes of the class.

        **PARAMETERS**

        * *img* - SimpleCV.ImageClass.Image
        * *bb* - A tuple consisting of (x, y, w, h) of the bounding box
        * *shift* - Object Shift calcluated in Median Flow

        **RETURNS**

        SimpleCV.Tracking.TrackClass.MFTrack object

        **EXAMPLE**

        >>> track = MFTrack(image, bb, shift)
        """
        self = Track.__init__(self, img, bb)
        self.shift = shift

    def getShift(self):
        """
        **SUMMARY**

        Returns object shift that was calcluated in Median Flow.

        **RETURNS**

        float

        **EXAMPLE**

        >>> track = MFTrack(image, bb, pts)
        >>> pts = track.getShift()
        """
        return self.shift

    def showShift(self, pos=None, color=Color.GREEN, size=None):
        """
        **SUMMARY**

        Show the Pixel Veloctiy (pixels/second) of the object in text on the image.

        **PARAMETERS**
        * *pos* - A tuple consisting of x, y values. where to put to the text
        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *size* - Fontsize of the text

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**
        >>> ts = []
        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("mftrack", ts, img, bb)
            ... ts[-1].showShift()
            ... img1.show()
        """
        f = self
        img = f.image
        shift = f.shift
        if not pos:
            imgsize = img.size()
            pos = (imgsize[0]-120, 50)
        if not size:
            size = 16
        text = "Shift = %.2f" % (shift)
        img.drawText(text, pos[0], pos[1], color, size)
        img.drawText("in pixels/second", pos[0], pos[1]+size, color, size)

########NEW FILE########
__FILENAME__ = TrackSet
from SimpleCV.Color import Color
from SimpleCV.base import time, cv, np
from SimpleCV.Features.Features import Feature, FeatureSet
from SimpleCV.ImageClass import Image


class TrackSet(FeatureSet):
    """
    **SUMMARY**

    TrackSet is a class extended from FeatureSet which is a class
    extended from Python's list. So, TrackSet has all the properties
    of a list as well as all the properties of FeatureSet.

    In general, functions dealing with attributes will return
    numpy arrays.

    This class is specifically made for Tracking.

    **EXAMPLE**

    >>> image = Image("/path/to/image.png")
    >>> ts = image.track("camshift", img1=image, bb)  #ts is the track set
    >>> ts.draw()
    >>> ts.x()
    """
    try:
        import cv2
    except ImportError:
        warnings.warn("OpenCV >= 2.3.1 required.")
    
    def __init__(self):
        self.kalman = None
        self.predict_pt = (0,0)
        self.__kalman()

    def append(self, f):
        """
        **SUMMARY**

        This is a substitute function for append. This is used in
        Image.track(). To get z, vel, etc I have to use this.
        This sets few parameters up and appends Tracking object to
        TrackSet list.
        Users are discouraged to use this function.

        **RETURNS**
            Nothing.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... img = img1
        >>> ts.append(CAMShift(img,bb,ellipse))
        """
        list.append(self,f)
        ts = self
        if ts[0].area <= 0:
            return
        f.sizeRatio = float(ts[-1].area)/float(ts[0].area)
        f.vel = self.__pixelVelocity()
        f.rt_vel = self.__pixleVelocityRealTime()
        self.__setKalman()
        self.__predictKalman()
        self.__changeMeasure()
        self.__correctKalman()
        f.predict_pt = self.predict_pt
        f.state_pt = self.state_pt

    # Issue #256 - (Bug) Memory management issue due to too many number of images.
    def trimList(self, num):
        """
        **SUMMARY**

        Trims the TrackSet(lists of all the saved objects) to save memory. It is implemented in
        Image.track() by default, but if you want to trim the list manually, use this.

        **RETURNS**

        Nothing.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... if len(ts) > 30:
                ... ts.trimList(10)
            ... img = img1
        """
        ts = self
        for i in range(num):
            ts.pop(0)

    def areaRatio(self):
        """
        **SUMMARY**

        Returns a numpy array of the areaRatio of each feature.
        where areaRatio is the ratio of the size of the current bounding box to
        the size of the initial bounding box

        **RETURNS**

        A numpy array.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... img = img1
        >>> print ts.areaRatio

        """
        return np.array([f.areaRatio for f in self])

    def drawPath(self, color=Color.GREEN, thickness=2):
        """
        **SUMMARY**

        Draw the complete path traced by the center of the object on current frame

        **PARAMETERS**

        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *thickness* - Thickness of the tracing path.

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... ts.drawPath() # For continuous tracing
            ... img = img1
        >>> ts.drawPath() # draw the path at the end of tracking
        """

        ts = self
        img = self[-1].image
        for i in range(len(ts)-1):
            img.drawLine((ts[i].center),(ts[i+1].center), color=color, thickness=thickness)

    def draw(self, color=Color.GREEN, rad=1, thickness=1):
        """
        **SUMMARY**

        Draw the center of the object on the current frame.

        **PARAMETERS**

        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *rad* - Radius of the circle to be plotted on the center of the object.
        * *thickness* - Thickness of the boundary of the center circle.

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... ts.draw() # For continuous tracking of the center
            ... img = img1
        """
        f = self[-1]
        f.image.drawCircle(f.center, rad, color, thickness)

    def drawBB(self, color=Color.GREEN, thickness=3):
        """
        **SUMMARY**

        Draw the bounding box over the object on the current frame.

        **PARAMETERS**

        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *thickness* - Thickness of the boundary of the bounding box.

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... ts.drawBB() # For continuous bounding box
            ... img = img1
        """
        f = self[-1]
        f.image.drawRectangle(f.bb_x, f.bb_y, f.w, f.h, color, thickness)

    def trackLength(self):
        """
        **SUMMARY**

        Get total number of tracked frames.

        **PARAMETERS**

        No Parameters required.

        **RETURNS**

        * *int* * -Number of tracked image frames

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... img = img1
        >>> print ts.trackLength()
        """
        return len(self)

    def trackImages(self, cv2_numpy=False):
        """
        **SUMMARY**

        Get all the tracked images in a list

        **PARAMETERS**

        No Parameters required.

        **RETURNS**

        * *list* * - A list of all the tracked SimpleCV.ImageClass.Image

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... img = img1
        >>> imgset = ts.trackImages()
        """
        if cv2_numpy:
            return [f.cv2numpy for f in self]
        return [f.image for f in self]

    def BBTrack(self):
        """
        **SUMMARY**

        Get all the bounding box in a list

        **PARAMETERS**

        No Parameters required.

        **RETURNS**

        * *list* * - All the bounding box co-ordinates in a list

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... img = img1
        >>> print ts.BBTrack()
        """
        return [f.bb for f in self]

    def __pixelVelocity(self):
        """
        **SUMMARY**

        Get Pixel Velocity of the tracked object in pixel/frame.

        **PARAMETERS**

        No Parameters required.

        **RETURNS**

        * *tuple* * - (Velocity of x, Velocity of y)

        """
        ts = self
        if len(ts) < 2:
            return (0,0)
        dx = ts[-1].x - ts[-2].x
        dy = ts[-1].y - ts[-2].y
        return (dx, dy)

    def pixelVelocity(self):
        """
        **SUMMARY**

        Get each Pixel Velocity of the tracked object in pixel/frames.

        **PARAMETERS**

        No Parameters required.

        **RETURNS**

        * *numpy array* * - array of pixel velocity tuple.

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... img = img1
        >>> print ts.pixelVelocity()
        """
        return np.array([f.vel for f in self])

    def __pixleVelocityRealTime(self):
        """
        **SUMMARY**

        Get each Pixel Velocity of the tracked object in pixel/second.

        **PARAMETERS**

        No Parameters required.

        **RETURNS**

        * *tuple* * - velocity tuple
        """
        ts = self
        if len(ts) < 2:
            return (0,0)
        dx = ts[-1].x - ts[-2].x
        dy = ts[-1].y - ts[-2].y
        dt = ts[-1].time - ts[-2].time
        return (float(dx)/dt, float(dy)/dt)

    def pixleVelocityRealTime(self):
        """
        **SUMMARY**

        Get each Pixel Velocity of the tracked object in pixel/frames.

        **PARAMETERS**

        No Parameters required.

        **RETURNS**

        * *numpy array* * - array of pixel velocity tuple.

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... img = img1
        >>> print ts.pixelVelocityRealTime()
        """
        return np.array([f.rt_vel for f in self])

    def showCoordinates(self, pos=None, color=Color.GREEN, size=None):
        """
        **SUMMARY**

        Show the co-ordinates of the object in text on the current frame.

        **PARAMETERS**
        * *pos* - A tuple consisting of x, y values. where to put to the text
        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *size* - Fontsize of the text

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... ts.showCoordinates() # For continuous bounding box
            ... img = img1
        """
        ts = self
        f = ts[-1]
        img = f.image
        if not pos:
            imgsize = img.size()
            pos = (imgsize[0]-120, 10)
        if not size:
            size = 16
        text = "x = %d  y = %d" % (f.x, f.y)
        img.drawText(text, pos[0], pos[1], color, size)

    def showSizeRatio(self, pos=None, color=Color.GREEN, size=None):
        """
        **SUMMARY**

        Show the sizeRatio of the object in text on the current frame.

        **PARAMETERS**
        * *pos* - A tuple consisting of x, y values. where to put to the text
        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *size* - Fontsize of the text

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... ts.showZ() # For continuous bounding box
            ... img = img1
        """
        ts = self
        f = ts[-1]
        img = f.image
        if not pos:
            imgsize = img.size()
            pos = (imgsize[0]-120, 30)
        if not size:
            size = 16
        text = "size = %f" % (f.sizeRatio)
        img.drawText(text, pos[0], pos[1], color, size)

    def showPixelVelocity(self, pos=None, color=Color.GREEN, size=None):
        """
        **SUMMARY**

        show the Pixel Veloctiy (pixel/frame) of the object in text on the current frame.

        **PARAMETERS**
        * *pos* - A tuple consisting of x, y values. where to put to the text
        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *size* - Fontsize of the text

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... ts.showPixelVelocity() # For continuous bounding box
            ... img = img1
        """
        ts = self
        f = ts[-1]
        img = f.image
        vel = f.vel
        if not pos:
            imgsize = img.size()
            pos = (imgsize[0]-120, 50)
        if not size:
            size = 16
        text = "Vx = %.2f Vy = %.2f" % (vel[0], vel[1])
        img.drawText(text, pos[0], pos[1], color, size)
        img.drawText("in pixels/frame", pos[0], pos[1]+size, color, size)

    def showPixelVelocityRT(self, pos=None, color=Color.GREEN, size=None):
        """
        **SUMMARY**

        show the Pixel Veloctiy (pixels/second) of the object in text on the current frame.

        **PARAMETERS**
        * *pos* - A tuple consisting of x, y values. where to put to the text
        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *size* - Fontsize of the text

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... ts.showPixelVelocityRT() # For continuous bounding box
            ... img = img1
        """
        ts = self
        f = ts[-1]
        img = f.image
        vel_rt = f.rt_vel
        if not pos:
            imgsize = img.size()
            pos = (imgsize[0]-120, 90)
        if not size:
            size = 16
        text = "Vx = %.2f Vy = %.2f" % (vel_rt[0], vel_rt[1])
        img.drawText(text, pos[0], pos[1], color, size)
        img.drawText("in pixels/second", pos[0], pos[1]+size, color, size)

    def processTrack(self, func):
        """
        **SUMMARY**

        This method lets you use your own function on the entire imageset.

        **PARAMETERS**
        * *func* - some user defined function for SimpleCV.ImageClass.Image object

        **RETURNS**

        * *list* - list of the values returned by the function when applied on all the images

        **EXAMPLE**

        >>> def foo(img):
            ... return img.meanColor()
        >>> mean_color_list = ts.processTrack(foo)
        """
        return [func(f.image) for f in self]

    def getBackground(self):
        """
        **SUMMARY**

        Get Background of the Image. For more info read
        http://opencvpython.blogspot.in/2012/07/background-extraction-using-running.html

        **PARAMETERS**
        No Parameters

        **RETURNS**

        Image - SimpleCV.ImageClass.Image

        **EXAMPLE**

        >>> while (some_condition):
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... img = img1
        >>> ts.getBackground().show()
        """
        imgs = self.trackImages(cv2_numpy=True)
        f = imgs[0]
        avg = np.float32(f)
        for img in imgs[1:]:
            f = img
            cv2.accumulateWeighted(f,avg,0.01)
            res = cv2.convertScaleAbs(avg)
        return Image(res, cv2image=True)

    def __kalman(self):
        self.kalman = cv.CreateKalman(4, 2, 0)
        self.kalman_state = cv.CreateMat(4, 1, cv.CV_32FC1)  # (phi, delta_phi)
        self.kalman_process_noise = cv.CreateMat(4, 1, cv.CV_32FC1)
        self.kalman_measurement = cv.CreateMat(2, 1, cv.CV_32FC1)

    def __setKalman(self):
        ts = self
        if len(ts) < 2:
            self.kalman_x = ts[-1].x
            self.kalman_y = ts[-1].y
        else:
            self.kalman_x = ts[-2].x
            self.kalman_y = ts[-2].y

        self.kalman.state_pre[0,0]  = self.kalman_x
        self.kalman.state_pre[1,0]  = self.kalman_y
        self.kalman.state_pre[2,0]  = self.predict_pt[0]
        self.kalman.state_pre[3,0]  = self.predict_pt[1]

        self.kalman.transition_matrix[0,0] = 1
        self.kalman.transition_matrix[0,1] = 0
        self.kalman.transition_matrix[0,2] = 1
        self.kalman.transition_matrix[0,3] = 0
        self.kalman.transition_matrix[1,0] = 0
        self.kalman.transition_matrix[1,1] = 1
        self.kalman.transition_matrix[1,2] = 0
        self.kalman.transition_matrix[1,3] = 1
        self.kalman.transition_matrix[2,0] = 0
        self.kalman.transition_matrix[2,1] = 0
        self.kalman.transition_matrix[2,2] = 1
        self.kalman.transition_matrix[2,3] = 0
        self.kalman.transition_matrix[3,0] = 0
        self.kalman.transition_matrix[3,1] = 0
        self.kalman.transition_matrix[3,2] = 0
        self.kalman.transition_matrix[3,3] = 1

        cv.SetIdentity(self.kalman.measurement_matrix, cv.RealScalar(1))
        cv.SetIdentity(self.kalman.process_noise_cov, cv.RealScalar(1e-5))
        cv.SetIdentity(self.kalman.measurement_noise_cov, cv.RealScalar(1e-1))
        cv.SetIdentity(self.kalman.error_cov_post, cv.RealScalar(1))

    def __predictKalman(self):
        self.kalman_prediction = cv.KalmanPredict(self.kalman)
        self.predict_pt  = (self.kalman_prediction[0,0], self.kalman_prediction[1,0])

    def __correctKalman(self):
        self.kalman_estimated = cv.KalmanCorrect(self.kalman, self.kalman_measurement)
        self.state_pt = (self.kalman_estimated[0,0], self.kalman_estimated[1,0])

    def __changeMeasure(self):
        ts = self
        self.kalman_measurement[0, 0] = ts[-1].x
        self.kalman_measurement[1, 0] = ts[-1].y

    def predictedCoordinates(self):
        """
        **SUMMARY**

        Returns a numpy array of the predicted coordinates of each feature.

        **RETURNS**

        A numpy array.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... img = img1
        >>> print ts.predictedCoordinates()

        """
        return np.array([f.predict_pt for f in self])

    def predictX(self):
        """
        **SUMMARY**

        Returns a numpy array of the predicted x (vertical) coordinate of each feature.

        **RETURNS**

        A numpy array.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... img = img1
        >>> print ts.predictX()

        """
        return np.array([f.predict_pt[0] for f in self])

    def predictY(self):
        """
        **SUMMARY**

        Returns a numpy array of the predicted y (vertical) coordinate of each feature.

        **RETURNS**

        A numpy array.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... img = img1
        >>> print ts.predictY()

        """
        return np.array([f.predict_pt[1] for f in self])

    def drawPredicted(self, color=Color.GREEN, rad=1, thickness=1):
        """
        **SUMMARY**

        Draw the predcited center of the object on the current frame.

        **PARAMETERS**

        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *rad* - Radius of the circle to be plotted on the center of the object.
        * *thickness* - Thickness of the boundary of the center circle.

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... ts.drawPredicted() # For continuous tracking of the center
            ... img = img1
        """
        f = self[-1]
        f.image.drawCircle(f.predict_pt, rad, color, thickness)

    def drawCorrected(self, color=Color.GREEN, rad=1, thickness=1):
        """
        **SUMMARY**

        Draw the predcited center of the object on the current frame.

        **PARAMETERS**

        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *rad* - Radius of the circle to be plotted on the center of the object.
        * *thickness* - Thickness of the boundary of the center circle.

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... ts.drawPredicted() # For continuous tracking of the center
            ... img = img1
        """
        f = self[-1]
        f.image.drawCircle(f.state_pt, rad, color, thickness)

    def drawPredictedPath(self, color=Color.GREEN, thickness=2):
        """
        **SUMMARY**

        Draw the complete predicted path of the center of the object on current frame

        **PARAMETERS**

        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *thickness* - Thickness of the tracing path.

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... ts.drawPredictedPath() # For continuous tracing
            ... img = img1
        >>> ts.drawPredictedPath() # draw the path at the end of tracking
        """

        ts = self
        img = self[-1].image
        for i in range(1, len(ts)-1):
            img.drawLine((ts[i].predict_pt),(ts[i+1].predict_pt), color=color, thickness=thickness)

    def showPredictedCoordinates(self, pos=None, color=Color.GREEN, size=None):
        """
        **SUMMARY**

        Show the co-ordinates of the object in text on the current frame.

        **PARAMETERS**
        * *pos* - A tuple consisting of x, y values. where to put to the text
        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *size* - Fontsize of the text

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... ts.showPredictedCoordinates() # For continuous bounding box
            ... img = img1
        """
        ts = self
        f = ts[-1]
        img = f.image
        if not pos:
            imgsize = img.size()
            pos = (5, 10)
        if not size:
            size = 16
        text = "Predicted: x = %d  y = %d" % (f.predict_pt[0], f.predict_pt[1])
        img.drawText(text, pos[0], pos[1], color, size)

    def showCorrectedCoordinates(self, pos=None, color=Color.GREEN, size=None):
        """
        **SUMMARY**

        Show the co-ordinates of the object in text on the current frame.

        **PARAMETERS**
        * *pos* - A tuple consisting of x, y values. where to put to the text
        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *size* - Fontsize of the text

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... ts.showCorrectedCoordinates() # For continuous bounding box
            ... img = img1
        """
        ts = self
        f = ts[-1]
        img = f.image
        if not pos:
            imgsize = img.size()
            pos = (5, 40)
        if not size:
            size = 16
        text = "Corrected: x = %d  y = %d" % (f.state_pt[0], f.state_pt[1])
        img.drawText(text, pos[0], pos[1], color, size)

    def correctX(self):
        """
        **SUMMARY**

        Returns a numpy array of the corrected x coordinate of each feature.

        **RETURNS**

        A numpy array.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... img = img1
        >>> print ts.correctX()

        """
        return np.array([f.state_pt[0] for f in self])

    def correctY(self):
        """
        **SUMMARY**

        Returns a numpy array of the corrected y coordinate of each feature.

        **RETURNS**

        A numpy array.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... img = img1
        >>> print ts.correctY()

        """
        return np.array([f.state_pt[1] for f in self])

    def correctedCoordinates(self):
        """
        **SUMMARY**

        Returns a numpy array of the corrected coordinates of each feature.

        **RETURNS**

        A numpy array.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... img = img1
        >>> print ts.predictedCoordinates()

        """
        return np.array([f.state_pt for f in self])

    def drawCorrectedPath(self, color=Color.GREEN, thickness=2):
        """
        **SUMMARY**

        Draw the complete corrected path of the center of the object on current frame

        **PARAMETERS**

        * *color* - The color to draw the object. Either an BGR tuple or a member of the :py:class:`Color` class.
        * *thickness* - Thickness of the tracing path.

        **RETURNS**

        Nada. Nothing. Zilch.

        **EXAMPLE**

        >>> while True:
            ... img1 = cam.getImage()
            ... ts = img1.track("camshift", ts1, img, bb)
            ... ts.drawCorrectedPath() # For continuous tracing
            ... img = img1
        >>> ts.drawPredictedPath() # draw the path at the end of tracking
        """

        ts = self
        img = self[-1].image
        for i in range(len(ts)-1):
            img.drawLine((ts[i].state_pt),(ts[i+1].state_pt), color=color, thickness=thickness)

########NEW FILE########
