__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# snakebite documentation build configuration file, created by
# sphinx-quickstart on Tue Apr 30 11:39:44 2013.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys
import os
import snakebite.version

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.insert(0, os.path.abspath('../../snakebite'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.ifconfig', 'sphinx.ext.autosummary']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'snakebite'
copyright = u'2013 - 2014, Spotify AB'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = snakebite.version.version()
# The full version, including alpha/beta/rc tags.
release = snakebite.version.version()

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = []

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []

# If true, keep warnings as "system message" paragraphs in the built documents.
#keep_warnings = False


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'snakebitedoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'snakebite.tex', u'snakebite Documentation',
   u'Wouter de Bie', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'snakebite', u'snakebite Documentation',
     [u'Wouter de Bie'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'snakebite', u'snakebite Documentation',
   u'Wouter de Bie', 'snakebite', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

# If true, do not generate a @detailmenu in the "Top" node's menu.
#texinfo_no_detailmenu = False

autoclass_content = 'both'

########NEW FILE########
__FILENAME__ = channel
# -*- coding: utf-8 -*-
# Copyright (c) 2009 Las Cumbres Observatory (www.lcogt.net)
# Copyright (c) 2010 Jan Dittberner
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.
'''
channel.py - Socket implementation of Google's Protocol Buffers RPC
service interface.

This package contains classes providing a socket implementation of the
RPCChannel abstract class.

Original Authors: Martin Norbury (mnorbury@lcogt.net)
         Eric Saunders (esaunders@lcogt.net)
         Jan Dittberner (jan@dittberner.info)

May 2009, Nov 2010

Modified for snakebite: Wouter de Bie (wouter@spotify.com)

May 2012

'''

# Standard library imports
import socket
import os
import pwd
import math

# Third party imports
from google.protobuf.service import RpcChannel

# Protobuf imports
from snakebite.protobuf.RpcHeader_pb2 import RpcRequestHeaderProto, RpcResponseHeaderProto
from snakebite.protobuf.IpcConnectionContext_pb2 import IpcConnectionContextProto
from snakebite.protobuf.ProtobufRpcEngine_pb2 import RequestHeaderProto
from snakebite.protobuf.datatransfer_pb2 import OpReadBlockProto, BlockOpResponseProto, PacketHeaderProto, ClientReadStatusProto

from snakebite.formatter import format_bytes
from snakebite.errors import RequestError
from snakebite.crc32c import crc

import google.protobuf.internal.encoder as encoder
import google.protobuf.internal.decoder as decoder

# Module imports

import logger
import logging
import struct
import uuid

# Configure package logging
log = logger.getLogger(__name__)


def log_protobuf_message(header, message):
    log.debug("%s:\n\n\033[92m%s\033[0m" % (header, message))


def get_delimited_message_bytes(byte_stream, nr=4):
    ''' Parse a delimited protobuf message. This is done by first getting a protobuf varint from
    the stream that represents the length of the message, then reading that amount of
    from the message and then parse it.
    Since the int can be represented as max 4 bytes, first get 4 bytes and try to decode.
    The decoder returns the value and the position where the value was found, so we need
    to rewind the buffer to the position, because the remaining bytes belong to the message
    after.
    '''

    (length, pos) = decoder._DecodeVarint32(byte_stream.read(nr), 0)
    if log.getEffectiveLevel() == logging.DEBUG:
        log.debug("Delimited message length (pos %d): %d" % (pos, length))

    delimiter_bytes = nr - pos

    byte_stream.rewind(delimiter_bytes)
    message_bytes = byte_stream.read(length)
    if log.getEffectiveLevel() == logging.DEBUG:
        log.debug("Delimited message bytes (%d): %s" % (len(message_bytes), format_bytes(message_bytes)))

    total_len = length + pos
    return (total_len, message_bytes)


class RpcBufferedReader(object):
    '''Class that wraps a socket and provides some utility methods for reading
    and rewinding of the buffer. This comes in handy when reading protobuf varints.
    '''
    MAX_READ_ATTEMPTS = 100

    def __init__(self, socket):
        self.socket = socket
        self.reset()

    def read(self, n):
        '''Reads n bytes into the internal buffer'''
        bytes_wanted = n - self.buffer_length + self.pos + 1
        if bytes_wanted > 0:
            self._buffer_bytes(bytes_wanted)

        end_pos = self.pos + n
        ret = self.buffer[self.pos + 1:end_pos + 1]
        self.pos = end_pos
        return ret

    def _buffer_bytes(self, n):
        to_read = n
        for _ in xrange(self.MAX_READ_ATTEMPTS):
            bytes_read = self.socket.recv(to_read)
            self.buffer += bytes_read
            to_read -= len(bytes_read)
            if to_read == 0:
                log.debug("Bytes read: %d, total: %d" % (len(bytes_read), self.buffer_length))
                return n
        if len(bytes_read) < n:
            raise Exception("RpcBufferedReader only managed to read %s out of %s bytes" % (len(bytes_read), n))

    def rewind(self, places):
        '''Rewinds the current buffer to a position. Needed for reading varints,
        because we might read bytes that belong to the stream after the varint.
        '''
        log.debug("Rewinding pos %d with %d places" % (self.pos, places))
        self.pos -= places
        log.debug("Reset buffer to pos %d" % self.pos)

    def reset(self):
        self.buffer = ""
        self.pos = -1  # position of last byte read

    @property
    def buffer_length(self):
        '''Returns the length of the current buffer.'''
        return len(self.buffer)


class SocketRpcChannel(RpcChannel):
    ERROR_BYTES = 18446744073709551615L
    RPC_HEADER = "hrpc"
    RPC_SERVICE_CLASS = 0x00
    AUTH_PROTOCOL_NONE = 0x00
    RPC_PROTOCOL_BUFFFER = 0x02


    '''Socket implementation of an RpcChannel.
    '''

    def __init__(self, host, port, version, effective_user=None):
        '''SocketRpcChannel to connect to a socket server on a user defined port.
           It possible to define version and effective user for the communication.'''
        self.host = host
        self.port = port
        self.sock = None
        self.call_id = -3  # First time (when the connection context is sent, the call_id should be -3, otherwise start with 0 and increment)
        self.version = version
        self.client_id = str(uuid.uuid4())
        self.effective_user = effective_user or pwd.getpwuid(os.getuid())[0]

    def validate_request(self, request):
        '''Validate the client request against the protocol file.'''

        # Check the request is correctly initialized
        if not request.IsInitialized():
            raise Exception("Client request (%s) is missing mandatory fields" % type(request))

    def get_connection(self, host, port):
        '''Open a socket connection to a given host and port and writes the Hadoop header
        The Hadoop RPC protocol looks like this when creating a connection:

        +---------------------------------------------------------------------+
        |  Header, 4 bytes ("hrpc")                                           |
        +---------------------------------------------------------------------+
        |  Version, 1 byte (default verion 9)                                 |
        +---------------------------------------------------------------------+
        |  RPC service class, 1 byte (0x00)                                   |
        +---------------------------------------------------------------------+
        |  Auth protocol, 1 byte (Auth method None = 0)                       |
        +---------------------------------------------------------------------+
        |  Length of the RpcRequestHeaderProto  + length of the               |
        |  of the IpcConnectionContextProto (4 bytes/32 bit int)              |
        +---------------------------------------------------------------------+
        |  Serialized delimited RpcRequestHeaderProto                         |
        +---------------------------------------------------------------------+
        |  Serialized delimited IpcConnectionContextProto                     |
        +---------------------------------------------------------------------+
        '''

        log.debug("############## CONNECTING ##############")
        # Open socket
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
        self.sock.settimeout(10)
        # Connect socket to server - defined by host and port arguments
        self.sock.connect((host, port))

        # Send RPC headers
        self.write(self.RPC_HEADER)                             # header
        self.write(struct.pack('B', self.version))              # version
        self.write(struct.pack('B', self.RPC_SERVICE_CLASS))    # RPC service class
        self.write(struct.pack('B', self.AUTH_PROTOCOL_NONE))   # serialization type (protobuf = 0)

        rpc_header = self.create_rpc_request_header()
        context = self.create_connection_context()

        header_length = len(rpc_header) + encoder._VarintSize(len(rpc_header)) +len(context) + encoder._VarintSize(len(context))

        if log.getEffectiveLevel() == logging.DEBUG:
            log.debug("Header length: %s (%s)" % (header_length, format_bytes(struct.pack('!I', header_length))))

        self.write(struct.pack('!I', header_length))

        self.write_delimited(rpc_header)
        self.write_delimited(context)
    
    def write(self, data):
        if log.getEffectiveLevel() == logging.DEBUG:
            log.debug("Sending: %s", format_bytes(data))
        self.sock.send(data)

    def write_delimited(self, data):
        self.write(encoder._VarintBytes(len(data)))
        self.write(data)

    def create_rpc_request_header(self):
        '''Creates and serializes a delimited RpcRequestHeaderProto message.'''
        rpcheader = RpcRequestHeaderProto()
        rpcheader.rpcKind = 2  # rpcheaderproto.RpcKindProto.Value('RPC_PROTOCOL_BUFFER')
        rpcheader.rpcOp = 0  # rpcheaderproto.RpcPayloadOperationProto.Value('RPC_FINAL_PACKET')
        rpcheader.callId = self.call_id
        rpcheader.retryCount = -1
        rpcheader.clientId = self.client_id[0:16]

        if self.call_id == -3:
            self.call_id = 0
        else:
            self.call_id += 1

        # Serialize delimited
        s_rpcHeader = rpcheader.SerializeToString()
        log_protobuf_message("RpcRequestHeaderProto (len: %d)" % (len(s_rpcHeader)), rpcheader)
        return s_rpcHeader

    def create_connection_context(self):
        '''Creates and seriazlies a IpcConnectionContextProto (not delimited)'''
        context = IpcConnectionContextProto()
        context.userInfo.effectiveUser = self.effective_user
        context.protocol = "org.apache.hadoop.hdfs.protocol.ClientProtocol"

        s_context = context.SerializeToString()
        log_protobuf_message("RequestContext (len: %d)" % len(s_context), context)
        return s_context

    def send_rpc_message(self, method, request):
        '''Sends a Hadoop RPC request to the NameNode.

        The IpcConnectionContextProto, RpcPayloadHeaderProto and HadoopRpcRequestProto
        should already be serialized in the right way (delimited or not) before
        they are passed in this method.

        The Hadoop RPC protocol looks like this for sending requests:

        When sending requests
        +---------------------------------------------------------------------+
        |  Length of the next three parts (4 bytes/32 bit int)                |
        +---------------------------------------------------------------------+
        |  Delimited serialized RpcRequestHeaderProto (varint len + header)   |
        +---------------------------------------------------------------------+
        |  Delimited serialized RequestHeaderProto (varint len + header)      |
        +---------------------------------------------------------------------+
        |  Delimited serialized Request (varint len + request)                |
        +---------------------------------------------------------------------+
        '''
        log.debug("############## SENDING ##############")

        #0. RpcRequestHeaderProto
        rpc_request_header = self.create_rpc_request_header()
        #1. RequestHeaderProto
        request_header = self.create_request_header(method)
        #2. Param
        param = request.SerializeToString()
        if log.getEffectiveLevel() == logging.DEBUG:
            log_protobuf_message("Request", request)

        rpc_message_length = len(rpc_request_header) + encoder._VarintSize(len(rpc_request_header)) + \
                             len(request_header) + encoder._VarintSize(len(request_header)) + \
                             len(param) + encoder._VarintSize(len(param))

        if log.getEffectiveLevel() == logging.DEBUG:
            log.debug("RPC message length: %s (%s)" % (rpc_message_length, format_bytes(struct.pack('!I', rpc_message_length))))
        self.write(struct.pack('!I', rpc_message_length))

        self.write_delimited(rpc_request_header)
        self.write_delimited(request_header)
        self.write_delimited(param)

    def create_request_header(self, method):
        header = RequestHeaderProto()
        header.methodName = method.name
        header.declaringClassProtocolName = "org.apache.hadoop.hdfs.protocol.ClientProtocol"
        header.clientProtocolVersion = 1

        s_header = header.SerializeToString()
        log_protobuf_message("RequestHeaderProto (len: %d)" % len(s_header), header)
        return s_header

    def recv_rpc_message(self):
        '''Handle reading an RPC reply from the server. This is done by wrapping the
        socket in a RcpBufferedReader that allows for rewinding of the buffer stream.
        '''
        log.debug("############## RECVING ##############")
        byte_stream = RpcBufferedReader(self.sock)
        return byte_stream

    def get_length(self, byte_stream):
        ''' In Hadoop protobuf RPC, some parts of the stream are delimited with protobuf varint,
        while others are delimited with 4 byte integers. This reads 4 bytes from the byte stream
        and retruns the length of the delimited part that follows, by unpacking the 4 bytes
        and returning the first element from a tuple. The tuple that is returned from struc.unpack()
        only contains one element.
        '''
        length = struct.unpack("!i", byte_stream.read(4))[0]
        log.debug("4 bytes delimited part length: %d" % length)
        return length

    def parse_response(self, byte_stream, response_class):
        '''Parses a Hadoop RPC response.

        The RpcResponseHeaderProto contains a status field that marks SUCCESS or ERROR.
        The Hadoop RPC protocol looks like the diagram below for receiving SUCCESS requests.
        +-----------------------------------------------------------+
        |  Length of the RPC resonse (4 bytes/32 bit int)           |
        +-----------------------------------------------------------+
        |  Delimited serialized RpcResponseHeaderProto              |
        +-----------------------------------------------------------+
        |  Serialized delimited RPC response                        |
        +-----------------------------------------------------------+

        In case of an error, the header status is set to ERROR and the error fields are set.
        '''

        log.debug("############## PARSING ##############")
        log.debug("Payload class: %s" % response_class)

        # Read first 4 bytes to get the total length
        len_bytes = byte_stream.read(4)
        total_length = struct.unpack("!I", len_bytes)[0]
        log.debug("Total response length: %s" % total_length)

        header = RpcResponseHeaderProto()
        (header_len, header_bytes) = get_delimited_message_bytes(byte_stream)

        log.debug("Header read %d" % header_len)
        header.ParseFromString(header_bytes)
        log_protobuf_message("RpcResponseHeaderProto", header)

        if header.status == 0:
            log.debug("header: %s, total: %s" % (header_len, total_length))
            if header_len >= total_length:
                return
            response = response_class()
            response_bytes = get_delimited_message_bytes(byte_stream, total_length - header_len)[1]
            if len(response_bytes) > 0:
                response.ParseFromString(response_bytes)
                if log.getEffectiveLevel() == logging.DEBUG:
                    log_protobuf_message("Response", response)
                return response
        else:
            self.handle_error(header)

    def handle_error(self, header):
        raise RequestError("\n".join([header.exceptionClassName, header.errorMsg]))

    def close_socket(self):
        '''Closes the socket and resets the channel.'''
        log.debug("Closing socket")
        if self.sock:
            try:
                self.sock.close()
            except:
                pass

            self.sock = None

    def CallMethod(self, method, controller, request, response_class, done):
        '''Call the RPC method. The naming doesn't confirm PEP8, since it's
        a method called by protobuf
        '''
        try:
            self.validate_request(request)

            if not self.sock:
                self.get_connection(self.host, self.port)

            self.send_rpc_message(method, request)

            byte_stream = self.recv_rpc_message()
            return self.parse_response(byte_stream, response_class)
        except RequestError:  # Raise a request error, but don't close the socket
            raise
        except Exception:  # All other errors close the socket
            self.close_socket()
            raise


class DataXceiverChannel(object):
    # For internal reading: should be larger than bytes_per_chunk
    LOAD_SIZE = 16000.0

    # Op codes
    WRITE_BLOCK = 80
    READ_BLOCK = 81
    READ_METADATA = 82
    REPLACE_BLOCK = 83
    COPY_BLOCK = 84
    BLOCK_CHECKSUM = 85
    TRANSFER_BLOCK = 86

    # Checksum types
    CHECKSUM_NULL = 0
    CHECKSUM_CRC32 = 1
    CHECKSUM_CRC32C = 2
    CHECKSUM_DEFAULT = 3
    CHECKSUM_MIXED = 4

    MAX_READ_ATTEMPTS = 100

    def __init__(self, host, port):
        self.host, self.port = host, port
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

    def connect(self):
        try:
            self.sock.connect((self.host, self.port))
            log.debug("%s connected to DataNode" % self)
            return True
        except Exception:
            log.debug("%s connection to DataNode failed" % self)
            return False

    def _close_socket(self):
        self.sock.close()

    def _read_bytes(self, n, depth=0):
        if depth > self.MAX_READ_ATTEMPTS:
            raise Exception("Tried to read %d more bytes, but failed after %d attempts" % (n, self.MAX_READ_ATTEMPTS))

        bytes = self.sock.recv(n)
        if len(bytes) < n:
            left = n - len(bytes)
            depth += 1
            bytes += self._read_bytes(left, depth)
        return bytes

    def write(self, data):
        if log.getEffectiveLevel() == logging.DEBUG:
            log.debug("Sending: %s", format_bytes(data))
        self.sock.send(data)

    def write_delimited(self, data):
        self.write(encoder._VarintBytes(len(data)))
        self.write(data)

    def readBlock(self, length, pool_id, block_id, generation_stamp, offset, check_crc):
        '''Send a read request to given block. If we receive a successful response,
        we start reading packets.

        Send read request:
        +---------------------------------------------------------------------+
        |  Data Transfer Protocol Version, 2 bytes                            |
        +---------------------------------------------------------------------+
        |  Op code, 1 byte (READ_BLOCK = 81)                                  |
        +---------------------------------------------------------------------+
        |  Delimited serialized OpReadBlockProto (varint len + request)       |
        +---------------------------------------------------------------------+

        Receive response:
        +---------------------------------------------------------------------+
        |  Delimited BlockOpResponseProto (varint len + response)             |
        +---------------------------------------------------------------------+

        Start reading packets. Each packet has the following structure:
        +---------------------------------------------------------------------+
        |  Packet length (4 bytes/32 bit int)                                 |
        +---------------------------------------------------------------------+
        |  Serialized size of header, 2 bytes                                 |
        +---------------------------------------------------------------------+
        |  Packet Header Proto                                                |
        +---------------------------------------------------------------------+
        |  x checksums, 4 bytes each                                          |
        +---------------------------------------------------------------------+
        |  x chunks of payload data                                           |
        +---------------------------------------------------------------------+

        '''
        log.debug("%s sending readBlock request" % self)

        # Send version and opcode
        self.sock.send(struct.pack('>h', 28))
        self.sock.send(struct.pack('b', self.READ_BLOCK))
        length = length - offset

        # Create and send OpReadBlockProto message
        request = OpReadBlockProto()
        request.offset = offset
        request.len = length
        header = request.header
        header.clientName = "snakebite"
        base_header = header.baseHeader
        block = base_header.block
        block.poolId = pool_id
        block.blockId = block_id
        block.generationStamp = generation_stamp
        s_request = request.SerializeToString()
        log_protobuf_message("OpReadBlockProto:", request)
        self.write_delimited(s_request)

        byte_stream = RpcBufferedReader(self.sock)
        block_op_response_bytes = get_delimited_message_bytes(byte_stream)[1]

        block_op_response = BlockOpResponseProto()
        block_op_response.ParseFromString(block_op_response_bytes)
        log_protobuf_message("BlockOpResponseProto", block_op_response)

        checksum_type = block_op_response.readOpChecksumInfo.checksum.type
        bytes_per_chunk = block_op_response.readOpChecksumInfo.checksum.bytesPerChecksum
        log.debug("Checksum type: %s, bytesPerChecksum: %s" % (checksum_type, bytes_per_chunk))
        if checksum_type in [self.CHECKSUM_CRC32C, self.CHECKSUM_CRC32]:
            checksum_len = 4
        else:
            raise Exception("Checksum type %s not implemented" % checksum_type)

        total_read = 0
        if block_op_response.status == 0:  # datatransfer_proto.Status.Value('SUCCESS')
            while total_read < length:
                log.debug("== Reading next packet")

                packet_len = struct.unpack("!I", byte_stream.read(4))[0]
                log.debug("Packet length: %s", packet_len)

                serialized_size = struct.unpack("!H", byte_stream.read(2))[0]
                log.debug("Serialized size: %s", serialized_size)

                packet_header_bytes = byte_stream.read(serialized_size)
                packet_header = PacketHeaderProto()
                packet_header.ParseFromString(packet_header_bytes)
                log_protobuf_message("PacketHeaderProto", packet_header)

                data_len = packet_header.dataLen

                chunks_per_packet = int((data_len + bytes_per_chunk - 1) / bytes_per_chunk)
                log.debug("Nr of chunks: %d", chunks_per_packet)

                data_len = packet_len - 4 - chunks_per_packet * checksum_len
                log.debug("Payload len: %d", data_len)

                byte_stream.reset()

                # Collect checksums
                if check_crc:
                    checksums = []
                    for _ in xrange(0, chunks_per_packet):
                        checksum = self._read_bytes(checksum_len)
                        checksum = struct.unpack("!I", checksum)[0]
                        checksums.append(checksum)
                else:
                    self._read_bytes(checksum_len * chunks_per_packet)

                # We use a fixed size buffer (a "load") to read only a couple of chunks at once. 
                bytes_per_load = self.LOAD_SIZE - (self.LOAD_SIZE % bytes_per_chunk)
                chunks_per_load = int(bytes_per_load / bytes_per_chunk)
                loads_per_packet = int(math.ceil(bytes_per_chunk * chunks_per_packet / bytes_per_load))

                read_on_packet = 0
                for i in range(loads_per_packet):
                    load = ''
                    for j in range(chunks_per_load):
                        log.debug("Reading chunk %s in load %s:", j, i)
                        bytes_to_read = min(bytes_per_chunk, data_len - read_on_packet)
                        chunk = self._read_bytes(bytes_to_read)
                        if check_crc:
                            checksum_index = i * chunks_per_load + j
                            if checksum_index < len(checksums) and crc(chunk) != checksums[checksum_index]:
                                raise Exception("Checksum doesn't match")
                        load += chunk
                        total_read += len(chunk)
                        read_on_packet += len(chunk)
                    yield load
           
            # Send ClientReadStatusProto message confirming successful read
            request = ClientReadStatusProto()
            request.status = 0  # SUCCESS
            log_protobuf_message("ClientReadStatusProto:", request)
            s_request = request.SerializeToString()
            self.write_delimited(s_request)
            self._close_socket()

    def __repr__(self):
        return "DataXceiverChannel<%s:%d>" % (self.host, self.port)

########NEW FILE########
__FILENAME__ = client
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.

import snakebite.protobuf.ClientNamenodeProtocol_pb2 as client_proto
import snakebite.glob as glob
from snakebite.errors import RequestError
from snakebite.service import RpcService
from snakebite.errors import FileNotFoundException
from snakebite.errors import DirectoryException
from snakebite.errors import FileException
from snakebite.errors import InvalidInputException
from snakebite.errors import OutOfNNException
from snakebite.channel import DataXceiverChannel
from snakebite.config import HDFSConfig
from snakebite.namenode import Namenode

import Queue
import zlib
import bz2
import logging
import os
import os.path
import pwd
import fnmatch
import inspect
import socket
import errno
import time

log = logging.getLogger(__name__)


class Client(object):
    ''' A pure python HDFS client.

    **Example:**

    >>> from snakebite.client import Client
    >>> client = Client("localhost", 54310, use_trash=False)
    >>> for x in client.ls(['/']):
    ...     print x

    .. warning::

        Many methods return generators, which mean they need to be consumed to execute! Documentation will explicitly
        specify which methods return generators.

    .. note::
        ``paths`` parameters in methods are often passed as lists, since operations can work on multiple
        paths.

    .. note::
        Parameters like ``include_children`` and ``recurse`` are not used
        when paths contain globs.

    .. note::
        Different Hadoop distributions use different protocol versions. Snakebite defaults to 9, but this can be set by passing
        in the ``hadoop_version`` parameter to the constructor.
    '''
    FILETYPES = {
        1: "d",
        2: "f",
        3: "s"
    }

    def __init__(self, host, port=Namenode.DEFAULT_PORT, hadoop_version=Namenode.DEFAULT_VERSION, use_trash=False, effective_user=None):
        '''
        :param host: Hostname or IP address of the NameNode
        :type host: string
        :param port: RPC Port of the NameNode
        :type port: int
        :param hadoop_version: What hadoop protocol version should be used (default: 9)
        :type hadoop_version: int
        :param use_trash: Use a trash when removing files.
        :type use_trash: boolean
        :param effective_user: Effective user for the HDFS operations (default: None - current user)
        :type effective_user: string
        '''
        if hadoop_version < 9:
            raise Exception("Only protocol versions >= 9 supported")

        self.host = host
        self.port = port
        self.service_stub_class = client_proto.ClientNamenodeProtocol_Stub
        self.service = RpcService(self.service_stub_class, self.port, self.host, hadoop_version, effective_user)
        self.use_trash = use_trash
        self.trash = self._join_user_path(".Trash")

        log.debug("Created client for %s:%s with trash=%s" % (host, port, use_trash))

    def ls(self, paths, recurse=False, include_toplevel=False, include_children=True):
        ''' Issues 'ls' command and returns a list of maps that contain fileinfo

        :param paths: Paths to list
        :type paths: list
        :param recurse: Recursive listing
        :type recurse: boolean
        :param include_toplevel: Include the given path in the listing. If the path is a file, include_toplevel is always True.
        :type include_toplevel: boolean
        :param include_children: Include child nodes in the listing.
        :type include_children: boolean
        :returns: a generator that yields dictionaries

        **Examples:**

        Directory listing

        >>> list(client.ls(["/"]))
        [{'group': u'supergroup', 'permission': 420, 'file_type': 'f', 'access_time': 1367317324982L, 'block_replication': 1, 'modification_time': 1367317325346L, 'length': 6783L, 'blocksize': 134217728L, 'owner': u'wouter', 'path': '/Makefile'}, {'group': u'supergroup', 'permission': 493, 'file_type': 'd', 'access_time': 0L, 'block_replication': 0, 'modification_time': 1367317325431L, 'length': 0L, 'blocksize': 0L, 'owner': u'wouter', 'path': '/build'}, {'group': u'supergroup', 'permission': 420, 'file_type': 'f', 'access_time': 1367317326510L, 'block_replication': 1, 'modification_time': 1367317326522L, 'length': 100L, 'blocksize': 134217728L, 'owner': u'wouter', 'path': '/index.asciidoc'}, {'group': u'supergroup', 'permission': 493, 'file_type': 'd', 'access_time': 0L, 'block_replication': 0, 'modification_time': 1367317326628L, 'length': 0L, 'blocksize': 0L, 'owner': u'wouter', 'path': '/source'}]

        File listing

        >>> list(client.ls(["/Makefile"]))
        [{'group': u'supergroup', 'permission': 420, 'file_type': 'f', 'access_time': 1367317324982L, 'block_replication': 1, 'modification_time': 1367317325346L, 'length': 6783L, 'blocksize': 134217728L, 'owner': u'wouter', 'path': '/Makefile'}]

        Get directory information

        >>> list(client.ls(["/source"], include_toplevel=True, include_children=False))
        [{'group': u'supergroup', 'permission': 493, 'file_type': 'd', 'access_time': 0L, 'block_replication': 0, 'modification_time': 1367317326628L, 'length': 0L, 'blocksize': 0L, 'owner': u'wouter', 'path': '/source'}]
        '''
        if not isinstance(paths, list):
            raise InvalidInputException("Paths should be a list")

        for item in self._find_items(paths, self._handle_ls,
                                     include_toplevel=include_toplevel,
                                     include_children=include_children,
                                     recurse=recurse):
            if item:
                yield item

    LISTING_ATTRIBUTES = ['length', 'owner', 'group', 'block_replication',
                          'modification_time', 'access_time', 'blocksize']

    def _handle_ls(self, path, node):
        ''' Handle every node received for an ls request'''
        entry = {}

        entry["file_type"] = self.FILETYPES[node.fileType]
        entry["permission"] = node.permission.perm
        entry["path"] = path

        for attribute in self.LISTING_ATTRIBUTES:
            entry[attribute] = node.__getattribute__(attribute)

        return entry

    def chmod(self, paths, mode, recurse=False):
        ''' Change the mode for paths. This returns a list of maps containing the resut of the operation.

        :param paths: List of paths to chmod
        :type paths: list
        :param mode: Octal mode (e.g. 0755)
        :type mode: int
        :param recurse: Recursive chmod
        :type recurse: boolean
        :returns: a generator that yields dictionaries

        .. note:: The top level directory is always included when `recurse=True`'''
        if not isinstance(paths, list):
            raise InvalidInputException("Paths should be a list")
        if not paths:
            raise InvalidInputException("chmod: no path given")
        if not mode:
            raise InvalidInputException("chmod: no mode given")

        processor = lambda path, node, mode=mode: self._handle_chmod(path, node, mode)
        for item in self._find_items(paths, processor, include_toplevel=True,
                                     include_children=False, recurse=recurse):
            if item:
                yield item

    def _handle_chmod(self, path, node, mode):
        request = client_proto.SetPermissionRequestProto()
        request.src = path
        request.permission.perm = mode
        self.service.setPermission(request)
        return {"result": True, "path": path}

    def chown(self, paths, owner, recurse=False):
        ''' Change the owner for paths. The owner can be specified as `user` or `user:group`

        :param paths: List of paths to chmod
        :type paths: list
        :param owner: New owner
        :type owner: string
        :param recurse: Recursive chown
        :type recurse: boolean
        :returns: a generator that yields dictionaries

        This always include the toplevel when recursing.'''
        if not isinstance(paths, list):
            raise InvalidInputException("Paths should be a list")
        if not paths:
            raise InvalidInputException("chown: no path given")
        if not owner:
            raise InvalidInputException("chown: no owner given")

        processor = lambda path, node, owner=owner: self._handle_chown(path, node, owner)
        for item in self._find_items(paths, processor, include_toplevel=True,
                                     include_children=False, recurse=recurse):
            if item:
                yield item

    def _handle_chown(self, path, node, owner):
        if ":" in owner:
            (owner, group) = owner.split(":")
        else:
            group = ""

        request = client_proto.SetOwnerRequestProto()
        request.src = path
        if owner:
            request.username = owner
        if group:
            request.groupname = group
        self.service.setOwner(request)
        return {"result": True, "path": path}

    def chgrp(self, paths, group, recurse=False):
        ''' Change the group of paths.

        :param paths: List of paths to chgrp
        :type paths: list
        :param group: New group
        :type mode: string
        :param recurse: Recursive chgrp
        :type recurse: boolean
        :returns: a generator that yields dictionaries

        '''
        if not isinstance(paths, list):
            raise InvalidInputException("Paths should be a list")
        if not paths:
            raise InvalidInputException("chgrp: no paths given")
        if not group:
            raise InvalidInputException("chgrp: no group given")

        owner = ":%s" % group
        processor = lambda path, node, owner=owner: self._handle_chown(path, node, owner)
        for item in self._find_items(paths, processor, include_toplevel=True,
                                     include_children=False, recurse=recurse):
            if item:
                yield item

    def count(self, paths):
        ''' Count files in a path

        :param paths: List of paths to count
        :type paths: list
        :returns: a generator that yields dictionaries

        **Examples:**

        >>> list(client.count(['/']))
        [{'spaceConsumed': 260185L, 'quota': 2147483647L, 'spaceQuota': 18446744073709551615L, 'length': 260185L, 'directoryCount': 9L, 'path': '/', 'fileCount': 34L}]

        '''
        if not isinstance(paths, list):
            raise InvalidInputException("Paths should be a list")
        if not paths:
            raise InvalidInputException("count: no path given")

        for item in self._find_items(paths, self._handle_count, include_toplevel=True,
                                     include_children=False, recurse=False):
            if item:
                yield item

    COUNT_ATTRIBUTES = ['length', 'fileCount', 'directoryCount', 'quota', 'spaceConsumed', 'spaceQuota']

    def _handle_count(self, path, node):
        request = client_proto.GetContentSummaryRequestProto()
        request.path = path
        response = self.service.getContentSummary(request)
        entry = {"path": path}
        for attribute in self.COUNT_ATTRIBUTES:
            entry[attribute] = response.summary.__getattribute__(attribute)
        return entry

    def df(self):
        ''' Get FS information

        :returns: a dictionary

        **Examples:**

        >>> client.df()
        {'used': 491520L, 'capacity': 120137519104L, 'under_replicated': 0L, 'missing_blocks': 0L, 'filesystem': 'hdfs://localhost:54310', 'remaining': 19669295104L, 'corrupt_blocks': 0L}
        '''
        processor = lambda path, node: self._handle_df(path, node)
        return list(self._find_items(['/'], processor, include_toplevel=True, include_children=False, recurse=False))[0]

    def _handle_df(self, path, node):
        request = client_proto.GetFsStatusRequestProto()
        response = self.service.getFsStats(request)
        entry = {"filesystem": "hdfs://%s:%d" % (self.host, self.port)}
        for i in ['capacity', 'used', 'remaining', 'under_replicated',
                  'corrupt_blocks', 'missing_blocks']:
            entry[i] = response.__getattribute__(i)
        return entry

    def du(self, paths, include_toplevel=False, include_children=True):
        '''Returns size information for paths

        :param paths: Paths to du
        :type paths: list
        :param include_toplevel: Include the given path in the result. If the path is a file, include_toplevel is always True.
        :type include_toplevel: boolean
        :param include_children: Include child nodes in the result.
        :type include_children: boolean
        :returns: a generator that yields dictionaries

        **Examples:**

        Children:

        >>> list(client.du(['/']))
        [{'path': '/Makefile', 'length': 6783L}, {'path': '/build', 'length': 244778L}, {'path': '/index.asciidoc', 'length': 100L}, {'path': '/source', 'length': 8524L}]

        Directory only:

        >>> list(client.du(['/'], include_toplevel=True, include_children=False))
        [{'path': '/', 'length': 260185L}]

        '''
        if not isinstance(paths, list):
            raise InvalidInputException("Paths should be a list")
        if not paths:
            raise InvalidInputException("du: no path given")

        processor = lambda path, node: self._handle_du(path, node)
        for item in self._find_items(paths, processor, include_toplevel=include_toplevel,
                                     include_children=include_children, recurse=False):
            if item:
                yield item

    def _handle_du(self, path, node):
        if self._is_dir(node):
            request = client_proto.GetContentSummaryRequestProto()
            request.path = path
            try:
                response = self.service.getContentSummary(request)
                return {"path": path, "length": response.summary.length}
            except RequestError, e:
                print e
        else:
            return {"path": path, "length": node.length}

    def rename(self, paths, dst):
        ''' Rename (move) path(s) to a destination

        :param paths: Source paths
        :type paths: list
        :param dst: destination
        :type dst: string
        :returns: a generator that yields dictionaries
        '''
        if not isinstance(paths, list):
            raise InvalidInputException("Paths should be a list")
        if not paths:
            raise InvalidInputException("rename: no path given")
        if not dst:
            raise InvalidInputException("rename: no destination given")

        processor = lambda path, node, dst=dst: self._handle_rename(path, node, dst)
        for item in self._find_items(paths, processor, include_toplevel=True):
            if item:
                yield item

    def _handle_rename(self, path, node, dst):
        if not dst.startswith("/"):
            dst = self._join_user_path(dst)
        request = client_proto.RenameRequestProto()
        request.src = path
        request.dst = dst
        response = self.service.rename(request)
        return {"path": path, "result": response.result}

    def delete(self, paths, recurse=False):
        ''' Delete paths

        :param paths: Paths to delete
        :type paths: list
        :param recurse: Recursive delete (use with care!)
        :type recurse: boolean
        :returns: a generator that yields dictionaries

        .. note:: Recursive deletion uses the NameNode recursive deletion functionality
                 instead of letting the client recurse. Hadoops client recurses
                 by itself and thus showing all files and directories that are
                 deleted. Snakebite doesn't.
        '''
        if not isinstance(paths, list):
            raise InvalidInputException("Paths should be a list")
        if not paths:
            raise InvalidInputException("delete: no path given")

        processor = lambda path, node, recurse=recurse: self._handle_delete(path, node, recurse)
        for item in self._find_items(paths, processor, include_toplevel=True):
            if item:
                yield item

    def _handle_delete(self, path, node, recurse):
        if (self._is_dir(node) and not recurse):
            raise DirectoryException("rm: `%s': Is a directory" % path)

        # None might be passed in for recurse
        if not recurse:
            recurse = False

        if self.__should_move_to_trash(path):
            if path.endswith("/"):
                suffix_path = path[1:-1]
            else:
                suffix_path = path[1:]

            trash_path = os.path.join(self.trash, "Current", suffix_path)
            if trash_path.endswith("/"):
                trash_path = trash_path[:-1]

            base_trash_path = os.path.join(self.trash, "Current", os.path.dirname(suffix_path))
            if base_trash_path.endswith("/"):
                base_trash_path = base_trash_path[:-1]

            # Try twice, in case checkpoint between mkdir() and rename()
            for i in range(0, 2):
                list(self.mkdir([base_trash_path], create_parent=True, mode=0700))

                original_path = trash_path

                while self.test(trash_path, exists=True):
                    unix_timestamp = str(int(time.time() * 1000))
                    trash_path = "%s%s" % (original_path, unix_timestamp)

                result = self._handle_rename(path, node, trash_path)
                if result['result']:
                    result['message'] = ". Moved %s to %s" % (path, trash_path)
                    return result
            raise Exception("Failed to move to trash: %s" % path)
        else:
            request = client_proto.DeleteRequestProto()
            request.src = path
            request.recursive = recurse
            response = self.service.delete(request)
            return {"path": path, "result": response.result}

    def __should_move_to_trash(self, path):
        if not self.use_trash:
            return False
        if path.startswith(self.trash):
            return False  # Path already in trash
        if os.path.dirname(self.trash).startswith(path):
            raise Exception("Cannot move %s to the trash, as it contains the trash" % path)

        return True

    def rmdir(self, paths):
        ''' Delete a directory

        :param paths: Paths to delete
        :type paths: list
        :returns: a generator that yields dictionaries

        .. note: directories have to be empty.
        '''
        if not isinstance(paths, list):
            raise InvalidInputException("Paths should be a list")
        if not paths:
            raise InvalidInputException("rmdir: no path given")

        processor = lambda path, node: self._handle_rmdir(path, node)
        for item in self._find_items(paths, processor, include_toplevel=True):
            if item:
                yield item

    def _handle_rmdir(self, path, node):
        if not self._is_dir(node):
            raise DirectoryException("rmdir: `%s': Is not a directory" % path)

        # Check if the directory is empty
        files = self.ls([path])
        if len(list(files)) > 0:
            raise DirectoryException("rmdir: `%s': Directory is not empty" % path)

        return self._handle_delete(path, node, recurse=True)

    def touchz(self, paths, replication=None, blocksize=None):
        ''' Create a zero length file or updates the timestamp on a zero length file

        :param paths: Paths
        :type paths: list
        :param replication: Replication factor
        :type recurse: int
        :param blocksize: Block size (in bytes) of the newly created file
        :type blocksize: int
        :returns: a generator that yields dictionaries
        '''

        if not isinstance(paths, list):
            raise InvalidInputException("Paths should be a list")
        if not paths:
            raise InvalidInputException("touchz: no path given")

        # Let's get the blocksize and replication from the server defaults
        # provided by the namenode if they are not specified
        if not replication or not blocksize:
            defaults = self.serverdefaults()

        if not replication:
            replication = defaults['replication']
        if not blocksize:
            blocksize = defaults['blockSize']

        processor = lambda path, node, replication=replication, blocksize=blocksize: self._handle_touchz(path, node, replication, blocksize)
        for item in self._find_items(paths, processor, include_toplevel=True, check_nonexistence=True, include_children=False):
            if item:
                yield item

    def _handle_touchz(self, path, node, replication, blocksize):
        # Item already exists
        if node:
            if node.length != 0:
                raise FileException("touchz: `%s': Not a zero-length file" % path)
            if self._is_dir(node):
                raise DirectoryException("touchz: `%s': Is a directory" % path)

            response = self._create_file(path, replication, blocksize, overwrite=True)
        else:
            # Check if the parent directory exists
            parent = self._get_file_info(os.path.dirname(path))
            if not parent:
                raise DirectoryException("touchz: `%s': No such file or directory" % path)
            else:
                response = self._create_file(path, replication, blocksize, overwrite=False)
        return {"path": path, "result": response.result}

    def setrep(self, paths, replication, recurse=False):
        ''' Set the replication factor for paths

        :param paths: Paths
        :type paths: list
        :param replication: Replication factor
        :type recurse: int
        :param recurse: Apply replication factor recursive
        :type recurse: boolean
        :returns: a generator that yields dictionaries
        '''
        if not isinstance(paths, list):
            raise InvalidInputException("Paths should be a list")
        if not paths:
            raise InvalidInputException("setrep: no path given")
        if not replication:
            raise InvalidInputException("setrep: no replication given")

        processor = lambda path, node, replication=replication: self._handle_setrep(path, node, replication)
        for item in self._find_items(paths, processor, include_toplevel=True,
                                     include_children=False, recurse=recurse):
            if item:
                yield item

    def _handle_setrep(self, path, node, replication):
        if not self._is_dir(node):
            request = client_proto.SetReplicationRequestProto()
            request.src = path
            request.replication = replication
            response = self.service.setReplication(request)
            return {"result": response.result, "path": path}

    def cat(self, paths, check_crc=False):
        ''' Fetch all files that match the source file pattern
        and display their content on stdout.

        :param paths: Paths to display
        :type paths: list of strings
        :param check_crc: Check for checksum errors
        :type check_crc: boolean
        :returns: a generator that yields strings
        '''
        if not isinstance(paths, list):
            raise InvalidInputException("Paths should be a list")
        if not paths:
            raise InvalidInputException("cat: no path given")

        processor = lambda path, node, check_crc=check_crc: self._handle_cat(path, node, check_crc)
        for item in self._find_items(paths, processor, include_toplevel=True,
                                     include_children=False, recurse=False):
            if item:
                yield item

    def _handle_cat(self, path, node, check_crc):
        if self._is_dir(node):
            raise DirectoryException("cat: `%s': Is a directory" % path)

        for load in self._read_file(path, node, False, check_crc):
            if load:
                yield load

    def copyToLocal(self, paths, dst, check_crc=False):
        ''' Copy files that match the file source pattern
        to the local name.  Source is kept.  When copying multiple,
        files, the destination must be a directory.

        :param paths: Paths to copy
        :type paths: list of strings
        :param dst: Destination path
        :type dst: string
        :param check_crc: Check for checksum errors
        :type check_crc: boolean
        :returns: a generator that yields strings
        '''
        if not isinstance(paths, list):
            raise InvalidInputException("Paths should be a list")
        if not paths:
            raise InvalidInputException("copyToLocal: no path given")
        if not dst:
            raise InvalidInputException("copyToLocal: no destination given")

        self.base_source = None
        processor = lambda path, node, dst=dst, check_crc=check_crc: self._handle_copyToLocal(path, node, dst, check_crc)
        for item in self._find_items(paths, processor, include_toplevel=True, recurse=True, include_children=True):
            if item:
                yield item

    def _handle_copyToLocal(self, path, node, dst, check_crc):
        # Calculate base directory using the first node only
        if self.base_source is None:
            self.dst = os.path.abspath(dst)
            if os.path.isdir(dst):  # If input destination is an existing directory, include toplevel
                self.base_source = os.path.dirname(path)
            else:
                self.base_source = path

            if self.base_source.endswith("/"):
                self.base_source = self.base_source[:-1]

        target = dst + (path.replace(self.base_source, "", 1))

        error = ""
        result = False
        # Target is an existing file
        if os.path.isfile(target):
            error += "file exists"
        # Target is an existing directory
        elif os.path.isdir(target):
            error += "directory exists"
        # Source is a directory
        elif self._is_dir(node):
            os.makedirs(target, mode=node.permission.perm)
            result = True
        # Source is a file
        elif self._is_file(node):
            temporary_target = "%s._COPYING_" % target
            f = open(temporary_target, 'w')
            try:
                for load in self._read_file(path, node, tail_only=False, check_crc=check_crc):
                    f.write(load)
                f.close()
                os.rename(temporary_target, target)
                result = True
            except Exception, e:
                result = False
                error = e
                if os.path.isfile(temporary_target):
                    os.remove(temporary_target)

        return {"path": target, "result": result, "error": error, "source_path": path}

    def getmerge(self, path, dst, newline=False, check_crc=False):
        ''' Get all the files in the directories that
        match the source file pattern and merge and sort them to only
        one file on local fs.

        :param paths: Directory containing files that will be merged
        :type paths: string
        :param dst: Path of file that will be written
        :type dst: string
        :param nl: Add a newline character at the end of each file.
        :type nl: boolean
        :returns: string content of the merged file at dst
        '''
        if not path:
            raise InvalidInputException("getmerge: no path given")
        if not dst:
            raise InvalidInputException("getmerge: no destination given")

        temporary_target = "%s._COPYING_" % dst
        f = open(temporary_target, 'w')

        processor = lambda path, node, dst=dst, check_crc=check_crc: self._handle_getmerge(path, node, dst, check_crc)
        try:
            for item in self._find_items([path], processor, include_toplevel=True, recurse=False, include_children=True):
                for load in item:
                    if load['result']:
                        f.write(load['response'])
                    elif not load['error'] is '':
                        if os.path.isfile(temporary_target):
                            os.remove(temporary_target)
                        raise Exception(load['error'])
                if newline and load['response']:
                    f.write("\n")
            yield {"path": dst, "response": '', "result": True, "error": load['error'], "source_path": path}

        finally:
            if os.path.isfile(temporary_target):
                f.close()
                os.rename(temporary_target, dst)

    def _handle_getmerge(self, path, node, dst, check_crc):
        log.debug("in handle getmerge")
        error = ''
        if not self._is_file(node):
            # Target is an existing file
            if os.path.isfile(dst):
                error += "target file exists"
            # Target is an existing directory
            elif os.path.isdir(dst):
                error += "target directory exists"
            yield {"path": path, "response": '', "result": False, "error": error, "source_path": path}
        # Source is a file
        else:
            if node.length == 0:  # Empty file
                yield {"path": path, "response": '', "result": True, "error": error, "source_path": path}
            else:
                try:
                    for load in self._read_file(path, node, tail_only=False, check_crc=check_crc):
                        yield {"path": path, "response": load, "result": True, "error": error, "source_path": path}
                except Exception, e:
                    error = e
                    yield {"path": path, "response": '', "result": False, "error": error, "source_path": path}

    def stat(self, paths):
        ''' Stat a fileCount

        :param paths: Path
        :type paths: string
        :returns: a dictionary

        **Example:**

        >>> client.stat(['/index.asciidoc'])
        {'blocksize': 134217728L, 'owner': u'wouter', 'length': 100L, 'access_time': 1367317326510L, 'group': u'supergroup', 'permission': 420, 'file_type': 'f', 'path': '/index.asciidoc', 'modification_time': 1367317326522L, 'block_replication': 1}
        '''
        if not isinstance(paths, list):
            raise InvalidInputException("Paths should be a list")
        if not paths:
            raise InvalidInputException("stat: no path given")

        processor = lambda path, node: self._handle_stat(path, node)
        return list(self._find_items(paths, processor, include_toplevel=True))[0]

    def _handle_stat(self, path, node):
        return {"path": path,
                "file_type": self.FILETYPES[node.fileType],
                "length": node.length,
                "permission": node.permission.perm,
                "owner": node.owner,
                "group": node.group,
                "modification_time": node.modification_time,
                "access_time": node.access_time,
                "block_replication": node.block_replication,
                "blocksize": node.blocksize}

    def tail(self, path, append=False):
        # Note: append is currently not implemented.
        ''' Show the last 1KB of the file.

        :param path: Path to read
        :type path: string
        :param f: Shows appended data as the file grows.
        :type f: boolean
        :returns: a generator that yields strings
        '''
        if not path:
            raise InvalidInputException("tail: no path given")

        processor = lambda path, node, tail_only=True, append=append: self._handle_tail(path, node, tail_only, append)
        for item in self._find_items([path], processor, include_toplevel=True,
                                     include_children=False, recurse=False):
            if item:
                yield item

    def _handle_tail(self, path, node, tail_only, append):
        data = ''
        for load in self._read_file(path, node, tail_only=True, check_crc=False):
            data += load
        # We read only the necessary packets but still
        # need to cut off at the packet level.
        return data[max(0, len(data)-1024):len(data)]

    def test(self, path, exists=False, directory=False, zero_length=False):
        '''Test if a path exist, is a directory or has zero length

        :param path: Path to test
        :type path: string
        :param exists: Check if the path exists
        :type exists: boolean
        :param directory: Check if the path is a directory
        :type exists: boolean
        :param zero_length: Check if the path is zero-length
        :type zero_length: boolean
        :returns: a boolean

        .. note:: directory and zero length are AND'd.
        '''
        if not isinstance(path, str):
            raise InvalidInputException("Path should be a string")
        if not path:
            raise InvalidInputException("test: no path given")

        processor = lambda path, node, exists=exists, directory=directory, zero_length=zero_length: self._handle_test(path, node, exists, directory, zero_length)
        try:
            items = list(self._find_items([path], processor, include_toplevel=True))
            if len(items) == 0:
                return False
            return all(items)
        except FileNotFoundException, e:
            if exists:
                return False
            else:
                raise e

    def _handle_test(self, path, node, exists, directory, zero_length):
        return self._is_directory(directory, node) and self._is_zero_length(zero_length, node)

    def text(self, paths, check_crc=False):
        ''' Takes a source file and outputs the file in text format.
        The allowed formats are gzip and bzip2

        :param paths: Paths to display
        :type paths: list of strings
        :param check_crc: Check for checksum errors
        :type check_crc: boolean
        :returns: a generator that yields strings
        '''
        if not isinstance(paths, list):
            raise InvalidInputException("Paths should be a list")
        if not paths:
            raise InvalidInputException("text: no path given")

        processor = lambda path, node, check_crc=check_crc: self._handle_text(path, node, check_crc)
        for item in self._find_items(paths, processor, include_toplevel=True,
                                     include_children=False, recurse=False):
            if item:
                yield item

    def _handle_text(self, path, node, check_crc):
        if self._is_dir(node):
            raise DirectoryException("text: `%s': Is a directory" % path)

        text = ''
        for load in self._read_file(path, node, False, check_crc):
            text += load

        extension = os.path.splitext(path)[1]
        if extension == '.gz':
            return zlib.decompress(text, 16+zlib.MAX_WBITS)
        elif extension == '.bz2':
            return bz2.decompress(text)
        else:
            return text

    def mkdir(self, paths, create_parent=False, mode=0755):
        ''' Create a directoryCount

        :param paths: Paths to create
        :type paths: list of strings
        :param create_parent: Also create the parent directories
        :type create_parent: boolean
        :param mode: Mode the directory should be created with
        :type mode: int
        :returns: a generator that yields dictionaries
        '''
        if not isinstance(paths, list):
            raise InvalidInputException("Paths should be a list")
        if not paths:
            raise InvalidInputException("mkdirs: no path given")

        for path in paths:
            if not path.startswith("/"):
                path = self._join_user_path(path)

            fileinfo = self._get_file_info(path)
            if not fileinfo:
                try:
                    request = client_proto.MkdirsRequestProto()
                    request.src = path
                    request.masked.perm = mode
                    request.createParent = create_parent
                    response = self.service.mkdirs(request)
                    yield {"path": path, "result": response.result}
                except RequestError, e:
                    yield {"path": path, "result": False, "error": str(e)}
            else:
                yield {"path": path, "result": False, "error": "mkdir: `%s': File exists" % path}

    def serverdefaults(self):
        '''Get server defaults

        :returns: dictionary

        **Example:**

        >>> client.serverdefaults()
        [{'writePacketSize': 65536, 'fileBufferSize': 4096, 'replication': 1, 'bytesPerChecksum': 512, 'trashInterval': 0L, 'blockSize': 134217728L, 'encryptDataTransfer': False, 'checksumType': 2}]
        '''
        request = client_proto.GetServerDefaultsRequestProto()
        response = self.service.getServerDefaults(request).serverDefaults
        return {'blockSize': response.blockSize, 'bytesPerChecksum': response.bytesPerChecksum,
                'writePacketSize': response.writePacketSize, 'replication': response.replication,
                'fileBufferSize': response.fileBufferSize, 'encryptDataTransfer': response.encryptDataTransfer,
                'trashInterval': response.trashInterval, 'checksumType': response.checksumType}

    def _is_directory(self, should_check, node):
        if not should_check:
            return True
        return self._is_dir(node)

    def _is_zero_length(self, should_check, node):
        if not should_check:
            return True
        return node.length == 0

    def _get_full_path(self, path, node):
        if node.path:
            return os.path.join(path, node.path)
        else:
            return path

    def _create_file(self, path, replication, blocksize, overwrite):
        if overwrite:
            createFlag = 0x02
        else:
            createFlag = 0x01

        # Issue a CreateRequestProto
        request = client_proto.CreateRequestProto()
        request.src = path
        request.masked.perm = 0644
        request.clientName = "snakebite"
        request.createFlag = createFlag
        request.createParent = False
        request.replication = replication
        request.blockSize = blocksize

        # The response doesn't contain anything
        self.service.create(request)

        # Issue a CompleteRequestProto
        request = client_proto.CompleteRequestProto()
        request.src = path
        request.clientName = "snakebite"

        return self.service.complete(request)

    def _read_file(self, path, node, tail_only, check_crc):
        length = node.length

        request = client_proto.GetBlockLocationsRequestProto()
        request.src = path
        request.length = length

        if tail_only:  # Only read last KB
            request.offset = max(0, length - 1024)
        else:
            request.offset = 0L
        response = self.service.getBlockLocations(request)

        if response.locations.fileLength == 0:  # Can't read empty file
            yield ""
        lastblock = response.locations.lastBlock

        if tail_only:
            if lastblock.b.blockId == response.locations.blocks[0].b.blockId:
                num_blocks_tail = 1  # Tail is on last block
            else:
                num_blocks_tail = 2  # Tail is on two blocks

        failed_nodes = []
        total_bytes_read = 0
        for block in response.locations.blocks:
            length = block.b.numBytes
            pool_id = block.b.poolId
            offset_in_block = 0
            if tail_only:
                if num_blocks_tail == 2 and block.b.blockId != lastblock.b.blockId:
                    offset_in_block = block.b.numBytes - (1024 - lastblock.b.numBytes)
                elif num_blocks_tail == 1:
                    offset_in_block = max(0, lastblock.b.numBytes - 1024)

            # Prioritize locations to read from
            locations_queue = Queue.PriorityQueue()  # Primitive queuing based on a node's past failure
            for location in block.locs:
                if location.id.storageID in failed_nodes:
                    locations_queue.put((1, location))  # Priority num, data
                else:
                    locations_queue.put((0, location))

            # Read data
            successful_read = False
            while not locations_queue.empty():
                location = locations_queue.get()[1]
                host = location.id.ipAddr
                port = int(location.id.xferPort)
                data_xciever = DataXceiverChannel(host, port)
                if data_xciever.connect():
                    try:
                        for load in data_xciever.readBlock(length, pool_id, block.b.blockId, block.b.generationStamp, offset_in_block, check_crc):
                            offset_in_block += len(load)
                            total_bytes_read += len(load)
                            successful_read = True
                            yield load
                    except Exception, e:
                        log.error(e)
                        if not location.id.storageID in failed_nodes:
                            failed_nodes.append(location.id.storageID)
                        successful_read = False
                else:
                    raise Exception
                if successful_read:
                    break
            if successful_read is False:
                raise Exception("Failure to read block %s" % block.b.blockId)

    def _find_items(self, paths, processor, include_toplevel=False, include_children=False, recurse=False, check_nonexistence=False):
        ''' Request file info from the NameNode and call the processor on the node(s) returned

        :param paths:
            A list of paths that need to be processed
        :param processor:
            Method that is called on an node. Method signature should be foo(path, node). For additional
            (static) params, use a lambda.
        :param include_toplevel:
            Boolean to enable the inclusion of the first node found.
            Example: listing a directory should not include the toplevel, but chmod should
            only operate on the path that is input, so it should include the toplevel.
        :param include_children:
            Include children (when the path is a directory) in processing. Recurse will always
            include children.
            Example: listing a directory should include children, but chmod shouldn't.
        :param recurse:
            Recurse into children if they are directories.
        '''

        if not paths:
            paths = [os.path.join("/user", pwd.getpwuid(os.getuid())[0])]

        # Expand paths if necessary (/foo/{bar,baz} --> ['/foo/bar', '/foo/baz'])
        paths = glob.expand_paths(paths)

        for path in paths:
            if not path.startswith("/"):
                path = self._join_user_path(path)

            log.debug("Trying to find path %s" % path)

            if glob.has_magic(path):
                log.debug("Dealing with globs in %s" % path)
                for item in self._glob_find(path, processor, include_toplevel):
                    yield item
            else:
                fileinfo = self._get_file_info(path)
                if not fileinfo and not check_nonexistence:
                    raise FileNotFoundException("`%s': No such file or directory" % path)
                elif not fileinfo and check_nonexistence:
                    yield processor(path, None)
                    return

                if (include_toplevel and fileinfo) or not self._is_dir(fileinfo.fs):
                    # Construct the full path before processing
                    full_path = self._get_full_path(path, fileinfo.fs)
                    log.debug("Added %s to to result set" % full_path)
                    entry = processor(full_path, fileinfo.fs)
                    yield entry

                if self._is_dir(fileinfo.fs) and (include_children or recurse):
                    for node in self._get_dir_listing(path):
                        full_path = self._get_full_path(path, node)
                        entry = processor(full_path, node)
                        yield entry

                        # Recurse into directories
                        if recurse and self._is_dir(node):
                            # Construct the full path before processing
                            full_path = os.path.join(path, node.path)
                            for item in self._find_items([full_path],
                                                         processor,
                                                         include_toplevel=False,
                                                         include_children=False,
                                                         recurse=recurse):
                                yield item

    def _get_dir_listing(self, path, start_after=''):
        request = client_proto.GetListingRequestProto()
        request.src = path
        request.startAfter = start_after
        request.needLocation = False
        listing = self.service.getListing(request)
        if not listing:
            return
        for node in listing.dirList.partialListing:
            start_after = node.path
            yield node
        if listing.dirList.remainingEntries > 0:
            for node in self._get_dir_listing(path, start_after):
                yield node

    def _glob_find(self, path, processor, include_toplevel):
        '''Handle globs in paths.
        This is done by listing the directory before a glob and checking which
        node matches the initial glob. If there are more globs in the path,
        we don't add the found children to the result, but traverse into paths
        that did have a match.
        '''

        # Remove the last / from the path, since hadoop doesn't understand it
        if path.endswith("/"):
            path = path[:-1]

        # Split path elements and check where the first occurence of magic is
        path_elements = path.split("/")
        for i, element in enumerate(path_elements):
            if glob.has_magic(element):
                first_magic = i
                break

        # Create path that we check first to get a listing we match all children
        # against. If the 2nd path element is a glob, we need to check "/", and
        # we hardcode that, since "/".join(['']) doesn't return "/"
        if first_magic == 1:
            check_path = "/"
        else:
            check_path = "/".join(path_elements[:first_magic])

        # Path that we need to match against
        match_path = "/".join(path_elements[:first_magic + 1])

        # Rest of the unmatched path. In case the rest is only one element long
        # we prepend it with "/", since "/".join(['x']) doesn't return "/x"
        rest_elements = path_elements[first_magic + 1:]
        if len(rest_elements) == 1:
            rest = rest_elements[0]
        else:
            rest = "/".join(rest_elements)
        # Check if the path exists and that it's a directory (which it should..)
        fileinfo = self._get_file_info(check_path)
        if fileinfo and self._is_dir(fileinfo.fs):
            # List all child nodes and match them agains the glob
            for node in self._get_dir_listing(check_path):
                full_path = self._get_full_path(check_path, node)
                if fnmatch.fnmatch(full_path, match_path):
                    # If we have a match, but need to go deeper, we recurse
                    if rest and glob.has_magic(rest):
                        traverse_path = "/".join([full_path, rest])
                        for item in self._glob_find(traverse_path, processor, include_toplevel):
                            yield item
                    elif rest:
                        # we have more rest, but it's not magic, which is either a file or a directory
                        final_path = os.path.join(full_path, rest)
                        fi = self._get_file_info(final_path)
                        if fi and self._is_dir(fi.fs):
                            for n in self._get_dir_listing(final_path):
                                full_child_path = self._get_full_path(final_path, n)
                                yield processor(full_child_path, n)
                        elif fi:
                            yield processor(final_path, fi.fs)
                    else:
                        # If the matching node is a directory, we list the directory
                        # This is what the hadoop client does at least.
                        if self._is_dir(node):
                            if include_toplevel:
                                yield processor(full_path, node)
                            fp = self._get_full_path(check_path, node)
                            dir_list = self._get_dir_listing(fp)
                            if dir_list:  # It might happen that the directory above has been removed
                                for n in dir_list:
                                    full_child_path = self._get_full_path(fp, n)
                                    yield processor(full_child_path, n)
                        else:
                            yield processor(full_path, node)

    def _is_dir(self, entry):
        return self.FILETYPES.get(entry.fileType) == "d"

    def _is_file(self, entry):
        return self.FILETYPES.get(entry.fileType) == "f"

    def _get_file_info(self, path):
        request = client_proto.GetFileInfoRequestProto()
        request.src = path

        return self.service.getFileInfo(request)

    def _join_user_path(self, path):
        return os.path.join("/user", pwd.getpwuid(os.getuid())[0], path)

    def _remove_user_path(self, path):
        dir_to_remove = os.path.join("/user", pwd.getpwuid(os.getuid())[0])
        return path.replace(dir_to_remove+'/', "", 1)


class HAClient(Client):
    ''' Snakebite client with support for High Availability

    HAClient is fully backwards compatible with the vanilla Client and can be used for a non HA cluster as well.

    **Example:**

    >>> from snakebite.client import HAClient
    >>> from snakebite.namenode import Namenode
    >>> n1 = Namenode("namenode1.mydomain", 54310)
    >>> n2 = Namenode("namenode2.mydomain", 54310)
    >>> client = HAClient([n1, n2], use_trash=True)
    >>> for x in client.ls(['/']):
    ...     print x

    .. note::
        Different Hadoop distributions use different protocol versions. Snakebite defaults to 9, but this can be set by passing
        in the ``version`` parameter to the Namenode class constructor.
    '''

    @classmethod
    def _wrap_methods(cls):
        # Add HA support to all public Client methods, but only do this when we haven't done this before
        for name, meth in inspect.getmembers(cls, inspect.ismethod):
            if not name.startswith("_"): # Only public methods
                if inspect.isgeneratorfunction(meth):
                    setattr(cls, name, cls._ha_gen_method(meth))
                else:
                    setattr(cls, name, cls._ha_return_method(meth))

    def __init__(self, namenodes, use_trash=False, effective_user=None):
        '''
        :param namenodes: Set of namenodes for HA setup
        :type namenodes: list
        :param use_trash: Use a trash when removing files.
        :type use_trash: boolean
        :param effective_user: Effective user for the HDFS operations (default: None - current user)
        :type effective_user: string
        '''
        self.use_trash = use_trash
        self.effective_user = effective_user
        self.namenode = self._switch_namenode(namenodes)
        self.namenode.next()

    def _switch_namenode(self, namenodes):
        for namenode in namenodes:
            log.debug("Switch to namenode: %s:%d" % (namenode.host, namenode.port))

            yield super(HAClient, self).__init__(namenode.host,
                                                 namenode.port,
                                                 namenode.version,
                                                 self.use_trash,
                                                 self.effective_user)
        else:
            msg = "Request tried and failed for all %d namenodes: " % len(namenodes)
            for namenode in namenodes:
                msg += "\n\t* %s:%d" % (namenode.host, namenode.port)
            msg += "\nLook into debug messages - add -D flag!"
            raise OutOfNNException(msg)

    def __handle_request_error(self, exception):
        log.debug("Request failed with %s" % exception)
        if exception.args[0].startswith("org.apache.hadoop.ipc.StandbyException"):
            pass
        else:
            # There's a valid NN in active state, but there's still request error - raise
            raise
        self.namenode.next()

    def __handle_socket_error(self, exception):
        log.debug("Request failed with %s" % exception)
        if exception.errno == errno.ECONNREFUSED:
            # if NN is down or machine is not available, pass it:
            pass
        elif isinstance(exception, socket.timeout):
            # if there's communication/socket timeout, pass it:
            pass
        else:
            raise
        self.namenode.next()

    @staticmethod
    def _ha_return_method(func):
        ''' Method decorator for 'return type' methods '''
        def wrapped(self, *args, **kw):
            while(True): # switch between all namenodes
                try:
                    return func(self, *args, **kw)
                except RequestError as e:
                    self.__handle_request_error(e)
                except socket.error as e:
                    self.__handle_socket_error(e)
        return wrapped

    @staticmethod
    def _ha_gen_method(func):
        ''' Method decorator for 'generator type' methods '''
        def wrapped(self, *args, **kw):
            while(True): # switch between all namenodes
                try:
                    results = func(self, *args, **kw)
                    while(True): # yield all results
                        yield results.next()
                except RequestError as e:
                    self.__handle_request_error(e)
                except socket.error as e:
                    self.__handle_socket_error(e)
        return wrapped

HAClient._wrap_methods()

class AutoConfigClient(HAClient):
    ''' A pure python HDFS client that support HA and is auto configured through the ``HADOOP_PATH`` environment variable.

    HAClient is fully backwards compatible with the vanilla Client and can be used for a non HA cluster as well.
    This client tries to read ``${HADOOP_PATH}/conf/hdfs-site.xml`` to get the address of the namenode.
    The behaviour is the same as Client.

    **Example:**

    >>> from snakebite.client import AutoConfigClient
    >>> client = AutoConfigClient()
    >>> for x in client.ls(['/']):
    ...     print x

    .. note::
        Different Hadoop distributions use different protocol versions. Snakebite defaults to 9, but this can be set by passing
        in the ``hadoop_version`` parameter to the constructor.
    '''
    def __init__(self, hadoop_version=Namenode.DEFAULT_VERSION, effective_user=None):
        '''
        :param hadoop_version: What hadoop protocol version should be used (default: 9)
        :type hadoop_version: int
        :param effective_user: Effective user for the HDFS operations (default: None - current user)
        :type effective_user: string
        '''

        configs = HDFSConfig.get_external_config()
        nns = [Namenode(c['namenode'], c['port'], hadoop_version) for c in configs]
        super(AutoConfigClient, self).__init__(nns, HDFSConfig.use_trash, effective_user)

########NEW FILE########
__FILENAME__ = commandlineparser
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.

import argparse
import sys
import os
import pwd
import json
from urlparse import urlparse

from snakebite.client import HAClient
from snakebite.errors import FileNotFoundException
from snakebite.errors import DirectoryException
from snakebite.errors import FileException
from snakebite.errors import RequestError
from snakebite.formatter import format_listing
from snakebite.formatter import format_results
from snakebite.formatter import format_counts
from snakebite.formatter import format_fs_stats
from snakebite.formatter import format_stat
from snakebite.formatter import format_du
from snakebite.config import HDFSConfig
from snakebite.version import version
from snakebite.namenode import Namenode


def print_error_exit(msg, fd=sys.stderr):
    print >> fd, "Error: %s" % msg
    sys.exit(-1)

def print_info(msg, fd=sys.stderr):
    print >> fd, "Info: %s" % msg

def exitError(error):
    if isinstance(error, FileNotFoundException) or \
       isinstance(error, DirectoryException) or \
       isinstance(error, FileException):
        print str(error)
    elif isinstance(error, RequestError):
        print "Request error: %s" % str(error)
    else:
        raise error
    sys.exit(-1)


def command(args="", descr="", allowed_opts="", visible=True):
    def wrap(f):
        Commands.methods[f.func_name] = {"method": f,
                                         "args": args,
                                         "descr": descr,
                                         "allowed_opts": allowed_opts,
                                         "visible": visible}
    return wrap


class Commands(object):
    methods = {}


class ArgumentParserError(Exception):

    def __init__(self, message, error_message, prog, stdout=None, stderr=None, error_code=None):
        Exception.__init__(self, message, stdout, stderr)
        self.message = message
        self.error_message = error_message
        self.prog = prog


class Parser(argparse.ArgumentParser):
    def print_help(self):
        print ''.join([self.usage, self.epilog])

    def error(self, message):  # Override error message to show custom help.
        raise ArgumentParserError("SystemExit", message, self.prog)


class CommandLineParser(object):

    GENERIC_OPTS = {'D': {"short": '-D',
                          "long": '--debug',
                          "help": 'Show debug information',
                          "action": 'store_true'},
                    'j': {"short": '-j',
                          "long": '--json',
                          "help": 'JSON output',
                          "action": 'store_true'},
                    'n': {"short": '-n',
                          "long": '--namenode',
                          "help": 'namenode host',
                          "type": str},
                    'V': {"short": '-V',
                          "long": '--version',
                          "help": 'Hadoop protocol version (default:%d)' % Namenode.DEFAULT_VERSION,
                          "default": Namenode.DEFAULT_VERSION,
                          "type": float},
                    'p': {"short": '-p',
                          "long": '--port',
                          "help": 'namenode RPC port (default: %d)' % Namenode.DEFAULT_PORT,
                          "type": int},
                    'h': {"short": '-h',
                          "long": '--help',
                          "help": 'show help',
                          "type": int},
                    'v': {"short": '-v',
                          "long": '--ver',
                          "help": 'Display snakebite version',
                          "type": int}
                    }

    SUB_OPTS = {'R': {"short": '-R',
                      "long": '--recurse',
                      "help": 'recurse into subdirectories',
                      "action": 'store_true'},
                'd': {"short": '-d',
                      "long": '--directory',
                      "help": 'show only the path and no children / check if path is a dir',
                      "action": 'store_true'},
                's': {"short": '-s',
                      "long": '--summary',
                      "help": 'print summarized output',
                      "action": 'store_true'},
                'S': {"short": '-S',
                      "long": '--skiptrash',
                      "help": 'skip the trash (when trash is enabled)',
                      "default": False,
                      "action": 'store_true'},
                'T': {"short": '-T',
                      "long": "--usetrash",
                      "help": "enable the trash",
                      "action": 'store_true'},
                'z': {"short": '-z',
                      "long": '--zero',
                      "help": 'check for zero length',
                      "action": 'store_true'},
                'e': {"short": '-e',
                      "long": '--exists',
                      "help": 'check if file exists',
                      "action": 'store_true'},
                'checkcrc': {"short": '-checkcrc',
                             "long": "--checkcrc",
                             "help": 'check Crc',
                             "action": 'store_true'},
                'f': {"short": '-f',
                      "long": "--append",
                      "help": 'show appended data as the file grows',
                      "action": 'store_true'},
                'nl': {"short": '-nl',
                       "long": "--newline",
                       "help": 'add a newline character at the end of each file.',
                       "action": 'store_true'},
                'h': {"short": '-h',
                      "long": '--human',
                      "help": 'human readable output',
                      "action": 'store_true'}
                }

    def __init__(self):
        usage = "snakebite [general options] cmd [arguments]"
        epilog = "\ngeneral options:\n"
        epilog += "\n".join(sorted(["  %-30s %s" % ("%s %s" % (v['short'], v['long']), v['help']) for k, v in self.GENERIC_OPTS.iteritems()]))
        epilog += "\n\ncommands:\n"
        epilog += "\n".join(sorted(["  %-30s %s" % ("%s %s" % (k, v['args']), v['descr']) for k, v in Commands.methods.iteritems() if v['visible']]))
        epilog += "\n\nto see command-specific options use: snakebite [cmd] --help"

        self.parser = Parser(usage=usage, epilog=epilog, formatter_class=argparse.RawTextHelpFormatter, add_help=False)
        self._build_parent_parser()
        self._add_subparsers()
        self.namenodes = []

    def _build_parent_parser(self):
        #general options
        for opt_name, opt_data in self.GENERIC_OPTS.iteritems():
            if 'action' in opt_data:
                self.parser.add_argument(opt_data['short'], opt_data['long'], help=opt_data['help'], action=opt_data['action'])
            else:
                if 'default' in opt_data:
                    self.parser.add_argument(opt_data['short'], opt_data['long'], help=opt_data['help'], type=opt_data['type'], default=opt_data['default'])
                else:
                    self.parser.add_argument(opt_data['short'], opt_data['long'], help=opt_data['help'], type=opt_data['type'])

    def _add_subparsers(self):
        default_dir = os.path.join("/user", pwd.getpwuid(os.getuid())[0])

        #sub-options
        arg_parsers = {}
        for opt_name, opt_data in self.SUB_OPTS.iteritems():
            arg_parsers[opt_name] = argparse.ArgumentParser(add_help=False)
            arg_parsers[opt_name].add_argument(opt_data['short'], opt_data['long'], help=opt_data['help'],
                                               action=opt_data['action'])

        subcommand_help_parser = argparse.ArgumentParser(add_help=False)
        subcommand_help_parser.add_argument('-H', '--help', action='store_true')

        # NOTE: args and dirs are logically equivalent except for default val.
        # Difference in naming gives more valuable error/help output.

        # 0 or more dirs
        positional_arg_parsers = {}
        positional_arg_parsers['[dirs]'] = argparse.ArgumentParser(add_help=False)
        positional_arg_parsers['[dirs]'].add_argument('dir', nargs='*', default=[default_dir], help="[dirs]")

        # 1 or more dirs
        positional_arg_parsers['dir [dirs]'] = argparse.ArgumentParser(add_help=False)
        positional_arg_parsers['dir [dirs]'].add_argument('dir', nargs='+', default=[default_dir], help="dir [dirs]")

        # 2 dirs
        positional_arg_parsers['src dst'] = argparse.ArgumentParser(add_help=False)
        positional_arg_parsers['src dst'].add_argument('src_dst', nargs=2, default=[default_dir], help="src dst")

        # 1 or more args
        positional_arg_parsers['[args]'] = argparse.ArgumentParser(add_help=False)
        positional_arg_parsers['[args]'].add_argument('arg', nargs='*', help="[args]")

        # 1 arg
        positional_arg_parsers['arg'] = argparse.ArgumentParser(add_help=False)
        positional_arg_parsers['arg'].add_argument('single_arg', default=default_dir, help="arg")

        # 1 (integer) arg
        positional_arg_parsers['(int) arg'] = argparse.ArgumentParser(add_help=False)
        positional_arg_parsers['(int) arg'].add_argument('single_int_arg', default='0', help="(integer) arg",
                                                         type=int)

        subparsers = self.parser.add_subparsers()
        for cmd_name, cmd_info in Commands.methods.iteritems():
            parents = [arg_parsers[opt] for opt in cmd_info['allowed_opts'] if opt in arg_parsers]
            parents += [subcommand_help_parser]
            if 'req_args' in cmd_info and not cmd_info['req_args'] is None:
                parents += [positional_arg_parsers[arg] for arg in cmd_info['req_args']]
            command_parser = subparsers.add_parser(cmd_name, add_help=False, parents=parents)
            command_parser.set_defaults(command=cmd_name)

    def init(self):
        self.read_config()
        self._clean_args()
        self.setup_client()

    def _clean_args(self):
        for path in self.__get_all_directories():
            if path.startswith('hdfs://'):
                parse_result = urlparse(path)
                if path in self.args.dir:
                    self.args.dir.remove(path)
                    self.args.dir.append(parse_result.path)
                else:
                    self.args.single_arg = parse_result.path

    def __usetrash_unset(self):
        return not 'usetrash' in self.args or self.args.usetrash == False

    def __use_cl_port_first(self, alt):
        # Port provided from CL has the highest priority:
        return self.args.port if self.args.port else alt

    def read_config(self):

        # Try to retrieve namenode config from within CL arguments
        if self._read_config_cl():
            return

        ''' Try to read the config from ~/.snakebiterc and if that doesn't exist,
        check $HADOOP_HOME/core-site.xml and $HADOOP_HOME/hdfs-site.xml
        and create a ~/.snakebiterc from that.
        '''
        config_file = os.path.join(os.path.expanduser('~'), '.snakebiterc')

        if os.path.exists(config_file):
            #if ~/.snakebiterc exists - read config from it
            self._read_config_snakebiterc()
        else:
            # Try to read the configuration for HDFS configuration files
            configs = HDFSConfig.get_external_config()
            # if configs exist and contain something
            if configs:
                for config in configs:
                    nn = Namenode(config['namenode'],
                                  self.__use_cl_port_first(config['port']))
                    self.namenodes.append(nn)
                if self.__usetrash_unset():
                    self.args.usetrash = HDFSConfig.use_trash

        if len(self.namenodes):
            return
        else:
            print "No ~/.snakebiterc found, no HADOOP_HOME set and no -n and -p provided"
            print "Tried to find core-site.xml in:"
            for core_conf_path in HDFSConfig.core_try_paths:
                print " - %s" % core_conf_path
            print "Tried to find hdfs-site.xml in:"
            for hdfs_conf_path in HDFSConfig.hdfs_try_paths:
                print " - %s" % hdfs_conf_path
            print "\nYou can manually create ~/.snakebiterc with the following content:"
            print '{'
            print '  "config_version": 2,'
            print '  "use_trash": true,'
            print '  "namenodes": ['
            print '    {"host": "namenode-ha1", "port": %d, "version": %d},' % (Namenode.DEFAULT_PORT, Namenode.DEFAULT_VERSION)
            print '    {"host": "namenode-ha2", "port": %d, "version": %d}' % (Namenode.DEFAULT_PORT, Namenode.DEFAULT_VERSION)
            print '  ]'
            print '}'

            sys.exit(1)

    def _read_config_snakebiterc(self):
        old_version_info = "You're are using snakebite %s with Trash support together with old snakebiterc, please update/remove your ~/.snakebiterc file. By default Trash is %s." % (version(), 'disabled' if not HDFSConfig.use_trash else 'enabled')
        with open(os.path.join(os.path.expanduser('~'), '.snakebiterc')) as config_file:
            configs = json.load(config_file)

        if isinstance(configs, list):
            # Version 1: List of namenodes
            # config is a list of namenode(s) - possibly HA
            for config in configs:
                nn = Namenode(config['namenode'],
                              self.__use_cl_port_first(config.get('port', Namenode.DEFAULT_PORT)),
                              config.get('version', Namenode.DEFAULT_VERSION))
                self.namenodes.append(nn)
            if self.__usetrash_unset():
                # commandline setting has higher priority
                print_info(old_version_info)
                # There's no info about Trash in version 1, use default policy:
                self.args.usetrash = HDFSConfig.use_trash
        elif isinstance(configs, dict):
            # Version 2: {}
            # Can be either new configuration or just one namenode
            # which was the very first configuration syntax
            if 'config_version' in configs:
                # Config version => 2
                for nn_config in configs['namenodes']:
                    nn = Namenode(nn_config['host'],
                                  self.__use_cl_port_first(nn_config.get('port', Namenode.DEFAULT_PORT)),
                                  nn_config.get('version', Namenode.DEFAULT_VERSION))
                    self.namenodes.append(nn)

                if self.__usetrash_unset():
                    # commandline setting has higher priority
                    self.args.usetrash = configs.get("use_trash", HDFSConfig.use_trash)
            else:
                # config is a single namenode - no HA
                self.namenodes.append(Namenode(configs['namenode'],
                                               self.__use_cl_port_first(configs.get('port', Namenode.DEFAULT_PORT)),
                                               configs.get('version', Namenode.DEFAULT_VERSION)))
                if self.__usetrash_unset():
                    # commandline setting has higher priority
                    print_info(old_version_info)
                    self.args.usetrash = HDFSConfig.use_trash
        else:
            print_error_exit("Config retrieved from ~/.snakebiterc is corrupted! Remove it!")

    def __get_all_directories(self):
        if self.args and 'dir' in self.args:
            dirs_to_check = list(self.args.dir)
            if self.args.command == 'mv':
                dirs_to_check.append(self.args.single_arg)
            return dirs_to_check
        else:
            return ()

    def _read_config_cl(self):
        ''' Check if any directory arguments contain hdfs://'''
        dirs_to_check = self.__get_all_directories()
        hosts, ports = [], []
        for path in dirs_to_check:
            if path.startswith('hdfs://'):
                parse_result = urlparse(path)
                hosts.append(parse_result.hostname)
                ports.append(parse_result.port)

        # remove duplicates and None from (hosts + self.args.namenode)
        hosts = filter(lambda x: x != None, set(hosts + [self.args.namenode]))
        if len(hosts) > 1:
            print_error_exit('Conficiting namenode hosts in commandline arguments, hosts: %s' % str(hosts))

        ports = filter(lambda x: x != None, set(ports + [self.args.port]))
        if len(ports) > 1:
            print_error_exit('Conflicting namenode ports in commandline arguments, ports: %s' % str(ports))

        # Store port from CL in arguments - CL port has the highest priority
        if len(ports) == 1:
            self.args.port = ports[0]

        # do we agree on one namenode?
        if len(hosts) == 1 and len(ports) <= 1:
            self.args.namenode = hosts[0]
            self.args.port = ports[0] if len(ports) == 1 else Namenode.DEFAULT_PORT
            self.namenodes.append(Namenode(self.args.namenode, self.args.port))
            # we got the info from CL -> check if use_trash is set - if not use default policy:
            if self.__usetrash_unset():
                self.args.usetrash = HDFSConfig.use_trash
            return True
        else:
            return False

    def parse(self, non_cli_input=None):  # Allow input for testing purposes
        if not sys.argv[1:] and not non_cli_input:
            self.parser.print_help()
            sys.exit(-1)

        try:
            args = self.parser.parse_args(non_cli_input)
        except ArgumentParserError, error:
            if "-h" in sys.argv or "--help" in sys.argv:  # non cli input?
                commands = [cmd for (cmd, description) in Commands.methods.iteritems() if description['visible'] is True]
                command = error.prog.split()[-1]
                if command in commands:
                    self.usage_helper(command)
                else:
                    self.parser.print_help()
                self.parser.exit(2)
            elif "-v" in sys.argv or "--ver" in sys.argv:
                print version()
                self.parser.exit(0)
            else:
                self.parser.print_usage(sys.stderr)
                self.parser.exit(2, 'error: %s. Use -h for help.\n' % (error.error_message))

        self.cmd = args.command
        self.args = args
        return self.args

    def setup_client(self):
        if 'skiptrash' in self.args:
            use_trash = self.args.usetrash and not self.args.skiptrash
        else:
            use_trash = self.args.usetrash
        self.client = HAClient(self.namenodes, use_trash)

    def execute(self):
        if self.args.help:
            #if 'ls -H' is called, execute 'usage ls'
            self.args.arg = [self.cmd]
            return Commands.methods['usage']['method'](self)
        if not Commands.methods.get(self.cmd):
            self.parser.print_help()
            sys.exit(-1)
        try:
            return Commands.methods[self.cmd]['method'](self)
        except Exception, e:
            exitError(e)

    def command(args="", descr="", allowed_opts="", visible=True, req_args=None):
        def wrap(f):
            Commands.methods[f.func_name] = {"method": f,
                                             "args": args,
                                             "descr": descr,
                                             "allowed_opts": allowed_opts,
                                             "visible": visible,
                                             "req_args": req_args}
        return wrap

    @command(visible=False)
    def commands(self):
        print "\n".join(sorted([k for k, v in Commands.methods.iteritems() if v['visible']]))

    @command(args="[path]", descr="Used for command line completion", visible=False, req_args=['[dirs]'])
    def complete(self):
        self.args.summary = True
        self.args.directory = False
        self.args.recurse = False
        self.args.human = False
        try:
            for line in self._listing():
                print line.replace(" ", "\\\\ ")
        except FileNotFoundException:
            pass

    @command(args="[paths]", descr="list a path", allowed_opts=["d", "R", "s", "h"], req_args=['[dirs]'])
    def ls(self):
        for line in self._listing():
            print line

    def _listing(self):
        # Mimicking hadoop client behaviour
        if self.args.directory:
            include_children = False
            recurse = False
            include_toplevel = True
        else:
            include_children = True
            include_toplevel = False
            recurse = self.args.recurse

        listing = self.client.ls(self.args.dir, recurse=recurse,
                                 include_toplevel=include_toplevel,
                                 include_children=include_children)

        for line in format_listing(listing, json_output=self.args.json,
                                   human_readable=self.args.human,
                                   recursive=recurse,
                                   summary=self.args.summary):
            yield line

    @command(args="[paths]", descr="create directories", req_args=['dir [dirs]'])
    def mkdir(self):
        creations = self.client.mkdir(self.args.dir)
        for line in format_results(creations, json_output=self.args.json):
            print line

    @command(args="[paths]", descr="create directories and their parents", req_args=['dir [dirs]'])
    def mkdirp(self):
        creations = self.client.mkdir(self.args.dir, create_parent=True)
        for line in format_results(creations, json_output=self.args.json):
            print line

    @command(args="<owner:grp> [paths]", descr="change owner", allowed_opts=["R"], req_args=['arg', 'dir [dirs]'])
    def chown(self):
        owner = self.args.single_arg
        try:
            mods = self.client.chown(self.args.dir, owner, recurse=self.args.recurse)
            for line in format_results(mods, json_output=self.args.json):
                print line
        except FileNotFoundException, e:
            exitError(e)

    @command(args="<mode> [paths]", descr="change file mode (octal)", allowed_opts=["R"], req_args=['(int) arg', 'dir [dirs]'])
    def chmod(self):
        mode = int(str(self.args.single_int_arg), 8)
        mods = self.client.chmod(self.args.dir, mode, recurse=self.args.recurse)
        for line in format_results(mods, json_output=self.args.json):
            print line

    @command(args="<grp> [paths]", descr="change group", allowed_opts=["R"], req_args=['arg', 'dir [dirs]'])
    def chgrp(self):
        grp = self.args.single_arg
        mods = self.client.chgrp(self.args.dir, grp, recurse=self.args.recurse)
        for line in format_results(mods, json_output=self.args.json):
            print line

    @command(args="[paths]", descr="display stats for paths", allowed_opts=['h'], req_args=['[dirs]'])
    def count(self):
        counts = self.client.count(self.args.dir)
        for line in format_counts(counts, json_output=self.args.json,
                                  human_readable=self.args.human):
            print line

    @command(args="", descr="display fs stats", allowed_opts=['h'])
    def df(self):
        result = self.client.df()
        for line in format_fs_stats(result, json_output=self.args.json,
                                    human_readable=self.args.human):
            print line

    @command(args="[paths]", descr="display disk usage statistics", allowed_opts=["s", "h"], req_args=['[dirs]'])
    def du(self):
        if self.args.summary:
            include_children = False
            include_toplevel = True
        else:
            include_children = True
            include_toplevel = False
        result = self.client.du(self.args.dir, include_toplevel=include_toplevel, include_children=include_children)
        for line in format_du(result, json_output=self.args.json, human_readable=self.args.human):
            print line

    @command(args="[paths] dst", descr="move paths to destination", req_args=['dir [dirs]', 'arg'])
    def mv(self):
        paths = self.args.dir
        dst = self.args.single_arg
        result = self.client.rename(paths, dst)
        for line in format_results(result, json_output=self.args.json):
            print line

    @command(args="[paths]", descr="remove paths", allowed_opts=["R", "S", "T"], req_args=['dir [dirs]'])
    def rm(self):
        result = self.client.delete(self.args.dir, recurse=self.args.recurse)
        for line in format_results(result, json_output=self.args.json):
            print line

    @command(args="[paths]", descr="creates a file of zero length", req_args=['dir [dirs]'])
    def touchz(self):
        result = self.client.touchz(self.args.dir)
        for line in format_results(result, json_output=self.args.json):
            print line

    @command(args="", descr="show server information")
    def serverdefaults(self):
        print self.client.serverdefaults()

    @command(args="[dirs]", descr="delete a directory", req_args=['dir [dirs]'])
    def rmdir(self):
        result = self.client.rmdir(self.args.dir)
        for line in format_results(result, json_output=self.args.json):
            print line

    @command(args="<rep> [paths]", descr="set replication factor", allowed_opts=['R'], req_args=['(int) arg', 'dir [dirs]'])
    def setrep(self):
        rep_factor = int(self.args.single_int_arg)
        result = self.client.setrep(self.args.dir, rep_factor, recurse=self.args.recurse)
        for line in format_results(result, json_output=self.args.json):
            print line

    @command(args="<cmd>", descr="show cmd usage", req_args=['[args]'])
    def usage(self):
        if not 'arg' in self.args or self.args.arg == []:
            self.parser.print_help()
            sys.exit(-1)

        for sub_cmd in self.args.arg:
            self.usage_helper(sub_cmd)

    def usage_helper(self, command):
        cmd_entry = Commands.methods.get(command)
        if not cmd_entry:
            self.parser.print_help()
            sys.exit(-1)
        cmd_args = []
        cmd_descriptions = "\ncommand options: \n"
        allowed_opts = cmd_entry.get('allowed_opts')
        if allowed_opts:
            cmd_args += ["[-%s]" % o for o in allowed_opts]
            cmd_descriptions += "\n".join(sorted([" %-30s %s" % ("%s %s" % (self.SUB_OPTS[o]['short'], self.SUB_OPTS[o]['long']), self.SUB_OPTS[o]['help']) for o in allowed_opts]))
        args = cmd_entry.get('args')
        if args:
            cmd_args.append(args)

        print "usage: snakebite [general options] %s %s" % (command, " ".join(cmd_args))

        general_opts = "\ngeneral options:\n"
        general_opts += "\n".join(sorted(["  %-30s %s" % ("%s %s" % (v['short'], v['long']), v['help']) for k, v in self.GENERIC_OPTS.iteritems()]))
        print general_opts

        if allowed_opts:
            print cmd_descriptions

    @command(args="[paths]", descr="stat information", req_args=['dir [dirs]'])
    def stat(self):
        print format_stat(self.client.stat(self.args.dir), json_output=self.args.json)

    @command(args="path", descr="test a path", allowed_opts=['d', 'z', 'e'], req_args=['arg'])
    def test(self):
        path = self.args.single_arg
        if self.client.test(path, exists=self.args.exists, directory=self.args.directory, zero_length=self.args.zero):
            sys.exit(0)
        else:
            sys.exit(1)

    @command(args="[paths]", descr="copy source paths to stdout", allowed_opts=['checkcrc'], req_args=['dir [dirs]'])
    def cat(self):
        for file_to_read in self.client.cat(self.args.dir, check_crc=self.args.checkcrc):
            for load in file_to_read:
                sys.stdout.write(load)

    @command(args="path dst", descr="copy local file reference to destination", req_args=['dir [dirs]', 'arg'], visible=False)
    def copyFromLocal(self):
        src = self.args.dir
        dst = self.args.single_arg
        result = self.client.copyFromLocal(src, dst)
        for line in format_results(result, json_output=self.args.json):
            print line

    @command(args="[paths] dst", descr="copy paths to local file system destination", allowed_opts=['checkcrc'], req_args=['dir [dirs]', 'arg'])
    def copyToLocal(self):
        paths = self.args.dir
        dst = self.args.single_arg
        result = self.client.copyToLocal(paths, dst, check_crc=self.args.checkcrc)
        for line in format_results(result, json_output=self.args.json):
            print line

    @command(args="[paths] dst", descr="copy files from source to destination", allowed_opts=['checkcrc'], req_args=['dir [dirs]', 'arg'], visible=False)
    def cp(self):
        paths = self.args.dir
        dst = self.args.single_arg
        result = self.client.cp(paths, dst, checkcrc=self.args.checkcrc)
        for line in format_results(result, json_output=self.args.json):
            print line

    @command(args="file dst", descr="copy files to local file system destination", allowed_opts=['checkcrc'], req_args=['dir [dirs]', 'arg'])
    def get(self):
        paths = self.args.dir
        dst = self.args.single_arg
        result = self.client.copyToLocal(paths, dst, check_crc=self.args.checkcrc)
        for line in format_results(result, json_output=self.args.json):
            print line

    @command(args="dir dst", descr="concatenates files in source dir into destination local file", allowed_opts=['nl'], req_args=['src dst'])
    def getmerge(self):
        source = self.args.src_dst[0]
        dst = self.args.src_dst[1]
        result = self.client.getmerge(source, dst, newline=self.args.newline)
        for line in format_results(result, json_output=self.args.json):
            print line

    # @command(args="[paths] dst", descr="copy sources from local file system to destination", req_args=['dir [dirs]', 'arg'])
    # def put(self):
    #     paths = self.args.dir
    #     dst = self.args.single_arg
    #     result = self.client.put(paths, dst)
    #     for line in format_results(result, json_output=self.args.json):
    #         print line

    @command(args="path", descr="display last kilobyte of the file to stdout", allowed_opts=['f'], req_args=['arg'])
    def tail(self):
        path = self.args.single_arg
        result = self.client.tail(path, append=self.args.append)
        for line in result:
            print line

    @command(args="path [paths]", descr="output file in text format", allowed_opts=['checkcrc'], req_args=['dir [dirs]'])
    def text(self):
        paths = self.args.dir
        result = self.client.text(paths)
        for line in result:
            print line

########NEW FILE########
__FILENAME__ = config
import os
import sys
import logging
import xml.etree.ElementTree as ET

from urlparse import urlparse
from namenode import Namenode

log = logging.getLogger(__name__)

class HDFSConfig(object):
    use_trash = False

    @classmethod
    def get_config_from_env(cls):
        '''Gets configuration out of environment.

        Returns list of dicts - list of namenode representations
        '''
        core_path = os.path.join(os.environ['HADOOP_HOME'], 'conf', 'core-site.xml')
        configs = cls.read_core_config(core_path)

        hdfs_path = os.path.join(os.environ['HADOOP_HOME'], 'conf', 'hdfs-site.xml')
        tmp_config = cls.read_hdfs_config(hdfs_path)

        if tmp_config:
            # if config exists in hdfs - it's HA config, update configs
            configs = tmp_config

        if not configs:
            raise Exception("No config found in %s nor in %s" % (core_path, hdfs_path))

        return configs


    @staticmethod
    def read_hadoop_config(hdfs_conf_path):
        if os.path.exists(hdfs_conf_path):
            tree = ET.parse(hdfs_conf_path)
            root = tree.getroot()
            for p in root.findall("./property"):
                yield p


    @classmethod
    def read_core_config(cls, core_site_path):
        config = []
        for property in cls.read_hadoop_config(core_site_path):
            if property.findall('name')[0].text == 'fs.defaultFS':
                parse_result = urlparse(property.findall('value')[0].text)
                log.debug("Got namenode '%s' from %s" % (parse_result.geturl(), core_site_path))

                config.append({"namenode": parse_result.hostname,
                               "port": parse_result.port if parse_result.port
                                                         else Namenode.DEFAULT_PORT})

            if property.findall('name')[0].text == 'fs.trash.interval':
                cls.use_trash = True

        return config

    @classmethod
    def read_hdfs_config(cls, hdfs_site_path):
        configs = []
        for property in cls.read_hadoop_config(hdfs_site_path):
            if property.findall('name')[0].text.startswith("dfs.namenode.rpc-address"):
                parse_result = urlparse("//" + property.findall('value')[0].text)
                log.debug("Got namenode '%s' from %s" % (parse_result.geturl(), hdfs_site_path))
                configs.append({"namenode": parse_result.hostname,
                                "port": parse_result.port if parse_result.port
                                                          else Namenode.DEFAULT_PORT})

            if property.findall('name')[0].text == 'fs.trash.interval':
                cls.use_trash = True

        return configs

    core_try_paths = ('/etc/hadoop/conf/core-site.xml',
                      '/usr/local/etc/hadoop/conf/core-site.xml',
                      '/usr/local/hadoop/conf/core-site.xml')

    hdfs_try_paths = ('/etc/hadoop/conf/hdfs-site.xml',
                      '/usr/local/etc/hadoop/conf/hdfs-site.xml',
                      '/usr/local/hadoop/conf/hdfs-site.xml')

    @classmethod
    def get_external_config(cls):
        if os.environ.get('HADOOP_HOME'):
            configs = cls.get_config_from_env()
            return configs
        else:
            # Try to find other paths
            configs = []
            for core_conf_path in cls.core_try_paths:
                configs = cls.read_core_config(core_conf_path)
                if configs:
                    break

            for hdfs_conf_path in cls.hdfs_try_paths:
                tmp_config = cls.read_hdfs_config(hdfs_conf_path)
                if tmp_config:
                    # if there is hdfs-site data available return it
                    return tmp_config
            return configs

########NEW FILE########
__FILENAME__ = crc32c
#
# Copyright 2007 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#


"""Implementation of CRC-32C checksumming as in rfc3720 section B.4.

See http://en.wikipedia.org/wiki/Cyclic_redundancy_check for details on CRC-32C.

This code is a manual python translation of c code generated by
pycrc 0.7.1 (http://www.tty1.net/pycrc/). Command line used:
'./pycrc.py --model=crc-32c --generate c --algorithm=table-driven'
"""


import array

CRC_TABLE = (
    0x00000000L, 0xf26b8303L, 0xe13b70f7L, 0x1350f3f4L,
    0xc79a971fL, 0x35f1141cL, 0x26a1e7e8L, 0xd4ca64ebL,
    0x8ad958cfL, 0x78b2dbccL, 0x6be22838L, 0x9989ab3bL,
    0x4d43cfd0L, 0xbf284cd3L, 0xac78bf27L, 0x5e133c24L,
    0x105ec76fL, 0xe235446cL, 0xf165b798L, 0x030e349bL,
    0xd7c45070L, 0x25afd373L, 0x36ff2087L, 0xc494a384L,
    0x9a879fa0L, 0x68ec1ca3L, 0x7bbcef57L, 0x89d76c54L,
    0x5d1d08bfL, 0xaf768bbcL, 0xbc267848L, 0x4e4dfb4bL,
    0x20bd8edeL, 0xd2d60dddL, 0xc186fe29L, 0x33ed7d2aL,
    0xe72719c1L, 0x154c9ac2L, 0x061c6936L, 0xf477ea35L,
    0xaa64d611L, 0x580f5512L, 0x4b5fa6e6L, 0xb93425e5L,
    0x6dfe410eL, 0x9f95c20dL, 0x8cc531f9L, 0x7eaeb2faL,
    0x30e349b1L, 0xc288cab2L, 0xd1d83946L, 0x23b3ba45L,
    0xf779deaeL, 0x05125dadL, 0x1642ae59L, 0xe4292d5aL,
    0xba3a117eL, 0x4851927dL, 0x5b016189L, 0xa96ae28aL,
    0x7da08661L, 0x8fcb0562L, 0x9c9bf696L, 0x6ef07595L,
    0x417b1dbcL, 0xb3109ebfL, 0xa0406d4bL, 0x522bee48L,
    0x86e18aa3L, 0x748a09a0L, 0x67dafa54L, 0x95b17957L,
    0xcba24573L, 0x39c9c670L, 0x2a993584L, 0xd8f2b687L,
    0x0c38d26cL, 0xfe53516fL, 0xed03a29bL, 0x1f682198L,
    0x5125dad3L, 0xa34e59d0L, 0xb01eaa24L, 0x42752927L,
    0x96bf4dccL, 0x64d4cecfL, 0x77843d3bL, 0x85efbe38L,
    0xdbfc821cL, 0x2997011fL, 0x3ac7f2ebL, 0xc8ac71e8L,
    0x1c661503L, 0xee0d9600L, 0xfd5d65f4L, 0x0f36e6f7L,
    0x61c69362L, 0x93ad1061L, 0x80fde395L, 0x72966096L,
    0xa65c047dL, 0x5437877eL, 0x4767748aL, 0xb50cf789L,
    0xeb1fcbadL, 0x197448aeL, 0x0a24bb5aL, 0xf84f3859L,
    0x2c855cb2L, 0xdeeedfb1L, 0xcdbe2c45L, 0x3fd5af46L,
    0x7198540dL, 0x83f3d70eL, 0x90a324faL, 0x62c8a7f9L,
    0xb602c312L, 0x44694011L, 0x5739b3e5L, 0xa55230e6L,
    0xfb410cc2L, 0x092a8fc1L, 0x1a7a7c35L, 0xe811ff36L,
    0x3cdb9bddL, 0xceb018deL, 0xdde0eb2aL, 0x2f8b6829L,
    0x82f63b78L, 0x709db87bL, 0x63cd4b8fL, 0x91a6c88cL,
    0x456cac67L, 0xb7072f64L, 0xa457dc90L, 0x563c5f93L,
    0x082f63b7L, 0xfa44e0b4L, 0xe9141340L, 0x1b7f9043L,
    0xcfb5f4a8L, 0x3dde77abL, 0x2e8e845fL, 0xdce5075cL,
    0x92a8fc17L, 0x60c37f14L, 0x73938ce0L, 0x81f80fe3L,
    0x55326b08L, 0xa759e80bL, 0xb4091bffL, 0x466298fcL,
    0x1871a4d8L, 0xea1a27dbL, 0xf94ad42fL, 0x0b21572cL,
    0xdfeb33c7L, 0x2d80b0c4L, 0x3ed04330L, 0xccbbc033L,
    0xa24bb5a6L, 0x502036a5L, 0x4370c551L, 0xb11b4652L,
    0x65d122b9L, 0x97baa1baL, 0x84ea524eL, 0x7681d14dL,
    0x2892ed69L, 0xdaf96e6aL, 0xc9a99d9eL, 0x3bc21e9dL,
    0xef087a76L, 0x1d63f975L, 0x0e330a81L, 0xfc588982L,
    0xb21572c9L, 0x407ef1caL, 0x532e023eL, 0xa145813dL,
    0x758fe5d6L, 0x87e466d5L, 0x94b49521L, 0x66df1622L,
    0x38cc2a06L, 0xcaa7a905L, 0xd9f75af1L, 0x2b9cd9f2L,
    0xff56bd19L, 0x0d3d3e1aL, 0x1e6dcdeeL, 0xec064eedL,
    0xc38d26c4L, 0x31e6a5c7L, 0x22b65633L, 0xd0ddd530L,
    0x0417b1dbL, 0xf67c32d8L, 0xe52cc12cL, 0x1747422fL,
    0x49547e0bL, 0xbb3ffd08L, 0xa86f0efcL, 0x5a048dffL,
    0x8ecee914L, 0x7ca56a17L, 0x6ff599e3L, 0x9d9e1ae0L,
    0xd3d3e1abL, 0x21b862a8L, 0x32e8915cL, 0xc083125fL,
    0x144976b4L, 0xe622f5b7L, 0xf5720643L, 0x07198540L,
    0x590ab964L, 0xab613a67L, 0xb831c993L, 0x4a5a4a90L,
    0x9e902e7bL, 0x6cfbad78L, 0x7fab5e8cL, 0x8dc0dd8fL,
    0xe330a81aL, 0x115b2b19L, 0x020bd8edL, 0xf0605beeL,
    0x24aa3f05L, 0xd6c1bc06L, 0xc5914ff2L, 0x37faccf1L,
    0x69e9f0d5L, 0x9b8273d6L, 0x88d28022L, 0x7ab90321L,
    0xae7367caL, 0x5c18e4c9L, 0x4f48173dL, 0xbd23943eL,
    0xf36e6f75L, 0x0105ec76L, 0x12551f82L, 0xe03e9c81L,
    0x34f4f86aL, 0xc69f7b69L, 0xd5cf889dL, 0x27a40b9eL,
    0x79b737baL, 0x8bdcb4b9L, 0x988c474dL, 0x6ae7c44eL,
    0xbe2da0a5L, 0x4c4623a6L, 0x5f16d052L, 0xad7d5351L,
)


CRC_INIT = 0

_MASK = 0xFFFFFFFFL


def crc_update(crc, data):
    """Update CRC-32C checksum with data.

    Args:
      crc: 32-bit checksum to update as long.
      data: byte array, string or iterable over bytes.

    Returns:
      32-bit updated CRC-32C as long.
    """

    if type(data) != array.array or data.itemsize != 1:
        buf = array.array("B", data)
    else:
        buf = data

    crc = crc ^ _MASK
    for b in buf:
        table_index = (crc ^ b) & 0xff
        crc = (CRC_TABLE[table_index] ^ (crc >> 8)) & _MASK
    return crc ^ _MASK


def crc_finalize(crc):
    """Finalize CRC-32C checksum.

    This function should be called as last step of crc calculation.

    Args:
      crc: 32-bit checksum as long.

    Returns:
      finalized 32-bit checksum as long
    """
    return crc & _MASK


def crc(data):
    """Compute CRC-32C checksum of the data.

    Args:
      data: byte array, string or iterable over bytes.

    Returns:
      32-bit CRC-32C checksum of data as long.
    """
    return crc_finalize(crc_update(CRC_INIT, data))

########NEW FILE########
__FILENAME__ = errors
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.


class FileNotFoundException(Exception):
    def __init__(self, msg):
        super(FileNotFoundException, self).__init__(msg)


class FileExistsException(Exception):
    def __init__(self, msg):
        super(FileExistsException, self).__init__(msg)


class RequestError(Exception):
    def __init__(self, msg):
        super(RequestError, self).__init__(msg)


class DirectoryException(Exception):
    def __init__(self, msg):
        super(DirectoryException, self).__init__(msg)


class FileException(Exception):
    def __init__(self, msg):
        super(FileException, self).__init__(msg)


class InvalidInputException(Exception):
    def __init__(self, msg):
        super(InvalidInputException, self).__init__(msg)


class OutOfNNException(Exception):
    def __init__(self, msg):
        super(OutOfNNException, self).__init__(msg)

########NEW FILE########
__FILENAME__ = formatter
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
import datetime
import stat
import json
import binascii
import os.path


def _octal_to_perm(octal):
    perms = list("-" * 9)
    if octal & stat.S_IRUSR:
        perms[0] = "r"
    if octal & stat.S_IWUSR:
        perms[1] = "w"
    if octal & stat.S_IXUSR:
        perms[2] = "x"
    if octal & stat.S_IRGRP:
        perms[3] = "r"
    if octal & stat.S_IWGRP:
        perms[4] = "w"
    if octal & stat.S_IXGRP:
        perms[5] = "x"
    if octal & stat.S_IROTH:
        perms[6] = "r"
    if octal & stat.S_IWOTH:
        perms[7] = "w"
    if octal & stat.S_IXOTH:
        perms[8] = "x"
    return "".join(perms)


def _sizeof_fmt(num):
    for x in ['', 'k', 'M', 'G', 'T', 'P']:
        if num < 1024.0:
            if x == '':
                return num
            else:
                return "%3.1f%s" % (num, x)
        num /= 1024.0


def format_column(col, node, human_readable):
    value = node.get(col)

    if col == "file_type":
        if value == "f":
            return "-"
        else:
            return value
    elif col == "permission":
        return _octal_to_perm(value)
    elif col == "modification_time":
        timestamp = datetime.datetime.fromtimestamp(value / 1000)
        return timestamp.strftime('%Y-%m-%d %H:%M')
    elif col == "block_replication":
        if node["file_type"] == "f":
            return value
        else:
            return "-"
    elif col == "length":
        if human_readable:
            return _sizeof_fmt(int(value))
        else:
            return value
    else:
        return value


def format_listing(listing, json_output=False, human_readable=False, recursive=False, summary=False):
    if json_output:
        for node in listing:
            yield json.dumps(node)
    else:
        nodes = []
        last_dir = None
        try:
            while True:
                node = listing.next()
                dir_name = os.path.dirname(node['path'])
                if dir_name != last_dir:
                    if last_dir:
                        yield _create_dir_listing(nodes, human_readable, recursive, summary)
                    last_dir = dir_name
                    nodes = []
                nodes.append(node)

        except StopIteration:
            yield _create_dir_listing(nodes, human_readable, recursive, summary)


def _create_dir_listing(nodes, human_readable, recursive, summary):
    ret = []
    if not recursive and not summary:
        ret.append("Found %d items" % len(nodes))

    if summary:
        for node in nodes:
            path = node['path']
            if node['file_type'] == "d":
                path += "/"
            ret.append(path)
    else:
        columns = ['file_type', 'permission', 'block_replication', 'owner', 'group', 'length', 'modification_time', 'path']

        max_len = max([len(str(node.get('length'))) for node in nodes] + [10])
        max_owner = max([len(str(node.get('owner'))) for node in nodes] + [10])
        max_group = max([len(str(node.get('group'))) for node in nodes] + [10])
        templ = "%%s%%s %%3s %%-%ds %%-%ds %%%ds %%s %%s" % (max_owner, max_group, max_len)
        for node in nodes:
            cols = [str(format_column(col, node, human_readable)) for col in columns]
            ret.append(templ % tuple(cols))

    return "\n".join(ret)


def format_results(results, json_output=False):
    if json_output:
        for result in results:
            yield json.dumps(result)
    else:
        for r in results:
            if r['result']:
                if r.get('message'):
                    yield "OK: %s %s" % (r.get('path'), r.get('message'))
                else:
                    yield "OK: %s" % r.get('path')
            else:
                yield "ERROR: %s (reason: %s)" % (r.get('path'), r.get('error', ''))


def format_counts(results, json_output=False, human_readable=False):
    if json_output:
        for result in results:
            yield json.dumps(result)
    else:
        for result in results:
            space_consumed = result.get('spaceConsumed')
            if human_readable:
                space_consumed = _sizeof_fmt(int(result.get('spaceConsumed')))

            yield "%12s %12s %18s %s" % (result.get('directoryCount'),
                                                result.get('fileCount'),
                                                space_consumed,
                                                result.get('path'))


def format_fs_stats(result, json_output=False, human_readable=False):
    if json_output:
        yield json.dumps(result)
    else:
        fs = result['filesystem']
        size = result['capacity']
        used = result['used']
        avail = result['remaining']
        if size == 0:
            pct_used = "0.00"
        else:
            pct_used = (float(used) / float(size)) * 100.0
            pct_used = "%.2f" % pct_used

        if human_readable:
            size = _sizeof_fmt(int(size))
            used = _sizeof_fmt(int(used))
            avail = _sizeof_fmt(int(avail))

        tmpl = "%%-%ds  %%%ds  %%%ds  %%%ds  %%%ds%%%%" % (max(len(str(fs)), len('Filesystem')),
                                                           max(len(str(size)), len('Size')),
                                                           max(len(str(used)), len('Used')),
                                                           max(len(str(avail)), len('Available')),
                                                           max(len(str(pct_used)), len('Use%')))

        header = tmpl % ('Filesystem', 'Size', 'Used', 'Available', 'Use')
        data = tmpl % (fs, size, used, avail, pct_used)
        yield "%s\n%s" % (header, data)


def format_du(listing, json_output=False, human_readable=False):
    if json_output:
        for result in listing:
            yield json.dumps(result)
    else:
        # for result in listing:
        #     if human_readable:
        #         result['lenght'] =  _sizeof_fmt(result['length'])
        #     yield "%s %s" % (result['length'], result['path'])

        nodes = []
        last_dir = None
        try:
            while True:
                node = listing.next()
                dir_name = os.path.dirname(node['path'])
                if dir_name != last_dir:
                    if last_dir:
                        yield _create_count_listing(nodes, human_readable)
                    last_dir = dir_name
                    nodes = []
                nodes.append(node)
        except StopIteration:
            yield _create_count_listing(nodes, human_readable)


def _create_count_listing(nodes, human_readable):
    ret = []
    if human_readable:
        for node in nodes:
            node['length'] = _sizeof_fmt(node['length'])
    max_len = max([len(str(r['length'])) for r in nodes])
    templ = "%%-%ds  %%s" % max_len
    for node in nodes:
        ret.append(templ % (node['length'], node['path']))
    return "\n".join(ret)


def format_stat(results, json_output=False):
    ret = []
    if json_output:
        return json.dumps(results)
    for k, v in sorted(results.iteritems()):
        if k == "permission":
            v = oct(v)
        ret.append("%-20s\t%s" % (k, v))
    return "\n".join(ret)


def format_bytes(bytes):
    ascii = binascii.b2a_hex(bytes)
    byte_array = [ascii[i:i + 2] for i in range(0, len(ascii), 2)]
    return  "%s (len: %d)"% (" ".join(byte_array), len(byte_array))

########NEW FILE########
__FILENAME__ = glob
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.

import re
import itertools


def expand_paths(paths):
    ''' Expand paths like /foo/{bar,baz} becomes /foo/bar, /foo/bar'''
    result = []
    exp = re.compile("{(.*?)}")
    for path in paths:
        m = exp.findall(path)
        if not m:
            result.append(path)
        else:
            x = [item.split(",") for item in m]
            template = re.sub("{.*?}", "%s", path)
            product = list(itertools.product(*x))
            for s in product:
                result.append(template % s)
    return result

magick_check = re.compile('[*?[{}]')


def has_magic(s):
    return magick_check.search(s) is not None

########NEW FILE########
__FILENAME__ = logger
# -*- coding: utf-8 -*-
# Copyright (c) 2009 Las Cumbres Observatory (www.lcogt.net)
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.

'''protobuf/logger.py - Module for configuring the package level logging.

This module contains a convenience function for creating and retrieving a
logger with a given name. In addition a Null handler is added to the logger
to prevent client software not implementing the logging package from
receiving "No handler" error messages.

Authors: Martin Norbury (mnorbury@lcogt.net)
         Eric Saunders (esaunders@lcogt.net)

May 2009
'''

# Standard library imports
import logging


class _NullHandler(logging.Handler):
    ''' NULL logging handler.

    A null logging handler to prevent clients that don't require the
    logging package from reporting no handlers found.
    '''

    def emit(self, record):
        ''' Override the emit function to do nothing. '''
        pass


def getLogger(name):
    ''' Create and return a logger with the specified name. '''

    # Create logger and add a default NULL handler
    log = logging.getLogger(name)
    log.addHandler(_NullHandler())

    return log

########NEW FILE########
__FILENAME__ = minicluster
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
import os
import subprocess
import select
import re
import datetime


class MiniCluster(object):
    ''' Class that spawns a hadoop mini cluster and wrap hadoop functionality

    This class requires the ``HADOOP_HOME`` environment variable to be set to run the ``hadoop`` command.
    It will search ``HADOOP_HOME`` for ``hadoop-mapreduce-client-jobclient<version>-tests.jar``, but the
    location of this jar can also be supplied by the ``HADOOP_JOBCLIENT_JAR`` environment variable.

    Since the current minicluster interface doesn't provide for specifying the namenode post number, and
    chooses a random one, this class parses the output from the minicluster to find the port numer.

    All supplied methods (like :py:func:`put`, :py:func:`ls`, etc) use the hadoop command to perform operations, and not
    the snakebite client, since this is used for testing snakebite itself.

    All methods return a list of maps that are snakebite compatible.

    Example without :mod:`snakebite.client <client>`

    >>> from snakebite.minicluster import MiniCluster
    >>> cluster = MiniCluster("/path/to/test/files")
    >>> ls_output = cluster.ls(["/"])

    Example with :mod:`snakebite.client <client>`

    >>> from snakebite.minicluster import MiniCluster
    >>> from snakebite.client import Client
    >>> cluster = MiniCluster("/path/to/test/files")
    >>> client = Client('localhost', cluster.port)
    >>> ls_output = client.ls(["/"])

    Just as the snakebite client, the cluster methods take a list of strings as paths. Wherever a method
    takes ``extra_args``, normal hadoop command arguments can be given (like -r, -f, etc).

    More info can be found at http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/CLIMiniCluster.html

    .. note:: A minicluster will be started at instantiation
    .. note:: Not all hadoop commands have been implemented, only the ones that
              were necessary for testing the snakebite client, but please feel free to add them
    '''
    def __init__(self, testfiles_path, start_cluster=True):
        '''
        :param testfiles_path: Local path where test files can be found. Mainly used for ``put()``
        :type testfiles_path: string
        :param start_cluster: start a MiniCluster on initialization. If False, this class will act as an interface to the ``hadoop fs`` command
        :type start_cluster: boolean
        '''
        self._testfiles_path = testfiles_path
        self._hadoop_home = os.environ['HADOOP_HOME']
        self._jobclient_jar = os.environ.get('HADOOP_JOBCLIENT_JAR')
        self._hadoop_cmd = "%s/bin/hadoop" % self._hadoop_home
        if start_cluster:
            self._start_mini_cluster()
            self.host = "localhost"
            self.port = self._get_namenode_port()
            self.hdfs_url = "hdfs://%s:%d" % (self.host, self.port)
        else:
            self.hdfs_url = "hdfs://"

    def terminate(self):
        ''' Terminate the cluster

        Since the minicluster is started as a subprocess, this method has to be called explicitely when
        your program ends.
        '''
        self.hdfs.terminate()

    def put(self, src, dst):
        '''Upload a file to HDFS

        This will take a file from the ``testfiles_path`` supplied in the constuctor.
        '''
        src = "%s%s" % (self._testfiles_path, src)
        return self._runCmd([self._hadoop_cmd, 'fs', '-put', src, self._full_hdfs_path(dst)])

    def put_subprocess(self, src, dst):  # This is used for testing with large files.
        cmd = [self._hadoop_cmd, 'fs', '-put', src, self._full_hdfs_path(dst)]
        return subprocess.Popen(cmd, bufsize=0, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)

    def ls(self, src, extra_args=[]):
        '''List files in a directory'''
        src = [self._full_hdfs_path(x) for x in src]
        output = self._runCmd([self._hadoop_cmd, 'fs', '-ls'] + extra_args + src)
        return self._transform_ls_output(output, self.hdfs_url)

    def mkdir(self, src, extra_args=[]):
        '''Create a directory'''
        return self._runCmd([self._hadoop_cmd, 'fs', '-mkdir'] + extra_args + [self._full_hdfs_path(src)])

    def df(self, src):
        '''Perform ``df`` on a path'''
        return self._runCmd([self._hadoop_cmd, 'fs', '-df', self._full_hdfs_path(src)])

    def du(self, src, extra_args=[]):
        '''Perform ``du`` on a path'''
        src = [self._full_hdfs_path(x) for x in src]
        return self._transform_du_output(self._runCmd([self._hadoop_cmd, 'fs', '-du'] + extra_args + src), self.hdfs_url)

    def count(self, src):
        '''Perform ``count`` on a path'''
        src = [self._full_hdfs_path(x) for x in src]
        return self._transform_count_output(self._runCmd([self._hadoop_cmd, 'fs', '-count'] + src), self.hdfs_url)

    def cat(self, src, extra_args=[]):
        return self._runCmd([self._hadoop_cmd, 'fs', '-cat'] + extra_args + [self._full_hdfs_path(src)])

    def copyToLocal(self, src, dst, extra_args=[]):
        return self._runCmd([self._hadoop_cmd, 'fs', '-copyToLocal'] + extra_args + [self._full_hdfs_path(src), dst])

    def getmerge(self, src, dst, extra_args=[]):
        return self._runCmd([self._hadoop_cmd, 'fs', '-getmerge'] + extra_args + [self._full_hdfs_path(src), dst])

    def tail(self, src, extra_args=[]):
        return self._runCmd([self._hadoop_cmd, 'fs', '-tail'] + extra_args + [self._full_hdfs_path(src)])

    def text(self, src):
        return self._runCmd([self._hadoop_cmd, 'fs', '-text', self._full_hdfs_path(src)])

    def _runCmd(self, cmd):
        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return p.communicate()[0]

    def _full_hdfs_path(self, src):
        return "%s%s" % (self.hdfs_url, src)

    def _find_mini_cluster_jar(self, path):
        for dirpath, dirnames, filenames in os.walk(path):
            for files in filenames:
                if re.match(".*hadoop-mapreduce-client-jobclient.+-tests.jar", files):
                    return os.path.join(dirpath, files)

    def _start_mini_cluster(self):
        if self._jobclient_jar:
            hadoop_jar = self._jobclient_jar
        else:
            hadoop_jar = self._find_mini_cluster_jar(self._hadoop_home)
        if not hadoop_jar:
            raise Exception("No hadoop jobclient test jar found")
        self.hdfs = subprocess.Popen([self._hadoop_cmd,
                                      'jar',
                                      hadoop_jar,
                                      'minicluster', '-nomr', '-format'],
                                      bufsize=0,
                                      stdout=subprocess.PIPE, stderr=subprocess.PIPE)

    def _get_namenode_port(self):
        port_found = False
        while self.hdfs.poll() is None and not port_found:
            rlist, wlist, xlist = select.select([self.hdfs.stderr, self.hdfs.stdout], [], [])
            for f in rlist:
                line = f.readline()
                print line,
                m = re.match(".*Started MiniDFSCluster -- namenode on port (\d+).*", line)
                if m:
                    return int(m.group(1))

    def _transform_ls_output(self, i, base_path):
        result = []
        for line in i.split("\n"):
            if not line or line.startswith("Found"):
                continue

            (perms, replication, owner, group, length, date, time, path) = re.split("\s+", line)
            node = {}

            if replication == '-':
                replication = 0

            node['permission'] = self._perms_to_int(perms)
            node['block_replication'] = int(replication)
            node['owner'] = owner
            node['group'] = group
            node['length'] = int(length)
            dt = "%s %s" % (date, time)
            node['modification_time'] = long(datetime.datetime.strptime(dt, '%Y-%m-%d %H:%M').strftime('%s'))
            node['path'] = path.replace(base_path, '')
            node['file_type'] = self._get_file_type(perms[0])
            result.append(node)
        return result

    def _transform_du_output(self, i, base_path):
        result = []
        for line in i.split("\n"):
            if line:
                (length, path) = re.split("\s+", line)
                result.append({"path": path.replace(base_path, ""), "length": long(length)})
        return result

    def _transform_count_output(self, i, base_path):
        result = []
        for line in i.split("\n"):
            if line:
                (_, dir_count, file_count, length, path) = re.split("\s+", line)
                result.append({"path": path.replace(base_path, ""), "length": long(length),
                               "directoryCount": long(dir_count), "fileCount": long(file_count)})
        return result

    def _get_file_type(self, i):
        if i == "-":
            return "f"
        else:
            return i

    def _perms_to_int(self, perms):
        s = ""
        for x in perms[1:]:
            if x == "-":
                s += "0"
            else:
                s += "1"
        octal = "%d%d%d" % (int(s[0:3], 2), int(s[3:6], 2), int(s[6:9], 2))
        return int(octal, 8)

########NEW FILE########
__FILENAME__ = namenode
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.

class Namenode(object):
    '''Namenode class - represents HDFS namenode'''
    DEFAULT_PORT = 8020
    DEFAULT_VERSION = 9

    def __init__(self, host, port=DEFAULT_PORT, version=DEFAULT_VERSION):
        self.host = host
        self.port = port
        self.version = version

    def is_active(self):
        return True

    def toDict(self):
        return {"namenode": self.host,
                "port": self.port,
                "version": self.version}

########NEW FILE########
__FILENAME__ = ClientNamenodeProtocol_pb2
# Generated by the protocol buffer compiler.  DO NOT EDIT!

from google.protobuf import descriptor
from google.protobuf import message
from google.protobuf import reflection
from google.protobuf import service
from google.protobuf import service_reflection
from google.protobuf import descriptor_pb2
# @@protoc_insertion_point(imports)


import Security_pb2
import hdfs_pb2

DESCRIPTOR = descriptor.FileDescriptor(
  name='ClientNamenodeProtocol.proto',
  package='hadoop.hdfs',
  serialized_pb='\n\x1c\x43lientNamenodeProtocol.proto\x12\x0bhadoop.hdfs\x1a\x0eSecurity.proto\x1a\nhdfs.proto\"L\n\x1dGetBlockLocationsRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\x12\x0e\n\x06offset\x18\x02 \x02(\x04\x12\x0e\n\x06length\x18\x03 \x02(\x04\"T\n\x1eGetBlockLocationsResponseProto\x12\x32\n\tlocations\x18\x01 \x01(\x0b\x32\x1f.hadoop.hdfs.LocatedBlocksProto\"\x1f\n\x1dGetServerDefaultsRequestProto\"\\\n\x1eGetServerDefaultsResponseProto\x12:\n\x0eserverDefaults\x18\x01 \x02(\x0b\x32\".hadoop.hdfs.FsServerDefaultsProto\"\xb7\x01\n\x12\x43reateRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\x12.\n\x06masked\x18\x02 \x02(\x0b\x32\x1e.hadoop.hdfs.FsPermissionProto\x12\x12\n\nclientName\x18\x03 \x02(\t\x12\x12\n\ncreateFlag\x18\x04 \x02(\r\x12\x14\n\x0c\x63reateParent\x18\x05 \x02(\x08\x12\x13\n\x0breplication\x18\x06 \x02(\r\x12\x11\n\tblockSize\x18\x07 \x02(\x04\"C\n\x13\x43reateResponseProto\x12,\n\x02\x66s\x18\x01 \x01(\x0b\x32 .hadoop.hdfs.HdfsFileStatusProto\"5\n\x12\x41ppendRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\x12\x12\n\nclientName\x18\x02 \x02(\t\"D\n\x13\x41ppendResponseProto\x12-\n\x05\x62lock\x18\x01 \x01(\x0b\x32\x1e.hadoop.hdfs.LocatedBlockProto\">\n\x1aSetReplicationRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\x12\x13\n\x0breplication\x18\x02 \x02(\r\"-\n\x1bSetReplicationResponseProto\x12\x0e\n\x06result\x18\x01 \x02(\x08\"\\\n\x19SetPermissionRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\x12\x32\n\npermission\x18\x02 \x02(\x0b\x32\x1e.hadoop.hdfs.FsPermissionProto\"\x1c\n\x1aSetPermissionResponseProto\"H\n\x14SetOwnerRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\x12\x10\n\x08username\x18\x02 \x01(\t\x12\x11\n\tgroupname\x18\x03 \x01(\t\"\x17\n\x15SetOwnerResponseProto\"c\n\x18\x41\x62\x61ndonBlockRequestProto\x12*\n\x01\x62\x18\x01 \x02(\x0b\x32\x1f.hadoop.hdfs.ExtendedBlockProto\x12\x0b\n\x03src\x18\x02 \x02(\t\x12\x0e\n\x06holder\x18\x03 \x02(\t\"\x1b\n\x19\x41\x62\x61ndonBlockResponseProto\"\xc9\x01\n\x14\x41\x64\x64\x42lockRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\x12\x12\n\nclientName\x18\x02 \x02(\t\x12\x31\n\x08previous\x18\x03 \x01(\x0b\x32\x1f.hadoop.hdfs.ExtendedBlockProto\x12\x34\n\x0c\x65xcludeNodes\x18\x04 \x03(\x0b\x32\x1e.hadoop.hdfs.DatanodeInfoProto\x12\x11\n\x06\x66ileId\x18\x05 \x01(\x04:\x01\x30\x12\x14\n\x0c\x66\x61voredNodes\x18\x06 \x03(\t\"F\n\x15\x41\x64\x64\x42lockResponseProto\x12-\n\x05\x62lock\x18\x01 \x02(\x0b\x32\x1e.hadoop.hdfs.LocatedBlockProto\"\xf3\x01\n!GetAdditionalDatanodeRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\x12,\n\x03\x62lk\x18\x02 \x02(\x0b\x32\x1f.hadoop.hdfs.ExtendedBlockProto\x12\x31\n\texistings\x18\x03 \x03(\x0b\x32\x1e.hadoop.hdfs.DatanodeInfoProto\x12\x30\n\x08\x65xcludes\x18\x04 \x03(\x0b\x32\x1e.hadoop.hdfs.DatanodeInfoProto\x12\x1a\n\x12numAdditionalNodes\x18\x05 \x02(\r\x12\x12\n\nclientName\x18\x06 \x02(\t\"S\n\"GetAdditionalDatanodeResponseProto\x12-\n\x05\x62lock\x18\x01 \x02(\x0b\x32\x1e.hadoop.hdfs.LocatedBlockProto\"y\n\x14\x43ompleteRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\x12\x12\n\nclientName\x18\x02 \x02(\t\x12-\n\x04last\x18\x03 \x01(\x0b\x32\x1f.hadoop.hdfs.ExtendedBlockProto\x12\x11\n\x06\x66ileId\x18\x04 \x01(\x04:\x01\x30\"\'\n\x15\x43ompleteResponseProto\x12\x0e\n\x06result\x18\x01 \x02(\x08\"M\n\x1bReportBadBlocksRequestProto\x12.\n\x06\x62locks\x18\x01 \x03(\x0b\x32\x1e.hadoop.hdfs.LocatedBlockProto\"\x1e\n\x1cReportBadBlocksResponseProto\"/\n\x12\x43oncatRequestProto\x12\x0b\n\x03trg\x18\x01 \x02(\t\x12\x0c\n\x04srcs\x18\x02 \x03(\t\"\x15\n\x13\x43oncatResponseProto\".\n\x12RenameRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\x12\x0b\n\x03\x64st\x18\x02 \x02(\t\"%\n\x13RenameResponseProto\x12\x0e\n\x06result\x18\x01 \x02(\x08\"F\n\x13Rename2RequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\x12\x0b\n\x03\x64st\x18\x02 \x02(\t\x12\x15\n\roverwriteDest\x18\x03 \x02(\x08\"\x16\n\x14Rename2ResponseProto\"4\n\x12\x44\x65leteRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\x12\x11\n\trecursive\x18\x02 \x02(\x08\"%\n\x13\x44\x65leteResponseProto\x12\x0e\n\x06result\x18\x01 \x02(\x08\"g\n\x12MkdirsRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\x12.\n\x06masked\x18\x02 \x02(\x0b\x32\x1e.hadoop.hdfs.FsPermissionProto\x12\x14\n\x0c\x63reateParent\x18\x03 \x02(\x08\"%\n\x13MkdirsResponseProto\x12\x0e\n\x06result\x18\x01 \x02(\x08\"O\n\x16GetListingRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\x12\x12\n\nstartAfter\x18\x02 \x02(\x0c\x12\x14\n\x0cneedLocation\x18\x03 \x02(\x08\"N\n\x17GetListingResponseProto\x12\x33\n\x07\x64irList\x18\x01 \x01(\x0b\x32\".hadoop.hdfs.DirectoryListingProto\"(\n&GetSnapshottableDirListingRequestProto\"x\n\'GetSnapshottableDirListingResponseProto\x12M\n\x14snapshottableDirList\x18\x01 \x01(\x0b\x32/.hadoop.hdfs.SnapshottableDirectoryListingProto\"c\n!GetSnapshotDiffReportRequestProto\x12\x14\n\x0csnapshotRoot\x18\x01 \x02(\t\x12\x14\n\x0c\x66romSnapshot\x18\x02 \x02(\t\x12\x12\n\ntoSnapshot\x18\x03 \x02(\t\"^\n\"GetSnapshotDiffReportResponseProto\x12\x38\n\ndiffReport\x18\x01 \x02(\x0b\x32$.hadoop.hdfs.SnapshotDiffReportProto\",\n\x16RenewLeaseRequestProto\x12\x12\n\nclientName\x18\x01 \x02(\t\"\x19\n\x17RenewLeaseResponseProto\";\n\x18RecoverLeaseRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\x12\x12\n\nclientName\x18\x02 \x02(\t\"+\n\x19RecoverLeaseResponseProto\x12\x0e\n\x06result\x18\x01 \x02(\x08\"\x19\n\x17GetFsStatusRequestProto\"\x96\x01\n\x17GetFsStatsResponseProto\x12\x10\n\x08\x63\x61pacity\x18\x01 \x02(\x04\x12\x0c\n\x04used\x18\x02 \x02(\x04\x12\x11\n\tremaining\x18\x03 \x02(\x04\x12\x18\n\x10under_replicated\x18\x04 \x02(\x04\x12\x16\n\x0e\x63orrupt_blocks\x18\x05 \x02(\x04\x12\x16\n\x0emissing_blocks\x18\x06 \x02(\x04\"S\n\x1dGetDatanodeReportRequestProto\x12\x32\n\x04type\x18\x01 \x02(\x0e\x32$.hadoop.hdfs.DatanodeReportTypeProto\"L\n\x1eGetDatanodeReportResponseProto\x12*\n\x02\x64i\x18\x01 \x03(\x0b\x32\x1e.hadoop.hdfs.DatanodeInfoProto\"5\n!GetPreferredBlockSizeRequestProto\x12\x10\n\x08\x66ilename\x18\x01 \x02(\t\"3\n\"GetPreferredBlockSizeResponseProto\x12\r\n\x05\x62size\x18\x01 \x02(\x04\"c\n\x17SetSafeModeRequestProto\x12\x30\n\x06\x61\x63tion\x18\x01 \x02(\x0e\x32 .hadoop.hdfs.SafeModeActionProto\x12\x16\n\x07\x63hecked\x18\x02 \x01(\x08:\x05\x66\x61lse\"*\n\x18SetSafeModeResponseProto\x12\x0e\n\x06result\x18\x01 \x02(\x08\"\x1b\n\x19SaveNamespaceRequestProto\"\x1c\n\x1aSaveNamespaceResponseProto\"\x17\n\x15RollEditsRequestProto\"0\n\x16RollEditsResponseProto\x12\x16\n\x0enewSegmentTxId\x18\x01 \x02(\x04\"/\n RestoreFailedStorageRequestProto\x12\x0b\n\x03\x61rg\x18\x01 \x02(\t\"3\n!RestoreFailedStorageResponseProto\x12\x0e\n\x06result\x18\x01 \x02(\x08\"\x1a\n\x18RefreshNodesRequestProto\"\x1b\n\x19RefreshNodesResponseProto\"\x1d\n\x1b\x46inalizeUpgradeRequestProto\"\x1e\n\x1c\x46inalizeUpgradeResponseProto\"A\n!ListCorruptFileBlocksRequestProto\x12\x0c\n\x04path\x18\x01 \x02(\t\x12\x0e\n\x06\x63ookie\x18\x02 \x01(\t\"Z\n\"ListCorruptFileBlocksResponseProto\x12\x34\n\x07\x63orrupt\x18\x01 \x02(\x0b\x32#.hadoop.hdfs.CorruptFileBlocksProto\"(\n\x14MetaSaveRequestProto\x12\x10\n\x08\x66ilename\x18\x01 \x02(\t\"\x17\n\x15MetaSaveResponseProto\"&\n\x17GetFileInfoRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\"H\n\x18GetFileInfoResponseProto\x12,\n\x02\x66s\x18\x01 \x01(\x0b\x32 .hadoop.hdfs.HdfsFileStatusProto\"\'\n\x18IsFileClosedRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\"+\n\x19IsFileClosedResponseProto\x12\x0e\n\x06result\x18\x01 \x02(\x08\"*\n\x1bGetFileLinkInfoRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\"L\n\x1cGetFileLinkInfoResponseProto\x12,\n\x02\x66s\x18\x01 \x01(\x0b\x32 .hadoop.hdfs.HdfsFileStatusProto\"-\n\x1dGetContentSummaryRequestProto\x12\x0c\n\x04path\x18\x01 \x02(\t\"S\n\x1eGetContentSummaryResponseProto\x12\x31\n\x07summary\x18\x01 \x02(\x0b\x32 .hadoop.hdfs.ContentSummaryProto\"T\n\x14SetQuotaRequestProto\x12\x0c\n\x04path\x18\x01 \x02(\t\x12\x16\n\x0enamespaceQuota\x18\x02 \x02(\x04\x12\x16\n\x0e\x64iskspaceQuota\x18\x03 \x02(\x04\"\x17\n\x15SetQuotaResponseProto\"M\n\x11\x46syncRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\x12\x0e\n\x06\x63lient\x18\x02 \x02(\t\x12\x1b\n\x0flastBlockLength\x18\x03 \x01(\x12:\x02-1\"\x14\n\x12\x46syncResponseProto\"A\n\x14SetTimesRequestProto\x12\x0b\n\x03src\x18\x01 \x02(\t\x12\r\n\x05mtime\x18\x02 \x02(\x04\x12\r\n\x05\x61time\x18\x03 \x02(\x04\"\x17\n\x15SetTimesResponseProto\"\x80\x01\n\x19\x43reateSymlinkRequestProto\x12\x0e\n\x06target\x18\x01 \x02(\t\x12\x0c\n\x04link\x18\x02 \x02(\t\x12/\n\x07\x64irPerm\x18\x03 \x02(\x0b\x32\x1e.hadoop.hdfs.FsPermissionProto\x12\x14\n\x0c\x63reateParent\x18\x04 \x02(\x08\"\x1c\n\x1a\x43reateSymlinkResponseProto\")\n\x19GetLinkTargetRequestProto\x12\x0c\n\x04path\x18\x01 \x02(\t\"0\n\x1aGetLinkTargetResponseProto\x12\x12\n\ntargetPath\x18\x01 \x01(\t\"h\n\"UpdateBlockForPipelineRequestProto\x12.\n\x05\x62lock\x18\x01 \x02(\x0b\x32\x1f.hadoop.hdfs.ExtendedBlockProto\x12\x12\n\nclientName\x18\x02 \x02(\t\"T\n#UpdateBlockForPipelineResponseProto\x12-\n\x05\x62lock\x18\x01 \x02(\x0b\x32\x1e.hadoop.hdfs.LocatedBlockProto\"\xc6\x01\n\x1aUpdatePipelineRequestProto\x12\x12\n\nclientName\x18\x01 \x02(\t\x12\x31\n\x08oldBlock\x18\x02 \x02(\x0b\x32\x1f.hadoop.hdfs.ExtendedBlockProto\x12\x31\n\x08newBlock\x18\x03 \x02(\x0b\x32\x1f.hadoop.hdfs.ExtendedBlockProto\x12.\n\x08newNodes\x18\x04 \x03(\x0b\x32\x1c.hadoop.hdfs.DatanodeIDProto\"\x1d\n\x1bUpdatePipelineResponseProto\"5\n SetBalancerBandwidthRequestProto\x12\x11\n\tbandwidth\x18\x01 \x02(\x03\"#\n!SetBalancerBandwidthResponseProto\"\"\n GetDataEncryptionKeyRequestProto\"c\n!GetDataEncryptionKeyResponseProto\x12>\n\x11\x64\x61taEncryptionKey\x18\x01 \x01(\x0b\x32#.hadoop.hdfs.DataEncryptionKeyProto\"H\n\x1a\x43reateSnapshotRequestProto\x12\x14\n\x0csnapshotRoot\x18\x01 \x02(\t\x12\x14\n\x0csnapshotName\x18\x02 \x01(\t\"3\n\x1b\x43reateSnapshotResponseProto\x12\x14\n\x0csnapshotPath\x18\x01 \x02(\t\"d\n\x1aRenameSnapshotRequestProto\x12\x14\n\x0csnapshotRoot\x18\x01 \x02(\t\x12\x17\n\x0fsnapshotOldName\x18\x02 \x02(\t\x12\x17\n\x0fsnapshotNewName\x18\x03 \x02(\t\"\x1d\n\x1bRenameSnapshotResponseProto\"1\n\x19\x41llowSnapshotRequestProto\x12\x14\n\x0csnapshotRoot\x18\x01 \x02(\t\"\x1c\n\x1a\x41llowSnapshotResponseProto\"4\n\x1c\x44isallowSnapshotRequestProto\x12\x14\n\x0csnapshotRoot\x18\x01 \x02(\t\"\x1f\n\x1d\x44isallowSnapshotResponseProto\"H\n\x1a\x44\x65leteSnapshotRequestProto\x12\x14\n\x0csnapshotRoot\x18\x01 \x02(\t\x12\x14\n\x0csnapshotName\x18\x02 \x02(\t\"\x1d\n\x1b\x44\x65leteSnapshotResponseProto*8\n\x0f\x43reateFlagProto\x12\n\n\x06\x43REATE\x10\x01\x12\r\n\tOVERWRITE\x10\x02\x12\n\n\x06\x41PPEND\x10\x04*6\n\x17\x44\x61tanodeReportTypeProto\x12\x07\n\x03\x41LL\x10\x01\x12\x08\n\x04LIVE\x10\x02\x12\x08\n\x04\x44\x45\x41\x44\x10\x03*O\n\x13SafeModeActionProto\x12\x12\n\x0eSAFEMODE_LEAVE\x10\x01\x12\x12\n\x0eSAFEMODE_ENTER\x10\x02\x12\x10\n\x0cSAFEMODE_GET\x10\x03\x32\x88*\n\x16\x43lientNamenodeProtocol\x12l\n\x11getBlockLocations\x12*.hadoop.hdfs.GetBlockLocationsRequestProto\x1a+.hadoop.hdfs.GetBlockLocationsResponseProto\x12l\n\x11getServerDefaults\x12*.hadoop.hdfs.GetServerDefaultsRequestProto\x1a+.hadoop.hdfs.GetServerDefaultsResponseProto\x12K\n\x06\x63reate\x12\x1f.hadoop.hdfs.CreateRequestProto\x1a .hadoop.hdfs.CreateResponseProto\x12K\n\x06\x61ppend\x12\x1f.hadoop.hdfs.AppendRequestProto\x1a .hadoop.hdfs.AppendResponseProto\x12\x63\n\x0esetReplication\x12\'.hadoop.hdfs.SetReplicationRequestProto\x1a(.hadoop.hdfs.SetReplicationResponseProto\x12`\n\rsetPermission\x12&.hadoop.hdfs.SetPermissionRequestProto\x1a\'.hadoop.hdfs.SetPermissionResponseProto\x12Q\n\x08setOwner\x12!.hadoop.hdfs.SetOwnerRequestProto\x1a\".hadoop.hdfs.SetOwnerResponseProto\x12]\n\x0c\x61\x62\x61ndonBlock\x12%.hadoop.hdfs.AbandonBlockRequestProto\x1a&.hadoop.hdfs.AbandonBlockResponseProto\x12Q\n\x08\x61\x64\x64\x42lock\x12!.hadoop.hdfs.AddBlockRequestProto\x1a\".hadoop.hdfs.AddBlockResponseProto\x12x\n\x15getAdditionalDatanode\x12..hadoop.hdfs.GetAdditionalDatanodeRequestProto\x1a/.hadoop.hdfs.GetAdditionalDatanodeResponseProto\x12Q\n\x08\x63omplete\x12!.hadoop.hdfs.CompleteRequestProto\x1a\".hadoop.hdfs.CompleteResponseProto\x12\x66\n\x0freportBadBlocks\x12(.hadoop.hdfs.ReportBadBlocksRequestProto\x1a).hadoop.hdfs.ReportBadBlocksResponseProto\x12K\n\x06\x63oncat\x12\x1f.hadoop.hdfs.ConcatRequestProto\x1a .hadoop.hdfs.ConcatResponseProto\x12K\n\x06rename\x12\x1f.hadoop.hdfs.RenameRequestProto\x1a .hadoop.hdfs.RenameResponseProto\x12N\n\x07rename2\x12 .hadoop.hdfs.Rename2RequestProto\x1a!.hadoop.hdfs.Rename2ResponseProto\x12K\n\x06\x64\x65lete\x12\x1f.hadoop.hdfs.DeleteRequestProto\x1a .hadoop.hdfs.DeleteResponseProto\x12K\n\x06mkdirs\x12\x1f.hadoop.hdfs.MkdirsRequestProto\x1a .hadoop.hdfs.MkdirsResponseProto\x12W\n\ngetListing\x12#.hadoop.hdfs.GetListingRequestProto\x1a$.hadoop.hdfs.GetListingResponseProto\x12W\n\nrenewLease\x12#.hadoop.hdfs.RenewLeaseRequestProto\x1a$.hadoop.hdfs.RenewLeaseResponseProto\x12]\n\x0crecoverLease\x12%.hadoop.hdfs.RecoverLeaseRequestProto\x1a&.hadoop.hdfs.RecoverLeaseResponseProto\x12X\n\ngetFsStats\x12$.hadoop.hdfs.GetFsStatusRequestProto\x1a$.hadoop.hdfs.GetFsStatsResponseProto\x12l\n\x11getDatanodeReport\x12*.hadoop.hdfs.GetDatanodeReportRequestProto\x1a+.hadoop.hdfs.GetDatanodeReportResponseProto\x12x\n\x15getPreferredBlockSize\x12..hadoop.hdfs.GetPreferredBlockSizeRequestProto\x1a/.hadoop.hdfs.GetPreferredBlockSizeResponseProto\x12Z\n\x0bsetSafeMode\x12$.hadoop.hdfs.SetSafeModeRequestProto\x1a%.hadoop.hdfs.SetSafeModeResponseProto\x12`\n\rsaveNamespace\x12&.hadoop.hdfs.SaveNamespaceRequestProto\x1a\'.hadoop.hdfs.SaveNamespaceResponseProto\x12T\n\trollEdits\x12\".hadoop.hdfs.RollEditsRequestProto\x1a#.hadoop.hdfs.RollEditsResponseProto\x12u\n\x14restoreFailedStorage\x12-.hadoop.hdfs.RestoreFailedStorageRequestProto\x1a..hadoop.hdfs.RestoreFailedStorageResponseProto\x12]\n\x0crefreshNodes\x12%.hadoop.hdfs.RefreshNodesRequestProto\x1a&.hadoop.hdfs.RefreshNodesResponseProto\x12\x66\n\x0f\x66inalizeUpgrade\x12(.hadoop.hdfs.FinalizeUpgradeRequestProto\x1a).hadoop.hdfs.FinalizeUpgradeResponseProto\x12x\n\x15listCorruptFileBlocks\x12..hadoop.hdfs.ListCorruptFileBlocksRequestProto\x1a/.hadoop.hdfs.ListCorruptFileBlocksResponseProto\x12Q\n\x08metaSave\x12!.hadoop.hdfs.MetaSaveRequestProto\x1a\".hadoop.hdfs.MetaSaveResponseProto\x12Z\n\x0bgetFileInfo\x12$.hadoop.hdfs.GetFileInfoRequestProto\x1a%.hadoop.hdfs.GetFileInfoResponseProto\x12\x66\n\x0fgetFileLinkInfo\x12(.hadoop.hdfs.GetFileLinkInfoRequestProto\x1a).hadoop.hdfs.GetFileLinkInfoResponseProto\x12l\n\x11getContentSummary\x12*.hadoop.hdfs.GetContentSummaryRequestProto\x1a+.hadoop.hdfs.GetContentSummaryResponseProto\x12Q\n\x08setQuota\x12!.hadoop.hdfs.SetQuotaRequestProto\x1a\".hadoop.hdfs.SetQuotaResponseProto\x12H\n\x05\x66sync\x12\x1e.hadoop.hdfs.FsyncRequestProto\x1a\x1f.hadoop.hdfs.FsyncResponseProto\x12Q\n\x08setTimes\x12!.hadoop.hdfs.SetTimesRequestProto\x1a\".hadoop.hdfs.SetTimesResponseProto\x12`\n\rcreateSymlink\x12&.hadoop.hdfs.CreateSymlinkRequestProto\x1a\'.hadoop.hdfs.CreateSymlinkResponseProto\x12`\n\rgetLinkTarget\x12&.hadoop.hdfs.GetLinkTargetRequestProto\x1a\'.hadoop.hdfs.GetLinkTargetResponseProto\x12{\n\x16updateBlockForPipeline\x12/.hadoop.hdfs.UpdateBlockForPipelineRequestProto\x1a\x30.hadoop.hdfs.UpdateBlockForPipelineResponseProto\x12\x63\n\x0eupdatePipeline\x12\'.hadoop.hdfs.UpdatePipelineRequestProto\x1a(.hadoop.hdfs.UpdatePipelineResponseProto\x12s\n\x12getDelegationToken\x12-.hadoop.common.GetDelegationTokenRequestProto\x1a..hadoop.common.GetDelegationTokenResponseProto\x12y\n\x14renewDelegationToken\x12/.hadoop.common.RenewDelegationTokenRequestProto\x1a\x30.hadoop.common.RenewDelegationTokenResponseProto\x12|\n\x15\x63\x61ncelDelegationToken\x12\x30.hadoop.common.CancelDelegationTokenRequestProto\x1a\x31.hadoop.common.CancelDelegationTokenResponseProto\x12u\n\x14setBalancerBandwidth\x12-.hadoop.hdfs.SetBalancerBandwidthRequestProto\x1a..hadoop.hdfs.SetBalancerBandwidthResponseProto\x12u\n\x14getDataEncryptionKey\x12-.hadoop.hdfs.GetDataEncryptionKeyRequestProto\x1a..hadoop.hdfs.GetDataEncryptionKeyResponseProto\x12\x63\n\x0e\x63reateSnapshot\x12\'.hadoop.hdfs.CreateSnapshotRequestProto\x1a(.hadoop.hdfs.CreateSnapshotResponseProto\x12\x63\n\x0erenameSnapshot\x12\'.hadoop.hdfs.RenameSnapshotRequestProto\x1a(.hadoop.hdfs.RenameSnapshotResponseProto\x12`\n\rallowSnapshot\x12&.hadoop.hdfs.AllowSnapshotRequestProto\x1a\'.hadoop.hdfs.AllowSnapshotResponseProto\x12i\n\x10\x64isallowSnapshot\x12).hadoop.hdfs.DisallowSnapshotRequestProto\x1a*.hadoop.hdfs.DisallowSnapshotResponseProto\x12\x87\x01\n\x1agetSnapshottableDirListing\x12\x33.hadoop.hdfs.GetSnapshottableDirListingRequestProto\x1a\x34.hadoop.hdfs.GetSnapshottableDirListingResponseProto\x12\x63\n\x0e\x64\x65leteSnapshot\x12\'.hadoop.hdfs.DeleteSnapshotRequestProto\x1a(.hadoop.hdfs.DeleteSnapshotResponseProto\x12x\n\x15getSnapshotDiffReport\x12..hadoop.hdfs.GetSnapshotDiffReportRequestProto\x1a/.hadoop.hdfs.GetSnapshotDiffReportResponseProto\x12]\n\x0cisFileClosed\x12%.hadoop.hdfs.IsFileClosedRequestProto\x1a&.hadoop.hdfs.IsFileClosedResponseProtoBN\n%org.apache.hadoop.hdfs.protocol.protoB\x1c\x43lientNamenodeProtocolProtos\x88\x01\x01\x90\x01\x01\xa0\x01\x01')

_CREATEFLAGPROTO = descriptor.EnumDescriptor(
  name='CreateFlagProto',
  full_name='hadoop.hdfs.CreateFlagProto',
  filename=None,
  file=DESCRIPTOR,
  values=[
    descriptor.EnumValueDescriptor(
      name='CREATE', index=0, number=1,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='OVERWRITE', index=1, number=2,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='APPEND', index=2, number=4,
      options=None,
      type=None),
  ],
  containing_type=None,
  options=None,
  serialized_start=6659,
  serialized_end=6715,
)


_DATANODEREPORTTYPEPROTO = descriptor.EnumDescriptor(
  name='DatanodeReportTypeProto',
  full_name='hadoop.hdfs.DatanodeReportTypeProto',
  filename=None,
  file=DESCRIPTOR,
  values=[
    descriptor.EnumValueDescriptor(
      name='ALL', index=0, number=1,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='LIVE', index=1, number=2,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='DEAD', index=2, number=3,
      options=None,
      type=None),
  ],
  containing_type=None,
  options=None,
  serialized_start=6717,
  serialized_end=6771,
)


_SAFEMODEACTIONPROTO = descriptor.EnumDescriptor(
  name='SafeModeActionProto',
  full_name='hadoop.hdfs.SafeModeActionProto',
  filename=None,
  file=DESCRIPTOR,
  values=[
    descriptor.EnumValueDescriptor(
      name='SAFEMODE_LEAVE', index=0, number=1,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='SAFEMODE_ENTER', index=1, number=2,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='SAFEMODE_GET', index=2, number=3,
      options=None,
      type=None),
  ],
  containing_type=None,
  options=None,
  serialized_start=6773,
  serialized_end=6852,
)


CREATE = 1
OVERWRITE = 2
APPEND = 4
ALL = 1
LIVE = 2
DEAD = 3
SAFEMODE_LEAVE = 1
SAFEMODE_ENTER = 2
SAFEMODE_GET = 3



_GETBLOCKLOCATIONSREQUESTPROTO = descriptor.Descriptor(
  name='GetBlockLocationsRequestProto',
  full_name='hadoop.hdfs.GetBlockLocationsRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.GetBlockLocationsRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='offset', full_name='hadoop.hdfs.GetBlockLocationsRequestProto.offset', index=1,
      number=2, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='length', full_name='hadoop.hdfs.GetBlockLocationsRequestProto.length', index=2,
      number=3, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=73,
  serialized_end=149,
)


_GETBLOCKLOCATIONSRESPONSEPROTO = descriptor.Descriptor(
  name='GetBlockLocationsResponseProto',
  full_name='hadoop.hdfs.GetBlockLocationsResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='locations', full_name='hadoop.hdfs.GetBlockLocationsResponseProto.locations', index=0,
      number=1, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=151,
  serialized_end=235,
)


_GETSERVERDEFAULTSREQUESTPROTO = descriptor.Descriptor(
  name='GetServerDefaultsRequestProto',
  full_name='hadoop.hdfs.GetServerDefaultsRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=237,
  serialized_end=268,
)


_GETSERVERDEFAULTSRESPONSEPROTO = descriptor.Descriptor(
  name='GetServerDefaultsResponseProto',
  full_name='hadoop.hdfs.GetServerDefaultsResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='serverDefaults', full_name='hadoop.hdfs.GetServerDefaultsResponseProto.serverDefaults', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=270,
  serialized_end=362,
)


_CREATEREQUESTPROTO = descriptor.Descriptor(
  name='CreateRequestProto',
  full_name='hadoop.hdfs.CreateRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.CreateRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='masked', full_name='hadoop.hdfs.CreateRequestProto.masked', index=1,
      number=2, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='clientName', full_name='hadoop.hdfs.CreateRequestProto.clientName', index=2,
      number=3, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='createFlag', full_name='hadoop.hdfs.CreateRequestProto.createFlag', index=3,
      number=4, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='createParent', full_name='hadoop.hdfs.CreateRequestProto.createParent', index=4,
      number=5, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='replication', full_name='hadoop.hdfs.CreateRequestProto.replication', index=5,
      number=6, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='blockSize', full_name='hadoop.hdfs.CreateRequestProto.blockSize', index=6,
      number=7, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=365,
  serialized_end=548,
)


_CREATERESPONSEPROTO = descriptor.Descriptor(
  name='CreateResponseProto',
  full_name='hadoop.hdfs.CreateResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='fs', full_name='hadoop.hdfs.CreateResponseProto.fs', index=0,
      number=1, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=550,
  serialized_end=617,
)


_APPENDREQUESTPROTO = descriptor.Descriptor(
  name='AppendRequestProto',
  full_name='hadoop.hdfs.AppendRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.AppendRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='clientName', full_name='hadoop.hdfs.AppendRequestProto.clientName', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=619,
  serialized_end=672,
)


_APPENDRESPONSEPROTO = descriptor.Descriptor(
  name='AppendResponseProto',
  full_name='hadoop.hdfs.AppendResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='block', full_name='hadoop.hdfs.AppendResponseProto.block', index=0,
      number=1, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=674,
  serialized_end=742,
)


_SETREPLICATIONREQUESTPROTO = descriptor.Descriptor(
  name='SetReplicationRequestProto',
  full_name='hadoop.hdfs.SetReplicationRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.SetReplicationRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='replication', full_name='hadoop.hdfs.SetReplicationRequestProto.replication', index=1,
      number=2, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=744,
  serialized_end=806,
)


_SETREPLICATIONRESPONSEPROTO = descriptor.Descriptor(
  name='SetReplicationResponseProto',
  full_name='hadoop.hdfs.SetReplicationResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='result', full_name='hadoop.hdfs.SetReplicationResponseProto.result', index=0,
      number=1, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=808,
  serialized_end=853,
)


_SETPERMISSIONREQUESTPROTO = descriptor.Descriptor(
  name='SetPermissionRequestProto',
  full_name='hadoop.hdfs.SetPermissionRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.SetPermissionRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='permission', full_name='hadoop.hdfs.SetPermissionRequestProto.permission', index=1,
      number=2, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=855,
  serialized_end=947,
)


_SETPERMISSIONRESPONSEPROTO = descriptor.Descriptor(
  name='SetPermissionResponseProto',
  full_name='hadoop.hdfs.SetPermissionResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=949,
  serialized_end=977,
)


_SETOWNERREQUESTPROTO = descriptor.Descriptor(
  name='SetOwnerRequestProto',
  full_name='hadoop.hdfs.SetOwnerRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.SetOwnerRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='username', full_name='hadoop.hdfs.SetOwnerRequestProto.username', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='groupname', full_name='hadoop.hdfs.SetOwnerRequestProto.groupname', index=2,
      number=3, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=979,
  serialized_end=1051,
)


_SETOWNERRESPONSEPROTO = descriptor.Descriptor(
  name='SetOwnerResponseProto',
  full_name='hadoop.hdfs.SetOwnerResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1053,
  serialized_end=1076,
)


_ABANDONBLOCKREQUESTPROTO = descriptor.Descriptor(
  name='AbandonBlockRequestProto',
  full_name='hadoop.hdfs.AbandonBlockRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='b', full_name='hadoop.hdfs.AbandonBlockRequestProto.b', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.AbandonBlockRequestProto.src', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='holder', full_name='hadoop.hdfs.AbandonBlockRequestProto.holder', index=2,
      number=3, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1078,
  serialized_end=1177,
)


_ABANDONBLOCKRESPONSEPROTO = descriptor.Descriptor(
  name='AbandonBlockResponseProto',
  full_name='hadoop.hdfs.AbandonBlockResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1179,
  serialized_end=1206,
)


_ADDBLOCKREQUESTPROTO = descriptor.Descriptor(
  name='AddBlockRequestProto',
  full_name='hadoop.hdfs.AddBlockRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.AddBlockRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='clientName', full_name='hadoop.hdfs.AddBlockRequestProto.clientName', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='previous', full_name='hadoop.hdfs.AddBlockRequestProto.previous', index=2,
      number=3, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='excludeNodes', full_name='hadoop.hdfs.AddBlockRequestProto.excludeNodes', index=3,
      number=4, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='fileId', full_name='hadoop.hdfs.AddBlockRequestProto.fileId', index=4,
      number=5, type=4, cpp_type=4, label=1,
      has_default_value=True, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='favoredNodes', full_name='hadoop.hdfs.AddBlockRequestProto.favoredNodes', index=5,
      number=6, type=9, cpp_type=9, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1209,
  serialized_end=1410,
)


_ADDBLOCKRESPONSEPROTO = descriptor.Descriptor(
  name='AddBlockResponseProto',
  full_name='hadoop.hdfs.AddBlockResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='block', full_name='hadoop.hdfs.AddBlockResponseProto.block', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1412,
  serialized_end=1482,
)


_GETADDITIONALDATANODEREQUESTPROTO = descriptor.Descriptor(
  name='GetAdditionalDatanodeRequestProto',
  full_name='hadoop.hdfs.GetAdditionalDatanodeRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.GetAdditionalDatanodeRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='blk', full_name='hadoop.hdfs.GetAdditionalDatanodeRequestProto.blk', index=1,
      number=2, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='existings', full_name='hadoop.hdfs.GetAdditionalDatanodeRequestProto.existings', index=2,
      number=3, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='excludes', full_name='hadoop.hdfs.GetAdditionalDatanodeRequestProto.excludes', index=3,
      number=4, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='numAdditionalNodes', full_name='hadoop.hdfs.GetAdditionalDatanodeRequestProto.numAdditionalNodes', index=4,
      number=5, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='clientName', full_name='hadoop.hdfs.GetAdditionalDatanodeRequestProto.clientName', index=5,
      number=6, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1485,
  serialized_end=1728,
)


_GETADDITIONALDATANODERESPONSEPROTO = descriptor.Descriptor(
  name='GetAdditionalDatanodeResponseProto',
  full_name='hadoop.hdfs.GetAdditionalDatanodeResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='block', full_name='hadoop.hdfs.GetAdditionalDatanodeResponseProto.block', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1730,
  serialized_end=1813,
)


_COMPLETEREQUESTPROTO = descriptor.Descriptor(
  name='CompleteRequestProto',
  full_name='hadoop.hdfs.CompleteRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.CompleteRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='clientName', full_name='hadoop.hdfs.CompleteRequestProto.clientName', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='last', full_name='hadoop.hdfs.CompleteRequestProto.last', index=2,
      number=3, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='fileId', full_name='hadoop.hdfs.CompleteRequestProto.fileId', index=3,
      number=4, type=4, cpp_type=4, label=1,
      has_default_value=True, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1815,
  serialized_end=1936,
)


_COMPLETERESPONSEPROTO = descriptor.Descriptor(
  name='CompleteResponseProto',
  full_name='hadoop.hdfs.CompleteResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='result', full_name='hadoop.hdfs.CompleteResponseProto.result', index=0,
      number=1, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1938,
  serialized_end=1977,
)


_REPORTBADBLOCKSREQUESTPROTO = descriptor.Descriptor(
  name='ReportBadBlocksRequestProto',
  full_name='hadoop.hdfs.ReportBadBlocksRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='blocks', full_name='hadoop.hdfs.ReportBadBlocksRequestProto.blocks', index=0,
      number=1, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1979,
  serialized_end=2056,
)


_REPORTBADBLOCKSRESPONSEPROTO = descriptor.Descriptor(
  name='ReportBadBlocksResponseProto',
  full_name='hadoop.hdfs.ReportBadBlocksResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2058,
  serialized_end=2088,
)


_CONCATREQUESTPROTO = descriptor.Descriptor(
  name='ConcatRequestProto',
  full_name='hadoop.hdfs.ConcatRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='trg', full_name='hadoop.hdfs.ConcatRequestProto.trg', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='srcs', full_name='hadoop.hdfs.ConcatRequestProto.srcs', index=1,
      number=2, type=9, cpp_type=9, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2090,
  serialized_end=2137,
)


_CONCATRESPONSEPROTO = descriptor.Descriptor(
  name='ConcatResponseProto',
  full_name='hadoop.hdfs.ConcatResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2139,
  serialized_end=2160,
)


_RENAMEREQUESTPROTO = descriptor.Descriptor(
  name='RenameRequestProto',
  full_name='hadoop.hdfs.RenameRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.RenameRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='dst', full_name='hadoop.hdfs.RenameRequestProto.dst', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2162,
  serialized_end=2208,
)


_RENAMERESPONSEPROTO = descriptor.Descriptor(
  name='RenameResponseProto',
  full_name='hadoop.hdfs.RenameResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='result', full_name='hadoop.hdfs.RenameResponseProto.result', index=0,
      number=1, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2210,
  serialized_end=2247,
)


_RENAME2REQUESTPROTO = descriptor.Descriptor(
  name='Rename2RequestProto',
  full_name='hadoop.hdfs.Rename2RequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.Rename2RequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='dst', full_name='hadoop.hdfs.Rename2RequestProto.dst', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='overwriteDest', full_name='hadoop.hdfs.Rename2RequestProto.overwriteDest', index=2,
      number=3, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2249,
  serialized_end=2319,
)


_RENAME2RESPONSEPROTO = descriptor.Descriptor(
  name='Rename2ResponseProto',
  full_name='hadoop.hdfs.Rename2ResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2321,
  serialized_end=2343,
)


_DELETEREQUESTPROTO = descriptor.Descriptor(
  name='DeleteRequestProto',
  full_name='hadoop.hdfs.DeleteRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.DeleteRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='recursive', full_name='hadoop.hdfs.DeleteRequestProto.recursive', index=1,
      number=2, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2345,
  serialized_end=2397,
)


_DELETERESPONSEPROTO = descriptor.Descriptor(
  name='DeleteResponseProto',
  full_name='hadoop.hdfs.DeleteResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='result', full_name='hadoop.hdfs.DeleteResponseProto.result', index=0,
      number=1, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2399,
  serialized_end=2436,
)


_MKDIRSREQUESTPROTO = descriptor.Descriptor(
  name='MkdirsRequestProto',
  full_name='hadoop.hdfs.MkdirsRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.MkdirsRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='masked', full_name='hadoop.hdfs.MkdirsRequestProto.masked', index=1,
      number=2, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='createParent', full_name='hadoop.hdfs.MkdirsRequestProto.createParent', index=2,
      number=3, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2438,
  serialized_end=2541,
)


_MKDIRSRESPONSEPROTO = descriptor.Descriptor(
  name='MkdirsResponseProto',
  full_name='hadoop.hdfs.MkdirsResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='result', full_name='hadoop.hdfs.MkdirsResponseProto.result', index=0,
      number=1, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2543,
  serialized_end=2580,
)


_GETLISTINGREQUESTPROTO = descriptor.Descriptor(
  name='GetListingRequestProto',
  full_name='hadoop.hdfs.GetListingRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.GetListingRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='startAfter', full_name='hadoop.hdfs.GetListingRequestProto.startAfter', index=1,
      number=2, type=12, cpp_type=9, label=2,
      has_default_value=False, default_value="",
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='needLocation', full_name='hadoop.hdfs.GetListingRequestProto.needLocation', index=2,
      number=3, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2582,
  serialized_end=2661,
)


_GETLISTINGRESPONSEPROTO = descriptor.Descriptor(
  name='GetListingResponseProto',
  full_name='hadoop.hdfs.GetListingResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='dirList', full_name='hadoop.hdfs.GetListingResponseProto.dirList', index=0,
      number=1, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2663,
  serialized_end=2741,
)


_GETSNAPSHOTTABLEDIRLISTINGREQUESTPROTO = descriptor.Descriptor(
  name='GetSnapshottableDirListingRequestProto',
  full_name='hadoop.hdfs.GetSnapshottableDirListingRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2743,
  serialized_end=2783,
)


_GETSNAPSHOTTABLEDIRLISTINGRESPONSEPROTO = descriptor.Descriptor(
  name='GetSnapshottableDirListingResponseProto',
  full_name='hadoop.hdfs.GetSnapshottableDirListingResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='snapshottableDirList', full_name='hadoop.hdfs.GetSnapshottableDirListingResponseProto.snapshottableDirList', index=0,
      number=1, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2785,
  serialized_end=2905,
)


_GETSNAPSHOTDIFFREPORTREQUESTPROTO = descriptor.Descriptor(
  name='GetSnapshotDiffReportRequestProto',
  full_name='hadoop.hdfs.GetSnapshotDiffReportRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='snapshotRoot', full_name='hadoop.hdfs.GetSnapshotDiffReportRequestProto.snapshotRoot', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='fromSnapshot', full_name='hadoop.hdfs.GetSnapshotDiffReportRequestProto.fromSnapshot', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='toSnapshot', full_name='hadoop.hdfs.GetSnapshotDiffReportRequestProto.toSnapshot', index=2,
      number=3, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2907,
  serialized_end=3006,
)


_GETSNAPSHOTDIFFREPORTRESPONSEPROTO = descriptor.Descriptor(
  name='GetSnapshotDiffReportResponseProto',
  full_name='hadoop.hdfs.GetSnapshotDiffReportResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='diffReport', full_name='hadoop.hdfs.GetSnapshotDiffReportResponseProto.diffReport', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3008,
  serialized_end=3102,
)


_RENEWLEASEREQUESTPROTO = descriptor.Descriptor(
  name='RenewLeaseRequestProto',
  full_name='hadoop.hdfs.RenewLeaseRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='clientName', full_name='hadoop.hdfs.RenewLeaseRequestProto.clientName', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3104,
  serialized_end=3148,
)


_RENEWLEASERESPONSEPROTO = descriptor.Descriptor(
  name='RenewLeaseResponseProto',
  full_name='hadoop.hdfs.RenewLeaseResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3150,
  serialized_end=3175,
)


_RECOVERLEASEREQUESTPROTO = descriptor.Descriptor(
  name='RecoverLeaseRequestProto',
  full_name='hadoop.hdfs.RecoverLeaseRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.RecoverLeaseRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='clientName', full_name='hadoop.hdfs.RecoverLeaseRequestProto.clientName', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3177,
  serialized_end=3236,
)


_RECOVERLEASERESPONSEPROTO = descriptor.Descriptor(
  name='RecoverLeaseResponseProto',
  full_name='hadoop.hdfs.RecoverLeaseResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='result', full_name='hadoop.hdfs.RecoverLeaseResponseProto.result', index=0,
      number=1, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3238,
  serialized_end=3281,
)


_GETFSSTATUSREQUESTPROTO = descriptor.Descriptor(
  name='GetFsStatusRequestProto',
  full_name='hadoop.hdfs.GetFsStatusRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3283,
  serialized_end=3308,
)


_GETFSSTATSRESPONSEPROTO = descriptor.Descriptor(
  name='GetFsStatsResponseProto',
  full_name='hadoop.hdfs.GetFsStatsResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='capacity', full_name='hadoop.hdfs.GetFsStatsResponseProto.capacity', index=0,
      number=1, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='used', full_name='hadoop.hdfs.GetFsStatsResponseProto.used', index=1,
      number=2, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='remaining', full_name='hadoop.hdfs.GetFsStatsResponseProto.remaining', index=2,
      number=3, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='under_replicated', full_name='hadoop.hdfs.GetFsStatsResponseProto.under_replicated', index=3,
      number=4, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='corrupt_blocks', full_name='hadoop.hdfs.GetFsStatsResponseProto.corrupt_blocks', index=4,
      number=5, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='missing_blocks', full_name='hadoop.hdfs.GetFsStatsResponseProto.missing_blocks', index=5,
      number=6, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3311,
  serialized_end=3461,
)


_GETDATANODEREPORTREQUESTPROTO = descriptor.Descriptor(
  name='GetDatanodeReportRequestProto',
  full_name='hadoop.hdfs.GetDatanodeReportRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='type', full_name='hadoop.hdfs.GetDatanodeReportRequestProto.type', index=0,
      number=1, type=14, cpp_type=8, label=2,
      has_default_value=False, default_value=1,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3463,
  serialized_end=3546,
)


_GETDATANODEREPORTRESPONSEPROTO = descriptor.Descriptor(
  name='GetDatanodeReportResponseProto',
  full_name='hadoop.hdfs.GetDatanodeReportResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='di', full_name='hadoop.hdfs.GetDatanodeReportResponseProto.di', index=0,
      number=1, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3548,
  serialized_end=3624,
)


_GETPREFERREDBLOCKSIZEREQUESTPROTO = descriptor.Descriptor(
  name='GetPreferredBlockSizeRequestProto',
  full_name='hadoop.hdfs.GetPreferredBlockSizeRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='filename', full_name='hadoop.hdfs.GetPreferredBlockSizeRequestProto.filename', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3626,
  serialized_end=3679,
)


_GETPREFERREDBLOCKSIZERESPONSEPROTO = descriptor.Descriptor(
  name='GetPreferredBlockSizeResponseProto',
  full_name='hadoop.hdfs.GetPreferredBlockSizeResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='bsize', full_name='hadoop.hdfs.GetPreferredBlockSizeResponseProto.bsize', index=0,
      number=1, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3681,
  serialized_end=3732,
)


_SETSAFEMODEREQUESTPROTO = descriptor.Descriptor(
  name='SetSafeModeRequestProto',
  full_name='hadoop.hdfs.SetSafeModeRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='action', full_name='hadoop.hdfs.SetSafeModeRequestProto.action', index=0,
      number=1, type=14, cpp_type=8, label=2,
      has_default_value=False, default_value=1,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='checked', full_name='hadoop.hdfs.SetSafeModeRequestProto.checked', index=1,
      number=2, type=8, cpp_type=7, label=1,
      has_default_value=True, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3734,
  serialized_end=3833,
)


_SETSAFEMODERESPONSEPROTO = descriptor.Descriptor(
  name='SetSafeModeResponseProto',
  full_name='hadoop.hdfs.SetSafeModeResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='result', full_name='hadoop.hdfs.SetSafeModeResponseProto.result', index=0,
      number=1, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3835,
  serialized_end=3877,
)


_SAVENAMESPACEREQUESTPROTO = descriptor.Descriptor(
  name='SaveNamespaceRequestProto',
  full_name='hadoop.hdfs.SaveNamespaceRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3879,
  serialized_end=3906,
)


_SAVENAMESPACERESPONSEPROTO = descriptor.Descriptor(
  name='SaveNamespaceResponseProto',
  full_name='hadoop.hdfs.SaveNamespaceResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3908,
  serialized_end=3936,
)


_ROLLEDITSREQUESTPROTO = descriptor.Descriptor(
  name='RollEditsRequestProto',
  full_name='hadoop.hdfs.RollEditsRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3938,
  serialized_end=3961,
)


_ROLLEDITSRESPONSEPROTO = descriptor.Descriptor(
  name='RollEditsResponseProto',
  full_name='hadoop.hdfs.RollEditsResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='newSegmentTxId', full_name='hadoop.hdfs.RollEditsResponseProto.newSegmentTxId', index=0,
      number=1, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3963,
  serialized_end=4011,
)


_RESTOREFAILEDSTORAGEREQUESTPROTO = descriptor.Descriptor(
  name='RestoreFailedStorageRequestProto',
  full_name='hadoop.hdfs.RestoreFailedStorageRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='arg', full_name='hadoop.hdfs.RestoreFailedStorageRequestProto.arg', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4013,
  serialized_end=4060,
)


_RESTOREFAILEDSTORAGERESPONSEPROTO = descriptor.Descriptor(
  name='RestoreFailedStorageResponseProto',
  full_name='hadoop.hdfs.RestoreFailedStorageResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='result', full_name='hadoop.hdfs.RestoreFailedStorageResponseProto.result', index=0,
      number=1, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4062,
  serialized_end=4113,
)


_REFRESHNODESREQUESTPROTO = descriptor.Descriptor(
  name='RefreshNodesRequestProto',
  full_name='hadoop.hdfs.RefreshNodesRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4115,
  serialized_end=4141,
)


_REFRESHNODESRESPONSEPROTO = descriptor.Descriptor(
  name='RefreshNodesResponseProto',
  full_name='hadoop.hdfs.RefreshNodesResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4143,
  serialized_end=4170,
)


_FINALIZEUPGRADEREQUESTPROTO = descriptor.Descriptor(
  name='FinalizeUpgradeRequestProto',
  full_name='hadoop.hdfs.FinalizeUpgradeRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4172,
  serialized_end=4201,
)


_FINALIZEUPGRADERESPONSEPROTO = descriptor.Descriptor(
  name='FinalizeUpgradeResponseProto',
  full_name='hadoop.hdfs.FinalizeUpgradeResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4203,
  serialized_end=4233,
)


_LISTCORRUPTFILEBLOCKSREQUESTPROTO = descriptor.Descriptor(
  name='ListCorruptFileBlocksRequestProto',
  full_name='hadoop.hdfs.ListCorruptFileBlocksRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='path', full_name='hadoop.hdfs.ListCorruptFileBlocksRequestProto.path', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='cookie', full_name='hadoop.hdfs.ListCorruptFileBlocksRequestProto.cookie', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4235,
  serialized_end=4300,
)


_LISTCORRUPTFILEBLOCKSRESPONSEPROTO = descriptor.Descriptor(
  name='ListCorruptFileBlocksResponseProto',
  full_name='hadoop.hdfs.ListCorruptFileBlocksResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='corrupt', full_name='hadoop.hdfs.ListCorruptFileBlocksResponseProto.corrupt', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4302,
  serialized_end=4392,
)


_METASAVEREQUESTPROTO = descriptor.Descriptor(
  name='MetaSaveRequestProto',
  full_name='hadoop.hdfs.MetaSaveRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='filename', full_name='hadoop.hdfs.MetaSaveRequestProto.filename', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4394,
  serialized_end=4434,
)


_METASAVERESPONSEPROTO = descriptor.Descriptor(
  name='MetaSaveResponseProto',
  full_name='hadoop.hdfs.MetaSaveResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4436,
  serialized_end=4459,
)


_GETFILEINFOREQUESTPROTO = descriptor.Descriptor(
  name='GetFileInfoRequestProto',
  full_name='hadoop.hdfs.GetFileInfoRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.GetFileInfoRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4461,
  serialized_end=4499,
)


_GETFILEINFORESPONSEPROTO = descriptor.Descriptor(
  name='GetFileInfoResponseProto',
  full_name='hadoop.hdfs.GetFileInfoResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='fs', full_name='hadoop.hdfs.GetFileInfoResponseProto.fs', index=0,
      number=1, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4501,
  serialized_end=4573,
)


_ISFILECLOSEDREQUESTPROTO = descriptor.Descriptor(
  name='IsFileClosedRequestProto',
  full_name='hadoop.hdfs.IsFileClosedRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.IsFileClosedRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4575,
  serialized_end=4614,
)


_ISFILECLOSEDRESPONSEPROTO = descriptor.Descriptor(
  name='IsFileClosedResponseProto',
  full_name='hadoop.hdfs.IsFileClosedResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='result', full_name='hadoop.hdfs.IsFileClosedResponseProto.result', index=0,
      number=1, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4616,
  serialized_end=4659,
)


_GETFILELINKINFOREQUESTPROTO = descriptor.Descriptor(
  name='GetFileLinkInfoRequestProto',
  full_name='hadoop.hdfs.GetFileLinkInfoRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.GetFileLinkInfoRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4661,
  serialized_end=4703,
)


_GETFILELINKINFORESPONSEPROTO = descriptor.Descriptor(
  name='GetFileLinkInfoResponseProto',
  full_name='hadoop.hdfs.GetFileLinkInfoResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='fs', full_name='hadoop.hdfs.GetFileLinkInfoResponseProto.fs', index=0,
      number=1, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4705,
  serialized_end=4781,
)


_GETCONTENTSUMMARYREQUESTPROTO = descriptor.Descriptor(
  name='GetContentSummaryRequestProto',
  full_name='hadoop.hdfs.GetContentSummaryRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='path', full_name='hadoop.hdfs.GetContentSummaryRequestProto.path', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4783,
  serialized_end=4828,
)


_GETCONTENTSUMMARYRESPONSEPROTO = descriptor.Descriptor(
  name='GetContentSummaryResponseProto',
  full_name='hadoop.hdfs.GetContentSummaryResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='summary', full_name='hadoop.hdfs.GetContentSummaryResponseProto.summary', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4830,
  serialized_end=4913,
)


_SETQUOTAREQUESTPROTO = descriptor.Descriptor(
  name='SetQuotaRequestProto',
  full_name='hadoop.hdfs.SetQuotaRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='path', full_name='hadoop.hdfs.SetQuotaRequestProto.path', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='namespaceQuota', full_name='hadoop.hdfs.SetQuotaRequestProto.namespaceQuota', index=1,
      number=2, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='diskspaceQuota', full_name='hadoop.hdfs.SetQuotaRequestProto.diskspaceQuota', index=2,
      number=3, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4915,
  serialized_end=4999,
)


_SETQUOTARESPONSEPROTO = descriptor.Descriptor(
  name='SetQuotaResponseProto',
  full_name='hadoop.hdfs.SetQuotaResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=5001,
  serialized_end=5024,
)


_FSYNCREQUESTPROTO = descriptor.Descriptor(
  name='FsyncRequestProto',
  full_name='hadoop.hdfs.FsyncRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.FsyncRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='client', full_name='hadoop.hdfs.FsyncRequestProto.client', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='lastBlockLength', full_name='hadoop.hdfs.FsyncRequestProto.lastBlockLength', index=2,
      number=3, type=18, cpp_type=2, label=1,
      has_default_value=True, default_value=-1,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=5026,
  serialized_end=5103,
)


_FSYNCRESPONSEPROTO = descriptor.Descriptor(
  name='FsyncResponseProto',
  full_name='hadoop.hdfs.FsyncResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=5105,
  serialized_end=5125,
)


_SETTIMESREQUESTPROTO = descriptor.Descriptor(
  name='SetTimesRequestProto',
  full_name='hadoop.hdfs.SetTimesRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='src', full_name='hadoop.hdfs.SetTimesRequestProto.src', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='mtime', full_name='hadoop.hdfs.SetTimesRequestProto.mtime', index=1,
      number=2, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='atime', full_name='hadoop.hdfs.SetTimesRequestProto.atime', index=2,
      number=3, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=5127,
  serialized_end=5192,
)


_SETTIMESRESPONSEPROTO = descriptor.Descriptor(
  name='SetTimesResponseProto',
  full_name='hadoop.hdfs.SetTimesResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=5194,
  serialized_end=5217,
)


_CREATESYMLINKREQUESTPROTO = descriptor.Descriptor(
  name='CreateSymlinkRequestProto',
  full_name='hadoop.hdfs.CreateSymlinkRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='target', full_name='hadoop.hdfs.CreateSymlinkRequestProto.target', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='link', full_name='hadoop.hdfs.CreateSymlinkRequestProto.link', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='dirPerm', full_name='hadoop.hdfs.CreateSymlinkRequestProto.dirPerm', index=2,
      number=3, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='createParent', full_name='hadoop.hdfs.CreateSymlinkRequestProto.createParent', index=3,
      number=4, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=5220,
  serialized_end=5348,
)


_CREATESYMLINKRESPONSEPROTO = descriptor.Descriptor(
  name='CreateSymlinkResponseProto',
  full_name='hadoop.hdfs.CreateSymlinkResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=5350,
  serialized_end=5378,
)


_GETLINKTARGETREQUESTPROTO = descriptor.Descriptor(
  name='GetLinkTargetRequestProto',
  full_name='hadoop.hdfs.GetLinkTargetRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='path', full_name='hadoop.hdfs.GetLinkTargetRequestProto.path', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=5380,
  serialized_end=5421,
)


_GETLINKTARGETRESPONSEPROTO = descriptor.Descriptor(
  name='GetLinkTargetResponseProto',
  full_name='hadoop.hdfs.GetLinkTargetResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='targetPath', full_name='hadoop.hdfs.GetLinkTargetResponseProto.targetPath', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=5423,
  serialized_end=5471,
)


_UPDATEBLOCKFORPIPELINEREQUESTPROTO = descriptor.Descriptor(
  name='UpdateBlockForPipelineRequestProto',
  full_name='hadoop.hdfs.UpdateBlockForPipelineRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='block', full_name='hadoop.hdfs.UpdateBlockForPipelineRequestProto.block', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='clientName', full_name='hadoop.hdfs.UpdateBlockForPipelineRequestProto.clientName', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=5473,
  serialized_end=5577,
)


_UPDATEBLOCKFORPIPELINERESPONSEPROTO = descriptor.Descriptor(
  name='UpdateBlockForPipelineResponseProto',
  full_name='hadoop.hdfs.UpdateBlockForPipelineResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='block', full_name='hadoop.hdfs.UpdateBlockForPipelineResponseProto.block', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=5579,
  serialized_end=5663,
)


_UPDATEPIPELINEREQUESTPROTO = descriptor.Descriptor(
  name='UpdatePipelineRequestProto',
  full_name='hadoop.hdfs.UpdatePipelineRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='clientName', full_name='hadoop.hdfs.UpdatePipelineRequestProto.clientName', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='oldBlock', full_name='hadoop.hdfs.UpdatePipelineRequestProto.oldBlock', index=1,
      number=2, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='newBlock', full_name='hadoop.hdfs.UpdatePipelineRequestProto.newBlock', index=2,
      number=3, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='newNodes', full_name='hadoop.hdfs.UpdatePipelineRequestProto.newNodes', index=3,
      number=4, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=5666,
  serialized_end=5864,
)


_UPDATEPIPELINERESPONSEPROTO = descriptor.Descriptor(
  name='UpdatePipelineResponseProto',
  full_name='hadoop.hdfs.UpdatePipelineResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=5866,
  serialized_end=5895,
)


_SETBALANCERBANDWIDTHREQUESTPROTO = descriptor.Descriptor(
  name='SetBalancerBandwidthRequestProto',
  full_name='hadoop.hdfs.SetBalancerBandwidthRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='bandwidth', full_name='hadoop.hdfs.SetBalancerBandwidthRequestProto.bandwidth', index=0,
      number=1, type=3, cpp_type=2, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=5897,
  serialized_end=5950,
)


_SETBALANCERBANDWIDTHRESPONSEPROTO = descriptor.Descriptor(
  name='SetBalancerBandwidthResponseProto',
  full_name='hadoop.hdfs.SetBalancerBandwidthResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=5952,
  serialized_end=5987,
)


_GETDATAENCRYPTIONKEYREQUESTPROTO = descriptor.Descriptor(
  name='GetDataEncryptionKeyRequestProto',
  full_name='hadoop.hdfs.GetDataEncryptionKeyRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=5989,
  serialized_end=6023,
)


_GETDATAENCRYPTIONKEYRESPONSEPROTO = descriptor.Descriptor(
  name='GetDataEncryptionKeyResponseProto',
  full_name='hadoop.hdfs.GetDataEncryptionKeyResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='dataEncryptionKey', full_name='hadoop.hdfs.GetDataEncryptionKeyResponseProto.dataEncryptionKey', index=0,
      number=1, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=6025,
  serialized_end=6124,
)


_CREATESNAPSHOTREQUESTPROTO = descriptor.Descriptor(
  name='CreateSnapshotRequestProto',
  full_name='hadoop.hdfs.CreateSnapshotRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='snapshotRoot', full_name='hadoop.hdfs.CreateSnapshotRequestProto.snapshotRoot', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='snapshotName', full_name='hadoop.hdfs.CreateSnapshotRequestProto.snapshotName', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=6126,
  serialized_end=6198,
)


_CREATESNAPSHOTRESPONSEPROTO = descriptor.Descriptor(
  name='CreateSnapshotResponseProto',
  full_name='hadoop.hdfs.CreateSnapshotResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='snapshotPath', full_name='hadoop.hdfs.CreateSnapshotResponseProto.snapshotPath', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=6200,
  serialized_end=6251,
)


_RENAMESNAPSHOTREQUESTPROTO = descriptor.Descriptor(
  name='RenameSnapshotRequestProto',
  full_name='hadoop.hdfs.RenameSnapshotRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='snapshotRoot', full_name='hadoop.hdfs.RenameSnapshotRequestProto.snapshotRoot', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='snapshotOldName', full_name='hadoop.hdfs.RenameSnapshotRequestProto.snapshotOldName', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='snapshotNewName', full_name='hadoop.hdfs.RenameSnapshotRequestProto.snapshotNewName', index=2,
      number=3, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=6253,
  serialized_end=6353,
)


_RENAMESNAPSHOTRESPONSEPROTO = descriptor.Descriptor(
  name='RenameSnapshotResponseProto',
  full_name='hadoop.hdfs.RenameSnapshotResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=6355,
  serialized_end=6384,
)


_ALLOWSNAPSHOTREQUESTPROTO = descriptor.Descriptor(
  name='AllowSnapshotRequestProto',
  full_name='hadoop.hdfs.AllowSnapshotRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='snapshotRoot', full_name='hadoop.hdfs.AllowSnapshotRequestProto.snapshotRoot', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=6386,
  serialized_end=6435,
)


_ALLOWSNAPSHOTRESPONSEPROTO = descriptor.Descriptor(
  name='AllowSnapshotResponseProto',
  full_name='hadoop.hdfs.AllowSnapshotResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=6437,
  serialized_end=6465,
)


_DISALLOWSNAPSHOTREQUESTPROTO = descriptor.Descriptor(
  name='DisallowSnapshotRequestProto',
  full_name='hadoop.hdfs.DisallowSnapshotRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='snapshotRoot', full_name='hadoop.hdfs.DisallowSnapshotRequestProto.snapshotRoot', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=6467,
  serialized_end=6519,
)


_DISALLOWSNAPSHOTRESPONSEPROTO = descriptor.Descriptor(
  name='DisallowSnapshotResponseProto',
  full_name='hadoop.hdfs.DisallowSnapshotResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=6521,
  serialized_end=6552,
)


_DELETESNAPSHOTREQUESTPROTO = descriptor.Descriptor(
  name='DeleteSnapshotRequestProto',
  full_name='hadoop.hdfs.DeleteSnapshotRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='snapshotRoot', full_name='hadoop.hdfs.DeleteSnapshotRequestProto.snapshotRoot', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='snapshotName', full_name='hadoop.hdfs.DeleteSnapshotRequestProto.snapshotName', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=6554,
  serialized_end=6626,
)


_DELETESNAPSHOTRESPONSEPROTO = descriptor.Descriptor(
  name='DeleteSnapshotResponseProto',
  full_name='hadoop.hdfs.DeleteSnapshotResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=6628,
  serialized_end=6657,
)

_GETBLOCKLOCATIONSRESPONSEPROTO.fields_by_name['locations'].message_type = hdfs_pb2._LOCATEDBLOCKSPROTO
_GETSERVERDEFAULTSRESPONSEPROTO.fields_by_name['serverDefaults'].message_type = hdfs_pb2._FSSERVERDEFAULTSPROTO
_CREATEREQUESTPROTO.fields_by_name['masked'].message_type = hdfs_pb2._FSPERMISSIONPROTO
_CREATERESPONSEPROTO.fields_by_name['fs'].message_type = hdfs_pb2._HDFSFILESTATUSPROTO
_APPENDRESPONSEPROTO.fields_by_name['block'].message_type = hdfs_pb2._LOCATEDBLOCKPROTO
_SETPERMISSIONREQUESTPROTO.fields_by_name['permission'].message_type = hdfs_pb2._FSPERMISSIONPROTO
_ABANDONBLOCKREQUESTPROTO.fields_by_name['b'].message_type = hdfs_pb2._EXTENDEDBLOCKPROTO
_ADDBLOCKREQUESTPROTO.fields_by_name['previous'].message_type = hdfs_pb2._EXTENDEDBLOCKPROTO
_ADDBLOCKREQUESTPROTO.fields_by_name['excludeNodes'].message_type = hdfs_pb2._DATANODEINFOPROTO
_ADDBLOCKRESPONSEPROTO.fields_by_name['block'].message_type = hdfs_pb2._LOCATEDBLOCKPROTO
_GETADDITIONALDATANODEREQUESTPROTO.fields_by_name['blk'].message_type = hdfs_pb2._EXTENDEDBLOCKPROTO
_GETADDITIONALDATANODEREQUESTPROTO.fields_by_name['existings'].message_type = hdfs_pb2._DATANODEINFOPROTO
_GETADDITIONALDATANODEREQUESTPROTO.fields_by_name['excludes'].message_type = hdfs_pb2._DATANODEINFOPROTO
_GETADDITIONALDATANODERESPONSEPROTO.fields_by_name['block'].message_type = hdfs_pb2._LOCATEDBLOCKPROTO
_COMPLETEREQUESTPROTO.fields_by_name['last'].message_type = hdfs_pb2._EXTENDEDBLOCKPROTO
_REPORTBADBLOCKSREQUESTPROTO.fields_by_name['blocks'].message_type = hdfs_pb2._LOCATEDBLOCKPROTO
_MKDIRSREQUESTPROTO.fields_by_name['masked'].message_type = hdfs_pb2._FSPERMISSIONPROTO
_GETLISTINGRESPONSEPROTO.fields_by_name['dirList'].message_type = hdfs_pb2._DIRECTORYLISTINGPROTO
_GETSNAPSHOTTABLEDIRLISTINGRESPONSEPROTO.fields_by_name['snapshottableDirList'].message_type = hdfs_pb2._SNAPSHOTTABLEDIRECTORYLISTINGPROTO
_GETSNAPSHOTDIFFREPORTRESPONSEPROTO.fields_by_name['diffReport'].message_type = hdfs_pb2._SNAPSHOTDIFFREPORTPROTO
_GETDATANODEREPORTREQUESTPROTO.fields_by_name['type'].enum_type = _DATANODEREPORTTYPEPROTO
_GETDATANODEREPORTRESPONSEPROTO.fields_by_name['di'].message_type = hdfs_pb2._DATANODEINFOPROTO
_SETSAFEMODEREQUESTPROTO.fields_by_name['action'].enum_type = _SAFEMODEACTIONPROTO
_LISTCORRUPTFILEBLOCKSRESPONSEPROTO.fields_by_name['corrupt'].message_type = hdfs_pb2._CORRUPTFILEBLOCKSPROTO
_GETFILEINFORESPONSEPROTO.fields_by_name['fs'].message_type = hdfs_pb2._HDFSFILESTATUSPROTO
_GETFILELINKINFORESPONSEPROTO.fields_by_name['fs'].message_type = hdfs_pb2._HDFSFILESTATUSPROTO
_GETCONTENTSUMMARYRESPONSEPROTO.fields_by_name['summary'].message_type = hdfs_pb2._CONTENTSUMMARYPROTO
_CREATESYMLINKREQUESTPROTO.fields_by_name['dirPerm'].message_type = hdfs_pb2._FSPERMISSIONPROTO
_UPDATEBLOCKFORPIPELINEREQUESTPROTO.fields_by_name['block'].message_type = hdfs_pb2._EXTENDEDBLOCKPROTO
_UPDATEBLOCKFORPIPELINERESPONSEPROTO.fields_by_name['block'].message_type = hdfs_pb2._LOCATEDBLOCKPROTO
_UPDATEPIPELINEREQUESTPROTO.fields_by_name['oldBlock'].message_type = hdfs_pb2._EXTENDEDBLOCKPROTO
_UPDATEPIPELINEREQUESTPROTO.fields_by_name['newBlock'].message_type = hdfs_pb2._EXTENDEDBLOCKPROTO
_UPDATEPIPELINEREQUESTPROTO.fields_by_name['newNodes'].message_type = hdfs_pb2._DATANODEIDPROTO
_GETDATAENCRYPTIONKEYRESPONSEPROTO.fields_by_name['dataEncryptionKey'].message_type = hdfs_pb2._DATAENCRYPTIONKEYPROTO
DESCRIPTOR.message_types_by_name['GetBlockLocationsRequestProto'] = _GETBLOCKLOCATIONSREQUESTPROTO
DESCRIPTOR.message_types_by_name['GetBlockLocationsResponseProto'] = _GETBLOCKLOCATIONSRESPONSEPROTO
DESCRIPTOR.message_types_by_name['GetServerDefaultsRequestProto'] = _GETSERVERDEFAULTSREQUESTPROTO
DESCRIPTOR.message_types_by_name['GetServerDefaultsResponseProto'] = _GETSERVERDEFAULTSRESPONSEPROTO
DESCRIPTOR.message_types_by_name['CreateRequestProto'] = _CREATEREQUESTPROTO
DESCRIPTOR.message_types_by_name['CreateResponseProto'] = _CREATERESPONSEPROTO
DESCRIPTOR.message_types_by_name['AppendRequestProto'] = _APPENDREQUESTPROTO
DESCRIPTOR.message_types_by_name['AppendResponseProto'] = _APPENDRESPONSEPROTO
DESCRIPTOR.message_types_by_name['SetReplicationRequestProto'] = _SETREPLICATIONREQUESTPROTO
DESCRIPTOR.message_types_by_name['SetReplicationResponseProto'] = _SETREPLICATIONRESPONSEPROTO
DESCRIPTOR.message_types_by_name['SetPermissionRequestProto'] = _SETPERMISSIONREQUESTPROTO
DESCRIPTOR.message_types_by_name['SetPermissionResponseProto'] = _SETPERMISSIONRESPONSEPROTO
DESCRIPTOR.message_types_by_name['SetOwnerRequestProto'] = _SETOWNERREQUESTPROTO
DESCRIPTOR.message_types_by_name['SetOwnerResponseProto'] = _SETOWNERRESPONSEPROTO
DESCRIPTOR.message_types_by_name['AbandonBlockRequestProto'] = _ABANDONBLOCKREQUESTPROTO
DESCRIPTOR.message_types_by_name['AbandonBlockResponseProto'] = _ABANDONBLOCKRESPONSEPROTO
DESCRIPTOR.message_types_by_name['AddBlockRequestProto'] = _ADDBLOCKREQUESTPROTO
DESCRIPTOR.message_types_by_name['AddBlockResponseProto'] = _ADDBLOCKRESPONSEPROTO
DESCRIPTOR.message_types_by_name['GetAdditionalDatanodeRequestProto'] = _GETADDITIONALDATANODEREQUESTPROTO
DESCRIPTOR.message_types_by_name['GetAdditionalDatanodeResponseProto'] = _GETADDITIONALDATANODERESPONSEPROTO
DESCRIPTOR.message_types_by_name['CompleteRequestProto'] = _COMPLETEREQUESTPROTO
DESCRIPTOR.message_types_by_name['CompleteResponseProto'] = _COMPLETERESPONSEPROTO
DESCRIPTOR.message_types_by_name['ReportBadBlocksRequestProto'] = _REPORTBADBLOCKSREQUESTPROTO
DESCRIPTOR.message_types_by_name['ReportBadBlocksResponseProto'] = _REPORTBADBLOCKSRESPONSEPROTO
DESCRIPTOR.message_types_by_name['ConcatRequestProto'] = _CONCATREQUESTPROTO
DESCRIPTOR.message_types_by_name['ConcatResponseProto'] = _CONCATRESPONSEPROTO
DESCRIPTOR.message_types_by_name['RenameRequestProto'] = _RENAMEREQUESTPROTO
DESCRIPTOR.message_types_by_name['RenameResponseProto'] = _RENAMERESPONSEPROTO
DESCRIPTOR.message_types_by_name['Rename2RequestProto'] = _RENAME2REQUESTPROTO
DESCRIPTOR.message_types_by_name['Rename2ResponseProto'] = _RENAME2RESPONSEPROTO
DESCRIPTOR.message_types_by_name['DeleteRequestProto'] = _DELETEREQUESTPROTO
DESCRIPTOR.message_types_by_name['DeleteResponseProto'] = _DELETERESPONSEPROTO
DESCRIPTOR.message_types_by_name['MkdirsRequestProto'] = _MKDIRSREQUESTPROTO
DESCRIPTOR.message_types_by_name['MkdirsResponseProto'] = _MKDIRSRESPONSEPROTO
DESCRIPTOR.message_types_by_name['GetListingRequestProto'] = _GETLISTINGREQUESTPROTO
DESCRIPTOR.message_types_by_name['GetListingResponseProto'] = _GETLISTINGRESPONSEPROTO
DESCRIPTOR.message_types_by_name['GetSnapshottableDirListingRequestProto'] = _GETSNAPSHOTTABLEDIRLISTINGREQUESTPROTO
DESCRIPTOR.message_types_by_name['GetSnapshottableDirListingResponseProto'] = _GETSNAPSHOTTABLEDIRLISTINGRESPONSEPROTO
DESCRIPTOR.message_types_by_name['GetSnapshotDiffReportRequestProto'] = _GETSNAPSHOTDIFFREPORTREQUESTPROTO
DESCRIPTOR.message_types_by_name['GetSnapshotDiffReportResponseProto'] = _GETSNAPSHOTDIFFREPORTRESPONSEPROTO
DESCRIPTOR.message_types_by_name['RenewLeaseRequestProto'] = _RENEWLEASEREQUESTPROTO
DESCRIPTOR.message_types_by_name['RenewLeaseResponseProto'] = _RENEWLEASERESPONSEPROTO
DESCRIPTOR.message_types_by_name['RecoverLeaseRequestProto'] = _RECOVERLEASEREQUESTPROTO
DESCRIPTOR.message_types_by_name['RecoverLeaseResponseProto'] = _RECOVERLEASERESPONSEPROTO
DESCRIPTOR.message_types_by_name['GetFsStatusRequestProto'] = _GETFSSTATUSREQUESTPROTO
DESCRIPTOR.message_types_by_name['GetFsStatsResponseProto'] = _GETFSSTATSRESPONSEPROTO
DESCRIPTOR.message_types_by_name['GetDatanodeReportRequestProto'] = _GETDATANODEREPORTREQUESTPROTO
DESCRIPTOR.message_types_by_name['GetDatanodeReportResponseProto'] = _GETDATANODEREPORTRESPONSEPROTO
DESCRIPTOR.message_types_by_name['GetPreferredBlockSizeRequestProto'] = _GETPREFERREDBLOCKSIZEREQUESTPROTO
DESCRIPTOR.message_types_by_name['GetPreferredBlockSizeResponseProto'] = _GETPREFERREDBLOCKSIZERESPONSEPROTO
DESCRIPTOR.message_types_by_name['SetSafeModeRequestProto'] = _SETSAFEMODEREQUESTPROTO
DESCRIPTOR.message_types_by_name['SetSafeModeResponseProto'] = _SETSAFEMODERESPONSEPROTO
DESCRIPTOR.message_types_by_name['SaveNamespaceRequestProto'] = _SAVENAMESPACEREQUESTPROTO
DESCRIPTOR.message_types_by_name['SaveNamespaceResponseProto'] = _SAVENAMESPACERESPONSEPROTO
DESCRIPTOR.message_types_by_name['RollEditsRequestProto'] = _ROLLEDITSREQUESTPROTO
DESCRIPTOR.message_types_by_name['RollEditsResponseProto'] = _ROLLEDITSRESPONSEPROTO
DESCRIPTOR.message_types_by_name['RestoreFailedStorageRequestProto'] = _RESTOREFAILEDSTORAGEREQUESTPROTO
DESCRIPTOR.message_types_by_name['RestoreFailedStorageResponseProto'] = _RESTOREFAILEDSTORAGERESPONSEPROTO
DESCRIPTOR.message_types_by_name['RefreshNodesRequestProto'] = _REFRESHNODESREQUESTPROTO
DESCRIPTOR.message_types_by_name['RefreshNodesResponseProto'] = _REFRESHNODESRESPONSEPROTO
DESCRIPTOR.message_types_by_name['FinalizeUpgradeRequestProto'] = _FINALIZEUPGRADEREQUESTPROTO
DESCRIPTOR.message_types_by_name['FinalizeUpgradeResponseProto'] = _FINALIZEUPGRADERESPONSEPROTO
DESCRIPTOR.message_types_by_name['ListCorruptFileBlocksRequestProto'] = _LISTCORRUPTFILEBLOCKSREQUESTPROTO
DESCRIPTOR.message_types_by_name['ListCorruptFileBlocksResponseProto'] = _LISTCORRUPTFILEBLOCKSRESPONSEPROTO
DESCRIPTOR.message_types_by_name['MetaSaveRequestProto'] = _METASAVEREQUESTPROTO
DESCRIPTOR.message_types_by_name['MetaSaveResponseProto'] = _METASAVERESPONSEPROTO
DESCRIPTOR.message_types_by_name['GetFileInfoRequestProto'] = _GETFILEINFOREQUESTPROTO
DESCRIPTOR.message_types_by_name['GetFileInfoResponseProto'] = _GETFILEINFORESPONSEPROTO
DESCRIPTOR.message_types_by_name['IsFileClosedRequestProto'] = _ISFILECLOSEDREQUESTPROTO
DESCRIPTOR.message_types_by_name['IsFileClosedResponseProto'] = _ISFILECLOSEDRESPONSEPROTO
DESCRIPTOR.message_types_by_name['GetFileLinkInfoRequestProto'] = _GETFILELINKINFOREQUESTPROTO
DESCRIPTOR.message_types_by_name['GetFileLinkInfoResponseProto'] = _GETFILELINKINFORESPONSEPROTO
DESCRIPTOR.message_types_by_name['GetContentSummaryRequestProto'] = _GETCONTENTSUMMARYREQUESTPROTO
DESCRIPTOR.message_types_by_name['GetContentSummaryResponseProto'] = _GETCONTENTSUMMARYRESPONSEPROTO
DESCRIPTOR.message_types_by_name['SetQuotaRequestProto'] = _SETQUOTAREQUESTPROTO
DESCRIPTOR.message_types_by_name['SetQuotaResponseProto'] = _SETQUOTARESPONSEPROTO
DESCRIPTOR.message_types_by_name['FsyncRequestProto'] = _FSYNCREQUESTPROTO
DESCRIPTOR.message_types_by_name['FsyncResponseProto'] = _FSYNCRESPONSEPROTO
DESCRIPTOR.message_types_by_name['SetTimesRequestProto'] = _SETTIMESREQUESTPROTO
DESCRIPTOR.message_types_by_name['SetTimesResponseProto'] = _SETTIMESRESPONSEPROTO
DESCRIPTOR.message_types_by_name['CreateSymlinkRequestProto'] = _CREATESYMLINKREQUESTPROTO
DESCRIPTOR.message_types_by_name['CreateSymlinkResponseProto'] = _CREATESYMLINKRESPONSEPROTO
DESCRIPTOR.message_types_by_name['GetLinkTargetRequestProto'] = _GETLINKTARGETREQUESTPROTO
DESCRIPTOR.message_types_by_name['GetLinkTargetResponseProto'] = _GETLINKTARGETRESPONSEPROTO
DESCRIPTOR.message_types_by_name['UpdateBlockForPipelineRequestProto'] = _UPDATEBLOCKFORPIPELINEREQUESTPROTO
DESCRIPTOR.message_types_by_name['UpdateBlockForPipelineResponseProto'] = _UPDATEBLOCKFORPIPELINERESPONSEPROTO
DESCRIPTOR.message_types_by_name['UpdatePipelineRequestProto'] = _UPDATEPIPELINEREQUESTPROTO
DESCRIPTOR.message_types_by_name['UpdatePipelineResponseProto'] = _UPDATEPIPELINERESPONSEPROTO
DESCRIPTOR.message_types_by_name['SetBalancerBandwidthRequestProto'] = _SETBALANCERBANDWIDTHREQUESTPROTO
DESCRIPTOR.message_types_by_name['SetBalancerBandwidthResponseProto'] = _SETBALANCERBANDWIDTHRESPONSEPROTO
DESCRIPTOR.message_types_by_name['GetDataEncryptionKeyRequestProto'] = _GETDATAENCRYPTIONKEYREQUESTPROTO
DESCRIPTOR.message_types_by_name['GetDataEncryptionKeyResponseProto'] = _GETDATAENCRYPTIONKEYRESPONSEPROTO
DESCRIPTOR.message_types_by_name['CreateSnapshotRequestProto'] = _CREATESNAPSHOTREQUESTPROTO
DESCRIPTOR.message_types_by_name['CreateSnapshotResponseProto'] = _CREATESNAPSHOTRESPONSEPROTO
DESCRIPTOR.message_types_by_name['RenameSnapshotRequestProto'] = _RENAMESNAPSHOTREQUESTPROTO
DESCRIPTOR.message_types_by_name['RenameSnapshotResponseProto'] = _RENAMESNAPSHOTRESPONSEPROTO
DESCRIPTOR.message_types_by_name['AllowSnapshotRequestProto'] = _ALLOWSNAPSHOTREQUESTPROTO
DESCRIPTOR.message_types_by_name['AllowSnapshotResponseProto'] = _ALLOWSNAPSHOTRESPONSEPROTO
DESCRIPTOR.message_types_by_name['DisallowSnapshotRequestProto'] = _DISALLOWSNAPSHOTREQUESTPROTO
DESCRIPTOR.message_types_by_name['DisallowSnapshotResponseProto'] = _DISALLOWSNAPSHOTRESPONSEPROTO
DESCRIPTOR.message_types_by_name['DeleteSnapshotRequestProto'] = _DELETESNAPSHOTREQUESTPROTO
DESCRIPTOR.message_types_by_name['DeleteSnapshotResponseProto'] = _DELETESNAPSHOTRESPONSEPROTO

class GetBlockLocationsRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETBLOCKLOCATIONSREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetBlockLocationsRequestProto)

class GetBlockLocationsResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETBLOCKLOCATIONSRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetBlockLocationsResponseProto)

class GetServerDefaultsRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETSERVERDEFAULTSREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetServerDefaultsRequestProto)

class GetServerDefaultsResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETSERVERDEFAULTSRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetServerDefaultsResponseProto)

class CreateRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _CREATEREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.CreateRequestProto)

class CreateResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _CREATERESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.CreateResponseProto)

class AppendRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _APPENDREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.AppendRequestProto)

class AppendResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _APPENDRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.AppendResponseProto)

class SetReplicationRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SETREPLICATIONREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SetReplicationRequestProto)

class SetReplicationResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SETREPLICATIONRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SetReplicationResponseProto)

class SetPermissionRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SETPERMISSIONREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SetPermissionRequestProto)

class SetPermissionResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SETPERMISSIONRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SetPermissionResponseProto)

class SetOwnerRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SETOWNERREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SetOwnerRequestProto)

class SetOwnerResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SETOWNERRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SetOwnerResponseProto)

class AbandonBlockRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _ABANDONBLOCKREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.AbandonBlockRequestProto)

class AbandonBlockResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _ABANDONBLOCKRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.AbandonBlockResponseProto)

class AddBlockRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _ADDBLOCKREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.AddBlockRequestProto)

class AddBlockResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _ADDBLOCKRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.AddBlockResponseProto)

class GetAdditionalDatanodeRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETADDITIONALDATANODEREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetAdditionalDatanodeRequestProto)

class GetAdditionalDatanodeResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETADDITIONALDATANODERESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetAdditionalDatanodeResponseProto)

class CompleteRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _COMPLETEREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.CompleteRequestProto)

class CompleteResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _COMPLETERESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.CompleteResponseProto)

class ReportBadBlocksRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _REPORTBADBLOCKSREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.ReportBadBlocksRequestProto)

class ReportBadBlocksResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _REPORTBADBLOCKSRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.ReportBadBlocksResponseProto)

class ConcatRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _CONCATREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.ConcatRequestProto)

class ConcatResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _CONCATRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.ConcatResponseProto)

class RenameRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _RENAMEREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.RenameRequestProto)

class RenameResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _RENAMERESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.RenameResponseProto)

class Rename2RequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _RENAME2REQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.Rename2RequestProto)

class Rename2ResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _RENAME2RESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.Rename2ResponseProto)

class DeleteRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _DELETEREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.DeleteRequestProto)

class DeleteResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _DELETERESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.DeleteResponseProto)

class MkdirsRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _MKDIRSREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.MkdirsRequestProto)

class MkdirsResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _MKDIRSRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.MkdirsResponseProto)

class GetListingRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETLISTINGREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetListingRequestProto)

class GetListingResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETLISTINGRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetListingResponseProto)

class GetSnapshottableDirListingRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETSNAPSHOTTABLEDIRLISTINGREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetSnapshottableDirListingRequestProto)

class GetSnapshottableDirListingResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETSNAPSHOTTABLEDIRLISTINGRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetSnapshottableDirListingResponseProto)

class GetSnapshotDiffReportRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETSNAPSHOTDIFFREPORTREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetSnapshotDiffReportRequestProto)

class GetSnapshotDiffReportResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETSNAPSHOTDIFFREPORTRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetSnapshotDiffReportResponseProto)

class RenewLeaseRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _RENEWLEASEREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.RenewLeaseRequestProto)

class RenewLeaseResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _RENEWLEASERESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.RenewLeaseResponseProto)

class RecoverLeaseRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _RECOVERLEASEREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.RecoverLeaseRequestProto)

class RecoverLeaseResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _RECOVERLEASERESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.RecoverLeaseResponseProto)

class GetFsStatusRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETFSSTATUSREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetFsStatusRequestProto)

class GetFsStatsResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETFSSTATSRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetFsStatsResponseProto)

class GetDatanodeReportRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETDATANODEREPORTREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetDatanodeReportRequestProto)

class GetDatanodeReportResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETDATANODEREPORTRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetDatanodeReportResponseProto)

class GetPreferredBlockSizeRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETPREFERREDBLOCKSIZEREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetPreferredBlockSizeRequestProto)

class GetPreferredBlockSizeResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETPREFERREDBLOCKSIZERESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetPreferredBlockSizeResponseProto)

class SetSafeModeRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SETSAFEMODEREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SetSafeModeRequestProto)

class SetSafeModeResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SETSAFEMODERESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SetSafeModeResponseProto)

class SaveNamespaceRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SAVENAMESPACEREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SaveNamespaceRequestProto)

class SaveNamespaceResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SAVENAMESPACERESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SaveNamespaceResponseProto)

class RollEditsRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _ROLLEDITSREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.RollEditsRequestProto)

class RollEditsResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _ROLLEDITSRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.RollEditsResponseProto)

class RestoreFailedStorageRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _RESTOREFAILEDSTORAGEREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.RestoreFailedStorageRequestProto)

class RestoreFailedStorageResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _RESTOREFAILEDSTORAGERESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.RestoreFailedStorageResponseProto)

class RefreshNodesRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _REFRESHNODESREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.RefreshNodesRequestProto)

class RefreshNodesResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _REFRESHNODESRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.RefreshNodesResponseProto)

class FinalizeUpgradeRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _FINALIZEUPGRADEREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.FinalizeUpgradeRequestProto)

class FinalizeUpgradeResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _FINALIZEUPGRADERESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.FinalizeUpgradeResponseProto)

class ListCorruptFileBlocksRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _LISTCORRUPTFILEBLOCKSREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.ListCorruptFileBlocksRequestProto)

class ListCorruptFileBlocksResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _LISTCORRUPTFILEBLOCKSRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.ListCorruptFileBlocksResponseProto)

class MetaSaveRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _METASAVEREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.MetaSaveRequestProto)

class MetaSaveResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _METASAVERESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.MetaSaveResponseProto)

class GetFileInfoRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETFILEINFOREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetFileInfoRequestProto)

class GetFileInfoResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETFILEINFORESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetFileInfoResponseProto)

class IsFileClosedRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _ISFILECLOSEDREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.IsFileClosedRequestProto)

class IsFileClosedResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _ISFILECLOSEDRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.IsFileClosedResponseProto)

class GetFileLinkInfoRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETFILELINKINFOREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetFileLinkInfoRequestProto)

class GetFileLinkInfoResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETFILELINKINFORESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetFileLinkInfoResponseProto)

class GetContentSummaryRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETCONTENTSUMMARYREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetContentSummaryRequestProto)

class GetContentSummaryResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETCONTENTSUMMARYRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetContentSummaryResponseProto)

class SetQuotaRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SETQUOTAREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SetQuotaRequestProto)

class SetQuotaResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SETQUOTARESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SetQuotaResponseProto)

class FsyncRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _FSYNCREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.FsyncRequestProto)

class FsyncResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _FSYNCRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.FsyncResponseProto)

class SetTimesRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SETTIMESREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SetTimesRequestProto)

class SetTimesResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SETTIMESRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SetTimesResponseProto)

class CreateSymlinkRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _CREATESYMLINKREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.CreateSymlinkRequestProto)

class CreateSymlinkResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _CREATESYMLINKRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.CreateSymlinkResponseProto)

class GetLinkTargetRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETLINKTARGETREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetLinkTargetRequestProto)

class GetLinkTargetResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETLINKTARGETRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetLinkTargetResponseProto)

class UpdateBlockForPipelineRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _UPDATEBLOCKFORPIPELINEREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.UpdateBlockForPipelineRequestProto)

class UpdateBlockForPipelineResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _UPDATEBLOCKFORPIPELINERESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.UpdateBlockForPipelineResponseProto)

class UpdatePipelineRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _UPDATEPIPELINEREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.UpdatePipelineRequestProto)

class UpdatePipelineResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _UPDATEPIPELINERESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.UpdatePipelineResponseProto)

class SetBalancerBandwidthRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SETBALANCERBANDWIDTHREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SetBalancerBandwidthRequestProto)

class SetBalancerBandwidthResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SETBALANCERBANDWIDTHRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SetBalancerBandwidthResponseProto)

class GetDataEncryptionKeyRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETDATAENCRYPTIONKEYREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetDataEncryptionKeyRequestProto)

class GetDataEncryptionKeyResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETDATAENCRYPTIONKEYRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.GetDataEncryptionKeyResponseProto)

class CreateSnapshotRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _CREATESNAPSHOTREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.CreateSnapshotRequestProto)

class CreateSnapshotResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _CREATESNAPSHOTRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.CreateSnapshotResponseProto)

class RenameSnapshotRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _RENAMESNAPSHOTREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.RenameSnapshotRequestProto)

class RenameSnapshotResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _RENAMESNAPSHOTRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.RenameSnapshotResponseProto)

class AllowSnapshotRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _ALLOWSNAPSHOTREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.AllowSnapshotRequestProto)

class AllowSnapshotResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _ALLOWSNAPSHOTRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.AllowSnapshotResponseProto)

class DisallowSnapshotRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _DISALLOWSNAPSHOTREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.DisallowSnapshotRequestProto)

class DisallowSnapshotResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _DISALLOWSNAPSHOTRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.DisallowSnapshotResponseProto)

class DeleteSnapshotRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _DELETESNAPSHOTREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.DeleteSnapshotRequestProto)

class DeleteSnapshotResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _DELETESNAPSHOTRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.DeleteSnapshotResponseProto)


_CLIENTNAMENODEPROTOCOL = descriptor.ServiceDescriptor(
  name='ClientNamenodeProtocol',
  full_name='hadoop.hdfs.ClientNamenodeProtocol',
  file=DESCRIPTOR,
  index=0,
  options=None,
  serialized_start=6855,
  serialized_end=12239,
  methods=[
  descriptor.MethodDescriptor(
    name='getBlockLocations',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.getBlockLocations',
    index=0,
    containing_service=None,
    input_type=_GETBLOCKLOCATIONSREQUESTPROTO,
    output_type=_GETBLOCKLOCATIONSRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='getServerDefaults',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.getServerDefaults',
    index=1,
    containing_service=None,
    input_type=_GETSERVERDEFAULTSREQUESTPROTO,
    output_type=_GETSERVERDEFAULTSRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='create',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.create',
    index=2,
    containing_service=None,
    input_type=_CREATEREQUESTPROTO,
    output_type=_CREATERESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='append',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.append',
    index=3,
    containing_service=None,
    input_type=_APPENDREQUESTPROTO,
    output_type=_APPENDRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='setReplication',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.setReplication',
    index=4,
    containing_service=None,
    input_type=_SETREPLICATIONREQUESTPROTO,
    output_type=_SETREPLICATIONRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='setPermission',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.setPermission',
    index=5,
    containing_service=None,
    input_type=_SETPERMISSIONREQUESTPROTO,
    output_type=_SETPERMISSIONRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='setOwner',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.setOwner',
    index=6,
    containing_service=None,
    input_type=_SETOWNERREQUESTPROTO,
    output_type=_SETOWNERRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='abandonBlock',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.abandonBlock',
    index=7,
    containing_service=None,
    input_type=_ABANDONBLOCKREQUESTPROTO,
    output_type=_ABANDONBLOCKRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='addBlock',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.addBlock',
    index=8,
    containing_service=None,
    input_type=_ADDBLOCKREQUESTPROTO,
    output_type=_ADDBLOCKRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='getAdditionalDatanode',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.getAdditionalDatanode',
    index=9,
    containing_service=None,
    input_type=_GETADDITIONALDATANODEREQUESTPROTO,
    output_type=_GETADDITIONALDATANODERESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='complete',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.complete',
    index=10,
    containing_service=None,
    input_type=_COMPLETEREQUESTPROTO,
    output_type=_COMPLETERESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='reportBadBlocks',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.reportBadBlocks',
    index=11,
    containing_service=None,
    input_type=_REPORTBADBLOCKSREQUESTPROTO,
    output_type=_REPORTBADBLOCKSRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='concat',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.concat',
    index=12,
    containing_service=None,
    input_type=_CONCATREQUESTPROTO,
    output_type=_CONCATRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='rename',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.rename',
    index=13,
    containing_service=None,
    input_type=_RENAMEREQUESTPROTO,
    output_type=_RENAMERESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='rename2',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.rename2',
    index=14,
    containing_service=None,
    input_type=_RENAME2REQUESTPROTO,
    output_type=_RENAME2RESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='delete',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.delete',
    index=15,
    containing_service=None,
    input_type=_DELETEREQUESTPROTO,
    output_type=_DELETERESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='mkdirs',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.mkdirs',
    index=16,
    containing_service=None,
    input_type=_MKDIRSREQUESTPROTO,
    output_type=_MKDIRSRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='getListing',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.getListing',
    index=17,
    containing_service=None,
    input_type=_GETLISTINGREQUESTPROTO,
    output_type=_GETLISTINGRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='renewLease',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.renewLease',
    index=18,
    containing_service=None,
    input_type=_RENEWLEASEREQUESTPROTO,
    output_type=_RENEWLEASERESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='recoverLease',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.recoverLease',
    index=19,
    containing_service=None,
    input_type=_RECOVERLEASEREQUESTPROTO,
    output_type=_RECOVERLEASERESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='getFsStats',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.getFsStats',
    index=20,
    containing_service=None,
    input_type=_GETFSSTATUSREQUESTPROTO,
    output_type=_GETFSSTATSRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='getDatanodeReport',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.getDatanodeReport',
    index=21,
    containing_service=None,
    input_type=_GETDATANODEREPORTREQUESTPROTO,
    output_type=_GETDATANODEREPORTRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='getPreferredBlockSize',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.getPreferredBlockSize',
    index=22,
    containing_service=None,
    input_type=_GETPREFERREDBLOCKSIZEREQUESTPROTO,
    output_type=_GETPREFERREDBLOCKSIZERESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='setSafeMode',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.setSafeMode',
    index=23,
    containing_service=None,
    input_type=_SETSAFEMODEREQUESTPROTO,
    output_type=_SETSAFEMODERESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='saveNamespace',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.saveNamespace',
    index=24,
    containing_service=None,
    input_type=_SAVENAMESPACEREQUESTPROTO,
    output_type=_SAVENAMESPACERESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='rollEdits',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.rollEdits',
    index=25,
    containing_service=None,
    input_type=_ROLLEDITSREQUESTPROTO,
    output_type=_ROLLEDITSRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='restoreFailedStorage',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.restoreFailedStorage',
    index=26,
    containing_service=None,
    input_type=_RESTOREFAILEDSTORAGEREQUESTPROTO,
    output_type=_RESTOREFAILEDSTORAGERESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='refreshNodes',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.refreshNodes',
    index=27,
    containing_service=None,
    input_type=_REFRESHNODESREQUESTPROTO,
    output_type=_REFRESHNODESRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='finalizeUpgrade',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.finalizeUpgrade',
    index=28,
    containing_service=None,
    input_type=_FINALIZEUPGRADEREQUESTPROTO,
    output_type=_FINALIZEUPGRADERESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='listCorruptFileBlocks',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.listCorruptFileBlocks',
    index=29,
    containing_service=None,
    input_type=_LISTCORRUPTFILEBLOCKSREQUESTPROTO,
    output_type=_LISTCORRUPTFILEBLOCKSRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='metaSave',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.metaSave',
    index=30,
    containing_service=None,
    input_type=_METASAVEREQUESTPROTO,
    output_type=_METASAVERESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='getFileInfo',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.getFileInfo',
    index=31,
    containing_service=None,
    input_type=_GETFILEINFOREQUESTPROTO,
    output_type=_GETFILEINFORESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='getFileLinkInfo',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.getFileLinkInfo',
    index=32,
    containing_service=None,
    input_type=_GETFILELINKINFOREQUESTPROTO,
    output_type=_GETFILELINKINFORESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='getContentSummary',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.getContentSummary',
    index=33,
    containing_service=None,
    input_type=_GETCONTENTSUMMARYREQUESTPROTO,
    output_type=_GETCONTENTSUMMARYRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='setQuota',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.setQuota',
    index=34,
    containing_service=None,
    input_type=_SETQUOTAREQUESTPROTO,
    output_type=_SETQUOTARESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='fsync',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.fsync',
    index=35,
    containing_service=None,
    input_type=_FSYNCREQUESTPROTO,
    output_type=_FSYNCRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='setTimes',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.setTimes',
    index=36,
    containing_service=None,
    input_type=_SETTIMESREQUESTPROTO,
    output_type=_SETTIMESRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='createSymlink',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.createSymlink',
    index=37,
    containing_service=None,
    input_type=_CREATESYMLINKREQUESTPROTO,
    output_type=_CREATESYMLINKRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='getLinkTarget',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.getLinkTarget',
    index=38,
    containing_service=None,
    input_type=_GETLINKTARGETREQUESTPROTO,
    output_type=_GETLINKTARGETRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='updateBlockForPipeline',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.updateBlockForPipeline',
    index=39,
    containing_service=None,
    input_type=_UPDATEBLOCKFORPIPELINEREQUESTPROTO,
    output_type=_UPDATEBLOCKFORPIPELINERESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='updatePipeline',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.updatePipeline',
    index=40,
    containing_service=None,
    input_type=_UPDATEPIPELINEREQUESTPROTO,
    output_type=_UPDATEPIPELINERESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='getDelegationToken',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.getDelegationToken',
    index=41,
    containing_service=None,
    input_type=Security_pb2._GETDELEGATIONTOKENREQUESTPROTO,
    output_type=Security_pb2._GETDELEGATIONTOKENRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='renewDelegationToken',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.renewDelegationToken',
    index=42,
    containing_service=None,
    input_type=Security_pb2._RENEWDELEGATIONTOKENREQUESTPROTO,
    output_type=Security_pb2._RENEWDELEGATIONTOKENRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='cancelDelegationToken',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.cancelDelegationToken',
    index=43,
    containing_service=None,
    input_type=Security_pb2._CANCELDELEGATIONTOKENREQUESTPROTO,
    output_type=Security_pb2._CANCELDELEGATIONTOKENRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='setBalancerBandwidth',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.setBalancerBandwidth',
    index=44,
    containing_service=None,
    input_type=_SETBALANCERBANDWIDTHREQUESTPROTO,
    output_type=_SETBALANCERBANDWIDTHRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='getDataEncryptionKey',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.getDataEncryptionKey',
    index=45,
    containing_service=None,
    input_type=_GETDATAENCRYPTIONKEYREQUESTPROTO,
    output_type=_GETDATAENCRYPTIONKEYRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='createSnapshot',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.createSnapshot',
    index=46,
    containing_service=None,
    input_type=_CREATESNAPSHOTREQUESTPROTO,
    output_type=_CREATESNAPSHOTRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='renameSnapshot',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.renameSnapshot',
    index=47,
    containing_service=None,
    input_type=_RENAMESNAPSHOTREQUESTPROTO,
    output_type=_RENAMESNAPSHOTRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='allowSnapshot',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.allowSnapshot',
    index=48,
    containing_service=None,
    input_type=_ALLOWSNAPSHOTREQUESTPROTO,
    output_type=_ALLOWSNAPSHOTRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='disallowSnapshot',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.disallowSnapshot',
    index=49,
    containing_service=None,
    input_type=_DISALLOWSNAPSHOTREQUESTPROTO,
    output_type=_DISALLOWSNAPSHOTRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='getSnapshottableDirListing',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.getSnapshottableDirListing',
    index=50,
    containing_service=None,
    input_type=_GETSNAPSHOTTABLEDIRLISTINGREQUESTPROTO,
    output_type=_GETSNAPSHOTTABLEDIRLISTINGRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='deleteSnapshot',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.deleteSnapshot',
    index=51,
    containing_service=None,
    input_type=_DELETESNAPSHOTREQUESTPROTO,
    output_type=_DELETESNAPSHOTRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='getSnapshotDiffReport',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.getSnapshotDiffReport',
    index=52,
    containing_service=None,
    input_type=_GETSNAPSHOTDIFFREPORTREQUESTPROTO,
    output_type=_GETSNAPSHOTDIFFREPORTRESPONSEPROTO,
    options=None,
  ),
  descriptor.MethodDescriptor(
    name='isFileClosed',
    full_name='hadoop.hdfs.ClientNamenodeProtocol.isFileClosed',
    index=53,
    containing_service=None,
    input_type=_ISFILECLOSEDREQUESTPROTO,
    output_type=_ISFILECLOSEDRESPONSEPROTO,
    options=None,
  ),
])

class ClientNamenodeProtocol(service.Service):
  __metaclass__ = service_reflection.GeneratedServiceType
  DESCRIPTOR = _CLIENTNAMENODEPROTOCOL
class ClientNamenodeProtocol_Stub(ClientNamenodeProtocol):
  __metaclass__ = service_reflection.GeneratedServiceStubType
  DESCRIPTOR = _CLIENTNAMENODEPROTOCOL

# @@protoc_insertion_point(module_scope)

########NEW FILE########
__FILENAME__ = datatransfer_pb2
# Generated by the protocol buffer compiler.  DO NOT EDIT!

from google.protobuf import descriptor
from google.protobuf import message
from google.protobuf import reflection
from google.protobuf import descriptor_pb2
# @@protoc_insertion_point(imports)


import Security_pb2
import hdfs_pb2

DESCRIPTOR = descriptor.FileDescriptor(
  name='datatransfer.proto',
  package='hadoop.hdfs',
  serialized_pb='\n\x12\x64\x61tatransfer.proto\x12\x0bhadoop.hdfs\x1a\x0eSecurity.proto\x1a\nhdfs.proto\"\xef\x01\n!DataTransferEncryptorMessageProto\x12Z\n\x06status\x18\x01 \x02(\x0e\x32J.hadoop.hdfs.DataTransferEncryptorMessageProto.DataTransferEncryptorStatus\x12\x0f\n\x07payload\x18\x02 \x01(\x0c\x12\x0f\n\x07message\x18\x03 \x01(\t\"L\n\x1b\x44\x61taTransferEncryptorStatus\x12\x0b\n\x07SUCCESS\x10\x00\x12\x15\n\x11\x45RROR_UNKNOWN_KEY\x10\x01\x12\t\n\x05\x45RROR\x10\x02\"k\n\x0f\x42\x61seHeaderProto\x12.\n\x05\x62lock\x18\x01 \x02(\x0b\x32\x1f.hadoop.hdfs.ExtendedBlockProto\x12(\n\x05token\x18\x02 \x01(\x0b\x32\x19.hadoop.common.TokenProto\"b\n\x1a\x43lientOperationHeaderProto\x12\x30\n\nbaseHeader\x18\x01 \x02(\x0b\x32\x1c.hadoop.hdfs.BaseHeaderProto\x12\x12\n\nclientName\x18\x02 \x02(\t\"=\n\x14\x43\x61\x63hingStrategyProto\x12\x12\n\ndropBehind\x18\x01 \x01(\x08\x12\x11\n\treadahead\x18\x02 \x01(\x03\"\xc1\x01\n\x10OpReadBlockProto\x12\x37\n\x06header\x18\x01 \x02(\x0b\x32\'.hadoop.hdfs.ClientOperationHeaderProto\x12\x0e\n\x06offset\x18\x02 \x02(\x04\x12\x0b\n\x03len\x18\x03 \x02(\x04\x12\x1b\n\rsendChecksums\x18\x04 \x01(\x08:\x04true\x12:\n\x0f\x63\x61\x63hingStrategy\x18\x05 \x01(\x0b\x32!.hadoop.hdfs.CachingStrategyProto\"W\n\rChecksumProto\x12,\n\x04type\x18\x01 \x02(\x0e\x32\x1e.hadoop.hdfs.ChecksumTypeProto\x12\x18\n\x10\x62ytesPerChecksum\x18\x02 \x02(\r\"\xd2\x05\n\x11OpWriteBlockProto\x12\x37\n\x06header\x18\x01 \x02(\x0b\x32\'.hadoop.hdfs.ClientOperationHeaderProto\x12/\n\x07targets\x18\x02 \x03(\x0b\x32\x1e.hadoop.hdfs.DatanodeInfoProto\x12.\n\x06source\x18\x03 \x01(\x0b\x32\x1e.hadoop.hdfs.DatanodeInfoProto\x12\x44\n\x05stage\x18\x04 \x02(\x0e\x32\x35.hadoop.hdfs.OpWriteBlockProto.BlockConstructionStage\x12\x14\n\x0cpipelineSize\x18\x05 \x02(\r\x12\x14\n\x0cminBytesRcvd\x18\x06 \x02(\x04\x12\x14\n\x0cmaxBytesRcvd\x18\x07 \x02(\x04\x12\x1d\n\x15latestGenerationStamp\x18\x08 \x02(\x04\x12\x35\n\x11requestedChecksum\x18\t \x02(\x0b\x32\x1a.hadoop.hdfs.ChecksumProto\x12:\n\x0f\x63\x61\x63hingStrategy\x18\n \x01(\x0b\x32!.hadoop.hdfs.CachingStrategyProto\"\x88\x02\n\x16\x42lockConstructionStage\x12\x19\n\x15PIPELINE_SETUP_APPEND\x10\x00\x12\"\n\x1ePIPELINE_SETUP_APPEND_RECOVERY\x10\x01\x12\x12\n\x0e\x44\x41TA_STREAMING\x10\x02\x12%\n!PIPELINE_SETUP_STREAMING_RECOVERY\x10\x03\x12\x12\n\x0ePIPELINE_CLOSE\x10\x04\x12\x1b\n\x17PIPELINE_CLOSE_RECOVERY\x10\x05\x12\x19\n\x15PIPELINE_SETUP_CREATE\x10\x06\x12\x10\n\x0cTRANSFER_RBW\x10\x07\x12\x16\n\x12TRANSFER_FINALIZED\x10\x08\"\x80\x01\n\x14OpTransferBlockProto\x12\x37\n\x06header\x18\x01 \x02(\x0b\x32\'.hadoop.hdfs.ClientOperationHeaderProto\x12/\n\x07targets\x18\x02 \x03(\x0b\x32\x1e.hadoop.hdfs.DatanodeInfoProto\"\x84\x01\n\x13OpReplaceBlockProto\x12,\n\x06header\x18\x01 \x02(\x0b\x32\x1c.hadoop.hdfs.BaseHeaderProto\x12\x0f\n\x07\x64\x65lHint\x18\x02 \x02(\t\x12.\n\x06source\x18\x03 \x02(\x0b\x32\x1e.hadoop.hdfs.DatanodeInfoProto\"@\n\x10OpCopyBlockProto\x12,\n\x06header\x18\x01 \x02(\x0b\x32\x1c.hadoop.hdfs.BaseHeaderProto\"D\n\x14OpBlockChecksumProto\x12,\n\x06header\x18\x01 \x02(\x0b\x32\x1c.hadoop.hdfs.BaseHeaderProto\"d\n OpRequestShortCircuitAccessProto\x12,\n\x06header\x18\x01 \x02(\x0b\x32\x1c.hadoop.hdfs.BaseHeaderProto\x12\x12\n\nmaxVersion\x18\x02 \x02(\r\"\x7f\n\x11PacketHeaderProto\x12\x15\n\roffsetInBlock\x18\x01 \x02(\x10\x12\r\n\x05seqno\x18\x02 \x02(\x10\x12\x19\n\x11lastPacketInBlock\x18\x03 \x02(\x08\x12\x0f\n\x07\x64\x61taLen\x18\x04 \x02(\x0f\x12\x18\n\tsyncBlock\x18\x05 \x01(\x08:\x05\x66\x61lse\"i\n\x10PipelineAckProto\x12\r\n\x05seqno\x18\x01 \x02(\x12\x12#\n\x06status\x18\x02 \x03(\x0e\x32\x13.hadoop.hdfs.Status\x12!\n\x16\x64ownstreamAckTimeNanos\x18\x03 \x01(\x04:\x01\x30\"\\\n\x17ReadOpChecksumInfoProto\x12,\n\x08\x63hecksum\x18\x01 \x02(\x0b\x32\x1a.hadoop.hdfs.ChecksumProto\x12\x13\n\x0b\x63hunkOffset\x18\x02 \x02(\x04\"\x8c\x02\n\x14\x42lockOpResponseProto\x12#\n\x06status\x18\x01 \x02(\x0e\x32\x13.hadoop.hdfs.Status\x12\x14\n\x0c\x66irstBadLink\x18\x02 \x01(\t\x12\x43\n\x10\x63hecksumResponse\x18\x03 \x01(\x0b\x32).hadoop.hdfs.OpBlockChecksumResponseProto\x12@\n\x12readOpChecksumInfo\x18\x04 \x01(\x0b\x32$.hadoop.hdfs.ReadOpChecksumInfoProto\x12\x0f\n\x07message\x18\x05 \x01(\t\x12!\n\x19shortCircuitAccessVersion\x18\x06 \x01(\r\"<\n\x15\x43lientReadStatusProto\x12#\n\x06status\x18\x01 \x02(\x0e\x32\x13.hadoop.hdfs.Status\"9\n\x12\x44NTransferAckProto\x12#\n\x06status\x18\x01 \x02(\x0e\x32\x13.hadoop.hdfs.Status\"\x86\x01\n\x1cOpBlockChecksumResponseProto\x12\x13\n\x0b\x62ytesPerCrc\x18\x01 \x02(\r\x12\x13\n\x0b\x63rcPerBlock\x18\x02 \x02(\x04\x12\x0b\n\x03md5\x18\x03 \x02(\x0c\x12/\n\x07\x63rcType\x18\x04 \x01(\x0e\x32\x1e.hadoop.hdfs.ChecksumTypeProto*\x99\x01\n\x06Status\x12\x0b\n\x07SUCCESS\x10\x00\x12\t\n\x05\x45RROR\x10\x01\x12\x12\n\x0e\x45RROR_CHECKSUM\x10\x02\x12\x11\n\rERROR_INVALID\x10\x03\x12\x10\n\x0c\x45RROR_EXISTS\x10\x04\x12\x16\n\x12\x45RROR_ACCESS_TOKEN\x10\x05\x12\x0f\n\x0b\x43HECKSUM_OK\x10\x06\x12\x15\n\x11\x45RROR_UNSUPPORTED\x10\x07\x42>\n%org.apache.hadoop.hdfs.protocol.protoB\x12\x44\x61taTransferProtos\xa0\x01\x01')

_STATUS = descriptor.EnumDescriptor(
  name='Status',
  full_name='hadoop.hdfs.Status',
  filename=None,
  file=DESCRIPTOR,
  values=[
    descriptor.EnumValueDescriptor(
      name='SUCCESS', index=0, number=0,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='ERROR', index=1, number=1,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='ERROR_CHECKSUM', index=2, number=2,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='ERROR_INVALID', index=3, number=3,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='ERROR_EXISTS', index=4, number=4,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='ERROR_ACCESS_TOKEN', index=5, number=5,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='CHECKSUM_OK', index=6, number=6,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='ERROR_UNSUPPORTED', index=7, number=7,
      options=None,
      type=None),
  ],
  containing_type=None,
  options=None,
  serialized_start=2951,
  serialized_end=3104,
)


SUCCESS = 0
ERROR = 1
ERROR_CHECKSUM = 2
ERROR_INVALID = 3
ERROR_EXISTS = 4
ERROR_ACCESS_TOKEN = 5
CHECKSUM_OK = 6
ERROR_UNSUPPORTED = 7


_DATATRANSFERENCRYPTORMESSAGEPROTO_DATATRANSFERENCRYPTORSTATUS = descriptor.EnumDescriptor(
  name='DataTransferEncryptorStatus',
  full_name='hadoop.hdfs.DataTransferEncryptorMessageProto.DataTransferEncryptorStatus',
  filename=None,
  file=DESCRIPTOR,
  values=[
    descriptor.EnumValueDescriptor(
      name='SUCCESS', index=0, number=0,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='ERROR_UNKNOWN_KEY', index=1, number=1,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='ERROR', index=2, number=2,
      options=None,
      type=None),
  ],
  containing_type=None,
  options=None,
  serialized_start=227,
  serialized_end=303,
)

_OPWRITEBLOCKPROTO_BLOCKCONSTRUCTIONSTAGE = descriptor.EnumDescriptor(
  name='BlockConstructionStage',
  full_name='hadoop.hdfs.OpWriteBlockProto.BlockConstructionStage',
  filename=None,
  file=DESCRIPTOR,
  values=[
    descriptor.EnumValueDescriptor(
      name='PIPELINE_SETUP_APPEND', index=0, number=0,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='PIPELINE_SETUP_APPEND_RECOVERY', index=1, number=1,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='DATA_STREAMING', index=2, number=2,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='PIPELINE_SETUP_STREAMING_RECOVERY', index=3, number=3,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='PIPELINE_CLOSE', index=4, number=4,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='PIPELINE_CLOSE_RECOVERY', index=5, number=5,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='PIPELINE_SETUP_CREATE', index=6, number=6,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='TRANSFER_RBW', index=7, number=7,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='TRANSFER_FINALIZED', index=8, number=8,
      options=None,
      type=None),
  ],
  containing_type=None,
  options=None,
  serialized_start=1321,
  serialized_end=1585,
)


_DATATRANSFERENCRYPTORMESSAGEPROTO = descriptor.Descriptor(
  name='DataTransferEncryptorMessageProto',
  full_name='hadoop.hdfs.DataTransferEncryptorMessageProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='status', full_name='hadoop.hdfs.DataTransferEncryptorMessageProto.status', index=0,
      number=1, type=14, cpp_type=8, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='payload', full_name='hadoop.hdfs.DataTransferEncryptorMessageProto.payload', index=1,
      number=2, type=12, cpp_type=9, label=1,
      has_default_value=False, default_value="",
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='message', full_name='hadoop.hdfs.DataTransferEncryptorMessageProto.message', index=2,
      number=3, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
    _DATATRANSFERENCRYPTORMESSAGEPROTO_DATATRANSFERENCRYPTORSTATUS,
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=64,
  serialized_end=303,
)


_BASEHEADERPROTO = descriptor.Descriptor(
  name='BaseHeaderProto',
  full_name='hadoop.hdfs.BaseHeaderProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='block', full_name='hadoop.hdfs.BaseHeaderProto.block', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='token', full_name='hadoop.hdfs.BaseHeaderProto.token', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=305,
  serialized_end=412,
)


_CLIENTOPERATIONHEADERPROTO = descriptor.Descriptor(
  name='ClientOperationHeaderProto',
  full_name='hadoop.hdfs.ClientOperationHeaderProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='baseHeader', full_name='hadoop.hdfs.ClientOperationHeaderProto.baseHeader', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='clientName', full_name='hadoop.hdfs.ClientOperationHeaderProto.clientName', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=414,
  serialized_end=512,
)


_CACHINGSTRATEGYPROTO = descriptor.Descriptor(
  name='CachingStrategyProto',
  full_name='hadoop.hdfs.CachingStrategyProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='dropBehind', full_name='hadoop.hdfs.CachingStrategyProto.dropBehind', index=0,
      number=1, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='readahead', full_name='hadoop.hdfs.CachingStrategyProto.readahead', index=1,
      number=2, type=3, cpp_type=2, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=514,
  serialized_end=575,
)


_OPREADBLOCKPROTO = descriptor.Descriptor(
  name='OpReadBlockProto',
  full_name='hadoop.hdfs.OpReadBlockProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='header', full_name='hadoop.hdfs.OpReadBlockProto.header', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='offset', full_name='hadoop.hdfs.OpReadBlockProto.offset', index=1,
      number=2, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='len', full_name='hadoop.hdfs.OpReadBlockProto.len', index=2,
      number=3, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='sendChecksums', full_name='hadoop.hdfs.OpReadBlockProto.sendChecksums', index=3,
      number=4, type=8, cpp_type=7, label=1,
      has_default_value=True, default_value=True,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='cachingStrategy', full_name='hadoop.hdfs.OpReadBlockProto.cachingStrategy', index=4,
      number=5, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=578,
  serialized_end=771,
)


_CHECKSUMPROTO = descriptor.Descriptor(
  name='ChecksumProto',
  full_name='hadoop.hdfs.ChecksumProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='type', full_name='hadoop.hdfs.ChecksumProto.type', index=0,
      number=1, type=14, cpp_type=8, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='bytesPerChecksum', full_name='hadoop.hdfs.ChecksumProto.bytesPerChecksum', index=1,
      number=2, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=773,
  serialized_end=860,
)


_OPWRITEBLOCKPROTO = descriptor.Descriptor(
  name='OpWriteBlockProto',
  full_name='hadoop.hdfs.OpWriteBlockProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='header', full_name='hadoop.hdfs.OpWriteBlockProto.header', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='targets', full_name='hadoop.hdfs.OpWriteBlockProto.targets', index=1,
      number=2, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='source', full_name='hadoop.hdfs.OpWriteBlockProto.source', index=2,
      number=3, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='stage', full_name='hadoop.hdfs.OpWriteBlockProto.stage', index=3,
      number=4, type=14, cpp_type=8, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='pipelineSize', full_name='hadoop.hdfs.OpWriteBlockProto.pipelineSize', index=4,
      number=5, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='minBytesRcvd', full_name='hadoop.hdfs.OpWriteBlockProto.minBytesRcvd', index=5,
      number=6, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='maxBytesRcvd', full_name='hadoop.hdfs.OpWriteBlockProto.maxBytesRcvd', index=6,
      number=7, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='latestGenerationStamp', full_name='hadoop.hdfs.OpWriteBlockProto.latestGenerationStamp', index=7,
      number=8, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='requestedChecksum', full_name='hadoop.hdfs.OpWriteBlockProto.requestedChecksum', index=8,
      number=9, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='cachingStrategy', full_name='hadoop.hdfs.OpWriteBlockProto.cachingStrategy', index=9,
      number=10, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
    _OPWRITEBLOCKPROTO_BLOCKCONSTRUCTIONSTAGE,
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=863,
  serialized_end=1585,
)


_OPTRANSFERBLOCKPROTO = descriptor.Descriptor(
  name='OpTransferBlockProto',
  full_name='hadoop.hdfs.OpTransferBlockProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='header', full_name='hadoop.hdfs.OpTransferBlockProto.header', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='targets', full_name='hadoop.hdfs.OpTransferBlockProto.targets', index=1,
      number=2, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1588,
  serialized_end=1716,
)


_OPREPLACEBLOCKPROTO = descriptor.Descriptor(
  name='OpReplaceBlockProto',
  full_name='hadoop.hdfs.OpReplaceBlockProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='header', full_name='hadoop.hdfs.OpReplaceBlockProto.header', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='delHint', full_name='hadoop.hdfs.OpReplaceBlockProto.delHint', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='source', full_name='hadoop.hdfs.OpReplaceBlockProto.source', index=2,
      number=3, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1719,
  serialized_end=1851,
)


_OPCOPYBLOCKPROTO = descriptor.Descriptor(
  name='OpCopyBlockProto',
  full_name='hadoop.hdfs.OpCopyBlockProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='header', full_name='hadoop.hdfs.OpCopyBlockProto.header', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1853,
  serialized_end=1917,
)


_OPBLOCKCHECKSUMPROTO = descriptor.Descriptor(
  name='OpBlockChecksumProto',
  full_name='hadoop.hdfs.OpBlockChecksumProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='header', full_name='hadoop.hdfs.OpBlockChecksumProto.header', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1919,
  serialized_end=1987,
)


_OPREQUESTSHORTCIRCUITACCESSPROTO = descriptor.Descriptor(
  name='OpRequestShortCircuitAccessProto',
  full_name='hadoop.hdfs.OpRequestShortCircuitAccessProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='header', full_name='hadoop.hdfs.OpRequestShortCircuitAccessProto.header', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='maxVersion', full_name='hadoop.hdfs.OpRequestShortCircuitAccessProto.maxVersion', index=1,
      number=2, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1989,
  serialized_end=2089,
)


_PACKETHEADERPROTO = descriptor.Descriptor(
  name='PacketHeaderProto',
  full_name='hadoop.hdfs.PacketHeaderProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='offsetInBlock', full_name='hadoop.hdfs.PacketHeaderProto.offsetInBlock', index=0,
      number=1, type=16, cpp_type=2, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='seqno', full_name='hadoop.hdfs.PacketHeaderProto.seqno', index=1,
      number=2, type=16, cpp_type=2, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='lastPacketInBlock', full_name='hadoop.hdfs.PacketHeaderProto.lastPacketInBlock', index=2,
      number=3, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='dataLen', full_name='hadoop.hdfs.PacketHeaderProto.dataLen', index=3,
      number=4, type=15, cpp_type=1, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='syncBlock', full_name='hadoop.hdfs.PacketHeaderProto.syncBlock', index=4,
      number=5, type=8, cpp_type=7, label=1,
      has_default_value=True, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2091,
  serialized_end=2218,
)


_PIPELINEACKPROTO = descriptor.Descriptor(
  name='PipelineAckProto',
  full_name='hadoop.hdfs.PipelineAckProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='seqno', full_name='hadoop.hdfs.PipelineAckProto.seqno', index=0,
      number=1, type=18, cpp_type=2, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='status', full_name='hadoop.hdfs.PipelineAckProto.status', index=1,
      number=2, type=14, cpp_type=8, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='downstreamAckTimeNanos', full_name='hadoop.hdfs.PipelineAckProto.downstreamAckTimeNanos', index=2,
      number=3, type=4, cpp_type=4, label=1,
      has_default_value=True, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2220,
  serialized_end=2325,
)


_READOPCHECKSUMINFOPROTO = descriptor.Descriptor(
  name='ReadOpChecksumInfoProto',
  full_name='hadoop.hdfs.ReadOpChecksumInfoProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='checksum', full_name='hadoop.hdfs.ReadOpChecksumInfoProto.checksum', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='chunkOffset', full_name='hadoop.hdfs.ReadOpChecksumInfoProto.chunkOffset', index=1,
      number=2, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2327,
  serialized_end=2419,
)


_BLOCKOPRESPONSEPROTO = descriptor.Descriptor(
  name='BlockOpResponseProto',
  full_name='hadoop.hdfs.BlockOpResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='status', full_name='hadoop.hdfs.BlockOpResponseProto.status', index=0,
      number=1, type=14, cpp_type=8, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='firstBadLink', full_name='hadoop.hdfs.BlockOpResponseProto.firstBadLink', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='checksumResponse', full_name='hadoop.hdfs.BlockOpResponseProto.checksumResponse', index=2,
      number=3, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='readOpChecksumInfo', full_name='hadoop.hdfs.BlockOpResponseProto.readOpChecksumInfo', index=3,
      number=4, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='message', full_name='hadoop.hdfs.BlockOpResponseProto.message', index=4,
      number=5, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='shortCircuitAccessVersion', full_name='hadoop.hdfs.BlockOpResponseProto.shortCircuitAccessVersion', index=5,
      number=6, type=13, cpp_type=3, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2422,
  serialized_end=2690,
)


_CLIENTREADSTATUSPROTO = descriptor.Descriptor(
  name='ClientReadStatusProto',
  full_name='hadoop.hdfs.ClientReadStatusProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='status', full_name='hadoop.hdfs.ClientReadStatusProto.status', index=0,
      number=1, type=14, cpp_type=8, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2692,
  serialized_end=2752,
)


_DNTRANSFERACKPROTO = descriptor.Descriptor(
  name='DNTransferAckProto',
  full_name='hadoop.hdfs.DNTransferAckProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='status', full_name='hadoop.hdfs.DNTransferAckProto.status', index=0,
      number=1, type=14, cpp_type=8, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2754,
  serialized_end=2811,
)


_OPBLOCKCHECKSUMRESPONSEPROTO = descriptor.Descriptor(
  name='OpBlockChecksumResponseProto',
  full_name='hadoop.hdfs.OpBlockChecksumResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='bytesPerCrc', full_name='hadoop.hdfs.OpBlockChecksumResponseProto.bytesPerCrc', index=0,
      number=1, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='crcPerBlock', full_name='hadoop.hdfs.OpBlockChecksumResponseProto.crcPerBlock', index=1,
      number=2, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='md5', full_name='hadoop.hdfs.OpBlockChecksumResponseProto.md5', index=2,
      number=3, type=12, cpp_type=9, label=2,
      has_default_value=False, default_value="",
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='crcType', full_name='hadoop.hdfs.OpBlockChecksumResponseProto.crcType', index=3,
      number=4, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2814,
  serialized_end=2948,
)

_DATATRANSFERENCRYPTORMESSAGEPROTO.fields_by_name['status'].enum_type = _DATATRANSFERENCRYPTORMESSAGEPROTO_DATATRANSFERENCRYPTORSTATUS
_DATATRANSFERENCRYPTORMESSAGEPROTO_DATATRANSFERENCRYPTORSTATUS.containing_type = _DATATRANSFERENCRYPTORMESSAGEPROTO;
_BASEHEADERPROTO.fields_by_name['block'].message_type = hdfs_pb2._EXTENDEDBLOCKPROTO
_BASEHEADERPROTO.fields_by_name['token'].message_type = Security_pb2._TOKENPROTO
_CLIENTOPERATIONHEADERPROTO.fields_by_name['baseHeader'].message_type = _BASEHEADERPROTO
_OPREADBLOCKPROTO.fields_by_name['header'].message_type = _CLIENTOPERATIONHEADERPROTO
_OPREADBLOCKPROTO.fields_by_name['cachingStrategy'].message_type = _CACHINGSTRATEGYPROTO
_CHECKSUMPROTO.fields_by_name['type'].enum_type = hdfs_pb2._CHECKSUMTYPEPROTO
_OPWRITEBLOCKPROTO.fields_by_name['header'].message_type = _CLIENTOPERATIONHEADERPROTO
_OPWRITEBLOCKPROTO.fields_by_name['targets'].message_type = hdfs_pb2._DATANODEINFOPROTO
_OPWRITEBLOCKPROTO.fields_by_name['source'].message_type = hdfs_pb2._DATANODEINFOPROTO
_OPWRITEBLOCKPROTO.fields_by_name['stage'].enum_type = _OPWRITEBLOCKPROTO_BLOCKCONSTRUCTIONSTAGE
_OPWRITEBLOCKPROTO.fields_by_name['requestedChecksum'].message_type = _CHECKSUMPROTO
_OPWRITEBLOCKPROTO.fields_by_name['cachingStrategy'].message_type = _CACHINGSTRATEGYPROTO
_OPWRITEBLOCKPROTO_BLOCKCONSTRUCTIONSTAGE.containing_type = _OPWRITEBLOCKPROTO;
_OPTRANSFERBLOCKPROTO.fields_by_name['header'].message_type = _CLIENTOPERATIONHEADERPROTO
_OPTRANSFERBLOCKPROTO.fields_by_name['targets'].message_type = hdfs_pb2._DATANODEINFOPROTO
_OPREPLACEBLOCKPROTO.fields_by_name['header'].message_type = _BASEHEADERPROTO
_OPREPLACEBLOCKPROTO.fields_by_name['source'].message_type = hdfs_pb2._DATANODEINFOPROTO
_OPCOPYBLOCKPROTO.fields_by_name['header'].message_type = _BASEHEADERPROTO
_OPBLOCKCHECKSUMPROTO.fields_by_name['header'].message_type = _BASEHEADERPROTO
_OPREQUESTSHORTCIRCUITACCESSPROTO.fields_by_name['header'].message_type = _BASEHEADERPROTO
_PIPELINEACKPROTO.fields_by_name['status'].enum_type = _STATUS
_READOPCHECKSUMINFOPROTO.fields_by_name['checksum'].message_type = _CHECKSUMPROTO
_BLOCKOPRESPONSEPROTO.fields_by_name['status'].enum_type = _STATUS
_BLOCKOPRESPONSEPROTO.fields_by_name['checksumResponse'].message_type = _OPBLOCKCHECKSUMRESPONSEPROTO
_BLOCKOPRESPONSEPROTO.fields_by_name['readOpChecksumInfo'].message_type = _READOPCHECKSUMINFOPROTO
_CLIENTREADSTATUSPROTO.fields_by_name['status'].enum_type = _STATUS
_DNTRANSFERACKPROTO.fields_by_name['status'].enum_type = _STATUS
_OPBLOCKCHECKSUMRESPONSEPROTO.fields_by_name['crcType'].enum_type = hdfs_pb2._CHECKSUMTYPEPROTO
DESCRIPTOR.message_types_by_name['DataTransferEncryptorMessageProto'] = _DATATRANSFERENCRYPTORMESSAGEPROTO
DESCRIPTOR.message_types_by_name['BaseHeaderProto'] = _BASEHEADERPROTO
DESCRIPTOR.message_types_by_name['ClientOperationHeaderProto'] = _CLIENTOPERATIONHEADERPROTO
DESCRIPTOR.message_types_by_name['CachingStrategyProto'] = _CACHINGSTRATEGYPROTO
DESCRIPTOR.message_types_by_name['OpReadBlockProto'] = _OPREADBLOCKPROTO
DESCRIPTOR.message_types_by_name['ChecksumProto'] = _CHECKSUMPROTO
DESCRIPTOR.message_types_by_name['OpWriteBlockProto'] = _OPWRITEBLOCKPROTO
DESCRIPTOR.message_types_by_name['OpTransferBlockProto'] = _OPTRANSFERBLOCKPROTO
DESCRIPTOR.message_types_by_name['OpReplaceBlockProto'] = _OPREPLACEBLOCKPROTO
DESCRIPTOR.message_types_by_name['OpCopyBlockProto'] = _OPCOPYBLOCKPROTO
DESCRIPTOR.message_types_by_name['OpBlockChecksumProto'] = _OPBLOCKCHECKSUMPROTO
DESCRIPTOR.message_types_by_name['OpRequestShortCircuitAccessProto'] = _OPREQUESTSHORTCIRCUITACCESSPROTO
DESCRIPTOR.message_types_by_name['PacketHeaderProto'] = _PACKETHEADERPROTO
DESCRIPTOR.message_types_by_name['PipelineAckProto'] = _PIPELINEACKPROTO
DESCRIPTOR.message_types_by_name['ReadOpChecksumInfoProto'] = _READOPCHECKSUMINFOPROTO
DESCRIPTOR.message_types_by_name['BlockOpResponseProto'] = _BLOCKOPRESPONSEPROTO
DESCRIPTOR.message_types_by_name['ClientReadStatusProto'] = _CLIENTREADSTATUSPROTO
DESCRIPTOR.message_types_by_name['DNTransferAckProto'] = _DNTRANSFERACKPROTO
DESCRIPTOR.message_types_by_name['OpBlockChecksumResponseProto'] = _OPBLOCKCHECKSUMRESPONSEPROTO

class DataTransferEncryptorMessageProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _DATATRANSFERENCRYPTORMESSAGEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.DataTransferEncryptorMessageProto)

class BaseHeaderProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _BASEHEADERPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.BaseHeaderProto)

class ClientOperationHeaderProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _CLIENTOPERATIONHEADERPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.ClientOperationHeaderProto)

class CachingStrategyProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _CACHINGSTRATEGYPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.CachingStrategyProto)

class OpReadBlockProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _OPREADBLOCKPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.OpReadBlockProto)

class ChecksumProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _CHECKSUMPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.ChecksumProto)

class OpWriteBlockProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _OPWRITEBLOCKPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.OpWriteBlockProto)

class OpTransferBlockProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _OPTRANSFERBLOCKPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.OpTransferBlockProto)

class OpReplaceBlockProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _OPREPLACEBLOCKPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.OpReplaceBlockProto)

class OpCopyBlockProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _OPCOPYBLOCKPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.OpCopyBlockProto)

class OpBlockChecksumProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _OPBLOCKCHECKSUMPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.OpBlockChecksumProto)

class OpRequestShortCircuitAccessProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _OPREQUESTSHORTCIRCUITACCESSPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.OpRequestShortCircuitAccessProto)

class PacketHeaderProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _PACKETHEADERPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.PacketHeaderProto)

class PipelineAckProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _PIPELINEACKPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.PipelineAckProto)

class ReadOpChecksumInfoProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _READOPCHECKSUMINFOPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.ReadOpChecksumInfoProto)

class BlockOpResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _BLOCKOPRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.BlockOpResponseProto)

class ClientReadStatusProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _CLIENTREADSTATUSPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.ClientReadStatusProto)

class DNTransferAckProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _DNTRANSFERACKPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.DNTransferAckProto)

class OpBlockChecksumResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _OPBLOCKCHECKSUMRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.OpBlockChecksumResponseProto)

# @@protoc_insertion_point(module_scope)

########NEW FILE########
__FILENAME__ = hdfs_pb2
# Generated by the protocol buffer compiler.  DO NOT EDIT!

from google.protobuf import descriptor
from google.protobuf import message
from google.protobuf import reflection
from google.protobuf import descriptor_pb2
# @@protoc_insertion_point(imports)


import Security_pb2

DESCRIPTOR = descriptor.FileDescriptor(
  name='hdfs.proto',
  package='hadoop.hdfs',
  serialized_pb='\n\nhdfs.proto\x12\x0bhadoop.hdfs\x1a\x0eSecurity.proto\"c\n\x12\x45xtendedBlockProto\x12\x0e\n\x06poolId\x18\x01 \x02(\t\x12\x0f\n\x07\x62lockId\x18\x02 \x02(\x04\x12\x17\n\x0fgenerationStamp\x18\x03 \x02(\x04\x12\x13\n\x08numBytes\x18\x04 \x01(\x04:\x01\x30\"\x96\x01\n\x0f\x44\x61tanodeIDProto\x12\x0e\n\x06ipAddr\x18\x01 \x02(\t\x12\x10\n\x08hostName\x18\x02 \x02(\t\x12\x11\n\tstorageID\x18\x03 \x02(\t\x12\x10\n\x08xferPort\x18\x04 \x02(\r\x12\x10\n\x08infoPort\x18\x05 \x02(\r\x12\x0f\n\x07ipcPort\x18\x06 \x02(\r\x12\x19\n\x0einfoSecurePort\x18\x07 \x01(\r:\x01\x30\"G\n\x12\x44\x61tanodeInfosProto\x12\x31\n\tdatanodes\x18\x01 \x03(\x0b\x32\x1e.hadoop.hdfs.DatanodeInfoProto\"\xea\x02\n\x11\x44\x61tanodeInfoProto\x12(\n\x02id\x18\x01 \x02(\x0b\x32\x1c.hadoop.hdfs.DatanodeIDProto\x12\x13\n\x08\x63\x61pacity\x18\x02 \x01(\x04:\x01\x30\x12\x12\n\x07\x64\x66sUsed\x18\x03 \x01(\x04:\x01\x30\x12\x14\n\tremaining\x18\x04 \x01(\x04:\x01\x30\x12\x18\n\rblockPoolUsed\x18\x05 \x01(\x04:\x01\x30\x12\x15\n\nlastUpdate\x18\x06 \x01(\x04:\x01\x30\x12\x17\n\x0cxceiverCount\x18\x07 \x01(\r:\x01\x30\x12\x10\n\x08location\x18\x08 \x01(\t\x12\x45\n\nadminState\x18\n \x01(\x0e\x32).hadoop.hdfs.DatanodeInfoProto.AdminState:\x06NORMAL\"I\n\nAdminState\x12\n\n\x06NORMAL\x10\x00\x12\x1b\n\x17\x44\x45\x43OMMISSION_INPROGRESS\x10\x01\x12\x12\n\x0e\x44\x45\x43OMMISSIONED\x10\x02\"\x8a\x01\n\x13\x43ontentSummaryProto\x12\x0e\n\x06length\x18\x01 \x02(\x04\x12\x11\n\tfileCount\x18\x02 \x02(\x04\x12\x16\n\x0e\x64irectoryCount\x18\x03 \x02(\x04\x12\r\n\x05quota\x18\x04 \x02(\x04\x12\x15\n\rspaceConsumed\x18\x05 \x02(\x04\x12\x12\n\nspaceQuota\x18\x06 \x02(\x04\"7\n\x16\x43orruptFileBlocksProto\x12\r\n\x05\x66iles\x18\x01 \x03(\t\x12\x0e\n\x06\x63ookie\x18\x02 \x02(\t\"!\n\x11\x46sPermissionProto\x12\x0c\n\x04perm\x18\x01 \x02(\r\"\xbd\x01\n\x11LocatedBlockProto\x12*\n\x01\x62\x18\x01 \x02(\x0b\x32\x1f.hadoop.hdfs.ExtendedBlockProto\x12\x0e\n\x06offset\x18\x02 \x02(\x04\x12,\n\x04locs\x18\x03 \x03(\x0b\x32\x1e.hadoop.hdfs.DatanodeInfoProto\x12\x0f\n\x07\x63orrupt\x18\x04 \x02(\x08\x12-\n\nblockToken\x18\x05 \x02(\x0b\x32\x19.hadoop.common.TokenProto\"\x93\x01\n\x16\x44\x61taEncryptionKeyProto\x12\r\n\x05keyId\x18\x01 \x02(\r\x12\x13\n\x0b\x62lockPoolId\x18\x02 \x02(\t\x12\r\n\x05nonce\x18\x03 \x02(\x0c\x12\x15\n\rencryptionKey\x18\x04 \x02(\x0c\x12\x12\n\nexpiryDate\x18\x05 \x02(\x04\x12\x1b\n\x13\x65ncryptionAlgorithm\x18\x06 \x01(\t\"\xc3\x01\n\x12LocatedBlocksProto\x12\x12\n\nfileLength\x18\x01 \x02(\x04\x12.\n\x06\x62locks\x18\x02 \x03(\x0b\x32\x1e.hadoop.hdfs.LocatedBlockProto\x12\x19\n\x11underConstruction\x18\x03 \x02(\x08\x12\x31\n\tlastBlock\x18\x04 \x01(\x0b\x32\x1e.hadoop.hdfs.LocatedBlockProto\x12\x1b\n\x13isLastBlockComplete\x18\x05 \x02(\x08\"\xcc\x03\n\x13HdfsFileStatusProto\x12;\n\x08\x66ileType\x18\x01 \x02(\x0e\x32).hadoop.hdfs.HdfsFileStatusProto.FileType\x12\x0c\n\x04path\x18\x02 \x02(\x0c\x12\x0e\n\x06length\x18\x03 \x02(\x04\x12\x32\n\npermission\x18\x04 \x02(\x0b\x32\x1e.hadoop.hdfs.FsPermissionProto\x12\r\n\x05owner\x18\x05 \x02(\t\x12\r\n\x05group\x18\x06 \x02(\t\x12\x19\n\x11modification_time\x18\x07 \x02(\x04\x12\x13\n\x0b\x61\x63\x63\x65ss_time\x18\x08 \x02(\x04\x12\x0f\n\x07symlink\x18\t \x01(\x0c\x12\x1c\n\x11\x62lock_replication\x18\n \x01(\r:\x01\x30\x12\x14\n\tblocksize\x18\x0b \x01(\x04:\x01\x30\x12\x32\n\tlocations\x18\x0c \x01(\x0b\x32\x1f.hadoop.hdfs.LocatedBlocksProto\x12\x11\n\x06\x66ileId\x18\r \x01(\x04:\x01\x30\x12\x17\n\x0b\x63hildrenNum\x18\x0e \x01(\x05:\x02-1\"3\n\x08\x46ileType\x12\n\n\x06IS_DIR\x10\x01\x12\x0b\n\x07IS_FILE\x10\x02\x12\x0e\n\nIS_SYMLINK\x10\x03\"\x8e\x02\n\x15\x46sServerDefaultsProto\x12\x11\n\tblockSize\x18\x01 \x02(\x04\x12\x18\n\x10\x62ytesPerChecksum\x18\x02 \x02(\r\x12\x17\n\x0fwritePacketSize\x18\x03 \x02(\r\x12\x13\n\x0breplication\x18\x04 \x02(\r\x12\x16\n\x0e\x66ileBufferSize\x18\x05 \x02(\r\x12\"\n\x13\x65ncryptDataTransfer\x18\x06 \x01(\x08:\x05\x66\x61lse\x12\x18\n\rtrashInterval\x18\x07 \x01(\x04:\x01\x30\x12\x44\n\x0c\x63hecksumType\x18\x08 \x01(\x0e\x32\x1e.hadoop.hdfs.ChecksumTypeProto:\x0e\x43HECKSUM_CRC32\"k\n\x15\x44irectoryListingProto\x12\x38\n\x0epartialListing\x18\x01 \x03(\x0b\x32 .hadoop.hdfs.HdfsFileStatusProto\x12\x18\n\x10remainingEntries\x18\x02 \x02(\r\"\xa2\x01\n!SnapshottableDirectoryStatusProto\x12\x33\n\tdirStatus\x18\x01 \x02(\x0b\x32 .hadoop.hdfs.HdfsFileStatusProto\x12\x16\n\x0esnapshot_quota\x18\x02 \x02(\r\x12\x17\n\x0fsnapshot_number\x18\x03 \x02(\r\x12\x17\n\x0fparent_fullpath\x18\x04 \x02(\x0c\"u\n\"SnapshottableDirectoryListingProto\x12O\n\x17snapshottableDirListing\x18\x01 \x03(\x0b\x32..hadoop.hdfs.SnapshottableDirectoryStatusProto\"K\n\x1cSnapshotDiffReportEntryProto\x12\x10\n\x08\x66ullpath\x18\x01 \x02(\x0c\x12\x19\n\x11modificationLabel\x18\x02 \x02(\t\"\x9f\x01\n\x17SnapshotDiffReportProto\x12\x14\n\x0csnapshotRoot\x18\x01 \x02(\t\x12\x14\n\x0c\x66romSnapshot\x18\x02 \x02(\t\x12\x12\n\ntoSnapshot\x18\x03 \x02(\t\x12\x44\n\x11\x64iffReportEntries\x18\x04 \x03(\x0b\x32).hadoop.hdfs.SnapshotDiffReportEntryProto\"_\n\x10StorageInfoProto\x12\x15\n\rlayoutVersion\x18\x01 \x02(\r\x12\x12\n\nnamespceID\x18\x02 \x02(\r\x12\x11\n\tclusterID\x18\x03 \x02(\t\x12\r\n\x05\x63Time\x18\x04 \x02(\x04\"\x89\x02\n\x19NamenodeRegistrationProto\x12\x12\n\nrpcAddress\x18\x01 \x02(\t\x12\x13\n\x0bhttpAddress\x18\x02 \x02(\t\x12\x32\n\x0bstorageInfo\x18\x03 \x02(\x0b\x32\x1d.hadoop.hdfs.StorageInfoProto\x12P\n\x04role\x18\x04 \x01(\x0e\x32\x38.hadoop.hdfs.NamenodeRegistrationProto.NamenodeRoleProto:\x08NAMENODE\"=\n\x11NamenodeRoleProto\x12\x0c\n\x08NAMENODE\x10\x01\x12\n\n\x06\x42\x41\x43KUP\x10\x02\x12\x0e\n\nCHECKPOINT\x10\x03\"\x9d\x01\n\x18\x43heckpointSignatureProto\x12\x13\n\x0b\x62lockPoolId\x18\x01 \x02(\t\x12 \n\x18mostRecentCheckpointTxId\x18\x02 \x02(\x04\x12\x16\n\x0e\x63urSegmentTxId\x18\x03 \x02(\x04\x12\x32\n\x0bstorageInfo\x18\x04 \x02(\x0b\x32\x1d.hadoop.hdfs.StorageInfoProto\"\xcc\x01\n\x14NamenodeCommandProto\x12\x0e\n\x06\x61\x63tion\x18\x01 \x02(\r\x12\x34\n\x04type\x18\x02 \x02(\x0e\x32&.hadoop.hdfs.NamenodeCommandProto.Type\x12:\n\rcheckpointCmd\x18\x03 \x01(\x0b\x32#.hadoop.hdfs.CheckpointCommandProto\"2\n\x04Type\x12\x13\n\x0fNamenodeCommand\x10\x00\x12\x15\n\x11\x43heckPointCommand\x10\x01\"m\n\x16\x43heckpointCommandProto\x12\x38\n\tsignature\x18\x01 \x02(\x0b\x32%.hadoop.hdfs.CheckpointSignatureProto\x12\x19\n\x11needToReturnImage\x18\x02 \x02(\x08\"D\n\nBlockProto\x12\x0f\n\x07\x62lockId\x18\x01 \x02(\x04\x12\x10\n\x08genStamp\x18\x02 \x02(\x04\x12\x13\n\x08numBytes\x18\x03 \x01(\x04:\x01\x30\"U\n\x17\x42lockWithLocationsProto\x12&\n\x05\x62lock\x18\x01 \x02(\x0b\x32\x17.hadoop.hdfs.BlockProto\x12\x12\n\nstorageIDs\x18\x02 \x03(\t\"P\n\x18\x42locksWithLocationsProto\x12\x34\n\x06\x62locks\x18\x01 \x03(\x0b\x32$.hadoop.hdfs.BlockWithLocationsProto\"U\n\x12RemoteEditLogProto\x12\x11\n\tstartTxId\x18\x01 \x02(\x04\x12\x0f\n\x07\x65ndTxId\x18\x02 \x02(\x04\x12\x1b\n\x0cisInProgress\x18\x03 \x01(\x08:\x05\x66\x61lse\"K\n\x1aRemoteEditLogManifestProto\x12-\n\x04logs\x18\x01 \x03(\x0b\x32\x1f.hadoop.hdfs.RemoteEditLogProto\"\x9c\x01\n\x12NamespaceInfoProto\x12\x14\n\x0c\x62uildVersion\x18\x01 \x02(\t\x12\x0e\n\x06unused\x18\x02 \x02(\r\x12\x13\n\x0b\x62lockPoolID\x18\x03 \x02(\t\x12\x32\n\x0bstorageInfo\x18\x04 \x02(\x0b\x32\x1d.hadoop.hdfs.StorageInfoProto\x12\x17\n\x0fsoftwareVersion\x18\x05 \x02(\t\"D\n\rBlockKeyProto\x12\r\n\x05keyId\x18\x01 \x02(\r\x12\x12\n\nexpiryDate\x18\x02 \x02(\x04\x12\x10\n\x08keyBytes\x18\x03 \x01(\x0c\"\xc4\x01\n\x16\x45xportedBlockKeysProto\x12\x1b\n\x13isBlockTokenEnabled\x18\x01 \x02(\x08\x12\x19\n\x11keyUpdateInterval\x18\x02 \x02(\x04\x12\x15\n\rtokenLifeTime\x18\x03 \x02(\x04\x12.\n\ncurrentKey\x18\x04 \x02(\x0b\x32\x1a.hadoop.hdfs.BlockKeyProto\x12+\n\x07\x61llKeys\x18\x05 \x03(\x0b\x32\x1a.hadoop.hdfs.BlockKeyProto\"Z\n\x14RecoveringBlockProto\x12\x13\n\x0bnewGenStamp\x18\x01 \x02(\x04\x12-\n\x05\x62lock\x18\x02 \x02(\x0b\x32\x1e.hadoop.hdfs.LocatedBlockProto\"\x15\n\x13VersionRequestProto\"E\n\x14VersionResponseProto\x12-\n\x04info\x18\x01 \x02(\x0b\x32\x1f.hadoop.hdfs.NamespaceInfoProto\"\xa5\x01\n\x11SnapshotInfoProto\x12\x14\n\x0csnapshotName\x18\x01 \x02(\t\x12\x14\n\x0csnapshotRoot\x18\x02 \x02(\t\x12\x32\n\npermission\x18\x03 \x02(\x0b\x32\x1e.hadoop.hdfs.FsPermissionProto\x12\r\n\x05owner\x18\x04 \x02(\t\x12\r\n\x05group\x18\x05 \x02(\t\x12\x12\n\ncreateTime\x18\x06 \x02(\t*O\n\x11\x43hecksumTypeProto\x12\x11\n\rCHECKSUM_NULL\x10\x00\x12\x12\n\x0e\x43HECKSUM_CRC32\x10\x01\x12\x13\n\x0f\x43HECKSUM_CRC32C\x10\x02*L\n\x11ReplicaStateProto\x12\r\n\tFINALIZED\x10\x00\x12\x07\n\x03RBW\x10\x01\x12\x07\n\x03RWR\x10\x02\x12\x07\n\x03RUR\x10\x03\x12\r\n\tTEMPORARY\x10\x04\x42\x36\n%org.apache.hadoop.hdfs.protocol.protoB\nHdfsProtos\xa0\x01\x01')

_CHECKSUMTYPEPROTO = descriptor.EnumDescriptor(
  name='ChecksumTypeProto',
  full_name='hadoop.hdfs.ChecksumTypeProto',
  filename=None,
  file=DESCRIPTOR,
  values=[
    descriptor.EnumValueDescriptor(
      name='CHECKSUM_NULL', index=0, number=0,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='CHECKSUM_CRC32', index=1, number=1,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='CHECKSUM_CRC32C', index=2, number=2,
      options=None,
      type=None),
  ],
  containing_type=None,
  options=None,
  serialized_start=4904,
  serialized_end=4983,
)


_REPLICASTATEPROTO = descriptor.EnumDescriptor(
  name='ReplicaStateProto',
  full_name='hadoop.hdfs.ReplicaStateProto',
  filename=None,
  file=DESCRIPTOR,
  values=[
    descriptor.EnumValueDescriptor(
      name='FINALIZED', index=0, number=0,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='RBW', index=1, number=1,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='RWR', index=2, number=2,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='RUR', index=3, number=3,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='TEMPORARY', index=4, number=4,
      options=None,
      type=None),
  ],
  containing_type=None,
  options=None,
  serialized_start=4985,
  serialized_end=5061,
)


CHECKSUM_NULL = 0
CHECKSUM_CRC32 = 1
CHECKSUM_CRC32C = 2
FINALIZED = 0
RBW = 1
RWR = 2
RUR = 3
TEMPORARY = 4


_DATANODEINFOPROTO_ADMINSTATE = descriptor.EnumDescriptor(
  name='AdminState',
  full_name='hadoop.hdfs.DatanodeInfoProto.AdminState',
  filename=None,
  file=DESCRIPTOR,
  values=[
    descriptor.EnumValueDescriptor(
      name='NORMAL', index=0, number=0,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='DECOMMISSION_INPROGRESS', index=1, number=1,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='DECOMMISSIONED', index=2, number=2,
      options=None,
      type=None),
  ],
  containing_type=None,
  options=None,
  serialized_start=660,
  serialized_end=733,
)

_HDFSFILESTATUSPROTO_FILETYPE = descriptor.EnumDescriptor(
  name='FileType',
  full_name='hadoop.hdfs.HdfsFileStatusProto.FileType',
  filename=None,
  file=DESCRIPTOR,
  values=[
    descriptor.EnumValueDescriptor(
      name='IS_DIR', index=0, number=1,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='IS_FILE', index=1, number=2,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='IS_SYMLINK', index=2, number=3,
      options=None,
      type=None),
  ],
  containing_type=None,
  options=None,
  serialized_start=1918,
  serialized_end=1969,
)

_NAMENODEREGISTRATIONPROTO_NAMENODEROLEPROTO = descriptor.EnumDescriptor(
  name='NamenodeRoleProto',
  full_name='hadoop.hdfs.NamenodeRegistrationProto.NamenodeRoleProto',
  filename=None,
  file=DESCRIPTOR,
  values=[
    descriptor.EnumValueDescriptor(
      name='NAMENODE', index=0, number=1,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='BACKUP', index=1, number=2,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='CHECKPOINT', index=2, number=3,
      options=None,
      type=None),
  ],
  containing_type=None,
  options=None,
  serialized_start=3178,
  serialized_end=3239,
)

_NAMENODECOMMANDPROTO_TYPE = descriptor.EnumDescriptor(
  name='Type',
  full_name='hadoop.hdfs.NamenodeCommandProto.Type',
  filename=None,
  file=DESCRIPTOR,
  values=[
    descriptor.EnumValueDescriptor(
      name='NamenodeCommand', index=0, number=0,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='CheckPointCommand', index=1, number=1,
      options=None,
      type=None),
  ],
  containing_type=None,
  options=None,
  serialized_start=3556,
  serialized_end=3606,
)


_EXTENDEDBLOCKPROTO = descriptor.Descriptor(
  name='ExtendedBlockProto',
  full_name='hadoop.hdfs.ExtendedBlockProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='poolId', full_name='hadoop.hdfs.ExtendedBlockProto.poolId', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='blockId', full_name='hadoop.hdfs.ExtendedBlockProto.blockId', index=1,
      number=2, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='generationStamp', full_name='hadoop.hdfs.ExtendedBlockProto.generationStamp', index=2,
      number=3, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='numBytes', full_name='hadoop.hdfs.ExtendedBlockProto.numBytes', index=3,
      number=4, type=4, cpp_type=4, label=1,
      has_default_value=True, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=43,
  serialized_end=142,
)


_DATANODEIDPROTO = descriptor.Descriptor(
  name='DatanodeIDProto',
  full_name='hadoop.hdfs.DatanodeIDProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='ipAddr', full_name='hadoop.hdfs.DatanodeIDProto.ipAddr', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='hostName', full_name='hadoop.hdfs.DatanodeIDProto.hostName', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='storageID', full_name='hadoop.hdfs.DatanodeIDProto.storageID', index=2,
      number=3, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='xferPort', full_name='hadoop.hdfs.DatanodeIDProto.xferPort', index=3,
      number=4, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='infoPort', full_name='hadoop.hdfs.DatanodeIDProto.infoPort', index=4,
      number=5, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='ipcPort', full_name='hadoop.hdfs.DatanodeIDProto.ipcPort', index=5,
      number=6, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='infoSecurePort', full_name='hadoop.hdfs.DatanodeIDProto.infoSecurePort', index=6,
      number=7, type=13, cpp_type=3, label=1,
      has_default_value=True, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=145,
  serialized_end=295,
)


_DATANODEINFOSPROTO = descriptor.Descriptor(
  name='DatanodeInfosProto',
  full_name='hadoop.hdfs.DatanodeInfosProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='datanodes', full_name='hadoop.hdfs.DatanodeInfosProto.datanodes', index=0,
      number=1, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=297,
  serialized_end=368,
)


_DATANODEINFOPROTO = descriptor.Descriptor(
  name='DatanodeInfoProto',
  full_name='hadoop.hdfs.DatanodeInfoProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='id', full_name='hadoop.hdfs.DatanodeInfoProto.id', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='capacity', full_name='hadoop.hdfs.DatanodeInfoProto.capacity', index=1,
      number=2, type=4, cpp_type=4, label=1,
      has_default_value=True, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='dfsUsed', full_name='hadoop.hdfs.DatanodeInfoProto.dfsUsed', index=2,
      number=3, type=4, cpp_type=4, label=1,
      has_default_value=True, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='remaining', full_name='hadoop.hdfs.DatanodeInfoProto.remaining', index=3,
      number=4, type=4, cpp_type=4, label=1,
      has_default_value=True, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='blockPoolUsed', full_name='hadoop.hdfs.DatanodeInfoProto.blockPoolUsed', index=4,
      number=5, type=4, cpp_type=4, label=1,
      has_default_value=True, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='lastUpdate', full_name='hadoop.hdfs.DatanodeInfoProto.lastUpdate', index=5,
      number=6, type=4, cpp_type=4, label=1,
      has_default_value=True, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='xceiverCount', full_name='hadoop.hdfs.DatanodeInfoProto.xceiverCount', index=6,
      number=7, type=13, cpp_type=3, label=1,
      has_default_value=True, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='location', full_name='hadoop.hdfs.DatanodeInfoProto.location', index=7,
      number=8, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='adminState', full_name='hadoop.hdfs.DatanodeInfoProto.adminState', index=8,
      number=10, type=14, cpp_type=8, label=1,
      has_default_value=True, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
    _DATANODEINFOPROTO_ADMINSTATE,
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=371,
  serialized_end=733,
)


_CONTENTSUMMARYPROTO = descriptor.Descriptor(
  name='ContentSummaryProto',
  full_name='hadoop.hdfs.ContentSummaryProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='length', full_name='hadoop.hdfs.ContentSummaryProto.length', index=0,
      number=1, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='fileCount', full_name='hadoop.hdfs.ContentSummaryProto.fileCount', index=1,
      number=2, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='directoryCount', full_name='hadoop.hdfs.ContentSummaryProto.directoryCount', index=2,
      number=3, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='quota', full_name='hadoop.hdfs.ContentSummaryProto.quota', index=3,
      number=4, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='spaceConsumed', full_name='hadoop.hdfs.ContentSummaryProto.spaceConsumed', index=4,
      number=5, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='spaceQuota', full_name='hadoop.hdfs.ContentSummaryProto.spaceQuota', index=5,
      number=6, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=736,
  serialized_end=874,
)


_CORRUPTFILEBLOCKSPROTO = descriptor.Descriptor(
  name='CorruptFileBlocksProto',
  full_name='hadoop.hdfs.CorruptFileBlocksProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='files', full_name='hadoop.hdfs.CorruptFileBlocksProto.files', index=0,
      number=1, type=9, cpp_type=9, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='cookie', full_name='hadoop.hdfs.CorruptFileBlocksProto.cookie', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=876,
  serialized_end=931,
)


_FSPERMISSIONPROTO = descriptor.Descriptor(
  name='FsPermissionProto',
  full_name='hadoop.hdfs.FsPermissionProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='perm', full_name='hadoop.hdfs.FsPermissionProto.perm', index=0,
      number=1, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=933,
  serialized_end=966,
)


_LOCATEDBLOCKPROTO = descriptor.Descriptor(
  name='LocatedBlockProto',
  full_name='hadoop.hdfs.LocatedBlockProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='b', full_name='hadoop.hdfs.LocatedBlockProto.b', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='offset', full_name='hadoop.hdfs.LocatedBlockProto.offset', index=1,
      number=2, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='locs', full_name='hadoop.hdfs.LocatedBlockProto.locs', index=2,
      number=3, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='corrupt', full_name='hadoop.hdfs.LocatedBlockProto.corrupt', index=3,
      number=4, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='blockToken', full_name='hadoop.hdfs.LocatedBlockProto.blockToken', index=4,
      number=5, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=969,
  serialized_end=1158,
)


_DATAENCRYPTIONKEYPROTO = descriptor.Descriptor(
  name='DataEncryptionKeyProto',
  full_name='hadoop.hdfs.DataEncryptionKeyProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='keyId', full_name='hadoop.hdfs.DataEncryptionKeyProto.keyId', index=0,
      number=1, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='blockPoolId', full_name='hadoop.hdfs.DataEncryptionKeyProto.blockPoolId', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='nonce', full_name='hadoop.hdfs.DataEncryptionKeyProto.nonce', index=2,
      number=3, type=12, cpp_type=9, label=2,
      has_default_value=False, default_value="",
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='encryptionKey', full_name='hadoop.hdfs.DataEncryptionKeyProto.encryptionKey', index=3,
      number=4, type=12, cpp_type=9, label=2,
      has_default_value=False, default_value="",
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='expiryDate', full_name='hadoop.hdfs.DataEncryptionKeyProto.expiryDate', index=4,
      number=5, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='encryptionAlgorithm', full_name='hadoop.hdfs.DataEncryptionKeyProto.encryptionAlgorithm', index=5,
      number=6, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1161,
  serialized_end=1308,
)


_LOCATEDBLOCKSPROTO = descriptor.Descriptor(
  name='LocatedBlocksProto',
  full_name='hadoop.hdfs.LocatedBlocksProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='fileLength', full_name='hadoop.hdfs.LocatedBlocksProto.fileLength', index=0,
      number=1, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='blocks', full_name='hadoop.hdfs.LocatedBlocksProto.blocks', index=1,
      number=2, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='underConstruction', full_name='hadoop.hdfs.LocatedBlocksProto.underConstruction', index=2,
      number=3, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='lastBlock', full_name='hadoop.hdfs.LocatedBlocksProto.lastBlock', index=3,
      number=4, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='isLastBlockComplete', full_name='hadoop.hdfs.LocatedBlocksProto.isLastBlockComplete', index=4,
      number=5, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1311,
  serialized_end=1506,
)


_HDFSFILESTATUSPROTO = descriptor.Descriptor(
  name='HdfsFileStatusProto',
  full_name='hadoop.hdfs.HdfsFileStatusProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='fileType', full_name='hadoop.hdfs.HdfsFileStatusProto.fileType', index=0,
      number=1, type=14, cpp_type=8, label=2,
      has_default_value=False, default_value=1,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='path', full_name='hadoop.hdfs.HdfsFileStatusProto.path', index=1,
      number=2, type=12, cpp_type=9, label=2,
      has_default_value=False, default_value="",
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='length', full_name='hadoop.hdfs.HdfsFileStatusProto.length', index=2,
      number=3, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='permission', full_name='hadoop.hdfs.HdfsFileStatusProto.permission', index=3,
      number=4, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='owner', full_name='hadoop.hdfs.HdfsFileStatusProto.owner', index=4,
      number=5, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='group', full_name='hadoop.hdfs.HdfsFileStatusProto.group', index=5,
      number=6, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='modification_time', full_name='hadoop.hdfs.HdfsFileStatusProto.modification_time', index=6,
      number=7, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='access_time', full_name='hadoop.hdfs.HdfsFileStatusProto.access_time', index=7,
      number=8, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='symlink', full_name='hadoop.hdfs.HdfsFileStatusProto.symlink', index=8,
      number=9, type=12, cpp_type=9, label=1,
      has_default_value=False, default_value="",
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='block_replication', full_name='hadoop.hdfs.HdfsFileStatusProto.block_replication', index=9,
      number=10, type=13, cpp_type=3, label=1,
      has_default_value=True, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='blocksize', full_name='hadoop.hdfs.HdfsFileStatusProto.blocksize', index=10,
      number=11, type=4, cpp_type=4, label=1,
      has_default_value=True, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='locations', full_name='hadoop.hdfs.HdfsFileStatusProto.locations', index=11,
      number=12, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='fileId', full_name='hadoop.hdfs.HdfsFileStatusProto.fileId', index=12,
      number=13, type=4, cpp_type=4, label=1,
      has_default_value=True, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='childrenNum', full_name='hadoop.hdfs.HdfsFileStatusProto.childrenNum', index=13,
      number=14, type=5, cpp_type=1, label=1,
      has_default_value=True, default_value=-1,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
    _HDFSFILESTATUSPROTO_FILETYPE,
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1509,
  serialized_end=1969,
)


_FSSERVERDEFAULTSPROTO = descriptor.Descriptor(
  name='FsServerDefaultsProto',
  full_name='hadoop.hdfs.FsServerDefaultsProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='blockSize', full_name='hadoop.hdfs.FsServerDefaultsProto.blockSize', index=0,
      number=1, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='bytesPerChecksum', full_name='hadoop.hdfs.FsServerDefaultsProto.bytesPerChecksum', index=1,
      number=2, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='writePacketSize', full_name='hadoop.hdfs.FsServerDefaultsProto.writePacketSize', index=2,
      number=3, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='replication', full_name='hadoop.hdfs.FsServerDefaultsProto.replication', index=3,
      number=4, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='fileBufferSize', full_name='hadoop.hdfs.FsServerDefaultsProto.fileBufferSize', index=4,
      number=5, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='encryptDataTransfer', full_name='hadoop.hdfs.FsServerDefaultsProto.encryptDataTransfer', index=5,
      number=6, type=8, cpp_type=7, label=1,
      has_default_value=True, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='trashInterval', full_name='hadoop.hdfs.FsServerDefaultsProto.trashInterval', index=6,
      number=7, type=4, cpp_type=4, label=1,
      has_default_value=True, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='checksumType', full_name='hadoop.hdfs.FsServerDefaultsProto.checksumType', index=7,
      number=8, type=14, cpp_type=8, label=1,
      has_default_value=True, default_value=1,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1972,
  serialized_end=2242,
)


_DIRECTORYLISTINGPROTO = descriptor.Descriptor(
  name='DirectoryListingProto',
  full_name='hadoop.hdfs.DirectoryListingProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='partialListing', full_name='hadoop.hdfs.DirectoryListingProto.partialListing', index=0,
      number=1, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='remainingEntries', full_name='hadoop.hdfs.DirectoryListingProto.remainingEntries', index=1,
      number=2, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2244,
  serialized_end=2351,
)


_SNAPSHOTTABLEDIRECTORYSTATUSPROTO = descriptor.Descriptor(
  name='SnapshottableDirectoryStatusProto',
  full_name='hadoop.hdfs.SnapshottableDirectoryStatusProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='dirStatus', full_name='hadoop.hdfs.SnapshottableDirectoryStatusProto.dirStatus', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='snapshot_quota', full_name='hadoop.hdfs.SnapshottableDirectoryStatusProto.snapshot_quota', index=1,
      number=2, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='snapshot_number', full_name='hadoop.hdfs.SnapshottableDirectoryStatusProto.snapshot_number', index=2,
      number=3, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='parent_fullpath', full_name='hadoop.hdfs.SnapshottableDirectoryStatusProto.parent_fullpath', index=3,
      number=4, type=12, cpp_type=9, label=2,
      has_default_value=False, default_value="",
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2354,
  serialized_end=2516,
)


_SNAPSHOTTABLEDIRECTORYLISTINGPROTO = descriptor.Descriptor(
  name='SnapshottableDirectoryListingProto',
  full_name='hadoop.hdfs.SnapshottableDirectoryListingProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='snapshottableDirListing', full_name='hadoop.hdfs.SnapshottableDirectoryListingProto.snapshottableDirListing', index=0,
      number=1, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2518,
  serialized_end=2635,
)


_SNAPSHOTDIFFREPORTENTRYPROTO = descriptor.Descriptor(
  name='SnapshotDiffReportEntryProto',
  full_name='hadoop.hdfs.SnapshotDiffReportEntryProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='fullpath', full_name='hadoop.hdfs.SnapshotDiffReportEntryProto.fullpath', index=0,
      number=1, type=12, cpp_type=9, label=2,
      has_default_value=False, default_value="",
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='modificationLabel', full_name='hadoop.hdfs.SnapshotDiffReportEntryProto.modificationLabel', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2637,
  serialized_end=2712,
)


_SNAPSHOTDIFFREPORTPROTO = descriptor.Descriptor(
  name='SnapshotDiffReportProto',
  full_name='hadoop.hdfs.SnapshotDiffReportProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='snapshotRoot', full_name='hadoop.hdfs.SnapshotDiffReportProto.snapshotRoot', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='fromSnapshot', full_name='hadoop.hdfs.SnapshotDiffReportProto.fromSnapshot', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='toSnapshot', full_name='hadoop.hdfs.SnapshotDiffReportProto.toSnapshot', index=2,
      number=3, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='diffReportEntries', full_name='hadoop.hdfs.SnapshotDiffReportProto.diffReportEntries', index=3,
      number=4, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2715,
  serialized_end=2874,
)


_STORAGEINFOPROTO = descriptor.Descriptor(
  name='StorageInfoProto',
  full_name='hadoop.hdfs.StorageInfoProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='layoutVersion', full_name='hadoop.hdfs.StorageInfoProto.layoutVersion', index=0,
      number=1, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='namespceID', full_name='hadoop.hdfs.StorageInfoProto.namespceID', index=1,
      number=2, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='clusterID', full_name='hadoop.hdfs.StorageInfoProto.clusterID', index=2,
      number=3, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='cTime', full_name='hadoop.hdfs.StorageInfoProto.cTime', index=3,
      number=4, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2876,
  serialized_end=2971,
)


_NAMENODEREGISTRATIONPROTO = descriptor.Descriptor(
  name='NamenodeRegistrationProto',
  full_name='hadoop.hdfs.NamenodeRegistrationProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='rpcAddress', full_name='hadoop.hdfs.NamenodeRegistrationProto.rpcAddress', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='httpAddress', full_name='hadoop.hdfs.NamenodeRegistrationProto.httpAddress', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='storageInfo', full_name='hadoop.hdfs.NamenodeRegistrationProto.storageInfo', index=2,
      number=3, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='role', full_name='hadoop.hdfs.NamenodeRegistrationProto.role', index=3,
      number=4, type=14, cpp_type=8, label=1,
      has_default_value=True, default_value=1,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
    _NAMENODEREGISTRATIONPROTO_NAMENODEROLEPROTO,
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=2974,
  serialized_end=3239,
)


_CHECKPOINTSIGNATUREPROTO = descriptor.Descriptor(
  name='CheckpointSignatureProto',
  full_name='hadoop.hdfs.CheckpointSignatureProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='blockPoolId', full_name='hadoop.hdfs.CheckpointSignatureProto.blockPoolId', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='mostRecentCheckpointTxId', full_name='hadoop.hdfs.CheckpointSignatureProto.mostRecentCheckpointTxId', index=1,
      number=2, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='curSegmentTxId', full_name='hadoop.hdfs.CheckpointSignatureProto.curSegmentTxId', index=2,
      number=3, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='storageInfo', full_name='hadoop.hdfs.CheckpointSignatureProto.storageInfo', index=3,
      number=4, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3242,
  serialized_end=3399,
)


_NAMENODECOMMANDPROTO = descriptor.Descriptor(
  name='NamenodeCommandProto',
  full_name='hadoop.hdfs.NamenodeCommandProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='action', full_name='hadoop.hdfs.NamenodeCommandProto.action', index=0,
      number=1, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='type', full_name='hadoop.hdfs.NamenodeCommandProto.type', index=1,
      number=2, type=14, cpp_type=8, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='checkpointCmd', full_name='hadoop.hdfs.NamenodeCommandProto.checkpointCmd', index=2,
      number=3, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
    _NAMENODECOMMANDPROTO_TYPE,
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3402,
  serialized_end=3606,
)


_CHECKPOINTCOMMANDPROTO = descriptor.Descriptor(
  name='CheckpointCommandProto',
  full_name='hadoop.hdfs.CheckpointCommandProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='signature', full_name='hadoop.hdfs.CheckpointCommandProto.signature', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='needToReturnImage', full_name='hadoop.hdfs.CheckpointCommandProto.needToReturnImage', index=1,
      number=2, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3608,
  serialized_end=3717,
)


_BLOCKPROTO = descriptor.Descriptor(
  name='BlockProto',
  full_name='hadoop.hdfs.BlockProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='blockId', full_name='hadoop.hdfs.BlockProto.blockId', index=0,
      number=1, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='genStamp', full_name='hadoop.hdfs.BlockProto.genStamp', index=1,
      number=2, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='numBytes', full_name='hadoop.hdfs.BlockProto.numBytes', index=2,
      number=3, type=4, cpp_type=4, label=1,
      has_default_value=True, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3719,
  serialized_end=3787,
)


_BLOCKWITHLOCATIONSPROTO = descriptor.Descriptor(
  name='BlockWithLocationsProto',
  full_name='hadoop.hdfs.BlockWithLocationsProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='block', full_name='hadoop.hdfs.BlockWithLocationsProto.block', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='storageIDs', full_name='hadoop.hdfs.BlockWithLocationsProto.storageIDs', index=1,
      number=2, type=9, cpp_type=9, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3789,
  serialized_end=3874,
)


_BLOCKSWITHLOCATIONSPROTO = descriptor.Descriptor(
  name='BlocksWithLocationsProto',
  full_name='hadoop.hdfs.BlocksWithLocationsProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='blocks', full_name='hadoop.hdfs.BlocksWithLocationsProto.blocks', index=0,
      number=1, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3876,
  serialized_end=3956,
)


_REMOTEEDITLOGPROTO = descriptor.Descriptor(
  name='RemoteEditLogProto',
  full_name='hadoop.hdfs.RemoteEditLogProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='startTxId', full_name='hadoop.hdfs.RemoteEditLogProto.startTxId', index=0,
      number=1, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='endTxId', full_name='hadoop.hdfs.RemoteEditLogProto.endTxId', index=1,
      number=2, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='isInProgress', full_name='hadoop.hdfs.RemoteEditLogProto.isInProgress', index=2,
      number=3, type=8, cpp_type=7, label=1,
      has_default_value=True, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=3958,
  serialized_end=4043,
)


_REMOTEEDITLOGMANIFESTPROTO = descriptor.Descriptor(
  name='RemoteEditLogManifestProto',
  full_name='hadoop.hdfs.RemoteEditLogManifestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='logs', full_name='hadoop.hdfs.RemoteEditLogManifestProto.logs', index=0,
      number=1, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4045,
  serialized_end=4120,
)


_NAMESPACEINFOPROTO = descriptor.Descriptor(
  name='NamespaceInfoProto',
  full_name='hadoop.hdfs.NamespaceInfoProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='buildVersion', full_name='hadoop.hdfs.NamespaceInfoProto.buildVersion', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='unused', full_name='hadoop.hdfs.NamespaceInfoProto.unused', index=1,
      number=2, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='blockPoolID', full_name='hadoop.hdfs.NamespaceInfoProto.blockPoolID', index=2,
      number=3, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='storageInfo', full_name='hadoop.hdfs.NamespaceInfoProto.storageInfo', index=3,
      number=4, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='softwareVersion', full_name='hadoop.hdfs.NamespaceInfoProto.softwareVersion', index=4,
      number=5, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4123,
  serialized_end=4279,
)


_BLOCKKEYPROTO = descriptor.Descriptor(
  name='BlockKeyProto',
  full_name='hadoop.hdfs.BlockKeyProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='keyId', full_name='hadoop.hdfs.BlockKeyProto.keyId', index=0,
      number=1, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='expiryDate', full_name='hadoop.hdfs.BlockKeyProto.expiryDate', index=1,
      number=2, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='keyBytes', full_name='hadoop.hdfs.BlockKeyProto.keyBytes', index=2,
      number=3, type=12, cpp_type=9, label=1,
      has_default_value=False, default_value="",
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4281,
  serialized_end=4349,
)


_EXPORTEDBLOCKKEYSPROTO = descriptor.Descriptor(
  name='ExportedBlockKeysProto',
  full_name='hadoop.hdfs.ExportedBlockKeysProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='isBlockTokenEnabled', full_name='hadoop.hdfs.ExportedBlockKeysProto.isBlockTokenEnabled', index=0,
      number=1, type=8, cpp_type=7, label=2,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='keyUpdateInterval', full_name='hadoop.hdfs.ExportedBlockKeysProto.keyUpdateInterval', index=1,
      number=2, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='tokenLifeTime', full_name='hadoop.hdfs.ExportedBlockKeysProto.tokenLifeTime', index=2,
      number=3, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='currentKey', full_name='hadoop.hdfs.ExportedBlockKeysProto.currentKey', index=3,
      number=4, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='allKeys', full_name='hadoop.hdfs.ExportedBlockKeysProto.allKeys', index=4,
      number=5, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4352,
  serialized_end=4548,
)


_RECOVERINGBLOCKPROTO = descriptor.Descriptor(
  name='RecoveringBlockProto',
  full_name='hadoop.hdfs.RecoveringBlockProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='newGenStamp', full_name='hadoop.hdfs.RecoveringBlockProto.newGenStamp', index=0,
      number=1, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='block', full_name='hadoop.hdfs.RecoveringBlockProto.block', index=1,
      number=2, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4550,
  serialized_end=4640,
)


_VERSIONREQUESTPROTO = descriptor.Descriptor(
  name='VersionRequestProto',
  full_name='hadoop.hdfs.VersionRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4642,
  serialized_end=4663,
)


_VERSIONRESPONSEPROTO = descriptor.Descriptor(
  name='VersionResponseProto',
  full_name='hadoop.hdfs.VersionResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='info', full_name='hadoop.hdfs.VersionResponseProto.info', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4665,
  serialized_end=4734,
)


_SNAPSHOTINFOPROTO = descriptor.Descriptor(
  name='SnapshotInfoProto',
  full_name='hadoop.hdfs.SnapshotInfoProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='snapshotName', full_name='hadoop.hdfs.SnapshotInfoProto.snapshotName', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='snapshotRoot', full_name='hadoop.hdfs.SnapshotInfoProto.snapshotRoot', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='permission', full_name='hadoop.hdfs.SnapshotInfoProto.permission', index=2,
      number=3, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='owner', full_name='hadoop.hdfs.SnapshotInfoProto.owner', index=3,
      number=4, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='group', full_name='hadoop.hdfs.SnapshotInfoProto.group', index=4,
      number=5, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='createTime', full_name='hadoop.hdfs.SnapshotInfoProto.createTime', index=5,
      number=6, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=4737,
  serialized_end=4902,
)

_DATANODEINFOSPROTO.fields_by_name['datanodes'].message_type = _DATANODEINFOPROTO
_DATANODEINFOPROTO.fields_by_name['id'].message_type = _DATANODEIDPROTO
_DATANODEINFOPROTO.fields_by_name['adminState'].enum_type = _DATANODEINFOPROTO_ADMINSTATE
_DATANODEINFOPROTO_ADMINSTATE.containing_type = _DATANODEINFOPROTO;
_LOCATEDBLOCKPROTO.fields_by_name['b'].message_type = _EXTENDEDBLOCKPROTO
_LOCATEDBLOCKPROTO.fields_by_name['locs'].message_type = _DATANODEINFOPROTO
_LOCATEDBLOCKPROTO.fields_by_name['blockToken'].message_type = Security_pb2._TOKENPROTO
_LOCATEDBLOCKSPROTO.fields_by_name['blocks'].message_type = _LOCATEDBLOCKPROTO
_LOCATEDBLOCKSPROTO.fields_by_name['lastBlock'].message_type = _LOCATEDBLOCKPROTO
_HDFSFILESTATUSPROTO.fields_by_name['fileType'].enum_type = _HDFSFILESTATUSPROTO_FILETYPE
_HDFSFILESTATUSPROTO.fields_by_name['permission'].message_type = _FSPERMISSIONPROTO
_HDFSFILESTATUSPROTO.fields_by_name['locations'].message_type = _LOCATEDBLOCKSPROTO
_HDFSFILESTATUSPROTO_FILETYPE.containing_type = _HDFSFILESTATUSPROTO;
_FSSERVERDEFAULTSPROTO.fields_by_name['checksumType'].enum_type = _CHECKSUMTYPEPROTO
_DIRECTORYLISTINGPROTO.fields_by_name['partialListing'].message_type = _HDFSFILESTATUSPROTO
_SNAPSHOTTABLEDIRECTORYSTATUSPROTO.fields_by_name['dirStatus'].message_type = _HDFSFILESTATUSPROTO
_SNAPSHOTTABLEDIRECTORYLISTINGPROTO.fields_by_name['snapshottableDirListing'].message_type = _SNAPSHOTTABLEDIRECTORYSTATUSPROTO
_SNAPSHOTDIFFREPORTPROTO.fields_by_name['diffReportEntries'].message_type = _SNAPSHOTDIFFREPORTENTRYPROTO
_NAMENODEREGISTRATIONPROTO.fields_by_name['storageInfo'].message_type = _STORAGEINFOPROTO
_NAMENODEREGISTRATIONPROTO.fields_by_name['role'].enum_type = _NAMENODEREGISTRATIONPROTO_NAMENODEROLEPROTO
_NAMENODEREGISTRATIONPROTO_NAMENODEROLEPROTO.containing_type = _NAMENODEREGISTRATIONPROTO;
_CHECKPOINTSIGNATUREPROTO.fields_by_name['storageInfo'].message_type = _STORAGEINFOPROTO
_NAMENODECOMMANDPROTO.fields_by_name['type'].enum_type = _NAMENODECOMMANDPROTO_TYPE
_NAMENODECOMMANDPROTO.fields_by_name['checkpointCmd'].message_type = _CHECKPOINTCOMMANDPROTO
_NAMENODECOMMANDPROTO_TYPE.containing_type = _NAMENODECOMMANDPROTO;
_CHECKPOINTCOMMANDPROTO.fields_by_name['signature'].message_type = _CHECKPOINTSIGNATUREPROTO
_BLOCKWITHLOCATIONSPROTO.fields_by_name['block'].message_type = _BLOCKPROTO
_BLOCKSWITHLOCATIONSPROTO.fields_by_name['blocks'].message_type = _BLOCKWITHLOCATIONSPROTO
_REMOTEEDITLOGMANIFESTPROTO.fields_by_name['logs'].message_type = _REMOTEEDITLOGPROTO
_NAMESPACEINFOPROTO.fields_by_name['storageInfo'].message_type = _STORAGEINFOPROTO
_EXPORTEDBLOCKKEYSPROTO.fields_by_name['currentKey'].message_type = _BLOCKKEYPROTO
_EXPORTEDBLOCKKEYSPROTO.fields_by_name['allKeys'].message_type = _BLOCKKEYPROTO
_RECOVERINGBLOCKPROTO.fields_by_name['block'].message_type = _LOCATEDBLOCKPROTO
_VERSIONRESPONSEPROTO.fields_by_name['info'].message_type = _NAMESPACEINFOPROTO
_SNAPSHOTINFOPROTO.fields_by_name['permission'].message_type = _FSPERMISSIONPROTO
DESCRIPTOR.message_types_by_name['ExtendedBlockProto'] = _EXTENDEDBLOCKPROTO
DESCRIPTOR.message_types_by_name['DatanodeIDProto'] = _DATANODEIDPROTO
DESCRIPTOR.message_types_by_name['DatanodeInfosProto'] = _DATANODEINFOSPROTO
DESCRIPTOR.message_types_by_name['DatanodeInfoProto'] = _DATANODEINFOPROTO
DESCRIPTOR.message_types_by_name['ContentSummaryProto'] = _CONTENTSUMMARYPROTO
DESCRIPTOR.message_types_by_name['CorruptFileBlocksProto'] = _CORRUPTFILEBLOCKSPROTO
DESCRIPTOR.message_types_by_name['FsPermissionProto'] = _FSPERMISSIONPROTO
DESCRIPTOR.message_types_by_name['LocatedBlockProto'] = _LOCATEDBLOCKPROTO
DESCRIPTOR.message_types_by_name['DataEncryptionKeyProto'] = _DATAENCRYPTIONKEYPROTO
DESCRIPTOR.message_types_by_name['LocatedBlocksProto'] = _LOCATEDBLOCKSPROTO
DESCRIPTOR.message_types_by_name['HdfsFileStatusProto'] = _HDFSFILESTATUSPROTO
DESCRIPTOR.message_types_by_name['FsServerDefaultsProto'] = _FSSERVERDEFAULTSPROTO
DESCRIPTOR.message_types_by_name['DirectoryListingProto'] = _DIRECTORYLISTINGPROTO
DESCRIPTOR.message_types_by_name['SnapshottableDirectoryStatusProto'] = _SNAPSHOTTABLEDIRECTORYSTATUSPROTO
DESCRIPTOR.message_types_by_name['SnapshottableDirectoryListingProto'] = _SNAPSHOTTABLEDIRECTORYLISTINGPROTO
DESCRIPTOR.message_types_by_name['SnapshotDiffReportEntryProto'] = _SNAPSHOTDIFFREPORTENTRYPROTO
DESCRIPTOR.message_types_by_name['SnapshotDiffReportProto'] = _SNAPSHOTDIFFREPORTPROTO
DESCRIPTOR.message_types_by_name['StorageInfoProto'] = _STORAGEINFOPROTO
DESCRIPTOR.message_types_by_name['NamenodeRegistrationProto'] = _NAMENODEREGISTRATIONPROTO
DESCRIPTOR.message_types_by_name['CheckpointSignatureProto'] = _CHECKPOINTSIGNATUREPROTO
DESCRIPTOR.message_types_by_name['NamenodeCommandProto'] = _NAMENODECOMMANDPROTO
DESCRIPTOR.message_types_by_name['CheckpointCommandProto'] = _CHECKPOINTCOMMANDPROTO
DESCRIPTOR.message_types_by_name['BlockProto'] = _BLOCKPROTO
DESCRIPTOR.message_types_by_name['BlockWithLocationsProto'] = _BLOCKWITHLOCATIONSPROTO
DESCRIPTOR.message_types_by_name['BlocksWithLocationsProto'] = _BLOCKSWITHLOCATIONSPROTO
DESCRIPTOR.message_types_by_name['RemoteEditLogProto'] = _REMOTEEDITLOGPROTO
DESCRIPTOR.message_types_by_name['RemoteEditLogManifestProto'] = _REMOTEEDITLOGMANIFESTPROTO
DESCRIPTOR.message_types_by_name['NamespaceInfoProto'] = _NAMESPACEINFOPROTO
DESCRIPTOR.message_types_by_name['BlockKeyProto'] = _BLOCKKEYPROTO
DESCRIPTOR.message_types_by_name['ExportedBlockKeysProto'] = _EXPORTEDBLOCKKEYSPROTO
DESCRIPTOR.message_types_by_name['RecoveringBlockProto'] = _RECOVERINGBLOCKPROTO
DESCRIPTOR.message_types_by_name['VersionRequestProto'] = _VERSIONREQUESTPROTO
DESCRIPTOR.message_types_by_name['VersionResponseProto'] = _VERSIONRESPONSEPROTO
DESCRIPTOR.message_types_by_name['SnapshotInfoProto'] = _SNAPSHOTINFOPROTO

class ExtendedBlockProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _EXTENDEDBLOCKPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.ExtendedBlockProto)

class DatanodeIDProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _DATANODEIDPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.DatanodeIDProto)

class DatanodeInfosProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _DATANODEINFOSPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.DatanodeInfosProto)

class DatanodeInfoProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _DATANODEINFOPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.DatanodeInfoProto)

class ContentSummaryProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _CONTENTSUMMARYPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.ContentSummaryProto)

class CorruptFileBlocksProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _CORRUPTFILEBLOCKSPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.CorruptFileBlocksProto)

class FsPermissionProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _FSPERMISSIONPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.FsPermissionProto)

class LocatedBlockProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _LOCATEDBLOCKPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.LocatedBlockProto)

class DataEncryptionKeyProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _DATAENCRYPTIONKEYPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.DataEncryptionKeyProto)

class LocatedBlocksProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _LOCATEDBLOCKSPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.LocatedBlocksProto)

class HdfsFileStatusProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _HDFSFILESTATUSPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.HdfsFileStatusProto)

class FsServerDefaultsProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _FSSERVERDEFAULTSPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.FsServerDefaultsProto)

class DirectoryListingProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _DIRECTORYLISTINGPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.DirectoryListingProto)

class SnapshottableDirectoryStatusProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SNAPSHOTTABLEDIRECTORYSTATUSPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SnapshottableDirectoryStatusProto)

class SnapshottableDirectoryListingProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SNAPSHOTTABLEDIRECTORYLISTINGPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SnapshottableDirectoryListingProto)

class SnapshotDiffReportEntryProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SNAPSHOTDIFFREPORTENTRYPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SnapshotDiffReportEntryProto)

class SnapshotDiffReportProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SNAPSHOTDIFFREPORTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SnapshotDiffReportProto)

class StorageInfoProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _STORAGEINFOPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.StorageInfoProto)

class NamenodeRegistrationProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _NAMENODEREGISTRATIONPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.NamenodeRegistrationProto)

class CheckpointSignatureProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _CHECKPOINTSIGNATUREPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.CheckpointSignatureProto)

class NamenodeCommandProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _NAMENODECOMMANDPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.NamenodeCommandProto)

class CheckpointCommandProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _CHECKPOINTCOMMANDPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.CheckpointCommandProto)

class BlockProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _BLOCKPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.BlockProto)

class BlockWithLocationsProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _BLOCKWITHLOCATIONSPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.BlockWithLocationsProto)

class BlocksWithLocationsProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _BLOCKSWITHLOCATIONSPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.BlocksWithLocationsProto)

class RemoteEditLogProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _REMOTEEDITLOGPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.RemoteEditLogProto)

class RemoteEditLogManifestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _REMOTEEDITLOGMANIFESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.RemoteEditLogManifestProto)

class NamespaceInfoProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _NAMESPACEINFOPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.NamespaceInfoProto)

class BlockKeyProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _BLOCKKEYPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.BlockKeyProto)

class ExportedBlockKeysProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _EXPORTEDBLOCKKEYSPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.ExportedBlockKeysProto)

class RecoveringBlockProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _RECOVERINGBLOCKPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.RecoveringBlockProto)

class VersionRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _VERSIONREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.VersionRequestProto)

class VersionResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _VERSIONRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.VersionResponseProto)

class SnapshotInfoProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _SNAPSHOTINFOPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.hdfs.SnapshotInfoProto)

# @@protoc_insertion_point(module_scope)

########NEW FILE########
__FILENAME__ = IpcConnectionContext_pb2
# Generated by the protocol buffer compiler.  DO NOT EDIT!

from google.protobuf import descriptor
from google.protobuf import message
from google.protobuf import reflection
from google.protobuf import descriptor_pb2
# @@protoc_insertion_point(imports)



DESCRIPTOR = descriptor.FileDescriptor(
  name='IpcConnectionContext.proto',
  package='hadoop.common',
  serialized_pb='\n\x1aIpcConnectionContext.proto\x12\rhadoop.common\"?\n\x14UserInformationProto\x12\x15\n\reffectiveUser\x18\x01 \x01(\t\x12\x10\n\x08realUser\x18\x02 \x01(\t\"d\n\x19IpcConnectionContextProto\x12\x35\n\x08userInfo\x18\x02 \x01(\x0b\x32#.hadoop.common.UserInformationProto\x12\x10\n\x08protocol\x18\x03 \x01(\tB?\n\x1eorg.apache.hadoop.ipc.protobufB\x1aIpcConnectionContextProtos\xa0\x01\x01')




_USERINFORMATIONPROTO = descriptor.Descriptor(
  name='UserInformationProto',
  full_name='hadoop.common.UserInformationProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='effectiveUser', full_name='hadoop.common.UserInformationProto.effectiveUser', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='realUser', full_name='hadoop.common.UserInformationProto.realUser', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=45,
  serialized_end=108,
)


_IPCCONNECTIONCONTEXTPROTO = descriptor.Descriptor(
  name='IpcConnectionContextProto',
  full_name='hadoop.common.IpcConnectionContextProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='userInfo', full_name='hadoop.common.IpcConnectionContextProto.userInfo', index=0,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='protocol', full_name='hadoop.common.IpcConnectionContextProto.protocol', index=1,
      number=3, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=110,
  serialized_end=210,
)

_IPCCONNECTIONCONTEXTPROTO.fields_by_name['userInfo'].message_type = _USERINFORMATIONPROTO
DESCRIPTOR.message_types_by_name['UserInformationProto'] = _USERINFORMATIONPROTO
DESCRIPTOR.message_types_by_name['IpcConnectionContextProto'] = _IPCCONNECTIONCONTEXTPROTO

class UserInformationProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _USERINFORMATIONPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.common.UserInformationProto)

class IpcConnectionContextProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _IPCCONNECTIONCONTEXTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.common.IpcConnectionContextProto)

# @@protoc_insertion_point(module_scope)

########NEW FILE########
__FILENAME__ = ProtobufRpcEngine_pb2
# Generated by the protocol buffer compiler.  DO NOT EDIT!

from google.protobuf import descriptor
from google.protobuf import message
from google.protobuf import reflection
from google.protobuf import descriptor_pb2
# @@protoc_insertion_point(imports)



DESCRIPTOR = descriptor.FileDescriptor(
  name='ProtobufRpcEngine.proto',
  package='hadoop.common',
  serialized_pb='\n\x17ProtobufRpcEngine.proto\x12\rhadoop.common\"k\n\x12RequestHeaderProto\x12\x12\n\nmethodName\x18\x01 \x02(\t\x12\"\n\x1a\x64\x65\x63laringClassProtocolName\x18\x02 \x02(\t\x12\x1d\n\x15\x63lientProtocolVersion\x18\x03 \x02(\x04\x42<\n\x1eorg.apache.hadoop.ipc.protobufB\x17ProtobufRpcEngineProtos\xa0\x01\x01')




_REQUESTHEADERPROTO = descriptor.Descriptor(
  name='RequestHeaderProto',
  full_name='hadoop.common.RequestHeaderProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='methodName', full_name='hadoop.common.RequestHeaderProto.methodName', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='declaringClassProtocolName', full_name='hadoop.common.RequestHeaderProto.declaringClassProtocolName', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='clientProtocolVersion', full_name='hadoop.common.RequestHeaderProto.clientProtocolVersion', index=2,
      number=3, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=42,
  serialized_end=149,
)

DESCRIPTOR.message_types_by_name['RequestHeaderProto'] = _REQUESTHEADERPROTO

class RequestHeaderProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _REQUESTHEADERPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.common.RequestHeaderProto)

# @@protoc_insertion_point(module_scope)

########NEW FILE########
__FILENAME__ = RpcHeader_pb2
# Generated by the protocol buffer compiler.  DO NOT EDIT!

from google.protobuf import descriptor
from google.protobuf import message
from google.protobuf import reflection
from google.protobuf import descriptor_pb2
# @@protoc_insertion_point(imports)



DESCRIPTOR = descriptor.FileDescriptor(
  name='RpcHeader.proto',
  package='hadoop.common',
  serialized_pb='\n\x0fRpcHeader.proto\x12\rhadoop.common\"\xa2\x02\n\x15RpcRequestHeaderProto\x12,\n\x07rpcKind\x18\x01 \x01(\x0e\x32\x1b.hadoop.common.RpcKindProto\x12\x42\n\x05rpcOp\x18\x02 \x01(\x0e\x32\x33.hadoop.common.RpcRequestHeaderProto.OperationProto\x12\x0e\n\x06\x63\x61llId\x18\x03 \x02(\x11\x12\x10\n\x08\x63lientId\x18\x04 \x02(\x0c\x12\x16\n\nretryCount\x18\x05 \x01(\x11:\x02-1\"]\n\x0eOperationProto\x12\x14\n\x10RPC_FINAL_PACKET\x10\x00\x12\x1b\n\x17RPC_CONTINUATION_PACKET\x10\x01\x12\x18\n\x14RPC_CLOSE_CONNECTION\x10\x02\"\xca\x05\n\x16RpcResponseHeaderProto\x12\x0e\n\x06\x63\x61llId\x18\x01 \x02(\r\x12\x44\n\x06status\x18\x02 \x02(\x0e\x32\x34.hadoop.common.RpcResponseHeaderProto.RpcStatusProto\x12\x1b\n\x13serverIpcVersionNum\x18\x03 \x01(\r\x12\x1a\n\x12\x65xceptionClassName\x18\x04 \x01(\t\x12\x10\n\x08\x65rrorMsg\x18\x05 \x01(\t\x12L\n\x0b\x65rrorDetail\x18\x06 \x01(\x0e\x32\x37.hadoop.common.RpcResponseHeaderProto.RpcErrorCodeProto\x12\x10\n\x08\x63lientId\x18\x07 \x01(\x0c\x12\x16\n\nretryCount\x18\x08 \x01(\x11:\x02-1\"3\n\x0eRpcStatusProto\x12\x0b\n\x07SUCCESS\x10\x00\x12\t\n\x05\x45RROR\x10\x01\x12\t\n\x05\x46\x41TAL\x10\x02\"\xe1\x02\n\x11RpcErrorCodeProto\x12\x15\n\x11\x45RROR_APPLICATION\x10\x01\x12\x18\n\x14\x45RROR_NO_SUCH_METHOD\x10\x02\x12\x1a\n\x16\x45RROR_NO_SUCH_PROTOCOL\x10\x03\x12\x14\n\x10\x45RROR_RPC_SERVER\x10\x04\x12\x1e\n\x1a\x45RROR_SERIALIZING_RESPONSE\x10\x05\x12\x1e\n\x1a\x45RROR_RPC_VERSION_MISMATCH\x10\x06\x12\x11\n\rFATAL_UNKNOWN\x10\n\x12#\n\x1f\x46\x41TAL_UNSUPPORTED_SERIALIZATION\x10\x0b\x12\x1c\n\x18\x46\x41TAL_INVALID_RPC_HEADER\x10\x0c\x12\x1f\n\x1b\x46\x41TAL_DESERIALIZING_REQUEST\x10\r\x12\x1a\n\x16\x46\x41TAL_VERSION_MISMATCH\x10\x0e\x12\x16\n\x12\x46\x41TAL_UNAUTHORIZED\x10\x0f\"\xdd\x02\n\x0cRpcSaslProto\x12\x0f\n\x07version\x18\x01 \x01(\r\x12\x34\n\x05state\x18\x02 \x02(\x0e\x32%.hadoop.common.RpcSaslProto.SaslState\x12\r\n\x05token\x18\x03 \x01(\x0c\x12\x33\n\x05\x61uths\x18\x04 \x03(\x0b\x32$.hadoop.common.RpcSaslProto.SaslAuth\x1a\x64\n\x08SaslAuth\x12\x0e\n\x06method\x18\x01 \x02(\t\x12\x11\n\tmechanism\x18\x02 \x02(\t\x12\x10\n\x08protocol\x18\x03 \x01(\t\x12\x10\n\x08serverId\x18\x04 \x01(\t\x12\x11\n\tchallenge\x18\x05 \x01(\x0c\"\\\n\tSaslState\x12\x0b\n\x07SUCCESS\x10\x00\x12\r\n\tNEGOTIATE\x10\x01\x12\x0c\n\x08INITIATE\x10\x02\x12\r\n\tCHALLENGE\x10\x03\x12\x0c\n\x08RESPONSE\x10\x04\x12\x08\n\x04WRAP\x10\x05*J\n\x0cRpcKindProto\x12\x0f\n\x0bRPC_BUILTIN\x10\x00\x12\x10\n\x0cRPC_WRITABLE\x10\x01\x12\x17\n\x13RPC_PROTOCOL_BUFFER\x10\x02\x42\x34\n\x1eorg.apache.hadoop.ipc.protobufB\x0fRpcHeaderProtos\xa0\x01\x01')

_RPCKINDPROTO = descriptor.EnumDescriptor(
  name='RpcKindProto',
  full_name='hadoop.common.RpcKindProto',
  filename=None,
  file=DESCRIPTOR,
  values=[
    descriptor.EnumValueDescriptor(
      name='RPC_BUILTIN', index=0, number=0,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='RPC_WRITABLE', index=1, number=1,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='RPC_PROTOCOL_BUFFER', index=2, number=2,
      options=None,
      type=None),
  ],
  containing_type=None,
  options=None,
  serialized_start=1396,
  serialized_end=1470,
)


RPC_BUILTIN = 0
RPC_WRITABLE = 1
RPC_PROTOCOL_BUFFER = 2


_RPCREQUESTHEADERPROTO_OPERATIONPROTO = descriptor.EnumDescriptor(
  name='OperationProto',
  full_name='hadoop.common.RpcRequestHeaderProto.OperationProto',
  filename=None,
  file=DESCRIPTOR,
  values=[
    descriptor.EnumValueDescriptor(
      name='RPC_FINAL_PACKET', index=0, number=0,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='RPC_CONTINUATION_PACKET', index=1, number=1,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='RPC_CLOSE_CONNECTION', index=2, number=2,
      options=None,
      type=None),
  ],
  containing_type=None,
  options=None,
  serialized_start=232,
  serialized_end=325,
)

_RPCRESPONSEHEADERPROTO_RPCSTATUSPROTO = descriptor.EnumDescriptor(
  name='RpcStatusProto',
  full_name='hadoop.common.RpcResponseHeaderProto.RpcStatusProto',
  filename=None,
  file=DESCRIPTOR,
  values=[
    descriptor.EnumValueDescriptor(
      name='SUCCESS', index=0, number=0,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='ERROR', index=1, number=1,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='FATAL', index=2, number=2,
      options=None,
      type=None),
  ],
  containing_type=None,
  options=None,
  serialized_start=635,
  serialized_end=686,
)

_RPCRESPONSEHEADERPROTO_RPCERRORCODEPROTO = descriptor.EnumDescriptor(
  name='RpcErrorCodeProto',
  full_name='hadoop.common.RpcResponseHeaderProto.RpcErrorCodeProto',
  filename=None,
  file=DESCRIPTOR,
  values=[
    descriptor.EnumValueDescriptor(
      name='ERROR_APPLICATION', index=0, number=1,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='ERROR_NO_SUCH_METHOD', index=1, number=2,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='ERROR_NO_SUCH_PROTOCOL', index=2, number=3,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='ERROR_RPC_SERVER', index=3, number=4,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='ERROR_SERIALIZING_RESPONSE', index=4, number=5,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='ERROR_RPC_VERSION_MISMATCH', index=5, number=6,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='FATAL_UNKNOWN', index=6, number=10,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='FATAL_UNSUPPORTED_SERIALIZATION', index=7, number=11,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='FATAL_INVALID_RPC_HEADER', index=8, number=12,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='FATAL_DESERIALIZING_REQUEST', index=9, number=13,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='FATAL_VERSION_MISMATCH', index=10, number=14,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='FATAL_UNAUTHORIZED', index=11, number=15,
      options=None,
      type=None),
  ],
  containing_type=None,
  options=None,
  serialized_start=689,
  serialized_end=1042,
)

_RPCSASLPROTO_SASLSTATE = descriptor.EnumDescriptor(
  name='SaslState',
  full_name='hadoop.common.RpcSaslProto.SaslState',
  filename=None,
  file=DESCRIPTOR,
  values=[
    descriptor.EnumValueDescriptor(
      name='SUCCESS', index=0, number=0,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='NEGOTIATE', index=1, number=1,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='INITIATE', index=2, number=2,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='CHALLENGE', index=3, number=3,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='RESPONSE', index=4, number=4,
      options=None,
      type=None),
    descriptor.EnumValueDescriptor(
      name='WRAP', index=5, number=5,
      options=None,
      type=None),
  ],
  containing_type=None,
  options=None,
  serialized_start=1302,
  serialized_end=1394,
)


_RPCREQUESTHEADERPROTO = descriptor.Descriptor(
  name='RpcRequestHeaderProto',
  full_name='hadoop.common.RpcRequestHeaderProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='rpcKind', full_name='hadoop.common.RpcRequestHeaderProto.rpcKind', index=0,
      number=1, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='rpcOp', full_name='hadoop.common.RpcRequestHeaderProto.rpcOp', index=1,
      number=2, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='callId', full_name='hadoop.common.RpcRequestHeaderProto.callId', index=2,
      number=3, type=17, cpp_type=1, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='clientId', full_name='hadoop.common.RpcRequestHeaderProto.clientId', index=3,
      number=4, type=12, cpp_type=9, label=2,
      has_default_value=False, default_value="",
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='retryCount', full_name='hadoop.common.RpcRequestHeaderProto.retryCount', index=4,
      number=5, type=17, cpp_type=1, label=1,
      has_default_value=True, default_value=-1,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
    _RPCREQUESTHEADERPROTO_OPERATIONPROTO,
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=35,
  serialized_end=325,
)


_RPCRESPONSEHEADERPROTO = descriptor.Descriptor(
  name='RpcResponseHeaderProto',
  full_name='hadoop.common.RpcResponseHeaderProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='callId', full_name='hadoop.common.RpcResponseHeaderProto.callId', index=0,
      number=1, type=13, cpp_type=3, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='status', full_name='hadoop.common.RpcResponseHeaderProto.status', index=1,
      number=2, type=14, cpp_type=8, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='serverIpcVersionNum', full_name='hadoop.common.RpcResponseHeaderProto.serverIpcVersionNum', index=2,
      number=3, type=13, cpp_type=3, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='exceptionClassName', full_name='hadoop.common.RpcResponseHeaderProto.exceptionClassName', index=3,
      number=4, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='errorMsg', full_name='hadoop.common.RpcResponseHeaderProto.errorMsg', index=4,
      number=5, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='errorDetail', full_name='hadoop.common.RpcResponseHeaderProto.errorDetail', index=5,
      number=6, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=1,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='clientId', full_name='hadoop.common.RpcResponseHeaderProto.clientId', index=6,
      number=7, type=12, cpp_type=9, label=1,
      has_default_value=False, default_value="",
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='retryCount', full_name='hadoop.common.RpcResponseHeaderProto.retryCount', index=7,
      number=8, type=17, cpp_type=1, label=1,
      has_default_value=True, default_value=-1,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
    _RPCRESPONSEHEADERPROTO_RPCSTATUSPROTO,
    _RPCRESPONSEHEADERPROTO_RPCERRORCODEPROTO,
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=328,
  serialized_end=1042,
)


_RPCSASLPROTO_SASLAUTH = descriptor.Descriptor(
  name='SaslAuth',
  full_name='hadoop.common.RpcSaslProto.SaslAuth',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='method', full_name='hadoop.common.RpcSaslProto.SaslAuth.method', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='mechanism', full_name='hadoop.common.RpcSaslProto.SaslAuth.mechanism', index=1,
      number=2, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='protocol', full_name='hadoop.common.RpcSaslProto.SaslAuth.protocol', index=2,
      number=3, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='serverId', full_name='hadoop.common.RpcSaslProto.SaslAuth.serverId', index=3,
      number=4, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='challenge', full_name='hadoop.common.RpcSaslProto.SaslAuth.challenge', index=4,
      number=5, type=12, cpp_type=9, label=1,
      has_default_value=False, default_value="",
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1200,
  serialized_end=1300,
)

_RPCSASLPROTO = descriptor.Descriptor(
  name='RpcSaslProto',
  full_name='hadoop.common.RpcSaslProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='version', full_name='hadoop.common.RpcSaslProto.version', index=0,
      number=1, type=13, cpp_type=3, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='state', full_name='hadoop.common.RpcSaslProto.state', index=1,
      number=2, type=14, cpp_type=8, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='token', full_name='hadoop.common.RpcSaslProto.token', index=2,
      number=3, type=12, cpp_type=9, label=1,
      has_default_value=False, default_value="",
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='auths', full_name='hadoop.common.RpcSaslProto.auths', index=3,
      number=4, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[_RPCSASLPROTO_SASLAUTH, ],
  enum_types=[
    _RPCSASLPROTO_SASLSTATE,
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=1045,
  serialized_end=1394,
)

_RPCREQUESTHEADERPROTO.fields_by_name['rpcKind'].enum_type = _RPCKINDPROTO
_RPCREQUESTHEADERPROTO.fields_by_name['rpcOp'].enum_type = _RPCREQUESTHEADERPROTO_OPERATIONPROTO
_RPCREQUESTHEADERPROTO_OPERATIONPROTO.containing_type = _RPCREQUESTHEADERPROTO;
_RPCRESPONSEHEADERPROTO.fields_by_name['status'].enum_type = _RPCRESPONSEHEADERPROTO_RPCSTATUSPROTO
_RPCRESPONSEHEADERPROTO.fields_by_name['errorDetail'].enum_type = _RPCRESPONSEHEADERPROTO_RPCERRORCODEPROTO
_RPCRESPONSEHEADERPROTO_RPCSTATUSPROTO.containing_type = _RPCRESPONSEHEADERPROTO;
_RPCRESPONSEHEADERPROTO_RPCERRORCODEPROTO.containing_type = _RPCRESPONSEHEADERPROTO;
_RPCSASLPROTO_SASLAUTH.containing_type = _RPCSASLPROTO;
_RPCSASLPROTO.fields_by_name['state'].enum_type = _RPCSASLPROTO_SASLSTATE
_RPCSASLPROTO.fields_by_name['auths'].message_type = _RPCSASLPROTO_SASLAUTH
_RPCSASLPROTO_SASLSTATE.containing_type = _RPCSASLPROTO;
DESCRIPTOR.message_types_by_name['RpcRequestHeaderProto'] = _RPCREQUESTHEADERPROTO
DESCRIPTOR.message_types_by_name['RpcResponseHeaderProto'] = _RPCRESPONSEHEADERPROTO
DESCRIPTOR.message_types_by_name['RpcSaslProto'] = _RPCSASLPROTO

class RpcRequestHeaderProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _RPCREQUESTHEADERPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.common.RpcRequestHeaderProto)

class RpcResponseHeaderProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _RPCRESPONSEHEADERPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.common.RpcResponseHeaderProto)

class RpcSaslProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  
  class SaslAuth(message.Message):
    __metaclass__ = reflection.GeneratedProtocolMessageType
    DESCRIPTOR = _RPCSASLPROTO_SASLAUTH
    
    # @@protoc_insertion_point(class_scope:hadoop.common.RpcSaslProto.SaslAuth)
  DESCRIPTOR = _RPCSASLPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.common.RpcSaslProto)

# @@protoc_insertion_point(module_scope)

########NEW FILE########
__FILENAME__ = Security_pb2
# Generated by the protocol buffer compiler.  DO NOT EDIT!

from google.protobuf import descriptor
from google.protobuf import message
from google.protobuf import reflection
from google.protobuf import descriptor_pb2
# @@protoc_insertion_point(imports)



DESCRIPTOR = descriptor.FileDescriptor(
  name='Security.proto',
  package='hadoop.common',
  serialized_pb='\n\x0eSecurity.proto\x12\rhadoop.common\"Q\n\nTokenProto\x12\x12\n\nidentifier\x18\x01 \x02(\x0c\x12\x10\n\x08password\x18\x02 \x02(\x0c\x12\x0c\n\x04kind\x18\x03 \x02(\t\x12\x0f\n\x07service\x18\x04 \x02(\t\"1\n\x1eGetDelegationTokenRequestProto\x12\x0f\n\x07renewer\x18\x01 \x02(\t\"K\n\x1fGetDelegationTokenResponseProto\x12(\n\x05token\x18\x01 \x01(\x0b\x32\x19.hadoop.common.TokenProto\"L\n RenewDelegationTokenRequestProto\x12(\n\x05token\x18\x01 \x02(\x0b\x32\x19.hadoop.common.TokenProto\":\n!RenewDelegationTokenResponseProto\x12\x15\n\rnewExpiryTime\x18\x01 \x02(\x04\"M\n!CancelDelegationTokenRequestProto\x12(\n\x05token\x18\x01 \x02(\x0b\x32\x19.hadoop.common.TokenProto\"$\n\"CancelDelegationTokenResponseProtoB8\n org.apache.hadoop.security.protoB\x0eSecurityProtos\x88\x01\x01\xa0\x01\x01')




_TOKENPROTO = descriptor.Descriptor(
  name='TokenProto',
  full_name='hadoop.common.TokenProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='identifier', full_name='hadoop.common.TokenProto.identifier', index=0,
      number=1, type=12, cpp_type=9, label=2,
      has_default_value=False, default_value="",
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='password', full_name='hadoop.common.TokenProto.password', index=1,
      number=2, type=12, cpp_type=9, label=2,
      has_default_value=False, default_value="",
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='kind', full_name='hadoop.common.TokenProto.kind', index=2,
      number=3, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    descriptor.FieldDescriptor(
      name='service', full_name='hadoop.common.TokenProto.service', index=3,
      number=4, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=33,
  serialized_end=114,
)


_GETDELEGATIONTOKENREQUESTPROTO = descriptor.Descriptor(
  name='GetDelegationTokenRequestProto',
  full_name='hadoop.common.GetDelegationTokenRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='renewer', full_name='hadoop.common.GetDelegationTokenRequestProto.renewer', index=0,
      number=1, type=9, cpp_type=9, label=2,
      has_default_value=False, default_value=unicode("", "utf-8"),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=116,
  serialized_end=165,
)


_GETDELEGATIONTOKENRESPONSEPROTO = descriptor.Descriptor(
  name='GetDelegationTokenResponseProto',
  full_name='hadoop.common.GetDelegationTokenResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='token', full_name='hadoop.common.GetDelegationTokenResponseProto.token', index=0,
      number=1, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=167,
  serialized_end=242,
)


_RENEWDELEGATIONTOKENREQUESTPROTO = descriptor.Descriptor(
  name='RenewDelegationTokenRequestProto',
  full_name='hadoop.common.RenewDelegationTokenRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='token', full_name='hadoop.common.RenewDelegationTokenRequestProto.token', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=244,
  serialized_end=320,
)


_RENEWDELEGATIONTOKENRESPONSEPROTO = descriptor.Descriptor(
  name='RenewDelegationTokenResponseProto',
  full_name='hadoop.common.RenewDelegationTokenResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='newExpiryTime', full_name='hadoop.common.RenewDelegationTokenResponseProto.newExpiryTime', index=0,
      number=1, type=4, cpp_type=4, label=2,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=322,
  serialized_end=380,
)


_CANCELDELEGATIONTOKENREQUESTPROTO = descriptor.Descriptor(
  name='CancelDelegationTokenRequestProto',
  full_name='hadoop.common.CancelDelegationTokenRequestProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    descriptor.FieldDescriptor(
      name='token', full_name='hadoop.common.CancelDelegationTokenRequestProto.token', index=0,
      number=1, type=11, cpp_type=10, label=2,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=382,
  serialized_end=459,
)


_CANCELDELEGATIONTOKENRESPONSEPROTO = descriptor.Descriptor(
  name='CancelDelegationTokenResponseProto',
  full_name='hadoop.common.CancelDelegationTokenResponseProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  extension_ranges=[],
  serialized_start=461,
  serialized_end=497,
)

_GETDELEGATIONTOKENRESPONSEPROTO.fields_by_name['token'].message_type = _TOKENPROTO
_RENEWDELEGATIONTOKENREQUESTPROTO.fields_by_name['token'].message_type = _TOKENPROTO
_CANCELDELEGATIONTOKENREQUESTPROTO.fields_by_name['token'].message_type = _TOKENPROTO
DESCRIPTOR.message_types_by_name['TokenProto'] = _TOKENPROTO
DESCRIPTOR.message_types_by_name['GetDelegationTokenRequestProto'] = _GETDELEGATIONTOKENREQUESTPROTO
DESCRIPTOR.message_types_by_name['GetDelegationTokenResponseProto'] = _GETDELEGATIONTOKENRESPONSEPROTO
DESCRIPTOR.message_types_by_name['RenewDelegationTokenRequestProto'] = _RENEWDELEGATIONTOKENREQUESTPROTO
DESCRIPTOR.message_types_by_name['RenewDelegationTokenResponseProto'] = _RENEWDELEGATIONTOKENRESPONSEPROTO
DESCRIPTOR.message_types_by_name['CancelDelegationTokenRequestProto'] = _CANCELDELEGATIONTOKENREQUESTPROTO
DESCRIPTOR.message_types_by_name['CancelDelegationTokenResponseProto'] = _CANCELDELEGATIONTOKENRESPONSEPROTO

class TokenProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _TOKENPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.common.TokenProto)

class GetDelegationTokenRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETDELEGATIONTOKENREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.common.GetDelegationTokenRequestProto)

class GetDelegationTokenResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _GETDELEGATIONTOKENRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.common.GetDelegationTokenResponseProto)

class RenewDelegationTokenRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _RENEWDELEGATIONTOKENREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.common.RenewDelegationTokenRequestProto)

class RenewDelegationTokenResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _RENEWDELEGATIONTOKENRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.common.RenewDelegationTokenResponseProto)

class CancelDelegationTokenRequestProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _CANCELDELEGATIONTOKENREQUESTPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.common.CancelDelegationTokenRequestProto)

class CancelDelegationTokenResponseProto(message.Message):
  __metaclass__ = reflection.GeneratedProtocolMessageType
  DESCRIPTOR = _CANCELDELEGATIONTOKENRESPONSEPROTO
  
  # @@protoc_insertion_point(class_scope:hadoop.common.CancelDelegationTokenResponseProto)

# @@protoc_insertion_point(module_scope)

########NEW FILE########
__FILENAME__ = service
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.

from snakebite.channel import SocketRpcChannel
import google.protobuf.service as service


class RpcService(object):

    def __init__(self, service_stub_class, port, host, hadoop_version, effective_user=None):
        self.service_stub_class = service_stub_class
        self.port = port
        self.host = host

        # Setup the RPC channel
        self.channel = SocketRpcChannel(host=self.host, port=self.port, version=hadoop_version, effective_user=effective_user)
        self.service = self.service_stub_class(self.channel)

        # go through service_stub methods and add a wrapper function to
        # this object that will call the method
        for method in service_stub_class.GetDescriptor().methods:
            # Add service methods to the this object
            rpc = lambda request, service=self, method=method.name: service.call(service_stub_class.__dict__[method], request)

            self.__dict__[method.name] = rpc

    def call(self, method, request):
        controller = SocketRpcController()
        return method(self.service, controller, request)


class SocketRpcController(service.RpcController):
    ''' RpcController implementation to be used by the SocketRpcChannel class.

    The RpcController is used to mediate a single method call.
    '''

    def __init__(self):
        '''Constructor which initializes the controller's state.'''
        self._fail = False
        self._error = None
        self.reason = None

    def handleError(self, error_code, message):
        '''Log and set the controller state.'''
        self._fail = True
        self.reason = error_code
        self._error = message

    def reset(self):
        '''Resets the controller i.e. clears the error state.'''
        self._fail = False
        self._error = None
        self.reason = None

    def failed(self):
        '''Returns True if the controller is in a failed state.'''
        return self._fail

    def error(self):
        return self._error

########NEW FILE########
__FILENAME__ = version
VERSION = "2.4.1"


def version():
    return VERSION

########NEW FILE########
__FILENAME__ = cat_test
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.

from minicluster_testbase import MiniClusterTestBase
import os


class CatTest(MiniClusterTestBase):

    def test_cat_file_on_1_block(self):  # Size < 1 block
        client_output = ''
        for file_to_read in self.client.cat(['/test3']):
            for data in file_to_read:
                client_output += data
        expected_output = self.cluster.cat('/test3')
        self.assertEqual(expected_output, client_output)

    def test_cat_file_on_1_block_checkcrc(self):  # Size < 1 block
        client_output = ''
        for file_to_read in self.client.cat(['/test3'], check_crc=True):
            for data in file_to_read:
                client_output += data
        expected_output = self.cluster.cat('/test3')
        self.assertEqual(expected_output, client_output)

    def test_cat_file_on_2_blocks(self):  # 1 < size < 2 blocks
        self._write_to_test_cluster('/test1', 200, '/temp_test')  # 677,972 * 200 = 135,594,400 bytes

        client_output = ''
        for file_to_read in self.client.cat(['/temp_test']):
            for data in file_to_read:
                client_output += data
        expected_output = self.cluster.cat('/temp_test')
        self.assertEqual(expected_output, client_output)

    def test_cat_file_on_3_blocks(self):  # 2 < size < 3 blocks
        self._write_to_test_cluster('/test1', 400, '/temp_test2')  # 677,972 * 400 = 271,188,800 bytes

        client_output = ''
        for file_to_read in self.client.cat(['/temp_test2']):
            for data in file_to_read:
                client_output += data
        expected_output = self.cluster.cat('/temp_test2')
        self.assertEqual(expected_output, client_output)

    def test_cat_file_on_exactly_1_block(self):  # Size == 1 block
        self._write_to_test_cluster('/test3', 131072, '/temp_test3')  # 1024 * 131072 = 134,217,728 (default block size)

        client_output = ''
        for file_to_read in self.client.cat(['/temp_test3']):
            for data in file_to_read:
                client_output += data
        expected_output = self.cluster.cat('/temp_test3')
        self.assertEqual(expected_output, client_output)

    def _write_to_test_cluster(self, testfile, times, dst):
        testfiles_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "testfiles")
        f = open(''.join([testfiles_path, testfile]))
        p = self.cluster.put_subprocess('-', dst)
        for _ in xrange(times):
            f.seek(0)
            for line in f.readlines():
                print >> p.stdin, line
        p.communicate()

########NEW FILE########
__FILENAME__ = chgrp_test
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
from minicluster_testbase import MiniClusterTestBase
from snakebite.errors import FileNotFoundException
from snakebite.errors import InvalidInputException


class ChgrpTest(MiniClusterTestBase):

    def test_onepath(self):
        list(self.client.chgrp(['/dir1'], "onepathgroup"))
        client_output = list(self.client.ls(['/dir1'], include_toplevel=True, include_children=False))
        self.assertEqual(client_output[0]["group"], "onepathgroup")

    def test_multipath(self):
        list(self.client.chgrp(['/dir1', '/zerofile'], "multipathgroup"))
        client_output = self.client.ls(['/dir1', '/zerofile'], include_toplevel=True, include_children=False)
        for node in client_output:
            self.assertEqual(node["group"], "multipathgroup")

    def test_recursive(self):
        list(self.client.chgrp(['/'], 'recursivegroup', recurse=True))
        expected_output = self.cluster.ls(["/"], ["-R"])
        for node in expected_output:
            self.assertEqual(node["group"], "recursivegroup")

    def test_unknown_file(self):
        result = self.client.chgrp(['/nonexistent'], 'myOnwer', recurse=True)
        self.assertRaises(FileNotFoundException, result.next)

    def test_invalid_input(self):
        result = self.client.chgrp('/doesnotexist', 'myOnwer')
        self.assertRaises(InvalidInputException, result.next)

########NEW FILE########
__FILENAME__ = chmod_test
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
from minicluster_testbase import MiniClusterTestBase
from snakebite.errors import FileNotFoundException
from snakebite.errors import InvalidInputException


class ChmodTest(MiniClusterTestBase):

    def test_onepath(self):
        list(self.client.chmod(['/dir1'], 0777))
        client_output = list(self.client.ls(['/dir1'], include_toplevel=True, include_children=False))
        self.assertEqual(client_output[0]["permission"], 511)

    def test_multipath(self):
        list(self.client.chmod(['/dir1', '/zerofile'], 0700))
        client_output = self.client.ls(['/dir1', '/zerofile'], include_toplevel=True, include_children=False)
        for node in client_output:
            self.assertEqual(node["permission"], 448)

    def test_recursive(self):
        list(self.client.chmod(['/'], 0770, recurse=True))
        expected_output = self.cluster.ls(["/"], ["-R"])
        for node in expected_output:
            self.assertEqual(node["permission"], 504)

    def test_unknown_file(self):
        result = self.client.chmod(['/nonexistent'], 0777, recurse=True)
        self.assertRaises(FileNotFoundException, result.next)

    def test_invalid_input(self):
        result = self.client.chmod('/stringpath', 777)
        self.assertRaises(InvalidInputException, result.next)

########NEW FILE########
__FILENAME__ = chown_test
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
from minicluster_testbase import MiniClusterTestBase
from snakebite.errors import FileNotFoundException
from snakebite.errors import InvalidInputException


class ChownTest(MiniClusterTestBase):

    def test_onepath(self):
        list(self.client.chown(['/dir1'], "onepathowner"))
        client_output = list(self.client.ls(['/dir1'], include_toplevel=True, include_children=False))
        self.assertEqual(client_output[0]["owner"], "onepathowner")

    def test_multipath(self):
        list(self.client.chown(['/dir1', '/zerofile'], "multipathowner"))
        client_output = self.client.ls(['/dir1', '/zerofile'], include_toplevel=True, include_children=False)
        for node in client_output:
            self.assertEqual(node["owner"], "multipathowner")

    def test_recursive(self):
        list(self.client.chown(['/'], 'recursiveowner', recurse=True))
        expected_output = self.cluster.ls(["/"], ["-R"])
        for node in expected_output:
            self.assertEqual(node["owner"], "recursiveowner")

    def test_unknown_file(self):
        result = self.client.chown(['/nonexistent'], 'myGroup', recurse=True)
        self.assertRaises(FileNotFoundException, result.next)

    def test_user_group(self):
        list(self.client.chown(['/dir1'], "myUser:myGroup"))
        client_output = list(self.client.ls(['/dir1'], include_toplevel=True, include_children=False))
        self.assertEqual(client_output[0]["owner"], "myUser")
        self.assertEqual(client_output[0]["group"], "myGroup")

    def test_group(self):
        client_output = list(self.client.ls(['/dir1'], include_toplevel=True, include_children=False))
        expected_owner = client_output[0]["owner"]
        list(self.client.chown(['/dir1'], ":mySuperGroup"))
        client_output = list(self.client.ls(['/dir1'], include_toplevel=True, include_children=False))
        self.assertEqual(client_output[0]["owner"], expected_owner)
        self.assertEqual(client_output[0]["group"], "mySuperGroup")

    def test_invalid_input(self):
        result = self.client.chown('/stringpath', 'myGroup')
        self.assertRaises(InvalidInputException, result.next)

########NEW FILE########
__FILENAME__ = client_test
import unittest2
import inspect

from snakebite.client import HAClient

class ClientTest(unittest2.TestCase):
    def test_wrapped_methods(self):
        public_methods = [(name, method) for name, method in inspect.getmembers(HAClient, inspect.ismethod) if not name.startswith("_")]
        self.assertGreater(len(public_methods), 0)
        wrapped_methods = [str(method) for name, method in public_methods if ".wrapped" in str(method)]
        self.assertEqual(len(public_methods), len(wrapped_methods))
########NEW FILE########
__FILENAME__ = commandlineparser_test
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
import unittest2
import os
import pwd
import json

from mock import patch, mock_open

from snakebite.config import HDFSConfig
from snakebite.commandlineparser import CommandLineParser
from snakebite.namenode import Namenode

from config_test import ConfigTest

class CommandLineParserTest(unittest2.TestCase):

    def setUp(self):
        self.parser = CommandLineParser()
        self.default_dir = os.path.join("/user", pwd.getpwuid(os.getuid())[0])

    def test_general_options(self):
        parser = self.parser

        output = parser.parse('ls some_folder'.split())
        self.assertFalse(output.debug)
        self.assertFalse(output.human)
        self.assertFalse(output.json)
        self.assertEqual(output.namenode, None)
        self.assertEqual(output.port, None)

        #each option
        output = parser.parse('-D ls some_folder'.split())
        self.assertTrue(output.debug)
        output = parser.parse('--debug ls some_folder'.split())
        self.assertTrue(output.debug)

        output = parser.parse('-j ls some_folder'.split())
        self.assertTrue(output.json)
        output = parser.parse('--json ls some_folder'.split())
        self.assertTrue(output.json)

        output = parser.parse('-n namenode_fqdn ls some_folder'.split())  # what are typical values for namenodes?
        self.assertEqual(output.namenode, "namenode_fqdn")
        output = parser.parse('--namenode namenode_fqdn ls some_folder'.split())
        self.assertEqual(output.namenode, "namenode_fqdn")

        output = parser.parse('-p 1234 ls some_folder'.split())
        self.assertEqual(output.port, 1234)
        output = parser.parse('--port 1234 ls some_folder'.split())
        self.assertEqual(output.port, 1234)

        output = parser.parse('-V 4 ls some_folder'.split())
        self.assertEqual(output.version, 4)
        output = parser.parse('--version 4 ls some_folder'.split())
        self.assertEqual(output.version, 4)

        #all options
        output = parser.parse('-D -j -n namenode_fqdn -p 1234 -V 4 ls some_folder'.split())
        self.assertTrue(output.debug)
        self.assertTrue(output.json)
        self.assertEqual(output.namenode, "namenode_fqdn")
        self.assertEqual(output.port, 1234)
        self.assertEqual(output.version, 4)

        #options in illegal position
        with self.assertRaises(SystemExit):
            parser.parse('ls -D some_folder'.split())
        with self.assertRaises(SystemExit):
            parser.parse('ls some_folder -D'.split())

    def test_ls(self):
        parser = self.parser

        #no dir
        output = parser.parse('ls'.split())
        self.assertEqual(output.command, 'ls')
        self.assertEqual(output.dir, [self.default_dir])

        #one dir
        output = parser.parse('ls some_dir'.split())
        self.assertEqual(output.dir, ['some_dir'])

        #multiple dirs
        output = parser.parse('ls dir1 dir2 dir3'.split())
        self.assertEqual(output.dir, ['dir1', 'dir2', 'dir3'])

        #specific commands
        output = parser.parse('ls -d -R -s -h some_dir'.split())
        self.assertTrue(output.directory)
        self.assertTrue(output.recurse)
        self.assertTrue(output.summary)
        self.assertTrue(output.human)
        self.assertEqual(output.dir, ['some_dir'])

    def test_mkdir(self):
        parser = self.parser

        #no dir
        with self.assertRaises(SystemExit):
            parser.parse('mkdir'.split())

        #one dir
        output = parser.parse('mkdir some_dir'.split())
        self.assertEqual(output.command, 'mkdir')
        self.assertEqual(output.dir, ['some_dir'])

        #multiple dirs
        output = parser.parse('mkdir dir1 dir2 dir3'.split())
        self.assertEqual(output.dir, ['dir1', 'dir2', 'dir3'])

    def test_mkdirp(self):
        parser = self.parser

        #no dir
        with self.assertRaises(SystemExit):
            parser.parse('mkdirp'.split())

        #one dir
        output = parser.parse('mkdirp some_dir'.split())
        self.assertEqual(output.command, 'mkdirp')
        self.assertEqual(output.dir, ['some_dir'])

        #multiple dirs
        output = parser.parse('mkdirp dir1 dir2 dir3'.split())
        self.assertEqual(output.dir, ['dir1', 'dir2', 'dir3'])

    def test_chown(self):
        parser = self.parser

        #no dir and/or no owner
        with self.assertRaises(SystemExit):
            parser.parse('chown'.split())
        with self.assertRaises(SystemExit):
            parser.parse('chown owner_or_dir'.split())

        #one dir
        output = parser.parse('chown root some_dir'.split())
        self.assertEqual(output.command, 'chown')
        self.assertEqual(output.dir, ['some_dir'])
        self.assertEqual(output.single_arg, 'root')

        #multiple dirs
        output = parser.parse('chown root dir1 dir2 dir3'.split())
        self.assertEqual(output.dir, ['dir1', 'dir2', 'dir3'])
        self.assertEqual(output.single_arg, 'root')

        #recursive
        output = parser.parse('chown -R root some_dir'.split())
        self.assertTrue(output.recurse)

    def test_chmod(self):
        parser = self.parser

        #no dir and/or no mode
        with self.assertRaises(SystemExit):
            parser.parse('chmod'.split())
        with self.assertRaises(SystemExit):
            parser.parse('chmod mode_or_dir'.split())

        #one dir
        output = parser.parse('chmod 664 some_dir'.split())
        self.assertEqual(output.command, 'chmod')
        self.assertEqual(output.dir, ['some_dir'])
        self.assertEqual(output.single_int_arg, 664)

        #wrong type for mode argument
        with self.assertRaises(SystemExit):
            parser.parse('chmod not_an_int some_dir'.split())

        #multiple dirs
        output = parser.parse('chmod 664 dir1 dir2 dir3'.split())
        self.assertEqual(output.dir, ['dir1', 'dir2', 'dir3'])
        self.assertEqual(output.single_int_arg, 664)

        #recursive
        output = parser.parse('chmod -R 664 some_dir'.split())
        self.assertTrue(output.recurse)

    def test_chgrp(self):
        parser = self.parser

        #no dir and/or no group
        with self.assertRaises(SystemExit):
            parser.parse('chgrp'.split())
        with self.assertRaises(SystemExit):
            parser.parse('chgrp group_or_dir'.split())

        #one dir
        output = parser.parse('chgrp group some_dir'.split())
        self.assertEqual(output.command, 'chgrp')
        self.assertEqual(output.dir, ['some_dir'])
        self.assertEqual(output.single_arg, 'group')

        #multiple dirs
        output = parser.parse('chgrp group dir1 dir2 dir3'.split())
        self.assertEqual(output.dir, ['dir1', 'dir2', 'dir3'])
        self.assertEqual(output.single_arg, 'group')

        #recursive
        output = parser.parse('chgrp -R group some_dir'.split())
        self.assertTrue(output.recurse)

    def test_count(self):
        parser = self.parser

        #no dir
        output = parser.parse('count'.split())
        self.assertEqual(output.command, 'count')
        self.assertEqual(output.dir, [self.default_dir])

        #one dir
        output = parser.parse('count some_dir'.split())
        self.assertEqual(output.dir, ['some_dir'])

        #multiple dirs
        output = parser.parse('count dir1 dir2 dir3'.split())
        self.assertEqual(output.dir, ['dir1', 'dir2', 'dir3'])

        # Human output
        output = parser.parse('count -h dir1 dir2 dir3'.split())
        self.assertTrue(output.human)

    def test_df(self):
        parser = self.parser

        #no dir
        output = parser.parse('df'.split())
        self.assertEqual(output.command, 'df')

        # Human output
        output = parser.parse('df -h'.split())
        self.assertEqual(output.command, 'df')
        self.assertTrue(output.human)

        with self.assertRaises(SystemExit):
            parser.parse('df some_additional_argument'.split())

    def test_du(self):
        parser = self.parser

        #no dir
        output = parser.parse('du'.split())
        self.assertEqual(output.command, 'du')
        self.assertEqual(output.dir, [self.default_dir])

        #one dir
        output = parser.parse('du some_dir'.split())
        self.assertEqual(output.dir, ['some_dir'])

        #multiple dirs
        output = parser.parse('du dir1 dir2 dir3'.split())
        self.assertEqual(output.dir, ['dir1', 'dir2', 'dir3'])

        #summary
        output = parser.parse('du -s some_dir'.split())
        self.assertTrue(output.summary)

        #human
        output = parser.parse('du -h some_dir'.split())
        self.assertTrue(output.human)

    def test_mv(self):
        parser = self.parser

        #no source and/or no destination
        with self.assertRaises(SystemExit):
            parser.parse('mv'.split())
        with self.assertRaises(SystemExit):
            parser.parse('mv src_or_dest'.split())

        #one source
        output = parser.parse('mv source some_dest'.split())
        self.assertEqual(output.command, 'mv')
        self.assertEqual(output.dir, ['source'])
        self.assertEqual(output.single_arg, 'some_dest')

        #multiple sources
        output = parser.parse('mv source1 source2 source3 some_dest'.split())
        self.assertEqual(output.dir, ['source1', 'source2', 'source3'])
        self.assertEqual(output.single_arg, 'some_dest')

    def test_rm(self):
        parser = self.parser

        #no dir and/or no group
        with self.assertRaises(SystemExit):
            parser.parse('rm'.split())

        #one dir
        output = parser.parse('rm some_dir'.split())
        self.assertEqual(output.command, 'rm')
        self.assertEqual(output.dir, ['some_dir'])

        #multiple dirs
        output = parser.parse('rm dir1 dir2 dir3'.split())
        self.assertEqual(output.dir, ['dir1', 'dir2', 'dir3'])

        #recursive
        output = parser.parse('rm -R some_dir'.split())
        self.assertTrue(output.recurse)

        #skiptrash
        output = parser.parse('rm -S some_dir'.split())
        self.assertTrue(output.skiptrash)

        #skiptrash
        output = parser.parse('rm --skiptrash some_dir'.split())
        self.assertTrue(output.skiptrash)

        #usetrash
        output = parser.parse('rm -T some_dir'.split())
        self.assertTrue(output.usetrash)

        #usetrash
        output =parser.parse('rm --usetrash some_dir'.split())
        self.assertTrue(output.usetrash)

        #usetrash & skiptrash
        output = parser.parse('rm --usetrash --skiptrash some_dir'.split())
        self.assertTrue(output.usetrash)
        self.assertTrue(output.skiptrash)

    def test_touchz(self):
        parser = self.parser

        #no dir and/or no group
        with self.assertRaises(SystemExit):
            parser.parse('touchz'.split())

        #one dir
        output = parser.parse('touchz some_dir'.split())
        self.assertEqual(output.command, 'touchz')
        self.assertEqual(output.dir, ['some_dir'])

        #multiple dirs
        output = parser.parse('touchz dir1 dir2 dir3'.split())
        self.assertEqual(output.dir, ['dir1', 'dir2', 'dir3'])

    def test_serverdefaults(self):
        parser = self.parser

        #no arg
        output = parser.parse('serverdefaults'.split())
        self.assertEqual(output.command, 'serverdefaults')

        #too many args
        with self.assertRaises(SystemExit):
            parser.parse('serverdefaults some_additional_argument'.split())

    def test_rmdir(self):
        parser = self.parser

        #no dir and/or no group
        with self.assertRaises(SystemExit):
            parser.parse('rmdir'.split())

        #one dir
        output = parser.parse('rmdir some_dir'.split())
        self.assertEqual(output.command, 'rmdir')
        self.assertEqual(output.dir, ['some_dir'])

        #multiple dirs
        output = parser.parse('rmdir dir1 dir2 dir3'.split())
        self.assertEqual(output.dir, ['dir1', 'dir2', 'dir3'])

    def test_setrep(self):
        parser = self.parser

        #no dir and/or no replication factor
        with self.assertRaises(SystemExit):
            parser.parse('setrep'.split())
        with self.assertRaises(SystemExit):
            parser.parse('setrep some_dir'.split())
        with self.assertRaises(SystemExit):
            parser.parse('setrep 3'.split())

        #one dir
        output = parser.parse('setrep 3 some_dir'.split())
        self.assertEqual(output.command, 'setrep')
        self.assertEqual(output.dir, ['some_dir'])
        self.assertEqual(output.single_int_arg, 3)

        #wrong type for mode argument
        with self.assertRaises(SystemExit):
            parser.parse('setrep not_an_int some_dir'.split())

        #multiple dirs
        output = parser.parse('setrep 3 dir1 dir2 dir3'.split())
        self.assertEqual(output.dir, ['dir1', 'dir2', 'dir3'])
        self.assertEqual(output.single_int_arg, 3)

        #recursive
        output = parser.parse('setrep -R 3 some_dir'.split())
        self.assertTrue(output.recurse)

    def test_usage(self):
        parser = self.parser

        #no command
        output = parser.parse('usage'.split())
        self.assertEqual(output.command, 'usage')

        #one dir
        output = parser.parse('usage some_cmd'.split())
        self.assertEqual(output.command, 'usage')
        self.assertEqual(output.arg, ['some_cmd'])

        #multiple dirs
        output = parser.parse('usage cmd1 cmd2 cmd3'.split())
        self.assertEqual(output.arg, ['cmd1', 'cmd2', 'cmd3'])

    def test_stat(self):
        parser = self.parser

        #no dir
        with self.assertRaises(SystemExit):
            parser.parse('stat'.split())

        #one dir
        output = parser.parse('stat some_dir'.split())
        self.assertEqual(output.command, 'stat')
        self.assertEqual(output.dir, ['some_dir'])

        #multiple dirs
        output = parser.parse('stat dir1 dir2 dir3'.split())
        self.assertEqual(output.dir, ['dir1', 'dir2', 'dir3'])

    def test_test(self):
        parser = self.parser

        #no dir
        with self.assertRaises(SystemExit):
            parser.parse('test'.split())

        #one dir
        output = parser.parse('test some_dir'.split())
        self.assertEqual(output.command, 'test')
        self.assertEqual(output.single_arg, 'some_dir')

        #multiple dirs
        with self.assertRaises(SystemExit):
            parser.parse('test dir1 dir2 dir3'.split())

        #specific commands
        output = parser.parse('test -d -z -e some_dir'.split())
        self.assertTrue(output.directory)
        self.assertTrue(output.zero)
        self.assertTrue(output.exists)
        self.assertEqual(output.single_arg, 'some_dir')

    def test_cat(self):
        parser = self.parser

        #no path
        with self.assertRaises(SystemExit):
            parser.parse('cat'.split())

        #one path
        output = parser.parse('cat some_file'.split())
        self.assertEqual(output.command, 'cat')
        self.assertEqual(output.dir, ['some_file'])

        #multiple paths
        output = parser.parse('cat dir1 dir2 dir3'.split())
        self.assertEqual(output.dir, ['dir1', 'dir2', 'dir3'])

        #specific commands
        output = parser.parse('cat -checkcrc dir1 dir2'.split())
        self.assertEqual(output.checkcrc, True)

    def test_copyFromLocal(self):
        parser = self.parser

        #no dir
        with self.assertRaises(SystemExit):
            parser.parse('copyFromLocal'.split())

        #one dir
        with self.assertRaises(SystemExit):
            parser.parse('copyFromLocal some_dir'.split())

        #two dirs
        output = parser.parse('copyFromLocal dir1 dir2'.split())
        self.assertEqual(output.dir, ['dir1'])
        self.assertEqual(output.single_arg, 'dir2')

    def test_copyToLocal(self):
        parser = self.parser

        #no dir
        with self.assertRaises(SystemExit):
            parser.parse('copyToLocal'.split())

        #one dir
        with self.assertRaises(SystemExit):
            parser.parse('copyToLocal some_dir'.split())

        #two dirs
        output = parser.parse('copyToLocal dir1 dir2'.split())
        self.assertEqual(output.dir, ['dir1'])
        self.assertEqual(output.single_arg, 'dir2')
        self.assertEqual(output.checkcrc, False)

        #specific commands
        output = parser.parse('copyToLocal -checkcrc dir1 dir2'.split())
        self.assertEqual(output.checkcrc, True)

    def test_cp(self):
        parser = self.parser

        #no dir
        with self.assertRaises(SystemExit):
            parser.parse('cp'.split())

        #one dir
        with self.assertRaises(SystemExit):
            parser.parse('cp some_dir'.split())

        #multiple dirs
        output = parser.parse('cp dir1 dir2 dir3'.split())
        self.assertEqual(output.dir, ['dir1', 'dir2'])
        self.assertEqual(output.single_arg, 'dir3')

    def test_get(self):
        parser = self.parser

        #no dir
        with self.assertRaises(SystemExit):
            parser.parse('get'.split())

        #one dir
        with self.assertRaises(SystemExit):
            parser.parse('get some_dir'.split())

        #multiple dirs
        output = parser.parse('get dir1 dir2 dir3'.split())
        self.assertEqual(output.dir, ['dir1', 'dir2'])
        self.assertEqual(output.single_arg, 'dir3')

        #specific commands
        output = parser.parse('get -checkcrc dir1 dir2'.split())
        self.assertEqual(output.checkcrc, True)

    def test_getmerge(self):
        parser = self.parser

        #no dir
        with self.assertRaises(SystemExit):
            parser.parse('getmerge'.split())

        #one dir
        with self.assertRaises(SystemExit):
            parser.parse('getmerge some_dir'.split())

        #two dirs
        output = parser.parse('getmerge dir1 dir2'.split())
        self.assertEqual(output.src_dst[0], 'dir1')
        self.assertEqual(output.src_dst[1], 'dir2')

        #multiple dirs
        with self.assertRaises(SystemExit):
            parser.parse('getmerge dir1 dir2 dir3'.split())

    # def test_put(self):
    #     parser = self.parser

    #     #no dir
    #     with self.assertRaises(SystemExit):
    #         parser.parse('put'.split())

    #     #one dir
    #     with self.assertRaises(SystemExit):
    #         parser.parse('put some_dir'.split())

    #     #multiple dirs
    #     output = parser.parse('put dir1 dir2 dir3'.split())
    #     self.assertEqual(output.dir, ['dir1', 'dir2'])
    #     self.assertEqual(output.single_arg, 'dir3')

    def test_tail(self):
        parser = self.parser

        #no dir
        with self.assertRaises(SystemExit):
            parser.parse('tail'.split())

        #one dir
        output = parser.parse('tail some_dir'.split())
        self.assertEqual(output.single_arg, 'some_dir')

        #multiple dirs
        with self.assertRaises(SystemExit):
            parser.parse('tail dir1 dir2'.split())

        #specific commands
        output = parser.parse('tail -f some_dir'.split())
        self.assertTrue(output.append)

    def test_text(self):
        parser = self.parser

        #no path
        with self.assertRaises(SystemExit):
            parser.parse('text'.split())

        #one path
        output = parser.parse('text some_file'.split())
        self.assertEqual(output.command, 'text')
        self.assertEqual(output.dir, ['some_file'])

        #multiple paths
        output = parser.parse('text dir1 dir2 dir3'.split())
        self.assertEqual(output.dir, ['dir1', 'dir2', 'dir3'])

        #specific commands
        output = parser.parse('text -checkcrc dir1 dir2'.split())
        self.assertEqual(output.checkcrc, True)



class MockParseArgs(object):
    # dir is a list of directories
    def __init__(self, dir=[],
                single_arg=None,
                command=None,
                namenode=None,
                port=None,
                usetrash=False,
                skiptrash=False):
        self.dir = dir
        self.single_arg = single_arg
        self.command = command
        self.namenode = namenode
        self.port = port
        self.usetrash = usetrash
        self.skiptrash = skiptrash

    def __contains__(self, b):
        return b in self.__dict__

class CommandLineParserInternalConfigTest(unittest2.TestCase):
    def setUp(self):
        self.parser = CommandLineParser()
        self.default_dir = os.path.join("/user", pwd.getpwuid(os.getuid())[0])


    def assert_namenode_spec(self, host, port, version=None):
        self.assertEqual(self.parser.args.namenode, host)
        self.assertEqual(self.parser.args.port, port)
        if version:
            self.assertEqual(self.parser.args.version, version)

    def assert_namenodes_spec(self, host, port, version=None):
        for namenode in self.parser.namenodes:
            try:
                self.assertEqual(namenode.host, host)
                self.assertEqual(namenode.port, port)
                if version:
                    self.assertEqual(namenode.version, version)
            except AssertionError:
                continue
            # There was no AssertError -> we found our NN
            return
        self.fail("NN not found in namenodes")


    def test_cl_config_conflicted(self):

        self.parser.args = MockParseArgs(dir=["hdfs://foobar:50070/user/rav",
                                              "hdfs://foobar2:50070/user/rav"])
        with self.assertRaises(SystemExit):
            self.parser.read_config()


        self.parser.args = MockParseArgs(dir=["hdfs://foobar:50071/user/rav",
                                              "hdfs://foobar:50070/user/rav"])
        with self.assertRaises(SystemExit):
            self.parser.read_config()

        self.parser.args = MockParseArgs(dir=["hdfs://foobar:50072/user/rav",
                                              "hdfs://foobar2:50070/user/rav"])
        with self.assertRaises(SystemExit):
            self.parser.read_config()

        self.parser.args = MockParseArgs(dir=["hdfs://foobar:50070/user/rav",
                                              "hdfs://foobar:50070/user/rav"],
                                         single_arg="hdfs://foobar2:50070/user/rav",
                                         command="mv")
        with self.assertRaises(SystemExit):
            self.parser.read_config()

    def test_cl_config_simple(self):
        self.parser.args = MockParseArgs(dir=["hdfs://foobar:50070/user/rav",
                                              "hdfs://foobar:50070/user/rav2"])

        self.parser.read_config()
        self.assert_namenode_spec("foobar", 50070)
        self.assert_namenodes_spec("foobar", 50070)

        self.parser.args = MockParseArgs(dir=["hdfs://foobar:50070/user/rav",
                                              "hdfs://foobar:50070/user/rav2"],
                                         single_arg="hdfs://foobar:50070/user/rav",
                                         command="mv")
        self.parser.read_config()
        self.assert_namenode_spec("foobar", 50070)
        self.assert_namenodes_spec("foobar", 50070)

    def test_cl_config_reduce_paths(self):
        self.parser.args = MockParseArgs(dir=["hdfs://foobar:50070/user/rav",
                                              "hdfs://foobar:50070/user/rav2"],
                                         single_arg="hdfs://foobar:50070/user/rav3",
                                         command="mv")
        self.parser.init()
        self.assert_namenode_spec("foobar", 50070)
        self.assertIn("/user/rav", self.parser.args.dir)
        self.assertIn("/user/rav2", self.parser.args.dir)
        self.assertEqual(self.parser.args.single_arg, "/user/rav3")

    import snakebite.config
    @patch.object(snakebite.config.HDFSConfig, 'get_external_config')
    @patch("snakebite.commandlineparser.CommandLineParser._read_config_snakebiterc", return_value=None)
    def test_config_no_config(self, config_mock, read_config_mock):
        hadoop_home = None
        config_mock.return_value = []
        if os.environ.get("HADOOP_HOME"):
            hadoop_home = os.environ["HADOOP_HOME"]
            del os.environ["HADOOP_HOME"]
        self.parser.args = MockParseArgs()
        with self.assertRaises(SystemExit):
            self.parser.read_config()

        if hadoop_home:
            os.environ["HADOOP_HOME"] = hadoop_home

        self.assert_namenode_spec(None, None)


    valid_snake_one_rc = {"namenode": "foobar", "version": 9, "port": 54310}
    valid_snake_ha_rc = [{"namenode": "foobar", "version": 9, "port": 54310},
                         {"namenode": "foobar2", "version": 9, "port": 54310}]

    invalid_snake_rc = "hdfs://foobar:54310"

    @patch("os.path.exists")
    def test_read_config_snakebiterc_one_valid(self, exists_mock):
        m = mock_open(read_data=json.dumps(self.valid_snake_one_rc))

        with patch("snakebite.commandlineparser.open", m, create=True):
            self.parser.args = MockParseArgs()
            self.parser.read_config()
            self.assert_namenodes_spec("foobar", 54310, 9)
            self.assertEquals(self.parser.args.usetrash, HDFSConfig.use_trash)

    @patch("os.path.exists")
    def test_read_config_snakebiterc_ha_valid(self, exists_mock):
        m = mock_open(read_data=json.dumps(self.valid_snake_ha_rc))

        with patch("snakebite.commandlineparser.open", m, create=True):
            self.parser.args = MockParseArgs()
            self.parser.read_config()
            self.assert_namenodes_spec("foobar", 54310, 9)
            self.assert_namenodes_spec("foobar2", 54310, 9)
            self.assertEquals(self.parser.args.usetrash, HDFSConfig.use_trash)

    @patch("os.path.exists")
    def test_read_config_snakebiterc_invalid(self, exists_mock):
        m = mock_open(read_data=json.dumps(self.invalid_snake_rc))

        with patch("snakebite.commandlineparser.open", m, create=True):
            self.parser.args = MockParseArgs()
            with self.assertRaises(SystemExit):
                self.parser.read_config()

    valid_snake_noport_one_rc = {"namenode": "foobar", "version": 11}
    valid_snake_noport_ha_rc = [{"namenode": "foobar", "version": 100},
                                {"namenode": "foobar2", "version": 100}]

    @patch("os.path.exists")
    def test_read_config_snakebiterc_noport_one_valid(self, exists_mock):
        m = mock_open(read_data=json.dumps(self.valid_snake_noport_one_rc))

        with patch("snakebite.commandlineparser.open", m, create=True):
            self.parser.args = MockParseArgs()
            self.parser.read_config()
            self.assert_namenodes_spec("foobar", Namenode.DEFAULT_PORT, 11)
            self.assertEquals(self.parser.args.usetrash, HDFSConfig.use_trash)

    @patch("os.path.exists")
    def test_read_config_snakebiterc_noport_ha_valid(self, exists_mock):
        m = mock_open(read_data=json.dumps(self.valid_snake_noport_ha_rc))

        with patch("snakebite.commandlineparser.open", m, create=True):
            self.parser.args = MockParseArgs()
            self.parser.read_config()
            self.assert_namenodes_spec("foobar", Namenode.DEFAULT_PORT, 100)
            self.assert_namenodes_spec("foobar2", Namenode.DEFAULT_PORT, 100)
            self.assertEquals(self.parser.args.usetrash, HDFSConfig.use_trash)


    valid_snake_noport_nov_one_rc = {"namenode": "foobar"}
    valid_snake_noport_nov_ha_rc = [{"namenode": "foobar"},
                                    {"namenode": "foobar2"}]

    @patch("os.path.exists")
    def test_read_config_snakebiterc_noport_nov_one_valid(self, exists_mock):
        m = mock_open(read_data=json.dumps(self.valid_snake_noport_nov_one_rc))

        with patch("snakebite.commandlineparser.open", m, create=True):
            self.parser.args = MockParseArgs()
            self.parser.read_config()
            self.assert_namenodes_spec("foobar", Namenode.DEFAULT_PORT, Namenode.DEFAULT_VERSION)
            self.assertEquals(self.parser.args.usetrash, HDFSConfig.use_trash)

    @patch("os.path.exists")
    def test_read_config_snakebiterc_noport_nov_ha_valid(self, exists_mock):
        m = mock_open(read_data=json.dumps(self.valid_snake_noport_nov_ha_rc))

        with patch("snakebite.commandlineparser.open", m, create=True):
            self.parser.args = MockParseArgs()
            self.parser.read_config()
            self.assert_namenodes_spec("foobar", Namenode.DEFAULT_PORT, Namenode.DEFAULT_VERSION)
            self.assert_namenodes_spec("foobar2", Namenode.DEFAULT_PORT, Namenode.DEFAULT_VERSION)
            self.assertEquals(self.parser.args.usetrash, HDFSConfig.use_trash)

    valid_snake_noport_mix_rc = [{"namenode": "foobar", "version": 100},
                                 {"namenode": "foobar2", "port": 66}]

    @patch("os.path.exists")
    def test_read_config_snakebiterc_noport_mix_valid(self, exists_mock):
        m = mock_open(read_data=json.dumps(self.valid_snake_noport_mix_rc))

        with patch("snakebite.commandlineparser.open", m, create=True):
            self.parser.args = MockParseArgs()
            self.parser.read_config()
            self.assert_namenodes_spec("foobar", Namenode.DEFAULT_PORT, 100)
            self.assert_namenodes_spec("foobar2", 66, Namenode.DEFAULT_VERSION)
            self.assertEquals(self.parser.args.usetrash, HDFSConfig.use_trash)

    valid_snake_one_rc_v2 = {
                                "config_version": 2,
                                "use_trash": False,
                                "namenodes": [
                                    {"host": "foobar3", "version": 9, "port": 54310}
                                ]
                            }

    valid_snake_ha_rc_v2 = {
                                "config_version": 2,
                                "use_trash": True,
                                "namenodes": [
                                    {"host": "foobar4", "version": 9, "port": 54310},
                                    {"host": "foobar5", "version": 9, "port": 54310}
                                ]
                            }

    invalid_snake_rc_v2 = "hdfs://foobar:54310"

    @patch("os.path.exists")
    def test_read_config_snakebiterc_one_valid_v2(self, exists_mock):
        m = mock_open(read_data=json.dumps(self.valid_snake_one_rc_v2))

        with patch("snakebite.commandlineparser.open", m, create=True):
            self.parser.args = MockParseArgs()
            self.parser.read_config()
            self.assertFalse(self.parser.args.usetrash)
            self.assert_namenodes_spec("foobar3", 54310, 9)

    @patch("os.path.exists")
    def test_read_config_snakebiterc_ha_valid_v2(self, exists_mock):
        m = mock_open(read_data=json.dumps(self.valid_snake_ha_rc_v2))

        with patch("snakebite.commandlineparser.open", m, create=True):
            self.parser.args = MockParseArgs()
            self.parser.read_config()
            self.assertTrue(self.parser.args.usetrash)
            self.assert_namenodes_spec("foobar4", 54310, 9)
            self.assert_namenodes_spec("foobar5", 54310, 9)


    @patch("os.path.exists")
    def test_read_config_snakebiterc_invalid_v2(self, exists_mock):
        m = mock_open(read_data=json.dumps(self.invalid_snake_rc_v2))

        with patch("snakebite.commandlineparser.open", m, create=True):
            self.parser.args = MockParseArgs()
            with self.assertRaises(SystemExit):
                self.parser.read_config()


    valid_snake_noport_one_rc_v2 = {
                                    "config_version": 2,
                                    "use_trash": False,
                                    "namenodes": [
                                        {"host": "foobar3", "version": 9}
                                    ]
                                   }

    valid_snake_mix_ha_rc_v2 = {
                                   "config_version": 2,
                                   "use_trash": True,
                                   "namenodes": [
                                        {"host": "foobar4", "version": 100},
                                        {"host": "foobar5", "port": 54310}
                                    ]
                                  }

    @patch("os.path.exists")
    def test_read_config_snakebiterc_noport_one_valid_v2(self, exists_mock):
        m = mock_open(read_data=json.dumps(self.valid_snake_noport_one_rc_v2))

        with patch("snakebite.commandlineparser.open", m, create=True):
            self.parser.args = MockParseArgs()
            self.parser.read_config()
            self.assertFalse(self.parser.args.usetrash)
            self.assert_namenodes_spec("foobar3", Namenode.DEFAULT_PORT, 9)

    @patch("os.path.exists")
    def test_read_config_snakebiterc_mix_ha_valid_v2(self, exists_mock):
        m = mock_open(read_data=json.dumps(self.valid_snake_mix_ha_rc_v2))

        with patch("snakebite.commandlineparser.open", m, create=True):
            self.parser.args = MockParseArgs()
            self.parser.read_config()
            self.assertTrue(self.parser.args.usetrash)
            self.assert_namenodes_spec("foobar4", Namenode.DEFAULT_PORT, 100)
            self.assert_namenodes_spec("foobar5", 54310, Namenode.DEFAULT_VERSION)


    def test_cl_default_port(self):
        self.parser.args = MockParseArgs(dir=["hdfs://foobar/user/rav"],
                                         single_arg="hdfs://foobar/user/rav",
                                         command="mv")
        self.parser.read_config()
        self.assert_namenode_spec("foobar", Namenode.DEFAULT_PORT)

    def test_cl_trash_setting_preserved_after_cl_config(self):
        # no snakebiterc
        # read config from CL
        self.parser.args = MockParseArgs(dir=["hdfs://foobar:50070/user/rav"],
                                         skiptrash=True,
                                         command="rm")
        self.parser.read_config()
        self.assert_namenode_spec("foobar", 50070)
        self.assert_namenodes_spec("foobar", 50070)
        self.assertEquals(self.parser.args.skiptrash, True)

    def _revert_hdfs_try_paths(self):
        # Make sure HDFSConfig is in vanilla state
        HDFSConfig.use_trash = False
        HDFSConfig.hdfs_try_paths = ConfigTest.original_hdfs_try_path
        HDFSConfig.core_try_paths = ConfigTest.original_core_try_path

    @patch("os.path.exists")
    def test_cl_trash_setting_preserved_after_snakebiterc_one_valid(self, exists_mock):
        m = mock_open(read_data=json.dumps(self.valid_snake_one_rc))

        with patch("snakebite.commandlineparser.open", m, create=True):
            self.parser.args = MockParseArgs(usetrash=True)
            self.parser.read_config()
            self.assert_namenodes_spec("foobar", 54310, 9)
            self.assertTrue(self.parser.args.usetrash)


    @patch('os.environ.get')
    def test_cl_usetrash_setting_preserved_after_external_nontrash_config(self, environ_get):
        environ_get.return_value = False
        # no snakebiterc
        # read external config (hdfs-site, core-site)
        self.parser.args = MockParseArgs(dir=["/user/rav/test"],
                                         usetrash=True,
                                         command="rm")
        try:
            HDFSConfig.core_try_paths = (ConfigTest.get_config_path('ha-core-site.xml'),)
            HDFSConfig.hdfs_try_paths = (ConfigTest.get_config_path('ha-noport-hdfs-site.xml'),)
            self.parser.init()

            self.assertTrue(self.parser.args.usetrash)
            self.assertTrue(self.parser.client.use_trash)
        finally:
            self._revert_hdfs_try_paths()

    @patch('os.environ.get')
    def test_cl_skiptrash_setting_preserved_after_external_nontrash_config(self, environ_get):
        environ_get.return_value = False
        # no snakebiterc
        # read external config (hdfs-site, core-site)
        self.parser.args = MockParseArgs(dir=["/user/rav/test"],
                                         skiptrash=True,
                                         usetrash=True,
                                         command="rm")
        try:
            HDFSConfig.core_try_paths = (ConfigTest.get_config_path('ha-core-site.xml'),)
            HDFSConfig.hdfs_try_paths = (ConfigTest.get_config_path('ha-noport-hdfs-site.xml'),)
            self.parser.init()

            self.assertTrue(self.parser.args.skiptrash)
            self.assertTrue(self.parser.args.usetrash)
            self.assertFalse(self.parser.client.use_trash)
        finally:
            self._revert_hdfs_try_paths()

########NEW FILE########
__FILENAME__ = config_test
import unittest2
import os
import time

import os.path
import snakebite
from snakebite.config import HDFSConfig
from snakebite.client import AutoConfigClient
from mock import patch, mock_open

class ConfigTest(unittest2.TestCase):
    original_hdfs_try_path = set(HDFSConfig.hdfs_try_paths)
    original_core_try_path = set(HDFSConfig.core_try_paths)

    def setUp(self):
        # Make sure HDFSConfig is in vanilla state
        HDFSConfig.use_trash = False
        HDFSConfig.hdfs_try_paths = self.original_hdfs_try_path
        HDFSConfig.core_try_paths = self.original_core_try_path

    @staticmethod
    def get_config_path(config_name):
        return os.path.abspath(os.path.join(snakebite.__file__, os.pardir, os.pardir, 'test/testconfig/conf/%s' % config_name))

    def _verify_hdfs_settings(self, config):
        self.assertEquals(len(config), 2)
        # assert first NN
        self.assertEqual('namenode1.mydomain', config[0]['namenode'])
        self.assertEqual(8888, config[0]['port'])
        # assert second NN
        self.assertEqual('namenode2.mydomain', config[1]['namenode'])
        self.assertEqual(8888, config[1]['port'])

    def _verify_hdfs_noport_settings(self, config):
        self.assertEquals(len(config), 2)
        # assert first NN
        self.assertEqual('namenode1.mydomain', config[0]['namenode'])
        self.assertEqual(8020, config[0]['port'])
        # assert second NN
        self.assertEqual('namenode2.mydomain', config[1]['namenode'])
        self.assertEqual(8020, config[1]['port'])

    def test_read_hdfs_config_ha(self):
        hdfs_site_path = self.get_config_path('ha-port-hdfs-site.xml')
        config = HDFSConfig.read_hdfs_config(hdfs_site_path)
        self._verify_hdfs_settings(config)

    def test_read_core_config_ha(self):
        core_site_path = self.get_config_path('ha-core-site.xml')
        config = HDFSConfig.read_core_config(core_site_path)
        self.assertEquals(len(config), 1)
        self.assertEquals('testha', config[0]['namenode'])
        self.assertEquals(8020, config[0]['port'])
        self.assertFalse(HDFSConfig.use_trash)

    @patch('os.environ.get')
    def test_read_config_ha_with_ports(self, environ_get):
        environ_get.return_value = False
        HDFSConfig.core_try_paths = (self.get_config_path('ha-core-site.xml'),)
        HDFSConfig.hdfs_try_paths = (self.get_config_path('ha-port-hdfs-site.xml'),)
        config = HDFSConfig.get_external_config()

        self._verify_hdfs_settings(config)

    @patch('os.environ.get')
    def test_read_config_non_ha_with_ports(self, environ_get):
        environ_get.return_value = False
        HDFSConfig.core_try_paths = (self.get_config_path('non-ha-port-core-site.xml'),)
        HDFSConfig.hdfs_try_paths = ()
        config = HDFSConfig.get_external_config()

        self.assertEquals(len(config), 1)
        self.assertEquals(config[0]['namenode'], 'testhost.net')
        self.assertEquals(config[0]['port'], 8888)
        self.assertFalse(HDFSConfig.use_trash)

    @patch('os.environ.get')
    def test_ha_without_ports(self, environ_get):
        environ_get.return_value = False
        HDFSConfig.core_try_paths = (self.get_config_path('ha-core-site.xml'),)
        HDFSConfig.hdfs_try_paths = (self.get_config_path('ha-noport-hdfs-site.xml'),)
        config = HDFSConfig.get_external_config()

        self._verify_hdfs_noport_settings(config)

    @patch('os.environ.get')
    def test_ha_config_trash_in_core(self, environ_get):
        environ_get.return_value = False
        HDFSConfig.core_try_paths = (self.get_config_path('core-with-trash.xml'),)
        HDFSConfig.hdfs_try_paths = (self.get_config_path('ha-noport-hdfs-site.xml'),)
        config = HDFSConfig.get_external_config()

        self._verify_hdfs_noport_settings(config)
        self.assertTrue(HDFSConfig.use_trash)

    @patch('os.environ.get')
    def test_ha_config_trash_in_hdfs(self, environ_get):
        environ_get.return_value = False
        HDFSConfig.core_try_paths = (self.get_config_path('ha-core-site.xml'),)
        HDFSConfig.hdfs_try_paths = (self.get_config_path('ha-noport-trash-hdfs-site.xml'),)
        config = HDFSConfig.get_external_config()

        self._verify_hdfs_noport_settings(config)
        self.assertTrue(HDFSConfig.use_trash)

    @patch('os.environ.get')
    def test_autoconfig_client_trash_true(self, environ_get):
        environ_get.return_value = False
        HDFSConfig.core_try_paths = (self.get_config_path('ha-core-site.xml'),)
        HDFSConfig.hdfs_try_paths = (self.get_config_path('ha-noport-trash-hdfs-site.xml'),)
        client = AutoConfigClient()
        self.assertTrue(client.use_trash)

    @patch('os.environ.get')
    def test_autoconfig_client_trash_false(self, environ_get):
        environ_get.return_value = False
        HDFSConfig.core_try_paths = (self.get_config_path('ha-core-site.xml'),)
        HDFSConfig.hdfs_try_paths = (self.get_config_path('ha-noport-hdfs-site.xml'),)
        client = AutoConfigClient()
        self.assertFalse(client.use_trash)

########NEW FILE########
__FILENAME__ = copytolocal_test
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
import os
import shutil

from minicluster_testbase import MiniClusterTestBase

TESTFILES_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "testfiles/temp_testfiles")

class CopyToLocalTest(MiniClusterTestBase):

    def tearDown(self):
        shutil.rmtree(TESTFILES_PATH)

    def test_copyToLocal_file_content(self):
        os.mkdir(TESTFILES_PATH)
        target_dir = "%s/file1" % TESTFILES_PATH

        expected_content = self.cluster.cat('/test3')
        for result in self.client.copyToLocal(['/test3'], target_dir):
            self.assertEqual(result['path'], target_dir)

            client_content = self._read_file(target_dir)
            self.assertEqual(client_content, expected_content)

            self.assertEqual(result['result'], True)

    def test_copyToLocal_conflicting_names(self):
        os.mkdir(TESTFILES_PATH)
        target_dir = "%s/file2" % TESTFILES_PATH

        self.cluster.copyToLocal('/test3', target_dir)
        for result in self.client.copyToLocal(['/test3'], target_dir):
            self.assertEqual(result['error'], "file exists")
            self.assertEqual(result['path'], target_dir)
            self.assertEqual(result['result'], False)

    def test_copyToLocal_directory_structure(self):
        test_dir = '%s/actual' % TESTFILES_PATH
        expected_dir = '%s/expected' % TESTFILES_PATH
        os.mkdir(TESTFILES_PATH)
        os.mkdir(expected_dir)
        os.mkdir(test_dir)
        expected_dir_structure = []
        test_dir_structure = []

        self.cluster.copyToLocal('/bar', expected_dir)

        for result in self.client.copyToLocal(['/bar'], test_dir):
            self.assertEqual(result['result'], True)

        for path, dirs, files in os.walk(expected_dir):
            expected_dir_structure.append(path.replace('/expected', "", 1))
            for f in files:
                f = "%s/%s" % (path, f)
                data = self._read_file(f)
                expected_dir_structure.append((f.replace('/expected', "", 1), data))

        for path, dirs, files in os.walk(test_dir):
            test_dir_structure.append(path.replace('/actual', "", 1))
            for f in files:
                f = "%s/%s" % (path, f)
                data = self._read_file(f)
                test_dir_structure.append((f.replace('/actual', "", 1), data))

        self.assertEqual(expected_dir_structure, test_dir_structure)

    def _read_file(self, file):
        f = open(file, 'r')
        data = f.read()
        f.close()
        return data

########NEW FILE########
__FILENAME__ = count_test
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
from snakebite.errors import FileNotFoundException
from snakebite.errors import InvalidInputException
from minicluster_testbase import MiniClusterTestBase


class CountTest(MiniClusterTestBase):
    def test_count_path(self):
        client_output = sorted(self.client.count(["/"]), key=lambda node: node['path'])
        expected_output = sorted(self.cluster.count(["/"]), key=lambda node: node['path'])
        self.assertEqual(len(client_output), len(client_output))
        for i, expected_node in enumerate(expected_output):
            client_node = client_output[i]
            for key in ['path', 'length', 'directoryCount', 'fileCount']:
                self.assertEqual(client_node[key], expected_node[key])

    def test_count_multi(self):
        client_output = sorted(self.client.count(["/", "/dir1"]), key=lambda node: node['path'])
        expected_output = sorted(self.cluster.count(["/", "/dir1"]), key=lambda node: node['path'])
        self.assertEqual(len(client_output), len(client_output))
        for i, expected_node in enumerate(expected_output):
            client_node = client_output[i]
            for key in ['path', 'length', 'directoryCount', 'fileCount']:
                self.assertEqual(client_node[key], expected_node[key])

    def test_unknown_file(self):
        result = self.client.count(['/doesnotexist'])
        self.assertRaises(FileNotFoundException, result.next)

    def test_invalid_input(self):
        result = self.client.count('/stringpath')
        self.assertRaises(InvalidInputException, result.next)

########NEW FILE########
__FILENAME__ = delete_test
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
from snakebite.errors import FileNotFoundException
from snakebite.errors import InvalidInputException
from minicluster_testbase import MiniClusterTestBase

import pwd
import os
import re

class DeleteTest(MiniClusterTestBase):
    def test_delete_file(self):
        before_state = set([node['path'] for node in self.client.ls(['/'])])
        list(self.client.delete(['/zerofile']))
        after_state = set([node['path'] for node in self.client.ls(['/'])])
        self.assertEqual(len(after_state), len(before_state) - 1)
        self.assertFalse('/zerofile' in after_state)

    def test_delete_multi(self):
        before_state = set([node['path'] for node in self.client.ls(['/'])])
        list(self.client.delete(['/test1', '/test2']))
        after_state = set([node['path'] for node in self.client.ls(['/'])])
        self.assertEqual(len(after_state), len(before_state) - 2)
        self.assertFalse('/test1' in after_state or '/test2' in after_state)

    def test_unknown_file(self):
        result = self.client.delete(['/doesnotexist'])
        self.assertRaises(FileNotFoundException, result.next)

    def test_invalid_input(self):
        result = self.client.delete('/stringpath')
        self.assertRaises(InvalidInputException, result.next)

    def test_recurse(self):
        list(self.client.delete(['/foo'], recurse=True))
        client_output = self.client.ls(['/'])
        paths = [node['path'] for node in client_output]
        self.assertFalse('/foo' in paths)

    def test_glob(self):
        list(self.client.delete(['/ba*'], recurse=True))
        client_output = self.client.ls(['/'])
        paths = [node['path'] for node in client_output]
        self.assertFalse('/bar' in paths)

class DeleteWithTrashTest(MiniClusterTestBase):
    def setUp(self):
        super(DeleteWithTrashTest, self).setUp()
        self.client.use_trash = True
        self.username = pwd.getpwuid(os.getuid())[0]
        self.trash_location = "/user/%s/.Trash/Current" % self.username

    def assertNotExists(self, location_under_test):
        self.assertFalse(self.client.test(location_under_test, exists=True))

    def assertExists(self, location_under_test):
        self.assertTrue(self.client.test(location_under_test, exists=True))

    def assertTrashExists(self):
        list(self.client.ls([self.trash_location]))

    def assertInTrash(self, location_under_test):
        self.assertTrashExists()
        trash_location = "%s%s" % (self.trash_location, location_under_test)
        self.assertTrue(self.client.test(trash_location, exists=True))

    def assertNotInTrash(self, location_under_test):
        self.assertTrashExists()
        trash_location = "%s%s" % (self.trash_location, location_under_test)
        self.assertFalse(self.client.test(trash_location, exists=True))

    def assertSecondaryTrash(self, location_under_test):
        client_output = self.client.ls([self.trash_location])
        regex = r"%s/bar\d{13}" % self.trash_location
        augmented_path = [n['path'] for n in client_output if re.match(regex, n['path'])][0]

        trash_contents = set([node['path'] for node in self.client.ls([augmented_path])])
        self.assertTrue(location_under_test % augmented_path in trash_contents)

    def test_delete_file(self):
        location_under_test = '/zerofile'
        list(self.client.delete([location_under_test]))
        self.assertNotExists(location_under_test)
        self.assertInTrash(location_under_test)

    def test_delete_multi(self):
        locations_under_test = ['/test1', '/test2']
        list(self.client.delete(locations_under_test))
        for location_under_test in locations_under_test:
            self.assertNotExists(location_under_test)
            self.assertInTrash(location_under_test)

    def test_unknown_file(self):
        result = self.client.delete(['/doesnotexist'])
        self.assertRaises(FileNotFoundException, result.next)

    def test_invalid_input(self):
        result = self.client.delete('/stringpath')
        self.assertRaises(InvalidInputException, result.next)

    def test_recurse(self):
        location_under_test = '/foo'
        list(self.client.delete(['/foo'], recurse=True))
        self.assertNotExists(location_under_test)
        self.assertInTrash(location_under_test)

    def test_subdir(self):
        location_under_test = "/bar/baz"

        list(self.client.delete([location_under_test], recurse=True))

        # Check if /bar still exists, but also that /user/<myuser>/.Trash/Current/bar has been created
        self.assertExists(os.path.dirname(location_under_test))
        self.assertInTrash(os.path.dirname(location_under_test))

        self.assertNotExists(location_under_test)
        self.assertInTrash(location_under_test)

        # Remove /bar and see if a 2nd version was created
        location_under_test = "/bar"
        list(self.client.delete([location_under_test], recurse=True))
        self.assertNotExists(location_under_test)

        client_output = self.client.ls([self.trash_location])
        regex = r"%s/bar\d{13}" % self.trash_location
        augmented_path = [n['path'] for n in client_output if re.match(regex, n['path'])][0]
        self.assertExists("%s%s" % (augmented_path, "/foo"))

    def test_glob(self):
        list(self.client.delete(['/zipped/*'], recurse=True))
        self.assertNotExists('/zipped/test1.gz')
        self.assertNotExists('/zipped/test1.bz2')
        self.assertInTrash('/zipped/test1.gz')
        self.assertInTrash('/zipped/test1.bz2')

    def test_path_in_trash(self):
        location_under_test = '/test3'
        list(self.client.delete([location_under_test]))
        self.assertInTrash(location_under_test)
        list(self.client.delete(["%s%s" % (self.trash_location, location_under_test)]))
        self.assertNotInTrash(location_under_test)

    def test_delete_trash_parent(self):
        list(self.client.delete(['/test4']))
        try_path = [os.path.dirname(os.path.dirname(self.trash_location))]

        with self.assertRaises(Exception):
            list(self.client.delete(try_path))

########NEW FILE########
__FILENAME__ = df_test
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
import re
import unittest2

from minicluster_testbase import MiniClusterTestBase

from snakebite.formatter import format_fs_stats

class DfTest(MiniClusterTestBase):

    def test_df(self):
        client_output = self.client.df()
        expected_output = self.cluster.df("/").split("\n")[1]

        (filesystem, capacity, used, remaining, pct) = re.split("\s+", expected_output)

        self.assertEqual(filesystem, client_output["filesystem"])
        self.assertEqual(long(capacity), client_output["capacity"])
        self.assertEqual(long(used), client_output["used"])

class StatsMock(dict):
    def __init__(self, capacity,
            used,
            remaining,
            under_replicated,
            corrupted_blocks,
            missing_blocks,
            filesystem):
        super(StatsMock, self).__init__({"capacity": capacity,
            "used":     used,
            "remaining": remaining,
            "under_replicated": under_replicated,
            "corrupted_blocks": corrupted_blocks,
            "missing_blocks": missing_blocks,
            "filesystem": filesystem})

class DfFormatTest(unittest2.TestCase):
  
    def test_middle(self):
        fake = StatsMock(100, 50, 50, 0, 0, 0, "foobar.com")
        output = format_fs_stats(fake).next().split('\n')
        stats = output[1].split()
        self.assertEqual(stats[4], "50.00%")
        self.assertEqual(stats[0], "foobar.com")

    def test_frag(self):
        fake = StatsMock(312432, 23423, 289009, 0, 0, 0, "foobar.com")
        output = format_fs_stats(fake).next().split('\n')
        stats = output[1].split()
        self.assertEqual(stats[4], "7.50%")
 
    def test_zero_size(self):
        fake = StatsMock(0, 0, 0, 0, 0, 0, "foobar.com")
        output = format_fs_stats(fake).next().split('\n')
        stats = output[1].split()
        self.assertEqual(stats[4], "0.00%")

    def test_corrupted_zero_size(self):
        fake = StatsMock(0, 50, 50, 0, 0, 0, "foobar.com")
        output = format_fs_stats(fake).next().split('\n')
        stats = output[1].split()
        self.assertEqual(stats[4], "0.00%")

    def test_full_size(self):
        fake = StatsMock(50, 50, 0, 0, 0, 0, "foobar.com")
        output = format_fs_stats(fake).next().split('\n')
        stats = output[1].split()
        self.assertEqual(stats[4], "100.00%")


 

########NEW FILE########
__FILENAME__ = du_test
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
from snakebite.errors import FileNotFoundException
from snakebite.errors import InvalidInputException
from minicluster_testbase import MiniClusterTestBase
from util import assertDu


class DfTest(MiniClusterTestBase):

    def test_onepath(self):
        client_output = sorted(self.client.du(['/']), key=lambda node: node['path'])
        expected_output = sorted(self.cluster.du('/'), key=lambda node: node['path'])
        assertDu(expected_output, client_output, self.assertEqual, self.assertEqual)

    def test_multipath(self):
        client_output = sorted(self.client.du(['/', '/dir1']), key=lambda node: node['path'])
        expected_output = sorted(self.cluster.du(['/', '/dir1']), key=lambda node: node['path'])
        assertDu(expected_output, client_output, self.assertEqual, self.assertEqual)

    def test_toplevel(self):
        client_output = sorted(self.client.du(['/'], include_toplevel=True, include_children=False), key=lambda node: node['path'])
        expected_output = sorted(self.cluster.du(['/'], ['-s']), key=lambda node: node['path'])
        self.assertEqual(len(client_output), 1)
        assertDu(expected_output, client_output, self.assertEqual, self.assertEqual)

    def test_unknown_file(self):
        result = self.client.du(['/nonexistent'])
        self.assertRaises(FileNotFoundException, result.next)

    def test_invalid_input(self):
        result = self.client.du('/stringpath')
        self.assertRaises(InvalidInputException, result.next)

########NEW FILE########
__FILENAME__ = effective_user_test
from minicluster_testbase import MiniClusterTestBase
from snakebite.client import Client
import os

class EffectiveUserTest(MiniClusterTestBase):
    ERR_MSG_TOUCH = "org.apache.hadoop.security.AccessControlException\nPermission denied: user=__foobar"
    ERR_MSG_STAT = "`/foobar2': No such file or directory"

    VALID_FILE = '/foobar'
    INVALID_FILE = '/foobar2'

    def setUp(self):
        self.custom_client = Client(self.cluster.host, self.cluster.port)
        self.custom_foobar_client = Client(host=self.cluster.host,
                                           port=self.cluster.port,
                                           effective_user='__foobar')

    def test_touch(self):
        print tuple(self.custom_client.touchz([self.VALID_FILE]))
        try:
            tuple(self.custom_foobar_client.touchz([self.INVALID_FILE]))
	except Exception, e:
            self.assertTrue(e.message.startswith(self.ERR_MSG_TOUCH))

        self.custom_client.stat([self.VALID_FILE])
        try:
            self.custom_client.stat([self.INVALID_FILE])
        except Exception, e:
            self.assertEquals(e.message, self.ERR_MSG_STAT)

########NEW FILE########
__FILENAME__ = getmerge_test
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
from minicluster_testbase import MiniClusterTestBase

import os
import shutil


TESTFILES_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "testfiles/temp_testfiles")


class GetmergeTest(MiniClusterTestBase):

    def tearDown(self):
        shutil.rmtree(TESTFILES_PATH)

    def test_getmerge_file(self):
        os.mkdir(TESTFILES_PATH)
        self.cluster.getmerge('/test3', '%s/expected' % TESTFILES_PATH)
        expected_output = self._read_file('%s/expected' % TESTFILES_PATH)
        self.client.getmerge('/test3', '%s/client' % TESTFILES_PATH).next()
        client_output = self._read_file('%s/client' % TESTFILES_PATH)
        self.assertEqual(expected_output, client_output)

    def test_getmerge_directory(self):
        os.mkdir(TESTFILES_PATH)
        self.cluster.getmerge('/dir2/dir3', '%s/expected' % TESTFILES_PATH)
        expected_output = self._read_file('%s/expected' % TESTFILES_PATH)
        self.client.getmerge('/dir2/dir3', '%s/client' % TESTFILES_PATH).next()
        client_output = self._read_file('%s/client' % TESTFILES_PATH)
        self.assertEqual(expected_output, client_output)

    def test_getmerge_directory_nl(self):
        os.mkdir(TESTFILES_PATH)
        self.cluster.getmerge('/dir2/dir3', '%s/expected' % TESTFILES_PATH, extra_args=['-nl'])
        expected_output = self._read_file('%s/expected' % TESTFILES_PATH)
        self.client.getmerge('/dir2/dir3', '%s/client' % TESTFILES_PATH, newline=True).next()
        client_output = self._read_file('%s/client' % TESTFILES_PATH)
        self.assertEqual(expected_output, client_output)

    def test_getmerge_directory_tree(self):
        # Should only merge files in the specified directory level (don't recurse)
        os.mkdir(TESTFILES_PATH)
        self.cluster.getmerge('/dir2', '%s/expected' % TESTFILES_PATH)
        expected_output = self._read_file('%s/expected' % TESTFILES_PATH)
        self.client.getmerge('/dir2', '%s/client' % TESTFILES_PATH).next()
        client_output = self._read_file('%s/client' % TESTFILES_PATH)
        self.assertEqual(expected_output, client_output)

    def _read_file(self, file):
        f = open(file, 'r')
        data = f.read()
        f.close()
        return data

########NEW FILE########
__FILENAME__ = glob_test
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
import unittest2
import snakebite.glob as glob


class GlobTest(unittest2.TestCase):
    def test_path_expansion(self):
        paths = ['/foo/bar', '/foo/{bar,baz}', '/foo/{bar,baz}/{quz,quux}', '/foo/{bar,baz}/{quz,quux}/{aap}']
        expected = ['/foo/bar', '/foo/bar', '/foo/baz', '/foo/bar/quz', '/foo/bar/quux', '/foo/baz/quz', '/foo/baz/quux', '/foo/bar/quz/aap', '/foo/bar/quux/aap', '/foo/baz/quz/aap', '/foo/baz/quux/aap']
        new_paths = glob.expand_paths(paths)
        self.assertEqual(new_paths, expected)

########NEW FILE########
__FILENAME__ = list_test
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
from util import assertListings
from minicluster_testbase import MiniClusterTestBase
from snakebite.errors import FileNotFoundException
from snakebite.errors import InvalidInputException


class ListTest(MiniClusterTestBase):

    def test_toplevel_root(self):
        expected_output = self.cluster.ls(['/'])
        client_output = list(self.client.ls(['/']))
        assertListings(expected_output, client_output, self.assertEqual, self.assertEqual)

    def test_toplevel_dir(self):
        client_output = list(self.client.ls(['/dir1'], include_toplevel=True, include_children=False))
        self.assertEqual(len(client_output), 1)
        self.assertEqual(client_output[0]['file_type'], 'd')
        self.assertEqual(client_output[0]['length'], 0)

    def test_zerofile(self):
        client_output = list(self.client.ls(['/zerofile'], include_toplevel=True, include_children=False))
        self.assertEqual(len(client_output), 1)
        self.assertEqual(client_output[0]['file_type'], 'f')
        self.assertEqual(client_output[0]['length'], 0)

    def test_file_even_if_toplevel_is_false(self):
        client_output = list(self.client.ls(['/zerofile']))
        self.assertEqual(len(client_output), 1)
        self.assertEqual(client_output[0]['file_type'], 'f')
        self.assertEqual(client_output[0]['length'], 0)

    def test_root_incl_toplevel(self):
        expected_output = self.cluster.ls(['/'])
        client_output = list(self.client.ls(['/'], include_toplevel=True, include_children=True))
        self.assertEqual(len(client_output), len(expected_output) + 1)

    def test_root_recursive(self):
        expected_output = self.cluster.ls(['/'], ['-R'])
        client_output = list(self.client.ls(['/'], include_toplevel=False, recurse=True))
        assertListings(expected_output, client_output, self.assertEqual, self.assertEqual)

    def test_multiple_files(self):
        client_output = self.client.ls(['/zerofile', '/dir1'], include_toplevel=True, include_children=False)
        client_output = sorted(client_output, key=lambda node: node['path'])
        self.assertEqual(len(client_output), 2)
        self.assertEqual(client_output[0]['path'], '/dir1')
        self.assertEqual(client_output[1]['path'], '/zerofile')

    def test_glob(self):
        expected_output = self.cluster.ls(['/b*'])
        client_output = list(self.client.ls(['/b*']))
        self.assertTrue(len(client_output) > 1)
        self.assertTrue(len(expected_output) > 1)
        assertListings(expected_output, client_output, self.assertEqual, self.assertEqual)

        expected_output = self.cluster.ls(['/{foo,bar}'])
        client_output = list(self.client.ls(['/{foo,bar}']))
        self.assertTrue(len(client_output) > 1)
        self.assertTrue(len(expected_output) > 1)
        assertListings(expected_output, client_output, self.assertEqual, self.assertEqual)

        expected_output = self.cluster.ls(['/[fb]*/*/*/qux'])
        client_output = list(self.client.ls(['/[fb]*/*/*/qux']))
        self.assertTrue(len(client_output) > 1)
        self.assertTrue(len(expected_output) > 1)
        assertListings(expected_output, client_output, self.assertEqual, self.assertEqual)

        expected_output = self.cluster.ls(['/{foo,bar}/*/*/qux'])
        client_output = list(self.client.ls(['/{foo,bar}/*/*/qux']))
        self.assertTrue(len(client_output) > 1)
        self.assertTrue(len(expected_output) > 1)
        assertListings(expected_output, client_output, self.assertEqual, self.assertEqual)

        expected_output = self.cluster.ls(['/log/service/2013-05-17/*/Message10'])
        client_output = list(self.client.ls(['/log/service/2013-05-17/*/Message10']))
        self.assertTrue(len(expected_output) > 1)
        self.assertTrue(len(client_output) > 1)
        assertListings(expected_output, client_output, self.assertEqual, self.assertEqual)

        expected_output = self.cluster.ls(['/log/service/2013-05-17/*/Message10/'])
        client_output = list(self.client.ls(['/log/service/2013-05-17/*/Message10/']))
        self.assertTrue(len(expected_output) > 1)
        self.assertTrue(len(client_output) > 1)
        assertListings(expected_output, client_output, self.assertEqual, self.assertEqual)

        expected_output = self.cluster.ls(['/log/service/2013-05-17/*/Message10/*'])
        client_output = list(self.client.ls(['/log/service/2013-05-17/*/Message10/*']))
        self.assertTrue(len(expected_output) > 1)
        self.assertTrue(len(client_output) > 1)
        assertListings(expected_output, client_output, self.assertEqual, self.assertEqual)

    def test_unknown_file(self):
        result = self.client.ls(['/doesnotexist'])
        self.assertRaises(FileNotFoundException, result.next)

    def test_invalid_input(self):
        result = self.client.ls('/stringpath')
        self.assertRaises(InvalidInputException, result.next)

########NEW FILE########
__FILENAME__ = minicluster_testbase
import unittest2
import os
import time
from snakebite.minicluster import MiniCluster
from snakebite.client import Client


class MiniClusterTestBase(unittest2.TestCase):

    cluster = None

    @classmethod
    def setupClass(cls):
        if not cls.cluster:
            # Prevent running tests if a hadoop cluster is reachable. This guard
            # is in place because the MiniCluster java class can break things on
            # a production cluster. The MiniCluster python class is used, but doesn't
            # start an actual cluster. We only use convenience methods to call java
            # hadoop.

            c = MiniCluster(None, start_cluster=False)
            result = c.ls("/")
            if result:
                raise Exception("An active Hadoop cluster is found! Not running tests!")

            testfiles_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "testfiles")
            cls.cluster = MiniCluster(testfiles_path)
            cls.cluster.put("/test1", "/test1")
            cls.cluster.put("/test1", "/test2")
            cls.cluster.put("/test3", "/test3") #1024 bytes
            cls.cluster.put("/test1", "/test4")

            cls.cluster.mkdir("/zipped")
            cls.cluster.put("/zipped/test1.gz", "/zipped")
            cls.cluster.put("/zipped/test1.bz2", "/zipped")

            cls.cluster.put("/zerofile", "/")

            cls.cluster.mkdir("/dir1")
            cls.cluster.put("/zerofile", "/dir1")
            cls.cluster.mkdir("/dir2")
            cls.cluster.mkdir("/dir2/dir3")
            cls.cluster.put("/test1", "/dir2/dir3")
            cls.cluster.put("/test3", "/dir2/dir3")

            cls.cluster.mkdir("/foo/bar/baz", ['-p'])
            cls.cluster.put("/zerofile", "/foo/bar/baz/qux")
            cls.cluster.mkdir("/bar/baz/foo", ['-p'])
            cls.cluster.put("/zerofile", "/bar/baz/foo/qux")
            cls.cluster.mkdir("/bar/foo/baz", ['-p'])
            cls.cluster.put("/zerofile", "/bar/foo/baz/qux")
            cls.cluster.put("/log", "/")

    @classmethod
    def tearDownClass(cls):
        if cls.cluster:
            cls.cluster.terminate()

    def setUp(self):
        version = os.environ.get("HADOOP_PROTOCOL_VER", 9)
        self.cluster = self.__class__.cluster
        self.client = Client(self.cluster.host, self.cluster.port, int(version))

if __name__ == '__main__':
    try:
        MiniClusterTestBase.setupClass()
        while True:
            time.sleep(5)
    finally:
        MiniClusterTestBase.cluster.terminate()

########NEW FILE########
__FILENAME__ = rename_test
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
from snakebite.errors import FileNotFoundException
from snakebite.errors import InvalidInputException
from minicluster_testbase import MiniClusterTestBase


class RenameTest(MiniClusterTestBase):
    def test_rename_file(self):
        print list(self.client.rename(['/zerofile'], '/zerofile2'))
        expected_output = list(self.client.ls(['/zerofile2'], include_toplevel=True))
        self.assertEqual(len(expected_output), 1)
        self.assertEqual(expected_output[0]['path'], '/zerofile2')
        result = self.client.ls(['/zerofile'])
        self.assertRaises(FileNotFoundException, result.next)

    def test_rename_multi(self):
        list(self.client.rename(['/test1', '/test2'], '/dir1'))
        expected_output = self.client.ls(['/dir1'])
        paths = set([node["path"] for node in expected_output])
        for path in ['/dir1/test1', '/dir1/test2']:
            self.assertTrue(path in paths)

    def test_unknown_file(self):
        result = self.client.rename(['/doesnotexist'], '/somewhereelse')
        self.assertRaises(FileNotFoundException, result.next)

    def test_invalid_input(self):
        result = self.client.rename('/stringpath', '777')
        self.assertRaises(InvalidInputException, result.next)

########NEW FILE########
__FILENAME__ = tail_test
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.

from minicluster_testbase import MiniClusterTestBase
import os


class TailTest(MiniClusterTestBase):

    def test_tail_on_one_block(self):
        expected_output = self.cluster.tail('/test1')
        client_output = list(self.client.tail('/test1'))[0]
        self.assertEqual(expected_output, client_output)

    def test_tail_on_file_smaller_than_1KB(self):
        p = self.cluster.put_subprocess('-', '/temp_test')
        print >> p.stdin, "just a couple of bytes"
        p.communicate()

        expected_output = self.cluster.tail('/temp_test')
        client_output = list(self.client.tail('/temp_test'))[0]
        self.assertEqual(expected_output, client_output)

    def test_tail_over_two_blocks(self):  # Last KB of file spans 2 blocks.
        testfiles_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "testfiles")
        f = open('%s/test3' % testfiles_path)

        p = self.cluster.put_subprocess('-', '/temp_test2')
        for _ in xrange(131072):  # 1024 * 131072 = 134,217,728 (default block size)
            f.seek(0)
            for line in f.readlines():
                print >> p.stdin, line
        print >> p.stdin, "some extra bytes to exceed one blocksize"  # +40
        p.communicate()

        expected_output = self.cluster.tail('/temp_test2')
        client_output = list(self.client.tail('/temp_test2'))[0]
        self.assertEqual(expected_output, client_output)

########NEW FILE########
__FILENAME__ = test
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
import sys
import os
import glob
import unittest2

def suite():
    suite = unittest2.TestSuite()
    for filename in glob.glob('test/*_test.py'):
        print filename
        f = os.path.splitext(os.path.basename(filename))[0]
        module = __import__(f)
        suite.addTest(unittest2.defaultTestLoader.loadTestsFromModule(module))
    return suite


class run(unittest2.TestProgram):
    """Runs tests and counts errors."""
    def __init__(self):
        #sys.path.append(test)
        unittest2.TestProgram.__init__(self, '__main__', 'suite')

    def usageExit(self, msg=None):
        if msg:
            print msg
        print self.USAGE % self.__dict__
        sys.exit(-2)

    def runTests(self):
        if self.testRunner is None:
            self.testRunner = unittest2.TextTestRunner(verbosity=self.verbosity)
        result = self.testRunner.run(self.test)
        error_count = len(result.errors) + len(result.failures)
        sys.exit(error_count)

if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = test_test
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
from minicluster_testbase import MiniClusterTestBase


class TestTest(MiniClusterTestBase):
    def test_exists(self):
        result = self.client.test('/zerofile', exists=True)
        self.assertTrue(result)

    def test_not_exist(self):
        result = self.client.test('/zerofile-foo', exists=True)
        self.assertFalse(result)

    def test_dir_exists(self):
        result = self.client.test('/dir1', exists=True, directory=True)
        self.assertTrue(result)

    def test_dir_not_exists(self):
        result = self.client.test('/dir1337', exists=True, directory=True)
        self.assertFalse(result)

    def test_glob(self):
        result = self.client.test('/foo/bar/baz/*', exists=True)
        self.assertTrue(result)

    def test_glob_not_exists(self):
        result = self.client.test('/foo/bar/flep/*', exists=True)
        self.assertFalse(result)

    def test_zero(self):
        result = self.client.test('/zerofile', exists=True, zero_length=True)
        self.assertTrue(result)

########NEW FILE########
__FILENAME__ = text_test
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.

from minicluster_testbase import MiniClusterTestBase


class TextTest(MiniClusterTestBase):
    def test_text_gzip(self):
        expected_output = self.cluster.text('/zipped/test1.gz')
        client_output = list(self.client.text(['/zipped/test1.gz']))[0]
        self.assertEqual(expected_output, client_output)

    def test_text_bzip2(self):
        expected_output = self.cluster.text('/zipped/test1.bz2')
        client_output = list(self.client.text(['/zipped/test1.bz2']))[0]
        self.assertEqual(expected_output, client_output)

########NEW FILE########
__FILENAME__ = util
# -*- coding: utf-8 -*-
# Copyright (c) 2013 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
import datetime


def assertListings(expected, actual, len_method, content_method):
    # Sort both listings by path name
    expected = sorted(expected, key=lambda node: node["path"])
    actual = sorted(actual, key=lambda node: node["path"])

    # Only test for the following attributes, since CLI hadoop doesn't provide all
    # attributes
    test_attributes = ['path', 'permission', 'block_replication', 'owner', 'group',
                   'length', 'modification_time', 'file_type']

    # Assert the length of both listings with the len_method
    len_method(len(actual), len(expected))

    # Assert both listings with the content_method
    for i, expected_node in enumerate(expected):
        client_node = actual[i]
        for attribute in test_attributes:
            # Modification times from CLI hadoop have less granularity,
            # so we transform both listings to an equal format
            if attribute == 'modification_time':
                client_time = datetime.datetime.fromtimestamp(client_node[attribute] / 1000).strftime("%Y%d%m%H%%M")
                expected_time = datetime.datetime.fromtimestamp(expected_node[attribute]).strftime("%Y%d%m%H%%M")
                content_method(expected_time, client_time)
            else:
                content_method(expected_node[attribute], client_node[attribute])


def assertDu(expected, actual, len_method, content_method):
    len_method(len(expected), len(actual))
    for i, expected_node in enumerate(expected):
        client_node = actual[i]
        content_method(expected_node, client_node)

########NEW FILE########
