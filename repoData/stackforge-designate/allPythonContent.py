__FILENAME__ = sample
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from oslo.config import cfg
from designate.openstack.common import log as logging
from designate.notification_handler.base import NotificationHandler

LOG = logging.getLogger(__name__)

# Setup a config group
cfg.CONF.register_group(cfg.OptGroup(
    name='handler:sample',
    title="Configuration for Sample Notification Handler"
))

# Setup the config options
cfg.CONF.register_opts([
    cfg.StrOpt('control-exchange', default='nova'),
    cfg.ListOpt('notification-topics', default=['designate']),
    cfg.StrOpt('domain-name', default='example.org.'),
    cfg.StrOpt('domain-id', default='12345'),
], group='handler:sample')


class SampleHandler(NotificationHandler):
    """ Sample Handler """
    __plugin_name__ = 'sample'

    def get_exchange_topics(self):
        """
        Return a tuple of (exchange, [topics]) this handler wants to receive
        events from.
        """
        exchange = cfg.CONF['handler:sample'].control_exchange

        notification_topics = cfg.CONF['handler:sample'].notification_topics
        notification_topics = [t + ".info" for t in notification_topics]

        return (exchange, notification_topics)

    def get_event_types(self):
        return [
            'compute.instance.create.end'
        ]

    def process_notification(self, context, event_type, payload):
        # Do something with the notification.. e.g:
        domain_id = cfg.CONF['handler:sample'].domain_id
        domain_name = cfg.CONF['handler:sample'].domain_name

        hostname = '%s.%s' % (payload['instance_id'], domain_name)

        for fixed_ip in payload['fixed_ips']:
            if fixed_ip['version'] == 4:
                self.central_api.create_record(domain_id,
                                               type='A',
                                               name=hostname,
                                               data=fixed_ip['address'])

            elif fixed_ip['version'] == 6:
                self.central_api.create_record(domain_id,
                                               type='AAAA',
                                               name=hostname,
                                               data=fixed_ip['address'])

########NEW FILE########
__FILENAME__ = version
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import pbr.version

version_info = pbr.version.VersionInfo('designate-ext-samplehandler')

########NEW FILE########
__FILENAME__ = ipaextractor
# Copyright (C) 2014 Red Hat, Inc.
#
# Author: Rich Megginson <rmeggins@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import sys
import logging
import requests
import uuid
import pprint
import json
import copy
from oslo.config import cfg
from designate.backend import impl_ipa
from designate import utils

logging.basicConfig()
LOG = logging.getLogger(__name__)

cfg.CONF.import_opt('api_base_uri', 'designate.api', 'service:api')
cfg.CONF.import_opt('backend_driver', 'designate.central', 'service:central')


class NoNameServers(Exception):
    pass


class AddServerError(Exception):
    pass


class DeleteServerError(Exception):
    pass


class AddDomainError(Exception):
    pass


class DeleteDomainError(Exception):
    pass


class AddRecordError(Exception):
    pass


cuiberrorstr = """ERROR: You cannot have Designate configured
to use the IPA backend when running this script.  It will wipe
out your IPA DNS data.  Please follow these steps:
* shutdown designate-central
* edit designate.conf
[service:central]
backend_driver = fake # or something other than ipa
* restart designate-central and other designate services
"""


class CannotUseIPABackend(Exception):
    pass


# create mapping of ipa record types to designate types
iparectype2designate = {}
for rectype, tup in impl_ipa.rectype2iparectype.iteritems():
    iparectype = tup[0]
    iparectype2designate[iparectype] = rectype


# using the all: True flag returns fields we can't use
# strip these keys from zones
zoneskips = ['dn', 'nsrecord', 'idnszoneactive', 'objectclass']


def rec2des(rec, zonename):
    """Convert an IPA record to Designate format.  A single IPA record
    returned from the search may translate into multiple Designate.
    IPA dnsrecord_find returns a "name".  Each DNS name may contain
    multiple record types.  Each record type may contain multiple
    values.  Each one of these values must be added separately to
    Designate.  This function returns all of those as a list of
    dict designate records.
    """
    # convert record name
    if rec['idnsname'][0] == '@':
        name = zonename
    else:
        name = rec['idnsname'][0] + "." + zonename
    # find all record types
    rectypes = []
    for k in rec:
        if k.endswith("record"):
            if k in iparectype2designate:
                rectypes.append(k)
            else:
                LOG.info("Skipping unknown record type %s in %s" %
                         k, name)

    desrecs = []
    for rectype in rectypes:
        dtype = iparectype2designate[rectype]
        for ddata in rec[rectype]:
            desreq = {'name': name, 'type': dtype}
            if dtype == 'SRV' or dtype == 'MX':
                # split off the priority and send in a separate field
                idx = ddata.find(' ')
                desreq['priority'] = int(ddata[:idx])
                if dtype == 'SRV' and not ddata.endswith("."):
                    # if server is specified as relative, add zonename
                    desreq['data'] = ddata[(idx + 1):] + "." + zonename
                else:
                    desreq['data'] = ddata[(idx + 1):]
            else:
                desreq['data'] = ddata
            if rec.get('description', [None])[0]:
                desreq['description'] = rec.get('description')[0]
            if rec.get('ttl', [None])[0]:
                desreq['ttl'] = int(rec['dnsttl'][0])
            desrecs.append(desreq)
    return desrecs


def zone2des(ipazone):
    # next, try to add the fake domain to Designate
    zonename = ipazone['idnsname'][0].rstrip(".") + "."
    email = ipazone['idnssoarname'][0].rstrip(".").replace(".", "@", 1)
    desreq = {"name": zonename,
              "ttl": int(ipazone['idnssoarefresh'][0]),
              "email": email}
    return desreq


def getipadomains(ipabackend, version):
    # get the list of domains/zones from IPA
    ipareq = {'method': 'dnszone_find',
              'params': [[], {'version': version,
                              'all': True}]}
    iparesp = ipabackend._call_and_handle_error(ipareq)
    LOG.debug("Response: %s" % pprint.pformat(iparesp))
    return iparesp['result']['result']


def getiparecords(ipabackend, zonename, version):
    ipareq = {'method': 'dnsrecord_find',
              'params': [[zonename], {"version": version,
                                      "all": True}]}
    iparesp = ipabackend._call_and_handle_error(ipareq)
    return iparesp['result']['result']


def syncipaservers2des(servers, designatereq, designateurl):
    # get existing servers from designate
    dservers = {}
    srvurl = designateurl + "/servers"
    resp = designatereq.get(srvurl)
    LOG.debug("Response: %s" % pprint.pformat(resp.json()))
    if resp and resp.status_code == 200 and resp.json() and \
            'servers' in resp.json():
        for srec in resp.json()['servers']:
            dservers[srec['name']] = srec['id']
    else:
        LOG.warn("No servers in designate")

    # first - add servers from ipa not already in designate
    for server in servers:
        if server in dservers:
            LOG.info("Skipping ipa server %s already in designate" % server)
        else:
            desreq = {"name": server}
            resp = designatereq.post(srvurl, data=json.dumps(desreq))
            LOG.debug("Response: %s" % pprint.pformat(resp.json()))
            if resp.status_code == 200:
                LOG.info("Added server %s to designate" % server)
            else:
                raise AddServerError("Unable to add %s: %s" %
                                     (server, pprint.pformat(resp.json())))

    # next - delete servers in designate not in ipa
    for server, sid in dservers.iteritems():
        if server not in servers:
            delresp = designatereq.delete(srvurl + "/" + sid)
            if delresp.status_code == 200:
                LOG.info("Deleted server %s" % server)
            else:
                raise DeleteServerError("Unable to delete %s: %s" %
                                        (server,
                                         pprint.pformat(delresp.json())))


def main():
    # HACK HACK HACK - allow required config params to be passed
    # via the command line
    cfg.CONF['service:api']._group._opts['api_base_uri']['cli'] = True
    for optdict in cfg.CONF['backend:ipa']._group._opts.itervalues():
        if 'cli' in optdict:
            optdict['cli'] = True
    # HACK HACK HACK - allow api url to be passed in the usual way
    utils.read_config('designate', sys.argv)
    if cfg.CONF['service:central'].backend_driver == 'ipa':
        raise CannotUseIPABackend(cuiberrorstr)
    if cfg.CONF.debug:
        LOG.setLevel(logging.DEBUG)
    elif cfg.CONF.verbose:
        LOG.setLevel(logging.INFO)
    else:
        LOG.setLevel(logging.WARN)
    ipabackend = impl_ipa.IPABackend(None)
    ipabackend.start()
    version = cfg.CONF['backend:ipa'].ipa_version
    designateurl = cfg.CONF['service:api'].api_base_uri + "v1"

    # get the list of domains/zones from IPA
    ipazones = getipadomains(ipabackend, version)
    # get unique list of name servers
    servers = {}
    for zonerec in ipazones:
        for nsrec in zonerec['nsrecord']:
            servers[nsrec] = nsrec
    if not servers:
        raise NoNameServers("Error: no name servers found in IPA")

    # let's see if designate is using the IPA backend
    # create a fake domain in IPA
    # create a fake server in Designate
    # try to create the same fake domain in Designate
    # if we get a DuplicateDomain error from Designate, then
    # raise the CannotUseIPABackend error, after deleting
    # the fake server and fake domain
    # find the first non-reverse zone
    zone = {}
    for zrec in ipazones:
        if not zrec['idnsname'][0].endswith("in-addr.arpa.") and \
                zrec['idnszoneactive'][0] == 'TRUE':
            # ipa returns every data field as a list
            # convert the list to a scalar
            for n, v in zrec.iteritems():
                if n in zoneskips:
                    continue
                if isinstance(v, list):
                    zone[n] = v[0]
                else:
                    zone[n] = v
            break

    assert(zone)

    # create a fake subdomain of this zone
    domname = "%s.%s" % (uuid.uuid4(), zone['idnsname'])
    args = copy.copy(zone)
    del args['idnsname']
    args['version'] = version
    ipareq = {'method': 'dnszone_add',
              'params': [[domname], args]}
    iparesp = ipabackend._call_and_handle_error(ipareq)
    LOG.debug("Response: %s" % pprint.pformat(iparesp))
    if iparesp['error']:
        raise AddDomainError(pprint.pformat(iparesp))

    # set up designate connection
    designatereq = requests.Session()
    xtra_hdrs = {'Content-Type': 'application/json'}
    designatereq.headers.update(xtra_hdrs)

    # sync ipa name servers to designate
    syncipaservers2des(servers, designatereq, designateurl)

    domainurl = designateurl + "/domains"
    # next, try to add the fake domain to Designate
    email = zone['idnssoarname'].rstrip(".").replace(".", "@", 1)
    desreq = {"name": domname,
              "ttl": int(zone['idnssoarefresh'][0]),
              "email": email}
    resp = designatereq.post(domainurl, data=json.dumps(desreq))
    exc = None
    fakezoneid = None
    if resp.status_code == 200:
        LOG.info("Added domain %s" % domname)
        fakezoneid = resp.json()['id']
        delresp = designatereq.delete(domainurl + "/" + fakezoneid)
        if delresp.status_code != 200:
            LOG.error("Unable to delete %s: %s" %
                      (domname, pprint.pformat(delresp.json())))
    else:
        exc = CannotUseIPABackend(cuiberrorstr)

    # cleanup fake stuff
    ipareq = {'method': 'dnszone_del',
              'params': [[domname], {'version': version}]}
    iparesp = ipabackend._call_and_handle_error(ipareq)
    LOG.debug("Response: %s" % pprint.pformat(iparesp))
    if iparesp['error']:
        LOG.error(pprint.pformat(iparesp))

    if exc:
        raise exc

    # get and delete existing domains
    resp = designatereq.get(domainurl)
    LOG.debug("Response: %s" % pprint.pformat(resp.json()))
    if resp and resp.status_code == 200 and resp.json() and \
            'domains' in resp.json():
        # domains must be deleted in child/parent order i.e. delete
        # sub-domains before parent domains - simple way to get this
        # order is to sort the domains in reverse order of name len
        dreclist = sorted(resp.json()['domains'],
                          key=lambda drec: len(drec['name']),
                          reverse=True)
        for drec in dreclist:
            delresp = designatereq.delete(domainurl + "/" + drec['id'])
            if delresp.status_code != 200:
                raise DeleteDomainError("Unable to delete %s: %s" %
                                        (drec['name'],
                                         pprint.pformat(delresp.json())))

    # key is zonename, val is designate rec id
    zonerecs = {}
    for zonerec in ipazones:
        desreq = zone2des(zonerec)
        resp = designatereq.post(domainurl, data=json.dumps(desreq))
        if resp.status_code == 200:
            LOG.info("Added domain %s" % desreq['name'])
        else:
            raise AddDomainError("Unable to add domain %s: %s" %
                                 (desreq['name'], pprint.pformat(resp.json())))
        zonerecs[desreq['name']] = resp.json()['id']

    # get the records for each zone
    for zonename, domainid in zonerecs.iteritems():
        recurl = designateurl + "/domains/" + domainid + "/records"
        iparecs = getiparecords(ipabackend, zonename, version)
        for rec in iparecs:
            desreqs = rec2des(rec, zonename)
            for desreq in desreqs:
                resp = designatereq.post(recurl, data=json.dumps(desreq))
                if resp.status_code == 200:
                    LOG.info("Added record %s for domain %s" %
                             (desreq['name'], zonename))
                else:
                    raise AddRecordError("Could not add record %s: %s" %
                                         (desreq['name'],
                                          pprint.pformat(resp.json())))

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = zoneextractor
# Copyright (C) 2013 eNovance SAS <licensing@enovance.com>
#
# Author: Artom Lifshitz <artom.lifshitz@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import sys
import re
import os
import dns.zone
import argparse
import logging


logging.basicConfig()
LOG = logging.getLogger(__name__)


class Zone:
    """
    Encapsulates a dnspython zone to provide easier printing and writing to
    files
    """

    def __init__(self, dnszone):
        self._dnszone = dnszone

    def to_stdout(self):
        self.to_file(sys.stdout)

    def to_file(self, f):
        if type(f) is file:
            fd = f
        elif type(f) is str:
            if os.path.isdir(f):
                fd = open(os.path.join(f, self._dnszone.origin.to_text()), 'w')
            else:
                fd = open(f, 'w')
        else:
            raise ValueError('f must be a file name or file object')
        fd.write('$ORIGIN %s\n' % self._dnszone.origin.to_text())
        self._dnszone.to_file(fd, relativize=False)
        fd.write('\n')
        if fd is not sys.stdout:
            fd.close()


class Extractor:
    """
    Extracts all the zones configured in a named.conf, including included
    files
    """

    # The regexes we use to extract information from the config file
    _include_regex = re.compile(
        r"""
        include \s*       # The include keyword, possibly followed by
                          # whitespace
        "                 # Open quote
        (?P<file> [^"]+ ) # The included file (without quotes), as group 'file'
        "                 # Close quote
        \s* ;             # Semicolon, possibly preceded by whitespace
        """, re.MULTILINE | re.VERBOSE)

    _zone_regex = re.compile(
        r"""
        zone \s*              # The zone keyword, possibly followed by
                              # whitespace
        "                     # Open quote
        (?P<name> [^"]+ )     # The zone name (without quotes), as group 'name'
        "                     # Close quote
        \s*                   # Possible whitespace
        {                     # Open bracket
        (?P<content> [^{}]+ ) # The contents of the zone block (without
                              # brackets) as group 'content'
        }                     # Close bracket
        \s* ;                 # Semicolon, possibly preceded by whitespace
        """, re.MULTILINE | re.VERBOSE)

    _type_master_regex = re.compile(
        r"""
        type \s+ # The type keyword, followed by some whitespace
        master   # The master keyword
        \s* ;    # Semicolon, possibly preceded by whitespace
        """, re.MULTILINE | re.VERBOSE)

    _zonefile_regex = re.compile(r"""
        file \s*          # The file keyword, possible followed by whitespace
        "                 # Open quote
        (?P<file> [^"]+ ) # The zonefile (without quotes), as group 'file'
        "                 # Close quote
        \s* ;             # Semicolor, possible preceded by whitespace
        """, re.MULTILINE | re.VERBOSE)

    def __init__(self, conf_file):
        self._conf_file = conf_file
        self._conf = self._filter_comments(conf_file)

    def _skip_until(self, f, stop):
        skip = ''
        while True:
            skip += f.read(1)
            if skip.endswith(stop):
                break

    def _filter_comments(self, conf_file):
        """
        Reads the named.conf, skipping comments and returning the filtered
        configuration
        """
        f = open(conf_file)
        conf = ''
        while True:
            c = f.read(1)
            if c == '':
                break
            conf += c
            # If we just appended a commenter:
            if conf.endswith('#'):
                self._skip_until(f, '\n')
                # Strip the '#' we appended earlier
                conf = conf[:-1]
            elif conf.endswith('//'):
                self._skip_until(f, '\n')
                # Strip the '//' we appended earlier
                conf = conf[:-2]
            elif conf.endswith('/*'):
                self._skip_until(f, '*/')
                # Strip the '/*' we appended earlier
                conf = conf[:-2]
        f.close()
        return conf

    def extract(self):
        zones = []
        zones.extend(self._process_includes())
        zones.extend(self._extract_zones())
        return zones

    def _process_includes(self):
        zones = []
        for include in self._include_regex.finditer(self._conf):
            x = Extractor(include.group('file'))
            zones.extend(x.extract())
        return zones

    def _extract_zones(self):
        zones = []
        for zone in self._zone_regex.finditer(self._conf):
            content = zone.group('content')
            name = zone.group('name')
            # Make sure it's a master zone:
            if self._type_master_regex.search(content):
                zonefile = self._zonefile_regex.search(content).group('file')
                try:
                    zone_object = dns.zone.from_file(zonefile,
                                                     allow_include=True)
                except dns.zone.UnknownOrigin:
                    LOG.info('%s is missing $ORIGIN, inserting %s' %
                             (zonefile, name))
                    zone_object = dns.zone.from_file(zonefile,
                                                     allow_include=True,
                                                     origin=name)
                except dns.zone.NoSOA:
                    LOG.error('%s has no SOA' % zonefile)
                zones.append(Zone(zone_object))
        return zones


def main():
    parser = argparse.ArgumentParser(
        description='Extract zonefiles from named.conf.')
    parser.add_argument('named_conf', metavar='FILE', type=str, nargs=1,
                        help='the named.conf to parse')
    parser.add_argument('-w', '--write', metavar='DIR', type=str,
                        help='Wwrite each extracted zonefile as its own file'
                        ' in DIR')
    parser.add_argument('-v', '--verbose', action='store_true',
                        help='verbose output')
    args = parser.parse_args()
    if args.verbose:
        LOG.setLevel(logging.INFO)
    else:
        LOG.setLevel(logging.WARNING)
    try:
        x = Extractor(args.named_conf[0])
        for zone in x.extract():
            if args.write is not None:
                zone.to_file(args.write)
            else:
                zone.to_stdout()
    except IOError as e:
        LOG.error(e)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = rpcapi
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from oslo.config import cfg
from oslo import messaging

from designate.openstack.common import log as logging
from designate import rpc


LOG = logging.getLogger(__name__)


class AgentAPI(object):
    """
    Client side of the agent Rpc API.

    API version history:

        1.0 - Initial version
    """
    RPC_API_VERSION = '1.0'

    def __init__(self, topic=None):
        topic = topic if topic else cfg.CONF.agent_topic

        target = messaging.Target(topic=topic, version=self.RPC_API_VERSION)
        self.client = rpc.get_client(target, version_cap='1.0')

       # Server Methods
    def create_server(self, context, server, host=None):
        cctxt = self.client.prepare(server=host)

        return cctxt.call(context, 'create_server', server=server)

    def update_server(self, context, server, host=None):
        cctxt = self.client.prepare(server=host)

        return cctxt.client.call(context, 'update_server', server=server)

    def delete_server(self, context, server, host=None):
        cctxt = self.client.prepare(server=host)

        return cctxt.call(context, 'delete_server', server=server)

    # TSIG Key Methods
    def create_tsigkey(self, context, tsigkey, host=None):
        cctxt = self.client.prepare(server=host)

        return cctxt.call(context, 'create_tsigkey', tsigkey=tsigkey)

    def update_tsigkey(self, context, tsigkey, host=None):
        cctxt = self.client.prepare(server=host)

        return cctxt.call(context, 'update_tsigkey', tsigkey=tsigkey)

    def delete_tsigkey(self, context, tsigkey, host=None):
        cctxt = self.client.prepare(server=host)

        return cctxt.call(context, 'delete_tsigkey', tsigkey=tsigkey)

    # Domain Methods
    def create_domain(self, context, domain, host=None):
        cctxt = self.client.prepare(server=host)

        return cctxt.call(context, 'create_domain', domain=domain)

    def update_domain(self, context, domain, host=None):
        cctxt = self.client.prepare(server=host)

        return cctxt.call(context, 'update_domain', domain=domain)

    def delete_domain(self, context, domain, host=None):
        cctxt = self.client.prepare(server=host)

        return cctxt.call(context, 'delete_domain', domain=domain)

    # Record Methods
    def update_recordset(self, context, domain, recordset, host=None):
        cctxt = self.client.prepare(server=host)

        return cctxt.call(context, 'update_recordset', domain=domain,
                          recordset=recordset)

    def delete_recordset(self, context, domain, recordset, host=None):
        cctxt = self.client.prepare(server=host)

        return cctxt.call(context, 'delete_recordset', domain=domain,
                          recordset=recordset)

    def create_record(self, context, domain, recordset, record, host=None):
        cctxt = self.client.prepare(server=host)

        return cctxt.call(context, 'create_record', domain=domain,
                          recordset=recordset, record=record)

    def update_record(self, context, domain, recordset, record, host=None):
        cctxt = self.client.prepare(server=host)

        return cctxt.call(context, 'update_record', domain=domain,
                          recordset=recordset, record=record)

    def delete_record(self, context, domain, recordset, record, host=None):
        cctxt = self.client.prepare(server=host)

        return cctxt.call(context, 'delete_record', domain=domain,
                          recordset=recordset, record=record)

    # Sync Methods
    def sync_domain(self, context, domain, records, host=None):
        cctxt = self.client.prepare(server=host)

        return cctxt.call(context, 'sync_domain', domain=domain,
                          record=records)

    def sync_record(self, context, domain, record, host=None):
        cctxt = self.client.prepare(server=host)

        return cctxt.call(context, 'sync_record', domain=domain, record=record)

########NEW FILE########
__FILENAME__ = service
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from oslo.config import cfg
from designate.openstack.common import log as logging
from designate import backend
from designate import rpc
from designate import service
from designate.central import rpcapi as central_rpcapi

LOG = logging.getLogger(__name__)


class Service(service.Service):
    def __init__(self, *args, **kwargs):
        # NOTE: Central api needs a transport if not it fails. This is
        # normally done by the service init method.
        rpc.init(cfg.CONF)
        central_api = central_rpcapi.CentralAPI()

        manager = backend.get_backend(
            cfg.CONF['service:agent'].backend_driver,
            central_service=central_api)

        kwargs['manager'] = manager

        super(Service, self).__init__(*args, **kwargs)

    def start(self):
        super(Service, self).start()
        self.manager.start()

    def stop(self):
        super(Service, self).stop()
        self.manager.stop()

########NEW FILE########
__FILENAME__ = middleware
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import flask
import webob.dec

from oslo.config import cfg
from oslo import messaging

from designate import exceptions
from designate import notifications
from designate import wsgi
from designate.context import DesignateContext
from designate.openstack.common import jsonutils as json
from designate.openstack.common import local
from designate.openstack.common import log as logging
from designate.openstack.common import strutils

LOG = logging.getLogger(__name__)

cfg.CONF.register_opts([
    cfg.BoolOpt('maintenance-mode', default=False,
                help='Enable API Maintenance Mode'),
    cfg.StrOpt('maintenance-mode-role', default='admin',
               help='Role allowed to bypass maintaince mode'),
], group='service:api')


class MaintenanceMiddleware(wsgi.Middleware):
    def __init__(self, application):
        super(MaintenanceMiddleware, self).__init__(application)

        LOG.info('Starting designate maintenance middleware')

        self.enabled = cfg.CONF['service:api'].maintenance_mode
        self.role = cfg.CONF['service:api'].maintenance_mode_role

    def process_request(self, request):
        # If maintaince mode is not enabled, pass the request on as soon as
        # possible
        if not self.enabled:
            return None

        # If the caller has the bypass role, let them through
        if ('context' in request.environ
                and self.role in request.environ['context'].roles):
            LOG.warning('Request authorized to bypass maintenance mode')
            return None

        # Otherwise, reject the request with a 503 Service Unavailable
        return flask.Response(status=503, headers={'Retry-After': 60})


def auth_pipeline_factory(loader, global_conf, **local_conf):
    """
    A paste pipeline replica that keys off of auth_strategy.

    Code nabbed from cinder.
    """
    pipeline = local_conf[cfg.CONF['service:api'].auth_strategy]
    pipeline = pipeline.split()
    LOG.info('Getting auth pipeline: %s' % pipeline[:-1])
    filters = [loader.get_filter(n) for n in pipeline[:-1]]
    app = loader.get_app(pipeline[-1])
    filters.reverse()
    for filter in filters:
        app = filter(app)
    return app


class ContextMiddleware(wsgi.Middleware):
    def process_response(self, response):
        try:
            context = local.store.context
        except Exception:
            pass
        else:
            # Add the Request ID as a response header
            response.headers['X-DNS-Request-ID'] = context.request_id

        return response


class KeystoneContextMiddleware(ContextMiddleware):
    def __init__(self, application):
        super(KeystoneContextMiddleware, self).__init__(application)

        LOG.info('Starting designate keystonecontext middleware')

    def process_request(self, request):
        headers = request.headers

        try:
            if headers['X-Identity-Status'] is 'Invalid':
                # TODO(graham) fix the return to use non-flask resources
                return flask.Response(status=401)
        except KeyError:
            # If the key is valid, Keystone does not include this header at all
            pass

        if headers.get('X-Service-Catalog'):
            catalog = json.loads(headers.get('X-Service-Catalog'))
        else:
            catalog = None

        roles = headers.get('X-Roles').split(',')

        context = DesignateContext(auth_token=headers.get('X-Auth-Token'),
                                   user=headers.get('X-User-ID'),
                                   tenant=headers.get('X-Tenant-ID'),
                                   roles=roles,
                                   service_catalog=catalog)

        # Store the context where oslo-log exepcts to find it.
        local.store.context = context

        # Attach the context to the request environment
        request.environ['context'] = context


class NoAuthContextMiddleware(ContextMiddleware):
    def __init__(self, application):
        super(NoAuthContextMiddleware, self).__init__(application)

        LOG.info('Starting designate noauthcontext middleware')

    def process_request(self, request):
        headers = request.headers

        context = DesignateContext(
            auth_token=headers.get('X-Auth-Token', None),
            user=headers.get('X-Auth-User-ID', 'noauth-user'),
            tenant=headers.get('X-Auth-Project-ID', 'noauth-project'),
            roles=headers.get('X-Roles', 'admin').split(',')
        )

        # Store the context where oslo-log exepcts to find it.
        local.store.context = context

        # Attach the context to the request environment
        request.environ['context'] = context


class TestContextMiddleware(ContextMiddleware):
    def __init__(self, application, tenant_id=None, user_id=None):
        super(TestContextMiddleware, self).__init__(application)

        LOG.critical('Starting designate testcontext middleware')
        LOG.critical('**** DO NOT USE IN PRODUCTION ****')

        self.default_tenant_id = tenant_id
        self.default_user_id = user_id

    def process_request(self, request):
        headers = request.headers

        all_tenants = strutils.bool_from_string(
            headers.get('X-Test-All-Tenants', 'False'))

        context = DesignateContext(
            user=headers.get('X-Test-User-ID', self.default_user_id),
            tenant=headers.get('X-Test-Tenant-ID', self.default_tenant_id),
            all_tenants=all_tenants)

        # Store the context where oslo-log exepcts to find it.
        local.store.context = context

        # Attach the context to the request environment
        request.environ['context'] = context


class FaultWrapperMiddleware(wsgi.Middleware):
    def __init__(self, application):
        super(FaultWrapperMiddleware, self).__init__(application)

        LOG.info('Starting designate faultwrapper middleware')

    @webob.dec.wsgify
    def __call__(self, request):
        try:
            return request.get_response(self.application)
        except exceptions.Base as e:
            # Handle Designate Exceptions
            status = e.error_code if hasattr(e, 'error_code') else 500

            # Start building up a response
            response = {
                'code': status
            }

            if e.error_type:
                response['type'] = e.error_type

            if e.error_message:
                response['message'] = e.error_message

            if e.errors:
                response['errors'] = e.errors

            return self._handle_exception(request, e, status, response)
        except messaging.MessagingTimeout as e:
            # Special case for RPC timeout's
            response = {
                'code': 504,
                'type': 'timeout',
            }

            return self._handle_exception(request, e, 504, response)
        except Exception as e:
            # Handle all other exception types
            return self._handle_exception(request, e)

    def _handle_exception(self, request, e, status=500, response={}):
        # Log the exception ASAP
        LOG.exception(e)

        headers = [
            ('Content-Type', 'application/json'),
        ]

        url = getattr(request, 'url', None)

        # Set a response code and type, if they are missing.
        if 'code' not in response:
            response['code'] = status

        if 'type' not in response:
            response['type'] = 'unknown'

        if 'context' in request.environ:
            response['request_id'] = request.environ['context'].request_id

            notifications.send_api_fault(request.environ['context'], url,
                                         response['code'], e)
        else:
            #TODO(ekarlso): Remove after verifying that there's actually a
            # context always set
            LOG.error('Missing context in request, please check.')

        # Return the new response
        return flask.Response(status=status, headers=headers,
                              response=json.dumps(response))

########NEW FILE########
__FILENAME__ = service
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from paste import deploy
from designate.openstack.common import log as logging
from designate.openstack.deprecated import wsgi
from oslo.config import cfg
from designate import exceptions
from designate import utils
from designate import policy


LOG = logging.getLogger(__name__)


class Service(wsgi.Service):
    def __init__(self, backlog=128, threads=1000):

        api_paste_config = cfg.CONF['service:api'].api_paste_config
        config_paths = utils.find_config(api_paste_config)

        if len(config_paths) == 0:
            msg = 'Unable to determine appropriate api-paste-config file'
            raise exceptions.ConfigurationError(msg)

        LOG.info('Using api-paste-config found at: %s' % config_paths[0])

        policy.init()

        application = deploy.loadapp("config:%s" % config_paths[0],
                                     name='osapi_dns')

        super(Service, self).__init__(application=application,
                                      host=cfg.CONF['service:api'].api_host,
                                      port=cfg.CONF['service:api'].api_port,
                                      backlog=backlog,
                                      threads=threads)

########NEW FILE########
__FILENAME__ = domains
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import flask
from designate.openstack.common import log as logging
from designate import schema
from designate.api import get_central_api

LOG = logging.getLogger(__name__)
blueprint = flask.Blueprint('domains', __name__)
domain_schema = schema.Schema('v1', 'domain')
domains_schema = schema.Schema('v1', 'domains')
servers_schema = schema.Schema('v1', 'servers')


@blueprint.route('/schemas/domain', methods=['GET'])
def get_domain_schema():
    return flask.jsonify(domain_schema.raw)


@blueprint.route('/schemas/domains', methods=['GET'])
def get_domains_schema():
    return flask.jsonify(domains_schema.raw)


@blueprint.route('/domains', methods=['POST'])
def create_domain():
    context = flask.request.environ.get('context')
    values = flask.request.json

    domain_schema.validate(values)
    domain = get_central_api().create_domain(context, values)

    response = flask.jsonify(domain_schema.filter(domain))
    response.status_int = 201
    response.location = flask.url_for('.get_domain', domain_id=domain['id'])

    return response


@blueprint.route('/domains', methods=['GET'])
def get_domains():
    context = flask.request.environ.get('context')

    domains = get_central_api().find_domains(context)

    return flask.jsonify(domains_schema.filter({'domains': domains}))


@blueprint.route('/domains/<uuid:domain_id>', methods=['GET'])
def get_domain(domain_id):
    context = flask.request.environ.get('context')

    domain = get_central_api().get_domain(context, domain_id)

    return flask.jsonify(domain_schema.filter(domain))


@blueprint.route('/domains/<uuid:domain_id>', methods=['PUT'])
def update_domain(domain_id):
    context = flask.request.environ.get('context')
    values = flask.request.json

    domain = get_central_api().get_domain(context, domain_id)
    domain = domain_schema.filter(domain)
    domain.update(values)

    domain_schema.validate(domain)
    domain = get_central_api().update_domain(context, domain_id, values)

    return flask.jsonify(domain_schema.filter(domain))


@blueprint.route('/domains/<uuid:domain_id>', methods=['DELETE'])
def delete_domain(domain_id):
    context = flask.request.environ.get('context')

    get_central_api().delete_domain(context, domain_id)

    return flask.Response(status=200)


@blueprint.route('/domains/<uuid:domain_id>/servers', methods=['GET'])
def get_domain_servers(domain_id):
    context = flask.request.environ.get('context')

    servers = get_central_api().get_domain_servers(context, domain_id)

    return flask.jsonify(servers_schema.filter({'servers': servers}))

########NEW FILE########
__FILENAME__ = diagnostics
# Copyright 2012 Hewlett-Packard Development Company, L.P. All Rights Reserved.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import flask
from oslo import messaging

from designate.openstack.common import log as logging
from designate import rpc

LOG = logging.getLogger(__name__)
blueprint = flask.Blueprint('diagnostics', __name__)


@blueprint.route('/diagnostics/ping/<topic>/<host>', methods=['GET'])
def ping_host(topic, host):
    context = flask.request.environ.get('context')

    client = rpc.get_client(messaging.Target(topic=topic))
    cctxt = client.prepare(server=host, timeout=10)

    pong = cctxt.call(context, 'ping')

    return flask.jsonify(pong)

########NEW FILE########
__FILENAME__ = quotas
# Copyright 2012 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import flask
from designate.openstack.common import log as logging
from designate.central import rpcapi as central_rpcapi

LOG = logging.getLogger(__name__)
central_api = central_rpcapi.CentralAPI()
blueprint = flask.Blueprint('quotas', __name__)


@blueprint.route('/quotas/<tenant_id>', methods=['GET'])
def get_quotas(tenant_id):
    context = flask.request.environ.get('context')

    quotas = central_api.get_quotas(context, tenant_id)

    return flask.jsonify(quotas)


@blueprint.route('/quotas/<tenant_id>', methods=['PUT', 'POST'])
def set_quota(tenant_id):
    context = flask.request.environ.get('context')
    values = flask.request.json

    for resource, hard_limit in values.items():
        central_api.set_quota(context, tenant_id, resource, hard_limit)

    quotas = central_api.get_quotas(context, tenant_id)
    return flask.jsonify(quotas)


@blueprint.route('/quotas/<tenant_id>', methods=['DELETE'])
def reset_quotas(tenant_id):
    context = flask.request.environ.get('context')

    central_api.reset_quotas(context, tenant_id)

    return flask.Response(status=200)

########NEW FILE########
__FILENAME__ = reports
# Copyright 2012 Hewlett-Packard Development Company, L.P. All Rights Reserved.
#
# Author: Simon McCartney <simon.mccartney@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import flask
from designate.openstack.common import log as logging
from designate.central import rpcapi as central_rpcapi

LOG = logging.getLogger(__name__)
central_api = central_rpcapi.CentralAPI()
blueprint = flask.Blueprint('reports', __name__)


@blueprint.route('/reports/tenants', methods=['GET'])
def reports_tenants():
    context = flask.request.environ.get('context')

    tenants = central_api.find_tenants(context)

    return flask.jsonify(tenants=tenants)


@blueprint.route('/reports/tenants/<tenant_id>', methods=['GET'])
def reports_tenant(tenant_id):
    context = flask.request.environ.get('context')

    tenant = central_api.get_tenant(context, tenant_id)

    return flask.jsonify(tenant)


@blueprint.route('/reports/counts', methods=['GET'])
def reports_counts():
    context = flask.request.environ.get('context')

    tenants = central_api.count_tenants(context)
    domains = central_api.count_domains(context)
    records = central_api.count_records(context)

    return flask.jsonify(tenants=tenants, domains=domains, records=records)


@blueprint.route('/reports/counts/tenants', methods=['GET'])
def reports_counts_tenants():
    context = flask.request.environ.get('context')

    count = central_api.count_tenants(context)

    return flask.jsonify(tenants=count)


@blueprint.route('/reports/counts/domains', methods=['GET'])
def reports_counts_domains():
    context = flask.request.environ.get('context')

    count = central_api.count_domains(context)

    return flask.jsonify(domains=count)


@blueprint.route('/reports/counts/records', methods=['GET'])
def reports_counts_records():
    context = flask.request.environ.get('context')

    count = central_api.count_records(context)

    return flask.jsonify(records=count)

########NEW FILE########
__FILENAME__ = sync
# Copyright 2012 Hewlett-Packard Development Company, L.P. All Rights Reserved.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import flask
from designate.openstack.common import log as logging
from designate.central import rpcapi as central_rpcapi

LOG = logging.getLogger(__name__)
central_api = central_rpcapi.CentralAPI()
blueprint = flask.Blueprint('sync', __name__)


@blueprint.route('/domains/sync', methods=['POST'])
def sync_domains():
    context = flask.request.environ.get('context')

    central_api.sync_domains(context)

    return flask.Response(status=200)


@blueprint.route('/domains/<uuid:domain_id>/sync', methods=['POST'])
def sync_domain(domain_id):
    context = flask.request.environ.get('context')

    central_api.sync_domain(context, domain_id)

    return flask.Response(status=200)


@blueprint.route('/domains/<uuid:domain_id>/records/<uuid:record_id>/sync',
                 methods=['POST'])
def sync_record(domain_id, record_id):
    context = flask.request.environ.get('context')

    record = central_api.find_record(context, {'id': record_id})
    central_api.sync_record(context, domain_id, record['recordset_id'],
                            record_id)

    return flask.Response(status=200)

########NEW FILE########
__FILENAME__ = touch
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import flask
from designate.central import rpcapi as central_rpcapi

central_api = central_rpcapi.CentralAPI()
blueprint = flask.Blueprint('touch', __name__)


@blueprint.route('/domains/<uuid:domain_id>/touch', methods=['POST'])
def touch_domain(domain_id):
    context = flask.request.environ.get('context')

    central_api.touch_domain(context, domain_id)

    return flask.Response(status=200)

########NEW FILE########
__FILENAME__ = limits
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import flask
from designate.openstack.common import log as logging
from designate import schema
from designate.api import get_central_api

LOG = logging.getLogger(__name__)
blueprint = flask.Blueprint('limits', __name__)
limits_schema = schema.Schema('v1', 'limits')


@blueprint.route('/schemas/limits', methods=['GET'])
def get_limits_schema():
    return flask.jsonify(limits_schema.raw)


@blueprint.route('/limits', methods=['GET'])
def get_limits():
    context = flask.request.environ.get('context')

    absolute_limits = get_central_api().get_absolute_limits(context)

    return flask.jsonify(limits_schema.filter({
        "limits": {
            "absolute": {
                "maxDomains": absolute_limits['domains'],
                "maxDomainRecords": absolute_limits['domain_records']
            }
        }
    }))

########NEW FILE########
__FILENAME__ = records
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import flask
from designate.openstack.common import log as logging
from designate import exceptions
from designate import schema
from designate.api import get_central_api

LOG = logging.getLogger(__name__)
blueprint = flask.Blueprint('records', __name__)
record_schema = schema.Schema('v1', 'record')
records_schema = schema.Schema('v1', 'records')


def _find_recordset(context, domain_id, name, type):
    return get_central_api().find_recordset(context, {
        'domain_id': domain_id,
        'name': name,
        'type': type,
    })


def _find_or_create_recordset(context, domain_id, name, type, ttl):
    try:
        recordset = _find_recordset(context, domain_id, name, type)
    except exceptions.RecordSetNotFound:
        recordset = get_central_api().create_recordset(context, domain_id, {
            'name': name,
            'type': type,
            'ttl': ttl,
        })

    return recordset


def _extract_record_values(values):
    record_values = ('data', 'priority', 'comment',)
    return dict((k, values[k]) for k in record_values if k in values)


def _extract_recordset_values(values):
    recordset_values = ('name', 'type', 'ttl',)
    return dict((k, values[k]) for k in recordset_values if k in values)


def _format_record_v1(record, recordset):
    record.update({
        'name': recordset['name'],
        'type': recordset['type'],
        'ttl': recordset['ttl'],
    })

    return record


def _fetch_domain_recordsets(context, domain_id):
    criterion = {'domain_id': domain_id}

    recordsets = get_central_api().find_recordsets(context, criterion)

    return dict((r['id'], r) for r in recordsets)


@blueprint.route('/schemas/record', methods=['GET'])
def get_record_schema():
    return flask.jsonify(record_schema.raw)


@blueprint.route('/schemas/records', methods=['GET'])
def get_records_schema():
    return flask.jsonify(records_schema.raw)


@blueprint.route('/domains/<uuid:domain_id>/records', methods=['POST'])
def create_record(domain_id):
    context = flask.request.environ.get('context')
    values = flask.request.json

    record_schema.validate(values)

    recordset = _find_or_create_recordset(context,
                                          domain_id,
                                          values['name'],
                                          values['type'],
                                          values.get('ttl', None))

    record = get_central_api().create_record(context, domain_id,
                                             recordset['id'],
                                             _extract_record_values(values))

    record = _format_record_v1(record, recordset)

    response = flask.jsonify(record_schema.filter(record))
    response.status_int = 201
    response.location = flask.url_for('.get_record', domain_id=domain_id,
                                      record_id=record['id'])

    return response


@blueprint.route('/domains/<uuid:domain_id>/records', methods=['GET'])
def get_records(domain_id):
    context = flask.request.environ.get('context')

    # NOTE: We need to ensure the domain actually exists, otherwise we may
    #       return an empty records array instead of a domain not found
    get_central_api().get_domain(context, domain_id)

    records = get_central_api().find_records(context, {'domain_id': domain_id})

    recordsets = _fetch_domain_recordsets(context, domain_id)

    def _inner(record):
        recordset = recordsets[record['recordset_id']]
        return _format_record_v1(record, recordset)

    records = [_inner(r) for r in records]

    return flask.jsonify(records_schema.filter({'records': records}))


@blueprint.route('/domains/<uuid:domain_id>/records/<uuid:record_id>',
                 methods=['GET'])
def get_record(domain_id, record_id):
    context = flask.request.environ.get('context')

    # NOTE: We need to ensure the domain actually exists, otherwise we may
    #       return an record not found instead of a domain not found
    get_central_api().get_domain(context, domain_id)

    criterion = {'domain_id': domain_id, 'id': record_id}
    record = get_central_api().find_record(context, criterion)

    recordset = get_central_api().get_recordset(
        context, domain_id, record['recordset_id'])

    record = _format_record_v1(record, recordset)

    return flask.jsonify(record_schema.filter(record))


@blueprint.route('/domains/<uuid:domain_id>/records/<uuid:record_id>',
                 methods=['PUT'])
def update_record(domain_id, record_id):
    context = flask.request.environ.get('context')
    values = flask.request.json

    # NOTE: We need to ensure the domain actually exists, otherwise we may
    #       return an record not found instead of a domain not found
    get_central_api().get_domain(context, domain_id)

    # Find the record
    criterion = {'domain_id': domain_id, 'id': record_id}
    record = get_central_api().find_record(context, criterion)

    # Find the associated recordset
    recordset = get_central_api().get_recordset(
        context, domain_id, record['recordset_id'])

    # Filter out any extra fields from the fetched record
    record = record_schema.filter(record)

    # Ensure all the API V1 fields are in place
    record = _format_record_v1(record, recordset)

    # Name and Type can't be updated on existing records
    if 'name' in values and record['name'] != values['name']:
        raise exceptions.InvalidOperation('The name field is immutable')

    if 'type' in values and record['type'] != values['type']:
        raise exceptions.InvalidOperation('The type field is immutable')

    # TTL Updates should be applied to the RecordSet
    update_recordset = False

    if 'ttl' in values and record['ttl'] != values['ttl']:
        update_recordset = True

    # Apply the updated fields to the record
    record.update(values)

    # Validate the record
    record_schema.validate(record)

    # Update the record
    record = get_central_api().update_record(
        context, domain_id, recordset['id'], record_id,
        _extract_record_values(values))

    # Update the recordset (if necessary)
    if update_recordset:
        recordset = get_central_api().update_recordset(
            context, domain_id, recordset['id'],
            _extract_recordset_values(values))

    # Format and return the response
    record = _format_record_v1(record, recordset)

    return flask.jsonify(record_schema.filter(record))


@blueprint.route('/domains/<uuid:domain_id>/records/<uuid:record_id>',
                 methods=['DELETE'])
def delete_record(domain_id, record_id):
    context = flask.request.environ.get('context')

    # NOTE: We need to ensure the domain actually exists, otherwise we may
    #       return a record not found instead of a domain not found
    get_central_api().get_domain(context, domain_id)

    # Find the record
    criterion = {'domain_id': domain_id, 'id': record_id}
    record = get_central_api().find_record(context, criterion)

    get_central_api().delete_record(
        context, domain_id, record['recordset_id'], record_id)

    return flask.Response(status=200)

########NEW FILE########
__FILENAME__ = servers
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import flask
from designate.openstack.common import log as logging
from designate import schema
from designate.api import get_central_api

LOG = logging.getLogger(__name__)
blueprint = flask.Blueprint('servers', __name__)
server_schema = schema.Schema('v1', 'server')
servers_schema = schema.Schema('v1', 'servers')


@blueprint.route('/schemas/server', methods=['GET'])
def get_server_schema():
    return flask.jsonify(server_schema.raw)


@blueprint.route('/schemas/servers', methods=['GET'])
def get_servers_schema():
    return flask.jsonify(servers_schema.raw)


@blueprint.route('/servers', methods=['POST'])
def create_server():
    context = flask.request.environ.get('context')
    values = flask.request.json

    server_schema.validate(values)
    server = get_central_api().create_server(context,
                                             values=flask.request.json)

    response = flask.jsonify(server_schema.filter(server))
    response.status_int = 201
    response.location = flask.url_for('.get_server', server_id=server['id'])

    return response


@blueprint.route('/servers', methods=['GET'])
def get_servers():
    context = flask.request.environ.get('context')

    servers = get_central_api().find_servers(context)

    return flask.jsonify(servers_schema.filter({'servers': servers}))


@blueprint.route('/servers/<uuid:server_id>', methods=['GET'])
def get_server(server_id):
    context = flask.request.environ.get('context')

    server = get_central_api().get_server(context, server_id)

    return flask.jsonify(server_schema.filter(server))


@blueprint.route('/servers/<uuid:server_id>', methods=['PUT'])
def update_server(server_id):
    context = flask.request.environ.get('context')
    values = flask.request.json

    server = get_central_api().get_server(context, server_id)
    server = server_schema.filter(server)
    server.update(values)

    server_schema.validate(server)
    server = get_central_api().update_server(context, server_id, values=values)

    return flask.jsonify(server_schema.filter(server))


@blueprint.route('/servers/<uuid:server_id>', methods=['DELETE'])
def delete_server(server_id):
    context = flask.request.environ.get('context')

    get_central_api().delete_server(context, server_id)

    return flask.Response(status=200)

########NEW FILE########
__FILENAME__ = tsigkeys
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import flask
from designate.openstack.common import log as logging
from designate import schema
from designate.api import get_central_api

LOG = logging.getLogger(__name__)
blueprint = flask.Blueprint('tsigkeys', __name__)
tsigkey_schema = schema.Schema('v1', 'tsigkey')
tsigkeys_schema = schema.Schema('v1', 'tsigkeys')


@blueprint.route('/schemas/tsigkey', methods=['GET'])
def get_tsigkey_schema():
    return flask.jsonify(tsigkey_schema.raw)


@blueprint.route('/schemas/tsigkeys', methods=['GET'])
def get_tsigkeys_schema():
    return flask.jsonify(tsigkeys_schema.raw)


@blueprint.route('/tsigkeys', methods=['POST'])
def create_tsigkey():
    context = flask.request.environ.get('context')
    values = flask.request.json

    tsigkey_schema.validate(values)
    tsigkey = get_central_api().create_tsigkey(
        context, values=flask.request.json)

    response = flask.jsonify(tsigkey_schema.filter(tsigkey))
    response.status_int = 201
    response.location = flask.url_for('.get_tsigkey', tsigkey_id=tsigkey['id'])

    return response


@blueprint.route('/tsigkeys', methods=['GET'])
def get_tsigkeys():
    context = flask.request.environ.get('context')

    tsigkeys = get_central_api().find_tsigkeys(context)

    return flask.jsonify(tsigkeys_schema.filter({'tsigkeys': tsigkeys}))


@blueprint.route('/tsigkeys/<uuid:tsigkey_id>', methods=['GET'])
def get_tsigkey(tsigkey_id):
    context = flask.request.environ.get('context')

    tsigkey = get_central_api().get_tsigkey(context, tsigkey_id)

    return flask.jsonify(tsigkey_schema.filter(tsigkey))


@blueprint.route('/tsigkeys/<uuid:tsigkey_id>', methods=['PUT'])
def update_tsigkey(tsigkey_id):
    context = flask.request.environ.get('context')
    values = flask.request.json

    tsigkey = get_central_api().get_tsigkey(context, tsigkey_id)
    tsigkey = tsigkey_schema.filter(tsigkey)
    tsigkey.update(values)

    tsigkey_schema.validate(tsigkey)
    tsigkey = get_central_api().update_tsigkey(context, tsigkey_id,
                                               values=values)

    return flask.jsonify(tsigkey_schema.filter(tsigkey))


@blueprint.route('/tsigkeys/<uuid:tsigkey_id>', methods=['DELETE'])
def delete_tsigkey(tsigkey_id):
    context = flask.request.environ.get('context')

    get_central_api().delete_tsigkey(context, tsigkey_id)

    return flask.Response(status=200)

########NEW FILE########
__FILENAME__ = app
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import pecan
import pecan.deploy
from oslo.config import cfg
from designate.openstack.common import log as logging

LOG = logging.getLogger(__name__)

cfg.CONF.register_opts([
    cfg.BoolOpt('pecan_debug', default=False,
                help='Pecan HTML Debug Interface'),
], group='service:api')


def setup_app(pecan_config):
    config = dict(pecan_config)

    config['app']['debug'] = cfg.CONF['service:api'].pecan_debug

    pecan.configuration.set_config(config, overwrite=True)

    app = pecan.make_app(
        pecan_config.app.root,
        debug=getattr(pecan_config.app, 'debug', False),
        force_canonical=getattr(pecan_config.app, 'force_canonical', True),
    )

    return app

########NEW FILE########
__FILENAME__ = blacklists
# Copyright 2014 Rackspace
#
# Author: Betsy Luzader <betsy.luzader@rackspace.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import pecan
from designate.openstack.common import log as logging
from designate import schema
from designate import utils
from designate.api.v2.controllers import rest
from designate.api.v2.views import blacklists as blacklists_view

LOG = logging.getLogger(__name__)


class BlacklistsController(rest.RestController):
    _view = blacklists_view.BlacklistsView()
    _resource_schema = schema.Schema('v2', 'blacklist')
    _collection_schema = schema.Schema('v2', 'blacklists')
    SORT_KEYS = ['created_at', 'id', 'updated_at', 'pattern']

    @pecan.expose(template='json:', content_type='application/json')
    @utils.validate_uuid('blacklist_id')
    def get_one(self, blacklist_id):
        """ Get Blacklist """

        request = pecan.request
        context = request.environ['context']

        blacklist = self.central_api.get_blacklist(context, blacklist_id)

        return self._view.show(context, request, blacklist)

    @pecan.expose(template='json:', content_type='application/json')
    def get_all(self, **params):
        """ List all Blacklisted Zones """
        request = pecan.request
        context = request.environ['context']

        # Extract the pagination params
        marker, limit, sort_key, sort_dir = self._get_paging_params(params)

        # Extract any filter params
        accepted_filters = ('pattern')
        criterion = dict((k, params[k]) for k in accepted_filters
                         if k in params)

        blacklist = self.central_api.find_blacklists(
            context, criterion, marker, limit, sort_key, sort_dir)

        return self._view.list(context, request, blacklist)

    @pecan.expose(template='json:', content_type='application/json')
    def post_all(self):
        """ Create Blacklisted Zone """
        request = pecan.request
        response = pecan.response
        context = request.environ['context']

        body = request.body_dict

        # Validate the request conforms to the schema
        self._resource_schema.validate(body)

        # Convert from APIv2 -> Central format
        values = self._view.load(context, request, body)

        # Create the blacklist
        blacklist = self.central_api.create_blacklist(context, values)

        response.status_int = 201

        response.headers['Location'] = self._view._get_resource_href(
            request, blacklist)

        # Prepare and return the response body
        return self._view.show(context, request, blacklist)

    @pecan.expose(template='json:', content_type='application/json')
    @pecan.expose(template='json:', content_type='application/json-patch+json')
    @utils.validate_uuid('blacklist_id')
    def patch_one(self, blacklist_id):
        """ Update Blacklisted Zone """
        request = pecan.request
        context = request.environ['context']
        body = request.body_dict
        response = pecan.response

        # Fetch the existing blacklisted zone
        blacklist = self.central_api.get_blacklist(context, blacklist_id)

        # Convert to APIv2 Format
        blacklist = self._view.show(context, request, blacklist)

        if request.content_type == 'application/json-patch+json':
            raise NotImplemented('json-patch not implemented')
        else:
            blacklist = utils.deep_dict_merge(blacklist, body)

            # Validate the request conforms to the schema
            self._resource_schema.validate(blacklist)

            values = self._view.load(context, request, body)

            blacklist = self.central_api.update_blacklist(context,
                                                          blacklist_id, values)

        response.status_int = 200

        return self._view.show(context, request, blacklist)

    @pecan.expose(template=None, content_type='application/json')
    @utils.validate_uuid('blacklist_id')
    def delete_one(self, blacklist_id):
        """ Delete Blacklisted Zone """
        request = pecan.request
        response = pecan.response
        context = request.environ['context']

        self.central_api.delete_blacklist(context, blacklist_id)

        response.status_int = 204

        # NOTE: This is a hack and a half.. But Pecan needs it.
        return ''

########NEW FILE########
__FILENAME__ = floatingips
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Endre Karlson <endre.karlson@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import pecan
import re
from designate import exceptions
from designate import schema
from designate.api.v2.controllers import rest
from designate.api.v2.views import floatingips as floatingips_views


FIP_REGEX = '^(?P<region>[A-Za-z0-9\\.\\-_]{1,100}):' \
            '(?P<id>[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-' \
            '[0-9a-fA-F]{4}-[0-9a-fA-F]{12})$'


def fip_key_to_data(key):
    m = re.match(FIP_REGEX, key)

    # NOTE: Ensure that the fip matches region:floatingip_id or raise, if
    # not this will cause a 500.
    if m is None:
        msg = 'Floating IP %s is not in the format of <region>:<uuid>'
        raise exceptions.BadRequest(msg % key)
    return m.groups()


class FloatingIPController(rest.RestController):
    _view = floatingips_views.FloatingIPView()
    _resource_schema = schema.Schema('v2', 'floatingip')
    _collection_schema = schema.Schema('v2', 'floatingips')

    @pecan.expose(template='json:', content_type='application/json')
    def get_all(self, **params):
        """ List Floating IP PTRs for a Tenant """
        request = pecan.request
        context = request.environ['context']

        fips = self.central_api.list_floatingips(context)
        return self._view.list(context, request, fips)

    @pecan.expose(template='json:', content_type='application/json')
    def patch_one(self, fip_key):
        """
        Set or unset a PTR
        """
        request = pecan.request
        context = request.environ['context']
        body = request.body_dict

        region, id_ = fip_key_to_data(fip_key)

        # Validate the request conforms to the schema
        self._resource_schema.validate(body)

        fip = self.central_api.update_floatingip(
            context, region, id_, body['floatingip'])

        if fip:
            return self._view.show(context, request, fip)

    @pecan.expose(template='json:', content_type='application/json')
    def get_one(self, fip_key):
        """
        Get PTR
        """
        request = pecan.request
        context = request.environ['context']

        region, id_ = fip_key_to_data(fip_key)

        fip = self.central_api.get_floatingip(context, region, id_)

        return self._view.show(context, request, fip)

########NEW FILE########
__FILENAME__ = limits
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import pecan
from designate.openstack.common import log as logging
from designate.api.v2.controllers import rest
from designate.api.v2.views import limits as limits_view

LOG = logging.getLogger(__name__)


class LimitsController(rest.RestController):
    _view = limits_view.LimitsView()

    @pecan.expose(template='json:', content_type='application/json')
    def get_all(self):
        request = pecan.request
        context = pecan.request.environ['context']

        absolute_limits = self.central_api.get_absolute_limits(context)

        return self._view.show(context, request, absolute_limits)

########NEW FILE########
__FILENAME__ = nameservers
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import pecan
from designate import utils
from designate.openstack.common import log as logging
from designate.api.v2.controllers import rest
from designate.api.v2.views import nameservers as nameservers_view

LOG = logging.getLogger(__name__)


class NameServersController(rest.RestController):
    _view = nameservers_view.NameServerView()

    @pecan.expose(template='json:', content_type='application/json')
    @utils.validate_uuid('zone_id')
    def get_all(self, zone_id):
        request = pecan.request
        context = pecan.request.environ['context']

        servers = self.central_api.get_domain_servers(context, zone_id)

        return self._view.list(context, request, servers, [zone_id])

########NEW FILE########
__FILENAME__ = records
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import pecan
from designate.openstack.common import log as logging
from designate import schema
from designate import utils
from designate.api.v2.controllers import rest
from designate.api.v2.views import records as records_view

LOG = logging.getLogger(__name__)


class RecordsController(rest.RestController):
    _view = records_view.RecordsView()
    _resource_schema = schema.Schema('v2', 'record')
    _collection_schema = schema.Schema('v2', 'records')
    SORT_KEYS = ['created_at', 'id', 'updated_at', 'domain_id', 'tenant_id',
                 'recordset_id', 'status']

    @pecan.expose(template='json:', content_type='application/json')
    @utils.validate_uuid('zone_id', 'recordset_id', 'record_id')
    def get_one(self, zone_id, recordset_id, record_id):
        """ Get Record """
        request = pecan.request
        context = request.environ['context']

        record = self.central_api.get_record(context, zone_id, recordset_id,
                                             record_id)

        return self._view.show(context, request, record)

    @pecan.expose(template='json:', content_type='application/json')
    @utils.validate_uuid('zone_id', 'recordset_id')
    def get_all(self, zone_id, recordset_id, **params):
        """ List Records """
        request = pecan.request
        context = request.environ['context']

        # Extract the pagination params
        marker, limit, sort_key, sort_dir = self._get_paging_params(params)

        # Extract any filter params.
        accepted_filters = ('data', )
        criterion = dict((k, params[k]) for k in accepted_filters
                         if k in params)

        criterion['domain_id'] = zone_id
        criterion['recordset_id'] = recordset_id

        records = self.central_api.find_records(
            context, criterion, marker, limit, sort_key, sort_dir)

        return self._view.list(context, request, records,
                               [zone_id, recordset_id])

    @pecan.expose(template='json:', content_type='application/json')
    @utils.validate_uuid('zone_id', 'recordset_id')
    def post_all(self, zone_id, recordset_id):
        """ Create Record """
        request = pecan.request
        response = pecan.response
        context = request.environ['context']

        body = request.body_dict

        # Validate the request conforms to the schema
        self._resource_schema.validate(body)

        # Convert from APIv2 -> Central format
        values = self._view.load(context, request, body)

        # Create the records
        record = self.central_api.create_record(context, zone_id, recordset_id,
                                                values)

        # Prepare the response headers
        if record['status'] == 'PENDING':
            response.status_int = 202
        else:
            response.status_int = 201

        response.headers['Location'] = self._view._get_resource_href(
            request, record, [zone_id, recordset_id])

        # Prepare and return the response body
        return self._view.show(context, request, record)

    @pecan.expose(template='json:', content_type='application/json')
    @pecan.expose(template='json:', content_type='application/json-patch+json')
    @utils.validate_uuid('zone_id', 'recordset_id', 'record_id')
    def patch_one(self, zone_id, recordset_id, record_id):
        """ Update Record """
        request = pecan.request
        context = request.environ['context']
        body = request.body_dict
        response = pecan.response

        # Fetch the existing record
        record = self.central_api.get_record(context, zone_id, recordset_id,
                                             record_id)

        # Convert to APIv2 Format
        record = self._view.show(context, request, record)

        if request.content_type == 'application/json-patch+json':
            raise NotImplemented('json-patch not implemented')
        else:
            record = utils.deep_dict_merge(record, body)

            # Validate the request conforms to the schema
            self._resource_schema.validate(record)

            values = self._view.load(context, request, body)
            record = self.central_api.update_record(
                context, zone_id, recordset_id, record_id, values)

        if record['status'] == 'PENDING':
            response.status_int = 202
        else:
            response.status_int = 200

        return self._view.show(context, request, record)

    @pecan.expose(template=None, content_type='application/json')
    @utils.validate_uuid('zone_id', 'recordset_id', 'record_id')
    def delete_one(self, zone_id, recordset_id, record_id):
        """ Delete Record """
        request = pecan.request
        response = pecan.response
        context = request.environ['context']

        record = self.central_api.delete_record(context, zone_id, recordset_id,
                                                record_id)

        if record['status'] == 'DELETING':
            response.status_int = 202
        else:
            response.status_int = 204

        # NOTE: This is a hack and a half.. But Pecan needs it.
        return ''

########NEW FILE########
__FILENAME__ = recordsets
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import pecan
from designate.openstack.common import log as logging
from designate import schema
from designate import utils
from designate.api.v2.controllers import rest
from designate.api.v2.views import recordsets as recordsets_view
from designate.api.v2.controllers import records

LOG = logging.getLogger(__name__)


class RecordSetsController(rest.RestController):
    _view = recordsets_view.RecordSetsView()
    _resource_schema = schema.Schema('v2', 'recordset')
    _collection_schema = schema.Schema('v2', 'recordsets')
    SORT_KEYS = ['created_at', 'id', 'updated_at', 'domain_id', 'tenant_id',
                 'name', 'type', 'ttl']

    records = records.RecordsController()

    @pecan.expose(template='json:', content_type='application/json')
    @utils.validate_uuid('zone_id', 'recordset_id')
    def get_one(self, zone_id, recordset_id):
        """ Get RecordSet """
        request = pecan.request
        context = request.environ['context']

        recordset = self.central_api.get_recordset(context, zone_id,
                                                   recordset_id)

        return self._view.show(context, request, recordset)

    @pecan.expose(template='json:', content_type='application/json')
    @utils.validate_uuid('zone_id')
    def get_all(self, zone_id, **params):
        """ List RecordSets """
        request = pecan.request
        context = request.environ['context']

        # Extract the pagination params
        marker, limit, sort_key, sort_dir = self._get_paging_params(params)

        # Extract any filter params.
        accepted_filters = ('name', 'type', 'ttl', )
        criterion = dict((k, params[k]) for k in accepted_filters
                         if k in params)

        criterion['domain_id'] = zone_id

        recordsets = self.central_api.find_recordsets(
            context, criterion, marker, limit, sort_key, sort_dir)

        return self._view.list(context, request, recordsets, [zone_id])

    @pecan.expose(template='json:', content_type='application/json')
    @utils.validate_uuid('zone_id')
    def post_all(self, zone_id):
        """ Create RecordSet """
        request = pecan.request
        response = pecan.response
        context = request.environ['context']

        body = request.body_dict

        # Validate the request conforms to the schema
        self._resource_schema.validate(body)

        # Convert from APIv2 -> Central format
        values = self._view.load(context, request, body)

        # Create the recordset
        recordset = self.central_api.create_recordset(context, zone_id, values)

        # Prepare the response headers
        response.status_int = 201
        response.headers['Location'] = self._view._get_resource_href(
            request, recordset, [zone_id])

        # Prepare and return the response body
        return self._view.show(context, request, recordset)

    @pecan.expose(template='json:', content_type='application/json')
    @pecan.expose(template='json:', content_type='application/json-patch+json')
    @utils.validate_uuid('zone_id', 'recordset_id')
    def patch_one(self, zone_id, recordset_id):
        """ Update RecordSet """
        request = pecan.request
        context = request.environ['context']
        body = request.body_dict
        response = pecan.response

        # Fetch the existing recordset
        recordset = self.central_api.get_recordset(context, zone_id,
                                                   recordset_id)

        # Convert to APIv2 Format
        recordset = self._view.show(context, request, recordset)

        if request.content_type == 'application/json-patch+json':
            raise NotImplemented('json-patch not implemented')
        else:
            recordset = utils.deep_dict_merge(recordset, body)

            # Validate the request conforms to the schema
            self._resource_schema.validate(recordset)

            values = self._view.load(context, request, body)
            recordset = self.central_api.update_recordset(
                context, zone_id, recordset_id, values)

        response.status_int = 200

        return self._view.show(context, request, recordset)

    @pecan.expose(template=None, content_type='application/json')
    @utils.validate_uuid('zone_id', 'recordset_id')
    def delete_one(self, zone_id, recordset_id):
        """ Delete RecordSet """
        request = pecan.request
        response = pecan.response
        context = request.environ['context']

        self.central_api.delete_recordset(context, zone_id, recordset_id)

        response.status_int = 204

        # NOTE: This is a hack and a half.. But Pecan needs it.
        return ''

########NEW FILE########
__FILENAME__ = rest
# flake8: noqa
# Copyright (c) <2011>, Jonathan LaCour
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#
#     * Redistributions of source code must retain the above copyright
#       notice, this list of conditions and the following disclaimer.
#     * Redistributions in binary form must reproduce the above copyright
#       notice, this list of conditions and the following disclaimer in the
#       documentation and/or other materials provided with the distribution.
#     * Neither the name of the <organization> nor the
#       names of its contributors may be used to endorse or promote products
#       derived from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
# ARE DISCLAIMED. IN NO EVENT SHALL <COPYRIGHT HOLDER> BE LIABLE FOR ANY
# DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
import six
import inspect
import pecan
import pecan.rest
import pecan.routing
from designate import exceptions
from designate import api
from designate.openstack.common import log as logging
from designate.openstack.common.gettextutils import _

LOG = logging.getLogger(__name__)


class RestController(pecan.rest.RestController):
    """
    Extension for Pecan's RestController to better handle POST/PUT/PATCH
    requests.

    Ideally, we get these additions merged upstream.
    """

    # default sort_keys.  The Controllers can override this.
    SORT_KEYS = ['created_at', 'id']

    @property
    def central_api(self):
        return api.get_central_api()

    def _get_paging_params(self, params):
        """
        Extract any paging parameters
        """
        marker = params.pop('marker', None)
        limit = params.pop('limit', None)
        sort_key = params.pop('sort_key', None)
        sort_dir = params.pop('sort_dir', None)

        # Negative and zero limits are not caught in storage.
        # With a number bigger than MAXSIZE, rpc throws an 'OverflowError long
        #  too big to convert'.
        # So the parameter 'limit' is checked here.
        if limit:
            try:
                invalid_limit_message = _(str.format(
                    'limit should be an integer between 1 and {0}',
                    six.MAXSIZE))
                int_limit = int(limit)
                if int_limit <= 0 or int_limit > six.MAXSIZE:
                    raise exceptions.InvalidLimit(invalid_limit_message)
            # This exception is raised for non ints when int(limit) is called
            except ValueError:
                raise exceptions.InvalidLimit(invalid_limit_message)

        # sort_dir is checked in paginate_query.
        # We duplicate the sort_dir check here to throw a more specific
        # exception than ValueError.
        if sort_dir and sort_dir not in ['asc', 'desc']:
            raise exceptions.InvalidSortDir(_("Unknown sort direction, "
                                              "must be 'desc' or 'asc'"))

        if sort_key and sort_key not in self.SORT_KEYS:
            raise exceptions.InvalidSortKey(_(str.format(
                'sort key must be one of {0}', str(self.SORT_KEYS))))

        return marker, limit, sort_key, sort_dir

    def _handle_post(self, method, remainder):
        '''
        Routes ``POST`` actions to the appropriate controller.
        '''
        # route to a post_all or get if no additional parts are available
        if not remainder or remainder == ['']:
            controller = self._find_controller('post_all', 'post')
            if controller:
                return controller, []
            pecan.abort(405)

        controller = getattr(self, remainder[0], None)
        if controller and not inspect.ismethod(controller):
            return pecan.routing.lookup_controller(controller, remainder[1:])

        # finally, check for the regular post_one/post requests
        controller = self._find_controller('post_one', 'post')
        if controller:
            return controller, remainder

        pecan.abort(405)

    def _handle_patch(self, method, remainder):
        '''
        Routes ``PATCH`` actions to the appropriate controller.
        '''
        # route to a patch_all or get if no additional parts are available
        if not remainder or remainder == ['']:
            controller = self._find_controller('patch_all', 'patch')
            if controller:
                return controller, []
            pecan.abort(405)

        controller = getattr(self, remainder[0], None)
        if controller and not inspect.ismethod(controller):
            return pecan.routing.lookup_controller(controller, remainder[1:])

        # finally, check for the regular patch_one/patch requests
        controller = self._find_controller('patch_one', 'patch')
        if controller:
            return controller, remainder

        pecan.abort(405)

    def _handle_put(self, method, remainder):
        '''
        Routes ``PUT`` actions to the appropriate controller.
        '''
        # route to a put_all or get if no additional parts are available
        if not remainder or remainder == ['']:
            controller = self._find_controller('put_all', 'put')
            if controller:
                return controller, []
            pecan.abort(405)

        controller = getattr(self, remainder[0], None)
        if controller and not inspect.ismethod(controller):
            return pecan.routing.lookup_controller(controller, remainder[1:])

        # finally, check for the regular put_one/put requests
        controller = self._find_controller('put_one', 'put')
        if controller:
            return controller, remainder

        pecan.abort(405)

    def _handle_delete(self, method, remainder):
        '''
        Routes ``DELETE`` actions to the appropriate controller.
        '''
        # route to a delete_all or get if no additional parts are available
        if not remainder or remainder == ['']:
            controller = self._find_controller('delete_all', 'delete')
            if controller:
                return controller, []
            pecan.abort(405)

        controller = getattr(self, remainder[0], None)
        if controller and not inspect.ismethod(controller):
            return pecan.routing.lookup_controller(controller, remainder[1:])

        # finally, check for the regular delete_one/delete requests
        controller = self._find_controller('delete_one', 'delete')
        if controller:
            return controller, remainder

        pecan.abort(405)

########NEW FILE########
__FILENAME__ = reverse
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Endre Karlson <endre.karlson@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.api.v2.controllers import rest
from designate.api.v2.controllers import floatingips


class ReverseController(rest.RestController):
    floatingips = floatingips.FloatingIPController()

########NEW FILE########
__FILENAME__ = root
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.openstack.common import log as logging
from designate.api.v2.controllers import limits
from designate.api.v2.controllers import reverse
from designate.api.v2.controllers import schemas
from designate.api.v2.controllers import tlds
from designate.api.v2.controllers import zones
from designate.api.v2.controllers import blacklists

LOG = logging.getLogger(__name__)


class RootController(object):
    """
    This is /v2/ Controller. Pecan will find all controllers via the object
    properties attached to this.
    """
    limits = limits.LimitsController()
    schemas = schemas.SchemasController()
    reverse = reverse.ReverseController()
    tlds = tlds.TldsController()
    zones = zones.ZonesController()
    blacklists = blacklists.BlacklistsController()

########NEW FILE########
__FILENAME__ = schemas
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import os
import pecan
from designate import exceptions
from designate import utils
from designate.openstack.common import log as logging


LOG = logging.getLogger(__name__)


class SchemasController(object):
    @pecan.expose(template='json:', content_type='application/schema+json')
    def _default(self, *remainder):
        if len(remainder) == 0:
            pecan.abort(404)

        try:
            schema_name = os.path.join(*remainder)
            schema_json = utils.load_schema('v2', schema_name)
        except exceptions.ResourceNotFound:
            pecan.abort(404)

        return schema_json

########NEW FILE########
__FILENAME__ = tlds
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
import pecan
from designate.openstack.common import log as logging
from designate import schema
from designate import utils
from designate.api.v2.controllers import rest
from designate.api.v2.views import tlds as tlds_view

LOG = logging.getLogger(__name__)


class TldsController(rest.RestController):
    _view = tlds_view.TldsView()
    _resource_schema = schema.Schema('v2', 'tld')
    _collection_schema = schema.Schema('v2', 'tlds')
    SORT_KEYS = ['created_at', 'id', 'updated_at', 'name']

    @pecan.expose(template='json:', content_type='application/json')
    @utils.validate_uuid('tld_id')
    def get_one(self, tld_id):
        """ Get Tld """

        request = pecan.request
        context = request.environ['context']

        tld = self.central_api.get_tld(context, tld_id)
        return self._view.show(context, request, tld)

    @pecan.expose(template='json:', content_type='application/json')
    def get_all(self, **params):
        """ List Tlds """
        request = pecan.request
        context = request.environ['context']

        # Extract the pagination params
        marker, limit, sort_key, sort_dir = self._get_paging_params(params)

        # Extract any filter params.
        accepted_filters = ('name')
        criterion = dict((k, params[k]) for k in accepted_filters
                         if k in params)

        tlds = self.central_api.find_tlds(
            context, criterion, marker, limit, sort_key, sort_dir)

        return self._view.list(context, request, tlds)

    @pecan.expose(template='json:', content_type='application/json')
    def post_all(self):
        """ Create Tld """
        request = pecan.request
        response = pecan.response
        context = request.environ['context']
        body = request.body_dict

        # Validate the request conforms to the schema
        self._resource_schema.validate(body)

        # Convert from APIv2 -> Central format
        values = self._view.load(context, request, body)

        # Create the tld
        tld = self.central_api.create_tld(context, values)
        response.status_int = 201

        response.headers['Location'] = self._view._get_resource_href(request,
                                                                     tld)
        # Prepare and return the response body
        return self._view.show(context, request, tld)

    @pecan.expose(template='json:', content_type='application/json')
    @pecan.expose(template='json:', content_type='application/json-patch+json')
    @utils.validate_uuid('tld_id')
    def patch_one(self, tld_id):
        """ Update Tld """
        request = pecan.request
        context = request.environ['context']
        body = request.body_dict
        response = pecan.response

        # Fetch the existing tld
        tld = self.central_api.get_tld(context, tld_id)

        # Convert to APIv2 Format
        tld = self._view.show(context, request, tld)

        if request.content_type == 'application/json-patch+json':
            raise NotImplemented('json-patch not implemented')
        else:
            tld = utils.deep_dict_merge(tld, body)

            # Validate the request conforms to the schema
            self._resource_schema.validate(tld)

            values = self._view.load(context, request, body)
            tld = self.central_api.update_tld(context, tld_id, values)

        response.status_int = 200

        return self._view.show(context, request, tld)

    @pecan.expose(template=None, content_type='application/json')
    @utils.validate_uuid('tld_id')
    def delete_one(self, tld_id):
        """ Delete Tld """
        request = pecan.request
        response = pecan.response
        context = request.environ['context']

        self.central_api.delete_tld(context, tld_id)

        response.status_int = 204

        # NOTE: This is a hack and a half.. But Pecan needs it.
        return ''

########NEW FILE########
__FILENAME__ = zones
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import pecan
from dns import zone as dnszone
from dns import rdatatype
from dns import exception as dnsexception
from designate import exceptions
from designate import utils
from designate import schema
from designate.api.v2.controllers import rest
from designate.api.v2.controllers import nameservers
from designate.api.v2.controllers import recordsets
from designate.api.v2.views import zones as zones_view
from designate.openstack.common import log as logging

LOG = logging.getLogger(__name__)


class ZonesController(rest.RestController):
    _view = zones_view.ZonesView()
    _resource_schema = schema.Schema('v2', 'zone')
    _collection_schema = schema.Schema('v2', 'zones')
    SORT_KEYS = ['created_at', 'id', 'updated_at', 'name', 'tenant_id',
                 'serial', 'ttl']

    nameservers = nameservers.NameServersController()
    recordsets = recordsets.RecordSetsController()

    @pecan.expose(template=None, content_type='text/dns')
    @pecan.expose(template='json:', content_type='application/json')
    @utils.validate_uuid('zone_id')
    def get_one(self, zone_id):
        """ Get Zone """
        # TODO(kiall): Validate we have a sane UUID for zone_id

        request = pecan.request
        context = request.environ['context']
        if 'Accept' not in request.headers:
            raise exceptions.BadRequest('Missing Accept header')
        best_match = request.accept.best_match(['text/dns',
                                                'application/json'])
        if best_match == 'text/dns':
            return self._get_zonefile(request, context, zone_id)
        elif best_match == 'application/json':
            return self._get_json(request, context, zone_id)
        else:
            raise exceptions.UnsupportedAccept(
                'Accept must be text/dns or application/json')

    def _get_json(self, request, context, zone_id):
        """ 'Normal' zone get """
        zone = self.central_api.get_domain(context, zone_id)

        return self._view.show(context, request, zone)

    def _get_zonefile(self, request, context, zone_id):
        """ Export zonefile """
        servers = self.central_api.get_domain_servers(context, zone_id)
        domain = self.central_api.get_domain(context, zone_id)

        criterion = {'domain_id': zone_id}
        recordsets = self.central_api.find_recordsets(context, criterion)

        records = []

        for recordset in recordsets:
            criterion = {
                'domain_id': domain['id'],
                'recordset_id': recordset['id']
            }

            raw_records = self.central_api.find_records(context, criterion)

            for record in raw_records:
                records.append({
                    'name': recordset['name'],
                    'type': recordset['type'],
                    'ttl': recordset['ttl'],
                    'priority': record['priority'],
                    'data': record['data'],
                })

        return utils.render_template('bind9-zone.jinja2',
                                     servers=servers,
                                     domain=domain,
                                     records=records)

    @pecan.expose(template='json:', content_type='application/json')
    def get_all(self, **params):
        """ List Zones """
        request = pecan.request
        context = request.environ['context']

        marker, limit, sort_key, sort_dir = self._get_paging_params(params)

        # Extract any filter params.
        accepted_filters = ('name', 'email', )
        criterion = dict((k, params[k]) for k in accepted_filters
                         if k in params)

        zones = self.central_api.find_domains(
            context, criterion, marker, limit, sort_key, sort_dir)

        return self._view.list(context, request, zones)

    @pecan.expose(template='json:', content_type='application/json')
    def post_all(self):
        """ Create Zone """
        request = pecan.request
        response = pecan.response
        context = request.environ['context']
        if request.content_type == 'text/dns':
            return self._post_zonefile(request, response, context)
        elif request.content_type == 'application/json':
            return self._post_json(request, response, context)
        else:
            raise exceptions.UnsupportedContentType(
                'Content-type must be text/dns or application/json')

    def _post_json(self, request, response, context):
        """ 'Normal' zone creation """
        body = request.body_dict

        # Validate the request conforms to the schema
        self._resource_schema.validate(body)

        # Convert from APIv2 -> Central format
        values = self._view.load(context, request, body)

        # Create the zone
        zone = self.central_api.create_domain(context, values)

        # Prepare the response headers
        # If the zone has been created asynchronously

        if zone['status'] == 'PENDING':
            response.status_int = 202
        else:
            response.status_int = 201

        response.headers['Location'] = self._view._get_resource_href(request,
                                                                     zone)

        # Prepare and return the response body
        return self._view.show(context, request, zone)

    def _post_zonefile(self, request, response, context):
        """ Import Zone """
        dnspython_zone = self._parse_zonefile(request)
        # TODO(artom) This should probably be handled with transactions
        zone = self._create_zone(context, dnspython_zone)

        try:
            self._create_records(context, zone['id'], dnspython_zone)
        except exceptions.Base as e:
            self.central_api.delete_domain(context, zone['id'])
            raise e

        if zone['status'] == 'PENDING':
            response.status_int = 202
        else:
            response.status_int = 201

        response.headers['Location'] = self._view._get_resource_href(request,
                                                                     zone)
        return self._view.show(context, request, zone)

    @pecan.expose(template='json:', content_type='application/json')
    @pecan.expose(template='json:', content_type='application/json-patch+json')
    @utils.validate_uuid('zone_id')
    def patch_one(self, zone_id):
        """ Update Zone """
        # TODO(kiall): This needs cleanup to say the least..
        request = pecan.request
        context = request.environ['context']
        body = request.body_dict
        response = pecan.response

        # TODO(kiall): Validate we have a sane UUID for zone_id

        # Fetch the existing zone
        zone = self.central_api.get_domain(context, zone_id)

        # Convert to APIv2 Format
        zone = self._view.show(context, request, zone)

        if request.content_type == 'application/json-patch+json':
            # Possible pattern:
            #
            # 1) Load existing zone.
            # 2) Apply patch, maintain list of changes.
            # 3) Return changes, after passing through the code ^ for plain
            #    JSON.
            #
            # Difficulties:
            #
            # 1) "Nested" resources? records inside a recordset.
            # 2) What to do when a zone doesn't exist in the first place?
            # 3) ...?
            raise NotImplemented('json-patch not implemented')
        else:
            zone = utils.deep_dict_merge(zone, body)

            # Validate the request conforms to the schema
            self._resource_schema.validate(zone)

            values = self._view.load(context, request, body)
            zone = self.central_api.update_domain(context, zone_id, values)

        if zone['status'] == 'PENDING':
            response.status_int = 202
        else:
            response.status_int = 200

        return self._view.show(context, request, zone)

    @pecan.expose(template=None, content_type='application/json')
    @utils.validate_uuid('zone_id')
    def delete_one(self, zone_id):
        """ Delete Zone """
        request = pecan.request
        response = pecan.response
        context = request.environ['context']

        # TODO(kiall): Validate we have a sane UUID for zone_id

        zone = self.central_api.delete_domain(context, zone_id)

        if zone['status'] == 'DELETING':
            response.status_int = 202
        else:
            response.status_int = 204

        # NOTE: This is a hack and a half.. But Pecan needs it.
        return ''

    #TODO(artom) Methods below may be useful elsewhere, consider putting them
    # somewhere reusable.

    def _create_zone(self, context, dnspython_zone):
        """ Creates the initial zone """
        # dnspython never builds a zone with more than one SOA, even if we give
        # it a zonefile that contains more than one
        soa = dnspython_zone.get_rdataset(dnspython_zone.origin, 'SOA')
        if soa is None:
            raise exceptions.BadRequest('An SOA record is required')
        email = soa[0].rname.to_text().rstrip('.')
        email = email.replace('.', '@', 1)
        values = {
            'name': dnspython_zone.origin.to_text(),
            'email': email,
            'ttl': str(soa.ttl)
        }
        return self.central_api.create_domain(context, values)

    def _record2json(self, record_type, rdata):
        if record_type == 'MX':
            return {
                'data': rdata.exchange.to_text(),
                'priority': str(rdata.preference)
            }
        elif record_type == 'SRV':
            return {
                'data': '%s %s %s' % (str(rdata.weight), str(rdata.port),
                                      rdata.target.to_text()),
                'priority': str(rdata.priority)
            }
        else:
            return {
                'data': rdata.to_text()
            }

    def _create_records(self, context, zone_id, dnspython_zone):
        """ Creates the records """
        for record_name in dnspython_zone.nodes.keys():
            for rdataset in dnspython_zone.nodes[record_name]:
                record_type = rdatatype.to_text(rdataset.rdtype)

                if record_type == 'SOA':
                    continue

                # Create the recordset
                values = {
                    'domain_id': zone_id,
                    'name': record_name.to_text(),
                    'type': record_type,
                }

                recordset = self.central_api.create_recordset(
                    context, zone_id, values)

                for rdata in rdataset:
                    if (record_type == 'NS'
                            and record_name == dnspython_zone.origin):
                        # Don't create NS records for the domain, they've been
                        # taken care of as servers
                        pass
                    else:
                        # Everything else, including delegation NS, gets
                        # created
                        values = self._record2json(record_type, rdata)

                        self.central_api.create_record(
                            context, zone_id, recordset['id'], values)

    def _parse_zonefile(self, request):
        """ Parses a POSTed zonefile into a dnspython zone object """
        try:
            dnspython_zone = dnszone.from_text(
                request.body,
                # Don't relativize, otherwise we end up with '@' record names.
                relativize=False,
                # Dont check origin, we allow missing NS records (missing SOA
                # records are taken care of in _create_zone).
                check_origin=False)
        except dnszone.UnknownOrigin:
            raise exceptions.BadRequest('The $ORIGIN statement is required and'
                                        ' must be the first statement in the'
                                        ' zonefile.')
        except dnsexception.SyntaxError:
            raise exceptions.BadRequest('Malformed zonefile.')
        return dnspython_zone

########NEW FILE########
__FILENAME__ = patches
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import pecan.core
from designate import exceptions
from designate.openstack.common import jsonutils

JSON_TYPES = ('application/json', 'application/json-patch+json')


class Request(pecan.core.Request):
    @property
    def body_dict(self):
        """
        Returns the body content as a dictonary, deserializing per the
        Content-Type header.

        We add this method to ease future XML support, so the main code
        is not hardcoded to call pecans "request.json()" method.
        """
        if self.content_type in JSON_TYPES:
            try:
                return jsonutils.load(self.body_file)
            except ValueError as valueError:
                raise exceptions.InvalidJson(valueError.message)
        else:
            raise Exception('TODO: Unsupported Content Type')


pecan.core.Request = Request

########NEW FILE########
__FILENAME__ = base
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import urllib
from oslo.config import cfg
from designate import exceptions
from designate.openstack.common import log as logging


LOG = logging.getLogger(__name__)
CONF = cfg.CONF


class BaseView(object):
    """
    The Views are responsible for coverting to/from the "intenal" and
    "external" representations of collections and resources. This includes
    adding "links" and adding/removing any other wrappers returned/received
    as part of the API call.

    For example, in the V2 API, we did s/domain/zone/. Adapting a record
    resources "domain_id" <-> "zone_id" is the responsibility of a View.
    """
    _resource_name = None
    _collection_name = None

    def __init__(self):
        super(BaseView, self).__init__()

        self.base_uri = CONF['service:api']['api_base_uri'].rstrip('/')

    def list(self, context, request, items, parents=None):
        """ View of a list of items """
        result = {
            "links": self._get_collection_links(request, items, parents)
        }

        if 'detail' in request.GET and request.GET['detail'] == 'yes':
            result[self._collection_name] = self.list_detail(context, request,
                                                             items)
        else:
            result[self._collection_name] = self.list_basic(context, request,
                                                            items)

        return result

    def list_basic(self, context, request, items):
        """ Non-detailed list of items """
        return [self.show_basic(context, request, i) for i in items]

    def list_detail(self, context, request, items):
        """ Detailed list of items """
        return [self.show_detail(context, request, i) for i in items]

    def show(self, context, request, item):
        """ Show a single item """
        result = {}

        if 'detail' in request.GET and request.GET['detail'] == 'yes':
            result[self._resource_name] = self.show_detail(context, request,
                                                           item)
        else:
            result[self._resource_name] = self.show_basic(context, request,
                                                          item)

        return result

    def show_basic(self, context, request, item):
        """ Non-detailed view of a item """
        raise NotImplementedError()

    def show_detail(self, context, request, item):
        """ Detailed view of a item """
        return self.show_basic(context, request, item)

    def _load(self, context, request, body, valid_keys):
        """ Extract a "central" compatible dict from an API call """
        result = {}
        item = body[self._resource_name]
        error_keys = []

        # Copy keys which need no alterations
        for k in item:
            if k in valid_keys:
                result[k] = item[k]
            else:
                error_keys.append(k)

        if error_keys:
            error_message = str.format(
                'Provided object does not match schema.  Keys {0} are not '
                'valid in the request body',
                error_keys)

            raise exceptions.InvalidObject(error_message)

        return result

    def _get_resource_links(self, request, item, parents=None):
        return {
            "self": self._get_resource_href(request, item, parents),
        }

    def _get_collection_links(self, request, items, parents=None):
        # TODO(kiall): Next and previous links should only be included
        #              when there are more/previous items.. This is what nova
        #              does.. But I think we can do better.

        params = request.GET

        result = {
            "self": self._get_collection_href(request, parents),
        }

        # See above
        #if 'marker' in params:
        #    result['previous'] = self._get_previous_href(request, items,
        #                                                 parents)

        if 'limit' in params and int(params['limit']) == len(items):
            result['next'] = self._get_next_href(request, items, parents)

        return result

    def _get_base_href(self, parents=None):
        href = "%s/v2/%s" % (self.base_uri, self._collection_name)

        return href.rstrip('?')

    def _get_resource_href(self, request, item, parents=None):
        base_href = self._get_base_href(parents)
        href = "%s/%s" % (base_href, item['id'])

        return href.rstrip('?')

    def _get_collection_href(self, request, parents=None, extra_params=None):
        params = request.GET

        if extra_params is not None:
            params.update(extra_params)

        base_href = self._get_base_href(parents)

        href = "%s?%s" % (base_href, urllib.urlencode(params))

        return href.rstrip('?')

    def _get_next_href(self, request, items, parents=None):
        # Prepare the extra params
        extra_params = {
            'marker': items[-1]['id']
        }

        return self._get_collection_href(request, parents, extra_params)

    def _get_previous_href(self, request, items, parents=None):
        # Prepare the extra params
        extra_params = {
            'marker': items[0]['id']
        }

        return self._get_collection_href(request, parents, extra_params)

########NEW FILE########
__FILENAME__ = blacklists
# Copyright 2014 Rackspace
#
# Author: Betsy Luzader <betsy.luzader@rackspace.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from designate.api.v2.views import base as base_view
from designate.openstack.common import log as logging


LOG = logging.getLogger(__name__)


class BlacklistsView(base_view.BaseView):
    """ Model a Blacklist API response as a python dictionary """

    _resource_name = 'blacklist'
    _collection_name = 'blacklists'

    def show_basic(self, context, request, blacklist):
        """ Detailed view of a blacklisted zone """
        return {
            "id": blacklist['id'],
            "pattern": blacklist['pattern'],
            "description": blacklist['description'],
            "created_at": blacklist['created_at'],
            "updated_at": blacklist['updated_at'],
            "links": self._get_resource_links(request, blacklist)
        }

    def load(self, context, request, body):
        """ Extract a "central" compatible dict from an API call """
        valid_keys = ('pattern', 'description')
        return self._load(context, request, body, valid_keys)

########NEW FILE########
__FILENAME__ = floatingips
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Endre Karlson <endre.karlson@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.api.v2.views import base as base_view
from designate.openstack.common import log as logging


LOG = logging.getLogger(__name__)


class FloatingIPView(base_view.BaseView):
    """ Model a FloatingIP PTR record as a python dict """
    _resource_name = 'floatingip'
    _collection_name = 'floatingips'

    def _get_base_href(self, parents=None):
        return '%s/v2/reverse/floatingips' % self.base_uri

    def show_basic(self, context, request, item):
        item['id'] = ":".join([item.pop('region'), item.pop('id')])
        item['links'] = self._get_resource_links(
            request, item, [item['id']])
        return item

########NEW FILE########
__FILENAME__ = limits
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.api.v2.views import base as base_view
from designate.openstack.common import log as logging


LOG = logging.getLogger(__name__)


class LimitsView(base_view.BaseView):
    """ Model a Limits API response as a python dictionary """

    _resource_name = 'limits'
    _collection_name = 'limits'

    def show_basic(self, context, request, absolute_limits):
        """ Basic view of the limits """

        return {
            "absolute": {
                "maxZones": absolute_limits['domains'],
                "maxZoneRecords": absolute_limits['domain_records']
            }
        }

########NEW FILE########
__FILENAME__ = nameservers
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from designate.api.v2.views import base as base_view
from designate.openstack.common import log as logging


LOG = logging.getLogger(__name__)


class NameServerView(base_view.BaseView):
    """ Model a NameServer API response as a python dictionary """

    _resource_name = 'nameserver'
    _collection_name = 'nameservers'

    def _get_base_href(self, parents=None):
        assert len(parents) == 1

        href = "%s/v2/zones/%s/nameservers" % (self.base_uri, parents[0])

        return href.rstrip('?')

    def show_basic(self, context, request, nameserver):
        """ Basic view of a nameserver """
        return {
            "id": nameserver["id"],
            "name": nameserver["name"]
        }

########NEW FILE########
__FILENAME__ = records
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.api.v2.views import base as base_view
from designate.openstack.common import log as logging


LOG = logging.getLogger(__name__)


class RecordsView(base_view.BaseView):
    """ Model a Record API response as a python dictionary """

    _resource_name = 'record'
    _collection_name = 'records'

    def _get_base_href(self, parents=None):
        assert len(parents) == 2

        href = "%s/v2/zones/%s/recordsets/%s/records" % (self.base_uri,
                                                         parents[0],
                                                         parents[1])

        return href.rstrip('?')

    def show_basic(self, context, request, record):
        """ Basic view of a record """
        return {
            "id": record['id'],
            "recordset_id": record['recordset_id'],
            "data": record['data'],
            "description": record['description'],
            "version": record['version'],
            "created_at": record['created_at'],
            "updated_at": record['updated_at'],
            "links": self._get_resource_links(
                request, record,
                [record['domain_id'], record['recordset_id']])
        }

    def load(self, context, request, body):
        """ Extract a "central" compatible dict from an API call """
        valid_keys = ('data', 'description')
        return self._load(context, request, body, valid_keys)

########NEW FILE########
__FILENAME__ = recordsets
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.api.v2.views import base as base_view
from designate.openstack.common import log as logging


LOG = logging.getLogger(__name__)


class RecordSetsView(base_view.BaseView):
    """ Model a Zone API response as a python dictionary """

    _resource_name = 'recordset'
    _collection_name = 'recordsets'

    def _get_base_href(self, parents=None):
        assert len(parents) == 1

        href = "%s/v2/zones/%s/recordsets" % (self.base_uri, parents[0])

        return href.rstrip('?')

    def show_basic(self, context, request, recordset):
        """ Basic view of a recordset """
        return {
            "id": recordset['id'],
            "zone_id": recordset['domain_id'],
            "name": recordset['name'],
            "type": recordset['type'],
            "ttl": recordset['ttl'],
            "description": recordset['description'],
            "version": recordset['version'],
            "created_at": recordset['created_at'],
            "updated_at": recordset['updated_at'],
            "links": self._get_resource_links(request, recordset,
                                              [recordset['domain_id']])
        }

    def load(self, context, request, body):
        """ Extract a "central" compatible dict from an API call """
        valid_keys = ('name', 'type', 'ttl', 'description')
        return self._load(context, request, body, valid_keys)

########NEW FILE########
__FILENAME__ = tlds
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.api.v2.views import base as base_view
from designate.openstack.common import log as logging


LOG = logging.getLogger(__name__)


class TldsView(base_view.BaseView):
    """ Model a TLD API response as a python dictionary """

    _resource_name = 'tld'
    _collection_name = 'tlds'

    def show_basic(self, context, request, tld):
        """ Basic view of a tld """
        return {
            "id": tld['id'],
            "name": tld['name'],
            "description": tld['description'],
            "created_at": tld['created_at'],
            "updated_at": tld['updated_at'],
            "links": self._get_resource_links(request, tld)
        }

    def load(self, context, request, body):
        """ Extract a "central" compatible dict from an API call """
        valid_keys = ('name', 'description')
        return self._load(context, request, body, valid_keys)

########NEW FILE########
__FILENAME__ = zones
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.api.v2.views import base as base_view
from designate.openstack.common import log as logging


LOG = logging.getLogger(__name__)


class ZonesView(base_view.BaseView):
    """ Model a Zone API response as a python dictionary """

    _resource_name = 'zone'
    _collection_name = 'zones'

    def show_basic(self, context, request, zone):
        """ Basic view of a zone """
        # TODO(kiall): pool_id should not be hardcoded.. even temp :)
        return {
            "id": zone['id'],
            "pool_id": "572ba08c-d929-4c70-8e42-03824bb24ca2",
            "project_id": zone['tenant_id'],
            "name": zone['name'],
            "email": zone['email'],
            "description": zone['description'],
            "ttl": zone['ttl'],
            "serial": zone['serial'],
            "status": zone['status'],
            "version": zone['version'],
            "created_at": zone['created_at'],
            "updated_at": zone['updated_at'],
            "links": self._get_resource_links(request, zone)
        }

    def load(self, context, request, body):
        """ Extract a "central" compatible dict from an API call """
        valid_keys = ('name', 'email', 'description', 'ttl')
        return self._load(context, request, body, valid_keys)

########NEW FILE########
__FILENAME__ = versions
# Copyright 2012 Hewlett-Packard Development Company, L.P. All Rights Reserved.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import flask
from oslo.config import cfg


def factory(global_config, **local_conf):
    app = flask.Flask('designate.api.versions')

    versions = []

    if cfg.CONF['service:api'].enable_api_v1:
        versions.append({
            "id": "v1",
            "status": "CURRENT"
        })

    if cfg.CONF['service:api'].enable_api_v2:
        versions.append({
            "id": "v2",
            "status": "EXPERIMENTAL"
        })

    @app.route('/', methods=['GET'])
    def version_list():
        return flask.jsonify({
            "versions": versions
        })

    return app

########NEW FILE########
__FILENAME__ = base
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import abc
from designate.openstack.common import log as logging
from designate import exceptions
from designate.context import DesignateContext
from designate.plugin import DriverPlugin


LOG = logging.getLogger(__name__)


class Backend(DriverPlugin):
    """ Base class for backend implementations """
    __plugin_type__ = 'backend'
    __plugin_ns__ = 'designate.backend'

    def __init__(self, central_service):
        super(Backend, self).__init__()
        self.central_service = central_service
        self.admin_context = DesignateContext.get_admin_context()
        self.admin_context.all_tenants = True

    def start(self):
        pass

    def stop(self):
        pass

    def create_tsigkey(self, context, tsigkey):
        """ Create a TSIG Key """
        raise exceptions.NotImplemented(
            'TSIG is not supported by this backend')

    def update_tsigkey(self, context, tsigkey):
        """ Update a TSIG Key """
        raise exceptions.NotImplemented(
            'TSIG is not supported by this backend')

    def delete_tsigkey(self, context, tsigkey):
        """ Delete a TSIG Key """
        raise exceptions.NotImplemented(
            'TSIG is not supported by this backend')

    @abc.abstractmethod
    def create_domain(self, context, domain):
        """ Create a DNS domain """

    @abc.abstractmethod
    def update_domain(self, context, domain):
        """ Update a DNS domain """

    @abc.abstractmethod
    def delete_domain(self, context, domain):
        """ Delete a DNS domain """

    def create_recordset(self, context, domain, recordset):
        """ Create a DNS recordset """

    @abc.abstractmethod
    def update_recordset(self, context, domain, recordset):
        """ Update a DNS recordset """

    @abc.abstractmethod
    def delete_recordset(self, context, domain, recordset):
        """ Delete a DNS recordset """

    @abc.abstractmethod
    def create_record(self, context, domain, recordset, record):
        """ Create a DNS record """

    @abc.abstractmethod
    def update_record(self, context, domain, recordset, record):
        """ Update a DNS record """

    @abc.abstractmethod
    def delete_record(self, context, domain, recordset, record):
        """ Delete a DNS record """

    @abc.abstractmethod
    def create_server(self, context, server):
        """ Create a DNS server """

    @abc.abstractmethod
    def update_server(self, context, server):
        """ Update a DNS server """

    @abc.abstractmethod
    def delete_server(self, context, server):
        """ Delete a DNS server """

    def sync_domain(self, context, domain, rdata):
        """
        Re-Sync a DNS domain

        This is the default, naive, domain synchronization implementation.
        """
        # First up, delete the domain from the backend.
        try:
            self.delete_domain(context, domain)
        except exceptions.DomainNotFound as e:
            # NOTE(Kiall): This means a domain was missing from the backend.
            #              Good thing we're doing a sync!
            LOG.warn("Failed to delete domain '%s' during sync. Message: %s",
                     domain['id'], str(e))

        # Next, re-create the domain in the backend.
        self.create_domain(context, domain)

        # Finally, re-create the records for the domain.
        for recordset, records in rdata:
            # Re-create the record in the backend.
            self.create_recordset(context, domain, recordset)
            for record in records:
                self.create_record(context, domain, recordset, record)

    def sync_record(self, context, domain, recordset, record):
        """
        Re-Sync a DNS record.

        This is the default, naive, record synchronization implementation.
        """
        # First up, delete the record from the backend.
        try:
            self.delete_record(context, domain, recordset, record)
        except exceptions.RecordNotFound as e:
            # NOTE(Kiall): This means a record was missing from the backend.
            #              Good thing we're doing a sync!
            LOG.warn("Failed to delete record '%s' in domain '%s' during sync."
                     " Message: %s", record['id'], domain['id'], str(e))

        # Finally, re-create the record in the backend.
        self.create_record(context, domain, recordset, record)

    def ping(self, context):
        """ Ping the Backend service """

        return {
            'status': None
        }

########NEW FILE########
__FILENAME__ = impl_bind9
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import os
from oslo.config import cfg
from designate.openstack.common import log as logging
from designate import utils
from designate.backend import base
import glob
import shutil

LOG = logging.getLogger(__name__)

cfg.CONF.register_group(cfg.OptGroup(
    name='backend:bind9', title="Configuration for BIND9 Backend"
))

cfg.CONF.register_opts([
    cfg.StrOpt('rndc-host', default='127.0.0.1', help='RNDC Host'),
    cfg.IntOpt('rndc-port', default=953, help='RNDC Port'),
    cfg.StrOpt('rndc-config-file', default=None,
               help='RNDC Config File'),
    cfg.StrOpt('rndc-key-file', default=None, help='RNDC Key File'),
    cfg.StrOpt('nzf-path', default='/var/cache/bind',
               help='Path where Bind9 stores the nzf files'),
], group='backend:bind9')


class Bind9Backend(base.Backend):
    __plugin_name__ = 'bind9'

    def start(self):
        super(Bind9Backend, self).start()

        domains = self.central_service.find_domains(self.admin_context)

        for domain in domains:
            rndc_op = 'reload'
            rndc_call = self._rndc_base() + [rndc_op]
            rndc_call.extend([domain['name']])

            try:
                LOG.debug('Calling RNDC with: %s' % " ".join(rndc_call))
                utils.execute(*rndc_call)
            except utils.processutils.ProcessExecutionError as proc_exec_err:
                stderr = proc_exec_err.stderr
                if stderr.count("rndc: 'reload' failed: not found") is not 0:
                    LOG.warn("Domain %s (%s) missing from backend, recreating",
                             domain['name'], domain['id'])
                    self._sync_domain(domain, new_domain_flag=True)
                else:
                    raise proc_exec_err

    def create_server(self, context, server):
        LOG.debug('Create Server')
        self._sync_domains_on_server_change()

    def update_server(self, context, server):
        LOG.debug('Update Server')
        self._sync_domains_on_server_change()

    def delete_server(self, context, server):
        LOG.debug('Delete Server')
        self._sync_domains_on_server_change()

    def create_domain(self, context, domain):
        LOG.debug('Create Domain')
        self._sync_domain(domain, new_domain_flag=True)

    def update_domain(self, context, domain):
        LOG.debug('Update Domain')
        self._sync_domain(domain)

    def delete_domain(self, context, domain):
        LOG.debug('Delete Domain')
        self._sync_delete_domain(domain)

    def update_recordset(self, context, domain, recordset):
        LOG.debug('Update RecordSet')
        self._sync_domain(domain)

    def delete_recordset(self, context, domain, recordset):
        LOG.debug('Delete RecordSet')
        self._sync_domain(domain)

    def create_record(self, context, domain, recordset, record):
        LOG.debug('Create Record')
        self._sync_domain(domain)

    def update_record(self, context, domain, recordset, record):
        LOG.debug('Update Record')
        self._sync_domain(domain)

    def delete_record(self, context, domain, recordset, record):
        LOG.debug('Delete Record')
        self._sync_domain(domain)

    def _rndc_base(self):
        rndc_call = [
            'rndc',
            '-s', cfg.CONF[self.name].rndc_host,
            '-p', str(cfg.CONF[self.name].rndc_port),
        ]

        if cfg.CONF[self.name].rndc_config_file:
            rndc_call.extend(['-c', cfg.CONF[self.name].rndc_config_file])

        if cfg.CONF[self.name].rndc_key_file:
            rndc_call.extend(['-k', cfg.CONF[self.name].rndc_key_file])

        return rndc_call

    def _sync_delete_domain(self, domain, new_domain_flag=False):
        """ Remove domain zone files and reload bind config """
        LOG.debug('Delete Domain: %s' % domain['id'])

        output_folder = os.path.join(os.path.abspath(cfg.CONF.state_path),
                                     'bind9')

        output_path = os.path.join(output_folder, '%s.zone' %
                                   "_".join([domain['name'], domain['id']]))

        os.remove(output_path)

        rndc_op = 'delzone'

        rndc_call = self._rndc_base() + [rndc_op, domain['name']]

        utils.execute(*rndc_call)

        #This goes and gets the name of the .nzf file that is a mirror of the
        #zones.config file we wish to maintain. The file name can change as it
        #is a hash of rndc view name, we're only interested in the first file
        #name this returns because there is only one .nzf file
        nzf_name = glob.glob('%s/*.nzf' % cfg.CONF[self.name].nzf_path)

        output_file = os.path.join(output_folder, 'zones.config')

        shutil.copyfile(nzf_name[0], output_file)

    def _sync_domain(self, domain, new_domain_flag=False):
        """ Sync a single domain's zone file and reload bind config """
        LOG.debug('Synchronising Domain: %s' % domain['id'])

        servers = self.central_service.find_servers(self.admin_context)

        recordsets = self.central_service.find_recordsets(
            self.admin_context, {'domain_id': domain['id']})

        records = []

        for recordset in recordsets:
            criterion = {
                'domain_id': domain['id'],
                'recordset_id': recordset['id']
            }

            raw_records = self.central_service.find_records(
                self.admin_context, criterion)

            for record in raw_records:
                records.append({
                    'name': recordset['name'],
                    'type': recordset['type'],
                    'ttl': recordset['ttl'],
                    'priority': record['priority'],
                    'data': record['data'],
                })

        output_folder = os.path.join(os.path.abspath(cfg.CONF.state_path),
                                     'bind9')

        output_path = os.path.join(output_folder, '%s.zone' %
                                   "_".join([domain['name'], domain['id']]))

        utils.render_template_to_file('bind9-zone.jinja2',
                                      output_path,
                                      servers=servers,
                                      domain=domain,
                                      records=records)

        rndc_call = self._rndc_base()

        if new_domain_flag:
            rndc_op = [
                'addzone',
                '%s { type master; file "%s"; };' % (domain['name'],
                                                     output_path),
            ]
            rndc_call.extend(rndc_op)
        else:
            rndc_op = 'reload'
            rndc_call.extend([rndc_op])
            rndc_call.extend([domain['name']])

        LOG.debug('Calling RNDC with: %s' % " ".join(rndc_call))
        utils.execute(*rndc_call)

        nzf_name = glob.glob('%s/*.nzf' % cfg.CONF[self.name].nzf_path)

        output_file = os.path.join(output_folder, 'zones.config')

        shutil.copyfile(nzf_name[0], output_file)

    def _sync_domains_on_server_change(self):
        # TODO(eankutse): Improve this so it scales. Need to design
        # for it in the new Pool Manager/Agent for the backend that is
        # being proposed
        LOG.debug('Synchronising domains on server change')

        domains = self.central_service.find_domains(self.admin_context)
        for domain in domains:
            self._sync_domain(domain)

########NEW FILE########
__FILENAME__ = impl_dynect
# Copyright 2014 Hewlett-Packard Development Company, L.P.
#
# Author: Endre Karlson <endre.karlson@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import json
import time

from eventlet import Timeout
from oslo.config import cfg
import requests
from requests.adapters import HTTPAdapter

from designate import exceptions
from designate.backend import base
from designate.openstack.common import log as logging

LOG = logging.getLogger(__name__)

GROUP = 'backend:dynect'

OPTS = [
    cfg.StrOpt('customer_name', help="Customer name at DynECT."),
    cfg.StrOpt('username', help="Username to auth with DynECT."),
    cfg.StrOpt('password', help="Password to auth with DynECT.", secret=True),
    cfg.ListOpt('masters',
                help="Master servers from which to transfer from."),
    cfg.StrOpt('contact_nickname',
               help="Nickname that will receive notifications."),
    cfg.StrOpt('tsig_key_name', help="TSIG key name."),
    cfg.IntOpt('job_timeout', default=30,
               help="Timeout in seconds for pulling a job in DynECT."),
    cfg.IntOpt('timeout', help="Timeout in seconds for API Requests.",
               default=3),
    cfg.BoolOpt('timings', help="Measure requests timings.", default=False)
]

cfg.CONF.register_group(
    cfg.OptGroup(name=GROUP, title='Backend options for DynECT'))

cfg.CONF.register_opts(OPTS, group=GROUP)


class DynClientError(exceptions.Backend):
    """The base exception class for all HTTP exceptions.
    """
    def __init__(self, data=None, job_id=None, msgs=None,
                 http_status=None, url=None, method=None, details=None):
        self.data = data
        self.job_id = job_id
        self.msgs = msgs

        self.http_status = http_status
        self.url = url
        self.method = method
        self.details = details
        formatted_string = "%s (HTTP %s to %s - %s) - %s" % (self.msgs,
                                                             self.method,
                                                             self.url,
                                                             self.http_status,
                                                             self.details)
        if job_id:
            formatted_string += " (Job-ID: %s)" % job_id
        super(DynClientError, self).__init__(formatted_string)

    @staticmethod
    def from_response(response, details=None):
        data = response.json()

        exc_kwargs = dict(
            data=data['data'],
            job_id=data['job_id'],
            msgs=data['msgs'],
            http_status=response.status_code,
            url=response.url,
            method=response.request.method,
            details=details)

        for msg in data.get('msgs', []):
            if msg['INFO'].startswith('login:'):
                raise DynClientAuthError(**exc_kwargs)
            elif 'Operation blocked' in msg['INFO']:
                raise DynClientOperationBlocked(**exc_kwargs)
        return DynClientError(**exc_kwargs)


class DynClientAuthError(DynClientError):
    pass


class DynTimeoutError(exceptions.Backend):
    """
    A job timedout.
    """
    error_code = 408
    error_type = 'dyn_timeout'


class DynClientOperationBlocked(exceptions.BadRequest, DynClientError):
    error_type = 'operation_blocked'


class DynClient(object):
    """
    DynECT service client.

    https://help.dynect.net/rest/
    """
    def __init__(self, customer_name, user_name, password,
                 endpoint="https://api.dynect.net:443",
                 api_version='3.5.6', headers={}, verify=True, retries=1,
                 timeout=3, timings=False, pool_maxsize=10,
                 pool_connections=10):
        self.customer_name = customer_name
        self.user_name = user_name
        self.password = password
        self.endpoint = endpoint
        self.api_version = api_version

        self.times = []  # [("item", starttime, endtime), ...]
        self.timings = timings
        self.timeout = timeout

        self.authing = False
        self.token = None

        session = requests.Session()
        session.verify = verify

        session.headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json',
            'API-Version': api_version,
            'User-Agent': 'DynECTClient'}
        session.headers.update(headers)

        adapter = HTTPAdapter(max_retries=int(retries),
                              pool_maxsize=int(pool_maxsize),
                              pool_connections=int(pool_connections),
                              pool_block=True)
        session.mount(endpoint, adapter)
        self.http = session

    def _http_log_req(self, method, url, kwargs):
        string_parts = [
            "curl -i",
            "-X '%s'" % method,
            "'%s'" % url,
        ]

        for element in kwargs['headers']:
            header = "-H '%s: %s'" % (element, kwargs['headers'][element])
            string_parts.append(header)

        LOG.debug("REQ: %s" % " ".join(string_parts))
        if 'data' in kwargs:
            LOG.debug("REQ BODY: %s\n" % (kwargs['data']))

    def _http_log_resp(self, resp):
        LOG.debug(
            "RESP: [%s] %s\n",
            resp.status_code,
            resp.headers)
        if resp._content_consumed:
            LOG.debug(
                "RESP BODY: %s\n",
                resp.text)

    def get_timings(self):
        return self.times

    def reset_timings(self):
        self.times = []

    def _request(self, method, url, **kwargs):
        """
        Low level request helper that actually executes the request towards a
        wanted URL.

        This does NOT do any authentication.
        """
        # NOTE: Allow passing the url as just the path or a full url
        if not url.startswith('http'):
            if not url.startswith('/REST'):
                url = '/REST' + url
            url = self.endpoint + url

        kwargs.setdefault("headers", kwargs.get("headers", {}))
        if self.token is not None:
            kwargs['headers']['Auth-Token'] = self.token
        if self.timeout is not None:
            kwargs.setdefault("timeout", self.timeout)

        data = kwargs.get('data')
        if data is not None:
            kwargs['data'] = data.copy()

            # NOTE: We don't want to log the credentials (password) that are
            # used in a auth request.
            if 'password' in kwargs['data']:
                kwargs['data']['password'] = '**SECRET**'

            self._http_log_req(method, url, kwargs)

            # NOTE: Set it back to the original data and serialize it.
            kwargs['data'] = json.dumps(data)
        else:
            self._http_log_req(method, url, kwargs)

        if self.timings:
            start_time = time.time()
        resp = self.http.request(method, url, **kwargs)
        if self.timings:
            self.times.append(("%s %s" % (method, url),
                               start_time, time.time()))
        self._http_log_resp(resp)

        if resp.status_code >= 400:
            LOG.debug(
                "Request returned failure status: %s",
                resp.status_code)
            raise DynClientError.from_response(resp)
        return resp

    def poll_response(self, response):
        """
        The API might return a job nr in the response in case of a async
        response: https://github.com/fog/fog/issues/575
        """
        status = response.status

        timeout = Timeout(cfg.CONF[GROUP].job_timeout)
        try:
            while status == 307:
                time.sleep(1)
                url = response.headers.get('Location')
                LOG.debug("Polling %s" % url)

                polled_response = self.get(url)
                status = response.status
        except Timeout as t:
            if t == timeout:
                raise DynTimeoutError('Timeout reached when pulling job.')
        finally:
            timeout.cancel()
        return polled_response

    def request(self, method, url, retries=2, **kwargs):
        if self.token is None and not self.authing:
            self.login()

        try:
            response = self._request(method, url, **kwargs)
        except DynClientAuthError as e:
            if retries > 0:
                self.token = None
                retries = retries - 1
                return self.request(method, url, retries, **kwargs)
            else:
                raise e

        if response.status_code == 307:
            response = self.poll_response(response)

        return response.json()

    def login(self):
        self.authing = True
        data = {
            'customer_name': self.customer_name,
            'user_name': self.user_name,
            'password': self.password
        }
        response = self.post('/Session', data=data)
        self.token = response['data']['token']
        self.authing = False

    def logout(self):
        self.delete('/Session')
        self.token = None

    def post(self, *args, **kwargs):
        response = self.request('POST', *args, **kwargs)
        return response

    def get(self, *args, **kwargs):
        response = self.request('GET', *args, **kwargs)
        return response

    def put(self, *args, **kwargs):
        response = self.request('PUT', *args, **kwargs)
        return response

    def patch(self, *args, **kwargs):
        response = self.request('PATCH', *args, **kwargs)
        return response

    def delete(self, *args, **kwargs):
        response = self.request('DELETE', *args, **kwargs)
        return response


class DynECTBackend(base.Backend):
    """
    Support for DynECT as a secondary DNS.
    """
    __plugin_name__ = 'dynect'

    def get_client(self):
        return DynClient(
            customer_name=cfg.CONF[GROUP].customer_name,
            user_name=cfg.CONF[GROUP].username,
            password=cfg.CONF[GROUP].password,
            timeout=cfg.CONF[GROUP].timeout,
            timings=cfg.CONF[GROUP].timings)

    def create_domain(self, context, domain):
        LOG.info('Creating domain %s / %s', domain['id'], domain['name'])

        url = '/Secondary/%s' % domain['name'].rstrip('.')
        data = {
            'masters': cfg.CONF[GROUP].masters
        }

        if cfg.CONF[GROUP].contact_nickname is not None:
            data['contact_nickname'] = cfg.CONF[GROUP].contact_nickname

        if cfg.CONF[GROUP].tsig_key_name is not None:
            data['tsig_key_name'] = cfg.CONF[GROUP].tsig_key_name

        client = self.get_client()
        client.post(url, data=data)
        client.put(url, data={'activate': True})
        client.logout()

    def update_domain(self, context, domain):
        LOG.debug('Discarding update_domain call, not-applicable')

    def delete_domain(self, context, domain):
        LOG.info('Deleting domain %s / %s', domain['id'], domain['name'])
        url = '/Zone/%s' % domain['name'].rstrip('.')
        client = self.get_client()
        try:
            client.delete(url)
        except DynClientError as e:
            if e.http_status == 404:
                msg = "Attempt to delete %s / %s caused 404, ignoring."
                LOG.warn(msg, domain['id'], domain['name'])
                pass
            else:
                raise
        client.logout()

    def update_recordset(self, context, domain, recordset):
        LOG.debug('Discarding update_recordset call, not-applicable')

    def delete_recordset(self, context, domain, recordset):
        LOG.debug('Discarding delete_recordset call, not-applicable')

    def create_record(self, context, domain, recordset, record):
        LOG.debug('Discarding create_record call, not-applicable')

    def update_record(self, context, domain, recordset, record):
        LOG.debug('Discarding update_record call, not-applicable')

    def delete_record(self, context, domain, recordset, record):
        LOG.debug('Discarding delete_record call, not-applicable')

    def create_server(self, context, server):
        LOG.debug('Discarding create_server call, not-applicable')

    def update_server(self, context, server):
        LOG.debug('Discarding update_server call, not-applicable')

    def delete_server(self, context, server):
        LOG.debug('Discarding delete_server call, not-applicable')

########NEW FILE########
__FILENAME__ = impl_fake
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.openstack.common import log as logging
from designate.backend import base

LOG = logging.getLogger(__name__)


class FakeBackend(base.Backend):
    __plugin_name__ = 'fake'

    def __init__(self, *args, **kwargs):
        super(FakeBackend, self).__init__(*args, **kwargs)

    def create_tsigkey(self, context, tsigkey):
        LOG.info('Create TSIG Key %r' % tsigkey)

    def update_tsigkey(self, context, tsigkey):
        LOG.info('Update TSIG Key %r' % tsigkey)

    def delete_tsigkey(self, context, tsigkey):
        LOG.info('Delete TSIG Key %r' % tsigkey)

    def create_server(self, context, server):
        LOG.info('Create Server %r' % server)

    def update_server(self, context, server):
        LOG.info('Update Server %r' % server)

    def delete_server(self, context, server):
        LOG.info('Delete Server %r' % server)

    def create_domain(self, context, domain):
        LOG.info('Create Domain %r' % domain)

    def update_domain(self, context, domain):
        LOG.info('Update Domain %r' % domain)

    def delete_domain(self, context, domain):
        LOG.info('Delete Domain %r' % domain)

    def create_recordset(self, context, domain, recordset):
        LOG.info('Create RecordSet %r / %r' % (domain, recordset))

    def update_recordset(self, context, domain, recordset):
        LOG.info('Update RecordSet %r / %r' % (domain, recordset))

    def delete_recordset(self, context, domain, recordset):
        LOG.info('Delete RecordSet %r / %r' % (domain, recordset))

    def create_record(self, context, domain, recordset, record):
        LOG.info('Create Record %r / %r / %r' % (domain, recordset, record))

    def update_record(self, context, domain, recordset, record):
        LOG.info('Update Record %r / %r / %r' % (domain, recordset, record))

    def delete_record(self, context, domain, recordset, record):
        LOG.info('Delete Record %r / %r / %r' % (domain, recordset, record))

    def sync_domain(self, context, domain, records):
        LOG.info('Sync Domain %r / %r' % (domain, records))

    def sync_record(self, context, domain, record):
        LOG.info('Sync Record %r / %r' % (domain, record))

    def ping(self, context):
        LOG.info('Ping')

########NEW FILE########
__FILENAME__ = auth
# Copyright 2014 Red Hat, Inc.
#
# Author: Rich Megginson <rmeggins@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import os
import logging
import uuid
from requests import auth
import kerberos
from designate.backend.impl_ipa import IPAAuthError

LOG = logging.getLogger(__name__)


class IPAAuth(auth.AuthBase):
    def __init__(self, keytab, hostname):
        # store the kerberos credentials in memory rather than on disk
        os.environ['KRB5CCNAME'] = "MEMORY:" + str(uuid.uuid4())
        self.token = None
        self.keytab = keytab
        self.hostname = hostname
        if self.keytab:
            os.environ['KRB5_CLIENT_KTNAME'] = self.keytab
        else:
            LOG.warn('No IPA client kerberos keytab file given')

    def __call__(self, request):
        if not self.token:
            self.refresh_auth()
        request.headers['Authorization'] = 'negotiate ' + self.token
        return request

    def refresh_auth(self):
        service = "HTTP@" + self.hostname
        flags = kerberos.GSS_C_MUTUAL_FLAG | kerberos.GSS_C_SEQUENCE_FLAG
        try:
            (_, vc) = kerberos.authGSSClientInit(service, flags)
        except kerberos.GSSError as e:
            LOG.error("caught kerberos exception %r" % e)
            raise IPAAuthError(str(e))
        try:
            kerberos.authGSSClientStep(vc, "")
        except kerberos.GSSError as e:
            LOG.error("caught kerberos exception %r" % e)
            raise IPAAuthError(str(e))
        self.token = kerberos.authGSSClientResponse(vc)

########NEW FILE########
__FILENAME__ = impl_multi
# Copyright (C) 2013 eNovance SAS <licensing@enovance.com>
#
# Author: Artom Lifshitz <artom.lifshitz@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from designate import backend
from designate import exceptions
from designate.backend import base
from designate.openstack.common import excutils
from oslo.config import cfg
import logging

LOG = logging.getLogger(__name__)

CFG_GRP = 'backend:multi'

cfg.CONF.register_group(cfg.OptGroup(
    name=CFG_GRP, title="Configuration for multi-backend Backend"
))

cfg.CONF.register_opts([
    cfg.StrOpt('master', default='fake', help='Master backend'),
    cfg.StrOpt('slave', default='fake', help='Slave backend'),
], group=CFG_GRP)


class MultiBackend(base.Backend):
    """
    Multi-backend backend

    This backend dispatches calls to a master backend and a slave backend.
    It enforces master/slave ordering semantics as follows:

    Creates for tsigkeys, servers and domains are done on the master first,
    then on the slave.

    Updates for tsigkeys, servers and domains and all operations on records
    are done on the master only. It's assumed masters and slaves use an
    external mechanism to sync existing domains, most likely XFR.

    Deletes are done on the slave first, then on the master.

    If the create on the slave fails, the domain/tsigkey/server is deleted from
    the master. If delete on the master fails, the domain/tdigkey/server is
    recreated on the slave.
    """
    __plugin_name__ = 'multi'

    def __init__(self, central_service):
        super(MultiBackend, self).__init__(central_service)
        self.central = central_service
        self.master = backend.get_backend(cfg.CONF[CFG_GRP].master,
                                          central_service)
        self.slave = backend.get_backend(cfg.CONF[CFG_GRP].slave,
                                         central_service)

    def start(self):
        self.master.start()
        self.slave.start()

    def stop(self):
        self.slave.stop()
        self.master.stop()

    def create_tsigkey(self, context, tsigkey):
        self.master.create_tsigkey(context, tsigkey)
        try:
            self.slave.create_tsigkey(context, tsigkey)
        except (exceptions.Base, exceptions.Backend):
            with excutils.save_and_reraise_exception():
                self.master.delete_tsigkey(context, tsigkey)

    def update_tsigkey(self, context, tsigkey):
        self.master.update_tsigkey(context, tsigkey)

    def delete_tsigkey(self, context, tsigkey):
        self.slave.delete_tsigkey(context, tsigkey)
        try:
            self.master.delete_tsigkey(context, tsigkey)
        except (exceptions.Base, exceptions.Backend):
            with excutils.save_and_reraise_exception():
                self.slave.create_tsigkey(context, tsigkey)

    def create_domain(self, context, domain):
        self.master.create_domain(context, domain)
        try:
            self.slave.create_domain(context, domain)
        except (exceptions.Base, exceptions.Backend):
            with excutils.save_and_reraise_exception():
                self.master.delete_domain(context, domain)

    def update_domain(self, context, domain):
        self.master.update_domain(context, domain)

    def delete_domain(self, context, domain):
        # Get the "full" domain (including id) from Central first, as we may
        # have to recreate it on slave if delete on master fails
        full_domain = self.central.find_domain(
            context, {'name': domain['name']})

        self.slave.delete_domain(context, domain)
        try:
            self.master.delete_domain(context, domain)
        except (exceptions.Base, exceptions.Backend):
            with excutils.save_and_reraise_exception():
                self.slave.create_domain(context, domain)

                [self.slave.create_record(context, domain, record)
                 for record in self.central.find_records(
                     context, {'domain_id': full_domain['id']})]

    def create_server(self, context, server):
        self.master.create_server(context, server)
        try:
            self.slave.create_server(context, server)
        except (exceptions.Base, exceptions.Backend):
            with excutils.save_and_reraise_exception():
                self.master.delete_server(context, server)

    def update_server(self, context, server):
        self.master.update_server(context, server)

    def delete_server(self, context, server):
        self.slave.delete_server(context, server)
        try:
            self.master.delete_server(context, server)
        except (exceptions.Base, exceptions.Backend):
            with excutils.save_and_reraise_exception():
                self.slave.create_server(context, server)

    def create_recordset(self, context, domain, recordset):
        self.master.create_recordset(context, domain, recordset)

    def update_recordset(self, context, domain, recordset):
        self.master.update_recordset(context, domain, recordset)

    def delete_recordset(self, context, domain, recordset):
        self.master.delete_recordset(context, domain, recordset)

    def create_record(self, context, domain, recordset, record):
        self.master.create_record(context, domain, recordset, record)

    def update_record(self, context, domain, recordset, record):
        self.master.update_record(context, domain, recordset, record)

    def delete_record(self, context, domain, recordset, record):
        self.master.delete_record(context, domain, recordset, record)

    def ping(self, context):
        return {
            'master': self.master.ping(context),
            'slave': self.slave.ping(context)
        }

########NEW FILE########
__FILENAME__ = impl_nsd4slave
# Copyright (C) 2013 eNovance SAS <licensing@enovance.com>
#
# Author: Artom Lifshitz <artom.lifshitz@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import eventlet
import os
import socket
import ssl
from designate import exceptions
from designate.backend import base
from designate.openstack.common import log as logging
from oslo.config import cfg

LOG = logging.getLogger(__name__)

CFG_GRP = 'backend:nsd4slave'

cfg.CONF.register_group(
    cfg.OptGroup(name=CFG_GRP, title='Configuration for NSD4-slave backend')
)

cfg.CONF.register_opts([
    cfg.StrOpt('keyfile', default='/etc/nsd/nsd_control.key',
               help='Keyfile to use when connecting to the NSD4 servers over '
                    'SSL'),
    cfg.StrOpt('certfile', default='/etc/nsd/nsd_control.pem',
               help='Certfile to use when connecting to the NSD4 servers over '
                    'SSL'),
    cfg.ListOpt('servers',
                help='Comma-separated list of servers to control, in '
                     ' <host>:<port> format. If <port> is omitted, '
                     ' the default 8952 is used.'),
    cfg.StrOpt('pattern', default='slave',
               help='Pattern to use when creating zones on the NSD4 servers. '
                    'This pattern must be identically configured on all NSD4 '
                    'servers.'),
], group=CFG_GRP)

DEFAULT_PORT = 8952


class NSD4SlaveBackend(base.Backend):
    __plugin__name__ = 'nsd4slave'
    NSDCT_VERSION = 'NSDCT1'

    def __init__(self, central_service):
        self._keyfile = cfg.CONF[CFG_GRP].keyfile
        self._certfile = cfg.CONF[CFG_GRP].certfile
        # Make sure keyfile and certfile are readable to avoid cryptic SSL
        # errors later
        if not os.access(self._keyfile, os.R_OK):
            raise exceptions.NSD4SlaveBackendError(
                'Keyfile %s missing or permission denied' % self._keyfile)
        if not os.access(self._certfile, os.R_OK):
            raise exceptions.NSD4SlaveBackendError(
                'Certfile %s missing or permission denied' % self._certfile)
        self._pattern = cfg.CONF[CFG_GRP].pattern
        try:
            self._servers = [self._parse_server(cfg_server)
                             for cfg_server in cfg.CONF[CFG_GRP].servers]
        except TypeError:
            raise exceptions.ConfigurationError('No NSD4 servers defined')

    def _parse_server(self, cfg_server):
        try:
            (host, port) = cfg_server.split(':')
            port = int(port)
        except ValueError:
            host = str(cfg_server)
            port = DEFAULT_PORT
        return {'host': host, 'port': port}

    def create_domain(self, context, domain):
        command = 'addzone %s %s' % (domain['name'], self._pattern)
        self._all_servers(command)

    def update_domain(self, context, domain):
        pass

    def delete_domain(self, context, domain):
        command = 'delzone %s' % domain['name']
        self._all_servers(command)

    def _all_servers(self, command):
        for server in self._servers:
            try:
                result = self._command(command, server['host'], server['port'])
            except (ssl.SSLError, socket.error) as e:
                raise exceptions.NSD4SlaveBackendError(e)
            if result != 'ok':
                raise exceptions.NSD4SlaveBackendError(result)

    def _command(self, command, host, port):
        sock = eventlet.wrap_ssl(eventlet.connect((host, port)),
                                 keyfile=self._keyfile,
                                 certfile=self._certfile)
        stream = sock.makefile()
        stream.write('%s %s\n' % (self.NSDCT_VERSION, command))
        stream.flush()
        result = stream.read()
        stream.close()
        sock.close()
        return result.rstrip()

    def update_recordset(self, context, domain, recordset):
        pass

    def delete_recordset(self, context, domain, recordset):
        pass

    def create_record(self, context, domain, recordset, record):
        pass

    def update_record(self, context, domain, recordset, record):
        pass

    def delete_record(self, context, domain, recordset, record):
        pass

    def create_server(self, context, server):
        pass

    def update_server(self, context, server):
        pass

    def delete_server(self, context, server):
        pass

########NEW FILE########
__FILENAME__ = manage
#!/usr/bin/env python
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2012 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
# Author: Patrick Galbraith <patg@hp.com>
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from migrate.versioning.shell import main

if __name__ == '__main__':
    main(debug='False')

########NEW FILE########
__FILENAME__ = 001_add_initial_schema
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2012 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
# Author: Patrick Galbraith <patg@hp.com>
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from sqlalchemy import Integer, String, Text, Boolean
from sqlalchemy.schema import Table, Column, MetaData, Index

meta = MetaData()

domains = Table('domains', meta,
                Column('id', Integer(), autoincrement=True,
                       primary_key=True, nullable=False),
                Column('name', String(255), nullable=False, unique=True),
                Column('master', String(20), default=None, nullable=True),
                Column('last_check', Integer(), default=None,
                       nullable=True),
                Column('type', String(6), nullable=False),
                Column('notified_serial', Integer(), default=None,
                       nullable=True),
                Column('account', String(40), default=None, nullable=True))

records = Table('records', meta,
                Column('id', Integer(), autoincrement=True,
                       primary_key=True, nullable=False),
                Column('domain_id', Integer(), default=None, nullable=True),
                Column('name', String(255), default=None, nullable=True),
                Column('type', String(10), default=None, nullable=True),
                Column('content', String(255), default=None, nullable=True),
                Column('ttl', Integer(), default=None, nullable=True),
                Column('prio', Integer(), default=None, nullable=True),
                Column('change_date', Integer(), default=None,
                       nullable=True),
                Column('ordername', String(255), default=None, nullable=True),
                Column('auth', Boolean(), default=None, nullable=True))

Index('rec_name_index', records.c.name)
Index('nametype_index', records.c.name, records.c.type)
Index('domain_id', records.c.domain_id)
Index('orderindex', records.c.ordername)

cryptokeys = Table('cryptokeys', meta,
                   Column('id', Integer(), autoincrement=True,
                          primary_key=True, nullable=False),
                   Column('domain_id', Integer(), nullable=False),
                   Column('flags', Integer(), nullable=False),
                   Column('active', Boolean(), default=None, nullable=True),
                   Column('content', Text()))

domainmetadata = Table('domainmetadata', meta,
                       Column('id', Integer(), autoincrement=True,
                              primary_key=True, nullable=False),
                       Column('domain_id', Integer(), nullable=False),
                       Column('kind', String(16), default=None, nullable=True),
                       Column('content', Text()))

supermasters = Table('supermasters', meta,
                     Column('ip', String(25), nullable=False),
                     Column('nameserver', String(255), nullable=False),
                     Column('account', String(40), default=None,
                            nullable=True))

tsigkeys = Table('tsigkeys', meta,
                 Column('id', Integer(), autoincrement=True,
                        primary_key=True, nullable=False),
                 Column('name', String(255), default=None, nullable=True),
                 Column('algorithm', String(255), default=None, nullable=True),
                 Column('secret', String(255), default=None, nullable=True))

Index('namealgoindex', tsigkeys.c.name, tsigkeys.c.algorithm, unique=True)


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    domains.create()
    records.create()
    cryptokeys.create()
    domainmetadata.create()
    supermasters.create()
    tsigkeys.create()


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    tsigkeys.drop()
    supermasters.drop()
    domainmetadata.drop()
    cryptokeys.drop()
    records.drop()
    domains.drop()

########NEW FILE########
__FILENAME__ = 002_add_moniker_id_cols
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import MetaData, Table, Column
from designate.sqlalchemy.types import UUID

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    tsigkeys_table = Table('tsigkeys', meta, autoload=True)
    domains_table = Table('domains', meta, autoload=True)
    records_table = Table('records', meta, autoload=True)

    tsigkeys_moniker_id = Column('moniker_id', UUID())
    tsigkeys_moniker_id.create(tsigkeys_table)

    domains_moniker_id = Column('moniker_id', UUID())
    domains_moniker_id.create(domains_table)

    records_moniker_id = Column('moniker_id', UUID())
    records_moniker_id.create(records_table)


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    tsigkeys_table = Table('tsigkeys', meta, autoload=True)
    domains_table = Table('domains', meta, autoload=True)
    records_table = Table('records', meta, autoload=True)

    tsigkeys_moniker_id = Column('moniker_id', UUID())
    tsigkeys_moniker_id.drop(tsigkeys_table)

    domains_moniker_id = Column('moniker_id', UUID())
    domains_moniker_id.drop(domains_table)

    records_moniker_id = Column('moniker_id', UUID())
    records_moniker_id.drop(records_table)

########NEW FILE########
__FILENAME__ = 003_correct_master_column_length
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import MetaData, Table, String

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    domains_table = Table('domains', meta, autoload=True)
    domains_table.c.master.alter(type=String(255))


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    domains_table = Table('domains', meta, autoload=True)
    domains_table.c.master.alter(type=String(20))

########NEW FILE########
__FILENAME__ = 004_correct_content_column_length
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import MetaData, Table, String, Text

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    records_table = Table('records', meta, autoload=True)
    records_table.c.content.alter(type=Text)


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    records_table = Table('records', meta, autoload=True)
    records_table.c.content.alter(type=String(255))

########NEW FILE########
__FILENAME__ = 005_rename_moniker_id_columns
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import MetaData, Table

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    tsigkeys_table = Table('tsigkeys', meta, autoload=True)
    domains_table = Table('domains', meta, autoload=True)
    records_table = Table('records', meta, autoload=True)

    tsigkeys_table.c.moniker_id.alter(name='designate_id')
    domains_table.c.moniker_id.alter(name='designate_id')
    records_table.c.moniker_id.alter(name='designate_id')


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    tsigkeys_table = Table('tsigkeys', meta, autoload=True)
    domains_table = Table('domains', meta, autoload=True)
    records_table = Table('records', meta, autoload=True)

    tsigkeys_table.c.designate_id.alter(name='moniker_id')
    domains_table.c.designate_id.alter(name='moniker_id')
    records_table.c.designate_id.alter(name='moniker_id')

########NEW FILE########
__FILENAME__ = 006_add_inherit_ttl_col
# flake8: noqa
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import MetaData, Table, Column, Boolean
from designate.openstack.common import log as logging

LOG = logging.getLogger(__name__)
meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    records_table = Table('records', meta, autoload=True)

    # Create the new inherit_ttl column
    inherit_ttl = Column('inherit_ttl', Boolean(), default=True)
    inherit_ttl.create(records_table)

    # Semi-Populate the new inherit_ttl column. We'll need to do a cross-db
    # join from powerdns.records -> powerdns.domains -> designate.domains, so
    # we can't perform the second half here.
    query = records_table.update().values(inherit_ttl=False)
    query = query.where(records_table.c.ttl != None)
    query.execute()

    # If there are records without an explicity configured TTL, we'll need
    # a manual post-migration step.
    query = records_table.select()
    query = query.where(records_table.c.ttl == None)
    c = query.count()

    if c > 0:
        pmq = ('UPDATE powerdns.records JOIN powerdns.domains ON powerdns.reco'
               'rds.domain_id = powerdns.domains.id JOIN designate.domains ON '
               'powerdns.domains.designate_id = designate.domains.id SET power'
               'dns.records.ttl = designate.domains.ttl WHERE powerdns.records'
               '.inherit_ttl = 1;')

        LOG.warning('**** A manual post-migration step is required ****')
        LOG.warning('Please issue this query: %s' % pmq)


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    records_table = Table('records', meta, autoload=True)
    records_table.c.inherit_ttl.drop()

########NEW FILE########
__FILENAME__ = 007_add_recordset_id_col
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import MetaData, Table, Column
from designate.sqlalchemy.types import UUID

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    records_table = Table('records', meta, autoload=True)

    recordset_id = Column('designate_recordset_id', UUID())
    recordset_id.create(records_table)


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    records_table = Table('records', meta, autoload=True)

    records_table.c.designate_recordset_id.drop()

########NEW FILE########
__FILENAME__ = models
# Copyright 2012 Hewlett-Packard Development Company, L.P. All Rights Reserved.
# Copyright 2012 Managed I.T.
#
# Author: Patrick Galbraith <patg@hp.com>
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import Column, String, Text, Integer, Boolean
from sqlalchemy.ext.declarative import declarative_base
from designate.sqlalchemy.models import Base as CommonBase
from designate.sqlalchemy.types import UUID


class Base(CommonBase):
    id = Column(Integer, primary_key=True, autoincrement=True)


Base = declarative_base(cls=Base)


class TsigKey(Base):
    __tablename__ = 'tsigkeys'

    designate_id = Column(UUID, nullable=False)

    name = Column(String(255), default=None, nullable=True)
    algorithm = Column(String(255), default=None, nullable=True)
    secret = Column(String(255), default=None, nullable=True)


class DomainMetadata(Base):
    __tablename__ = 'domainmetadata'

    domain_id = Column(Integer(), nullable=False)
    kind = Column(String(16), default=None, nullable=True)
    content = Column(Text())


class Domain(Base):
    __tablename__ = 'domains'

    designate_id = Column(UUID, nullable=False)

    name = Column(String(255), nullable=False, unique=True)
    master = Column(String(255), nullable=True)
    last_check = Column(Integer, default=None, nullable=True)
    type = Column(String(6), nullable=False)
    notified_serial = Column(Integer, default=None, nullable=True)
    account = Column(String(40), default=None, nullable=True)


class Record(Base):
    __tablename__ = 'records'

    designate_id = Column(UUID, nullable=False)
    designate_recordset_id = Column(UUID, default=None, nullable=True)

    domain_id = Column(Integer, default=None, nullable=True)
    name = Column(String(255), default=None, nullable=True)
    type = Column(String(10), default=None, nullable=True)
    content = Column(Text, default=None, nullable=True)
    ttl = Column(Integer, default=None, nullable=True)
    prio = Column(Integer, default=None, nullable=True)
    change_date = Column(Integer, default=None, nullable=True)
    ordername = Column(String(255), default=None, nullable=True)
    auth = Column(Boolean(), default=None, nullable=True)
    inherit_ttl = Column(Boolean(), default=True)

########NEW FILE########
__FILENAME__ = impl_rpc
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.backend import base
from designate.agent import rpcapi as agent_rpcapi


class RPCBackend(base.Backend):
    def __init__(self, central_service):
        super(RPCBackend, self).__init__(central_service)
        self.agent_api = agent_rpcapi.AgentAPI()

    def create_tsigkey(self, context, tsigkey):
        return self.agent_api.create_tsigkey(context, tsigkey)

    def update_tsigkey(self, context, tsigkey):
        return self.agent_api.update_tsigkey(context, tsigkey)

    def delete_tsigkey(self, context, tsigkey):
        return self.agent_api.delete_tsigkey(context, tsigkey)

    def create_server(self, context, server):
        return self.agent_api.create_server(context, server)

    def update_server(self, context, server):
        return self.agent_api.update_server(context, server)

    def delete_server(self, context, server):
        return self.agent_api.delete_server(context, server)

    def create_domain(self, context, domain):
        return self.agent_api.create_domain(context, domain)

    def update_domain(self, context, domain):
        return self.agent_api.update_domain(context, domain)

    def delete_domain(self, context, domain):
        return self.agent_api.delete_domain(context, domain)

    def update_recordset(self, context, domain, recordset):
        return self.agent_api.update_recordset(context, domain, recordset)

    def delete_recordset(self, context, domain, recordset):
        return self.agent_api.delete_recordset(context, domain, recordset)

    def create_record(self, context, domain, recordset, record):
        return self.agent_api.create_record(context, domain, recordset, record)

    def update_record(self, context, domain, recordset, record):
        return self.agent_api.update_record(context, domain, recordset, record)

    def delete_record(self, context, domain, recordset, record):
        return self.agent_api.delete_record(context, domain, recordset, record)

    def sync_domain(self, context, domain, records):
        return self.agent_api.sync_domain(context, domain, records)

    def sync_record(self, context, domain, record):
        return self.agent_api.sync_record(context, domain, record)

########NEW FILE########
__FILENAME__ = rpcapi
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from oslo.config import cfg
from oslo import messaging

from designate.openstack.common import log as logging
from designate import rpc


LOG = logging.getLogger(__name__)


class CentralAPI(object):
    """
    Client side of the central RPC API.

    API version history:

        1.0 - Initial version
        1.1 - Add new finder methods
        1.2 - Add get_tenant and get_tenants
        1.3 - Add get_absolute_limits
        2.0 - Renamed most get_resources to find_resources
        2.1 - Add quota methods
        3.0 - RecordSet Changes
        3.1 - Add floating ip ptr methods
        3.2 - TLD Api changes
        3.3 - Add methods for blacklisted domains
    """
    RPC_API_VERSION = '3.0'

    def __init__(self, topic=None):
        topic = topic if topic else cfg.CONF.central_topic

        target = messaging.Target(topic=topic, version=self.RPC_API_VERSION)
        self.client = rpc.get_client(target, version_cap='3.3')

    # Misc Methods
    def get_absolute_limits(self, context):
        LOG.info("get_absolute_limits: Calling central's get_absolute_limits.")

        return self.client.call(context, 'get_absolute_limits')

    # Quota Methods
    def get_quotas(self, context, tenant_id):
        LOG.info("get_quotas: Calling central's get_quotas.")

        return self.client.call(context, 'get_quotas', tenant_id=tenant_id)

    def get_quota(self, context, tenant_id, resource):
        LOG.info("get_quota: Calling central's get_quota.")

        return self.client.call(context, 'get_quota', tenant_id=tenant_id,
                                resource=resource)

    def set_quota(self, context, tenant_id, resource, hard_limit):
        LOG.info("set_quota: Calling central's set_quota.")

        return self.client.call(context, 'set_quota', tenant_id=tenant_id,
                                resource=resource, hard_limit=hard_limit)

    def reset_quotas(self, context, tenant_id):
        LOG.info("reset_quotas: Calling central's reset_quotas.")

        return self.client.call(context, 'reset_quotas', tenant_id=tenant_id)

    # Server Methods
    def create_server(self, context, values):
        LOG.info("create_server: Calling central's create_server.")

        return self.client.call(context, 'create_server', values=values)

    def find_servers(self, context, criterion=None, marker=None, limit=None,
                     sort_key=None, sort_dir=None):
        LOG.info("find_servers: Calling central's find_servers.")

        return self.client.call(context, 'find_servers', criterion=criterion,
                                marker=marker, limit=limit, sort_key=sort_key,
                                sort_dir=sort_dir)

    def get_server(self, context, server_id):
        LOG.info("get_server: Calling central's get_server.")

        return self.client.call(context, 'get_server', server_id=server_id)

    def update_server(self, context, server_id, values):
        LOG.info("update_server: Calling central's update_server.")

        return self.client.call(context, 'update_server', server_id=server_id,
                                values=values)

    def delete_server(self, context, server_id):
        LOG.info("delete_server: Calling central's delete_server.")

        return self.client.call(context, 'delete_server', server_id=server_id)

    # TSIG Key Methods
    def create_tsigkey(self, context, values):
        LOG.info("create_tsigkey: Calling central's create_tsigkey.")

        return self.client.call(context, 'create_tsigkey', values=values)

    def find_tsigkeys(self, context, criterion=None, marker=None, limit=None,
                      sort_key=None, sort_dir=None):
        LOG.info("find_tsigkeys: Calling central's find_tsigkeys.")

        return self.client.call(context, 'find_tsigkeys', criterion=criterion,
                                marker=marker, limit=limit, sort_key=sort_key,
                                sort_dir=sort_dir)

    def get_tsigkey(self, context, tsigkey_id):
        LOG.info("get_tsigkey: Calling central's get_tsigkey.")
        return self.client.call(context, 'get_tsigkey', tsigkey_id=tsigkey_id)

    def update_tsigkey(self, context, tsigkey_id, values):
        LOG.info("update_tsigkey: Calling central's update_tsigkey.")
        return self.client.call(context, 'update_tsigkey',
                                tsigkey_id=tsigkey_id,
                                values=values)

    def delete_tsigkey(self, context, tsigkey_id):
        LOG.info("delete_tsigkey: Calling central's delete_tsigkey.")
        return self.client.call(context, 'delete_tsigkey',
                                tsigkey_id=tsigkey_id)

    # Tenant Methods
    def find_tenants(self, context):
        LOG.info("find_tenants: Calling central's find_tenants.")
        return self.client.call(context, 'find_tenants')

    def get_tenant(self, context, tenant_id):
        LOG.info("get_tenant: Calling central's get_tenant.")
        return self.client.call(context, 'get_tenant', tenant_id=tenant_id)

    def count_tenants(self, context):
        LOG.info("count_tenants: Calling central's count_tenants.")
        return self.client.call(context, 'count_tenants')

    # Domain Methods
    def create_domain(self, context, values):
        LOG.info("create_domain: Calling central's create_domain.")
        return self.client.call(context, 'create_domain', values=values)

    def get_domain(self, context, domain_id):
        LOG.info("get_domain: Calling central's get_domain.")
        return self.client.call(context, 'get_domain', domain_id=domain_id)

    def get_domain_servers(self, context, domain_id):
        LOG.info("get_domain_servers: Calling central's get_domain_servers.")
        return self.client.call(context, 'get_domain_servers',
                                domain_id=domain_id)

    def find_domains(self, context, criterion=None, marker=None, limit=None,
                     sort_key=None, sort_dir=None):
        LOG.info("find_domains: Calling central's find_domains.")
        return self.client.call(context, 'find_domains', criterion=criterion,
                                marker=marker, limit=limit, sort_key=sort_key,
                                sort_dir=sort_dir)

    def find_domain(self, context, criterion=None):
        LOG.info("find_domain: Calling central's find_domain.")
        return self.client.call(context, 'find_domain', criterion=criterion)

    def update_domain(self, context, domain_id, values, increment_serial=True):
        LOG.info("update_domain: Calling central's update_domain.")
        return self.client.call(
            context, 'update_domain', domain_id=domain_id,
            values=values, increment_serial=increment_serial)

    def delete_domain(self, context, domain_id):
        LOG.info("delete_domain: Calling central's delete_domain.")
        return self.client.call(context, 'delete_domain', domain_id=domain_id)

    def count_domains(self, context, criterion=None):
        LOG.info("count_domains: Calling central's count_domains.")
        return self.client.call(context, 'count_domains', criterion=criterion)

    def touch_domain(self, context, domain_id):
        LOG.info("touch_domain: Calling central's touch_domain.")
        return self.client.call(context, 'touch_domain', domain_id=domain_id)

    # TLD Methods
    def create_tld(self, context, values):
        LOG.info("create_tld: Calling central's create_tld.")
        cctxt = self.client.prepare(version='3.2')
        return cctxt.call(context, 'create_tld', values=values)

    def find_tlds(self, context, criterion=None, marker=None, limit=None,
                  sort_key=None, sort_dir=None):
        LOG.info("find_tlds: Calling central's find_tlds.")

        cctxt = self.client.prepare(version='3.2')
        return cctxt.call(context, 'find_tlds', criterion=criterion,
                          marker=marker, limit=limit, sort_key=sort_key,
                          sort_dir=sort_dir)

    def get_tld(self, context, tld_id):
        LOG.info("get_tld: Calling central's get_tld.")

        cctxt = self.client.prepare(version='3.2')
        return cctxt.call(context, 'get_tld', tld_id=tld_id)

    def update_tld(self, context, tld_id, values):
        LOG.info("update_tld: Calling central's update_tld.")

        cctxt = self.client.prepare(version='3.2')
        return cctxt.call(context, 'update_tld', tld_id=tld_id, values=values)

    def delete_tld(self, context, tld_id):
        LOG.info("delete_tld: Calling central's delete_tld.")

        cctxt = self.client.prepare(version='3.2')
        return cctxt.call(context, 'delete_tld', tld_id=tld_id)

    # RecordSet Methods
    def create_recordset(self, context, domain_id, values):
        LOG.info("create_recordset: Calling central's create_recordset.")
        return self.client.call(context, 'create_recordset',
                                domain_id=domain_id, values=values)

    def get_recordset(self, context, domain_id, recordset_id):
        LOG.info("get_recordset: Calling central's get_recordset.")
        return self.client.call(context, 'get_recordset', domain_id=domain_id,
                                recordset_id=recordset_id)

    def find_recordsets(self, context, criterion=None, marker=None, limit=None,
                        sort_key=None, sort_dir=None):
        LOG.info("find_recordsets: Calling central's find_recordsets.")
        return self.client.call(context, 'find_recordsets',
                                criterion=criterion, marker=marker,
                                limit=limit, sort_key=sort_key,
                                sort_dir=sort_dir)

    def find_recordset(self, context, criterion=None):
        LOG.info("find_recordset: Calling central's find_recordset.")
        return self.client.call(context, 'find_recordset', criterion=criterion)

    def update_recordset(self, context, domain_id, recordset_id, values,
                         increment_serial=True):
        LOG.info("update_recordset: Calling central's update_recordset.")
        return self.client.call(context, 'update_recordset',
                                domain_id=domain_id,
                                recordset_id=recordset_id,
                                values=values,
                                increment_serial=increment_serial)

    def delete_recordset(self, context, domain_id, recordset_id,
                         increment_serial=True):
        LOG.info("delete_recordset: Calling central's delete_recordset.")
        return self.client.call(context, 'delete_recordset',
                                domain_id=domain_id,
                                recordset_id=recordset_id,
                                increment_serial=increment_serial)

    def count_recordsets(self, context, criterion=None):
        LOG.info("count_recordsets: Calling central's count_recordsets.")
        return self.client.call(context, 'count_recordsets',
                                criterion=criterion)

    # Record Methods
    def create_record(self, context, domain_id, recordset_id, values,
                      increment_serial=True):
        LOG.info("create_record: Calling central's create_record.")
        return self.client.call(context, 'create_record',
                                domain_id=domain_id,
                                recordset_id=recordset_id,
                                values=values,
                                increment_serial=increment_serial)

    def get_record(self, context, domain_id, recordset_id, record_id):
        LOG.info("get_record: Calling central's get_record.")
        return self.client.call(context, 'get_record',
                                domain_id=domain_id,
                                recordset_id=recordset_id,
                                record_id=record_id)

    def find_records(self, context, criterion=None, marker=None, limit=None,
                     sort_key=None, sort_dir=None):
        LOG.info("find_records: Calling central's find_records.")
        return self.client.call(context, 'find_records', criterion=criterion,
                                marker=marker, limit=limit, sort_key=sort_key,
                                sort_dir=sort_dir)

    def find_record(self, context, criterion=None):
        LOG.info("find_record: Calling central's find_record.")
        return self.client.call(context, 'find_record', criterion=criterion)

    def update_record(self, context, domain_id, recordset_id, record_id,
                      values, increment_serial=True):
        LOG.info("update_record: Calling central's update_record.")
        return self.client.call(context, 'update_record',
                                domain_id=domain_id,
                                recordset_id=recordset_id,
                                record_id=record_id,
                                values=values,
                                increment_serial=increment_serial)

    def delete_record(self, context, domain_id, recordset_id, record_id,
                      increment_serial=True):
        LOG.info("delete_record: Calling central's delete_record.")
        return self.client.call(context, 'delete_record',
                                domain_id=domain_id,
                                recordset_id=recordset_id,
                                record_id=record_id,
                                increment_serial=increment_serial)

    def count_records(self, context, criterion=None):
        LOG.info("count_records: Calling central's count_records.")
        return self.client.call(context, 'count_records', criterion=criterion)

    # Sync Methods
    def sync_domains(self, context):
        LOG.info("sync_domains: Calling central's sync_domains.")
        return self.client.call(context, 'sync_domains')

    def sync_domain(self, context, domain_id):
        LOG.info("sync_domain: Calling central's sync_domains.")
        return self.client.call(context, 'sync_domain', domain_id=domain_id)

    def sync_record(self, context, domain_id, recordset_id, record_id):
        LOG.info("sync_record: Calling central's sync_record.")
        return self.client.call(context, 'sync_record',
                                domain_id=domain_id,
                                recordset_id=recordset_id,
                                record_id=record_id)

    def list_floatingips(self, context):
        LOG.info("list_floatingips: Calling central's list_floatingips.")

        cctxt = self.client.prepare(version='3.1')
        return cctxt.call(context, 'list_floatingips')

    def get_floatingip(self, context, region, floatingip_id):
        LOG.info("get_floatingip: Calling central's get_floatingip.")

        cctxt = self.client.prepare(version='3.1')
        return cctxt.call(context, 'get_floatingip', region=region,
                          floatingip_id=floatingip_id)

    def update_floatingip(self, context, region, floatingip_id, values):
        LOG.info("update_floatingip: Calling central's update_floatingip.")

        cctxt = self.client.prepare(version='3.1')
        return cctxt.call(context, 'update_floatingip', region=region,
                          floatingip_id=floatingip_id, values=values)

    # Blacklisted Domain Methods
    def create_blacklist(self, context, values):
        LOG.info("create_blacklist: Calling central's create_blacklist")

        cctxt = self.client.prepare(version='3.3')
        return cctxt.call(context, 'create_blacklist', values=values)

    def get_blacklist(self, context, blacklist_id):
        LOG.info("get_blacklist: Calling central's get_blacklist.")

        cctxt = self.client.prepare(version='3.3')
        return cctxt.call(context, 'get_blacklist', blacklist_id=blacklist_id)

    def find_blacklists(self, context, criterion=None, marker=None, limit=None,
                        sort_key=None, sort_dir=None):
        LOG.info("find_blacklists: Calling central's find_blacklists.")

        cctxt = self.client.prepare(version='3.3')
        return cctxt.call(context, 'find_blacklists', criterion=criterion,
                          marker=marker, limit=limit, sort_key=sort_key,
                          sort_dir=sort_dir)

    def find_blacklist(self, context, criterion):
        LOG.info("find_blacklist: Calling central's find_blacklist.")

        cctxt = self.client.prepare(version='3.3')
        return cctxt.call(context, 'find_blacklist', criterion=criterion)

    def update_blacklist(self, context, blacklist_id, values):
        LOG.info("update_blacklist: Calling central's update_blacklist.")

        cctxt = self.client.prepare(version='3.3')
        return cctxt.call(context, 'update_blacklist',
                          blacklist_id=blacklist_id, values=values)

    def delete_blacklist(self, context, blacklist_id):
        LOG.info("delete_blacklist: Calling central's delete blacklist.")

        cctxt = self.client.prepare(version='3.3')
        return cctxt.call(context, 'delete_blacklist',
                          blacklist_id=blacklist_id)

########NEW FILE########
__FILENAME__ = service
# Copyright 2012 Managed I.T.
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import re
import contextlib

from oslo.config import cfg
from oslo import messaging

from designate.openstack.common import log as logging
from designate import backend
from designate import exceptions
from designate import network_api
from designate import policy
from designate import quota
from designate import service
from designate import utils
from designate.storage import api as storage_api

LOG = logging.getLogger(__name__)


@contextlib.contextmanager
def wrap_backend_call():
    """
    Wraps backend calls, ensuring any exception raised is a Backend exception.
    """
    try:
        yield
    except exceptions.Backend as exc:
        raise
    except Exception as exc:
        raise exceptions.Backend('Unknown backend failure: %r' % exc)


class Service(service.Service):
    RPC_API_VERSION = '3.3'

    target = messaging.Target(version=RPC_API_VERSION)

    def __init__(self, *args, **kwargs):
        super(Service, self).__init__(*args, **kwargs)

        backend_driver = cfg.CONF['service:central'].backend_driver
        self.backend = backend.get_backend(backend_driver, self)

        policy.init()

        # Get a storage connection
        self.storage_api = storage_api.StorageAPI()

        # Get a quota manager instance
        self.quota = quota.get_quota()

        self.network_api = network_api.get_network_api(cfg.CONF.network_api)

    def start(self):
        # Check to see if there are any TLDs in the database
        tlds = self.storage_api.find_tlds({})
        if tlds:
            self.check_for_tlds = True
            LOG.info("Checking for TLDs")
        else:
            self.check_for_tlds = False
            LOG.info("NOT checking for TLDs")

        self.backend.start()

        super(Service, self).start()

    def stop(self):
        super(Service, self).stop()

        self.backend.stop()

    def _is_valid_domain_name(self, context, domain_name):
        # Validate domain name length
        if len(domain_name) > cfg.CONF['service:central'].max_domain_name_len:
            raise exceptions.InvalidDomainName('Name too long')

        # Break the domain name up into its component labels
        domain_labels = domain_name.strip('.').split('.')

        # We need more than 1 label.
        if len(domain_labels) <= 1:
            raise exceptions.InvalidDomainName('More than one label is '
                                               'required')

        # Check the TLD for validity if there are entries in the database
        if self.check_for_tlds:
            try:
                self.storage_api.find_tld(context, {'name': domain_labels[-1]})
            except exceptions.TLDNotFound:
                raise exceptions.InvalidDomainName('Invalid TLD')

            # Now check that the domain name is not the same as a TLD
            try:
                stripped_domain_name = domain_name.strip('.').lower()
                self.storage_api.find_tld(
                    context,
                    {'name': stripped_domain_name})
            except exceptions.TLDNotFound:
                pass
            else:
                raise exceptions.InvalidDomainName(
                    'Domain name cannot be the same as a TLD')

        # Check domain name blacklist
        if self._is_blacklisted_domain_name(context, domain_name):
            # Some users are allowed bypass the blacklist.. Is this one?
            if not policy.check('use_blacklisted_domain', context,
                                do_raise=False):
                raise exceptions.InvalidDomainName('Blacklisted domain name')

        return True

    def _is_valid_recordset_name(self, context, domain, recordset_name):
        if not recordset_name.endswith('.'):
            raise ValueError('Please supply a FQDN')

        # Validate record name length
        max_len = cfg.CONF['service:central'].max_recordset_name_len
        if len(recordset_name) > max_len:
            raise exceptions.InvalidRecordSetName('Name too long')

        # RecordSets must be contained in the parent zone
        if not recordset_name.endswith(domain['name']):
            raise exceptions.InvalidRecordSetLocation(
                'RecordSet is not contained within it\'s parent domain')

    def _is_valid_recordset_placement(self, context, domain, recordset_name,
                                      recordset_type, recordset_id=None):
        # CNAME's must not be created at the zone apex.
        if recordset_type == 'CNAME' and recordset_name == domain['name']:
            raise exceptions.InvalidRecordSetLocation(
                'CNAME recordsets may not be created at the zone apex')

        # CNAME's must not share a name with other recordsets
        criterion = {
            'domain_id': domain['id'],
            'name': recordset_name,
        }

        if recordset_type != 'CNAME':
            criterion['type'] = 'CNAME'

        recordsets = self.storage_api.find_recordsets(context, criterion)

        if ((len(recordsets) == 1 and recordsets[0]['id'] != recordset_id)
                or len(recordsets) > 1):
            raise exceptions.InvalidRecordSetLocation(
                'CNAME recordsets may not share a name with any other records')

        return True

    def _is_valid_recordset_placement_subdomain(self, context, domain,
                                                recordset_name,
                                                criterion=None):
        """
        Check that the placement of the requested rrset belongs to any of the
        domains subdomains..
        """
        LOG.debug("Checking if %s belongs in any of %s subdomains",
                  recordset_name, domain['name'])

        criterion = criterion or {}

        context = context.elevated()
        context.all_tenants = True

        if domain['name'] == recordset_name:
            return

        child_domains = self.storage_api.find_domains(
            context, {"parent_domain_id": domain['id']})
        for child_domain in child_domains:
            try:
                self._is_valid_recordset_name(
                    context, child_domain, recordset_name)
            except Exception:
                continue
            else:
                msg = 'RecordSet belongs in a child zone: %s' % \
                    child_domain['name']
                raise exceptions.InvalidRecordSetLocation(msg)

    def _is_blacklisted_domain_name(self, context, domain_name):
        """
        Ensures the provided domain_name is not blacklisted.
        """

        blacklists = self.storage_api.find_blacklists(context)

        for blacklist in blacklists:
            if bool(re.search(blacklist["pattern"], domain_name)):
                return True

        return False

    def _is_subdomain(self, context, domain_name):
        context = context.elevated()
        context.all_tenants = True

        # Break the name up into it's component labels
        labels = domain_name.split(".")

        i = 1

        # Starting with label #2, search for matching domain's in the database
        while (i < len(labels)):
            name = '.'.join(labels[i:])

            try:
                domain = self.storage_api.find_domain(context, {'name': name})
            except exceptions.DomainNotFound:
                i += 1
            else:
                return domain

        return False

    def _is_valid_ttl(self, context, ttl):
        min_ttl = cfg.CONF['service:central'].min_ttl
        if min_ttl != "None" and ttl < int(min_ttl):
            try:
                policy.check('use_low_ttl', context)
            except exceptions.Forbidden:
                raise exceptions.InvalidTTL('TTL is below the minimum: %s'
                                            % min_ttl)

    def _increment_domain_serial(self, context, domain_id):
        domain = self.storage_api.get_domain(context, domain_id)

        # Increment the serial number
        values = {'serial': utils.increment_serial(domain['serial'])}

        with self.storage_api.update_domain(
                context, domain_id, values) as domain:
            with wrap_backend_call():
                self.backend.update_domain(context, domain)

        return domain

    # Quota Enforcement Methods
    def _enforce_domain_quota(self, context, tenant_id):
        criterion = {'tenant_id': tenant_id}
        count = self.storage_api.count_domains(context, criterion)

        self.quota.limit_check(context, tenant_id, domains=count)

    def _enforce_recordset_quota(self, context, domain):
        # TODO(kiall): Enforce RRSet Quotas
        pass

    def _enforce_record_quota(self, context, domain, recordset):
        # Ensure the records per domain quota is OK
        criterion = {'domain_id': domain['id']}
        count = self.storage_api.count_records(context, criterion)

        self.quota.limit_check(context, domain['tenant_id'],
                               domain_records=count)

        # TODO(kiall): Enforce Records per RRSet Quotas

    # Misc Methods
    def get_absolute_limits(self, context):
        # NOTE(Kiall): Currently, we only have quota based limits..
        return self.quota.get_quotas(context, context.tenant)

    # Quota Methods
    def get_quotas(self, context, tenant_id):
        target = {'tenant_id': tenant_id}
        policy.check('get_quotas', context, target)

        return self.quota.get_quotas(context, tenant_id)

    def get_quota(self, context, tenant_id, resource):
        target = {'tenant_id': tenant_id, 'resource': resource}
        policy.check('get_quota', context, target)

        return self.quota.get_quota(context, tenant_id, resource)

    def set_quota(self, context, tenant_id, resource, hard_limit):
        target = {
            'tenant_id': tenant_id,
            'resource': resource,
            'hard_limit': hard_limit,
        }

        policy.check('set_quota', context, target)

        return self.quota.set_quota(context, tenant_id, resource, hard_limit)

    def reset_quotas(self, context, tenant_id):
        target = {'tenant_id': tenant_id}
        policy.check('reset_quotas', context, target)

        self.quota.reset_quotas(context, tenant_id)

    # Server Methods
    def create_server(self, context, values):
        policy.check('create_server', context)

        with self.storage_api.create_server(context, values) as server:
            # Update backend with the new server..
            with wrap_backend_call():
                self.backend.create_server(context, server)

        self.notifier.info(context, 'dns.server.create', server)

        return server

    def find_servers(self, context, criterion=None, marker=None, limit=None,
                     sort_key=None, sort_dir=None):
        policy.check('find_servers', context)

        return self.storage_api.find_servers(context, criterion, marker, limit,
                                             sort_key, sort_dir)

    def get_server(self, context, server_id):
        policy.check('get_server', context, {'server_id': server_id})

        return self.storage_api.get_server(context, server_id)

    def update_server(self, context, server_id, values):
        policy.check('update_server', context, {'server_id': server_id})

        with self.storage_api.update_server(
                context, server_id, values) as server:
            # Update backend with the new details..
            with wrap_backend_call():
                self.backend.update_server(context, server)

        self.notifier.info(context, 'dns.server.update', server)

        return server

    def delete_server(self, context, server_id):
        policy.check('delete_server', context, {'server_id': server_id})

        # don't delete last of servers
        servers = self.storage_api.find_servers(context)
        if len(servers) == 1 and server_id == servers[0]['id']:
            raise exceptions.LastServerDeleteNotAllowed(
                "Not allowed to delete last of servers")

        with self.storage_api.delete_server(context, server_id) as server:
            # Update backend with the new server..
            with wrap_backend_call():
                self.backend.delete_server(context, server)

        self.notifier.info(context, 'dns.server.delete', server)

    # TLD Methods
    def create_tld(self, context, values):
        policy.check('create_tld', context)

        # The TLD is only created on central's storage and not on the backend.
        with self.storage_api.create_tld(context, values) as tld:
            pass
        self.notifier.info(context, 'dns.tld.create', tld)

        # Set check for tlds to be true
        self.check_for_tlds = True
        return tld

    def find_tlds(self, context, criterion=None, marker=None, limit=None,
                  sort_key=None, sort_dir=None):
        policy.check('find_tlds', context)

        return self.storage_api.find_tlds(context, criterion, marker, limit,
                                          sort_key, sort_dir)

    def get_tld(self, context, tld_id):
        policy.check('get_tld', context, {'tld_id': tld_id})

        return self.storage_api.get_tld(context, tld_id)

    def update_tld(self, context, tld_id, values):
        policy.check('update_tld', context, {'tld_id': tld_id})

        with self.storage_api.update_tld(context, tld_id, values) as tld:
            pass

        self.notifier.info(context, 'dns.tld.update', tld)

        return tld

    def delete_tld(self, context, tld_id):
        # Known issue - self.check_for_tld is not reset here.  So if the last
        # TLD happens to be deleted, then we would incorrectly do the TLD
        # validations.
        # This decision was influenced by weighing the (ultra low) probability
        # of hitting this issue vs doing the checks for every delete.
        policy.check('delete_tld', context, {'tld_id': tld_id})

        with self.storage_api.delete_tld(context, tld_id) as tld:
            pass

        self.notifier.info(context, 'dns.tld.delete', tld)

    # TSIG Key Methods
    def create_tsigkey(self, context, values):
        policy.check('create_tsigkey', context)

        with self.storage_api.create_tsigkey(context, values) as tsigkey:
            with wrap_backend_call():
                self.backend.create_tsigkey(context, tsigkey)

        self.notifier.info(context, 'dns.tsigkey.create', tsigkey)

        return tsigkey

    def find_tsigkeys(self, context, criterion=None, marker=None, limit=None,
                      sort_key=None, sort_dir=None):
        policy.check('find_tsigkeys', context)

        return self.storage_api.find_tsigkeys(context, criterion, marker,
                                              limit, sort_key, sort_dir)

    def get_tsigkey(self, context, tsigkey_id):
        policy.check('get_tsigkey', context, {'tsigkey_id': tsigkey_id})

        return self.storage_api.get_tsigkey(context, tsigkey_id)

    def update_tsigkey(self, context, tsigkey_id, values):
        policy.check('update_tsigkey', context, {'tsigkey_id': tsigkey_id})

        with self.storage_api.update_tsigkey(
                context, tsigkey_id, values) as tsigkey:
            with wrap_backend_call():
                self.backend.update_tsigkey(context, tsigkey)

        self.notifier.info(context, 'dns.tsigkey.update', tsigkey)

        return tsigkey

    def delete_tsigkey(self, context, tsigkey_id):
        policy.check('delete_tsigkey', context, {'tsigkey_id': tsigkey_id})

        with self.storage_api.delete_tsigkey(context, tsigkey_id) as tsigkey:
            with wrap_backend_call():
                self.backend.delete_tsigkey(context, tsigkey)

        self.notifier.info(context, 'dns.tsigkey.delete', tsigkey)

    # Tenant Methods
    def find_tenants(self, context):
        policy.check('find_tenants', context)
        return self.storage_api.find_tenants(context)

    def get_tenant(self, context, tenant_id):
        target = {
            'tenant_id': tenant_id
        }

        policy.check('get_tenant', context, target)

        return self.storage_api.get_tenant(context, tenant_id)

    def count_tenants(self, context):
        policy.check('count_tenants', context)
        return self.storage_api.count_tenants(context)

    # Domain Methods
    def create_domain(self, context, values):
        # TODO(kiall): Refactor this method into *MUCH* smaller chunks.

        # Default to creating in the current users tenant
        if 'tenant_id' not in values:
            values['tenant_id'] = context.tenant

        target = {
            'tenant_id': values['tenant_id'],
            'domain_name': values['name']
        }

        policy.check('create_domain', context, target)

        # Ensure the tenant has enough quota to continue
        self._enforce_domain_quota(context, values['tenant_id'])

        # Ensure the domain name is valid
        self._is_valid_domain_name(context, values['name'])

        # Ensure TTL is above the minimum
        ttl = values.get('ttl', None)
        if ttl is not None:
            self._is_valid_ttl(context, ttl)

        # Handle sub-domains appropriately
        parent_domain = self._is_subdomain(context, values['name'])

        if parent_domain:
            if parent_domain['tenant_id'] == values['tenant_id']:
                # Record the Parent Domain ID
                values['parent_domain_id'] = parent_domain['id']
            else:
                raise exceptions.Forbidden('Unable to create subdomain in '
                                           'another tenants domain')

        # TODO(kiall): Handle super-domains properly

        # NOTE(kiall): Fetch the servers before creating the domain, this way
        #              we can prevent domain creation if no servers are
        #              configured.
        servers = self.storage_api.find_servers(context)

        if len(servers) == 0:
            LOG.critical('No servers configured. Please create at least one '
                         'server')
            raise exceptions.NoServersConfigured()

        # Set the serial number
        values['serial'] = utils.increment_serial()

        with self.storage_api.create_domain(context, values) as domain:
            with wrap_backend_call():
                self.backend.create_domain(context, domain)

        self.notifier.info(context, 'dns.domain.create', domain)

        return domain

    def get_domain(self, context, domain_id):
        domain = self.storage_api.get_domain(context, domain_id)

        target = {
            'domain_id': domain_id,
            'domain_name': domain['name'],
            'tenant_id': domain['tenant_id']
        }
        policy.check('get_domain', context, target)

        return domain

    def get_domain_servers(self, context, domain_id, criterion=None):
        domain = self.storage_api.get_domain(context, domain_id)

        target = {
            'domain_id': domain_id,
            'domain_name': domain['name'],
            'tenant_id': domain['tenant_id']
        }

        policy.check('get_domain_servers', context, target)

        # TODO(kiall): Once we allow domains to be allocated on 1 of N server
        #              pools, return the filtered list here.
        return self.storage_api.find_servers(context, criterion)

    def find_domains(self, context, criterion=None, marker=None, limit=None,
                     sort_key=None, sort_dir=None):
        target = {'tenant_id': context.tenant}
        policy.check('find_domains', context, target)

        return self.storage_api.find_domains(context, criterion, marker, limit,
                                             sort_key, sort_dir)

    def find_domain(self, context, criterion=None):
        target = {'tenant_id': context.tenant}
        policy.check('find_domain', context, target)

        return self.storage_api.find_domain(context, criterion)

    def update_domain(self, context, domain_id, values, increment_serial=True):
        # TODO(kiall): Refactor this method into *MUCH* smaller chunks.
        domain = self.storage_api.get_domain(context, domain_id)

        target = {
            'domain_id': domain_id,
            'domain_name': domain['name'],
            'tenant_id': domain['tenant_id']
        }

        policy.check('update_domain', context, target)

        if 'tenant_id' in values:
            # NOTE(kiall): Ensure the user is allowed to delete a domain from
            #              the original tenant.
            policy.check('delete_domain', context, target)

            # NOTE(kiall): Ensure the user is allowed to create a domain in
            #              the new tenant.
            target = {'domain_id': domain_id, 'tenant_id': values['tenant_id']}
            policy.check('create_domain', context, target)

        if 'name' in values and values['name'] != domain['name']:
            raise exceptions.BadRequest('Renaming a domain is not allowed')

        # Ensure TTL is above the minimum
        ttl = values.get('ttl', None)
        if ttl is not None:
            self._is_valid_ttl(context, ttl)

        if increment_serial:
            # Increment the serial number
            values['serial'] = utils.increment_serial(domain['serial'])

        with self.storage_api.update_domain(
                context, domain_id, values) as domain:
            with wrap_backend_call():
                self.backend.update_domain(context, domain)

        self.notifier.info(context, 'dns.domain.update', domain)

        return domain

    def delete_domain(self, context, domain_id):
        domain = self.storage_api.get_domain(context, domain_id)

        target = {
            'domain_id': domain_id,
            'domain_name': domain['name'],
            'tenant_id': domain['tenant_id']
        }

        policy.check('delete_domain', context, target)

        # Prevent deletion of a zone which has child zones
        criterion = {'parent_domain_id': domain_id}

        if self.storage_api.count_domains(context, criterion) > 0:
            raise exceptions.DomainHasSubdomain('Please delete any subdomains '
                                                'before deleting this domain')

        with self.storage_api.delete_domain(context, domain_id) as domain:
            with wrap_backend_call():
                self.backend.delete_domain(context, domain)

        self.notifier.info(context, 'dns.domain.delete', domain)

        return domain

    def count_domains(self, context, criterion=None):
        if criterion is None:
            criterion = {}

        target = {
            'tenant_id': criterion.get('tenant_id', None)
        }

        policy.check('count_domains', context, target)

        return self.storage_api.count_domains(context, criterion)

    def touch_domain(self, context, domain_id):
        domain = self.storage_api.get_domain(context, domain_id)

        target = {
            'domain_id': domain_id,
            'domain_name': domain['name'],
            'tenant_id': domain['tenant_id']
        }

        policy.check('touch_domain', context, target)

        domain = self._increment_domain_serial(context, domain_id)

        self.notifier.info(context, 'dns.domain.touch', domain)

        return domain

    # RecordSet Methods
    def create_recordset(self, context, domain_id, values):
        domain = self.storage_api.get_domain(context, domain_id)

        target = {
            'domain_id': domain_id,
            'domain_name': domain['name'],
            'recordset_name': values['name'],
            'tenant_id': domain['tenant_id'],
        }

        policy.check('create_recordset', context, target)

        # Ensure the tenant has enough quota to continue
        self._enforce_recordset_quota(context, domain)

        # Ensure TTL is above the minimum
        ttl = values.get('ttl', None)
        if ttl is not None:
            self._is_valid_ttl(context, ttl)

        # Ensure the recordset name and placement is valid
        self._is_valid_recordset_name(context, domain, values['name'])
        self._is_valid_recordset_placement(context, domain, values['name'],
                                           values['type'])
        self._is_valid_recordset_placement_subdomain(
            context, domain, values['name'])

        with self.storage_api.create_recordset(
                context, domain_id, values) as recordset:
            with wrap_backend_call():
                self.backend.create_recordset(context, domain, recordset)

        # Send RecordSet creation notification
        self.notifier.info(context, 'dns.recordset.create', recordset)

        return recordset

    def get_recordset(self, context, domain_id, recordset_id):
        domain = self.storage_api.get_domain(context, domain_id)
        recordset = self.storage_api.get_recordset(context, recordset_id)

        # Ensure the domain_id matches the record's domain_id
        if domain['id'] != recordset['domain_id']:
            raise exceptions.RecordSetNotFound()

        target = {
            'domain_id': domain_id,
            'domain_name': domain['name'],
            'recordset_id': recordset['id'],
            'tenant_id': domain['tenant_id'],
        }

        policy.check('get_recordset', context, target)

        return recordset

    def find_recordsets(self, context, criterion=None, marker=None, limit=None,
                        sort_key=None, sort_dir=None):
        target = {'tenant_id': context.tenant}
        policy.check('find_recordsets', context, target)

        return self.storage_api.find_recordsets(context, criterion, marker,
                                                limit, sort_key, sort_dir)

    def find_recordset(self, context, criterion=None):
        target = {'tenant_id': context.tenant}
        policy.check('find_recordset', context, target)

        return self.storage_api.find_recordset(context, criterion)

    def update_recordset(self, context, domain_id, recordset_id, values,
                         increment_serial=True):
        domain = self.storage_api.get_domain(context, domain_id)
        recordset = self.storage_api.get_recordset(context, recordset_id)

        # Ensure the domain_id matches the recordset's domain_id
        if domain['id'] != recordset['domain_id']:
            raise exceptions.RecordSetNotFound()

        target = {
            'domain_id': domain_id,
            'domain_name': domain['name'],
            'recordset_id': recordset['id'],
            'tenant_id': domain['tenant_id']
        }

        policy.check('update_recordset', context, target)

        # Ensure the record name is valid
        recordset_name = values['name'] if 'name' in values \
            else recordset['name']
        recordset_type = values['type'] if 'type' in values \
            else recordset['type']

        self._is_valid_recordset_name(context, domain, recordset_name)
        self._is_valid_recordset_placement(context, domain, recordset_name,
                                           recordset_type, recordset_id)
        self._is_valid_recordset_placement_subdomain(
            context, domain, recordset_name)

        # Ensure TTL is above the minimum
        ttl = values.get('ttl', None)
        if ttl is not None:
            self._is_valid_ttl(context, ttl)

        # Update the recordset
        with self.storage_api.update_recordset(
                context, recordset_id, values) as recordset:
            with wrap_backend_call():
                self.backend.update_recordset(context, domain, recordset)

            if increment_serial:
                self._increment_domain_serial(context, domain_id)

        # Send RecordSet update notification
        self.notifier.info(context, 'dns.recordset.update', recordset)

        return recordset

    def delete_recordset(self, context, domain_id, recordset_id,
                         increment_serial=True):
        domain = self.storage_api.get_domain(context, domain_id)
        recordset = self.storage_api.get_recordset(context, recordset_id)

        # Ensure the domain_id matches the recordset's domain_id
        if domain['id'] != recordset['domain_id']:
            raise exceptions.RecordSetNotFound()

        target = {
            'domain_id': domain_id,
            'domain_name': domain['name'],
            'recordset_id': recordset['id'],
            'tenant_id': domain['tenant_id']
        }

        policy.check('delete_recordset', context, target)

        with self.storage_api.delete_recordset(context, recordset_id) \
                as recordset:
            with wrap_backend_call():
                self.backend.delete_recordset(context, domain, recordset)

            if increment_serial:
                self._increment_domain_serial(context, domain_id)

        # Send Record deletion notification
        self.notifier.info(context, 'dns.recordset.delete', recordset)

        return recordset

    def count_recordsets(self, context, criterion=None):
        if criterion is None:
            criterion = {}

        target = {
            'tenant_id': criterion.get('tenant_id', None)
        }

        policy.check('count_recordsets', context, target)

        return self.storage_api.count_recordsets(context, criterion)

    # Record Methods
    def create_record(self, context, domain_id, recordset_id, values,
                      increment_serial=True):
        domain = self.storage_api.get_domain(context, domain_id)
        recordset = self.storage_api.get_recordset(context, recordset_id)

        target = {
            'domain_id': domain_id,
            'domain_name': domain['name'],
            'recordset_id': recordset_id,
            'recordset_name': recordset['name'],
            'tenant_id': domain['tenant_id']
        }

        policy.check('create_record', context, target)

        # Ensure the tenant has enough quota to continue
        self._enforce_record_quota(context, domain, recordset)

        with self.storage_api.create_record(
                context, domain_id, recordset_id, values) as record:
            with wrap_backend_call():
                self.backend.create_record(context, domain, recordset, record)

            if increment_serial:
                self._increment_domain_serial(context, domain_id)

        # Send Record creation notification
        self.notifier.info(context, 'dns.record.create', record)

        return record

    def get_record(self, context, domain_id, recordset_id, record_id):
        domain = self.storage_api.get_domain(context, domain_id)
        recordset = self.storage_api.get_recordset(context, recordset_id)
        record = self.storage_api.get_record(context, record_id)

        # Ensure the domain_id matches the record's domain_id
        if domain['id'] != record['domain_id']:
            raise exceptions.RecordNotFound()

        # Ensure the recordset_id matches the record's recordset_id
        if recordset['id'] != record['recordset_id']:
            raise exceptions.RecordNotFound()

        target = {
            'domain_id': domain_id,
            'domain_name': domain['name'],
            'recordset_id': recordset_id,
            'recordset_name': recordset['name'],
            'record_id': record['id'],
            'tenant_id': domain['tenant_id']
        }

        policy.check('get_record', context, target)

        return record

    def find_records(self, context, criterion=None, marker=None, limit=None,
                     sort_key=None, sort_dir=None):
        target = {'tenant_id': context.tenant}
        policy.check('find_records', context, target)

        return self.storage_api.find_records(context, criterion, marker, limit,
                                             sort_key, sort_dir)

    def find_record(self, context, criterion=None):
        target = {'tenant_id': context.tenant}
        policy.check('find_record', context, target)

        return self.storage_api.find_record(context, criterion)

    def update_record(self, context, domain_id, recordset_id, record_id,
                      values, increment_serial=True):
        domain = self.storage_api.get_domain(context, domain_id)
        recordset = self.storage_api.get_recordset(context, recordset_id)
        record = self.storage_api.get_record(context, record_id)

        # Ensure the domain_id matches the record's domain_id
        if domain['id'] != record['domain_id']:
            raise exceptions.RecordNotFound()

        # Ensure the recordset_id matches the record's recordset_id
        if recordset['id'] != record['recordset_id']:
            raise exceptions.RecordNotFound()

        target = {
            'domain_id': domain_id,
            'domain_name': domain['name'],
            'recordset_id': recordset_id,
            'recordset_name': recordset['name'],
            'record_id': record['id'],
            'tenant_id': domain['tenant_id']
        }

        policy.check('update_record', context, target)

        # Update the record
        with self.storage_api.update_record(
                context, record_id, values) as record:
            with wrap_backend_call():
                self.backend.update_record(context, domain, recordset, record)

            if increment_serial:
                self._increment_domain_serial(context, domain_id)

        # Send Record update notification
        self.notifier.info(context, 'dns.record.update', record)

        return record

    def delete_record(self, context, domain_id, recordset_id, record_id,
                      increment_serial=True):
        domain = self.storage_api.get_domain(context, domain_id)
        recordset = self.storage_api.get_recordset(context, recordset_id)
        record = self.storage_api.get_record(context, record_id)

        # Ensure the domain_id matches the record's domain_id
        if domain['id'] != record['domain_id']:
            raise exceptions.RecordNotFound()

        # Ensure the recordset_id matches the record's recordset_id
        if recordset['id'] != record['recordset_id']:
            raise exceptions.RecordNotFound()

        target = {
            'domain_id': domain_id,
            'domain_name': domain['name'],
            'recordset_id': recordset_id,
            'recordset_name': recordset['name'],
            'record_id': record['id'],
            'tenant_id': domain['tenant_id']
        }

        policy.check('delete_record', context, target)

        with self.storage_api.delete_record(context, record_id) as record:
            with wrap_backend_call():
                self.backend.delete_record(context, domain, recordset, record)

            if increment_serial:
                self._increment_domain_serial(context, domain_id)

        # Send Record deletion notification
        self.notifier.info(context, 'dns.record.delete', record)

        return record

    def count_records(self, context, criterion=None):
        if criterion is None:
            criterion = {}

        target = {
            'tenant_id': criterion.get('tenant_id', None)
        }

        policy.check('count_records', context, target)
        return self.storage_api.count_records(context, criterion)

    # Diagnostics Methods
    def _sync_domain(self, context, domain):
        recordsets = self.storage_api.find_recordsets(
            context, criterion={'domain_id': domain['id']})

        # Since we now have records as well as recordsets we need to get the
        # records for it as well and pass that down since the backend wants it.
        rdata = []
        for recordset in recordsets:
            records = self.find_records(
                context, {'recordset_id': recordset['id']})
            rdata.append((recordset, records))
        with wrap_backend_call():
            return self.backend.sync_domain(context, domain, rdata)

    def sync_domains(self, context):
        policy.check('diagnostics_sync_domains', context)

        domains = self.storage_api.find_domains(context)

        results = {}
        for domain in domains:
            results[domain['id']] = self._sync_domain(context, domain)

        return results

    def sync_domain(self, context, domain_id):
        domain = self.storage_api.get_domain(context, domain_id)

        target = {
            'domain_id': domain_id,
            'domain_name': domain['name'],
            'tenant_id': domain['tenant_id']
        }

        policy.check('diagnostics_sync_domain', context, target)

        return self._sync_domain(context, domain)

    def sync_record(self, context, domain_id, recordset_id, record_id):
        domain = self.storage_api.get_domain(context, domain_id)
        recordset = self.storage_api.get_recordset(context, recordset_id)

        target = {
            'domain_id': domain_id,
            'domain_name': domain['name'],
            'recordset_id': recordset_id,
            'recordset_name': recordset['name'],
            'record_id': record_id,
            'tenant_id': domain['tenant_id']
        }

        policy.check('diagnostics_sync_record', context, target)

        record = self.storage_api.get_record(context, record_id)

        with wrap_backend_call():
            return self.backend.sync_record(context, domain, recordset, record)

    def ping(self, context):
        policy.check('diagnostics_ping', context)

        try:
            backend_status = self.backend.ping(context)
        except Exception as e:
            backend_status = {'status': False, 'message': str(e)}

        try:
            storage_status = self.storage_api.ping(context)
        except Exception as e:
            storage_status = {'status': False, 'message': str(e)}

        if backend_status and storage_status:
            status = True
        else:
            status = False

        return {
            'host': cfg.CONF.host,
            'status': status,
            'backend': backend_status,
            'storage': storage_status
        }

    def _determine_floatingips(self, context, fips, records=None,
                               tenant_id=None):
        """
        Given the context or tenant, records and fips it returns the valid
        floatingips either with a associated record or not. Deletes invalid
        records also.

        Returns a list of tuples with FloatingIPs and it's Record.
        """
        tenant_id = tenant_id or context.tenant

        elevated_context = context.elevated()
        elevated_context.all_tenants = True

        criterion = {
            'managed': True,
            'managed_resource_type': 'ptr:floatingip',
        }

        records = self.find_records(elevated_context, criterion)
        records = dict([(r['managed_extra'], r) for r in records])

        invalid = []
        data = {}
        # First populate the list of FIPS
        for fip_key, fip_values in fips.items():
            # Check if the FIP has a record
            record = records.get(fip_values['address'])

            # NOTE: Now check if it's owned by the tenant that actually has the
            # FIP in the external service and if not invalidate it (delete it)
            # thus not returning it with in the tuple with the FIP, but None..

            if record:
                record_tenant = record['managed_tenant_id']

                if record_tenant != tenant_id:
                    msg = "Invalid FloatingIP %s belongs to %s but record " \
                          "owner %s"
                    LOG.debug(msg, fip_key, tenant_id, record_tenant)

                    invalid.append(record)
                    record = None
            data[fip_key] = (fip_values, record)

        return data, invalid

    def _invalidate_floatingips(self, context, records):
        """
        Utility method to delete a list of records.
        """
        elevated_context = context.elevated()
        elevated_context.all_tenants = True

        if records > 0:
            for r in records:
                msg = 'Deleting record %s for FIP %s'
                LOG.debug(msg, r['id'], r['managed_resource_id'])
                self.delete_record(elevated_context, r['domain_id'],
                                   r['recordset_id'], r['id'])

    def _format_floatingips(self, context, data, recordsets=None):
        """
        Given a list of FloatingIP and Record tuples we look through creating
        a new dict of FloatingIPs
        """
        elevated_context = context.elevated()
        elevated_context.all_tenants = True

        fips = {}
        for key, value in data.items():
            fip_ptr = {
                'address': value[0]['address'],
                'id': value[0]['id'],
                'region': value[0]['region'],
                'ptrdname': None,
                'ttl': None,
                'description': None
            }

            # TTL population requires a present record in order to find the
            # RS or Zone
            if value[1]:
                # We can have a recordset dict passed in
                if (recordsets is not None and
                        value[1]['recordset_id'] in recordsets):
                    recordset = recordsets[value[1]['recordset_id']]
                else:
                    recordset = self.storage_api.get_recordset(
                        elevated_context, value[1]['recordset_id'])

                if recordset['ttl'] is not None:
                    fip_ptr['ttl'] = recordset['ttl']
                else:
                    zone = self.get_domain(
                        elevated_context, value[1]['domain_id'])
                    fip_ptr['ttl'] = zone['ttl']

                fip_ptr['ptrdname'] = value[1]['data']
            else:
                LOG.debug("No record information found for %s",
                          value[0]['id'])

            # Store the "fip_record" with the region and it's id as key
            fips[key] = fip_ptr
        return fips

    def _list_floatingips(self, context, region=None):
        data = self.network_api.list_floatingips(context, region=region)
        return self._list_to_dict(data, keys=['region', 'id'])

    def _list_to_dict(self, data, keys=['id']):
        new = {}
        for i in data:
            key = tuple([i[key] for key in keys])
            new[key] = i
        return new

    def _get_floatingip(self, context, region, floatingip_id, fips):
        if (region, floatingip_id) not in fips:
            msg = 'FloatingIP %s in %s is not associated for tenant "%s"' % \
                (floatingip_id, region, context.tenant)
            raise exceptions.NotFound(msg)
        return fips[region, floatingip_id]

    # PTR ops
    def list_floatingips(self, context):
        """
        List Floating IPs PTR

        A) We have service_catalog in the context and do a lookup using the
               token pr Neutron in the SC
        B) We lookup FIPs using the configured values for this deployment.
        """
        elevated_context = context.elevated()
        elevated_context.all_tenants = True

        tenant_fips = self._list_floatingips(context)

        valid, invalid = self._determine_floatingips(
            elevated_context, tenant_fips)

        self._invalidate_floatingips(context, invalid)

        return self._format_floatingips(context, valid).values()

    def get_floatingip(self, context, region, floatingip_id):
        """
        Get Floating IP PTR
        """
        elevated_context = context.elevated()
        elevated_context.all_tenants = True

        tenant_fips = self._list_floatingips(context, region=region)

        self._get_floatingip(context, region, floatingip_id, tenant_fips)

        valid, invalid = self._determine_floatingips(
            elevated_context, tenant_fips)

        self._invalidate_floatingips(context, invalid)

        mangled = self._format_floatingips(context, valid)
        return mangled[region, floatingip_id]

    def _set_floatingip_reverse(self, context, region, floatingip_id, values):
        """
        Set the FloatingIP's PTR record based on values.
        """
        values.setdefault('description', None)

        elevated_context = context.elevated()
        elevated_context.all_tenants = True

        tenant_fips = self._list_floatingips(context, region=region)

        fip = self._get_floatingip(context, region, floatingip_id, tenant_fips)

        zone_name = self.network_api.address_zone(fip['address'])

        # NOTE: Find existing zone or create it..
        try:
            zone = self.storage_api.find_domain(
                elevated_context, {'name': zone_name})
        except exceptions.DomainNotFound:
            msg = 'Creating zone for %s:%s - %s zone %s' % \
                (floatingip_id, region, fip['address'], zone_name)
            LOG.info(msg)

            email = cfg.CONF['service:central'].managed_resource_email
            tenant_id = cfg.CONF['service:central'].managed_resource_tenant_id

            zone_values = {
                'name': zone_name,
                'email': email,
                'tenant_id': tenant_id
            }

            zone = self.create_domain(elevated_context, zone_values)

        record_name = self.network_api.address_name(fip['address'])

        try:
            # NOTE: Delete the current recormdset if any (also purges records)
            LOG.debug("Removing old RRset / Record")
            rset = self.find_recordset(
                elevated_context, {'name': record_name, 'type': 'PTR'})

            records = self.find_records(
                elevated_context, {'recordset_id': rset['id']})

            for record in records:
                self.delete_record(
                    elevated_context,
                    rset['domain_id'],
                    rset['id'],
                    record['id'])
            self.delete_recordset(elevated_context, zone['id'], rset['id'])
        except exceptions.RecordSetNotFound:
            pass

        recordset_values = {
            'name': record_name,
            'type': 'PTR',
            'ttl': values.get('ttl', None)
        }

        recordset = self.create_recordset(
            elevated_context, zone['id'], recordset_values)

        record_values = {
            'data': values['ptrdname'],
            'description': values['description'],
            'type': 'PTR',
            'managed': True,
            'managed_extra': fip['address'],
            'managed_resource_id': floatingip_id,
            'managed_resource_region': region,
            'managed_resource_type': 'ptr:floatingip',
            'managed_tenant_id': context.tenant
        }

        record = self.create_record(
            elevated_context, zone['id'], recordset['id'], record_values)

        mangled = self._format_floatingips(
            context, {(region, floatingip_id): (fip, record)},
            {recordset['id']: recordset})

        return mangled[region, floatingip_id]

    def _unset_floatingip_reverse(self, context, region, floatingip_id):
        """
        Unset the FloatingIP PTR record based on the

        Service's FloatingIP ID > managed_resource_id
        Tenant ID > managed_tenant_id

        We find the record based on the criteria and delete it or raise.
        """
        elevated_context = context.elevated()
        elevated_context.all_tenants = True

        criterion = {
            'managed_resource_id': floatingip_id,
            'managed_tenant_id': context.tenant
        }

        try:
            record = self.storage_api.find_record(
                elevated_context, criterion=criterion)
        except exceptions.RecordNotFound:
            msg = 'No such FloatingIP %s:%s' % (region, floatingip_id)
            raise exceptions.NotFound(msg)

        self.delete_record(
            elevated_context,
            record['domain_id'],
            record['recordset_id'],
            record['id'])

    def update_floatingip(self, context, region, floatingip_id, values):
        """
        We strictly see if values['ptrdname'] is str or None and set / unset
        the requested FloatingIP's PTR record based on that.
        """
        if values['ptrdname'] is None:
            self._unset_floatingip_reverse(context, region, floatingip_id)
        elif isinstance(values['ptrdname'], basestring):
            return self._set_floatingip_reverse(
                context, region, floatingip_id, values)

    # Blacklisted Domains
    def create_blacklist(self, context, values):
        policy.check('create_blacklist', context)

        with self.storage_api.create_blacklist(context, values) as blacklist:
            pass  # NOTE: No other systems need updating

        self.notifier.info(context, 'dns.blacklist.create', blacklist)

        return blacklist

    def get_blacklist(self, context, blacklist_id):
        policy.check('get_blacklist', context)

        blacklist = self.storage_api.get_blacklist(context, blacklist_id)

        return blacklist

    def find_blacklists(self, context, criterion=None, marker=None,
                        limit=None, sort_key=None, sort_dir=None):
        policy.check('find_blacklists', context)

        blacklists = self.storage_api.find_blacklists(context, criterion,
                                                      marker, limit,
                                                      sort_key, sort_dir)

        return blacklists

    def find_blacklist(self, context, criterion):
        policy.check('find_blacklist', context)

        blacklist = self.storage_api.find_blacklist(context, criterion)

        return blacklist

    def update_blacklist(self, context, blacklist_id, values):
        policy.check('update_blacklist', context)

        with self.storage_api.update_blacklist(context,
                                               blacklist_id,
                                               values) as blacklist:
            pass  # NOTE: No other systems need updating

        self.notifier.info(context, 'dns.blacklist.update', blacklist)

        return blacklist

    def delete_blacklist(self, context, blacklist_id):
        policy.check('delete_blacklist', context)

        with self.storage_api.delete_blacklist(context,
                                               blacklist_id) as blacklist:
            pass  # NOTE: No other systems need updating

        self.notifier.info(context, 'dns.blacklist.delete', blacklist)

########NEW FILE########
__FILENAME__ = agent
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import sys
from oslo.config import cfg
from designate.openstack.common import log as logging
from designate import service
from designate import utils
from designate.agent import service as agent_service

CONF = cfg.CONF
CONF.import_opt('workers', 'designate.agent', group='service:agent')


def main():
    utils.read_config('designate', sys.argv)
    logging.setup('designate')

    server = agent_service.Service.create(
        binary='designate-agent')
    service.serve(server, workers=CONF['service:agent'].workers)
    service.wait()

########NEW FILE########
__FILENAME__ = api
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import sys
from oslo.config import cfg
from designate.openstack.common import log as logging
from designate.openstack.common import service
from designate import rpc
from designate import utils
from designate.api import service as api_service

CONF = cfg.CONF
CONF.import_opt('workers', 'designate.api', group='service:api')


def main():
    utils.read_config('designate', sys.argv)
    logging.setup('designate')

    rpc.init(CONF)

    launcher = service.launch(api_service.Service(),
                              CONF['service:api'].workers)
    launcher.wait()

########NEW FILE########
__FILENAME__ = central
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import sys
from oslo.config import cfg
from designate.openstack.common import log as logging
from designate import service
from designate import utils
from designate.central import service as central

CONF = cfg.CONF
CONF.import_opt('workers', 'designate.central', group='service:central')


def main():
    utils.read_config('designate', sys.argv)
    logging.setup('designate')

    server = central.Service.create(binary='designate-central',
                                    service_name='central')
    service.serve(server, workers=CONF['service:central'].workers)
    service.wait()

########NEW FILE########
__FILENAME__ = manage
# Copyright 2012 Bouvet ASA
#
# Author: Endre Karlson <endre.karlson@bouvet.no>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import sys

from oslo.config import cfg


from designate.openstack.common import log as logging
from designate.openstack.common import strutils
from designate import utils

from stevedore.extension import ExtensionManager


def methods_of(obj):
    """Get all callable methods of an object that don't start with underscore

    returns a list of tuples of the form (method_name, method)
    """
    result = []
    for i in dir(obj):
        if callable(getattr(obj, i)) and not i.startswith('_'):
            result.append((i, getattr(obj, i)))
    return result


def get_available_commands():
    em = ExtensionManager('designate.manage')
    return dict([(e.name, e.plugin) for e in em.extensions])


def add_command_parser(subparsers):
    #for name, category in get_available_commands()
    #parser = subparsers.add_parser('db')
    for name, cls in get_available_commands().items():
        obj = cls()

        # A Category like 'database' etc
        parser = subparsers.add_parser(name)
        parser.set_defaults(command_object=obj)

        category_subparsers = parser.add_subparsers(dest=name)

        for (action, action_fn) in methods_of(obj):
            action_name = getattr(action_fn, '_cmd_name', action)
            cmd_parser = category_subparsers.add_parser(action_name)

            action_kwargs = []
            for args, kwargs in getattr(action_fn, 'args', []):
                kwargs.setdefault('dest', args[0][2:])
                if kwargs['dest'].startswith('action_kwarg_'):
                    action_kwargs.append(
                        kwargs['dest'][len('action_kwarg_'):])
                else:
                    action_kwargs.append(kwargs['dest'])
                    kwargs['dest'] = 'action_kwarg_' + kwargs['dest']
                cmd_parser.add_argument(*args, **kwargs)

            cmd_parser.set_defaults(action_fn=action_fn)
            cmd_parser.set_defaults(action_kwargs=action_kwargs)

            cmd_parser.add_argument('action_args', nargs='*')


command_opt = cfg.SubCommandOpt('command', title="Commands",
                                help="Available Commands",
                                handler=add_command_parser)


def main():
    cfg.CONF.register_cli_opt(command_opt)

    utils.read_config('designate', sys.argv)
    logging.setup('designate')

    func_kwargs = {}
    for k in cfg.CONF.command.action_kwargs:
        v = getattr(cfg.CONF.command, 'action_kwarg_' + k)
        if v is None:
            continue
        func_kwargs[k] = strutils.safe_decode(v)
    func_args = [strutils.safe_decode(arg)
                 for arg in cfg.CONF.command.action_args]
    return cfg.CONF.command.action_fn(*func_args, **func_kwargs)

########NEW FILE########
__FILENAME__ = sink
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import sys
from oslo.config import cfg
from designate.openstack.common import log as logging
from designate import service
from designate import utils
from designate.sink import service as sink_service

CONF = cfg.CONF
CONF.import_opt('workers', 'designate.sink', group='service:sink')


def main():
    utils.read_config('designate', sys.argv)
    logging.setup('designate')

    server = sink_service.Service()
    service.serve(server, workers=CONF['service:sink'].workers)
    service.wait()

########NEW FILE########
__FILENAME__ = context
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import itertools
from designate.openstack.common import context
from designate.openstack.common import log as logging
from designate.openstack.common.gettextutils import _

LOG = logging.getLogger(__name__)


class DesignateContext(context.RequestContext):
    def __init__(self, auth_token=None, user=None, tenant=None, domain=None,
                 user_domain=None, project_domain=None, is_admin=False,
                 read_only=False, show_deleted=False, request_id=None,
                 instance_uuid=None, roles=[], service_catalog=None,
                 all_tenants=False, **kwargs):
        if kwargs:
                LOG.warn(_('Arguments dropped when creating context: %s') %
                         str(kwargs))
        super(DesignateContext, self).__init__(
            auth_token=auth_token,
            user=user,
            tenant=tenant,
            domain=domain,
            user_domain=user_domain,
            project_domain=project_domain,
            is_admin=is_admin,
            read_only=read_only,
            show_deleted=show_deleted,
            request_id=request_id,
            instance_uuid=instance_uuid)

        self.roles = roles
        self.service_catalog = service_catalog
        self.all_tenants = all_tenants

    def deepcopy(self):
        d = self.to_dict()

        return self.from_dict(d)

    def to_dict(self):
        d = super(DesignateContext, self).to_dict()

        user_idt = (
            self.user_idt_format.format(user=self.user or '-',
                                        tenant=self.tenant or '-',
                                        domain=self.domain or '-',
                                        user_domain=self.user_domain or '-',
                                        p_domain=self.project_domain or '-'))
        d.update({
            'roles': self.roles,
            'service_catalog': self.service_catalog,
            'all_tenants': self.all_tenants,
            'user_identity': user_idt
        })

        return d

    @classmethod
    def from_dict(cls, values):
        return cls(**values)

    def elevated(self, show_deleted=None):
        """Return a version of this context with admin flag set."""
        context = self.deepcopy()
        context.is_admin = True

        # NOTE(kiall): Ugly - required to match http://tinyurl.com/o3y8qmw
        context.roles.append('admin')

        if show_deleted is not None:
            context.show_deleted = show_deleted

        return context

    @classmethod
    def get_admin_context(cls, **kwargs):
        # TODO(kiall): Remove Me
        kwargs['is_admin'] = True
        kwargs['roles'] = ['admin']

        return cls(None, **kwargs)

    @classmethod
    def get_context_from_function_and_args(cls, function, args, kwargs):
        """
        Find an arg of type DesignateContext and return it.

        This is useful in a couple of decorators where we don't
        know much about the function we're wrapping.
        """

        for arg in itertools.chain(kwargs.values(), args):
            if isinstance(arg, cls):
                return arg

        return None

########NEW FILE########
__FILENAME__ = debug
# Copyright 2013 Hewlett-Packard Development Company, L.P. All Rights Reserved.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import cProfile
import functools
import tempfile
import pstats


def profile(lines=20, sort='cumtime'):
    def outer(func):
        @functools.wraps(func)
        def inner(*args, **kwargs):
            f = tempfile.NamedTemporaryFile()

            # Profile the function
            prof = cProfile.Profile()
            result = prof.runcall(func, *args, **kwargs)
            prof.dump_stats(f.name)

            # Read the results, and print the stats
            stats = pstats.Stats(f.name)
            stats.sort_stats(sort).print_stats(lines)

            return result
        return inner
    return outer

########NEW FILE########
__FILENAME__ = exceptions
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.


class Base(Exception):
    error_code = 500
    error_type = None
    error_message = None
    errors = None

    def __init__(self, *args, **kwargs):
        self.errors = kwargs.pop('errors', None)

        super(Base, self).__init__(*args, **kwargs)

        if len(args) > 0 and isinstance(args[0], basestring):
            self.error_message = args[0]


class Backend(Exception):
    pass


class NSD4SlaveBackendError(Backend):
    pass


class NotImplemented(Base, NotImplementedError):
    pass


class ConfigurationError(Base):
    error_type = 'configuration_error'


class CommunicationFailure(Base):
    error_code = 504
    error_type = 'communication_failure'


class NeutronCommunicationFailure(CommunicationFailure):
    """
    Raised in case one of the alleged Neutron endpoints fails.
    """
    error_type = 'neutron_communication_failure'


class NoServersConfigured(ConfigurationError):
    error_code = 500
    error_type = 'no_servers_configured'


class OverQuota(Base):
    error_code = 413
    error_type = 'over_quota'


class QuotaResourceUnknown(Base):
    error_type = 'quota_resource_unknown'


class InvalidObject(Base):
    error_code = 400
    error_type = 'invalid_object'


class BadRequest(Base):
    error_code = 400
    error_type = 'bad_request'


class InvalidUUID(BadRequest):
    error_type = 'invalid_uuid'


class NetworkEndpointNotFound(BadRequest):
    error_type = 'no_endpoint'
    error_code = 403


class MarkerNotFound(BadRequest):
    error_type = 'marker_not_found'


class ValueError(BadRequest):
    error_type = 'value_error'


class InvalidMarker(BadRequest):
    error_type = 'invalid_marker'


class InvalidSortDir(BadRequest):
    error_type = 'invalid_sort_dir'


class InvalidLimit(BadRequest):
    error_type = 'invalid_limit'


class InvalidSortKey(BadRequest):
    error_type = 'invalid_sort_key'


class InvalidJson(BadRequest):
    error_type = 'invalid_json'


class InvalidOperation(BadRequest):
    error_code = 400
    error_type = 'invalid_operation'


class UnsupportedAccept(BadRequest):
    error_code = 406
    error_type = 'unsupported_accept'


class UnsupportedContentType(BadRequest):
    error_code = 415
    error_type = 'unsupported_content_type'


class InvalidDomainName(Base):
    error_code = 400
    error_type = 'invalid_domain_name'


class InvalidRecordSetName(Base):
    error_code = 400
    error_type = 'invalid_recordset_name'


class InvalidRecordSetLocation(Base):
    error_code = 400
    error_type = 'invalid_recordset_location'


class InvalidTTL(Base):
    error_code = 400
    error_type = 'invalid_ttl'


class DomainHasSubdomain(Base):
    error_code = 400
    error_type = 'domain_has_subdomain'


class Forbidden(Base):
    error_code = 403
    error_type = 'forbidden'


class Duplicate(Base):
    error_code = 409
    error_type = 'duplicate'


class DuplicateQuota(Duplicate):
    error_type = 'duplicate_quota'


class DuplicateServer(Duplicate):
    error_type = 'duplicate_server'


class DuplicateTsigKey(Duplicate):
    error_type = 'duplicate_tsigkey'


class DuplicateDomain(Duplicate):
    error_type = 'duplicate_domain'


class DuplicateTLD(Duplicate):
    error_type = 'duplicate_tld'


class DuplicateRecordSet(Duplicate):
    error_type = 'duplicate_recordset'


class DuplicateRecord(Duplicate):
    error_type = 'duplicate_record'


class DuplicateBlacklist(Duplicate):
    error_type = 'duplicate_blacklist'


class NotFound(Base):
    error_code = 404
    error_type = 'not_found'


class QuotaNotFound(NotFound):
    error_type = 'quota_not_found'


class ServerNotFound(NotFound):
    error_type = 'server_not_found'


class TsigKeyNotFound(NotFound):
    error_type = 'tsigkey_not_found'


class BlacklistNotFound(NotFound):
    error_type = 'blacklist_not_found'


class DomainNotFound(NotFound):
    error_type = 'domain_not_found'


class TLDNotFound(NotFound):
    error_type = 'tld_not_found'


class RecordSetNotFound(NotFound):
    error_type = 'recordset_not_found'


class RecordNotFound(NotFound):
    error_type = 'record_not_found'


class LastServerDeleteNotAllowed(BadRequest):
    error_type = 'last_server_delete_not_allowed'


class ResourceNotFound(NotFound):
    # TODO(kiall): Should this be extending NotFound??
    pass

########NEW FILE########
__FILENAME__ = base
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.context import DesignateContext


# Decorators for actions
def args(*args, **kwargs):
    def _decorator(func):
        func.__dict__.setdefault('args', []).insert(0, (args, kwargs))
        return func
    return _decorator


def name(name):
    """
    Give a command a alternate name
    """
    def _decorator(func):
        func.__dict__['_cmd_name'] = name
        return func
    return _decorator


class Commands(object):
    def __init__(self):
        self.context = DesignateContext.get_admin_context(
            request_id='designate-manage')

########NEW FILE########
__FILENAME__ = database
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import os

from migrate.exceptions import (DatabaseAlreadyControlledError,
                                DatabaseNotControlledError)
from migrate.versioning import api as versioning_api
from designate.openstack.common import log as logging
from oslo.config import cfg
from designate.manage import base


LOG = logging.getLogger(__name__)
REPOSITORY = os.path.abspath(os.path.join(os.path.dirname(__file__), '..',
                                          'storage', 'impl_sqlalchemy',
                                          'migrate_repo'))
cfg.CONF.import_opt('database_connection', 'designate.storage.impl_sqlalchemy',
                    group='storage:sqlalchemy')


class DatabaseCommands(base.Commands):
    def init(self):
        url = cfg.CONF['storage:sqlalchemy'].database_connection

        try:
            LOG.info('Attempting to initialize database')
            versioning_api.version_control(url=url, repository=REPOSITORY)
            LOG.info('Database initialized successfully')
        except DatabaseAlreadyControlledError:
            raise Exception('Database already initialized')

    @base.args('--version', metavar='<version>', help="Database version")
    def sync(self, version=None):
        url = cfg.CONF['storage:sqlalchemy'].database_connection

        if not os.path.exists(REPOSITORY):
            raise Exception('Migration Repository Not Found')

        try:
            target_version = int(version) if version else None

            current_version = versioning_api.db_version(url=url,
                                                        repository=REPOSITORY)
        except DatabaseNotControlledError:
            raise Exception('Database not yet initialized')

        LOG.info("Attempting to synchronize database from version "
                 "'%s' to '%s'",
                 current_version,
                 target_version if target_version is not None else "latest")

        if target_version and target_version < current_version:
            versioning_api.downgrade(url=url, repository=REPOSITORY,
                                     version=version)
        else:
            versioning_api.upgrade(url=url, repository=REPOSITORY,
                                   version=version)

        LOG.info('Database synchronized successfully')

    def version(self):
        url = cfg.CONF['storage:sqlalchemy'].database_connection

        current = versioning_api.db_version(url=url, repository=REPOSITORY)
        latest = versioning_api.version(repository=REPOSITORY).value

        print("Current: %s Latest: %s" % (current, latest))

########NEW FILE########
__FILENAME__ = powerdns
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import os
from migrate.exceptions import (DatabaseAlreadyControlledError,
                                DatabaseNotControlledError)
from migrate.versioning import api as versioning_api
from designate.openstack.common import log as logging
from oslo.config import cfg
from designate.manage import base


LOG = logging.getLogger(__name__)
REPOSITORY = os.path.abspath(os.path.join(os.path.dirname(__file__), '..',
                                          'backend', 'impl_powerdns',
                                          'migrate_repo'))
cfg.CONF.import_opt('database_connection', 'designate.backend.impl_powerdns',
                    group='backend:powerdns')


class DatabaseCommands(base.Commands):
    def init(self):
        url = cfg.CONF['backend:powerdns'].database_connection

        if not os.path.exists(REPOSITORY):
            raise Exception('Migration Repository Not Found')

        try:
            LOG.info('Attempting to initialize PowerDNS database')
            versioning_api.version_control(url=url, repository=REPOSITORY)
            LOG.info('PowerDNS database initialized successfully')
        except DatabaseAlreadyControlledError:
            raise Exception('PowerDNS Database already initialized')

    @base.args('--version', metavar='<version>', help="Database version")
    def sync(self, version=None):
        url = cfg.CONF['backend:powerdns'].database_connection

        if not os.path.exists(REPOSITORY):
            raise Exception('Migration Repository Not Found')

        try:
            target_version = int(version) if version is not None else None

            current_version = versioning_api.db_version(url=url,
                                                        repository=REPOSITORY)
        except DatabaseNotControlledError:
            raise Exception('PowerDNS database not yet initialized')

        LOG.info("Attempting to synchronize PowerDNS database from version "
                 "'%s' to '%s'",
                 current_version,
                 target_version if target_version is not None else "latest")

        if target_version and target_version < current_version:
            versioning_api.downgrade(url=url, repository=REPOSITORY,
                                     version=version)
        else:
            versioning_api.upgrade(url=url, repository=REPOSITORY,
                                   version=version)

        LOG.info('PowerDNS database synchronized successfully')

    def version(self):
        url = cfg.CONF['backend:powerdns'].database_connection

        print(versioning_api.db_version(url=url, repository=REPOSITORY))

########NEW FILE########
__FILENAME__ = tlds
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
import csv
import os

from oslo.config import cfg

from designate import exceptions
from designate import rpc
from designate.central import rpcapi as central_rpcapi
from designate.openstack.common import log as logging
from designate.manage import base
from designate.schema import format

LOG = logging.getLogger(__name__)


class TLDCommands(base.Commands):
    """
    Import TLDs to Designate.  The format of the command is:
    designate-manage import-tlds --input-file="<complete path to input file>"
    [--delimiter="delimiter character"]
    The TLDs need to be provided in a csv file.  Each line in
    this file contains a TLD entry followed by an optional description.
    By default the delimiter character is ","

    If any lines in the input file result in an error, the program
    continues to the next line.

    On completion the output is reported (LOG.info) in the format:
    Number of tlds added: <number>

    If there are any errors, they are reported (LOG.err) in the format:
    <Error> --> <Line causing the error>

    <Error> can be one of the following:
    DuplicateTLD - This occurs if the TLD is already present.
    InvalidTLD - This occurs if the TLD does not conform to the TLD schema.
    InvalidDescription - This occurs if the description does not conform to
        the description schema
    InvalidLine - This occurs if the line contains more than 2 fields.
    """

    def __init__(self):
        super(TLDCommands, self).__init__()
        rpc.init(cfg.CONF)
        self.central_api = central_rpcapi.CentralAPI()

    # The dictionary function __str__() does not list the fields in any
    # particular order.
    # It makes it easier to read if the tld_name is printed first, so we have
    # a separate function to do the necessary conversions
    def _convert_tld_dict_to_str(self, line):
        keys = ['name', 'description', 'extra_fields']
        values = [line['name'],
                  line['description'],
                  line['extra_fields'] if 'extra_fields' in line else None]
        dict_str = ''.join([str.format("'{0}': '{1}', ", keys[x], values[x])
                            for x in range(len(values)) if values[x]])

        return '{' + dict_str.rstrip(' ,') + '}'

    # validates and returns the number of tlds added - either 0 in case of
    # any errors or 1 if everything is successful
    # In case of errors, the error message is appended to the list error_lines
    def _validate_and_create_tld(self, line, error_lines):
        # validate the tld name
        if not format.is_tldname(line['name']):
            error_lines.append("InvalidTLD --> " +
                               self._convert_tld_dict_to_str(line))
            return 0
        # validate the description if there is one
        elif (line['description']) and (len(line['description']) > 160):
            error_lines.append("InvalidDescription --> " +
                               self._convert_tld_dict_to_str(line))

            return 0
        else:
            try:
                self.central_api.create_tld(self.context, values=line)
                return 1
            except exceptions.DuplicateTLD:
                error_lines.append("DuplicateTLD --> " +
                                   self._convert_tld_dict_to_str(line))
                return 0

    @base.name('import')
    @base.args('--input_file', help="Input file path containing TLDs",
               default=None, required=True, type=str)
    @base.args('--delimiter',
               help="delimiter between fields in the input file",
               default=',', type=str)
    def from_file(self, input_file=None, delimiter=None):
        input_file = str(input_file) if input_file is not None else None

        if not os.path.exists(input_file):
            raise Exception('TLD Input file Not Found')

        LOG.info("Importing TLDs from %s", input_file)

        error_lines = []
        tlds_added = 0

        with open(input_file) as inf:
            csv.register_dialect('import-tlds', delimiter=str(delimiter))
            reader = csv.DictReader(inf,
                                    fieldnames=['name', 'description'],
                                    restkey='extra_fields',
                                    dialect='import-tlds')
            for line in reader:
                # check if there are more than 2 fields
                if 'extra_fields' in line:
                    error_lines.append("InvalidLine --> " +
                                       self._convert_tld_dict_to_str(line))
                else:
                    tlds_added += self._validate_and_create_tld(line,
                                                                error_lines)

        LOG.info("Number of tlds added: %d", tlds_added)

        errors = len(error_lines)
        if errors > 0:
            LOG.error("Number of errors: %d", errors)
            # Sorting the errors and printing them so that it is easier to
            # read the errors
            LOG.error("Error Lines:\n%s", '\n'.join(sorted(error_lines)))

########NEW FILE########
__FILENAME__ = base
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Endre Karlson <endre.karlson@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from dns import reversename
from oslo.config import cfg

from designate import exceptions
from designate.plugin import DriverPlugin
from designate.openstack.common import log as logging


LOG = logging.getLogger(__name__)


class NetworkAPI(DriverPlugin):
    """
    Base API
    """
    __plugin_ns__ = 'designate.network_api'
    __plugin_type__ = 'network_api'

    def _endpoints(self, service_catalog=None, service_type=None,
                   endpoint_type='publicURL', config_section=None,
                   region=None):
        if service_catalog is not None and len(service_catalog):
            endpoints = self._endpoints_from_catalog(
                service_catalog, service_type, endpoint_type,
                region=region)
        elif config_section is not None:
            endpoints = []
            for u in cfg.CONF[config_section].endpoints:
                e_region, e = u.split('|')
                # Filter if region is given
                if (e_region and region) and e_region != region:
                    continue
                endpoints.append((e, e_region))

            if not endpoints:
                msg = 'Endpoints are not configured'
                raise exceptions.ConfigurationError(msg)
        else:
            msg = 'No service_catalog and no configured endpoints'
            raise exceptions.ConfigurationError(msg)

        LOG.debug('Returning endpoints: %s' % endpoints)
        return endpoints

    def _endpoints_from_catalog(self, service_catalog, service_type,
                                endpoint_type, region=None):
        """
        Return the endpoints for the given service from the context's sc
        or lookup towards the configured keystone.

        return [('http://endpoint', 'region')]
        """
        urls = []
        for svc in service_catalog:
            if svc['type'] != service_type:
                continue
            for url in svc['endpoints']:
                if endpoint_type in url:
                    if region is not None and url['region'] != region:
                        continue
                    urls.append((url[endpoint_type], url['region']))
        if not urls:
            raise exceptions.NetworkEndpointNotFound
        return urls

    def list_floatingips(self, context, region=None):
        """
        List Floating IPs.

        Should return something like:

        [{
            'address': '<ip address'>,
            'region': '<region where this belongs>',
            'id': '<id of the FIP>'
        }]
        """
        raise NotImplementedError

    @staticmethod
    def address_zone(address):
        """
        Get the zone a address belongs to.
        """
        parts = reversed(address.split('.')[:-1])
        return '%s.in-addr.arpa.' % ".".join(parts)

    @staticmethod
    def address_name(address):
        """
        Get the name for the address
        """
        return reversename.from_address(address).to_text()

########NEW FILE########
__FILENAME__ = fake
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Endre Karlson <endre.karlson@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import uuid
from designate.openstack.common import log
from designate.network_api.base import NetworkAPI


LOG = log.getLogger(__name__)

POOL = dict([(str(uuid.uuid4()), '192.168.2.%s' % i) for i in xrange(0, 254)])
ALLOCATIONS = {}


def _format_floatingip(id_, address):
    return {
        'region': 'RegionOne',
        'address': address,
        'id': id_
    }


def allocate_floatingip(tenant_id, floatingip_id=None):
    """
    Allocates a floating ip from the pool to the tenant.
    """
    ALLOCATIONS.setdefault(tenant_id, {})

    id_ = floatingip_id or POOL.keys()[0]

    ALLOCATIONS[tenant_id][id_] = POOL.pop(id_)
    values = _format_floatingip(id_, ALLOCATIONS[tenant_id][id_])
    LOG.debug("Allocated to id_ %s to %s - %s", id_, tenant_id, values)
    return values


def deallocate_floatingip(id_):
    """
    Deallocate a floatingip
    """
    LOG.debug('De-allocating %s' % id_)
    for tenant_id, allocated in ALLOCATIONS.items():
        if id_ in allocated:
            POOL[id_] = allocated.pop(id_)
            break
    else:
        raise KeyError('No such FloatingIP %s' % id_)


def reset_floatingips():
    LOG.debug('Resetting any allocations.')
    for tenant_id, allocated in ALLOCATIONS.items():
        for key, value in allocated.items():
            POOL[key] = allocated.pop(key)


class FakeNetworkAPI(NetworkAPI):
    __plugin_name__ = 'fake'

    def list_floatingips(self, context, region=None):
        if context.is_admin:
            data = []
            for tenant_id, allocated in ALLOCATIONS.items():
                data.extend(allocated.items())
        else:
            data = ALLOCATIONS.get(context.tenant, {}).items()

        formatted = [_format_floatingip(k, v) for k, v in data]
        LOG.debug('Returning %i FloatingIPs: %s', len(formatted), formatted)
        return formatted

########NEW FILE########
__FILENAME__ = neutron
# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
# Copyright 2012 OpenStack Foundation
# All Rights Reserved
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
# Copied partially from nova

from neutronclient.v2_0 import client as clientv20
from neutronclient.common import exceptions as neutron_exceptions
from oslo.config import cfg

from designate import exceptions

from designate.openstack.common import log as logging
from designate.openstack.common import threadgroup
from designate.network_api.base import NetworkAPI


CONF = cfg.CONF
LOG = logging.getLogger(__name__)


neutron_opts = [
    cfg.ListOpt('endpoints',
                help='URL to use if None in the ServiceCatalog that is '
                'passed by the requrest context. Format: <region>|<url>'),
    cfg.StrOpt('endpoint_type', default='publicURL',
               help="Endpoint type to use"),
    cfg.IntOpt('timeout',
               default=30,
               help='timeout value for connecting to neutron in seconds'),
    cfg.StrOpt('admin_username',
               help='username for connecting to neutron in admin context'),
    cfg.StrOpt('admin_password',
               help='password for connecting to neutron in admin context',
               secret=True),
    cfg.StrOpt('admin_tenant_name',
               help='tenant name for connecting to neutron in admin context'),
    cfg.StrOpt('auth_url',
               help='auth url for connecting to neutron in admin context'),
    cfg.BoolOpt('insecure',
                default=False,
                help='if set, ignore any SSL validation issues'),
    cfg.StrOpt('auth_strategy',
               default='keystone',
               help='auth strategy for connecting to '
                    'neutron in admin context'),
    cfg.StrOpt('ca_certificates_file',
               help='Location of ca certificates file to use for '
                    'neutron client requests.'),
]

cfg.CONF.register_opts(neutron_opts, group='network_api:neutron')


def get_client(context, endpoint):
    params = {
        'endpoint_url': endpoint,
        'timeout': CONF['network_api:neutron'].timeout,
        'insecure': CONF['network_api:neutron'].insecure,
        'ca_cert': CONF['network_api:neutron'].ca_certificates_file,
    }

    if context.auth_token:
        params['token'] = context.auth_token
        params['auth_strategy'] = None
    elif CONF['network_api:neutron'].admin_username is not None:
        params['username'] = CONF['network_api:neutron'].admin_username
        params['tenant_name'] = CONF['network_api:neutron'].admin_tenant_name
        params['password'] = CONF['network_api:neutron'].admin_password
        params['auth_url'] = CONF['network_api:neutron'].admin_auth_url
        params['auth_strategy'] = CONF['network_api:neutron'].auth_strategy
    return clientv20.Client(**params)


class NeutronNetworkAPI(NetworkAPI):
    """
    Interact with the Neutron API
    """
    __plugin_name__ = 'neutron'

    def list_floatingips(self, context, region=None):
        """
        Get floating ips based on the current context from Neutron
        """
        endpoints = self._endpoints(
            service_catalog=context.service_catalog,
            service_type='network',
            endpoint_type=CONF['network_api:neutron'].endpoint_type,
            config_section='network_api:neutron',
            region=region)

        tg = threadgroup.ThreadGroup()

        failed = []
        data = []

        def _call(endpoint, region, *args, **kw):
            client = get_client(context, endpoint=endpoint)
            LOG.debug("Attempting to fetch FloatingIPs from %s @ %s",
                      endpoint, region)
            try:
                fips = client.list_floatingips(*args, **kw)
            except neutron_exceptions.Unauthorized as e:
                # NOTE: 401 might be that the user doesn't have neutron
                # activated in a particular region, we'll just log the failure
                # and go on with our lives.
                msg = "Calling Neutron resulted in a 401, please investigate."
                LOG.warning(msg)
                LOG.exception(e)
                return
            except Exception as e:
                LOG.error('Failed calling Neutron %s - %s', region, endpoint)
                LOG.exception(e)
                failed.append((e, endpoint, region))
                return

            for fip in fips['floatingips']:
                data.append({
                    'id': fip['id'],
                    'address': fip['floating_ip_address'],
                    'region': region
                })

            LOG.debug("Added %i FloatingIPs from %s @ %s", len(data),
                      endpoint, region)

        for endpoint, region in endpoints:
            tg.add_thread(_call, endpoint, region,
                          tenant_id=context.tenant)
        tg.wait()

        # NOTE: Sadly tg code doesn't give us a good way to handle failures.
        if failed:
            msg = 'Failed retrieving FLoatingIPs from Neutron in %s' % \
                ", ".join(['%s - %s' % (i[1], i[2]) for i in failed])
            raise exceptions.NeutronCommunicationFailure(msg)
        return data

########NEW FILE########
__FILENAME__ = notifications
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# Copied: nova.notifications

from oslo.config import cfg

from designate.openstack.common import log as logging
from designate import rpc

LOG = logging.getLogger(__name__)

notify_opts = [
    cfg.BoolOpt('notify_api_faults', default=False,
                help='Send notifications if there\'s a failure in the API.')
]

CONF = cfg.CONF
CONF.register_opts(notify_opts)


def send_api_fault(context, url, status, exception):
    """Send an api.fault notification."""

    if not CONF.notify_api_faults:
        return

    payload = {'url': url, 'exception': str(exception), 'status': status}

    rpc.get_notifier('api').error(context, 'dns.api.fault', payload)

########NEW FILE########
__FILENAME__ = base
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
# Author: Endre Karlson <endre.karlson@bouvet.no>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import abc
from oslo.config import cfg
from designate import exceptions
from designate.openstack.common import log as logging
from designate.central import rpcapi as central_rpcapi
from designate.context import DesignateContext
from designate.plugin import ExtensionPlugin


LOG = logging.getLogger(__name__)


def get_ip_data(addr_dict):
    ip = addr_dict['address']
    version = addr_dict['version']

    data = {
        'ip_version': version
    }

    # TODO(endre): Add v6 support
    if version == 4:
        data['ip_address'] = ip.replace('.', '-')
        ip_data = ip.split(".")
        for i in [0, 1, 2, 3]:
            data["octet%s" % i] = ip_data[i]
    return data


class NotificationHandler(ExtensionPlugin):
    """ Base class for notification handlers """
    __plugin_ns__ = 'designate.notification.handler'
    __plugin_type__ = 'handler'

    def __init__(self, *args, **kw):
        super(NotificationHandler, self).__init__(*args, **kw)
        self.central_api = central_rpcapi.CentralAPI()

    @abc.abstractmethod
    def get_exchange_topics(self):
        """
        Returns a tuple of (exchange, list(topics)) this handler wishes
        to receive notifications from.
        """

    @abc.abstractmethod
    def get_event_types(self):
        """
        Returns a list of event types this handler is capable of  processing
        """

    @abc.abstractmethod
    def process_notification(self, context, event_type, payload):
        """ Processes a given notification """

    def get_domain(self, domain_id):
        """
        Return the domain for this context
        """
        context = DesignateContext.get_admin_context(all_tenants=True)
        return self.central_api.get_domain(context, domain_id)

    def _find_or_create_recordset(self, context, domain_id, name, type,
                                  ttl=None):
        try:
            recordset = self.central_api.find_recordset(context, {
                'domain_id': domain_id,
                'name': name,
                'type': type,
            })
        except exceptions.RecordSetNotFound:
            recordset = self.central_api.create_recordset(context, domain_id, {
                'name': name,
                'type': type,
                'ttl': ttl,
            })

        return recordset


class BaseAddressHandler(NotificationHandler):
    default_format = '%(octet0)s-%(octet1)s-%(octet2)s-%(octet3)s.%(domain)s'

    def _get_format(self):
        return cfg.CONF[self.name].get('format') or self.default_format

    def _create(self, addresses, extra, managed=True,
                resource_type=None, resource_id=None):
        """
        Create a a record from addresses

        :param addresses: Address objects like
                          {'version': 4, 'ip': '10.0.0.1'}
        :param extra: Extra data to use when formatting the record
        :param managed: Is it a managed resource
        :param resource_type: The managed resource type
        :param resource_id: The managed resource ID
        """
        LOG.debug('Using DomainID: %s' % cfg.CONF[self.name].domain_id)
        domain = self.get_domain(cfg.CONF[self.name].domain_id)
        LOG.debug('Domain: %r' % domain)

        data = extra.copy()
        LOG.debug('Event data: %s' % data)
        data['domain'] = domain['name']

        context = DesignateContext.get_admin_context(all_tenants=True)

        for addr in addresses:
            event_data = data.copy()
            event_data.update(get_ip_data(addr))

            recordset_values = {
                'domain_id': domain['id'],
                'name': self._get_format() % event_data,
                'type': 'A' if addr['version'] == 4 else 'AAAA'}

            recordset = self._find_or_create_recordset(
                context, **recordset_values)

            record_values = {
                'data': addr['address']}

            if managed:
                record_values.update({
                    'managed': managed,
                    'managed_plugin_name': self.get_plugin_name(),
                    'managed_plugin_type': self.get_plugin_type(),
                    'managed_resource_type': resource_type,
                    'managed_resource_id': resource_id})

            LOG.debug('Creating record in %s / %s with values %r',
                      domain['id'], recordset['id'], record_values)
            self.central_api.create_record(context, domain['id'],
                                           recordset['id'],
                                           record_values)

    def _delete(self, managed=True, resource_id=None, resource_type='instance',
                criterion={}):
        """
        Handle a generic delete of a fixed ip within a domain

        :param criterion: Criterion to search and destroy records
        """
        context = DesignateContext.get_admin_context(all_tenants=True)

        criterion.update({'domain_id': cfg.CONF[self.name].domain_id})

        if managed:
            criterion.update({
                'managed': managed,
                'managed_plugin_name': self.get_plugin_name(),
                'managed_plugin_type': self.get_plugin_type(),
                'managed_resource_id': resource_id,
                'managed_resource_type': resource_type
            })

        records = self.central_api.find_records(context, criterion)

        for record in records:
            LOG.debug('Deleting record %s' % record['id'])

            self.central_api.delete_record(context,
                                           cfg.CONF[self.name].domain_id,
                                           record['recordset_id'],
                                           record['id'])

########NEW FILE########
__FILENAME__ = neutron
# Copyright 2012 Bouvet ASA
#
# Author: Endre Karlson <endre.karlson@bouvet.no>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from oslo.config import cfg
from designate.openstack.common import log as logging
from designate.notification_handler.base import BaseAddressHandler

LOG = logging.getLogger(__name__)

cfg.CONF.register_group(cfg.OptGroup(
    name='handler:neutron_floatingip',
    title="Configuration for Neutron Notification Handler"
))

cfg.CONF.register_opts([
    cfg.ListOpt('notification-topics', default=['monitor']),
    cfg.StrOpt('control-exchange', default='neutron'),
    cfg.StrOpt('domain-id', default=None),
    cfg.StrOpt('format', default=None)
], group='handler:neutron_floatingip')


class NeutronFloatingHandler(BaseAddressHandler):
    """ Handler for Neutron's notifications """
    __plugin_name__ = 'neutron_floatingip'

    def get_exchange_topics(self):
        exchange = cfg.CONF[self.name].control_exchange

        topics = [topic for topic in cfg.CONF[self.name].notification_topics]

        return (exchange, topics)

    def get_event_types(self):
        return [
            'floatingip.update.end',
            'floatingip.delete.start'
        ]

    def process_notification(self, context, event_type, payload):
        LOG.debug('%s received notification - %s',
                  self.get_canonical_name(), event_type)

        # FIXME: Neutron doesn't send ipv in the payload, should maybe
        # determine this?
        if event_type not in self.get_event_types():
            msg = '%s received an invalid event type %s' % (
                self, event_type)
            raise ValueError(msg)

        if event_type.startswith('floatingip.delete'):
            self._delete(resource_id=payload['floatingip_id'],
                         resource_type='floatingip')
        elif event_type.startswith('floatingip.update'):
            if payload['floatingip']['fixed_ip_address']:
                address = {
                    'version': 4,
                    'address': payload['floatingip']['floating_ip_address']}
                self._create([address], payload,
                             resource_id=payload['floatingip']['id'],
                             resource_type='floatingip')
            elif not payload['floatingip']['fixed_ip_address']:
                self._delete(resource_id=payload['floatingip']['id'],
                             resource_type='floatingip')

########NEW FILE########
__FILENAME__ = nova
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from oslo.config import cfg
from designate.openstack.common import log as logging
from designate.notification_handler.base import BaseAddressHandler

LOG = logging.getLogger(__name__)

cfg.CONF.register_group(cfg.OptGroup(
    name='handler:nova_fixed',
    title="Configuration for Nova Notification Handler"
))

cfg.CONF.register_opts([
    cfg.ListOpt('notification-topics', default=['monitor']),
    cfg.StrOpt('control-exchange', default='nova'),
    cfg.StrOpt('domain-id', default=None),
    cfg.StrOpt('format', default=None)
], group='handler:nova_fixed')


class NovaFixedHandler(BaseAddressHandler):
    """ Handler for Nova's notifications """
    __plugin_name__ = 'nova_fixed'

    def get_exchange_topics(self):
        exchange = cfg.CONF[self.name].control_exchange

        topics = [topic for topic in cfg.CONF[self.name].notification_topics]

        return (exchange, topics)

    def get_event_types(self):
        return [
            'compute.instance.create.end',
            'compute.instance.delete.start',
        ]

    def process_notification(self, context, event_type, payload):
        LOG.debug('NovaFixedHandler received notification - %s' % event_type)

        if event_type == 'compute.instance.create.end':
            self._create(payload['fixed_ips'], payload,
                         resource_id=payload['instance_id'],
                         resource_type='instance')

        elif event_type == 'compute.instance.delete.start':
            self._delete(resource_id=payload['instance_id'],
                         resource_type='instance')
        else:
            raise ValueError('NovaFixedHandler received an invalid event type')

########NEW FILE########
__FILENAME__ = base
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


class BaseObject(object):
    BASE_FIELDS = ('id', 'created_at', 'updated_at', 'version')
    FIELDS = []

    @classmethod
    def from_sqla(cls, obj):
        """
        Convert from a SQLA Model to a Designate Object
        Eventually, when we move from SQLA ORM to SQLA Core, this can be
        removed.
        """
        fields = {}
        fieldnames = []
        fieldnames += cls.FIELDS
        fieldnames += cls.BASE_FIELDS

        for fieldname in fieldnames:
            if hasattr(obj, fieldname):
                fields[fieldname] = getattr(obj, fieldname)

        return cls(**fields)

    @classmethod
    def from_primitive(cls, primitive):
        """
        Construct an object from primitive types.  This is used while
        deserializing the object.
        NOTE: Currently all the designate objects contain primitive types that
        do not need special handling.  If this changes we need to modify this
        function.
        """
        fields = primitive['designate_object.data']
        return cls(**fields)

    def __init__(self, **kwargs):
        fieldnames = []
        fieldnames += self.FIELDS
        fieldnames += self.BASE_FIELDS
        self._fieldnames = []

        for name, value in kwargs.items():
            if name in fieldnames:
                # We set _fieldnames to only the fields that are set.
                # This is useful in objects like Tenant which do not have
                # all the BASE_FIELDS set.
                # Only the fields in _fieldnames are displayed.
                self._fieldnames.append(name)
                setattr(self, name, value)
            else:
                raise TypeError("'%s' is an invalid keyword argument" % name)

    def __setitem__(self, key, value):
        setattr(self, key, value)

    def __getitem__(self, key):
        return getattr(self, key)

    def __iter__(self):
        self._i = iter(self._fieldnames)
        return self

    def __contains__(self, item):
        # Some of the tests like the following one need this function to
        # succeed. Hence we implement this function.
        #    self.assertIn('status', domain)
        return item in self._fieldnames

    def next(self):
        n = self._i.next()
        return n, getattr(self, n)

    def update(self, values):
        """ Make the model object behave like a dict """
        for k, v in values.iteritems():
            setattr(self, k, v)

    def iteritems(self):
        """
        Make the model object behave like a dict.

        Includes attributes from joins.
        """
        local = dict(self)
        joined = dict([(k, v) for k, v in self.__dict__.iteritems()
                      if not k[0] == '_'])
        local.update(joined)
        return local.iteritems()

    def get(self, key, default=None):
        return getattr(self, key, default)

    def to_primitive(self):
        """
        Convert the object to primitive types so that the object can be
        serialized.
        NOTE: Currently all the designate objects contain primitive types that
        do not need special handling.  If this changes we need to modify this
        function.
        """
        primitive = {}
        class_name = self.__class__.__name__
        if self.__module__:
            class_name = self.__module__ + '.' + self.__class__.__name__
        primitive['designate_object.name'] = class_name
        primitive['designate_object.data'] = dict(self)
        return primitive

########NEW FILE########
__FILENAME__ = blacklist
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.base import BaseObject


class Blacklist(BaseObject):
    FIELDS = ['pattern', 'description']

########NEW FILE########
__FILENAME__ = domain
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.base import BaseObject


class Domain(BaseObject):
    FIELDS = ['tenant_id', 'name', 'email', 'ttl', 'refresh', 'retry',
              'expire', 'minimum', 'parent_domain_id', 'serial', 'description',
              'status']

########NEW FILE########
__FILENAME__ = quota
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.base import BaseObject


class Quota(BaseObject):
    FIELDS = ['tenant_id', 'resource', 'hard_limit']

########NEW FILE########
__FILENAME__ = record
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.base import BaseObject


class Record(BaseObject):
    RECORD_FIELDS = ['data', 'priority', 'domain_id', 'managed',
                     'managed_resource_type', 'managed_resource_id',
                     'managed_plugin_name', 'managed_plugin_type', 'hash',
                     'description', 'status', 'tenant_id', 'recordset_id',
                     'managed_tenant_id', 'managed_resource_region',
                     'managed_extra']
    RRDATA_FIELDS = []

    @classmethod
    def from_sqla(cls, obj):
        """
        Convert from a SQLA Model to a Designate Object
        Eventually, when we move from SQLA ORM to SQLA Core, this can be
        removed.
        This overrides the method for Records
        """
        cls.FIELDS = cls.RECORD_FIELDS + cls.RRDATA_FIELDS
        return super(Record, cls).from_sqla(obj)

    def __init__(self, **kwargs):
        self.FIELDS = self.RECORD_FIELDS + self.RRDATA_FIELDS
        super(Record, self).__init__(**kwargs)
        self.from_text()
        self.to_text()

    def from_text(self):
        """
        self.'data' contains the text version of the rdata fields.
        This function splits self.data and puts it into the rdata fields.
        """
        # If length is one, then do not split - this is needed for SPF and TXT
        # records.
        if len(self.RRDATA_FIELDS) == 1:
            setattr(self, self.RRDATA_FIELDS[0], self.data)
        else:
            rdata_values = self.data.split()
            if len(self.RRDATA_FIELDS) != len(rdata_values):
                raise TypeError("Incorrect number of values. Expected: %s."
                                " Got %s" % (self.RRDATA_FIELDS, rdata_values))
            index = 0
            # TODO(vinod): Currently all the attributes are set as strings.
            # Once the schemas for the records are defined, the various fields
            # need to be transformed according to the schema.
            for rdata_field in self.RRDATA_FIELDS:
                setattr(self, rdata_field, rdata_values[index])
                index += 1

    def to_text(self):
        """
        This function joins the rdata fields and puts it into self.data
        """
        rdata_values = []
        for rdata_field in self.RRDATA_FIELDS:
            rdata_values.append(getattr(self, rdata_field))
        self.data = ' '.join(rdata_values)

########NEW FILE########
__FILENAME__ = recordset
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.base import BaseObject


class RecordSet(BaseObject):
    FIELDS = ['tenant_id', 'domain_id', 'name', 'type', 'ttl', 'description']

########NEW FILE########
__FILENAME__ = rrdata_a
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.record import Record


class RRData_A(Record):
    """
    A Resource Record Type
    Defined in: RFC1035
    """
    RRDATA_FIELDS = ['address']

    # The record type is defined in the RFC. This will be used when the record
    # is sent by mini-dns.
    RECORD_TYPE = 1

########NEW FILE########
__FILENAME__ = rrdata_aaaa
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.record import Record


class RRData_AAAA(Record):
    """
    AAAA Resource Record Type
    Defined in: RFC3596
    """
    RRDATA_FIELDS = ['address']

    # The record type is defined in the RFC. This will be used when the record
    # is sent by mini-dns.
    RECORD_TYPE = 28

########NEW FILE########
__FILENAME__ = rrdata_cname
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.record import Record


class RRData_CNAME(Record):
    """
    CNAME Resource Record Type
    Defined in: RFC1035
    """
    RRDATA_FIELDS = ['cname']

    # The record type is defined in the RFC. This will be used when the record
    # is sent by mini-dns.
    RECORD_TYPE = 5

########NEW FILE########
__FILENAME__ = rrdata_mx
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.record import Record


class RRData_MX(Record):
    """
    MX Resource Record Type
    Defined in: RFC1035
    """
    # priority is maintained separately for MX records and not in 'data'
    RRDATA_FIELDS = ['exchange']

    # The record type is defined in the RFC. This will be used when the record
    # is sent by mini-dns.
    RECORD_TYPE = 15

########NEW FILE########
__FILENAME__ = rrdata_ns
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.record import Record


class RRData_NS(Record):
    """
    NS Resource Record Type
    Defined in: RFC1035
    """
    RRDATA_FIELDS = ['nsdname']

    # The record type is defined in the RFC. This will be used when the record
    # is sent by mini-dns.
    RECORD_TYPE = 2

########NEW FILE########
__FILENAME__ = rrdata_ptr
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.record import Record


class RRData_PTR(Record):
    """
    PTR Resource Record Type
    Defined in: RFC1035
    """
    RRDATA_FIELDS = ['ptrdname']

    # The record type is defined in the RFC. This will be used when the record
    # is sent by mini-dns.
    RECORD_TYPE = 12

########NEW FILE########
__FILENAME__ = rrdata_soa
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.record import Record


class RRData_SOA(Record):
    """
    SOA Resource Record Type
    Defined in: RFC1035
    """
    RRDATA_FIELDS = ['mname', 'rname', 'serial', 'refresh', 'retry', 'expire',
                     'minimum']

    # The record type is defined in the RFC. This will be used when the record
    # is sent by mini-dns.
    RECORD_TYPE = 6

########NEW FILE########
__FILENAME__ = rrdata_spf
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.record import Record


class RRData_SPF(Record):
    """
    SPF Resource Record Type
    Defined in: RFC4408
    """
    RRDATA_FIELDS = ['txt-data']

    # The record type is defined in the RFC. This will be used when the record
    # is sent by mini-dns.
    RECORD_TYPE = 99

########NEW FILE########
__FILENAME__ = rrdata_srv
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.record import Record


class RRData_SRV(Record):
    """
    SRV Resource Record Type
    Defined in: RFC2782
    """
    # priority is maintained separately for SRV records and not in 'data'
    RRDATA_FIELDS = ['weight', 'port', 'target']

    # The record type is defined in the RFC. This will be used when the record
    # is sent by mini-dns.
    RECORD_TYPE = 33

########NEW FILE########
__FILENAME__ = rrdata_sshfp
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.record import Record


class RRData_SSHFP(Record):
    """
    SSHFP Resource Record Type
    Defined in: RFC4255
    """
    RRDATA_FIELDS = ['algorithm', 'fp_type', 'fingerprint']

    # The record type is defined in the RFC. This will be used when the record
    # is sent by mini-dns.
    RECORD_TYPE = 44

########NEW FILE########
__FILENAME__ = rrdata_txt
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.record import Record


class RRData_TXT(Record):
    """
    TXT Resource Record Type
    Defined in: RFC1035
    """
    RRDATA_FIELDS = ['txt-data']

    # The record type is defined in the RFC. This will be used when the record
    # is sent by mini-dns.
    RECORD_TYPE = 16

########NEW FILE########
__FILENAME__ = server
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.base import BaseObject


class Server(BaseObject):
    FIELDS = ['name']

########NEW FILE########
__FILENAME__ = tenant
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.base import BaseObject


class Tenant(BaseObject):
    FIELDS = ['domain_count', 'domains']

########NEW FILE########
__FILENAME__ = tld
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.base import BaseObject


class Tld(BaseObject):
    FIELDS = ['name', 'description']

########NEW FILE########
__FILENAME__ = tsigkey
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.objects.base import BaseObject


class TsigKey(BaseObject):
    FIELDS = ['name', 'algorithm', 'secret']

########NEW FILE########
__FILENAME__ = context
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Simple class that stores security context information in the web request.

Projects should subclass this class if they wish to enhance the request
context or provide additional information in their specific WSGI pipeline.
"""

import itertools
import uuid


def generate_request_id():
    return b'req-' + str(uuid.uuid4()).encode('ascii')


class RequestContext(object):

    """Helper class to represent useful information about a request context.

    Stores information about the security context under which the user
    accesses the system, as well as additional request information.
    """

    user_idt_format = '{user} {tenant} {domain} {user_domain} {p_domain}'

    def __init__(self, auth_token=None, user=None, tenant=None, domain=None,
                 user_domain=None, project_domain=None, is_admin=False,
                 read_only=False, show_deleted=False, request_id=None,
                 instance_uuid=None):
        self.auth_token = auth_token
        self.user = user
        self.tenant = tenant
        self.domain = domain
        self.user_domain = user_domain
        self.project_domain = project_domain
        self.is_admin = is_admin
        self.read_only = read_only
        self.show_deleted = show_deleted
        self.instance_uuid = instance_uuid
        if not request_id:
            request_id = generate_request_id()
        self.request_id = request_id

    def to_dict(self):
        user_idt = (
            self.user_idt_format.format(user=self.user or '-',
                                        tenant=self.tenant or '-',
                                        domain=self.domain or '-',
                                        user_domain=self.user_domain or '-',
                                        p_domain=self.project_domain or '-'))

        return {'user': self.user,
                'tenant': self.tenant,
                'domain': self.domain,
                'user_domain': self.user_domain,
                'project_domain': self.project_domain,
                'is_admin': self.is_admin,
                'read_only': self.read_only,
                'show_deleted': self.show_deleted,
                'auth_token': self.auth_token,
                'request_id': self.request_id,
                'instance_uuid': self.instance_uuid,
                'user_identity': user_idt}


def get_admin_context(show_deleted=False):
    context = RequestContext(None,
                             tenant=None,
                             is_admin=True,
                             show_deleted=show_deleted)
    return context


def get_context_from_function_and_args(function, args, kwargs):
    """Find an arg of type RequestContext and return it.

       This is useful in a couple of decorators where we don't
       know much about the function we're wrapping.
    """

    for arg in itertools.chain(kwargs.values(), args):
        if isinstance(arg, RequestContext):
            return arg

    return None


def is_user_context(context):
    """Indicates if the request context is a normal user."""
    if not context:
        return False
    if context.is_admin:
        return False
    if not context.user_id or not context.project_id:
        return False
    return True

########NEW FILE########
__FILENAME__ = api
# Copyright (c) 2013 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Multiple DB API backend support.

A DB backend module should implement a method named 'get_backend' which
takes no arguments.  The method can return any object that implements DB
API methods.
"""

import functools
import logging
import threading
import time

from designate.openstack.common.db import exception
from designate.openstack.common.gettextutils import _LE
from designate.openstack.common import importutils


LOG = logging.getLogger(__name__)


def safe_for_db_retry(f):
    """Enable db-retry for decorated function, if config option enabled."""
    f.__dict__['enable_retry'] = True
    return f


class wrap_db_retry(object):
    """Retry db.api methods, if DBConnectionError() raised

    Retry decorated db.api methods. If we enabled `use_db_reconnect`
    in config, this decorator will be applied to all db.api functions,
    marked with @safe_for_db_retry decorator.
    Decorator catchs DBConnectionError() and retries function in a
    loop until it succeeds, or until maximum retries count will be reached.
    """

    def __init__(self, retry_interval, max_retries, inc_retry_interval,
                 max_retry_interval):
        super(wrap_db_retry, self).__init__()

        self.retry_interval = retry_interval
        self.max_retries = max_retries
        self.inc_retry_interval = inc_retry_interval
        self.max_retry_interval = max_retry_interval

    def __call__(self, f):
        @functools.wraps(f)
        def wrapper(*args, **kwargs):
            next_interval = self.retry_interval
            remaining = self.max_retries

            while True:
                try:
                    return f(*args, **kwargs)
                except exception.DBConnectionError as e:
                    if remaining == 0:
                        LOG.exception(_LE('DB exceeded retry limit.'))
                        raise exception.DBError(e)
                    if remaining != -1:
                        remaining -= 1
                        LOG.exception(_LE('DB connection error.'))
                    # NOTE(vsergeyev): We are using patched time module, so
                    #                  this effectively yields the execution
                    #                  context to another green thread.
                    time.sleep(next_interval)
                    if self.inc_retry_interval:
                        next_interval = min(
                            next_interval * 2,
                            self.max_retry_interval
                        )
        return wrapper


class DBAPI(object):
    def __init__(self, backend_name, backend_mapping=None, lazy=False,
                 **kwargs):
        """Initialize the chosen DB API backend.

        :param backend_name: name of the backend to load
        :type backend_name: str

        :param backend_mapping: backend name -> module/class to load mapping
        :type backend_mapping: dict

        :param lazy: load the DB backend lazily on the first DB API method call
        :type lazy: bool

        Keyword arguments:

        :keyword use_db_reconnect: retry DB transactions on disconnect or not
        :type use_db_reconnect: bool

        :keyword retry_interval: seconds between transaction retries
        :type retry_interval: int

        :keyword inc_retry_interval: increase retry interval or not
        :type inc_retry_interval: bool

        :keyword max_retry_interval: max interval value between retries
        :type max_retry_interval: int

        :keyword max_retries: max number of retries before an error is raised
        :type max_retries: int

        """

        self._backend = None
        self._backend_name = backend_name
        self._backend_mapping = backend_mapping or {}
        self._lock = threading.Lock()

        if not lazy:
            self._load_backend()

        self.use_db_reconnect = kwargs.get('use_db_reconnect', False)
        self.retry_interval = kwargs.get('retry_interval', 1)
        self.inc_retry_interval = kwargs.get('inc_retry_interval', True)
        self.max_retry_interval = kwargs.get('max_retry_interval', 10)
        self.max_retries = kwargs.get('max_retries', 20)

    def _load_backend(self):
        with self._lock:
            if not self._backend:
                # Import the untranslated name if we don't have a mapping
                backend_path = self._backend_mapping.get(self._backend_name,
                                                         self._backend_name)
                backend_mod = importutils.import_module(backend_path)
                self._backend = backend_mod.get_backend()

    def __getattr__(self, key):
        if not self._backend:
            self._load_backend()

        attr = getattr(self._backend, key)
        if not hasattr(attr, '__call__'):
            return attr
        # NOTE(vsergeyev): If `use_db_reconnect` option is set to True, retry
        #                  DB API methods, decorated with @safe_for_db_retry
        #                  on disconnect.
        if self.use_db_reconnect and hasattr(attr, 'enable_retry'):
            attr = wrap_db_retry(
                retry_interval=self.retry_interval,
                max_retries=self.max_retries,
                inc_retry_interval=self.inc_retry_interval,
                max_retry_interval=self.max_retry_interval)(attr)

        return attr

########NEW FILE########
__FILENAME__ = exception
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""DB related custom exceptions."""

import six

from designate.openstack.common.gettextutils import _


class DBError(Exception):
    """Wraps an implementation specific exception."""
    def __init__(self, inner_exception=None):
        self.inner_exception = inner_exception
        super(DBError, self).__init__(six.text_type(inner_exception))


class DBDuplicateEntry(DBError):
    """Wraps an implementation specific exception."""
    def __init__(self, columns=[], inner_exception=None):
        self.columns = columns
        super(DBDuplicateEntry, self).__init__(inner_exception)


class DBDeadlock(DBError):
    def __init__(self, inner_exception=None):
        super(DBDeadlock, self).__init__(inner_exception)


class DBInvalidUnicodeParameter(Exception):
    message = _("Invalid Parameter: "
                "Unicode is not supported by the current database.")


class DbMigrationError(DBError):
    """Wraps migration specific exception."""
    def __init__(self, message=None):
        super(DbMigrationError, self).__init__(message)


class DBConnectionError(DBError):
    """Wraps connection specific exception."""
    pass

########NEW FILE########
__FILENAME__ = options
#  Licensed under the Apache License, Version 2.0 (the "License"); you may
#  not use this file except in compliance with the License. You may obtain
#  a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#  License for the specific language governing permissions and limitations
#  under the License.

import copy

from oslo.config import cfg


database_opts = [
    cfg.StrOpt('sqlite_db',
               deprecated_group='DEFAULT',
               default='designate.sqlite',
               help='The file name to use with SQLite'),
    cfg.BoolOpt('sqlite_synchronous',
                deprecated_group='DEFAULT',
                default=True,
                help='If True, SQLite uses synchronous mode'),
    cfg.StrOpt('backend',
               default='sqlalchemy',
               deprecated_name='db_backend',
               deprecated_group='DEFAULT',
               help='The backend to use for db'),
    cfg.StrOpt('connection',
               help='The SQLAlchemy connection string used to connect to the '
                    'database',
               secret=True,
               deprecated_opts=[cfg.DeprecatedOpt('sql_connection',
                                                  group='DEFAULT'),
                                cfg.DeprecatedOpt('sql_connection',
                                                  group='DATABASE'),
                                cfg.DeprecatedOpt('connection',
                                                  group='sql'), ]),
    cfg.StrOpt('mysql_sql_mode',
               default='TRADITIONAL',
               help='The SQL mode to be used for MySQL sessions. '
                    'This option, including the default, overrides any '
                    'server-set SQL mode. To use whatever SQL mode '
                    'is set by the server configuration, '
                    'set this to no value. Example: mysql_sql_mode='),
    cfg.IntOpt('idle_timeout',
               default=3600,
               deprecated_opts=[cfg.DeprecatedOpt('sql_idle_timeout',
                                                  group='DEFAULT'),
                                cfg.DeprecatedOpt('sql_idle_timeout',
                                                  group='DATABASE'),
                                cfg.DeprecatedOpt('idle_timeout',
                                                  group='sql')],
               help='Timeout before idle sql connections are reaped'),
    cfg.IntOpt('min_pool_size',
               default=1,
               deprecated_opts=[cfg.DeprecatedOpt('sql_min_pool_size',
                                                  group='DEFAULT'),
                                cfg.DeprecatedOpt('sql_min_pool_size',
                                                  group='DATABASE')],
               help='Minimum number of SQL connections to keep open in a '
                    'pool'),
    cfg.IntOpt('max_pool_size',
               default=None,
               deprecated_opts=[cfg.DeprecatedOpt('sql_max_pool_size',
                                                  group='DEFAULT'),
                                cfg.DeprecatedOpt('sql_max_pool_size',
                                                  group='DATABASE')],
               help='Maximum number of SQL connections to keep open in a '
                    'pool'),
    cfg.IntOpt('max_retries',
               default=10,
               deprecated_opts=[cfg.DeprecatedOpt('sql_max_retries',
                                                  group='DEFAULT'),
                                cfg.DeprecatedOpt('sql_max_retries',
                                                  group='DATABASE')],
               help='Maximum db connection retries during startup. '
                    '(setting -1 implies an infinite retry count)'),
    cfg.IntOpt('retry_interval',
               default=10,
               deprecated_opts=[cfg.DeprecatedOpt('sql_retry_interval',
                                                  group='DEFAULT'),
                                cfg.DeprecatedOpt('reconnect_interval',
                                                  group='DATABASE')],
               help='Interval between retries of opening a sql connection'),
    cfg.IntOpt('max_overflow',
               default=None,
               deprecated_opts=[cfg.DeprecatedOpt('sql_max_overflow',
                                                  group='DEFAULT'),
                                cfg.DeprecatedOpt('sqlalchemy_max_overflow',
                                                  group='DATABASE')],
               help='If set, use this value for max_overflow with sqlalchemy'),
    cfg.IntOpt('connection_debug',
               default=0,
               deprecated_opts=[cfg.DeprecatedOpt('sql_connection_debug',
                                                  group='DEFAULT')],
               help='Verbosity of SQL debugging information. 0=None, '
                    '100=Everything'),
    cfg.BoolOpt('connection_trace',
                default=False,
                deprecated_opts=[cfg.DeprecatedOpt('sql_connection_trace',
                                                   group='DEFAULT')],
                help='Add python stack traces to SQL as comment strings'),
    cfg.IntOpt('pool_timeout',
               default=None,
               deprecated_opts=[cfg.DeprecatedOpt('sqlalchemy_pool_timeout',
                                                  group='DATABASE')],
               help='If set, use this value for pool_timeout with sqlalchemy'),
    cfg.BoolOpt('use_db_reconnect',
                default=False,
                help='Enable the experimental use of database reconnect '
                     'on connection lost'),
    cfg.IntOpt('db_retry_interval',
               default=1,
               help='seconds between db connection retries'),
    cfg.BoolOpt('db_inc_retry_interval',
                default=True,
                help='Whether to increase interval between db connection '
                     'retries, up to db_max_retry_interval'),
    cfg.IntOpt('db_max_retry_interval',
               default=10,
               help='max seconds between db connection retries, if '
                    'db_inc_retry_interval is enabled'),
    cfg.IntOpt('db_max_retries',
               default=20,
               help='maximum db connection retries before error is raised. '
                    '(setting -1 implies an infinite retry count)'),
]

CONF = cfg.CONF
CONF.register_opts(database_opts, 'database')


def set_defaults(sql_connection, sqlite_db, max_pool_size=None,
                 max_overflow=None, pool_timeout=None):
    """Set defaults for configuration variables."""
    cfg.set_defaults(database_opts,
                     connection=sql_connection,
                     sqlite_db=sqlite_db)
    # Update the QueuePool defaults
    if max_pool_size is not None:
        cfg.set_defaults(database_opts,
                         max_pool_size=max_pool_size)
    if max_overflow is not None:
        cfg.set_defaults(database_opts,
                         max_overflow=max_overflow)
    if pool_timeout is not None:
        cfg.set_defaults(database_opts,
                         pool_timeout=pool_timeout)


def list_opts():
    """Returns a list of oslo.config options available in the library.

    The returned list includes all oslo.config options which may be registered
    at runtime by the library.

    Each element of the list is a tuple. The first element is the name of the
    group under which the list of elements in the second element will be
    registered. A group name of None corresponds to the [DEFAULT] group in
    config files.

    The purpose of this is to allow tools like the Oslo sample config file
    generator to discover the options exposed to users by this library.

    :returns: a list of (group_name, opts) tuples
    """
    return [('database', copy.deepcopy(database_opts))]

########NEW FILE########
__FILENAME__ = migration
# coding: utf-8
#
# Copyright (c) 2013 OpenStack Foundation
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# Base on code in migrate/changeset/databases/sqlite.py which is under
# the following license:
#
# The MIT License
#
# Copyright (c) 2009 Evan Rosson, Jan Dittberner, Domen Kožar
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.

import os
import re

from migrate.changeset import ansisql
from migrate.changeset.databases import sqlite
from migrate import exceptions as versioning_exceptions
from migrate.versioning import api as versioning_api
from migrate.versioning.repository import Repository
import sqlalchemy
from sqlalchemy.schema import UniqueConstraint

from designate.openstack.common.db import exception
from designate.openstack.common.gettextutils import _


def _get_unique_constraints(self, table):
    """Retrieve information about existing unique constraints of the table

    This feature is needed for _recreate_table() to work properly.
    Unfortunately, it's not available in sqlalchemy 0.7.x/0.8.x.

    """

    data = table.metadata.bind.execute(
        """SELECT sql
           FROM sqlite_master
           WHERE
               type='table' AND
               name=:table_name""",
        table_name=table.name
    ).fetchone()[0]

    UNIQUE_PATTERN = "CONSTRAINT (\w+) UNIQUE \(([^\)]+)\)"
    return [
        UniqueConstraint(
            *[getattr(table.columns, c.strip(' "')) for c in cols.split(",")],
            name=name
        )
        for name, cols in re.findall(UNIQUE_PATTERN, data)
    ]


def _recreate_table(self, table, column=None, delta=None, omit_uniques=None):
    """Recreate the table properly

    Unlike the corresponding original method of sqlalchemy-migrate this one
    doesn't drop existing unique constraints when creating a new one.

    """

    table_name = self.preparer.format_table(table)

    # we remove all indexes so as not to have
    # problems during copy and re-create
    for index in table.indexes:
        index.drop()

    # reflect existing unique constraints
    for uc in self._get_unique_constraints(table):
        table.append_constraint(uc)
    # omit given unique constraints when creating a new table if required
    table.constraints = set([
        cons for cons in table.constraints
        if omit_uniques is None or cons.name not in omit_uniques
    ])

    self.append('ALTER TABLE %s RENAME TO migration_tmp' % table_name)
    self.execute()

    insertion_string = self._modify_table(table, column, delta)

    table.create(bind=self.connection)
    self.append(insertion_string % {'table_name': table_name})
    self.execute()
    self.append('DROP TABLE migration_tmp')
    self.execute()


def _visit_migrate_unique_constraint(self, *p, **k):
    """Drop the given unique constraint

    The corresponding original method of sqlalchemy-migrate just
    raises NotImplemented error

    """

    self.recreate_table(p[0].table, omit_uniques=[p[0].name])


def patch_migrate():
    """A workaround for SQLite's inability to alter things

    SQLite abilities to alter tables are very limited (please read
    http://www.sqlite.org/lang_altertable.html for more details).
    E. g. one can't drop a column or a constraint in SQLite. The
    workaround for this is to recreate the original table omitting
    the corresponding constraint (or column).

    sqlalchemy-migrate library has recreate_table() method that
    implements this workaround, but it does it wrong:

        - information about unique constraints of a table
          is not retrieved. So if you have a table with one
          unique constraint and a migration adding another one
          you will end up with a table that has only the
          latter unique constraint, and the former will be lost

        - dropping of unique constraints is not supported at all

    The proper way to fix this is to provide a pull-request to
    sqlalchemy-migrate, but the project seems to be dead. So we
    can go on with monkey-patching of the lib at least for now.

    """

    # this patch is needed to ensure that recreate_table() doesn't drop
    # existing unique constraints of the table when creating a new one
    helper_cls = sqlite.SQLiteHelper
    helper_cls.recreate_table = _recreate_table
    helper_cls._get_unique_constraints = _get_unique_constraints

    # this patch is needed to be able to drop existing unique constraints
    constraint_cls = sqlite.SQLiteConstraintDropper
    constraint_cls.visit_migrate_unique_constraint = \
        _visit_migrate_unique_constraint
    constraint_cls.__bases__ = (ansisql.ANSIColumnDropper,
                                sqlite.SQLiteConstraintGenerator)


def db_sync(engine, abs_path, version=None, init_version=0, sanity_check=True):
    """Upgrade or downgrade a database.

    Function runs the upgrade() or downgrade() functions in change scripts.

    :param engine:       SQLAlchemy engine instance for a given database
    :param abs_path:     Absolute path to migrate repository.
    :param version:      Database will upgrade/downgrade until this version.
                         If None - database will update to the latest
                         available version.
    :param init_version: Initial database version
    :param sanity_check: Require schema sanity checking for all tables
    """

    if version is not None:
        try:
            version = int(version)
        except ValueError:
            raise exception.DbMigrationError(
                message=_("version should be an integer"))

    current_version = db_version(engine, abs_path, init_version)
    repository = _find_migrate_repo(abs_path)
    if sanity_check:
        _db_schema_sanity_check(engine)
    if version is None or version > current_version:
        return versioning_api.upgrade(engine, repository, version)
    else:
        return versioning_api.downgrade(engine, repository,
                                        version)


def _db_schema_sanity_check(engine):
    """Ensure all database tables were created with required parameters.

    :param engine:  SQLAlchemy engine instance for a given database

    """

    if engine.name == 'mysql':
        onlyutf8_sql = ('SELECT TABLE_NAME,TABLE_COLLATION '
                        'from information_schema.TABLES '
                        'where TABLE_SCHEMA=%s and '
                        'TABLE_COLLATION NOT LIKE "%%utf8%%"')

        # NOTE(morganfainberg): exclude the sqlalchemy-migrate and alembic
        # versioning tables from the tables we need to verify utf8 status on.
        # Non-standard table names are not supported.
        EXCLUDED_TABLES = ['migrate_version', 'alembic_version']

        table_names = [res[0] for res in
                       engine.execute(onlyutf8_sql, engine.url.database) if
                       res[0].lower() not in EXCLUDED_TABLES]

        if len(table_names) > 0:
            raise ValueError(_('Tables "%s" have non utf8 collation, '
                               'please make sure all tables are CHARSET=utf8'
                               ) % ','.join(table_names))


def db_version(engine, abs_path, init_version):
    """Show the current version of the repository.

    :param engine:  SQLAlchemy engine instance for a given database
    :param abs_path: Absolute path to migrate repository
    :param version:  Initial database version
    """
    repository = _find_migrate_repo(abs_path)
    try:
        return versioning_api.db_version(engine, repository)
    except versioning_exceptions.DatabaseNotControlledError:
        meta = sqlalchemy.MetaData()
        meta.reflect(bind=engine)
        tables = meta.tables
        if len(tables) == 0 or 'alembic_version' in tables:
            db_version_control(engine, abs_path, version=init_version)
            return versioning_api.db_version(engine, repository)
        else:
            raise exception.DbMigrationError(
                message=_(
                    "The database is not under version control, but has "
                    "tables. Please stamp the current version of the schema "
                    "manually."))


def db_version_control(engine, abs_path, version=None):
    """Mark a database as under this repository's version control.

    Once a database is under version control, schema changes should
    only be done via change scripts in this repository.

    :param engine:  SQLAlchemy engine instance for a given database
    :param abs_path: Absolute path to migrate repository
    :param version:  Initial database version
    """
    repository = _find_migrate_repo(abs_path)
    versioning_api.version_control(engine, repository, version)
    return version


def _find_migrate_repo(abs_path):
    """Get the project's change script repository

    :param abs_path: Absolute path to migrate repository
    """
    if not os.path.exists(abs_path):
        raise exception.DbMigrationError("Path %s not found" % abs_path)
    return Repository(abs_path)

########NEW FILE########
__FILENAME__ = models
# Copyright (c) 2011 X.commerce, a business unit of eBay Inc.
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2011 Piston Cloud Computing, Inc.
# Copyright 2012 Cloudscaling Group, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
SQLAlchemy models.
"""

import six

from sqlalchemy import Column, Integer
from sqlalchemy import DateTime
from sqlalchemy.orm import object_mapper

from designate.openstack.common import timeutils


class ModelBase(six.Iterator):
    """Base class for models."""
    __table_initialized__ = False

    def save(self, session):
        """Save this object."""

        # NOTE(boris-42): This part of code should be look like:
        #                       session.add(self)
        #                       session.flush()
        #                 But there is a bug in sqlalchemy and eventlet that
        #                 raises NoneType exception if there is no running
        #                 transaction and rollback is called. As long as
        #                 sqlalchemy has this bug we have to create transaction
        #                 explicitly.
        with session.begin(subtransactions=True):
            session.add(self)
            session.flush()

    def __setitem__(self, key, value):
        setattr(self, key, value)

    def __getitem__(self, key):
        return getattr(self, key)

    def get(self, key, default=None):
        return getattr(self, key, default)

    @property
    def _extra_keys(self):
        """Specifies custom fields

        Subclasses can override this property to return a list
        of custom fields that should be included in their dict
        representation.

        For reference check tests/db/sqlalchemy/test_models.py
        """
        return []

    def __iter__(self):
        columns = dict(object_mapper(self).columns).keys()
        # NOTE(russellb): Allow models to specify other keys that can be looked
        # up, beyond the actual db columns.  An example would be the 'name'
        # property for an Instance.
        columns.extend(self._extra_keys)
        self._i = iter(columns)
        return self

    # In Python 3, __next__() has replaced next().
    def __next__(self):
        n = six.advance_iterator(self._i)
        return n, getattr(self, n)

    def next(self):
        return self.__next__()

    def update(self, values):
        """Make the model object behave like a dict."""
        for k, v in six.iteritems(values):
            setattr(self, k, v)

    def iteritems(self):
        """Make the model object behave like a dict.

        Includes attributes from joins.
        """
        local = dict(self)
        joined = dict([(k, v) for k, v in six.iteritems(self.__dict__)
                      if not k[0] == '_'])
        local.update(joined)
        return six.iteritems(local)


class TimestampMixin(object):
    created_at = Column(DateTime, default=lambda: timeutils.utcnow())
    updated_at = Column(DateTime, onupdate=lambda: timeutils.utcnow())


class SoftDeleteMixin(object):
    deleted_at = Column(DateTime)
    deleted = Column(Integer, default=0)

    def soft_delete(self, session):
        """Mark this object as deleted."""
        self.deleted = self.id
        self.deleted_at = timeutils.utcnow()
        self.save(session=session)

########NEW FILE########
__FILENAME__ = provision
# Copyright 2013 Mirantis.inc
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Provision test environment for specific DB backends"""

import argparse
import logging
import os
import random
import string

from six import moves
import sqlalchemy

from designate.openstack.common.db import exception as exc


LOG = logging.getLogger(__name__)


def get_engine(uri):
    """Engine creation

    Call the function without arguments to get admin connection. Admin
    connection required to create temporary user and database for each
    particular test. Otherwise use existing connection to recreate connection
    to the temporary database.
    """
    return sqlalchemy.create_engine(uri, poolclass=sqlalchemy.pool.NullPool)


def _execute_sql(engine, sql, driver):
    """Initialize connection, execute sql query and close it."""
    try:
        with engine.connect() as conn:
            if driver == 'postgresql':
                conn.connection.set_isolation_level(0)
            for s in sql:
                conn.execute(s)
    except sqlalchemy.exc.OperationalError:
        msg = ('%s does not match database admin '
               'credentials or database does not exist.')
        LOG.exception(msg % engine.url)
        raise exc.DBConnectionError(msg % engine.url)


def create_database(engine):
    """Provide temporary user and database for each particular test."""
    driver = engine.name

    auth = {
        'database': ''.join(random.choice(string.ascii_lowercase)
                            for i in moves.range(10)),
        'user': engine.url.username,
        'passwd': engine.url.password,
    }

    sqls = [
        "drop database if exists %(database)s;",
        "create database %(database)s;"
    ]

    if driver == 'sqlite':
        return 'sqlite:////tmp/%s' % auth['database']
    elif driver in ['mysql', 'postgresql']:
        sql_query = map(lambda x: x % auth, sqls)
        _execute_sql(engine, sql_query, driver)
    else:
        raise ValueError('Unsupported RDBMS %s' % driver)

    params = auth.copy()
    params['backend'] = driver
    return "%(backend)s://%(user)s:%(passwd)s@localhost/%(database)s" % params


def drop_database(admin_engine, current_uri):
    """Drop temporary database and user after each particular test."""

    engine = get_engine(current_uri)
    driver = engine.name
    auth = {'database': engine.url.database, 'user': engine.url.username}

    if driver == 'sqlite':
        try:
            os.remove(auth['database'])
        except OSError:
            pass
    elif driver in ['mysql', 'postgresql']:
        sql = "drop database if exists %(database)s;"
        _execute_sql(admin_engine, [sql % auth], driver)
    else:
        raise ValueError('Unsupported RDBMS %s' % driver)


def main():
    """Controller to handle commands

    ::create: Create test user and database with random names.
    ::drop: Drop user and database created by previous command.
    """
    parser = argparse.ArgumentParser(
        description='Controller to handle database creation and dropping'
        ' commands.',
        epilog='Under normal circumstances is not used directly.'
        ' Used in .testr.conf to automate test database creation'
        ' and dropping processes.')
    subparsers = parser.add_subparsers(
        help='Subcommands to manipulate temporary test databases.')

    create = subparsers.add_parser(
        'create',
        help='Create temporary test '
        'databases and users.')
    create.set_defaults(which='create')
    create.add_argument(
        'instances_count',
        type=int,
        help='Number of databases to create.')

    drop = subparsers.add_parser(
        'drop',
        help='Drop temporary test databases and users.')
    drop.set_defaults(which='drop')
    drop.add_argument(
        'instances',
        nargs='+',
        help='List of databases uri to be dropped.')

    args = parser.parse_args()

    connection_string = os.getenv('OS_TEST_DBAPI_ADMIN_CONNECTION',
                                  'sqlite://')
    engine = get_engine(connection_string)
    which = args.which

    if which == "create":
        for i in range(int(args.instances_count)):
            print(create_database(engine))
    elif which == "drop":
        for db in args.instances:
            drop_database(engine, db)


if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = session
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Session Handling for SQLAlchemy backend.

Recommended ways to use sessions within this framework:

* Don't use them explicitly; this is like running with ``AUTOCOMMIT=1``.
  `model_query()` will implicitly use a session when called without one
  supplied. This is the ideal situation because it will allow queries
  to be automatically retried if the database connection is interrupted.

  .. note:: Automatic retry will be enabled in a future patch.

  It is generally fine to issue several queries in a row like this. Even though
  they may be run in separate transactions and/or separate sessions, each one
  will see the data from the prior calls. If needed, undo- or rollback-like
  functionality should be handled at a logical level. For an example, look at
  the code around quotas and `reservation_rollback()`.

  Examples:

  .. code:: python

    def get_foo(context, foo):
        return (model_query(context, models.Foo).
                filter_by(foo=foo).
                first())

    def update_foo(context, id, newfoo):
        (model_query(context, models.Foo).
                filter_by(id=id).
                update({'foo': newfoo}))

    def create_foo(context, values):
        foo_ref = models.Foo()
        foo_ref.update(values)
        foo_ref.save()
        return foo_ref


* Within the scope of a single method, keep all the reads and writes within
  the context managed by a single session. In this way, the session's
  `__exit__` handler will take care of calling `flush()` and `commit()` for
  you. If using this approach, you should not explicitly call `flush()` or
  `commit()`. Any error within the context of the session will cause the
  session to emit a `ROLLBACK`. Database errors like `IntegrityError` will be
  raised in `session`'s `__exit__` handler, and any try/except within the
  context managed by `session` will not be triggered. And catching other
  non-database errors in the session will not trigger the ROLLBACK, so
  exception handlers should  always be outside the session, unless the
  developer wants to do a partial commit on purpose. If the connection is
  dropped before this is possible, the database will implicitly roll back the
  transaction.

  .. note:: Statements in the session scope will not be automatically retried.

  If you create models within the session, they need to be added, but you
  do not need to call `model.save()`:

  .. code:: python

    def create_many_foo(context, foos):
        session = sessionmaker()
        with session.begin():
            for foo in foos:
                foo_ref = models.Foo()
                foo_ref.update(foo)
                session.add(foo_ref)

    def update_bar(context, foo_id, newbar):
        session = sessionmaker()
        with session.begin():
            foo_ref = (model_query(context, models.Foo, session).
                        filter_by(id=foo_id).
                        first())
            (model_query(context, models.Bar, session).
                        filter_by(id=foo_ref['bar_id']).
                        update({'bar': newbar}))

  .. note:: `update_bar` is a trivially simple example of using
     ``with session.begin``. Whereas `create_many_foo` is a good example of
     when a transaction is needed, it is always best to use as few queries as
     possible.

  The two queries in `update_bar` can be better expressed using a single query
  which avoids the need for an explicit transaction. It can be expressed like
  so:

  .. code:: python

    def update_bar(context, foo_id, newbar):
        subq = (model_query(context, models.Foo.id).
                filter_by(id=foo_id).
                limit(1).
                subquery())
        (model_query(context, models.Bar).
                filter_by(id=subq.as_scalar()).
                update({'bar': newbar}))

  For reference, this emits approximately the following SQL statement:

  .. code:: sql

    UPDATE bar SET bar = ${newbar}
        WHERE id=(SELECT bar_id FROM foo WHERE id = ${foo_id} LIMIT 1);

  .. note:: `create_duplicate_foo` is a trivially simple example of catching an
     exception while using ``with session.begin``. Here create two duplicate
     instances with same primary key, must catch the exception out of context
     managed by a single session:

  .. code:: python

    def create_duplicate_foo(context):
        foo1 = models.Foo()
        foo2 = models.Foo()
        foo1.id = foo2.id = 1
        session = sessionmaker()
        try:
            with session.begin():
                session.add(foo1)
                session.add(foo2)
        except exception.DBDuplicateEntry as e:
            handle_error(e)

* Passing an active session between methods. Sessions should only be passed
  to private methods. The private method must use a subtransaction; otherwise
  SQLAlchemy will throw an error when you call `session.begin()` on an existing
  transaction. Public methods should not accept a session parameter and should
  not be involved in sessions within the caller's scope.

  Note that this incurs more overhead in SQLAlchemy than the above means
  due to nesting transactions, and it is not possible to implicitly retry
  failed database operations when using this approach.

  This also makes code somewhat more difficult to read and debug, because a
  single database transaction spans more than one method. Error handling
  becomes less clear in this situation. When this is needed for code clarity,
  it should be clearly documented.

  .. code:: python

    def myfunc(foo):
        session = sessionmaker()
        with session.begin():
            # do some database things
            bar = _private_func(foo, session)
        return bar

    def _private_func(foo, session=None):
        if not session:
            session = sessionmaker()
        with session.begin(subtransaction=True):
            # do some other database things
        return bar


There are some things which it is best to avoid:

* Don't keep a transaction open any longer than necessary.

  This means that your ``with session.begin()`` block should be as short
  as possible, while still containing all the related calls for that
  transaction.

* Avoid ``with_lockmode('UPDATE')`` when possible.

  In MySQL/InnoDB, when a ``SELECT ... FOR UPDATE`` query does not match
  any rows, it will take a gap-lock. This is a form of write-lock on the
  "gap" where no rows exist, and prevents any other writes to that space.
  This can effectively prevent any INSERT into a table by locking the gap
  at the end of the index. Similar problems will occur if the SELECT FOR UPDATE
  has an overly broad WHERE clause, or doesn't properly use an index.

  One idea proposed at ODS Fall '12 was to use a normal SELECT to test the
  number of rows matching a query, and if only one row is returned,
  then issue the SELECT FOR UPDATE.

  The better long-term solution is to use
  ``INSERT .. ON DUPLICATE KEY UPDATE``.
  However, this can not be done until the "deleted" columns are removed and
  proper UNIQUE constraints are added to the tables.


Enabling soft deletes:

* To use/enable soft-deletes, the `SoftDeleteMixin` must be added
  to your model class. For example:

  .. code:: python

      class NovaBase(models.SoftDeleteMixin, models.ModelBase):
          pass


Efficient use of soft deletes:

* There are two possible ways to mark a record as deleted:
  `model.soft_delete()` and `query.soft_delete()`.

  The `model.soft_delete()` method works with a single already-fetched entry.
  `query.soft_delete()` makes only one db request for all entries that
  correspond to the query.

* In almost all cases you should use `query.soft_delete()`. Some examples:

  .. code:: python

        def soft_delete_bar():
            count = model_query(BarModel).find(some_condition).soft_delete()
            if count == 0:
                raise Exception("0 entries were soft deleted")

        def complex_soft_delete_with_synchronization_bar(session=None):
            if session is None:
                session = sessionmaker()
            with session.begin(subtransactions=True):
                count = (model_query(BarModel).
                            find(some_condition).
                            soft_delete(synchronize_session=True))
                            # Here synchronize_session is required, because we
                            # don't know what is going on in outer session.
                if count == 0:
                    raise Exception("0 entries were soft deleted")

* There is only one situation where `model.soft_delete()` is appropriate: when
  you fetch a single record, work with it, and mark it as deleted in the same
  transaction.

  .. code:: python

        def soft_delete_bar_model():
            session = sessionmaker()
            with session.begin():
                bar_ref = model_query(BarModel).find(some_condition).first()
                # Work with bar_ref
                bar_ref.soft_delete(session=session)

  However, if you need to work with all entries that correspond to query and
  then soft delete them you should use the `query.soft_delete()` method:

  .. code:: python

        def soft_delete_multi_models():
            session = sessionmaker()
            with session.begin():
                query = (model_query(BarModel, session=session).
                            find(some_condition))
                model_refs = query.all()
                # Work with model_refs
                query.soft_delete(synchronize_session=False)
                # synchronize_session=False should be set if there is no outer
                # session and these entries are not used after this.

  When working with many rows, it is very important to use query.soft_delete,
  which issues a single query. Using `model.soft_delete()`, as in the following
  example, is very inefficient.

  .. code:: python

        for bar_ref in bar_refs:
            bar_ref.soft_delete(session=session)
        # This will produce count(bar_refs) db requests.

"""

import functools
import logging
import re
import time

import six
from sqlalchemy import exc as sqla_exc
from sqlalchemy.interfaces import PoolListener
import sqlalchemy.orm
from sqlalchemy.pool import NullPool, StaticPool
from sqlalchemy.sql.expression import literal_column

from designate.openstack.common.db import exception
from designate.openstack.common.gettextutils import _LE, _LW
from designate.openstack.common import timeutils


LOG = logging.getLogger(__name__)


class SqliteForeignKeysListener(PoolListener):
    """Ensures that the foreign key constraints are enforced in SQLite.

    The foreign key constraints are disabled by default in SQLite,
    so the foreign key constraints will be enabled here for every
    database connection
    """
    def connect(self, dbapi_con, con_record):
        dbapi_con.execute('pragma foreign_keys=ON')


# note(boris-42): In current versions of DB backends unique constraint
# violation messages follow the structure:
#
# sqlite:
# 1 column - (IntegrityError) column c1 is not unique
# N columns - (IntegrityError) column c1, c2, ..., N are not unique
#
# sqlite since 3.7.16:
# 1 column - (IntegrityError) UNIQUE constraint failed: tbl.k1
#
# N columns - (IntegrityError) UNIQUE constraint failed: tbl.k1, tbl.k2
#
# postgres:
# 1 column - (IntegrityError) duplicate key value violates unique
#               constraint "users_c1_key"
# N columns - (IntegrityError) duplicate key value violates unique
#               constraint "name_of_our_constraint"
#
# mysql:
# 1 column - (IntegrityError) (1062, "Duplicate entry 'value_of_c1' for key
#               'c1'")
# N columns - (IntegrityError) (1062, "Duplicate entry 'values joined
#               with -' for key 'name_of_our_constraint'")
#
# ibm_db_sa:
# N columns - (IntegrityError) SQL0803N  One or more values in the INSERT
#                statement, UPDATE statement, or foreign key update caused by a
#                DELETE statement are not valid because the primary key, unique
#                constraint or unique index identified by "2" constrains table
#                "NOVA.KEY_PAIRS" from having duplicate values for the index
#                key.
_DUP_KEY_RE_DB = {
    "sqlite": (re.compile(r"^.*columns?([^)]+)(is|are)\s+not\s+unique$"),
               re.compile(r"^.*UNIQUE\s+constraint\s+failed:\s+(.+)$")),
    "postgresql": (re.compile(r"^.*duplicate\s+key.*\"([^\"]+)\"\s*\n.*$"),),
    "mysql": (re.compile(r"^.*\(1062,.*'([^\']+)'\"\)$"),),
    "ibm_db_sa": (re.compile(r"^.*SQL0803N.*$"),),
}


def _raise_if_duplicate_entry_error(integrity_error, engine_name):
    """Raise exception if two entries are duplicated.

    In this function will be raised DBDuplicateEntry exception if integrity
    error wrap unique constraint violation.
    """

    def get_columns_from_uniq_cons_or_name(columns):
        # note(vsergeyev): UniqueConstraint name convention: "uniq_t0c10c2"
        #                  where `t` it is table name and columns `c1`, `c2`
        #                  are in UniqueConstraint.
        uniqbase = "uniq_"
        if not columns.startswith(uniqbase):
            if engine_name == "postgresql":
                return [columns[columns.index("_") + 1:columns.rindex("_")]]
            return [columns]
        return columns[len(uniqbase):].split("0")[1:]

    if engine_name not in ["ibm_db_sa", "mysql", "sqlite", "postgresql"]:
        return

    # FIXME(johannes): The usage of the .message attribute has been
    # deprecated since Python 2.6. However, the exceptions raised by
    # SQLAlchemy can differ when using unicode() and accessing .message.
    # An audit across all three supported engines will be necessary to
    # ensure there are no regressions.
    for pattern in _DUP_KEY_RE_DB[engine_name]:
        match = pattern.match(integrity_error.message)
        if match:
            break
    else:
        return

    # NOTE(mriedem): The ibm_db_sa integrity error message doesn't provide the
    # columns so we have to omit that from the DBDuplicateEntry error.
    columns = ''

    if engine_name != 'ibm_db_sa':
        columns = match.group(1)

    if engine_name == "sqlite":
        columns = [c.split('.')[-1] for c in columns.strip().split(", ")]
    else:
        columns = get_columns_from_uniq_cons_or_name(columns)
    raise exception.DBDuplicateEntry(columns, integrity_error)


# NOTE(comstud): In current versions of DB backends, Deadlock violation
# messages follow the structure:
#
# mysql:
# (OperationalError) (1213, 'Deadlock found when trying to get lock; try '
#                     'restarting transaction') <query_str> <query_args>
_DEADLOCK_RE_DB = {
    "mysql": re.compile(r"^.*\(1213, 'Deadlock.*")
}


def _raise_if_deadlock_error(operational_error, engine_name):
    """Raise exception on deadlock condition.

    Raise DBDeadlock exception if OperationalError contains a Deadlock
    condition.
    """
    re = _DEADLOCK_RE_DB.get(engine_name)
    if re is None:
        return
    # FIXME(johannes): The usage of the .message attribute has been
    # deprecated since Python 2.6. However, the exceptions raised by
    # SQLAlchemy can differ when using unicode() and accessing .message.
    # An audit across all three supported engines will be necessary to
    # ensure there are no regressions.
    m = re.match(operational_error.message)
    if not m:
        return
    raise exception.DBDeadlock(operational_error)


def _wrap_db_error(f):
    @functools.wraps(f)
    def _wrap(self, *args, **kwargs):
        try:
            assert issubclass(
                self.__class__, sqlalchemy.orm.session.Session
            ), ('_wrap_db_error() can only be applied to methods of '
                'subclasses of sqlalchemy.orm.session.Session.')

            return f(self, *args, **kwargs)
        except UnicodeEncodeError:
            raise exception.DBInvalidUnicodeParameter()
        except sqla_exc.OperationalError as e:
            _raise_if_db_connection_lost(e, self.bind)
            _raise_if_deadlock_error(e, self.bind.dialect.name)
            # NOTE(comstud): A lot of code is checking for OperationalError
            # so let's not wrap it for now.
            raise
        # note(boris-42): We should catch unique constraint violation and
        # wrap it by our own DBDuplicateEntry exception. Unique constraint
        # violation is wrapped by IntegrityError.
        except sqla_exc.IntegrityError as e:
            # note(boris-42): SqlAlchemy doesn't unify errors from different
            # DBs so we must do this. Also in some tables (for example
            # instance_types) there are more than one unique constraint. This
            # means we should get names of columns, which values violate
            # unique constraint, from error message.
            _raise_if_duplicate_entry_error(e, self.bind.dialect.name)
            raise exception.DBError(e)
        except Exception as e:
            LOG.exception(_LE('DB exception wrapped.'))
            raise exception.DBError(e)
    return _wrap


def _synchronous_switch_listener(dbapi_conn, connection_rec):
    """Switch sqlite connections to non-synchronous mode."""
    dbapi_conn.execute("PRAGMA synchronous = OFF")


def _add_regexp_listener(dbapi_con, con_record):
    """Add REGEXP function to sqlite connections."""

    def regexp(expr, item):
        reg = re.compile(expr)
        return reg.search(six.text_type(item)) is not None
    dbapi_con.create_function('regexp', 2, regexp)


def _thread_yield(dbapi_con, con_record):
    """Ensure other greenthreads get a chance to be executed.

    If we use eventlet.monkey_patch(), eventlet.greenthread.sleep(0) will
    execute instead of time.sleep(0).
    Force a context switch. With common database backends (eg MySQLdb and
    sqlite), there is no implicit yield caused by network I/O since they are
    implemented by C libraries that eventlet cannot monkey patch.
    """
    time.sleep(0)


def _ping_listener(engine, dbapi_conn, connection_rec, connection_proxy):
    """Ensures that MySQL and DB2 connections are alive.

    Borrowed from:
    http://groups.google.com/group/sqlalchemy/msg/a4ce563d802c929f
    """
    cursor = dbapi_conn.cursor()
    try:
        ping_sql = 'select 1'
        if engine.name == 'ibm_db_sa':
            # DB2 requires a table expression
            ping_sql = 'select 1 from (values (1)) AS t1'
        cursor.execute(ping_sql)
    except Exception as ex:
        if engine.dialect.is_disconnect(ex, dbapi_conn, cursor):
            msg = _LW('Database server has gone away: %s') % ex
            LOG.warning(msg)

            # if the database server has gone away, all connections in the pool
            # have become invalid and we can safely close all of them here,
            # rather than waste time on checking of every single connection
            engine.dispose()

            # this will be handled by SQLAlchemy and will force it to create
            # a new connection and retry the original action
            raise sqla_exc.DisconnectionError(msg)
        else:
            raise


def _set_session_sql_mode(dbapi_con, connection_rec, sql_mode=None):
    """Set the sql_mode session variable.

    MySQL supports several server modes. The default is None, but sessions
    may choose to enable server modes like TRADITIONAL, ANSI,
    several STRICT_* modes and others.

    Note: passing in '' (empty string) for sql_mode clears
    the SQL mode for the session, overriding a potentially set
    server default.
    """

    cursor = dbapi_con.cursor()
    cursor.execute("SET SESSION sql_mode = %s", [sql_mode])


def _mysql_get_effective_sql_mode(engine):
    """Returns the effective SQL mode for connections from the engine pool.

    Returns ``None`` if the mode isn't available, otherwise returns the mode.

    """
    # Get the real effective SQL mode. Even when unset by
    # our own config, the server may still be operating in a specific
    # SQL mode as set by the server configuration.
    # Also note that the checkout listener will be called on execute to
    # set the mode if it's registered.
    row = engine.execute("SHOW VARIABLES LIKE 'sql_mode'").fetchone()
    if row is None:
        return
    return row[1]


def _mysql_check_effective_sql_mode(engine):
    """Logs a message based on the effective SQL mode for MySQL connections."""
    realmode = _mysql_get_effective_sql_mode(engine)

    if realmode is None:
        LOG.warning(_LW('Unable to detect effective SQL mode'))
        return

    LOG.debug('MySQL server mode set to %s', realmode)
    # 'TRADITIONAL' mode enables several other modes, so
    # we need a substring match here
    if not ('TRADITIONAL' in realmode.upper() or
            'STRICT_ALL_TABLES' in realmode.upper()):
        LOG.warning(_LW("MySQL SQL mode is '%s', "
                        "consider enabling TRADITIONAL or STRICT_ALL_TABLES"),
                    realmode)


def _mysql_set_mode_callback(engine, sql_mode):
    if sql_mode is not None:
        mode_callback = functools.partial(_set_session_sql_mode,
                                          sql_mode=sql_mode)
        sqlalchemy.event.listen(engine, 'connect', mode_callback)
    _mysql_check_effective_sql_mode(engine)


def _is_db_connection_error(args):
    """Return True if error in connecting to db."""
    # NOTE(adam_g): This is currently MySQL specific and needs to be extended
    #               to support Postgres and others.
    # For the db2, the error code is -30081 since the db2 is still not ready
    conn_err_codes = ('2002', '2003', '2006', '2013', '-30081')
    for err_code in conn_err_codes:
        if args.find(err_code) != -1:
            return True
    return False


def _raise_if_db_connection_lost(error, engine):
    # NOTE(vsergeyev): Function is_disconnect(e, connection, cursor)
    #                  requires connection and cursor in incoming parameters,
    #                  but we have no possibility to create connection if DB
    #                  is not available, so in such case reconnect fails.
    #                  But is_disconnect() ignores these parameters, so it
    #                  makes sense to pass to function None as placeholder
    #                  instead of connection and cursor.
    if engine.dialect.is_disconnect(error, None, None):
        raise exception.DBConnectionError(error)


def create_engine(sql_connection, sqlite_fk=False, mysql_sql_mode=None,
                  idle_timeout=3600,
                  connection_debug=0, max_pool_size=None, max_overflow=None,
                  pool_timeout=None, sqlite_synchronous=True,
                  connection_trace=False, max_retries=10, retry_interval=10):
    """Return a new SQLAlchemy engine."""

    connection_dict = sqlalchemy.engine.url.make_url(sql_connection)

    engine_args = {
        "pool_recycle": idle_timeout,
        'convert_unicode': True,
    }

    logger = logging.getLogger('sqlalchemy.engine')

    # Map SQL debug level to Python log level
    if connection_debug >= 100:
        logger.setLevel(logging.DEBUG)
    elif connection_debug >= 50:
        logger.setLevel(logging.INFO)
    else:
        logger.setLevel(logging.WARNING)

    if "sqlite" in connection_dict.drivername:
        if sqlite_fk:
            engine_args["listeners"] = [SqliteForeignKeysListener()]
        engine_args["poolclass"] = NullPool

        if sql_connection == "sqlite://":
            engine_args["poolclass"] = StaticPool
            engine_args["connect_args"] = {'check_same_thread': False}
    else:
        if max_pool_size is not None:
            engine_args['pool_size'] = max_pool_size
        if max_overflow is not None:
            engine_args['max_overflow'] = max_overflow
        if pool_timeout is not None:
            engine_args['pool_timeout'] = pool_timeout

    engine = sqlalchemy.create_engine(sql_connection, **engine_args)

    sqlalchemy.event.listen(engine, 'checkin', _thread_yield)

    if engine.name in ['mysql', 'ibm_db_sa']:
        ping_callback = functools.partial(_ping_listener, engine)
        sqlalchemy.event.listen(engine, 'checkout', ping_callback)
        if engine.name == 'mysql':
            if mysql_sql_mode:
                _mysql_set_mode_callback(engine, mysql_sql_mode)
    elif 'sqlite' in connection_dict.drivername:
        if not sqlite_synchronous:
            sqlalchemy.event.listen(engine, 'connect',
                                    _synchronous_switch_listener)
        sqlalchemy.event.listen(engine, 'connect', _add_regexp_listener)

    if connection_trace and engine.dialect.dbapi.__name__ == 'MySQLdb':
        _patch_mysqldb_with_stacktrace_comments()

    try:
        engine.connect()
    except sqla_exc.OperationalError as e:
        if not _is_db_connection_error(e.args[0]):
            raise

        remaining = max_retries
        if remaining == -1:
            remaining = 'infinite'
        while True:
            msg = _LW('SQL connection failed. %s attempts left.')
            LOG.warning(msg % remaining)
            if remaining != 'infinite':
                remaining -= 1
            time.sleep(retry_interval)
            try:
                engine.connect()
                break
            except sqla_exc.OperationalError as e:
                if (remaining != 'infinite' and remaining == 0) or \
                        not _is_db_connection_error(e.args[0]):
                    raise
    return engine


class Query(sqlalchemy.orm.query.Query):
    """Subclass of sqlalchemy.query with soft_delete() method."""
    def soft_delete(self, synchronize_session='evaluate'):
        return self.update({'deleted': literal_column('id'),
                            'updated_at': literal_column('updated_at'),
                            'deleted_at': timeutils.utcnow()},
                           synchronize_session=synchronize_session)


class Session(sqlalchemy.orm.session.Session):
    """Custom Session class to avoid SqlAlchemy Session monkey patching."""
    @_wrap_db_error
    def query(self, *args, **kwargs):
        return super(Session, self).query(*args, **kwargs)

    @_wrap_db_error
    def flush(self, *args, **kwargs):
        return super(Session, self).flush(*args, **kwargs)

    @_wrap_db_error
    def execute(self, *args, **kwargs):
        return super(Session, self).execute(*args, **kwargs)


def get_maker(engine, autocommit=True, expire_on_commit=False):
    """Return a SQLAlchemy sessionmaker using the given engine."""
    return sqlalchemy.orm.sessionmaker(bind=engine,
                                       class_=Session,
                                       autocommit=autocommit,
                                       expire_on_commit=expire_on_commit,
                                       query_cls=Query)


def _patch_mysqldb_with_stacktrace_comments():
    """Adds current stack trace as a comment in queries.

    Patches MySQLdb.cursors.BaseCursor._do_query.
    """
    import MySQLdb.cursors
    import traceback

    old_mysql_do_query = MySQLdb.cursors.BaseCursor._do_query

    def _do_query(self, q):
        stack = ''
        for filename, line, method, function in traceback.extract_stack():
            # exclude various common things from trace
            if filename.endswith('session.py') and method == '_do_query':
                continue
            if filename.endswith('api.py') and method == 'wrapper':
                continue
            if filename.endswith('utils.py') and method == '_inner':
                continue
            if filename.endswith('exception.py') and method == '_wrap':
                continue
            # db/api is just a wrapper around db/sqlalchemy/api
            if filename.endswith('db/api.py'):
                continue
            # only trace inside designate
            index = filename.rfind('designate')
            if index == -1:
                continue
            stack += "File:%s:%s Method:%s() Line:%s | " \
                     % (filename[index:], line, method, function)

        # strip trailing " | " from stack
        if stack:
            stack = stack[:-3]
            qq = "%s /* %s */" % (q, stack)
        else:
            qq = q
        old_mysql_do_query(self, qq)

    setattr(MySQLdb.cursors.BaseCursor, '_do_query', _do_query)


class EngineFacade(object):
    """A helper class for removing of global engine instances from designate.db.

    As a library, designate.db can't decide where to store/when to create engine
    and sessionmaker instances, so this must be left for a target application.

    On the other hand, in order to simplify the adoption of designate.db changes,
    we'll provide a helper class, which creates engine and sessionmaker
    on its instantiation and provides get_engine()/get_session() methods
    that are compatible with corresponding utility functions that currently
    exist in target projects, e.g. in Nova.

    engine/sessionmaker instances will still be global (and they are meant to
    be global), but they will be stored in the app context, rather that in the
    designate.db context.

    Note: using of this helper is completely optional and you are encouraged to
    integrate engine/sessionmaker instances into your apps any way you like
    (e.g. one might want to bind a session to a request context). Two important
    things to remember:

    1. An Engine instance is effectively a pool of DB connections, so it's
       meant to be shared (and it's thread-safe).
    2. A Session instance is not meant to be shared and represents a DB
       transactional context (i.e. it's not thread-safe). sessionmaker is
       a factory of sessions.

    """

    def __init__(self, sql_connection,
                 sqlite_fk=False, autocommit=True,
                 expire_on_commit=False, **kwargs):
        """Initialize engine and sessionmaker instances.

        :param sqlite_fk: enable foreign keys in SQLite
        :type sqlite_fk: bool

        :param autocommit: use autocommit mode for created Session instances
        :type autocommit: bool

        :param expire_on_commit: expire session objects on commit
        :type expire_on_commit: bool

        Keyword arguments:

        :keyword mysql_sql_mode: the SQL mode to be used for MySQL sessions.
                                 (defaults to TRADITIONAL)
        :keyword idle_timeout: timeout before idle sql connections are reaped
                               (defaults to 3600)
        :keyword connection_debug: verbosity of SQL debugging information.
                                   0=None, 100=Everything (defaults to 0)
        :keyword max_pool_size: maximum number of SQL connections to keep open
                                in a pool (defaults to SQLAlchemy settings)
        :keyword max_overflow: if set, use this value for max_overflow with
                               sqlalchemy (defaults to SQLAlchemy settings)
        :keyword pool_timeout: if set, use this value for pool_timeout with
                               sqlalchemy (defaults to SQLAlchemy settings)
        :keyword sqlite_synchronous: if True, SQLite uses synchronous mode
                                     (defaults to True)
        :keyword connection_trace: add python stack traces to SQL as comment
                                   strings (defaults to False)
        :keyword max_retries: maximum db connection retries during startup.
                              (setting -1 implies an infinite retry count)
                              (defaults to 10)
        :keyword retry_interval: interval between retries of opening a sql
                                 connection (defaults to 10)

        """

        super(EngineFacade, self).__init__()

        self._engine = create_engine(
            sql_connection=sql_connection,
            sqlite_fk=sqlite_fk,
            mysql_sql_mode=kwargs.get('mysql_sql_mode', 'TRADITIONAL'),
            idle_timeout=kwargs.get('idle_timeout', 3600),
            connection_debug=kwargs.get('connection_debug', 0),
            max_pool_size=kwargs.get('max_pool_size'),
            max_overflow=kwargs.get('max_overflow'),
            pool_timeout=kwargs.get('pool_timeout'),
            sqlite_synchronous=kwargs.get('sqlite_synchronous', True),
            connection_trace=kwargs.get('connection_trace', False),
            max_retries=kwargs.get('max_retries', 10),
            retry_interval=kwargs.get('retry_interval', 10))
        self._session_maker = get_maker(
            engine=self._engine,
            autocommit=autocommit,
            expire_on_commit=expire_on_commit)

    def get_engine(self):
        """Get the engine instance (note, that it's shared)."""

        return self._engine

    def get_session(self, **kwargs):
        """Get a Session instance.

        If passed, keyword arguments values override the ones used when the
        sessionmaker instance was created.

        :keyword autocommit: use autocommit mode for created Session instances
        :type autocommit: bool

        :keyword expire_on_commit: expire session objects on commit
        :type expire_on_commit: bool

        """

        for arg in kwargs:
            if arg not in ('autocommit', 'expire_on_commit'):
                del kwargs[arg]

        return self._session_maker(**kwargs)

    @classmethod
    def from_config(cls, connection_string, conf,
                    sqlite_fk=False, autocommit=True, expire_on_commit=False):
        """Initialize EngineFacade using oslo.config config instance options.

        :param connection_string: SQLAlchemy connection string
        :type connection_string: string

        :param conf: oslo.config config instance
        :type conf: oslo.config.cfg.ConfigOpts

        :param sqlite_fk: enable foreign keys in SQLite
        :type sqlite_fk: bool

        :param autocommit: use autocommit mode for created Session instances
        :type autocommit: bool

        :param expire_on_commit: expire session objects on commit
        :type expire_on_commit: bool

        """

        return cls(sql_connection=connection_string,
                   sqlite_fk=sqlite_fk,
                   autocommit=autocommit,
                   expire_on_commit=expire_on_commit,
                   **dict(conf.database.items()))

########NEW FILE########
__FILENAME__ = test_base
# Copyright (c) 2013 OpenStack Foundation
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import abc
import functools
import os

import fixtures
import six

from designate.openstack.common.db.sqlalchemy import session
from designate.openstack.common.db.sqlalchemy import utils
from designate.openstack.common.fixture import lockutils
from designate.openstack.common import test


class DbFixture(fixtures.Fixture):
    """Basic database fixture.

    Allows to run tests on various db backends, such as SQLite, MySQL and
    PostgreSQL. By default use sqlite backend. To override default backend
    uri set env variable OS_TEST_DBAPI_CONNECTION with database admin
    credentials for specific backend.
    """

    def _get_uri(self):
        return os.getenv('OS_TEST_DBAPI_CONNECTION', 'sqlite://')

    def __init__(self, test):
        super(DbFixture, self).__init__()

        self.test = test

    def setUp(self):
        super(DbFixture, self).setUp()

        self.test.engine = session.create_engine(self._get_uri())
        self.test.sessionmaker = session.get_maker(self.test.engine)
        self.addCleanup(self.test.engine.dispose)


class DbTestCase(test.BaseTestCase):
    """Base class for testing of DB code.

    Using `DbFixture`. Intended to be the main database test case to use all
    the tests on a given backend with user defined uri. Backend specific
    tests should be decorated with `backend_specific` decorator.
    """

    FIXTURE = DbFixture

    def setUp(self):
        super(DbTestCase, self).setUp()
        self.useFixture(self.FIXTURE(self))


ALLOWED_DIALECTS = ['sqlite', 'mysql', 'postgresql']


def backend_specific(*dialects):
    """Decorator to skip backend specific tests on inappropriate engines.

    ::dialects: list of dialects names under which the test will be launched.
    """
    def wrap(f):
        @functools.wraps(f)
        def ins_wrap(self):
            if not set(dialects).issubset(ALLOWED_DIALECTS):
                raise ValueError(
                    "Please use allowed dialects: %s" % ALLOWED_DIALECTS)
            if self.engine.name not in dialects:
                msg = ('The test "%s" can be run '
                       'only on %s. Current engine is %s.')
                args = (f.__name__, ' '.join(dialects), self.engine.name)
                self.skip(msg % args)
            else:
                return f(self)
        return ins_wrap
    return wrap


@six.add_metaclass(abc.ABCMeta)
class OpportunisticFixture(DbFixture):
    """Base fixture to use default CI databases.

    The databases exist in OpenStack CI infrastructure. But for the
    correct functioning in local environment the databases must be
    created manually.
    """

    DRIVER = abc.abstractproperty(lambda: None)
    DBNAME = PASSWORD = USERNAME = 'openstack_citest'

    def _get_uri(self):
        return utils.get_connect_string(backend=self.DRIVER,
                                        user=self.USERNAME,
                                        passwd=self.PASSWORD,
                                        database=self.DBNAME)


@six.add_metaclass(abc.ABCMeta)
class OpportunisticTestCase(DbTestCase):
    """Base test case to use default CI databases.

    The subclasses of the test case are running only when openstack_citest
    database is available otherwise a tests will be skipped.
    """

    FIXTURE = abc.abstractproperty(lambda: None)

    def setUp(self):
        # TODO(bnemec): Remove this once infra is ready for
        # https://review.openstack.org/#/c/74963/ to merge.
        self.useFixture(lockutils.LockFixture('opportunistic-db'))
        credentials = {
            'backend': self.FIXTURE.DRIVER,
            'user': self.FIXTURE.USERNAME,
            'passwd': self.FIXTURE.PASSWORD,
            'database': self.FIXTURE.DBNAME}

        if self.FIXTURE.DRIVER and not utils.is_backend_avail(**credentials):
            msg = '%s backend is not available.' % self.FIXTURE.DRIVER
            return self.skip(msg)

        super(OpportunisticTestCase, self).setUp()


class MySQLOpportunisticFixture(OpportunisticFixture):
    DRIVER = 'mysql'


class PostgreSQLOpportunisticFixture(OpportunisticFixture):
    DRIVER = 'postgresql'


class MySQLOpportunisticTestCase(OpportunisticTestCase):
    FIXTURE = MySQLOpportunisticFixture


class PostgreSQLOpportunisticTestCase(OpportunisticTestCase):
    FIXTURE = PostgreSQLOpportunisticFixture

########NEW FILE########
__FILENAME__ = test_migrations
# Copyright 2010-2011 OpenStack Foundation
# Copyright 2012-2013 IBM Corp.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import functools
import logging
import os
import subprocess

import lockfile
from six import moves
from six.moves.urllib import parse
import sqlalchemy
import sqlalchemy.exc

from designate.openstack.common.db.sqlalchemy import utils
from designate.openstack.common.gettextutils import _LE
from designate.openstack.common import test

LOG = logging.getLogger(__name__)


def _have_mysql(user, passwd, database):
    present = os.environ.get('TEST_MYSQL_PRESENT')
    if present is None:
        return utils.is_backend_avail(backend='mysql',
                                      user=user,
                                      passwd=passwd,
                                      database=database)
    return present.lower() in ('', 'true')


def _have_postgresql(user, passwd, database):
    present = os.environ.get('TEST_POSTGRESQL_PRESENT')
    if present is None:
        return utils.is_backend_avail(backend='postgres',
                                      user=user,
                                      passwd=passwd,
                                      database=database)
    return present.lower() in ('', 'true')


def _set_db_lock(lock_path=None, lock_prefix=None):
    def decorator(f):
        @functools.wraps(f)
        def wrapper(*args, **kwargs):
            try:
                path = lock_path or os.environ.get("DESIGNATE_LOCK_PATH")
                lock = lockfile.FileLock(os.path.join(path, lock_prefix))
                with lock:
                    LOG.debug('Got lock "%s"' % f.__name__)
                    return f(*args, **kwargs)
            finally:
                LOG.debug('Lock released "%s"' % f.__name__)
        return wrapper
    return decorator


class BaseMigrationTestCase(test.BaseTestCase):
    """Base class fort testing of migration utils."""

    def __init__(self, *args, **kwargs):
        super(BaseMigrationTestCase, self).__init__(*args, **kwargs)

        self.DEFAULT_CONFIG_FILE = os.path.join(os.path.dirname(__file__),
                                                'test_migrations.conf')
        # Test machines can set the TEST_MIGRATIONS_CONF variable
        # to override the location of the config file for migration testing
        self.CONFIG_FILE_PATH = os.environ.get('TEST_MIGRATIONS_CONF',
                                               self.DEFAULT_CONFIG_FILE)
        self.test_databases = {}
        self.migration_api = None

    def setUp(self):
        super(BaseMigrationTestCase, self).setUp()

        # Load test databases from the config file. Only do this
        # once. No need to re-run this on each test...
        LOG.debug('config_path is %s' % self.CONFIG_FILE_PATH)
        if os.path.exists(self.CONFIG_FILE_PATH):
            cp = moves.configparser.RawConfigParser()
            try:
                cp.read(self.CONFIG_FILE_PATH)
                defaults = cp.defaults()
                for key, value in defaults.items():
                    self.test_databases[key] = value
            except moves.configparser.ParsingError as e:
                self.fail("Failed to read test_migrations.conf config "
                          "file. Got error: %s" % e)
        else:
            self.fail("Failed to find test_migrations.conf config "
                      "file.")

        self.engines = {}
        for key, value in self.test_databases.items():
            self.engines[key] = sqlalchemy.create_engine(value)

        # We start each test case with a completely blank slate.
        self._reset_databases()

    def tearDown(self):
        # We destroy the test data store between each test case,
        # and recreate it, which ensures that we have no side-effects
        # from the tests
        self._reset_databases()
        super(BaseMigrationTestCase, self).tearDown()

    def execute_cmd(self, cmd=None):
        process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE,
                                   stderr=subprocess.STDOUT)
        output = process.communicate()[0]
        LOG.debug(output)
        self.assertEqual(0, process.returncode,
                         "Failed to run: %s\n%s" % (cmd, output))

    def _reset_pg(self, conn_pieces):
        (user,
         password,
         database,
         host) = utils.get_db_connection_info(conn_pieces)
        os.environ['PGPASSWORD'] = password
        os.environ['PGUSER'] = user
        # note(boris-42): We must create and drop database, we can't
        # drop database which we have connected to, so for such
        # operations there is a special database template1.
        sqlcmd = ("psql -w -U %(user)s -h %(host)s -c"
                  " '%(sql)s' -d template1")

        sql = ("drop database if exists %s;") % database
        droptable = sqlcmd % {'user': user, 'host': host, 'sql': sql}
        self.execute_cmd(droptable)

        sql = ("create database %s;") % database
        createtable = sqlcmd % {'user': user, 'host': host, 'sql': sql}
        self.execute_cmd(createtable)

        os.unsetenv('PGPASSWORD')
        os.unsetenv('PGUSER')

    @_set_db_lock(lock_prefix='migration_tests-')
    def _reset_databases(self):
        for key, engine in self.engines.items():
            conn_string = self.test_databases[key]
            conn_pieces = parse.urlparse(conn_string)
            engine.dispose()
            if conn_string.startswith('sqlite'):
                # We can just delete the SQLite database, which is
                # the easiest and cleanest solution
                db_path = conn_pieces.path.strip('/')
                if os.path.exists(db_path):
                    os.unlink(db_path)
                # No need to recreate the SQLite DB. SQLite will
                # create it for us if it's not there...
            elif conn_string.startswith('mysql'):
                # We can execute the MySQL client to destroy and re-create
                # the MYSQL database, which is easier and less error-prone
                # than using SQLAlchemy to do this via MetaData...trust me.
                (user, password, database, host) = \
                    utils.get_db_connection_info(conn_pieces)
                sql = ("drop database if exists %(db)s; "
                       "create database %(db)s;") % {'db': database}
                cmd = ("mysql -u \"%(user)s\" -p\"%(password)s\" -h %(host)s "
                       "-e \"%(sql)s\"") % {'user': user, 'password': password,
                                            'host': host, 'sql': sql}
                self.execute_cmd(cmd)
            elif conn_string.startswith('postgresql'):
                self._reset_pg(conn_pieces)


class WalkVersionsMixin(object):
    def _walk_versions(self, engine=None, snake_walk=False, downgrade=True):
        # Determine latest version script from the repo, then
        # upgrade from 1 through to the latest, with no data
        # in the databases. This just checks that the schema itself
        # upgrades successfully.

        # Place the database under version control
        self.migration_api.version_control(engine, self.REPOSITORY,
                                           self.INIT_VERSION)
        self.assertEqual(self.INIT_VERSION,
                         self.migration_api.db_version(engine,
                                                       self.REPOSITORY))

        LOG.debug('latest version is %s' % self.REPOSITORY.latest)
        versions = range(self.INIT_VERSION + 1, self.REPOSITORY.latest + 1)

        for version in versions:
            # upgrade -> downgrade -> upgrade
            self._migrate_up(engine, version, with_data=True)
            if snake_walk:
                downgraded = self._migrate_down(
                    engine, version - 1, with_data=True)
                if downgraded:
                    self._migrate_up(engine, version)

        if downgrade:
            # Now walk it back down to 0 from the latest, testing
            # the downgrade paths.
            for version in reversed(versions):
                # downgrade -> upgrade -> downgrade
                downgraded = self._migrate_down(engine, version - 1)

                if snake_walk and downgraded:
                    self._migrate_up(engine, version)
                    self._migrate_down(engine, version - 1)

    def _migrate_down(self, engine, version, with_data=False):
        try:
            self.migration_api.downgrade(engine, self.REPOSITORY, version)
        except NotImplementedError:
            # NOTE(sirp): some migrations, namely release-level
            # migrations, don't support a downgrade.
            return False

        self.assertEqual(
            version, self.migration_api.db_version(engine, self.REPOSITORY))

        # NOTE(sirp): `version` is what we're downgrading to (i.e. the 'target'
        # version). So if we have any downgrade checks, they need to be run for
        # the previous (higher numbered) migration.
        if with_data:
            post_downgrade = getattr(
                self, "_post_downgrade_%03d" % (version + 1), None)
            if post_downgrade:
                post_downgrade(engine)

        return True

    def _migrate_up(self, engine, version, with_data=False):
        """migrate up to a new version of the db.

        We allow for data insertion and post checks at every
        migration version with special _pre_upgrade_### and
        _check_### functions in the main test.
        """
        # NOTE(sdague): try block is here because it's impossible to debug
        # where a failed data migration happens otherwise
        try:
            if with_data:
                data = None
                pre_upgrade = getattr(
                    self, "_pre_upgrade_%03d" % version, None)
                if pre_upgrade:
                    data = pre_upgrade(engine)

            self.migration_api.upgrade(engine, self.REPOSITORY, version)
            self.assertEqual(version,
                             self.migration_api.db_version(engine,
                                                           self.REPOSITORY))
            if with_data:
                check = getattr(self, "_check_%03d" % version, None)
                if check:
                    check(engine, data)
        except Exception:
            LOG.error(_LE("Failed to migrate to version %s on engine %s") %
                      (version, engine))
            raise

########NEW FILE########
__FILENAME__ = utils
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2010-2011 OpenStack Foundation.
# Copyright 2012 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import logging
import re

import sqlalchemy
from sqlalchemy import Boolean
from sqlalchemy import CheckConstraint
from sqlalchemy import Column
from sqlalchemy.engine import reflection
from sqlalchemy.ext.compiler import compiles
from sqlalchemy import func
from sqlalchemy import Index
from sqlalchemy import Integer
from sqlalchemy import MetaData
from sqlalchemy import or_
from sqlalchemy.sql.expression import literal_column
from sqlalchemy.sql.expression import UpdateBase
from sqlalchemy import String
from sqlalchemy import Table
from sqlalchemy.types import NullType

from designate.openstack.common import context as request_context
from designate.openstack.common.db.sqlalchemy import models
from designate.openstack.common.gettextutils import _, _LI, _LW
from designate.openstack.common import timeutils


LOG = logging.getLogger(__name__)

_DBURL_REGEX = re.compile(r"[^:]+://([^:]+):([^@]+)@.+")


def sanitize_db_url(url):
    match = _DBURL_REGEX.match(url)
    if match:
        return '%s****:****%s' % (url[:match.start(1)], url[match.end(2):])
    return url


class InvalidSortKey(Exception):
    message = _("Sort key supplied was not valid.")


# copy from glance/db/sqlalchemy/api.py
def paginate_query(query, model, limit, sort_keys, marker=None,
                   sort_dir=None, sort_dirs=None):
    """Returns a query with sorting / pagination criteria added.

    Pagination works by requiring a unique sort_key, specified by sort_keys.
    (If sort_keys is not unique, then we risk looping through values.)
    We use the last row in the previous page as the 'marker' for pagination.
    So we must return values that follow the passed marker in the order.
    With a single-valued sort_key, this would be easy: sort_key > X.
    With a compound-values sort_key, (k1, k2, k3) we must do this to repeat
    the lexicographical ordering:
    (k1 > X1) or (k1 == X1 && k2 > X2) or (k1 == X1 && k2 == X2 && k3 > X3)

    We also have to cope with different sort_directions.

    Typically, the id of the last row is used as the client-facing pagination
    marker, then the actual marker object must be fetched from the db and
    passed in to us as marker.

    :param query: the query object to which we should add paging/sorting
    :param model: the ORM model class
    :param limit: maximum number of items to return
    :param sort_keys: array of attributes by which results should be sorted
    :param marker: the last item of the previous page; we returns the next
                    results after this value.
    :param sort_dir: direction in which results should be sorted (asc, desc)
    :param sort_dirs: per-column array of sort_dirs, corresponding to sort_keys

    :rtype: sqlalchemy.orm.query.Query
    :return: The query with sorting/pagination added.
    """

    if 'id' not in sort_keys:
        # TODO(justinsb): If this ever gives a false-positive, check
        # the actual primary key, rather than assuming its id
        LOG.warning(_LW('Id not in sort_keys; is sort_keys unique?'))

    assert(not (sort_dir and sort_dirs))

    # Default the sort direction to ascending
    if sort_dirs is None and sort_dir is None:
        sort_dir = 'asc'

    # Ensure a per-column sort direction
    if sort_dirs is None:
        sort_dirs = [sort_dir for _sort_key in sort_keys]

    assert(len(sort_dirs) == len(sort_keys))

    # Add sorting
    for current_sort_key, current_sort_dir in zip(sort_keys, sort_dirs):
        try:
            sort_dir_func = {
                'asc': sqlalchemy.asc,
                'desc': sqlalchemy.desc,
            }[current_sort_dir]
        except KeyError:
            raise ValueError(_("Unknown sort direction, "
                               "must be 'desc' or 'asc'"))
        try:
            sort_key_attr = getattr(model, current_sort_key)
        except AttributeError:
            raise InvalidSortKey()
        query = query.order_by(sort_dir_func(sort_key_attr))

    # Add pagination
    if marker is not None:
        marker_values = []
        for sort_key in sort_keys:
            v = getattr(marker, sort_key)
            marker_values.append(v)

        # Build up an array of sort criteria as in the docstring
        criteria_list = []
        for i in range(len(sort_keys)):
            crit_attrs = []
            for j in range(i):
                model_attr = getattr(model, sort_keys[j])
                crit_attrs.append((model_attr == marker_values[j]))

            model_attr = getattr(model, sort_keys[i])
            if sort_dirs[i] == 'desc':
                crit_attrs.append((model_attr < marker_values[i]))
            else:
                crit_attrs.append((model_attr > marker_values[i]))

            criteria = sqlalchemy.sql.and_(*crit_attrs)
            criteria_list.append(criteria)

        f = sqlalchemy.sql.or_(*criteria_list)
        query = query.filter(f)

    if limit is not None:
        query = query.limit(limit)

    return query


def _read_deleted_filter(query, db_model, read_deleted):
    if 'deleted' not in db_model.__table__.columns:
        raise ValueError(_("There is no `deleted` column in `%s` table. "
                           "Project doesn't use soft-deleted feature.")
                         % db_model.__name__)

    default_deleted_value = db_model.__table__.c.deleted.default.arg
    if read_deleted == 'no':
        query = query.filter(db_model.deleted == default_deleted_value)
    elif read_deleted == 'yes':
        pass  # omit the filter to include deleted and active
    elif read_deleted == 'only':
        query = query.filter(db_model.deleted != default_deleted_value)
    else:
        raise ValueError(_("Unrecognized read_deleted value '%s'")
                         % read_deleted)
    return query


def _project_filter(query, db_model, context, project_only):
    if project_only and 'project_id' not in db_model.__table__.columns:
        raise ValueError(_("There is no `project_id` column in `%s` table.")
                         % db_model.__name__)

    if request_context.is_user_context(context) and project_only:
        if project_only == 'allow_none':
            is_none = None
            query = query.filter(or_(db_model.project_id == context.project_id,
                                     db_model.project_id == is_none))
        else:
            query = query.filter(db_model.project_id == context.project_id)

    return query


def model_query(context, model, session, args=None, project_only=False,
                read_deleted=None):
    """Query helper that accounts for context's `read_deleted` field.

    :param context:      context to query under

    :param model:        Model to query. Must be a subclass of ModelBase.
    :type model:         models.ModelBase

    :param session:      The session to use.
    :type session:       sqlalchemy.orm.session.Session

    :param args:         Arguments to query. If None - model is used.
    :type args:          tuple

    :param project_only: If present and context is user-type, then restrict
                         query to match the context's project_id. If set to
                         'allow_none', restriction includes project_id = None.
    :type project_only:  bool

    :param read_deleted: If present, overrides context's read_deleted field.
    :type read_deleted:   bool

    Usage:

    ..code:: python

        result = (utils.model_query(context, models.Instance, session=session)
                       .filter_by(uuid=instance_uuid)
                       .all())

        query = utils.model_query(
                    context, Node,
                    session=session,
                    args=(func.count(Node.id), func.sum(Node.ram))
                    ).filter_by(project_id=project_id)

    """

    if not read_deleted:
        if hasattr(context, 'read_deleted'):
            # NOTE(viktors): some projects use `read_deleted` attribute in
            # their contexts instead of `show_deleted`.
            read_deleted = context.read_deleted
        else:
            read_deleted = context.show_deleted

    if not issubclass(model, models.ModelBase):
        raise TypeError(_("model should be a subclass of ModelBase"))

    query = session.query(model) if not args else session.query(*args)
    query = _read_deleted_filter(query, model, read_deleted)
    query = _project_filter(query, model, context, project_only)

    return query


def get_table(engine, name):
    """Returns an sqlalchemy table dynamically from db.

    Needed because the models don't work for us in migrations
    as models will be far out of sync with the current data.
    """
    metadata = MetaData()
    metadata.bind = engine
    return Table(name, metadata, autoload=True)


class InsertFromSelect(UpdateBase):
    """Form the base for `INSERT INTO table (SELECT ... )` statement."""
    def __init__(self, table, select):
        self.table = table
        self.select = select


@compiles(InsertFromSelect)
def visit_insert_from_select(element, compiler, **kw):
    """Form the `INSERT INTO table (SELECT ... )` statement."""
    return "INSERT INTO %s %s" % (
        compiler.process(element.table, asfrom=True),
        compiler.process(element.select))


class ColumnError(Exception):
    """Error raised when no column or an invalid column is found."""


def _get_not_supported_column(col_name_col_instance, column_name):
    try:
        column = col_name_col_instance[column_name]
    except KeyError:
        msg = _("Please specify column %s in col_name_col_instance "
                "param. It is required because column has unsupported "
                "type by sqlite).")
        raise ColumnError(msg % column_name)

    if not isinstance(column, Column):
        msg = _("col_name_col_instance param has wrong type of "
                "column instance for column %s It should be instance "
                "of sqlalchemy.Column.")
        raise ColumnError(msg % column_name)
    return column


def drop_unique_constraint(migrate_engine, table_name, uc_name, *columns,
                           **col_name_col_instance):
    """Drop unique constraint from table.

    DEPRECATED: this function is deprecated and will be removed from designate.db
    in a few releases. Please use UniqueConstraint.drop() method directly for
    sqlalchemy-migrate migration scripts.

    This method drops UC from table and works for mysql, postgresql and sqlite.
    In mysql and postgresql we are able to use "alter table" construction.
    Sqlalchemy doesn't support some sqlite column types and replaces their
    type with NullType in metadata. We process these columns and replace
    NullType with the correct column type.

    :param migrate_engine: sqlalchemy engine
    :param table_name:     name of table that contains uniq constraint.
    :param uc_name:        name of uniq constraint that will be dropped.
    :param columns:        columns that are in uniq constraint.
    :param col_name_col_instance:   contains pair column_name=column_instance.
                            column_instance is instance of Column. These params
                            are required only for columns that have unsupported
                            types by sqlite. For example BigInteger.
    """

    from migrate.changeset import UniqueConstraint

    meta = MetaData()
    meta.bind = migrate_engine
    t = Table(table_name, meta, autoload=True)

    if migrate_engine.name == "sqlite":
        override_cols = [
            _get_not_supported_column(col_name_col_instance, col.name)
            for col in t.columns
            if isinstance(col.type, NullType)
        ]
        for col in override_cols:
            t.columns.replace(col)

    uc = UniqueConstraint(*columns, table=t, name=uc_name)
    uc.drop()


def drop_old_duplicate_entries_from_table(migrate_engine, table_name,
                                          use_soft_delete, *uc_column_names):
    """Drop all old rows having the same values for columns in uc_columns.

    This method drop (or mark ad `deleted` if use_soft_delete is True) old
    duplicate rows form table with name `table_name`.

    :param migrate_engine:  Sqlalchemy engine
    :param table_name:      Table with duplicates
    :param use_soft_delete: If True - values will be marked as `deleted`,
                            if False - values will be removed from table
    :param uc_column_names: Unique constraint columns
    """
    meta = MetaData()
    meta.bind = migrate_engine

    table = Table(table_name, meta, autoload=True)
    columns_for_group_by = [table.c[name] for name in uc_column_names]

    columns_for_select = [func.max(table.c.id)]
    columns_for_select.extend(columns_for_group_by)

    duplicated_rows_select = sqlalchemy.sql.select(
        columns_for_select, group_by=columns_for_group_by,
        having=func.count(table.c.id) > 1)

    for row in migrate_engine.execute(duplicated_rows_select):
        # NOTE(boris-42): Do not remove row that has the biggest ID.
        delete_condition = table.c.id != row[0]
        is_none = None  # workaround for pyflakes
        delete_condition &= table.c.deleted_at == is_none
        for name in uc_column_names:
            delete_condition &= table.c[name] == row[name]

        rows_to_delete_select = sqlalchemy.sql.select(
            [table.c.id]).where(delete_condition)
        for row in migrate_engine.execute(rows_to_delete_select).fetchall():
            LOG.info(_LI("Deleting duplicated row with id: %(id)s from table: "
                         "%(table)s") % dict(id=row[0], table=table_name))

        if use_soft_delete:
            delete_statement = table.update().\
                where(delete_condition).\
                values({
                    'deleted': literal_column('id'),
                    'updated_at': literal_column('updated_at'),
                    'deleted_at': timeutils.utcnow()
                })
        else:
            delete_statement = table.delete().where(delete_condition)
        migrate_engine.execute(delete_statement)


def _get_default_deleted_value(table):
    if isinstance(table.c.id.type, Integer):
        return 0
    if isinstance(table.c.id.type, String):
        return ""
    raise ColumnError(_("Unsupported id columns type"))


def _restore_indexes_on_deleted_columns(migrate_engine, table_name, indexes):
    table = get_table(migrate_engine, table_name)

    insp = reflection.Inspector.from_engine(migrate_engine)
    real_indexes = insp.get_indexes(table_name)
    existing_index_names = dict(
        [(index['name'], index['column_names']) for index in real_indexes])

    # NOTE(boris-42): Restore indexes on `deleted` column
    for index in indexes:
        if 'deleted' not in index['column_names']:
            continue
        name = index['name']
        if name in existing_index_names:
            column_names = [table.c[c] for c in existing_index_names[name]]
            old_index = Index(name, *column_names, unique=index["unique"])
            old_index.drop(migrate_engine)

        column_names = [table.c[c] for c in index['column_names']]
        new_index = Index(index["name"], *column_names, unique=index["unique"])
        new_index.create(migrate_engine)


def change_deleted_column_type_to_boolean(migrate_engine, table_name,
                                          **col_name_col_instance):
    if migrate_engine.name == "sqlite":
        return _change_deleted_column_type_to_boolean_sqlite(
            migrate_engine, table_name, **col_name_col_instance)
    insp = reflection.Inspector.from_engine(migrate_engine)
    indexes = insp.get_indexes(table_name)

    table = get_table(migrate_engine, table_name)

    old_deleted = Column('old_deleted', Boolean, default=False)
    old_deleted.create(table, populate_default=False)

    table.update().\
        where(table.c.deleted == table.c.id).\
        values(old_deleted=True).\
        execute()

    table.c.deleted.drop()
    table.c.old_deleted.alter(name="deleted")

    _restore_indexes_on_deleted_columns(migrate_engine, table_name, indexes)


def _change_deleted_column_type_to_boolean_sqlite(migrate_engine, table_name,
                                                  **col_name_col_instance):
    insp = reflection.Inspector.from_engine(migrate_engine)
    table = get_table(migrate_engine, table_name)

    columns = []
    for column in table.columns:
        column_copy = None
        if column.name != "deleted":
            if isinstance(column.type, NullType):
                column_copy = _get_not_supported_column(col_name_col_instance,
                                                        column.name)
            else:
                column_copy = column.copy()
        else:
            column_copy = Column('deleted', Boolean, default=0)
        columns.append(column_copy)

    constraints = [constraint.copy() for constraint in table.constraints]

    meta = table.metadata
    new_table = Table(table_name + "__tmp__", meta,
                      *(columns + constraints))
    new_table.create()

    indexes = []
    for index in insp.get_indexes(table_name):
        column_names = [new_table.c[c] for c in index['column_names']]
        indexes.append(Index(index["name"], *column_names,
                             unique=index["unique"]))

    c_select = []
    for c in table.c:
        if c.name != "deleted":
            c_select.append(c)
        else:
            c_select.append(table.c.deleted == table.c.id)

    ins = InsertFromSelect(new_table, sqlalchemy.sql.select(c_select))
    migrate_engine.execute(ins)

    table.drop()
    [index.create(migrate_engine) for index in indexes]

    new_table.rename(table_name)
    new_table.update().\
        where(new_table.c.deleted == new_table.c.id).\
        values(deleted=True).\
        execute()


def change_deleted_column_type_to_id_type(migrate_engine, table_name,
                                          **col_name_col_instance):
    if migrate_engine.name == "sqlite":
        return _change_deleted_column_type_to_id_type_sqlite(
            migrate_engine, table_name, **col_name_col_instance)
    insp = reflection.Inspector.from_engine(migrate_engine)
    indexes = insp.get_indexes(table_name)

    table = get_table(migrate_engine, table_name)

    new_deleted = Column('new_deleted', table.c.id.type,
                         default=_get_default_deleted_value(table))
    new_deleted.create(table, populate_default=True)

    deleted = True  # workaround for pyflakes
    table.update().\
        where(table.c.deleted == deleted).\
        values(new_deleted=table.c.id).\
        execute()
    table.c.deleted.drop()
    table.c.new_deleted.alter(name="deleted")

    _restore_indexes_on_deleted_columns(migrate_engine, table_name, indexes)


def _change_deleted_column_type_to_id_type_sqlite(migrate_engine, table_name,
                                                  **col_name_col_instance):
    # NOTE(boris-42): sqlaclhemy-migrate can't drop column with check
    #                 constraints in sqlite DB and our `deleted` column has
    #                 2 check constraints. So there is only one way to remove
    #                 these constraints:
    #                 1) Create new table with the same columns, constraints
    #                 and indexes. (except deleted column).
    #                 2) Copy all data from old to new table.
    #                 3) Drop old table.
    #                 4) Rename new table to old table name.
    insp = reflection.Inspector.from_engine(migrate_engine)
    meta = MetaData(bind=migrate_engine)
    table = Table(table_name, meta, autoload=True)
    default_deleted_value = _get_default_deleted_value(table)

    columns = []
    for column in table.columns:
        column_copy = None
        if column.name != "deleted":
            if isinstance(column.type, NullType):
                column_copy = _get_not_supported_column(col_name_col_instance,
                                                        column.name)
            else:
                column_copy = column.copy()
        else:
            column_copy = Column('deleted', table.c.id.type,
                                 default=default_deleted_value)
        columns.append(column_copy)

    def is_deleted_column_constraint(constraint):
        # NOTE(boris-42): There is no other way to check is CheckConstraint
        #                 associated with deleted column.
        if not isinstance(constraint, CheckConstraint):
            return False
        sqltext = str(constraint.sqltext)
        return (sqltext.endswith("deleted in (0, 1)") or
                sqltext.endswith("deleted IN (:deleted_1, :deleted_2)"))

    constraints = []
    for constraint in table.constraints:
        if not is_deleted_column_constraint(constraint):
            constraints.append(constraint.copy())

    new_table = Table(table_name + "__tmp__", meta,
                      *(columns + constraints))
    new_table.create()

    indexes = []
    for index in insp.get_indexes(table_name):
        column_names = [new_table.c[c] for c in index['column_names']]
        indexes.append(Index(index["name"], *column_names,
                             unique=index["unique"]))

    ins = InsertFromSelect(new_table, table.select())
    migrate_engine.execute(ins)

    table.drop()
    [index.create(migrate_engine) for index in indexes]

    new_table.rename(table_name)
    deleted = True  # workaround for pyflakes
    new_table.update().\
        where(new_table.c.deleted == deleted).\
        values(deleted=new_table.c.id).\
        execute()

    # NOTE(boris-42): Fix value of deleted column: False -> "" or 0.
    deleted = False  # workaround for pyflakes
    new_table.update().\
        where(new_table.c.deleted == deleted).\
        values(deleted=default_deleted_value).\
        execute()


def get_connect_string(backend, database, user=None, passwd=None):
    """Get database connection

    Try to get a connection with a very specific set of values, if we get
    these then we'll run the tests, otherwise they are skipped
    """
    args = {'backend': backend,
            'user': user,
            'passwd': passwd,
            'database': database}
    if backend == 'sqlite':
        template = '%(backend)s:///%(database)s'
    else:
        template = "%(backend)s://%(user)s:%(passwd)s@localhost/%(database)s"
    return template % args


def is_backend_avail(backend, database, user=None, passwd=None):
    try:
        connect_uri = get_connect_string(backend=backend,
                                         database=database,
                                         user=user,
                                         passwd=passwd)
        engine = sqlalchemy.create_engine(connect_uri)
        connection = engine.connect()
    except Exception:
        # intentionally catch all to handle exceptions even if we don't
        # have any backend code loaded.
        return False
    else:
        connection.close()
        engine.dispose()
        return True


def get_db_connection_info(conn_pieces):
    database = conn_pieces.path.strip('/')
    loc_pieces = conn_pieces.netloc.split('@')
    host = loc_pieces[1]

    auth_pieces = loc_pieces[0].split(':')
    user = auth_pieces[0]
    password = ""
    if len(auth_pieces) > 1:
        password = auth_pieces[1].strip()

    return (user, password, database, host)

########NEW FILE########
__FILENAME__ = eventlet_backdoor
# Copyright (c) 2012 OpenStack Foundation.
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from __future__ import print_function

import errno
import gc
import os
import pprint
import socket
import sys
import traceback

import eventlet
import eventlet.backdoor
import greenlet
from oslo.config import cfg

from designate.openstack.common.gettextutils import _LI
from designate.openstack.common import log as logging

help_for_backdoor_port = (
    "Acceptable values are 0, <port>, and <start>:<end>, where 0 results "
    "in listening on a random tcp port number; <port> results in listening "
    "on the specified port number (and not enabling backdoor if that port "
    "is in use); and <start>:<end> results in listening on the smallest "
    "unused port number within the specified range of port numbers.  The "
    "chosen port is displayed in the service's log file.")
eventlet_backdoor_opts = [
    cfg.StrOpt('backdoor_port',
               default=None,
               help="Enable eventlet backdoor.  %s" % help_for_backdoor_port)
]

CONF = cfg.CONF
CONF.register_opts(eventlet_backdoor_opts)
LOG = logging.getLogger(__name__)


class EventletBackdoorConfigValueError(Exception):
    def __init__(self, port_range, help_msg, ex):
        msg = ('Invalid backdoor_port configuration %(range)s: %(ex)s. '
               '%(help)s' %
               {'range': port_range, 'ex': ex, 'help': help_msg})
        super(EventletBackdoorConfigValueError, self).__init__(msg)
        self.port_range = port_range


def _dont_use_this():
    print("Don't use this, just disconnect instead")


def _find_objects(t):
    return [o for o in gc.get_objects() if isinstance(o, t)]


def _print_greenthreads():
    for i, gt in enumerate(_find_objects(greenlet.greenlet)):
        print(i, gt)
        traceback.print_stack(gt.gr_frame)
        print()


def _print_nativethreads():
    for threadId, stack in sys._current_frames().items():
        print(threadId)
        traceback.print_stack(stack)
        print()


def _parse_port_range(port_range):
    if ':' not in port_range:
        start, end = port_range, port_range
    else:
        start, end = port_range.split(':', 1)
    try:
        start, end = int(start), int(end)
        if end < start:
            raise ValueError
        return start, end
    except ValueError as ex:
        raise EventletBackdoorConfigValueError(port_range, ex,
                                               help_for_backdoor_port)


def _listen(host, start_port, end_port, listen_func):
    try_port = start_port
    while True:
        try:
            return listen_func((host, try_port))
        except socket.error as exc:
            if (exc.errno != errno.EADDRINUSE or
               try_port >= end_port):
                raise
            try_port += 1


def initialize_if_enabled():
    backdoor_locals = {
        'exit': _dont_use_this,      # So we don't exit the entire process
        'quit': _dont_use_this,      # So we don't exit the entire process
        'fo': _find_objects,
        'pgt': _print_greenthreads,
        'pnt': _print_nativethreads,
    }

    if CONF.backdoor_port is None:
        return None

    start_port, end_port = _parse_port_range(str(CONF.backdoor_port))

    # NOTE(johannes): The standard sys.displayhook will print the value of
    # the last expression and set it to __builtin__._, which overwrites
    # the __builtin__._ that gettext sets. Let's switch to using pprint
    # since it won't interact poorly with gettext, and it's easier to
    # read the output too.
    def displayhook(val):
        if val is not None:
            pprint.pprint(val)
    sys.displayhook = displayhook

    sock = _listen('localhost', start_port, end_port, eventlet.listen)

    # In the case of backdoor port being zero, a port number is assigned by
    # listen().  In any case, pull the port number out here.
    port = sock.getsockname()[1]
    LOG.info(
        _LI('Eventlet backdoor listening on %(port)s for process %(pid)d') %
        {'port': port, 'pid': os.getpid()}
    )
    eventlet.spawn_n(eventlet.backdoor.backdoor_server, sock,
                     locals=backdoor_locals)
    return port

########NEW FILE########
__FILENAME__ = excutils
# Copyright 2011 OpenStack Foundation.
# Copyright 2012, Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Exception related utilities.
"""

import logging
import sys
import time
import traceback

import six

from designate.openstack.common.gettextutils import _LE


class save_and_reraise_exception(object):
    """Save current exception, run some code and then re-raise.

    In some cases the exception context can be cleared, resulting in None
    being attempted to be re-raised after an exception handler is run. This
    can happen when eventlet switches greenthreads or when running an
    exception handler, code raises and catches an exception. In both
    cases the exception context will be cleared.

    To work around this, we save the exception state, run handler code, and
    then re-raise the original exception. If another exception occurs, the
    saved exception is logged and the new exception is re-raised.

    In some cases the caller may not want to re-raise the exception, and
    for those circumstances this context provides a reraise flag that
    can be used to suppress the exception.  For example::

      except Exception:
          with save_and_reraise_exception() as ctxt:
              decide_if_need_reraise()
              if not should_be_reraised:
                  ctxt.reraise = False

    If another exception occurs and reraise flag is False,
    the saved exception will not be logged.

    If the caller wants to raise new exception during exception handling
    he/she sets reraise to False initially with an ability to set it back to
    True if needed::

      except Exception:
          with save_and_reraise_exception(reraise=False) as ctxt:
              [if statements to determine whether to raise a new exception]
              # Not raising a new exception, so reraise
              ctxt.reraise = True
    """
    def __init__(self, reraise=True):
        self.reraise = reraise

    def __enter__(self):
        self.type_, self.value, self.tb, = sys.exc_info()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type is not None:
            if self.reraise:
                logging.error(_LE('Original exception being dropped: %s'),
                              traceback.format_exception(self.type_,
                                                         self.value,
                                                         self.tb))
            return False
        if self.reraise:
            six.reraise(self.type_, self.value, self.tb)


def forever_retry_uncaught_exceptions(infunc):
    def inner_func(*args, **kwargs):
        last_log_time = 0
        last_exc_message = None
        exc_count = 0
        while True:
            try:
                return infunc(*args, **kwargs)
            except Exception as exc:
                this_exc_message = six.u(str(exc))
                if this_exc_message == last_exc_message:
                    exc_count += 1
                else:
                    exc_count = 1
                # Do not log any more frequently than once a minute unless
                # the exception message changes
                cur_time = int(time.time())
                if (cur_time - last_log_time > 60 or
                        this_exc_message != last_exc_message):
                    logging.exception(
                        _LE('Unexpected exception occurred %d time(s)... '
                            'retrying.') % exc_count)
                    last_log_time = cur_time
                    last_exc_message = this_exc_message
                    exc_count = 0
                # This should be a very rare event. In case it isn't, do
                # a sleep.
                time.sleep(1)
    return inner_func

########NEW FILE########
__FILENAME__ = fileutils
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import contextlib
import errno
import os
import tempfile

from designate.openstack.common import excutils
from designate.openstack.common import log as logging

LOG = logging.getLogger(__name__)

_FILE_CACHE = {}


def ensure_tree(path):
    """Create a directory (and any ancestor directories required)

    :param path: Directory to create
    """
    try:
        os.makedirs(path)
    except OSError as exc:
        if exc.errno == errno.EEXIST:
            if not os.path.isdir(path):
                raise
        else:
            raise


def read_cached_file(filename, force_reload=False):
    """Read from a file if it has been modified.

    :param force_reload: Whether to reload the file.
    :returns: A tuple with a boolean specifying if the data is fresh
              or not.
    """
    global _FILE_CACHE

    if force_reload and filename in _FILE_CACHE:
        del _FILE_CACHE[filename]

    reloaded = False
    mtime = os.path.getmtime(filename)
    cache_info = _FILE_CACHE.setdefault(filename, {})

    if not cache_info or mtime > cache_info.get('mtime', 0):
        LOG.debug("Reloading cached file %s" % filename)
        with open(filename) as fap:
            cache_info['data'] = fap.read()
        cache_info['mtime'] = mtime
        reloaded = True
    return (reloaded, cache_info['data'])


def delete_if_exists(path, remove=os.unlink):
    """Delete a file, but ignore file not found error.

    :param path: File to delete
    :param remove: Optional function to remove passed path
    """

    try:
        remove(path)
    except OSError as e:
        if e.errno != errno.ENOENT:
            raise


@contextlib.contextmanager
def remove_path_on_error(path, remove=delete_if_exists):
    """Protect code that wants to operate on PATH atomically.
    Any exception will cause PATH to be removed.

    :param path: File to work with
    :param remove: Optional function to remove passed path
    """

    try:
        yield
    except Exception:
        with excutils.save_and_reraise_exception():
            remove(path)


def file_open(*args, **kwargs):
    """Open file

    see built-in file() documentation for more details

    Note: The reason this is kept in a separate module is to easily
    be able to provide a stub module that doesn't alter system
    state at all (for unit tests)
    """
    return file(*args, **kwargs)


def write_to_tempfile(content, path=None, suffix='', prefix='tmp'):
    """Create temporary file or use existing file.

    This util is needed for creating temporary file with
    specified content, suffix and prefix. If path is not None,
    it will be used for writing content. If the path doesn't
    exist it'll be created.

    :param content: content for temporary file.
    :param path: same as parameter 'dir' for mkstemp
    :param suffix: same as parameter 'suffix' for mkstemp
    :param prefix: same as parameter 'prefix' for mkstemp

    For example: it can be used in database tests for creating
    configuration files.
    """
    if path:
        ensure_tree(path)

    (fd, path) = tempfile.mkstemp(suffix=suffix, dir=path, prefix=prefix)
    try:
        os.write(fd, content)
    finally:
        os.close(fd)
    return path

########NEW FILE########
__FILENAME__ = config
#
# Copyright 2013 Mirantis, Inc.
# Copyright 2013 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import fixtures
from oslo.config import cfg
import six


class Config(fixtures.Fixture):
    """Allows overriding configuration settings for the test.

    `conf` will be reset on cleanup.

    """

    def __init__(self, conf=cfg.CONF):
        self.conf = conf

    def setUp(self):
        super(Config, self).setUp()
        # NOTE(morganfainberg): unregister must be added to cleanup before
        # reset is because cleanup works in reverse order of registered items,
        # and a reset must occur before unregistering options can occur.
        self.addCleanup(self._unregister_config_opts)
        self.addCleanup(self.conf.reset)
        self._registered_config_opts = {}

    def config(self, **kw):
        """Override configuration values.

        The keyword arguments are the names of configuration options to
        override and their values.

        If a `group` argument is supplied, the overrides are applied to
        the specified configuration option group, otherwise the overrides
        are applied to the ``default`` group.

        """

        group = kw.pop('group', None)
        for k, v in six.iteritems(kw):
            self.conf.set_override(k, v, group)

    def _unregister_config_opts(self):
        for group in self._registered_config_opts:
            self.conf.unregister_opts(self._registered_config_opts[group],
                                      group=group)

    def register_opt(self, opt, group=None):
        """Register a single option for the test run.

        Options registered in this manner will automatically be unregistered
        during cleanup.

        If a `group` argument is supplied, it will register the new option
        to that group, otherwise the option is registered to the ``default``
        group.
        """
        self.conf.register_opt(opt, group=group)
        self._registered_config_opts.setdefault(group, set()).add(opt)

    def register_opts(self, opts, group=None):
        """Register multiple options for the test run.

        This works in the same manner as register_opt() but takes a list of
        options as the first argument. All arguments will be registered to the
        same group if the ``group`` argument is supplied, otherwise all options
        will be registered to the ``default`` group.
        """
        for opt in opts:
            self.register_opt(opt, group=group)

########NEW FILE########
__FILENAME__ = lockutils
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import fixtures

from designate.openstack.common import lockutils


class LockFixture(fixtures.Fixture):
    """External locking fixture.

    This fixture is basically an alternative to the synchronized decorator with
    the external flag so that tearDowns and addCleanups will be included in
    the lock context for locking between tests. The fixture is recommended to
    be the first line in a test method, like so::

        def test_method(self):
            self.useFixture(LockFixture)
                ...

    or the first line in setUp if all the test methods in the class are
    required to be serialized. Something like::

        class TestCase(testtools.testcase):
            def setUp(self):
                self.useFixture(LockFixture)
                super(TestCase, self).setUp()
                    ...

    This is because addCleanups are put on a LIFO queue that gets run after the
    test method exits. (either by completing or raising an exception)
    """
    def __init__(self, name, lock_file_prefix=None):
        self.mgr = lockutils.lock(name, lock_file_prefix, True)

    def setUp(self):
        super(LockFixture, self).setUp()
        self.addCleanup(self.mgr.__exit__, None, None, None)
        self.lock = self.mgr.__enter__()

########NEW FILE########
__FILENAME__ = logging
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import fixtures


def get_logging_handle_error_fixture():
    """returns a fixture to make logging raise formatting exceptions.

    Usage:
    self.useFixture(logging.get_logging_handle_error_fixture())
    """
    return fixtures.MonkeyPatch('logging.Handler.handleError',
                                _handleError)


def _handleError(self, record):
    """Monkey patch for logging.Handler.handleError.

    The default handleError just logs the error to stderr but we want
    the option of actually raising an exception.
    """
    raise

########NEW FILE########
__FILENAME__ = mockpatch
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

##############################################################################
##############################################################################
##
## DO NOT MODIFY THIS FILE
##
## This file is being graduated to the designatetest library. Please make all
## changes there, and only backport critical fixes here. - dhellmann
##
##############################################################################
##############################################################################

import fixtures
import mock


class PatchObject(fixtures.Fixture):
    """Deal with code around mock."""

    def __init__(self, obj, attr, new=mock.DEFAULT, **kwargs):
        self.obj = obj
        self.attr = attr
        self.kwargs = kwargs
        self.new = new

    def setUp(self):
        super(PatchObject, self).setUp()
        _p = mock.patch.object(self.obj, self.attr, self.new, **self.kwargs)
        self.mock = _p.start()
        self.addCleanup(_p.stop)


class Patch(fixtures.Fixture):

    """Deal with code around mock.patch."""

    def __init__(self, obj, new=mock.DEFAULT, **kwargs):
        self.obj = obj
        self.kwargs = kwargs
        self.new = new

    def setUp(self):
        super(Patch, self).setUp()
        _p = mock.patch(self.obj, self.new, **self.kwargs)
        self.mock = _p.start()
        self.addCleanup(_p.stop)

########NEW FILE########
__FILENAME__ = moxstubout
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

##############################################################################
##############################################################################
##
## DO NOT MODIFY THIS FILE
##
## This file is being graduated to the designatetest library. Please make all
## changes there, and only backport critical fixes here. - dhellmann
##
##############################################################################
##############################################################################

import fixtures
from six.moves import mox


class MoxStubout(fixtures.Fixture):
    """Deal with code around mox and stubout as a fixture."""

    def setUp(self):
        super(MoxStubout, self).setUp()
        # emulate some of the mox stuff, we can't use the metaclass
        # because it screws with our generators
        self.mox = mox.Mox()
        self.stubs = self.mox.stubs
        self.addCleanup(self.mox.UnsetStubs)
        self.addCleanup(self.mox.VerifyAll)

########NEW FILE########
__FILENAME__ = gettextutils
# Copyright 2012 Red Hat, Inc.
# Copyright 2013 IBM Corp.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
gettext for openstack-common modules.

Usual usage in an openstack.common module:

    from designate.openstack.common.gettextutils import _
"""

import copy
import functools
import gettext
import locale
from logging import handlers
import os

from babel import localedata
import six

_localedir = os.environ.get('designate'.upper() + '_LOCALEDIR')
_t = gettext.translation('designate', localedir=_localedir, fallback=True)

# We use separate translation catalogs for each log level, so set up a
# mapping between the log level name and the translator. The domain
# for the log level is project_name + "-log-" + log_level so messages
# for each level end up in their own catalog.
_t_log_levels = dict(
    (level, gettext.translation('designate' + '-log-' + level,
                                localedir=_localedir,
                                fallback=True))
    for level in ['info', 'warning', 'error', 'critical']
)

_AVAILABLE_LANGUAGES = {}
USE_LAZY = False


def enable_lazy():
    """Convenience function for configuring _() to use lazy gettext

    Call this at the start of execution to enable the gettextutils._
    function to use lazy gettext functionality. This is useful if
    your project is importing _ directly instead of using the
    gettextutils.install() way of importing the _ function.
    """
    global USE_LAZY
    USE_LAZY = True


def _(msg):
    if USE_LAZY:
        return Message(msg, domain='designate')
    else:
        if six.PY3:
            return _t.gettext(msg)
        return _t.ugettext(msg)


def _log_translation(msg, level):
    """Build a single translation of a log message
    """
    if USE_LAZY:
        return Message(msg, domain='designate' + '-log-' + level)
    else:
        translator = _t_log_levels[level]
        if six.PY3:
            return translator.gettext(msg)
        return translator.ugettext(msg)

# Translators for log levels.
#
# The abbreviated names are meant to reflect the usual use of a short
# name like '_'. The "L" is for "log" and the other letter comes from
# the level.
_LI = functools.partial(_log_translation, level='info')
_LW = functools.partial(_log_translation, level='warning')
_LE = functools.partial(_log_translation, level='error')
_LC = functools.partial(_log_translation, level='critical')


def install(domain, lazy=False):
    """Install a _() function using the given translation domain.

    Given a translation domain, install a _() function using gettext's
    install() function.

    The main difference from gettext.install() is that we allow
    overriding the default localedir (e.g. /usr/share/locale) using
    a translation-domain-specific environment variable (e.g.
    NOVA_LOCALEDIR).

    :param domain: the translation domain
    :param lazy: indicates whether or not to install the lazy _() function.
                 The lazy _() introduces a way to do deferred translation
                 of messages by installing a _ that builds Message objects,
                 instead of strings, which can then be lazily translated into
                 any available locale.
    """
    if lazy:
        # NOTE(mrodden): Lazy gettext functionality.
        #
        # The following introduces a deferred way to do translations on
        # messages in OpenStack. We override the standard _() function
        # and % (format string) operation to build Message objects that can
        # later be translated when we have more information.
        def _lazy_gettext(msg):
            """Create and return a Message object.

            Lazy gettext function for a given domain, it is a factory method
            for a project/module to get a lazy gettext function for its own
            translation domain (i.e. nova, glance, cinder, etc.)

            Message encapsulates a string so that we can translate
            it later when needed.
            """
            return Message(msg, domain=domain)

        from six import moves
        moves.builtins.__dict__['_'] = _lazy_gettext
    else:
        localedir = '%s_LOCALEDIR' % domain.upper()
        if six.PY3:
            gettext.install(domain,
                            localedir=os.environ.get(localedir))
        else:
            gettext.install(domain,
                            localedir=os.environ.get(localedir),
                            unicode=True)


class Message(six.text_type):
    """A Message object is a unicode object that can be translated.

    Translation of Message is done explicitly using the translate() method.
    For all non-translation intents and purposes, a Message is simply unicode,
    and can be treated as such.
    """

    def __new__(cls, msgid, msgtext=None, params=None,
                domain='designate', *args):
        """Create a new Message object.

        In order for translation to work gettext requires a message ID, this
        msgid will be used as the base unicode text. It is also possible
        for the msgid and the base unicode text to be different by passing
        the msgtext parameter.
        """
        # If the base msgtext is not given, we use the default translation
        # of the msgid (which is in English) just in case the system locale is
        # not English, so that the base text will be in that locale by default.
        if not msgtext:
            msgtext = Message._translate_msgid(msgid, domain)
        # We want to initialize the parent unicode with the actual object that
        # would have been plain unicode if 'Message' was not enabled.
        msg = super(Message, cls).__new__(cls, msgtext)
        msg.msgid = msgid
        msg.domain = domain
        msg.params = params
        return msg

    def translate(self, desired_locale=None):
        """Translate this message to the desired locale.

        :param desired_locale: The desired locale to translate the message to,
                               if no locale is provided the message will be
                               translated to the system's default locale.

        :returns: the translated message in unicode
        """

        translated_message = Message._translate_msgid(self.msgid,
                                                      self.domain,
                                                      desired_locale)
        if self.params is None:
            # No need for more translation
            return translated_message

        # This Message object may have been formatted with one or more
        # Message objects as substitution arguments, given either as a single
        # argument, part of a tuple, or as one or more values in a dictionary.
        # When translating this Message we need to translate those Messages too
        translated_params = _translate_args(self.params, desired_locale)

        translated_message = translated_message % translated_params

        return translated_message

    @staticmethod
    def _translate_msgid(msgid, domain, desired_locale=None):
        if not desired_locale:
            system_locale = locale.getdefaultlocale()
            # If the system locale is not available to the runtime use English
            if not system_locale[0]:
                desired_locale = 'en_US'
            else:
                desired_locale = system_locale[0]

        locale_dir = os.environ.get(domain.upper() + '_LOCALEDIR')
        lang = gettext.translation(domain,
                                   localedir=locale_dir,
                                   languages=[desired_locale],
                                   fallback=True)
        if six.PY3:
            translator = lang.gettext
        else:
            translator = lang.ugettext

        translated_message = translator(msgid)
        return translated_message

    def __mod__(self, other):
        # When we mod a Message we want the actual operation to be performed
        # by the parent class (i.e. unicode()), the only thing  we do here is
        # save the original msgid and the parameters in case of a translation
        params = self._sanitize_mod_params(other)
        unicode_mod = super(Message, self).__mod__(params)
        modded = Message(self.msgid,
                         msgtext=unicode_mod,
                         params=params,
                         domain=self.domain)
        return modded

    def _sanitize_mod_params(self, other):
        """Sanitize the object being modded with this Message.

        - Add support for modding 'None' so translation supports it
        - Trim the modded object, which can be a large dictionary, to only
        those keys that would actually be used in a translation
        - Snapshot the object being modded, in case the message is
        translated, it will be used as it was when the Message was created
        """
        if other is None:
            params = (other,)
        elif isinstance(other, dict):
            # Merge the dictionaries
            # Copy each item in case one does not support deep copy.
            params = {}
            if isinstance(self.params, dict):
                for key, val in self.params.items():
                    params[key] = self._copy_param(val)
            for key, val in other.items():
                params[key] = self._copy_param(val)
        else:
            params = self._copy_param(other)
        return params

    def _copy_param(self, param):
        try:
            return copy.deepcopy(param)
        except Exception:
            # Fallback to casting to unicode this will handle the
            # python code-like objects that can't be deep-copied
            return six.text_type(param)

    def __add__(self, other):
        msg = _('Message objects do not support addition.')
        raise TypeError(msg)

    def __radd__(self, other):
        return self.__add__(other)

    if six.PY2:
        def __str__(self):
            # NOTE(luisg): Logging in python 2.6 tries to str() log records,
            # and it expects specifically a UnicodeError in order to proceed.
            msg = _('Message objects do not support str() because they may '
                    'contain non-ascii characters. '
                    'Please use unicode() or translate() instead.')
            raise UnicodeError(msg)


def get_available_languages(domain):
    """Lists the available languages for the given translation domain.

    :param domain: the domain to get languages for
    """
    if domain in _AVAILABLE_LANGUAGES:
        return copy.copy(_AVAILABLE_LANGUAGES[domain])

    localedir = '%s_LOCALEDIR' % domain.upper()
    find = lambda x: gettext.find(domain,
                                  localedir=os.environ.get(localedir),
                                  languages=[x])

    # NOTE(mrodden): en_US should always be available (and first in case
    # order matters) since our in-line message strings are en_US
    language_list = ['en_US']
    # NOTE(luisg): Babel <1.0 used a function called list(), which was
    # renamed to locale_identifiers() in >=1.0, the requirements master list
    # requires >=0.9.6, uncapped, so defensively work with both. We can remove
    # this check when the master list updates to >=1.0, and update all projects
    list_identifiers = (getattr(localedata, 'list', None) or
                        getattr(localedata, 'locale_identifiers'))
    locale_identifiers = list_identifiers()

    for i in locale_identifiers:
        if find(i) is not None:
            language_list.append(i)

    # NOTE(luisg): Babel>=1.0,<1.3 has a bug where some OpenStack supported
    # locales (e.g. 'zh_CN', and 'zh_TW') aren't supported even though they
    # are perfectly legitimate locales:
    #     https://github.com/mitsuhiko/babel/issues/37
    # In Babel 1.3 they fixed the bug and they support these locales, but
    # they are still not explicitly "listed" by locale_identifiers().
    # That is  why we add the locales here explicitly if necessary so that
    # they are listed as supported.
    aliases = {'zh': 'zh_CN',
               'zh_Hant_HK': 'zh_HK',
               'zh_Hant': 'zh_TW',
               'fil': 'tl_PH'}
    for (locale, alias) in six.iteritems(aliases):
        if locale in language_list and alias not in language_list:
            language_list.append(alias)

    _AVAILABLE_LANGUAGES[domain] = language_list
    return copy.copy(language_list)


def translate(obj, desired_locale=None):
    """Gets the translated unicode representation of the given object.

    If the object is not translatable it is returned as-is.
    If the locale is None the object is translated to the system locale.

    :param obj: the object to translate
    :param desired_locale: the locale to translate the message to, if None the
                           default system locale will be used
    :returns: the translated object in unicode, or the original object if
              it could not be translated
    """
    message = obj
    if not isinstance(message, Message):
        # If the object to translate is not already translatable,
        # let's first get its unicode representation
        message = six.text_type(obj)
    if isinstance(message, Message):
        # Even after unicoding() we still need to check if we are
        # running with translatable unicode before translating
        return message.translate(desired_locale)
    return obj


def _translate_args(args, desired_locale=None):
    """Translates all the translatable elements of the given arguments object.

    This method is used for translating the translatable values in method
    arguments which include values of tuples or dictionaries.
    If the object is not a tuple or a dictionary the object itself is
    translated if it is translatable.

    If the locale is None the object is translated to the system locale.

    :param args: the args to translate
    :param desired_locale: the locale to translate the args to, if None the
                           default system locale will be used
    :returns: a new args object with the translated contents of the original
    """
    if isinstance(args, tuple):
        return tuple(translate(v, desired_locale) for v in args)
    if isinstance(args, dict):
        translated_dict = {}
        for (k, v) in six.iteritems(args):
            translated_v = translate(v, desired_locale)
            translated_dict[k] = translated_v
        return translated_dict
    return translate(args, desired_locale)


class TranslationHandler(handlers.MemoryHandler):
    """Handler that translates records before logging them.

    The TranslationHandler takes a locale and a target logging.Handler object
    to forward LogRecord objects to after translating them. This handler
    depends on Message objects being logged, instead of regular strings.

    The handler can be configured declaratively in the logging.conf as follows:

        [handlers]
        keys = translatedlog, translator

        [handler_translatedlog]
        class = handlers.WatchedFileHandler
        args = ('/var/log/api-localized.log',)
        formatter = context

        [handler_translator]
        class = openstack.common.log.TranslationHandler
        target = translatedlog
        args = ('zh_CN',)

    If the specified locale is not available in the system, the handler will
    log in the default locale.
    """

    def __init__(self, locale=None, target=None):
        """Initialize a TranslationHandler

        :param locale: locale to use for translating messages
        :param target: logging.Handler object to forward
                       LogRecord objects to after translation
        """
        # NOTE(luisg): In order to allow this handler to be a wrapper for
        # other handlers, such as a FileHandler, and still be able to
        # configure it using logging.conf, this handler has to extend
        # MemoryHandler because only the MemoryHandlers' logging.conf
        # parsing is implemented such that it accepts a target handler.
        handlers.MemoryHandler.__init__(self, capacity=0, target=target)
        self.locale = locale

    def setFormatter(self, fmt):
        self.target.setFormatter(fmt)

    def emit(self, record):
        # We save the message from the original record to restore it
        # after translation, so other handlers are not affected by this
        original_msg = record.msg
        original_args = record.args

        try:
            self._translate_and_log_record(record)
        finally:
            record.msg = original_msg
            record.args = original_args

    def _translate_and_log_record(self, record):
        record.msg = translate(record.msg, self.locale)

        # In addition to translating the message, we also need to translate
        # arguments that were passed to the log method that were not part
        # of the main message e.g., log.info(_('Some message %s'), this_one))
        record.args = _translate_args(record.args, self.locale)

        self.target.emit(record)

########NEW FILE########
__FILENAME__ = importutils
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Import related utilities and helper functions.
"""

import sys
import traceback


def import_class(import_str):
    """Returns a class from a string including module and class."""
    mod_str, _sep, class_str = import_str.rpartition('.')
    try:
        __import__(mod_str)
        return getattr(sys.modules[mod_str], class_str)
    except (ValueError, AttributeError):
        raise ImportError('Class %s cannot be found (%s)' %
                          (class_str,
                           traceback.format_exception(*sys.exc_info())))


def import_object(import_str, *args, **kwargs):
    """Import a class and return an instance of it."""
    return import_class(import_str)(*args, **kwargs)


def import_object_ns(name_space, import_str, *args, **kwargs):
    """Tries to import object from default namespace.

    Imports a class and return an instance of it, first by trying
    to find the class in a default namespace, then failing back to
    a full path if not found in the default namespace.
    """
    import_value = "%s.%s" % (name_space, import_str)
    try:
        return import_class(import_value)(*args, **kwargs)
    except ImportError:
        return import_class(import_str)(*args, **kwargs)


def import_module(import_str):
    """Import a module."""
    __import__(import_str)
    return sys.modules[import_str]


def import_versioned_module(version, submodule=None):
    module = 'designate.v%s' % version
    if submodule:
        module = '.'.join((module, submodule))
    return import_module(module)


def try_import(import_str, default=None):
    """Try to import a module and if it fails return default."""
    try:
        return import_module(import_str)
    except ImportError:
        return default

########NEW FILE########
__FILENAME__ = jsonutils
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

'''
JSON related utilities.

This module provides a few things:

    1) A handy function for getting an object down to something that can be
    JSON serialized.  See to_primitive().

    2) Wrappers around loads() and dumps().  The dumps() wrapper will
    automatically use to_primitive() for you if needed.

    3) This sets up anyjson to use the loads() and dumps() wrappers if anyjson
    is available.
'''


import datetime
import functools
import inspect
import itertools
import json

import six
import six.moves.xmlrpc_client as xmlrpclib

from designate.openstack.common import gettextutils
from designate.openstack.common import importutils
from designate.openstack.common import timeutils

netaddr = importutils.try_import("netaddr")

_nasty_type_tests = [inspect.ismodule, inspect.isclass, inspect.ismethod,
                     inspect.isfunction, inspect.isgeneratorfunction,
                     inspect.isgenerator, inspect.istraceback, inspect.isframe,
                     inspect.iscode, inspect.isbuiltin, inspect.isroutine,
                     inspect.isabstract]

_simple_types = (six.string_types + six.integer_types
                 + (type(None), bool, float))


def to_primitive(value, convert_instances=False, convert_datetime=True,
                 level=0, max_depth=3):
    """Convert a complex object into primitives.

    Handy for JSON serialization. We can optionally handle instances,
    but since this is a recursive function, we could have cyclical
    data structures.

    To handle cyclical data structures we could track the actual objects
    visited in a set, but not all objects are hashable. Instead we just
    track the depth of the object inspections and don't go too deep.

    Therefore, convert_instances=True is lossy ... be aware.

    """
    # handle obvious types first - order of basic types determined by running
    # full tests on nova project, resulting in the following counts:
    # 572754 <type 'NoneType'>
    # 460353 <type 'int'>
    # 379632 <type 'unicode'>
    # 274610 <type 'str'>
    # 199918 <type 'dict'>
    # 114200 <type 'datetime.datetime'>
    #  51817 <type 'bool'>
    #  26164 <type 'list'>
    #   6491 <type 'float'>
    #    283 <type 'tuple'>
    #     19 <type 'long'>
    if isinstance(value, _simple_types):
        return value

    if isinstance(value, datetime.datetime):
        if convert_datetime:
            return timeutils.strtime(value)
        else:
            return value

    # value of itertools.count doesn't get caught by nasty_type_tests
    # and results in infinite loop when list(value) is called.
    if type(value) == itertools.count:
        return six.text_type(value)

    # FIXME(vish): Workaround for LP bug 852095. Without this workaround,
    #              tests that raise an exception in a mocked method that
    #              has a @wrap_exception with a notifier will fail. If
    #              we up the dependency to 0.5.4 (when it is released) we
    #              can remove this workaround.
    if getattr(value, '__module__', None) == 'mox':
        return 'mock'

    if level > max_depth:
        return '?'

    # The try block may not be necessary after the class check above,
    # but just in case ...
    try:
        recursive = functools.partial(to_primitive,
                                      convert_instances=convert_instances,
                                      convert_datetime=convert_datetime,
                                      level=level,
                                      max_depth=max_depth)
        if isinstance(value, dict):
            return dict((k, recursive(v)) for k, v in six.iteritems(value))
        elif isinstance(value, (list, tuple)):
            return [recursive(lv) for lv in value]

        # It's not clear why xmlrpclib created their own DateTime type, but
        # for our purposes, make it a datetime type which is explicitly
        # handled
        if isinstance(value, xmlrpclib.DateTime):
            value = datetime.datetime(*tuple(value.timetuple())[:6])

        if convert_datetime and isinstance(value, datetime.datetime):
            return timeutils.strtime(value)
        elif isinstance(value, gettextutils.Message):
            return value.data
        elif hasattr(value, 'iteritems'):
            return recursive(dict(value.iteritems()), level=level + 1)
        elif hasattr(value, '__iter__'):
            return recursive(list(value))
        elif convert_instances and hasattr(value, '__dict__'):
            # Likely an instance of something. Watch for cycles.
            # Ignore class member vars.
            return recursive(value.__dict__, level=level + 1)
        elif netaddr and isinstance(value, netaddr.IPAddress):
            return six.text_type(value)
        else:
            if any(test(value) for test in _nasty_type_tests):
                return six.text_type(value)
            return value
    except TypeError:
        # Class objects are tricky since they may define something like
        # __iter__ defined but it isn't callable as list().
        return six.text_type(value)


def dumps(value, default=to_primitive, **kwargs):
    return json.dumps(value, default=default, **kwargs)


def loads(s):
    return json.loads(s)


def load(s):
    return json.load(s)


try:
    import anyjson
except ImportError:
    pass
else:
    anyjson._modules.append((__name__, 'dumps', TypeError,
                                       'loads', ValueError, 'load'))
    anyjson.force_implementation(__name__)

########NEW FILE########
__FILENAME__ = local
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Local storage of variables using weak references"""

import threading
import weakref


class WeakLocal(threading.local):
    def __getattribute__(self, attr):
        rval = super(WeakLocal, self).__getattribute__(attr)
        if rval:
            # NOTE(mikal): this bit is confusing. What is stored is a weak
            # reference, not the value itself. We therefore need to lookup
            # the weak reference and return the inner value here.
            rval = rval()
        return rval

    def __setattr__(self, attr, value):
        value = weakref.ref(value)
        return super(WeakLocal, self).__setattr__(attr, value)


# NOTE(mikal): the name "store" should be deprecated in the future
store = WeakLocal()

# A "weak" store uses weak references and allows an object to fall out of scope
# when it falls out of scope in the code that uses the thread local storage. A
# "strong" store will hold a reference to the object so that it never falls out
# of scope.
weak_store = WeakLocal()
strong_store = threading.local()

########NEW FILE########
__FILENAME__ = lockutils
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import contextlib
import errno
import fcntl
import functools
import os
import shutil
import subprocess
import sys
import tempfile
import threading
import time
import weakref

from oslo.config import cfg

from designate.openstack.common import fileutils
from designate.openstack.common.gettextutils import _, _LE, _LI
from designate.openstack.common import log as logging


LOG = logging.getLogger(__name__)


util_opts = [
    cfg.BoolOpt('disable_process_locking', default=False,
                help='Whether to disable inter-process locks'),
    cfg.StrOpt('lock_path',
               default=os.environ.get("DESIGNATE_LOCK_PATH"),
               help=('Directory to use for lock files.'))
]


CONF = cfg.CONF
CONF.register_opts(util_opts)


def set_defaults(lock_path):
    cfg.set_defaults(util_opts, lock_path=lock_path)


class _FileLock(object):
    """Lock implementation which allows multiple locks, working around
    issues like bugs.debian.org/cgi-bin/bugreport.cgi?bug=632857 and does
    not require any cleanup. Since the lock is always held on a file
    descriptor rather than outside of the process, the lock gets dropped
    automatically if the process crashes, even if __exit__ is not executed.

    There are no guarantees regarding usage by multiple green threads in a
    single process here. This lock works only between processes. Exclusive
    access between local threads should be achieved using the semaphores
    in the @synchronized decorator.

    Note these locks are released when the descriptor is closed, so it's not
    safe to close the file descriptor while another green thread holds the
    lock. Just opening and closing the lock file can break synchronisation,
    so lock files must be accessed only using this abstraction.
    """

    def __init__(self, name):
        self.lockfile = None
        self.fname = name

    def acquire(self):
        basedir = os.path.dirname(self.fname)

        if not os.path.exists(basedir):
            fileutils.ensure_tree(basedir)
            LOG.info(_LI('Created lock path: %s'), basedir)

        self.lockfile = open(self.fname, 'w')

        while True:
            try:
                # Using non-blocking locks since green threads are not
                # patched to deal with blocking locking calls.
                # Also upon reading the MSDN docs for locking(), it seems
                # to have a laughable 10 attempts "blocking" mechanism.
                self.trylock()
                LOG.debug('Got file lock "%s"', self.fname)
                return True
            except IOError as e:
                if e.errno in (errno.EACCES, errno.EAGAIN):
                    # external locks synchronise things like iptables
                    # updates - give it some time to prevent busy spinning
                    time.sleep(0.01)
                else:
                    raise threading.ThreadError(_("Unable to acquire lock on"
                                                  " `%(filename)s` due to"
                                                  " %(exception)s") %
                                                {
                                                    'filename': self.fname,
                                                    'exception': e,
                                                })

    def __enter__(self):
        self.acquire()
        return self

    def release(self):
        try:
            self.unlock()
            self.lockfile.close()
            LOG.debug('Released file lock "%s"', self.fname)
        except IOError:
            LOG.exception(_LE("Could not release the acquired lock `%s`"),
                          self.fname)

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release()

    def exists(self):
        return os.path.exists(self.fname)

    def trylock(self):
        raise NotImplementedError()

    def unlock(self):
        raise NotImplementedError()


class _WindowsLock(_FileLock):
    def trylock(self):
        msvcrt.locking(self.lockfile.fileno(), msvcrt.LK_NBLCK, 1)

    def unlock(self):
        msvcrt.locking(self.lockfile.fileno(), msvcrt.LK_UNLCK, 1)


class _FcntlLock(_FileLock):
    def trylock(self):
        fcntl.lockf(self.lockfile, fcntl.LOCK_EX | fcntl.LOCK_NB)

    def unlock(self):
        fcntl.lockf(self.lockfile, fcntl.LOCK_UN)


class _PosixLock(object):
    def __init__(self, name):
        # Hash the name because it's not valid to have POSIX semaphore
        # names with things like / in them. Then use base64 to encode
        # the digest() instead taking the hexdigest() because the
        # result is shorter and most systems can't have shm sempahore
        # names longer than 31 characters.
        h = hashlib.sha1()
        h.update(name.encode('ascii'))
        self.name = str((b'/' + base64.urlsafe_b64encode(
            h.digest())).decode('ascii'))

    def acquire(self, timeout=None):
        self.semaphore = posix_ipc.Semaphore(self.name,
                                             flags=posix_ipc.O_CREAT,
                                             initial_value=1)
        self.semaphore.acquire(timeout)
        return self

    def __enter__(self):
        self.acquire()
        return self

    def release(self):
        self.semaphore.release()
        self.semaphore.close()

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release()

    def exists(self):
        try:
            semaphore = posix_ipc.Semaphore(self.name)
        except posix_ipc.ExistentialError:
            return False
        else:
            semaphore.close()
        return True


if os.name == 'nt':
    import msvcrt
    InterProcessLock = _WindowsLock
    FileLock = _WindowsLock
else:
    import base64
    import hashlib
    import posix_ipc
    InterProcessLock = _PosixLock
    FileLock = _FcntlLock

_semaphores = weakref.WeakValueDictionary()
_semaphores_lock = threading.Lock()


def _get_lock_path(name, lock_file_prefix, lock_path=None):
    # NOTE(mikal): the lock name cannot contain directory
    # separators
    name = name.replace(os.sep, '_')
    if lock_file_prefix:
        sep = '' if lock_file_prefix.endswith('-') else '-'
        name = '%s%s%s' % (lock_file_prefix, sep, name)

    local_lock_path = lock_path or CONF.lock_path

    if not local_lock_path:
        # NOTE(bnemec): Create a fake lock path for posix locks so we don't
        # unnecessarily raise the RequiredOptError below.
        if InterProcessLock is not _PosixLock:
            raise cfg.RequiredOptError('lock_path')
        local_lock_path = 'posixlock:/'

    return os.path.join(local_lock_path, name)


def external_lock(name, lock_file_prefix=None, lock_path=None):
    LOG.debug('Attempting to grab external lock "%(lock)s"',
              {'lock': name})

    lock_file_path = _get_lock_path(name, lock_file_prefix, lock_path)

    # NOTE(bnemec): If an explicit lock_path was passed to us then it
    # means the caller is relying on file-based locking behavior, so
    # we can't use posix locks for those calls.
    if lock_path:
        return FileLock(lock_file_path)
    return InterProcessLock(lock_file_path)


def remove_external_lock_file(name, lock_file_prefix=None):
    """Remove a external lock file when it's not used anymore
    This will be helpful when we have a lot of lock files
    """
    with internal_lock(name):
        lock_file_path = _get_lock_path(name, lock_file_prefix)
        try:
            os.remove(lock_file_path)
        except OSError:
            LOG.info(_LI('Failed to remove file %(file)s'),
                     {'file': lock_file_path})


def internal_lock(name):
    with _semaphores_lock:
        try:
            sem = _semaphores[name]
        except KeyError:
            sem = threading.Semaphore()
            _semaphores[name] = sem

    LOG.debug('Got semaphore "%(lock)s"', {'lock': name})
    return sem


@contextlib.contextmanager
def lock(name, lock_file_prefix=None, external=False, lock_path=None):
    """Context based lock

    This function yields a `threading.Semaphore` instance (if we don't use
    eventlet.monkey_patch(), else `semaphore.Semaphore`) unless external is
    True, in which case, it'll yield an InterProcessLock instance.

    :param lock_file_prefix: The lock_file_prefix argument is used to provide
      lock files on disk with a meaningful prefix.

    :param external: The external keyword argument denotes whether this lock
      should work across multiple processes. This means that if two different
      workers both run a a method decorated with @synchronized('mylock',
      external=True), only one of them will execute at a time.
    """
    int_lock = internal_lock(name)
    with int_lock:
        if external and not CONF.disable_process_locking:
            ext_lock = external_lock(name, lock_file_prefix, lock_path)
            with ext_lock:
                yield ext_lock
        else:
            yield int_lock


def synchronized(name, lock_file_prefix=None, external=False, lock_path=None):
    """Synchronization decorator.

    Decorating a method like so::

        @synchronized('mylock')
        def foo(self, *args):
           ...

    ensures that only one thread will execute the foo method at a time.

    Different methods can share the same lock::

        @synchronized('mylock')
        def foo(self, *args):
           ...

        @synchronized('mylock')
        def bar(self, *args):
           ...

    This way only one of either foo or bar can be executing at a time.
    """

    def wrap(f):
        @functools.wraps(f)
        def inner(*args, **kwargs):
            try:
                with lock(name, lock_file_prefix, external, lock_path):
                    LOG.debug('Got semaphore / lock "%(function)s"',
                              {'function': f.__name__})
                    return f(*args, **kwargs)
            finally:
                LOG.debug('Semaphore / lock released "%(function)s"',
                          {'function': f.__name__})
        return inner
    return wrap


def synchronized_with_prefix(lock_file_prefix):
    """Partial object generator for the synchronization decorator.

    Redefine @synchronized in each project like so::

        (in nova/utils.py)
        from nova.openstack.common import lockutils

        synchronized = lockutils.synchronized_with_prefix('nova-')


        (in nova/foo.py)
        from nova import utils

        @utils.synchronized('mylock')
        def bar(self, *args):
           ...

    The lock_file_prefix argument is used to provide lock files on disk with a
    meaningful prefix.
    """

    return functools.partial(synchronized, lock_file_prefix=lock_file_prefix)


def main(argv):
    """Create a dir for locks and pass it to command from arguments

    If you run this:
    python -m openstack.common.lockutils python setup.py testr <etc>

    a temporary directory will be created for all your locks and passed to all
    your tests in an environment variable. The temporary dir will be deleted
    afterwards and the return value will be preserved.
    """

    lock_dir = tempfile.mkdtemp()
    os.environ["DESIGNATE_LOCK_PATH"] = lock_dir
    try:
        ret_val = subprocess.call(argv[1:])
    finally:
        shutil.rmtree(lock_dir, ignore_errors=True)
    return ret_val


if __name__ == '__main__':
    sys.exit(main(sys.argv))

########NEW FILE########
__FILENAME__ = log
# Copyright 2011 OpenStack Foundation.
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""OpenStack logging handler.

This module adds to logging functionality by adding the option to specify
a context object when calling the various log methods.  If the context object
is not specified, default formatting is used. Additionally, an instance uuid
may be passed as part of the log message, which is intended to make it easier
for admins to find messages related to a specific instance.

It also allows setting of formatting information through conf.

"""

import inspect
import itertools
import logging
import logging.config
import logging.handlers
import os
import re
import sys
import traceback

from oslo.config import cfg
import six
from six import moves

from designate.openstack.common.gettextutils import _
from designate.openstack.common import importutils
from designate.openstack.common import jsonutils
from designate.openstack.common import local


_DEFAULT_LOG_DATE_FORMAT = "%Y-%m-%d %H:%M:%S"

_SANITIZE_KEYS = ['adminPass', 'admin_pass', 'password', 'admin_password']

# NOTE(ldbragst): Let's build a list of regex objects using the list of
# _SANITIZE_KEYS we already have. This way, we only have to add the new key
# to the list of _SANITIZE_KEYS and we can generate regular expressions
# for XML and JSON automatically.
_SANITIZE_PATTERNS = []
_FORMAT_PATTERNS = [r'(%(key)s\s*[=]\s*[\"\']).*?([\"\'])',
                    r'(<%(key)s>).*?(</%(key)s>)',
                    r'([\"\']%(key)s[\"\']\s*:\s*[\"\']).*?([\"\'])',
                    r'([\'"].*?%(key)s[\'"]\s*:\s*u?[\'"]).*?([\'"])']

for key in _SANITIZE_KEYS:
    for pattern in _FORMAT_PATTERNS:
        reg_ex = re.compile(pattern % {'key': key}, re.DOTALL)
        _SANITIZE_PATTERNS.append(reg_ex)


common_cli_opts = [
    cfg.BoolOpt('debug',
                short='d',
                default=False,
                help='Print debugging output (set logging level to '
                     'DEBUG instead of default WARNING level).'),
    cfg.BoolOpt('verbose',
                short='v',
                default=False,
                help='Print more verbose output (set logging level to '
                     'INFO instead of default WARNING level).'),
]

logging_cli_opts = [
    cfg.StrOpt('log-config-append',
               metavar='PATH',
               deprecated_name='log-config',
               help='The name of logging configuration file. It does not '
                    'disable existing loggers, but just appends specified '
                    'logging configuration to any other existing logging '
                    'options. Please see the Python logging module '
                    'documentation for details on logging configuration '
                    'files.'),
    cfg.StrOpt('log-format',
               default=None,
               metavar='FORMAT',
               help='DEPRECATED. '
                    'A logging.Formatter log message format string which may '
                    'use any of the available logging.LogRecord attributes. '
                    'This option is deprecated.  Please use '
                    'logging_context_format_string and '
                    'logging_default_format_string instead.'),
    cfg.StrOpt('log-date-format',
               default=_DEFAULT_LOG_DATE_FORMAT,
               metavar='DATE_FORMAT',
               help='Format string for %%(asctime)s in log records. '
                    'Default: %(default)s'),
    cfg.StrOpt('log-file',
               metavar='PATH',
               deprecated_name='logfile',
               help='(Optional) Name of log file to output to. '
                    'If no default is set, logging will go to stdout.'),
    cfg.StrOpt('log-dir',
               deprecated_name='logdir',
               help='(Optional) The base directory used for relative '
                    '--log-file paths'),
    cfg.BoolOpt('use-syslog',
                default=False,
                help='Use syslog for logging. '
                     'Existing syslog format is DEPRECATED during I, '
                     'and then will be changed in J to honor RFC5424'),
    cfg.BoolOpt('use-syslog-rfc-format',
                # TODO(bogdando) remove or use True after existing
                #    syslog format deprecation in J
                default=False,
                help='(Optional) Use syslog rfc5424 format for logging. '
                     'If enabled, will add APP-NAME (RFC5424) before the '
                     'MSG part of the syslog message.  The old format '
                     'without APP-NAME is deprecated in I, '
                     'and will be removed in J.'),
    cfg.StrOpt('syslog-log-facility',
               default='LOG_USER',
               help='Syslog facility to receive log lines')
]

generic_log_opts = [
    cfg.BoolOpt('use_stderr',
                default=True,
                help='Log output to standard error')
]

log_opts = [
    cfg.StrOpt('logging_context_format_string',
               default='%(asctime)s.%(msecs)03d %(process)d %(levelname)s '
                       '%(name)s [%(request_id)s %(user_identity)s] '
                       '%(instance)s%(message)s',
               help='Format string to use for log messages with context'),
    cfg.StrOpt('logging_default_format_string',
               default='%(asctime)s.%(msecs)03d %(process)d %(levelname)s '
                       '%(name)s [-] %(instance)s%(message)s',
               help='Format string to use for log messages without context'),
    cfg.StrOpt('logging_debug_format_suffix',
               default='%(funcName)s %(pathname)s:%(lineno)d',
               help='Data to append to log format when level is DEBUG'),
    cfg.StrOpt('logging_exception_prefix',
               default='%(asctime)s.%(msecs)03d %(process)d TRACE %(name)s '
               '%(instance)s',
               help='Prefix each line of exception output with this format'),
    cfg.ListOpt('default_log_levels',
                default=[
                    'amqp=WARN',
                    'amqplib=WARN',
                    'boto=WARN',
                    'qpid=WARN',
                    'sqlalchemy=WARN',
                    'suds=INFO',
                    'oslo.messaging=INFO',
                    'iso8601=WARN',
                    'requests.packages.urllib3.connectionpool=WARN'
                ],
                help='List of logger=LEVEL pairs'),
    cfg.BoolOpt('publish_errors',
                default=False,
                help='Publish error events'),
    cfg.BoolOpt('fatal_deprecations',
                default=False,
                help='Make deprecations fatal'),

    # NOTE(mikal): there are two options here because sometimes we are handed
    # a full instance (and could include more information), and other times we
    # are just handed a UUID for the instance.
    cfg.StrOpt('instance_format',
               default='[instance: %(uuid)s] ',
               help='If an instance is passed with the log message, format '
                    'it like this'),
    cfg.StrOpt('instance_uuid_format',
               default='[instance: %(uuid)s] ',
               help='If an instance UUID is passed with the log message, '
                    'format it like this'),
]

CONF = cfg.CONF
CONF.register_cli_opts(common_cli_opts)
CONF.register_cli_opts(logging_cli_opts)
CONF.register_opts(generic_log_opts)
CONF.register_opts(log_opts)

# our new audit level
# NOTE(jkoelker) Since we synthesized an audit level, make the logging
#                module aware of it so it acts like other levels.
logging.AUDIT = logging.INFO + 1
logging.addLevelName(logging.AUDIT, 'AUDIT')


try:
    NullHandler = logging.NullHandler
except AttributeError:  # NOTE(jkoelker) NullHandler added in Python 2.7
    class NullHandler(logging.Handler):
        def handle(self, record):
            pass

        def emit(self, record):
            pass

        def createLock(self):
            self.lock = None


def _dictify_context(context):
    if context is None:
        return None
    if not isinstance(context, dict) and getattr(context, 'to_dict', None):
        context = context.to_dict()
    return context


def _get_binary_name():
    return os.path.basename(inspect.stack()[-1][1])


def _get_log_file_path(binary=None):
    logfile = CONF.log_file
    logdir = CONF.log_dir

    if logfile and not logdir:
        return logfile

    if logfile and logdir:
        return os.path.join(logdir, logfile)

    if logdir:
        binary = binary or _get_binary_name()
        return '%s.log' % (os.path.join(logdir, binary),)

    return None


def mask_password(message, secret="***"):
    """Replace password with 'secret' in message.

    :param message: The string which includes security information.
    :param secret: value with which to replace passwords.
    :returns: The unicode value of message with the password fields masked.

    For example:

    >>> mask_password("'adminPass' : 'aaaaa'")
    "'adminPass' : '***'"
    >>> mask_password("'admin_pass' : 'aaaaa'")
    "'admin_pass' : '***'"
    >>> mask_password('"password" : "aaaaa"')
    '"password" : "***"'
    >>> mask_password("'original_password' : 'aaaaa'")
    "'original_password' : '***'"
    >>> mask_password("u'original_password' :   u'aaaaa'")
    "u'original_password' :   u'***'"
    """
    message = six.text_type(message)

    # NOTE(ldbragst): Check to see if anything in message contains any key
    # specified in _SANITIZE_KEYS, if not then just return the message since
    # we don't have to mask any passwords.
    if not any(key in message for key in _SANITIZE_KEYS):
        return message

    secret = r'\g<1>' + secret + r'\g<2>'
    for pattern in _SANITIZE_PATTERNS:
        message = re.sub(pattern, secret, message)
    return message


class BaseLoggerAdapter(logging.LoggerAdapter):

    def audit(self, msg, *args, **kwargs):
        self.log(logging.AUDIT, msg, *args, **kwargs)


class LazyAdapter(BaseLoggerAdapter):
    def __init__(self, name='unknown', version='unknown'):
        self._logger = None
        self.extra = {}
        self.name = name
        self.version = version

    @property
    def logger(self):
        if not self._logger:
            self._logger = getLogger(self.name, self.version)
        return self._logger


class ContextAdapter(BaseLoggerAdapter):
    warn = logging.LoggerAdapter.warning

    def __init__(self, logger, project_name, version_string):
        self.logger = logger
        self.project = project_name
        self.version = version_string
        self._deprecated_messages_sent = dict()

    @property
    def handlers(self):
        return self.logger.handlers

    def deprecated(self, msg, *args, **kwargs):
        """Call this method when a deprecated feature is used.

        If the system is configured for fatal deprecations then the message
        is logged at the 'critical' level and :class:`DeprecatedConfig` will
        be raised.

        Otherwise, the message will be logged (once) at the 'warn' level.

        :raises: :class:`DeprecatedConfig` if the system is configured for
                 fatal deprecations.

        """
        stdmsg = _("Deprecated: %s") % msg
        if CONF.fatal_deprecations:
            self.critical(stdmsg, *args, **kwargs)
            raise DeprecatedConfig(msg=stdmsg)

        # Using a list because a tuple with dict can't be stored in a set.
        sent_args = self._deprecated_messages_sent.setdefault(msg, list())

        if args in sent_args:
            # Already logged this message, so don't log it again.
            return

        sent_args.append(args)
        self.warn(stdmsg, *args, **kwargs)

    def process(self, msg, kwargs):
        # NOTE(mrodden): catch any Message/other object and
        #                coerce to unicode before they can get
        #                to the python logging and possibly
        #                cause string encoding trouble
        if not isinstance(msg, six.string_types):
            msg = six.text_type(msg)

        if 'extra' not in kwargs:
            kwargs['extra'] = {}
        extra = kwargs['extra']

        context = kwargs.pop('context', None)
        if not context:
            context = getattr(local.store, 'context', None)
        if context:
            extra.update(_dictify_context(context))

        instance = kwargs.pop('instance', None)
        instance_uuid = (extra.get('instance_uuid') or
                         kwargs.pop('instance_uuid', None))
        instance_extra = ''
        if instance:
            instance_extra = CONF.instance_format % instance
        elif instance_uuid:
            instance_extra = (CONF.instance_uuid_format
                              % {'uuid': instance_uuid})
        extra['instance'] = instance_extra

        extra.setdefault('user_identity', kwargs.pop('user_identity', None))

        extra['project'] = self.project
        extra['version'] = self.version
        extra['extra'] = extra.copy()
        return msg, kwargs


class JSONFormatter(logging.Formatter):
    def __init__(self, fmt=None, datefmt=None):
        # NOTE(jkoelker) we ignore the fmt argument, but its still there
        #                since logging.config.fileConfig passes it.
        self.datefmt = datefmt

    def formatException(self, ei, strip_newlines=True):
        lines = traceback.format_exception(*ei)
        if strip_newlines:
            lines = [moves.filter(
                lambda x: x,
                line.rstrip().splitlines()) for line in lines]
            lines = list(itertools.chain(*lines))
        return lines

    def format(self, record):
        message = {'message': record.getMessage(),
                   'asctime': self.formatTime(record, self.datefmt),
                   'name': record.name,
                   'msg': record.msg,
                   'args': record.args,
                   'levelname': record.levelname,
                   'levelno': record.levelno,
                   'pathname': record.pathname,
                   'filename': record.filename,
                   'module': record.module,
                   'lineno': record.lineno,
                   'funcname': record.funcName,
                   'created': record.created,
                   'msecs': record.msecs,
                   'relative_created': record.relativeCreated,
                   'thread': record.thread,
                   'thread_name': record.threadName,
                   'process_name': record.processName,
                   'process': record.process,
                   'traceback': None}

        if hasattr(record, 'extra'):
            message['extra'] = record.extra

        if record.exc_info:
            message['traceback'] = self.formatException(record.exc_info)

        return jsonutils.dumps(message)


def _create_logging_excepthook(product_name):
    def logging_excepthook(exc_type, value, tb):
        extra = {}
        if CONF.verbose or CONF.debug:
            extra['exc_info'] = (exc_type, value, tb)
        getLogger(product_name).critical(
            "".join(traceback.format_exception_only(exc_type, value)),
            **extra)
    return logging_excepthook


class LogConfigError(Exception):

    message = _('Error loading logging config %(log_config)s: %(err_msg)s')

    def __init__(self, log_config, err_msg):
        self.log_config = log_config
        self.err_msg = err_msg

    def __str__(self):
        return self.message % dict(log_config=self.log_config,
                                   err_msg=self.err_msg)


def _load_log_config(log_config_append):
    try:
        logging.config.fileConfig(log_config_append,
                                  disable_existing_loggers=False)
    except moves.configparser.Error as exc:
        raise LogConfigError(log_config_append, str(exc))


def setup(product_name, version='unknown'):
    """Setup logging."""
    if CONF.log_config_append:
        _load_log_config(CONF.log_config_append)
    else:
        _setup_logging_from_conf(product_name, version)
    sys.excepthook = _create_logging_excepthook(product_name)


def set_defaults(logging_context_format_string):
    cfg.set_defaults(log_opts,
                     logging_context_format_string=
                     logging_context_format_string)


def _find_facility_from_conf():
    facility_names = logging.handlers.SysLogHandler.facility_names
    facility = getattr(logging.handlers.SysLogHandler,
                       CONF.syslog_log_facility,
                       None)

    if facility is None and CONF.syslog_log_facility in facility_names:
        facility = facility_names.get(CONF.syslog_log_facility)

    if facility is None:
        valid_facilities = facility_names.keys()
        consts = ['LOG_AUTH', 'LOG_AUTHPRIV', 'LOG_CRON', 'LOG_DAEMON',
                  'LOG_FTP', 'LOG_KERN', 'LOG_LPR', 'LOG_MAIL', 'LOG_NEWS',
                  'LOG_AUTH', 'LOG_SYSLOG', 'LOG_USER', 'LOG_UUCP',
                  'LOG_LOCAL0', 'LOG_LOCAL1', 'LOG_LOCAL2', 'LOG_LOCAL3',
                  'LOG_LOCAL4', 'LOG_LOCAL5', 'LOG_LOCAL6', 'LOG_LOCAL7']
        valid_facilities.extend(consts)
        raise TypeError(_('syslog facility must be one of: %s') %
                        ', '.join("'%s'" % fac
                                  for fac in valid_facilities))

    return facility


class RFCSysLogHandler(logging.handlers.SysLogHandler):
    def __init__(self, *args, **kwargs):
        self.binary_name = _get_binary_name()
        super(RFCSysLogHandler, self).__init__(*args, **kwargs)

    def format(self, record):
        msg = super(RFCSysLogHandler, self).format(record)
        msg = self.binary_name + ' ' + msg
        return msg


def _setup_logging_from_conf(project, version):
    log_root = getLogger(None).logger
    for handler in log_root.handlers:
        log_root.removeHandler(handler)

    if CONF.use_syslog:
        facility = _find_facility_from_conf()
        # TODO(bogdando) use the format provided by RFCSysLogHandler
        #   after existing syslog format deprecation in J
        if CONF.use_syslog_rfc_format:
            syslog = RFCSysLogHandler(address='/dev/log',
                                      facility=facility)
        else:
            syslog = logging.handlers.SysLogHandler(address='/dev/log',
                                                    facility=facility)
        log_root.addHandler(syslog)

    logpath = _get_log_file_path()
    if logpath:
        filelog = logging.handlers.WatchedFileHandler(logpath)
        log_root.addHandler(filelog)

    if CONF.use_stderr:
        streamlog = ColorHandler()
        log_root.addHandler(streamlog)

    elif not logpath:
        # pass sys.stdout as a positional argument
        # python2.6 calls the argument strm, in 2.7 it's stream
        streamlog = logging.StreamHandler(sys.stdout)
        log_root.addHandler(streamlog)

    if CONF.publish_errors:
        handler = importutils.import_object(
            "designate.openstack.common.log_handler.PublishErrorsHandler",
            logging.ERROR)
        log_root.addHandler(handler)

    datefmt = CONF.log_date_format
    for handler in log_root.handlers:
        # NOTE(alaski): CONF.log_format overrides everything currently.  This
        # should be deprecated in favor of context aware formatting.
        if CONF.log_format:
            handler.setFormatter(logging.Formatter(fmt=CONF.log_format,
                                                   datefmt=datefmt))
            log_root.info('Deprecated: log_format is now deprecated and will '
                          'be removed in the next release')
        else:
            handler.setFormatter(ContextFormatter(project=project,
                                                  version=version,
                                                  datefmt=datefmt))

    if CONF.debug:
        log_root.setLevel(logging.DEBUG)
    elif CONF.verbose:
        log_root.setLevel(logging.INFO)
    else:
        log_root.setLevel(logging.WARNING)

    for pair in CONF.default_log_levels:
        mod, _sep, level_name = pair.partition('=')
        level = logging.getLevelName(level_name)
        logger = logging.getLogger(mod)
        logger.setLevel(level)

_loggers = {}


def getLogger(name='unknown', version='unknown'):
    if name not in _loggers:
        _loggers[name] = ContextAdapter(logging.getLogger(name),
                                        name,
                                        version)
    return _loggers[name]


def getLazyLogger(name='unknown', version='unknown'):
    """Returns lazy logger.

    Creates a pass-through logger that does not create the real logger
    until it is really needed and delegates all calls to the real logger
    once it is created.
    """
    return LazyAdapter(name, version)


class WritableLogger(object):
    """A thin wrapper that responds to `write` and logs."""

    def __init__(self, logger, level=logging.INFO):
        self.logger = logger
        self.level = level

    def write(self, msg):
        self.logger.log(self.level, msg.rstrip())


class ContextFormatter(logging.Formatter):
    """A context.RequestContext aware formatter configured through flags.

    The flags used to set format strings are: logging_context_format_string
    and logging_default_format_string.  You can also specify
    logging_debug_format_suffix to append extra formatting if the log level is
    debug.

    For information about what variables are available for the formatter see:
    http://docs.python.org/library/logging.html#formatter

    If available, uses the context value stored in TLS - local.store.context

    """

    def __init__(self, *args, **kwargs):
        """Initialize ContextFormatter instance

        Takes additional keyword arguments which can be used in the message
        format string.

        :keyword project: project name
        :type project: string
        :keyword version: project version
        :type version: string

        """

        self.project = kwargs.pop('project', 'unknown')
        self.version = kwargs.pop('version', 'unknown')

        logging.Formatter.__init__(self, *args, **kwargs)

    def format(self, record):
        """Uses contextstring if request_id is set, otherwise default."""

        # store project info
        record.project = self.project
        record.version = self.version

        # store request info
        context = getattr(local.store, 'context', None)
        if context:
            d = _dictify_context(context)
            for k, v in d.items():
                setattr(record, k, v)

        # NOTE(sdague): default the fancier formatting params
        # to an empty string so we don't throw an exception if
        # they get used
        for key in ('instance', 'color', 'user_identity'):
            if key not in record.__dict__:
                record.__dict__[key] = ''

        if record.__dict__.get('request_id'):
            self._fmt = CONF.logging_context_format_string
        else:
            self._fmt = CONF.logging_default_format_string

        if (record.levelno == logging.DEBUG and
                CONF.logging_debug_format_suffix):
            self._fmt += " " + CONF.logging_debug_format_suffix

        # Cache this on the record, Logger will respect our formatted copy
        if record.exc_info:
            record.exc_text = self.formatException(record.exc_info, record)
        return logging.Formatter.format(self, record)

    def formatException(self, exc_info, record=None):
        """Format exception output with CONF.logging_exception_prefix."""
        if not record:
            return logging.Formatter.formatException(self, exc_info)

        stringbuffer = moves.StringIO()
        traceback.print_exception(exc_info[0], exc_info[1], exc_info[2],
                                  None, stringbuffer)
        lines = stringbuffer.getvalue().split('\n')
        stringbuffer.close()

        if CONF.logging_exception_prefix.find('%(asctime)') != -1:
            record.asctime = self.formatTime(record, self.datefmt)

        formatted_lines = []
        for line in lines:
            pl = CONF.logging_exception_prefix % record.__dict__
            fl = '%s%s' % (pl, line)
            formatted_lines.append(fl)
        return '\n'.join(formatted_lines)


class ColorHandler(logging.StreamHandler):
    LEVEL_COLORS = {
        logging.DEBUG: '\033[00;32m',  # GREEN
        logging.INFO: '\033[00;36m',  # CYAN
        logging.AUDIT: '\033[01;36m',  # BOLD CYAN
        logging.WARN: '\033[01;33m',  # BOLD YELLOW
        logging.ERROR: '\033[01;31m',  # BOLD RED
        logging.CRITICAL: '\033[01;31m',  # BOLD RED
    }

    def format(self, record):
        record.color = self.LEVEL_COLORS[record.levelno]
        return logging.StreamHandler.format(self, record)


class DeprecatedConfig(Exception):
    message = _("Fatal call to deprecated config: %(msg)s")

    def __init__(self, msg):
        super(Exception, self).__init__(self.message % dict(msg=msg))

########NEW FILE########
__FILENAME__ = loopingcall
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import sys

from eventlet import event
from eventlet import greenthread

from designate.openstack.common.gettextutils import _LE, _LW
from designate.openstack.common import log as logging
from designate.openstack.common import timeutils

LOG = logging.getLogger(__name__)


class LoopingCallDone(Exception):
    """Exception to break out and stop a LoopingCall.

    The poll-function passed to LoopingCall can raise this exception to
    break out of the loop normally. This is somewhat analogous to
    StopIteration.

    An optional return-value can be included as the argument to the exception;
    this return-value will be returned by LoopingCall.wait()

    """

    def __init__(self, retvalue=True):
        """:param retvalue: Value that LoopingCall.wait() should return."""
        self.retvalue = retvalue


class LoopingCallBase(object):
    def __init__(self, f=None, *args, **kw):
        self.args = args
        self.kw = kw
        self.f = f
        self._running = False
        self.done = None

    def stop(self):
        self._running = False

    def wait(self):
        return self.done.wait()


class FixedIntervalLoopingCall(LoopingCallBase):
    """A fixed interval looping call."""

    def start(self, interval, initial_delay=None):
        self._running = True
        done = event.Event()

        def _inner():
            if initial_delay:
                greenthread.sleep(initial_delay)

            try:
                while self._running:
                    start = timeutils.utcnow()
                    self.f(*self.args, **self.kw)
                    end = timeutils.utcnow()
                    if not self._running:
                        break
                    delay = interval - timeutils.delta_seconds(start, end)
                    if delay <= 0:
                        LOG.warn(_LW('task run outlasted interval by %s sec') %
                                 -delay)
                    greenthread.sleep(delay if delay > 0 else 0)
            except LoopingCallDone as e:
                self.stop()
                done.send(e.retvalue)
            except Exception:
                LOG.exception(_LE('in fixed duration looping call'))
                done.send_exception(*sys.exc_info())
                return
            else:
                done.send(True)

        self.done = done

        greenthread.spawn_n(_inner)
        return self.done


# TODO(mikal): this class name is deprecated in Havana and should be removed
# in the I release
LoopingCall = FixedIntervalLoopingCall


class DynamicLoopingCall(LoopingCallBase):
    """A looping call which sleeps until the next known event.

    The function called should return how long to sleep for before being
    called again.
    """

    def start(self, initial_delay=None, periodic_interval_max=None):
        self._running = True
        done = event.Event()

        def _inner():
            if initial_delay:
                greenthread.sleep(initial_delay)

            try:
                while self._running:
                    idle = self.f(*self.args, **self.kw)
                    if not self._running:
                        break

                    if periodic_interval_max is not None:
                        idle = min(idle, periodic_interval_max)
                    LOG.debug('Dynamic looping call sleeping for %.02f '
                              'seconds', idle)
                    greenthread.sleep(idle)
            except LoopingCallDone as e:
                self.stop()
                done.send(e.retvalue)
            except Exception:
                LOG.exception(_LE('in dynamic looping call'))
                done.send_exception(*sys.exc_info())
                return
            else:
                done.send(True)

        self.done = done

        greenthread.spawn(_inner)
        return self.done

########NEW FILE########
__FILENAME__ = network_utils
# Copyright 2012 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Network-related utilities and helper functions.
"""

# TODO(jd) Use six.moves once
# https://bitbucket.org/gutworth/six/pull-request/28
# is merged
try:
    import urllib.parse
    SplitResult = urllib.parse.SplitResult
except ImportError:
    import urlparse
    SplitResult = urlparse.SplitResult

from six.moves.urllib import parse


def parse_host_port(address, default_port=None):
    """Interpret a string as a host:port pair.

    An IPv6 address MUST be escaped if accompanied by a port,
    because otherwise ambiguity ensues: 2001:db8:85a3::8a2e:370:7334
    means both [2001:db8:85a3::8a2e:370:7334] and
    [2001:db8:85a3::8a2e:370]:7334.

    >>> parse_host_port('server01:80')
    ('server01', 80)
    >>> parse_host_port('server01')
    ('server01', None)
    >>> parse_host_port('server01', default_port=1234)
    ('server01', 1234)
    >>> parse_host_port('[::1]:80')
    ('::1', 80)
    >>> parse_host_port('[::1]')
    ('::1', None)
    >>> parse_host_port('[::1]', default_port=1234)
    ('::1', 1234)
    >>> parse_host_port('2001:db8:85a3::8a2e:370:7334', default_port=1234)
    ('2001:db8:85a3::8a2e:370:7334', 1234)

    """
    if address[0] == '[':
        # Escaped ipv6
        _host, _port = address[1:].split(']')
        host = _host
        if ':' in _port:
            port = _port.split(':')[1]
        else:
            port = default_port
    else:
        if address.count(':') == 1:
            host, port = address.split(':')
        else:
            # 0 means ipv4, >1 means ipv6.
            # We prohibit unescaped ipv6 addresses with port.
            host = address
            port = default_port

    return (host, None if port is None else int(port))


def urlsplit(url, scheme='', allow_fragments=True):
    """Parse a URL using urlparse.urlsplit(), splitting query and fragments.
    This function papers over Python issue9374 when needed.

    The parameters are the same as urlparse.urlsplit.
    """
    scheme, netloc, path, query, fragment = parse.urlsplit(
        url, scheme, allow_fragments)
    if allow_fragments and '#' in path:
        path, fragment = path.split('#', 1)
    if '?' in path:
        path, query = path.split('?', 1)
    return SplitResult(scheme, netloc, path, query, fragment)

########NEW FILE########
__FILENAME__ = policy
# Copyright (c) 2012 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Common Policy Engine Implementation

Policies can be expressed in one of two forms: A list of lists, or a
string written in the new policy language.

In the list-of-lists representation, each check inside the innermost
list is combined as with an "and" conjunction--for that check to pass,
all the specified checks must pass.  These innermost lists are then
combined as with an "or" conjunction.  This is the original way of
expressing policies, but there now exists a new way: the policy
language.

In the policy language, each check is specified the same way as in the
list-of-lists representation: a simple "a:b" pair that is matched to
the correct code to perform that check.  However, conjunction
operators are available, allowing for more expressiveness in crafting
policies.

As an example, take the following rule, expressed in the list-of-lists
representation::

    [["role:admin"], ["project_id:%(project_id)s", "role:projectadmin"]]

In the policy language, this becomes::

    role:admin or (project_id:%(project_id)s and role:projectadmin)

The policy language also has the "not" operator, allowing a richer
policy rule::

    project_id:%(project_id)s and not role:dunce

It is possible to perform policy checks on the following user
attributes (obtained through the token): user_id, domain_id or
project_id::

    domain_id:<some_value>

Attributes sent along with API calls can be used by the policy engine
(on the right side of the expression), by using the following syntax::

    <some_value>:user.id

Contextual attributes of objects identified by their IDs are loaded
from the database. They are also available to the policy engine and
can be checked through the `target` keyword::

    <some_value>:target.role.name

All these attributes (related to users, API calls, and context) can be
checked against each other or against constants, be it literals (True,
<a_number>) or strings.

Finally, two special policy checks should be mentioned; the policy
check "@" will always accept an access, and the policy check "!" will
always reject an access.  (Note that if a rule is either the empty
list ("[]") or the empty string, this is equivalent to the "@" policy
check.)  Of these, the "!" policy check is probably the most useful,
as it allows particular rules to be explicitly disabled.
"""

import abc
import ast
import re

from oslo.config import cfg
import six
import six.moves.urllib.parse as urlparse
import six.moves.urllib.request as urlrequest

from designate.openstack.common import fileutils
from designate.openstack.common.gettextutils import _, _LE
from designate.openstack.common import jsonutils
from designate.openstack.common import log as logging


policy_opts = [
    cfg.StrOpt('policy_file',
               default='policy.json',
               help=_('The JSON file that defines policies.')),
    cfg.StrOpt('policy_default_rule',
               default='default',
               help=_('Default rule. Enforced when a requested rule is not '
                      'found.')),
]

CONF = cfg.CONF
CONF.register_opts(policy_opts)

LOG = logging.getLogger(__name__)

_checks = {}


class PolicyNotAuthorized(Exception):

    def __init__(self, rule):
        msg = _("Policy doesn't allow %s to be performed.") % rule
        super(PolicyNotAuthorized, self).__init__(msg)


class Rules(dict):
    """A store for rules. Handles the default_rule setting directly."""

    @classmethod
    def load_json(cls, data, default_rule=None):
        """Allow loading of JSON rule data."""

        # Suck in the JSON data and parse the rules
        rules = dict((k, parse_rule(v)) for k, v in
                     jsonutils.loads(data).items())

        return cls(rules, default_rule)

    def __init__(self, rules=None, default_rule=None):
        """Initialize the Rules store."""

        super(Rules, self).__init__(rules or {})
        self.default_rule = default_rule

    def __missing__(self, key):
        """Implements the default rule handling."""

        if isinstance(self.default_rule, dict):
            raise KeyError(key)

        # If the default rule isn't actually defined, do something
        # reasonably intelligent
        if not self.default_rule:
            raise KeyError(key)

        if isinstance(self.default_rule, BaseCheck):
            return self.default_rule

        # We need to check this or we can get infinite recursion
        if self.default_rule not in self:
            raise KeyError(key)

        elif isinstance(self.default_rule, six.string_types):
            return self[self.default_rule]

    def __str__(self):
        """Dumps a string representation of the rules."""

        # Start by building the canonical strings for the rules
        out_rules = {}
        for key, value in self.items():
            # Use empty string for singleton TrueCheck instances
            if isinstance(value, TrueCheck):
                out_rules[key] = ''
            else:
                out_rules[key] = str(value)

        # Dump a pretty-printed JSON representation
        return jsonutils.dumps(out_rules, indent=4)


class Enforcer(object):
    """Responsible for loading and enforcing rules.

    :param policy_file: Custom policy file to use, if none is
                        specified, `CONF.policy_file` will be
                        used.
    :param rules: Default dictionary / Rules to use. It will be
                  considered just in the first instantiation. If
                  `load_rules(True)`, `clear()` or `set_rules(True)`
                  is called this will be overwritten.
    :param default_rule: Default rule to use, CONF.default_rule will
                         be used if none is specified.
    :param use_conf: Whether to load rules from cache or config file.
    """

    def __init__(self, policy_file=None, rules=None,
                 default_rule=None, use_conf=True):
        self.rules = Rules(rules, default_rule)
        self.default_rule = default_rule or CONF.policy_default_rule

        self.policy_path = None
        self.policy_file = policy_file or CONF.policy_file
        self.use_conf = use_conf

    def set_rules(self, rules, overwrite=True, use_conf=False):
        """Create a new Rules object based on the provided dict of rules.

        :param rules: New rules to use. It should be an instance of dict.
        :param overwrite: Whether to overwrite current rules or update them
                          with the new rules.
        :param use_conf: Whether to reload rules from cache or config file.
        """

        if not isinstance(rules, dict):
            raise TypeError(_("Rules must be an instance of dict or Rules, "
                            "got %s instead") % type(rules))
        self.use_conf = use_conf
        if overwrite:
            self.rules = Rules(rules, self.default_rule)
        else:
            self.rules.update(rules)

    def clear(self):
        """Clears Enforcer rules, policy's cache and policy's path."""
        self.set_rules({})
        self.default_rule = None
        self.policy_path = None

    def load_rules(self, force_reload=False):
        """Loads policy_path's rules.

        Policy file is cached and will be reloaded if modified.

        :param force_reload: Whether to overwrite current rules.
        """

        if force_reload:
            self.use_conf = force_reload

        if self.use_conf:
            if not self.policy_path:
                self.policy_path = self._get_policy_path()

            reloaded, data = fileutils.read_cached_file(
                self.policy_path, force_reload=force_reload)
            if reloaded or not self.rules:
                rules = Rules.load_json(data, self.default_rule)
                self.set_rules(rules)
                LOG.debug("Rules successfully reloaded")

    def _get_policy_path(self):
        """Locate the policy json data file.

        :param policy_file: Custom policy file to locate.

        :returns: The policy path

        :raises: ConfigFilesNotFoundError if the file couldn't
                 be located.
        """
        policy_file = CONF.find_file(self.policy_file)

        if policy_file:
            return policy_file

        raise cfg.ConfigFilesNotFoundError((self.policy_file,))

    def enforce(self, rule, target, creds, do_raise=False,
                exc=None, *args, **kwargs):
        """Checks authorization of a rule against the target and credentials.

        :param rule: A string or BaseCheck instance specifying the rule
                    to evaluate.
        :param target: As much information about the object being operated
                    on as possible, as a dictionary.
        :param creds: As much information about the user performing the
                    action as possible, as a dictionary.
        :param do_raise: Whether to raise an exception or not if check
                        fails.
        :param exc: Class of the exception to raise if the check fails.
                    Any remaining arguments passed to check() (both
                    positional and keyword arguments) will be passed to
                    the exception class. If not specified, PolicyNotAuthorized
                    will be used.

        :return: Returns False if the policy does not allow the action and
                exc is not provided; otherwise, returns a value that
                evaluates to True.  Note: for rules using the "case"
                expression, this True value will be the specified string
                from the expression.
        """

        # NOTE(flaper87): Not logging target or creds to avoid
        # potential security issues.
        LOG.debug("Rule %s will be now enforced" % rule)

        self.load_rules()

        # Allow the rule to be a Check tree
        if isinstance(rule, BaseCheck):
            result = rule(target, creds, self)
        elif not self.rules:
            # No rules to reference means we're going to fail closed
            result = False
        else:
            try:
                # Evaluate the rule
                result = self.rules[rule](target, creds, self)
            except KeyError:
                LOG.debug("Rule [%s] doesn't exist" % rule)
                # If the rule doesn't exist, fail closed
                result = False

        # If it is False, raise the exception if requested
        if do_raise and not result:
            if exc:
                raise exc(*args, **kwargs)

            raise PolicyNotAuthorized(rule)

        return result


@six.add_metaclass(abc.ABCMeta)
class BaseCheck(object):
    """Abstract base class for Check classes."""

    @abc.abstractmethod
    def __str__(self):
        """String representation of the Check tree rooted at this node."""

        pass

    @abc.abstractmethod
    def __call__(self, target, cred, enforcer):
        """Triggers if instance of the class is called.

        Performs the check. Returns False to reject the access or a
        true value (not necessary True) to accept the access.
        """

        pass


class FalseCheck(BaseCheck):
    """A policy check that always returns False (disallow)."""

    def __str__(self):
        """Return a string representation of this check."""

        return "!"

    def __call__(self, target, cred, enforcer):
        """Check the policy."""

        return False


class TrueCheck(BaseCheck):
    """A policy check that always returns True (allow)."""

    def __str__(self):
        """Return a string representation of this check."""

        return "@"

    def __call__(self, target, cred, enforcer):
        """Check the policy."""

        return True


class Check(BaseCheck):
    """A base class to allow for user-defined policy checks."""

    def __init__(self, kind, match):
        """Initiates Check instance.

        :param kind: The kind of the check, i.e., the field before the
                     ':'.
        :param match: The match of the check, i.e., the field after
                      the ':'.
        """

        self.kind = kind
        self.match = match

    def __str__(self):
        """Return a string representation of this check."""

        return "%s:%s" % (self.kind, self.match)


class NotCheck(BaseCheck):
    """Implements the "not" logical operator.

    A policy check that inverts the result of another policy check.
    """

    def __init__(self, rule):
        """Initialize the 'not' check.

        :param rule: The rule to negate.  Must be a Check.
        """

        self.rule = rule

    def __str__(self):
        """Return a string representation of this check."""

        return "not %s" % self.rule

    def __call__(self, target, cred, enforcer):
        """Check the policy.

        Returns the logical inverse of the wrapped check.
        """

        return not self.rule(target, cred, enforcer)


class AndCheck(BaseCheck):
    """Implements the "and" logical operator.

    A policy check that requires that a list of other checks all return True.
    """

    def __init__(self, rules):
        """Initialize the 'and' check.

        :param rules: A list of rules that will be tested.
        """

        self.rules = rules

    def __str__(self):
        """Return a string representation of this check."""

        return "(%s)" % ' and '.join(str(r) for r in self.rules)

    def __call__(self, target, cred, enforcer):
        """Check the policy.

        Requires that all rules accept in order to return True.
        """

        for rule in self.rules:
            if not rule(target, cred, enforcer):
                return False

        return True

    def add_check(self, rule):
        """Adds rule to be tested.

        Allows addition of another rule to the list of rules that will
        be tested.  Returns the AndCheck object for convenience.
        """

        self.rules.append(rule)
        return self


class OrCheck(BaseCheck):
    """Implements the "or" operator.

    A policy check that requires that at least one of a list of other
    checks returns True.
    """

    def __init__(self, rules):
        """Initialize the 'or' check.

        :param rules: A list of rules that will be tested.
        """

        self.rules = rules

    def __str__(self):
        """Return a string representation of this check."""

        return "(%s)" % ' or '.join(str(r) for r in self.rules)

    def __call__(self, target, cred, enforcer):
        """Check the policy.

        Requires that at least one rule accept in order to return True.
        """

        for rule in self.rules:
            if rule(target, cred, enforcer):
                return True
        return False

    def add_check(self, rule):
        """Adds rule to be tested.

        Allows addition of another rule to the list of rules that will
        be tested.  Returns the OrCheck object for convenience.
        """

        self.rules.append(rule)
        return self


def _parse_check(rule):
    """Parse a single base check rule into an appropriate Check object."""

    # Handle the special checks
    if rule == '!':
        return FalseCheck()
    elif rule == '@':
        return TrueCheck()

    try:
        kind, match = rule.split(':', 1)
    except Exception:
        LOG.exception(_LE("Failed to understand rule %s") % rule)
        # If the rule is invalid, we'll fail closed
        return FalseCheck()

    # Find what implements the check
    if kind in _checks:
        return _checks[kind](kind, match)
    elif None in _checks:
        return _checks[None](kind, match)
    else:
        LOG.error(_LE("No handler for matches of kind %s") % kind)
        return FalseCheck()


def _parse_list_rule(rule):
    """Translates the old list-of-lists syntax into a tree of Check objects.

    Provided for backwards compatibility.
    """

    # Empty rule defaults to True
    if not rule:
        return TrueCheck()

    # Outer list is joined by "or"; inner list by "and"
    or_list = []
    for inner_rule in rule:
        # Elide empty inner lists
        if not inner_rule:
            continue

        # Handle bare strings
        if isinstance(inner_rule, six.string_types):
            inner_rule = [inner_rule]

        # Parse the inner rules into Check objects
        and_list = [_parse_check(r) for r in inner_rule]

        # Append the appropriate check to the or_list
        if len(and_list) == 1:
            or_list.append(and_list[0])
        else:
            or_list.append(AndCheck(and_list))

    # If we have only one check, omit the "or"
    if not or_list:
        return FalseCheck()
    elif len(or_list) == 1:
        return or_list[0]

    return OrCheck(or_list)


# Used for tokenizing the policy language
_tokenize_re = re.compile(r'\s+')


def _parse_tokenize(rule):
    """Tokenizer for the policy language.

    Most of the single-character tokens are specified in the
    _tokenize_re; however, parentheses need to be handled specially,
    because they can appear inside a check string.  Thankfully, those
    parentheses that appear inside a check string can never occur at
    the very beginning or end ("%(variable)s" is the correct syntax).
    """

    for tok in _tokenize_re.split(rule):
        # Skip empty tokens
        if not tok or tok.isspace():
            continue

        # Handle leading parens on the token
        clean = tok.lstrip('(')
        for i in range(len(tok) - len(clean)):
            yield '(', '('

        # If it was only parentheses, continue
        if not clean:
            continue
        else:
            tok = clean

        # Handle trailing parens on the token
        clean = tok.rstrip(')')
        trail = len(tok) - len(clean)

        # Yield the cleaned token
        lowered = clean.lower()
        if lowered in ('and', 'or', 'not'):
            # Special tokens
            yield lowered, clean
        elif clean:
            # Not a special token, but not composed solely of ')'
            if len(tok) >= 2 and ((tok[0], tok[-1]) in
                                  [('"', '"'), ("'", "'")]):
                # It's a quoted string
                yield 'string', tok[1:-1]
            else:
                yield 'check', _parse_check(clean)

        # Yield the trailing parens
        for i in range(trail):
            yield ')', ')'


class ParseStateMeta(type):
    """Metaclass for the ParseState class.

    Facilitates identifying reduction methods.
    """

    def __new__(mcs, name, bases, cls_dict):
        """Create the class.

        Injects the 'reducers' list, a list of tuples matching token sequences
        to the names of the corresponding reduction methods.
        """

        reducers = []

        for key, value in cls_dict.items():
            if not hasattr(value, 'reducers'):
                continue
            for reduction in value.reducers:
                reducers.append((reduction, key))

        cls_dict['reducers'] = reducers

        return super(ParseStateMeta, mcs).__new__(mcs, name, bases, cls_dict)


def reducer(*tokens):
    """Decorator for reduction methods.

    Arguments are a sequence of tokens, in order, which should trigger running
    this reduction method.
    """

    def decorator(func):
        # Make sure we have a list of reducer sequences
        if not hasattr(func, 'reducers'):
            func.reducers = []

        # Add the tokens to the list of reducer sequences
        func.reducers.append(list(tokens))

        return func

    return decorator


@six.add_metaclass(ParseStateMeta)
class ParseState(object):
    """Implement the core of parsing the policy language.

    Uses a greedy reduction algorithm to reduce a sequence of tokens into
    a single terminal, the value of which will be the root of the Check tree.

    Note: error reporting is rather lacking.  The best we can get with
    this parser formulation is an overall "parse failed" error.
    Fortunately, the policy language is simple enough that this
    shouldn't be that big a problem.
    """

    def __init__(self):
        """Initialize the ParseState."""

        self.tokens = []
        self.values = []

    def reduce(self):
        """Perform a greedy reduction of the token stream.

        If a reducer method matches, it will be executed, then the
        reduce() method will be called recursively to search for any more
        possible reductions.
        """

        for reduction, methname in self.reducers:
            if (len(self.tokens) >= len(reduction) and
                    self.tokens[-len(reduction):] == reduction):
                # Get the reduction method
                meth = getattr(self, methname)

                # Reduce the token stream
                results = meth(*self.values[-len(reduction):])

                # Update the tokens and values
                self.tokens[-len(reduction):] = [r[0] for r in results]
                self.values[-len(reduction):] = [r[1] for r in results]

                # Check for any more reductions
                return self.reduce()

    def shift(self, tok, value):
        """Adds one more token to the state.  Calls reduce()."""

        self.tokens.append(tok)
        self.values.append(value)

        # Do a greedy reduce...
        self.reduce()

    @property
    def result(self):
        """Obtain the final result of the parse.

        Raises ValueError if the parse failed to reduce to a single result.
        """

        if len(self.values) != 1:
            raise ValueError("Could not parse rule")
        return self.values[0]

    @reducer('(', 'check', ')')
    @reducer('(', 'and_expr', ')')
    @reducer('(', 'or_expr', ')')
    def _wrap_check(self, _p1, check, _p2):
        """Turn parenthesized expressions into a 'check' token."""

        return [('check', check)]

    @reducer('check', 'and', 'check')
    def _make_and_expr(self, check1, _and, check2):
        """Create an 'and_expr'.

        Join two checks by the 'and' operator.
        """

        return [('and_expr', AndCheck([check1, check2]))]

    @reducer('and_expr', 'and', 'check')
    def _extend_and_expr(self, and_expr, _and, check):
        """Extend an 'and_expr' by adding one more check."""

        return [('and_expr', and_expr.add_check(check))]

    @reducer('check', 'or', 'check')
    def _make_or_expr(self, check1, _or, check2):
        """Create an 'or_expr'.

        Join two checks by the 'or' operator.
        """

        return [('or_expr', OrCheck([check1, check2]))]

    @reducer('or_expr', 'or', 'check')
    def _extend_or_expr(self, or_expr, _or, check):
        """Extend an 'or_expr' by adding one more check."""

        return [('or_expr', or_expr.add_check(check))]

    @reducer('not', 'check')
    def _make_not_expr(self, _not, check):
        """Invert the result of another check."""

        return [('check', NotCheck(check))]


def _parse_text_rule(rule):
    """Parses policy to the tree.

    Translates a policy written in the policy language into a tree of
    Check objects.
    """

    # Empty rule means always accept
    if not rule:
        return TrueCheck()

    # Parse the token stream
    state = ParseState()
    for tok, value in _parse_tokenize(rule):
        state.shift(tok, value)

    try:
        return state.result
    except ValueError:
        # Couldn't parse the rule
        LOG.exception(_LE("Failed to understand rule %r") % rule)

        # Fail closed
        return FalseCheck()


def parse_rule(rule):
    """Parses a policy rule into a tree of Check objects."""

    # If the rule is a string, it's in the policy language
    if isinstance(rule, six.string_types):
        return _parse_text_rule(rule)
    return _parse_list_rule(rule)


def register(name, func=None):
    """Register a function or Check class as a policy check.

    :param name: Gives the name of the check type, e.g., 'rule',
                 'role', etc.  If name is None, a default check type
                 will be registered.
    :param func: If given, provides the function or class to register.
                 If not given, returns a function taking one argument
                 to specify the function or class to register,
                 allowing use as a decorator.
    """

    # Perform the actual decoration by registering the function or
    # class.  Returns the function or class for compliance with the
    # decorator interface.
    def decorator(func):
        _checks[name] = func
        return func

    # If the function or class is given, do the registration
    if func:
        return decorator(func)

    return decorator


@register("rule")
class RuleCheck(Check):
    def __call__(self, target, creds, enforcer):
        """Recursively checks credentials based on the defined rules."""

        try:
            return enforcer.rules[self.match](target, creds, enforcer)
        except KeyError:
            # We don't have any matching rule; fail closed
            return False


@register("role")
class RoleCheck(Check):
    def __call__(self, target, creds, enforcer):
        """Check that there is a matching role in the cred dict."""

        return self.match.lower() in [x.lower() for x in creds['roles']]


@register('http')
class HttpCheck(Check):
    def __call__(self, target, creds, enforcer):
        """Check http: rules by calling to a remote server.

        This example implementation simply verifies that the response
        is exactly 'True'.
        """

        url = ('http:' + self.match) % target
        data = {'target': jsonutils.dumps(target),
                'credentials': jsonutils.dumps(creds)}
        post_data = urlparse.urlencode(data)
        f = urlrequest.urlopen(url, post_data)
        return f.read() == "True"


@register(None)
class GenericCheck(Check):
    def __call__(self, target, creds, enforcer):
        """Check an individual match.

        Matches look like:

            tenant:%(tenant_id)s
            role:compute:admin
            True:%(user.enabled)s
            'Member':%(role.name)s
        """

        # TODO(termie): do dict inspection via dot syntax
        try:
            match = self.match % target
        except KeyError:
            # While doing GenericCheck if key not
            # present in Target return false
            return False

        try:
            # Try to interpret self.kind as a literal
            leftval = ast.literal_eval(self.kind)
        except ValueError:
            try:
                leftval = creds[self.kind]
            except KeyError:
                return False
        return match == six.text_type(leftval)

########NEW FILE########
__FILENAME__ = processutils
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
System-level utilities and helper functions.
"""

import errno
import logging as stdlib_logging
import os
import random
import shlex
import signal

from eventlet.green import subprocess
from eventlet import greenthread
import six

from designate.openstack.common.gettextutils import _
from designate.openstack.common import log as logging


LOG = logging.getLogger(__name__)


class InvalidArgumentError(Exception):
    def __init__(self, message=None):
        super(InvalidArgumentError, self).__init__(message)


class UnknownArgumentError(Exception):
    def __init__(self, message=None):
        super(UnknownArgumentError, self).__init__(message)


class ProcessExecutionError(Exception):
    def __init__(self, stdout=None, stderr=None, exit_code=None, cmd=None,
                 description=None):
        self.exit_code = exit_code
        self.stderr = stderr
        self.stdout = stdout
        self.cmd = cmd
        self.description = description

        if description is None:
            description = _("Unexpected error while running command.")
        if exit_code is None:
            exit_code = '-'
        message = _('%(description)s\n'
                    'Command: %(cmd)s\n'
                    'Exit code: %(exit_code)s\n'
                    'Stdout: %(stdout)r\n'
                    'Stderr: %(stderr)r') % {'description': description,
                                             'cmd': cmd,
                                             'exit_code': exit_code,
                                             'stdout': stdout,
                                             'stderr': stderr}
        super(ProcessExecutionError, self).__init__(message)


class NoRootWrapSpecified(Exception):
    def __init__(self, message=None):
        super(NoRootWrapSpecified, self).__init__(message)


def _subprocess_setup():
    # Python installs a SIGPIPE handler by default. This is usually not what
    # non-Python subprocesses expect.
    signal.signal(signal.SIGPIPE, signal.SIG_DFL)


def execute(*cmd, **kwargs):
    """Helper method to shell out and execute a command through subprocess.

    Allows optional retry.

    :param cmd:             Passed to subprocess.Popen.
    :type cmd:              string
    :param process_input:   Send to opened process.
    :type process_input:    string
    :param check_exit_code: Single bool, int, or list of allowed exit
                            codes.  Defaults to [0].  Raise
                            :class:`ProcessExecutionError` unless
                            program exits with one of these code.
    :type check_exit_code:  boolean, int, or [int]
    :param delay_on_retry:  True | False. Defaults to True. If set to True,
                            wait a short amount of time before retrying.
    :type delay_on_retry:   boolean
    :param attempts:        How many times to retry cmd.
    :type attempts:         int
    :param run_as_root:     True | False. Defaults to False. If set to True,
                            the command is prefixed by the command specified
                            in the root_helper kwarg.
    :type run_as_root:      boolean
    :param root_helper:     command to prefix to commands called with
                            run_as_root=True
    :type root_helper:      string
    :param shell:           whether or not there should be a shell used to
                            execute this command. Defaults to false.
    :type shell:            boolean
    :param loglevel:        log level for execute commands.
    :type loglevel:         int.  (Should be stdlib_logging.DEBUG or
                            stdlib_logging.INFO)
    :returns:               (stdout, stderr) from process execution
    :raises:                :class:`UnknownArgumentError` on
                            receiving unknown arguments
    :raises:                :class:`ProcessExecutionError`
    """

    process_input = kwargs.pop('process_input', None)
    check_exit_code = kwargs.pop('check_exit_code', [0])
    ignore_exit_code = False
    delay_on_retry = kwargs.pop('delay_on_retry', True)
    attempts = kwargs.pop('attempts', 1)
    run_as_root = kwargs.pop('run_as_root', False)
    root_helper = kwargs.pop('root_helper', '')
    shell = kwargs.pop('shell', False)
    loglevel = kwargs.pop('loglevel', stdlib_logging.DEBUG)

    if isinstance(check_exit_code, bool):
        ignore_exit_code = not check_exit_code
        check_exit_code = [0]
    elif isinstance(check_exit_code, int):
        check_exit_code = [check_exit_code]

    if kwargs:
        raise UnknownArgumentError(_('Got unknown keyword args '
                                     'to utils.execute: %r') % kwargs)

    if run_as_root and hasattr(os, 'geteuid') and os.geteuid() != 0:
        if not root_helper:
            raise NoRootWrapSpecified(
                message=_('Command requested root, but did not '
                          'specify a root helper.'))
        cmd = shlex.split(root_helper) + list(cmd)

    cmd = map(str, cmd)

    while attempts > 0:
        attempts -= 1
        try:
            LOG.log(loglevel, 'Running cmd (subprocess): %s',
                    ' '.join(cmd))
            _PIPE = subprocess.PIPE  # pylint: disable=E1101

            if os.name == 'nt':
                preexec_fn = None
                close_fds = False
            else:
                preexec_fn = _subprocess_setup
                close_fds = True

            obj = subprocess.Popen(cmd,
                                   stdin=_PIPE,
                                   stdout=_PIPE,
                                   stderr=_PIPE,
                                   close_fds=close_fds,
                                   preexec_fn=preexec_fn,
                                   shell=shell)
            result = None
            for _i in six.moves.range(20):
                # NOTE(russellb) 20 is an arbitrary number of retries to
                # prevent any chance of looping forever here.
                try:
                    if process_input is not None:
                        result = obj.communicate(process_input)
                    else:
                        result = obj.communicate()
                except OSError as e:
                    if e.errno in (errno.EAGAIN, errno.EINTR):
                        continue
                    raise
                break
            obj.stdin.close()  # pylint: disable=E1101
            _returncode = obj.returncode  # pylint: disable=E1101
            LOG.log(loglevel, 'Result was %s' % _returncode)
            if not ignore_exit_code and _returncode not in check_exit_code:
                (stdout, stderr) = result
                raise ProcessExecutionError(exit_code=_returncode,
                                            stdout=stdout,
                                            stderr=stderr,
                                            cmd=' '.join(cmd))
            return result
        except ProcessExecutionError:
            if not attempts:
                raise
            else:
                LOG.log(loglevel, '%r failed. Retrying.', cmd)
                if delay_on_retry:
                    greenthread.sleep(random.randint(20, 200) / 100.0)
        finally:
            # NOTE(termie): this appears to be necessary to let the subprocess
            #               call clean something up in between calls, without
            #               it two execute calls in a row hangs the second one
            greenthread.sleep(0)


def trycmd(*args, **kwargs):
    """A wrapper around execute() to more easily handle warnings and errors.

    Returns an (out, err) tuple of strings containing the output of
    the command's stdout and stderr.  If 'err' is not empty then the
    command can be considered to have failed.

    :discard_warnings   True | False. Defaults to False. If set to True,
                        then for succeeding commands, stderr is cleared

    """
    discard_warnings = kwargs.pop('discard_warnings', False)

    try:
        out, err = execute(*args, **kwargs)
        failed = False
    except ProcessExecutionError as exn:
        out, err = '', str(exn)
        failed = True

    if not failed and discard_warnings and err:
        # Handle commands that output to stderr but otherwise succeed
        err = ''

    return out, err


def ssh_execute(ssh, cmd, process_input=None,
                addl_env=None, check_exit_code=True):
    LOG.debug('Running cmd (SSH): %s', cmd)
    if addl_env:
        raise InvalidArgumentError(_('Environment not supported over SSH'))

    if process_input:
        # This is (probably) fixable if we need it...
        raise InvalidArgumentError(_('process_input not supported over SSH'))

    stdin_stream, stdout_stream, stderr_stream = ssh.exec_command(cmd)
    channel = stdout_stream.channel

    # NOTE(justinsb): This seems suspicious...
    # ...other SSH clients have buffering issues with this approach
    stdout = stdout_stream.read()
    stderr = stderr_stream.read()
    stdin_stream.close()

    exit_status = channel.recv_exit_status()

    # exit_status == -1 if no exit code was returned
    if exit_status != -1:
        LOG.debug('Result was %s' % exit_status)
        if check_exit_code and exit_status != 0:
            raise ProcessExecutionError(exit_code=exit_status,
                                        stdout=stdout,
                                        stderr=stderr,
                                        cmd=cmd)

    return (stdout, stderr)

########NEW FILE########
__FILENAME__ = urlutils
#
# Copyright 2013 Canonical Ltd.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

"""
Python2/Python3 compatibility layer for OpenStack
"""

import six

if six.PY3:
    # python3
    import urllib.error
    import urllib.parse
    import urllib.request

    urlencode = urllib.parse.urlencode
    urljoin = urllib.parse.urljoin
    quote = urllib.parse.quote
    quote_plus = urllib.parse.quote_plus
    parse_qsl = urllib.parse.parse_qsl
    unquote = urllib.parse.unquote
    unquote_plus = urllib.parse.unquote_plus
    urlparse = urllib.parse.urlparse
    urlsplit = urllib.parse.urlsplit
    urlunsplit = urllib.parse.urlunsplit
    SplitResult = urllib.parse.SplitResult

    urlopen = urllib.request.urlopen
    URLError = urllib.error.URLError
    pathname2url = urllib.request.pathname2url
else:
    # python2
    import urllib
    import urllib2
    import urlparse

    urlencode = urllib.urlencode
    quote = urllib.quote
    quote_plus = urllib.quote_plus
    unquote = urllib.unquote
    unquote_plus = urllib.unquote_plus

    parse = urlparse
    parse_qsl = parse.parse_qsl
    urljoin = parse.urljoin
    urlparse = parse.urlparse
    urlsplit = parse.urlsplit
    urlunsplit = parse.urlunsplit
    SplitResult = parse.SplitResult

    urlopen = urllib2.urlopen
    URLError = urllib2.URLError
    pathname2url = urllib.pathname2url

########NEW FILE########
__FILENAME__ = service
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Generic Node base class for all workers that run on hosts."""

import errno
import logging as std_logging
import os
import random
import signal
import sys
import time

try:
    # Importing just the symbol here because the io module does not
    # exist in Python 2.6.
    from io import UnsupportedOperation  # noqa
except ImportError:
    # Python 2.6
    UnsupportedOperation = None

import eventlet
from eventlet import event
from oslo.config import cfg

from designate.openstack.common import eventlet_backdoor
from designate.openstack.common.gettextutils import _LE, _LI, _LW
from designate.openstack.common import importutils
from designate.openstack.common import log as logging
from designate.openstack.common import systemd
from designate.openstack.common import threadgroup


rpc = importutils.try_import('designate.openstack.common.rpc')
CONF = cfg.CONF
LOG = logging.getLogger(__name__)


def _sighup_supported():
    return hasattr(signal, 'SIGHUP')


def _is_daemon():
    # The process group for a foreground process will match the
    # process group of the controlling terminal. If those values do
    # not match, or ioctl() fails on the stdout file handle, we assume
    # the process is running in the background as a daemon.
    # http://www.gnu.org/software/bash/manual/bashref.html#Job-Control-Basics
    try:
        is_daemon = os.getpgrp() != os.tcgetpgrp(sys.stdout.fileno())
    except OSError as err:
        if err.errno == errno.ENOTTY:
            # Assume we are a daemon because there is no terminal.
            is_daemon = True
        else:
            raise
    except UnsupportedOperation:
        # Could not get the fileno for stdout, so we must be a daemon.
        is_daemon = True
    return is_daemon


def _is_sighup_and_daemon(signo):
    if not (_sighup_supported() and signo == signal.SIGHUP):
        # Avoid checking if we are a daemon, because the signal isn't
        # SIGHUP.
        return False
    return _is_daemon()


def _signo_to_signame(signo):
    signals = {signal.SIGTERM: 'SIGTERM',
               signal.SIGINT: 'SIGINT'}
    if _sighup_supported():
        signals[signal.SIGHUP] = 'SIGHUP'
    return signals[signo]


def _set_signals_handler(handler):
    signal.signal(signal.SIGTERM, handler)
    signal.signal(signal.SIGINT, handler)
    if _sighup_supported():
        signal.signal(signal.SIGHUP, handler)


class Launcher(object):
    """Launch one or more services and wait for them to complete."""

    def __init__(self):
        """Initialize the service launcher.

        :returns: None

        """
        self.services = Services()
        self.backdoor_port = eventlet_backdoor.initialize_if_enabled()

    def launch_service(self, service):
        """Load and start the given service.

        :param service: The service you would like to start.
        :returns: None

        """
        service.backdoor_port = self.backdoor_port
        self.services.add(service)

    def stop(self):
        """Stop all services which are currently running.

        :returns: None

        """
        self.services.stop()

    def wait(self):
        """Waits until all services have been stopped, and then returns.

        :returns: None

        """
        self.services.wait()

    def restart(self):
        """Reload config files and restart service.

        :returns: None

        """
        cfg.CONF.reload_config_files()
        self.services.restart()


class SignalExit(SystemExit):
    def __init__(self, signo, exccode=1):
        super(SignalExit, self).__init__(exccode)
        self.signo = signo


class ServiceLauncher(Launcher):
    def _handle_signal(self, signo, frame):
        # Allow the process to be killed again and die from natural causes
        _set_signals_handler(signal.SIG_DFL)
        raise SignalExit(signo)

    def handle_signal(self):
        _set_signals_handler(self._handle_signal)

    def _wait_for_exit_or_signal(self, ready_callback=None):
        status = None
        signo = 0

        LOG.debug('Full set of CONF:')
        CONF.log_opt_values(LOG, std_logging.DEBUG)

        try:
            if ready_callback:
                ready_callback()
            super(ServiceLauncher, self).wait()
        except SignalExit as exc:
            signame = _signo_to_signame(exc.signo)
            LOG.info(_LI('Caught %s, exiting'), signame)
            status = exc.code
            signo = exc.signo
        except SystemExit as exc:
            status = exc.code
        finally:
            self.stop()
            if rpc:
                try:
                    rpc.cleanup()
                except Exception:
                    # We're shutting down, so it doesn't matter at this point.
                    LOG.exception(_LE('Exception during rpc cleanup.'))

        return status, signo

    def wait(self, ready_callback=None):
        while True:
            self.handle_signal()
            status, signo = self._wait_for_exit_or_signal(ready_callback)
            if not _is_sighup_and_daemon(signo):
                return status
            self.restart()


class ServiceWrapper(object):
    def __init__(self, service, workers):
        self.service = service
        self.workers = workers
        self.children = set()
        self.forktimes = []


class ProcessLauncher(object):
    def __init__(self, wait_interval=0.01):
        """Constructor.

        :param wait_interval: The interval to sleep for between checks
                              of child process exit.
        """
        self.children = {}
        self.sigcaught = None
        self.running = True
        self.wait_interval = wait_interval
        rfd, self.writepipe = os.pipe()
        self.readpipe = eventlet.greenio.GreenPipe(rfd, 'r')
        self.handle_signal()

    def handle_signal(self):
        _set_signals_handler(self._handle_signal)

    def _handle_signal(self, signo, frame):
        self.sigcaught = signo
        self.running = False

        # Allow the process to be killed again and die from natural causes
        _set_signals_handler(signal.SIG_DFL)

    def _pipe_watcher(self):
        # This will block until the write end is closed when the parent
        # dies unexpectedly
        self.readpipe.read()

        LOG.info(_LI('Parent process has died unexpectedly, exiting'))

        sys.exit(1)

    def _child_process_handle_signal(self):
        # Setup child signal handlers differently
        def _sigterm(*args):
            signal.signal(signal.SIGTERM, signal.SIG_DFL)
            raise SignalExit(signal.SIGTERM)

        def _sighup(*args):
            signal.signal(signal.SIGHUP, signal.SIG_DFL)
            raise SignalExit(signal.SIGHUP)

        signal.signal(signal.SIGTERM, _sigterm)
        if _sighup_supported():
            signal.signal(signal.SIGHUP, _sighup)
        # Block SIGINT and let the parent send us a SIGTERM
        signal.signal(signal.SIGINT, signal.SIG_IGN)

    def _child_wait_for_exit_or_signal(self, launcher):
        status = 0
        signo = 0

        # NOTE(johannes): All exceptions are caught to ensure this
        # doesn't fallback into the loop spawning children. It would
        # be bad for a child to spawn more children.
        try:
            launcher.wait()
        except SignalExit as exc:
            signame = _signo_to_signame(exc.signo)
            LOG.info(_LI('Caught %s, exiting'), signame)
            status = exc.code
            signo = exc.signo
        except SystemExit as exc:
            status = exc.code
        except BaseException:
            LOG.exception(_LE('Unhandled exception'))
            status = 2
        finally:
            launcher.stop()

        return status, signo

    def _child_process(self, service):
        self._child_process_handle_signal()

        # Reopen the eventlet hub to make sure we don't share an epoll
        # fd with parent and/or siblings, which would be bad
        eventlet.hubs.use_hub()

        # Close write to ensure only parent has it open
        os.close(self.writepipe)
        # Create greenthread to watch for parent to close pipe
        eventlet.spawn_n(self._pipe_watcher)

        # Reseed random number generator
        random.seed()

        launcher = Launcher()
        launcher.launch_service(service)
        return launcher

    def _start_child(self, wrap):
        if len(wrap.forktimes) > wrap.workers:
            # Limit ourselves to one process a second (over the period of
            # number of workers * 1 second). This will allow workers to
            # start up quickly but ensure we don't fork off children that
            # die instantly too quickly.
            if time.time() - wrap.forktimes[0] < wrap.workers:
                LOG.info(_LI('Forking too fast, sleeping'))
                time.sleep(1)

            wrap.forktimes.pop(0)

        wrap.forktimes.append(time.time())

        pid = os.fork()
        if pid == 0:
            launcher = self._child_process(wrap.service)
            while True:
                self._child_process_handle_signal()
                status, signo = self._child_wait_for_exit_or_signal(launcher)
                if not _is_sighup_and_daemon(signo):
                    break
                launcher.restart()

            os._exit(status)

        LOG.info(_LI('Started child %d'), pid)

        wrap.children.add(pid)
        self.children[pid] = wrap

        return pid

    def launch_service(self, service, workers=1):
        wrap = ServiceWrapper(service, workers)

        LOG.info(_LI('Starting %d workers'), wrap.workers)
        while self.running and len(wrap.children) < wrap.workers:
            self._start_child(wrap)

    def _wait_child(self):
        try:
            # Don't block if no child processes have exited
            pid, status = os.waitpid(0, os.WNOHANG)
            if not pid:
                return None
        except OSError as exc:
            if exc.errno not in (errno.EINTR, errno.ECHILD):
                raise
            return None

        if os.WIFSIGNALED(status):
            sig = os.WTERMSIG(status)
            LOG.info(_LI('Child %(pid)d killed by signal %(sig)d'),
                     dict(pid=pid, sig=sig))
        else:
            code = os.WEXITSTATUS(status)
            LOG.info(_LI('Child %(pid)s exited with status %(code)d'),
                     dict(pid=pid, code=code))

        if pid not in self.children:
            LOG.warning(_LW('pid %d not in child list'), pid)
            return None

        wrap = self.children.pop(pid)
        wrap.children.remove(pid)
        return wrap

    def _respawn_children(self):
        while self.running:
            wrap = self._wait_child()
            if not wrap:
                # Yield to other threads if no children have exited
                # Sleep for a short time to avoid excessive CPU usage
                # (see bug #1095346)
                eventlet.greenthread.sleep(self.wait_interval)
                continue
            while self.running and len(wrap.children) < wrap.workers:
                self._start_child(wrap)

    def wait(self):
        """Loop waiting on children to die and respawning as necessary."""

        LOG.debug('Full set of CONF:')
        CONF.log_opt_values(LOG, std_logging.DEBUG)

        try:
            while True:
                self.handle_signal()
                self._respawn_children()
                if self.sigcaught:
                    signame = _signo_to_signame(self.sigcaught)
                    LOG.info(_LI('Caught %s, stopping children'), signame)
                if not _is_sighup_and_daemon(self.sigcaught):
                    break

                for pid in self.children:
                    os.kill(pid, signal.SIGHUP)
                self.running = True
                self.sigcaught = None
        except eventlet.greenlet.GreenletExit:
            LOG.info(_LI("Wait called after thread killed.  Cleaning up."))

        for pid in self.children:
            try:
                os.kill(pid, signal.SIGTERM)
            except OSError as exc:
                if exc.errno != errno.ESRCH:
                    raise

        # Wait for children to die
        if self.children:
            LOG.info(_LI('Waiting on %d children to exit'), len(self.children))
            while self.children:
                self._wait_child()


class Service(object):
    """Service object for binaries running on hosts."""

    def __init__(self, threads=1000):
        self.tg = threadgroup.ThreadGroup(threads)

        # signal that the service is done shutting itself down:
        self._done = event.Event()

    def reset(self):
        # NOTE(Fengqian): docs for Event.reset() recommend against using it
        self._done = event.Event()

    def start(self):
        pass

    def stop(self):
        self.tg.stop()
        self.tg.wait()
        # Signal that service cleanup is done:
        if not self._done.ready():
            self._done.send()

    def wait(self):
        self._done.wait()


class Services(object):

    def __init__(self):
        self.services = []
        self.tg = threadgroup.ThreadGroup()
        self.done = event.Event()

    def add(self, service):
        self.services.append(service)
        self.tg.add_thread(self.run_service, service, self.done)

    def stop(self):
        # wait for graceful shutdown of services:
        for service in self.services:
            service.stop()
            service.wait()

        # Each service has performed cleanup, now signal that the run_service
        # wrapper threads can now die:
        if not self.done.ready():
            self.done.send()

        # reap threads:
        self.tg.stop()

    def wait(self):
        self.tg.wait()

    def restart(self):
        self.stop()
        self.done = event.Event()
        for restart_service in self.services:
            restart_service.reset()
            self.tg.add_thread(self.run_service, restart_service, self.done)

    @staticmethod
    def run_service(service, done):
        """Service start wrapper.

        :param service: service to run
        :param done: event to wait on until a shutdown is triggered
        :returns: None

        """
        service.start()
        systemd.notify_once()
        done.wait()


def launch(service, workers=1):
    if workers is None or workers == 1:
        launcher = ServiceLauncher()
        launcher.launch_service(service)
    else:
        launcher = ProcessLauncher()
        launcher.launch_service(service, workers=workers)

    return launcher

########NEW FILE########
__FILENAME__ = sslutils
# Copyright 2013 IBM Corp.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os
import ssl

from oslo.config import cfg

from designate.openstack.common.gettextutils import _


ssl_opts = [
    cfg.StrOpt('ca_file',
               default=None,
               help="CA certificate file to use to verify "
                    "connecting clients."),
    cfg.StrOpt('cert_file',
               default=None,
               help="Certificate file to use when starting "
                    "the server securely."),
    cfg.StrOpt('key_file',
               default=None,
               help="Private key file to use when starting "
                    "the server securely."),
]


CONF = cfg.CONF
CONF.register_opts(ssl_opts, "ssl")


def is_enabled():
    cert_file = CONF.ssl.cert_file
    key_file = CONF.ssl.key_file
    ca_file = CONF.ssl.ca_file
    use_ssl = cert_file or key_file

    if cert_file and not os.path.exists(cert_file):
        raise RuntimeError(_("Unable to find cert_file : %s") % cert_file)

    if ca_file and not os.path.exists(ca_file):
        raise RuntimeError(_("Unable to find ca_file : %s") % ca_file)

    if key_file and not os.path.exists(key_file):
        raise RuntimeError(_("Unable to find key_file : %s") % key_file)

    if use_ssl and (not cert_file or not key_file):
        raise RuntimeError(_("When running server in SSL mode, you must "
                             "specify both a cert_file and key_file "
                             "option value in your configuration file"))

    return use_ssl


def wrap(sock):
    ssl_kwargs = {
        'server_side': True,
        'certfile': CONF.ssl.cert_file,
        'keyfile': CONF.ssl.key_file,
        'cert_reqs': ssl.CERT_NONE,
    }

    if CONF.ssl.ca_file:
        ssl_kwargs['ca_certs'] = CONF.ssl.ca_file
        ssl_kwargs['cert_reqs'] = ssl.CERT_REQUIRED

    return ssl.wrap_socket(sock, **ssl_kwargs)


_SSL_PROTOCOLS = {
    "tlsv1": ssl.PROTOCOL_TLSv1,
    "sslv23": ssl.PROTOCOL_SSLv23,
    "sslv3": ssl.PROTOCOL_SSLv3
}

try:
    _SSL_PROTOCOLS["sslv2"] = ssl.PROTOCOL_SSLv2
except AttributeError:
    pass


def validate_ssl_version(version):
    key = version.lower()
    try:
        return _SSL_PROTOCOLS[key]
    except KeyError:
        raise RuntimeError(_("Invalid SSL version : %s") % version)

########NEW FILE########
__FILENAME__ = strutils
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
System-level utilities and helper functions.
"""

import math
import re
import sys
import unicodedata

import six

from designate.openstack.common.gettextutils import _


UNIT_PREFIX_EXPONENT = {
    'k': 1,
    'K': 1,
    'Ki': 1,
    'M': 2,
    'Mi': 2,
    'G': 3,
    'Gi': 3,
    'T': 4,
    'Ti': 4,
}
UNIT_SYSTEM_INFO = {
    'IEC': (1024, re.compile(r'(^[-+]?\d*\.?\d+)([KMGT]i?)?(b|bit|B)$')),
    'SI': (1000, re.compile(r'(^[-+]?\d*\.?\d+)([kMGT])?(b|bit|B)$')),
}

TRUE_STRINGS = ('1', 't', 'true', 'on', 'y', 'yes')
FALSE_STRINGS = ('0', 'f', 'false', 'off', 'n', 'no')

SLUGIFY_STRIP_RE = re.compile(r"[^\w\s-]")
SLUGIFY_HYPHENATE_RE = re.compile(r"[-\s]+")


def int_from_bool_as_string(subject):
    """Interpret a string as a boolean and return either 1 or 0.

    Any string value in:

        ('True', 'true', 'On', 'on', '1')

    is interpreted as a boolean True.

    Useful for JSON-decoded stuff and config file parsing
    """
    return bool_from_string(subject) and 1 or 0


def bool_from_string(subject, strict=False, default=False):
    """Interpret a string as a boolean.

    A case-insensitive match is performed such that strings matching 't',
    'true', 'on', 'y', 'yes', or '1' are considered True and, when
    `strict=False`, anything else returns the value specified by 'default'.

    Useful for JSON-decoded stuff and config file parsing.

    If `strict=True`, unrecognized values, including None, will raise a
    ValueError which is useful when parsing values passed in from an API call.
    Strings yielding False are 'f', 'false', 'off', 'n', 'no', or '0'.
    """
    if not isinstance(subject, six.string_types):
        subject = str(subject)

    lowered = subject.strip().lower()

    if lowered in TRUE_STRINGS:
        return True
    elif lowered in FALSE_STRINGS:
        return False
    elif strict:
        acceptable = ', '.join(
            "'%s'" % s for s in sorted(TRUE_STRINGS + FALSE_STRINGS))
        msg = _("Unrecognized value '%(val)s', acceptable values are:"
                " %(acceptable)s") % {'val': subject,
                                      'acceptable': acceptable}
        raise ValueError(msg)
    else:
        return default


def safe_decode(text, incoming=None, errors='strict'):
    """Decodes incoming text/bytes string using `incoming` if they're not
       already unicode.

    :param incoming: Text's current encoding
    :param errors: Errors handling policy. See here for valid
        values http://docs.python.org/2/library/codecs.html
    :returns: text or a unicode `incoming` encoded
                representation of it.
    :raises TypeError: If text is not an instance of str
    """
    if not isinstance(text, (six.string_types, six.binary_type)):
        raise TypeError("%s can't be decoded" % type(text))

    if isinstance(text, six.text_type):
        return text

    if not incoming:
        incoming = (sys.stdin.encoding or
                    sys.getdefaultencoding())

    try:
        return text.decode(incoming, errors)
    except UnicodeDecodeError:
        # Note(flaper87) If we get here, it means that
        # sys.stdin.encoding / sys.getdefaultencoding
        # didn't return a suitable encoding to decode
        # text. This happens mostly when global LANG
        # var is not set correctly and there's no
        # default encoding. In this case, most likely
        # python will use ASCII or ANSI encoders as
        # default encodings but they won't be capable
        # of decoding non-ASCII characters.
        #
        # Also, UTF-8 is being used since it's an ASCII
        # extension.
        return text.decode('utf-8', errors)


def safe_encode(text, incoming=None,
                encoding='utf-8', errors='strict'):
    """Encodes incoming text/bytes string using `encoding`.

    If incoming is not specified, text is expected to be encoded with
    current python's default encoding. (`sys.getdefaultencoding`)

    :param incoming: Text's current encoding
    :param encoding: Expected encoding for text (Default UTF-8)
    :param errors: Errors handling policy. See here for valid
        values http://docs.python.org/2/library/codecs.html
    :returns: text or a bytestring `encoding` encoded
                representation of it.
    :raises TypeError: If text is not an instance of str
    """
    if not isinstance(text, (six.string_types, six.binary_type)):
        raise TypeError("%s can't be encoded" % type(text))

    if not incoming:
        incoming = (sys.stdin.encoding or
                    sys.getdefaultencoding())

    if isinstance(text, six.text_type):
        if six.PY3:
            return text.encode(encoding, errors).decode(incoming)
        else:
            return text.encode(encoding, errors)
    elif text and encoding != incoming:
        # Decode text before encoding it with `encoding`
        text = safe_decode(text, incoming, errors)
        if six.PY3:
            return text.encode(encoding, errors).decode(incoming)
        else:
            return text.encode(encoding, errors)

    return text


def string_to_bytes(text, unit_system='IEC', return_int=False):
    """Converts a string into an float representation of bytes.

    The units supported for IEC ::

        Kb(it), Kib(it), Mb(it), Mib(it), Gb(it), Gib(it), Tb(it), Tib(it)
        KB, KiB, MB, MiB, GB, GiB, TB, TiB

    The units supported for SI ::

        kb(it), Mb(it), Gb(it), Tb(it)
        kB, MB, GB, TB

    Note that the SI unit system does not support capital letter 'K'

    :param text: String input for bytes size conversion.
    :param unit_system: Unit system for byte size conversion.
    :param return_int: If True, returns integer representation of text
                       in bytes. (default: decimal)
    :returns: Numerical representation of text in bytes.
    :raises ValueError: If text has an invalid value.

    """
    try:
        base, reg_ex = UNIT_SYSTEM_INFO[unit_system]
    except KeyError:
        msg = _('Invalid unit system: "%s"') % unit_system
        raise ValueError(msg)
    match = reg_ex.match(text)
    if match:
        magnitude = float(match.group(1))
        unit_prefix = match.group(2)
        if match.group(3) in ['b', 'bit']:
            magnitude /= 8
    else:
        msg = _('Invalid string format: %s') % text
        raise ValueError(msg)
    if not unit_prefix:
        res = magnitude
    else:
        res = magnitude * pow(base, UNIT_PREFIX_EXPONENT[unit_prefix])
    if return_int:
        return int(math.ceil(res))
    return res


def to_slug(value, incoming=None, errors="strict"):
    """Normalize string.

    Convert to lowercase, remove non-word characters, and convert spaces
    to hyphens.

    Inspired by Django's `slugify` filter.

    :param value: Text to slugify
    :param incoming: Text's current encoding
    :param errors: Errors handling policy. See here for valid
        values http://docs.python.org/2/library/codecs.html
    :returns: slugified unicode representation of `value`
    :raises TypeError: If text is not an instance of str
    """
    value = safe_decode(value, incoming, errors)
    # NOTE(aababilov): no need to use safe_(encode|decode) here:
    # encodings are always "ascii", error handling is always "ignore"
    # and types are always known (first: unicode; second: str)
    value = unicodedata.normalize("NFKD", value).encode(
        "ascii", "ignore").decode("ascii")
    value = SLUGIFY_STRIP_RE.sub("", value).strip().lower()
    return SLUGIFY_HYPHENATE_RE.sub("-", value)

########NEW FILE########
__FILENAME__ = systemd
# Copyright 2012-2014 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Helper module for systemd service readiness notification.
"""

import os
import socket
import sys

from designate.openstack.common import log as logging


LOG = logging.getLogger(__name__)


def _abstractify(socket_name):
    if socket_name.startswith('@'):
        # abstract namespace socket
        socket_name = '\0%s' % socket_name[1:]
    return socket_name


def _sd_notify(unset_env, msg):
    notify_socket = os.getenv('NOTIFY_SOCKET')
    if notify_socket:
        sock = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)
        try:
            sock.connect(_abstractify(notify_socket))
            sock.sendall(msg)
            if unset_env:
                del os.environ['NOTIFY_SOCKET']
        except EnvironmentError:
            LOG.debug("Systemd notification failed", exc_info=True)
        finally:
            sock.close()


def notify():
    """Send notification to Systemd that service is ready.
    For details see
      http://www.freedesktop.org/software/systemd/man/sd_notify.html
    """
    _sd_notify(False, 'READY=1')


def notify_once():
    """Send notification once to Systemd that service is ready.
    Systemd sets NOTIFY_SOCKET environment variable with the name of the
    socket listening for notifications from services.
    This method removes the NOTIFY_SOCKET environment variable to ensure
    notification is sent only once.
    """
    _sd_notify(True, 'READY=1')


def onready(notify_socket, timeout):
    """Wait for systemd style notification on the socket.

    :param notify_socket: local socket address
    :type notify_socket:  string
    :param timeout:       socket timeout
    :type timeout:        float
    :returns:             0 service ready
                          1 service not ready
                          2 timeout occured
    """
    sock = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)
    sock.settimeout(timeout)
    sock.bind(_abstractify(notify_socket))
    try:
        msg = sock.recv(512)
    except socket.timeout:
        return 2
    finally:
        sock.close()
    if 'READY=1' in msg:
        return 0
    else:
        return 1


if __name__ == '__main__':
    # simple CLI for testing
    if len(sys.argv) == 1:
        notify()
    elif len(sys.argv) >= 2:
        timeout = float(sys.argv[1])
        notify_socket = os.getenv('NOTIFY_SOCKET')
        if notify_socket:
            retval = onready(notify_socket, timeout)
            sys.exit(retval)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

##############################################################################
##############################################################################
##
## DO NOT MODIFY THIS FILE
##
## This file is being graduated to the designatetest library. Please make all
## changes there, and only backport critical fixes here. - dhellmann
##
##############################################################################
##############################################################################

"""Common utilities used in testing"""

import logging
import os
import tempfile

import fixtures
import testtools

_TRUE_VALUES = ('True', 'true', '1', 'yes')
_LOG_FORMAT = "%(levelname)8s [%(name)s] %(message)s"


class BaseTestCase(testtools.TestCase):

    def setUp(self):
        super(BaseTestCase, self).setUp()
        self._set_timeout()
        self._fake_output()
        self._fake_logs()
        self.useFixture(fixtures.NestedTempfile())
        self.useFixture(fixtures.TempHomeDir())
        self.tempdirs = []

    def _set_timeout(self):
        test_timeout = os.environ.get('OS_TEST_TIMEOUT', 0)
        try:
            test_timeout = int(test_timeout)
        except ValueError:
            # If timeout value is invalid do not set a timeout.
            test_timeout = 0
        if test_timeout > 0:
            self.useFixture(fixtures.Timeout(test_timeout, gentle=True))

    def _fake_output(self):
        if os.environ.get('OS_STDOUT_CAPTURE') in _TRUE_VALUES:
            stdout = self.useFixture(fixtures.StringStream('stdout')).stream
            self.useFixture(fixtures.MonkeyPatch('sys.stdout', stdout))
        if os.environ.get('OS_STDERR_CAPTURE') in _TRUE_VALUES:
            stderr = self.useFixture(fixtures.StringStream('stderr')).stream
            self.useFixture(fixtures.MonkeyPatch('sys.stderr', stderr))

    def _fake_logs(self):
        if os.environ.get('OS_DEBUG') in _TRUE_VALUES:
            level = logging.DEBUG
        else:
            level = logging.INFO
        capture_logs = os.environ.get('OS_LOG_CAPTURE') in _TRUE_VALUES
        if capture_logs:
            self.useFixture(
                fixtures.FakeLogger(
                    format=_LOG_FORMAT,
                    level=level,
                    nuke_handlers=capture_logs,
                )
            )
        else:
            logging.basicConfig(format=_LOG_FORMAT, level=level)

    def create_tempfiles(self, files, ext='.conf'):
        tempfiles = []
        for (basename, contents) in files:
            if not os.path.isabs(basename):
                (fd, path) = tempfile.mkstemp(prefix=basename, suffix=ext)
            else:
                path = basename + ext
                fd = os.open(path, os.O_CREAT | os.O_WRONLY)
            tempfiles.append(path)
            try:
                os.write(fd, contents)
            finally:
                os.close(fd)
        return tempfiles

########NEW FILE########
__FILENAME__ = threadgroup
# Copyright 2012 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
import threading

import eventlet
from eventlet import greenpool

from designate.openstack.common import log as logging
from designate.openstack.common import loopingcall


LOG = logging.getLogger(__name__)


def _thread_done(gt, *args, **kwargs):
    """Callback function to be passed to GreenThread.link() when we spawn()
    Calls the :class:`ThreadGroup` to notify if.

    """
    kwargs['group'].thread_done(kwargs['thread'])


class Thread(object):
    """Wrapper around a greenthread, that holds a reference to the
    :class:`ThreadGroup`. The Thread will notify the :class:`ThreadGroup` when
    it has done so it can be removed from the threads list.
    """
    def __init__(self, thread, group):
        self.thread = thread
        self.thread.link(_thread_done, group=group, thread=self)

    def stop(self):
        self.thread.kill()

    def wait(self):
        return self.thread.wait()

    def link(self, func, *args, **kwargs):
        self.thread.link(func, *args, **kwargs)


class ThreadGroup(object):
    """The point of the ThreadGroup class is to:

    * keep track of timers and greenthreads (making it easier to stop them
      when need be).
    * provide an easy API to add timers.
    """
    def __init__(self, thread_pool_size=10):
        self.pool = greenpool.GreenPool(thread_pool_size)
        self.threads = []
        self.timers = []

    def add_dynamic_timer(self, callback, initial_delay=None,
                          periodic_interval_max=None, *args, **kwargs):
        timer = loopingcall.DynamicLoopingCall(callback, *args, **kwargs)
        timer.start(initial_delay=initial_delay,
                    periodic_interval_max=periodic_interval_max)
        self.timers.append(timer)

    def add_timer(self, interval, callback, initial_delay=None,
                  *args, **kwargs):
        pulse = loopingcall.FixedIntervalLoopingCall(callback, *args, **kwargs)
        pulse.start(interval=interval,
                    initial_delay=initial_delay)
        self.timers.append(pulse)

    def add_thread(self, callback, *args, **kwargs):
        gt = self.pool.spawn(callback, *args, **kwargs)
        th = Thread(gt, self)
        self.threads.append(th)
        return th

    def thread_done(self, thread):
        self.threads.remove(thread)

    def stop(self):
        current = threading.current_thread()

        # Iterate over a copy of self.threads so thread_done doesn't
        # modify the list while we're iterating
        for x in self.threads[:]:
            if x is current:
                # don't kill the current thread.
                continue
            try:
                x.stop()
            except Exception as ex:
                LOG.exception(ex)

        for x in self.timers:
            try:
                x.stop()
            except Exception as ex:
                LOG.exception(ex)
        self.timers = []

    def wait(self):
        for x in self.timers:
            try:
                x.wait()
            except eventlet.greenlet.GreenletExit:
                pass
            except Exception as ex:
                LOG.exception(ex)
        current = threading.current_thread()

        # Iterate over a copy of self.threads so thread_done doesn't
        # modify the list while we're iterating
        for x in self.threads[:]:
            if x is current:
                continue
            try:
                x.wait()
            except eventlet.greenlet.GreenletExit:
                pass
            except Exception as ex:
                LOG.exception(ex)

########NEW FILE########
__FILENAME__ = timeutils
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Time related utilities and helper functions.
"""

import calendar
import datetime
import time

import iso8601
import six


# ISO 8601 extended time format with microseconds
_ISO8601_TIME_FORMAT_SUBSECOND = '%Y-%m-%dT%H:%M:%S.%f'
_ISO8601_TIME_FORMAT = '%Y-%m-%dT%H:%M:%S'
PERFECT_TIME_FORMAT = _ISO8601_TIME_FORMAT_SUBSECOND


def isotime(at=None, subsecond=False):
    """Stringify time in ISO 8601 format."""
    if not at:
        at = utcnow()
    st = at.strftime(_ISO8601_TIME_FORMAT
                     if not subsecond
                     else _ISO8601_TIME_FORMAT_SUBSECOND)
    tz = at.tzinfo.tzname(None) if at.tzinfo else 'UTC'
    st += ('Z' if tz == 'UTC' else tz)
    return st


def parse_isotime(timestr):
    """Parse time from ISO 8601 format."""
    try:
        return iso8601.parse_date(timestr)
    except iso8601.ParseError as e:
        raise ValueError(six.text_type(e))
    except TypeError as e:
        raise ValueError(six.text_type(e))


def strtime(at=None, fmt=PERFECT_TIME_FORMAT):
    """Returns formatted utcnow."""
    if not at:
        at = utcnow()
    return at.strftime(fmt)


def parse_strtime(timestr, fmt=PERFECT_TIME_FORMAT):
    """Turn a formatted time back into a datetime."""
    return datetime.datetime.strptime(timestr, fmt)


def normalize_time(timestamp):
    """Normalize time in arbitrary timezone to UTC naive object."""
    offset = timestamp.utcoffset()
    if offset is None:
        return timestamp
    return timestamp.replace(tzinfo=None) - offset


def is_older_than(before, seconds):
    """Return True if before is older than seconds."""
    if isinstance(before, six.string_types):
        before = parse_strtime(before).replace(tzinfo=None)
    else:
        before = before.replace(tzinfo=None)

    return utcnow() - before > datetime.timedelta(seconds=seconds)


def is_newer_than(after, seconds):
    """Return True if after is newer than seconds."""
    if isinstance(after, six.string_types):
        after = parse_strtime(after).replace(tzinfo=None)
    else:
        after = after.replace(tzinfo=None)

    return after - utcnow() > datetime.timedelta(seconds=seconds)


def utcnow_ts():
    """Timestamp version of our utcnow function."""
    if utcnow.override_time is None:
        # NOTE(kgriffs): This is several times faster
        # than going through calendar.timegm(...)
        return int(time.time())

    return calendar.timegm(utcnow().timetuple())


def utcnow():
    """Overridable version of utils.utcnow."""
    if utcnow.override_time:
        try:
            return utcnow.override_time.pop(0)
        except AttributeError:
            return utcnow.override_time
    return datetime.datetime.utcnow()


def iso8601_from_timestamp(timestamp):
    """Returns a iso8601 formatted date from timestamp."""
    return isotime(datetime.datetime.utcfromtimestamp(timestamp))


utcnow.override_time = None


def set_time_override(override_time=None):
    """Overrides utils.utcnow.

    Make it return a constant time or a list thereof, one at a time.

    :param override_time: datetime instance or list thereof. If not
                          given, defaults to the current UTC time.
    """
    utcnow.override_time = override_time or datetime.datetime.utcnow()


def advance_time_delta(timedelta):
    """Advance overridden time using a datetime.timedelta."""
    assert(not utcnow.override_time is None)
    try:
        for dt in utcnow.override_time:
            dt += timedelta
    except TypeError:
        utcnow.override_time += timedelta


def advance_time_seconds(seconds):
    """Advance overridden time by seconds."""
    advance_time_delta(datetime.timedelta(0, seconds))


def clear_time_override():
    """Remove the overridden time."""
    utcnow.override_time = None


def marshall_now(now=None):
    """Make an rpc-safe datetime with microseconds.

    Note: tzinfo is stripped, but not required for relative times.
    """
    if not now:
        now = utcnow()
    return dict(day=now.day, month=now.month, year=now.year, hour=now.hour,
                minute=now.minute, second=now.second,
                microsecond=now.microsecond)


def unmarshall_time(tyme):
    """Unmarshall a datetime dict."""
    return datetime.datetime(day=tyme['day'],
                             month=tyme['month'],
                             year=tyme['year'],
                             hour=tyme['hour'],
                             minute=tyme['minute'],
                             second=tyme['second'],
                             microsecond=tyme['microsecond'])


def delta_seconds(before, after):
    """Return the difference between two timing objects.

    Compute the difference in seconds between two date, time, or
    datetime objects (as a float, to microsecond resolution).
    """
    delta = after - before
    return total_seconds(delta)


def total_seconds(delta):
    """Return the total seconds of datetime.timedelta object.

    Compute total seconds of datetime.timedelta, datetime.timedelta
    doesn't have method total_seconds in Python2.6, calculate it manually.
    """
    try:
        return delta.total_seconds()
    except AttributeError:
        return ((delta.days * 24 * 3600) + delta.seconds +
                float(delta.microseconds) / (10 ** 6))


def is_soon(dt, window):
    """Determines if time is going to happen in the next window seconds.

    :param dt: the time
    :param window: minimum seconds to remain to consider the time not soon

    :return: True if expiration is within the given duration
    """
    soon = (utcnow() + datetime.timedelta(seconds=window))
    return normalize_time(dt) <= soon

########NEW FILE########
__FILENAME__ = versionutils
# Copyright (c) 2013 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Helpers for comparing version strings.
"""

import functools
import pkg_resources

from designate.openstack.common.gettextutils import _
from designate.openstack.common import log as logging


LOG = logging.getLogger(__name__)


class deprecated(object):
    """A decorator to mark callables as deprecated.

    This decorator logs a deprecation message when the callable it decorates is
    used. The message will include the release where the callable was
    deprecated, the release where it may be removed and possibly an optional
    replacement.

    Examples:

    1. Specifying the required deprecated release

    >>> @deprecated(as_of=deprecated.ICEHOUSE)
    ... def a(): pass

    2. Specifying a replacement:

    >>> @deprecated(as_of=deprecated.ICEHOUSE, in_favor_of='f()')
    ... def b(): pass

    3. Specifying the release where the functionality may be removed:

    >>> @deprecated(as_of=deprecated.ICEHOUSE, remove_in=+1)
    ... def c(): pass

    """

    FOLSOM = 'F'
    GRIZZLY = 'G'
    HAVANA = 'H'
    ICEHOUSE = 'I'

    _RELEASES = {
        'F': 'Folsom',
        'G': 'Grizzly',
        'H': 'Havana',
        'I': 'Icehouse',
    }

    _deprecated_msg_with_alternative = _(
        '%(what)s is deprecated as of %(as_of)s in favor of '
        '%(in_favor_of)s and may be removed in %(remove_in)s.')

    _deprecated_msg_no_alternative = _(
        '%(what)s is deprecated as of %(as_of)s and may be '
        'removed in %(remove_in)s. It will not be superseded.')

    def __init__(self, as_of, in_favor_of=None, remove_in=2, what=None):
        """Initialize decorator

        :param as_of: the release deprecating the callable. Constants
            are define in this class for convenience.
        :param in_favor_of: the replacement for the callable (optional)
        :param remove_in: an integer specifying how many releases to wait
            before removing (default: 2)
        :param what: name of the thing being deprecated (default: the
            callable's name)

        """
        self.as_of = as_of
        self.in_favor_of = in_favor_of
        self.remove_in = remove_in
        self.what = what

    def __call__(self, func):
        if not self.what:
            self.what = func.__name__ + '()'

        @functools.wraps(func)
        def wrapped(*args, **kwargs):
            msg, details = self._build_message()
            LOG.deprecated(msg, details)
            return func(*args, **kwargs)
        return wrapped

    def _get_safe_to_remove_release(self, release):
        # TODO(dstanek): this method will have to be reimplemented once
        #    when we get to the X release because once we get to the Y
        #    release, what is Y+2?
        new_release = chr(ord(release) + self.remove_in)
        if new_release in self._RELEASES:
            return self._RELEASES[new_release]
        else:
            return new_release

    def _build_message(self):
        details = dict(what=self.what,
                       as_of=self._RELEASES[self.as_of],
                       remove_in=self._get_safe_to_remove_release(self.as_of))

        if self.in_favor_of:
            details['in_favor_of'] = self.in_favor_of
            msg = self._deprecated_msg_with_alternative
        else:
            msg = self._deprecated_msg_no_alternative
        return msg, details


def is_compatible(requested_version, current_version, same_major=True):
    """Determine whether `requested_version` is satisfied by
    `current_version`; in other words, `current_version` is >=
    `requested_version`.

    :param requested_version: version to check for compatibility
    :param current_version: version to check against
    :param same_major: if True, the major version must be identical between
        `requested_version` and `current_version`. This is used when a
        major-version difference indicates incompatibility between the two
        versions. Since this is the common-case in practice, the default is
        True.
    :returns: True if compatible, False if not
    """
    requested_parts = pkg_resources.parse_version(requested_version)
    current_parts = pkg_resources.parse_version(current_version)

    if same_major and (requested_parts[0] != current_parts[0]):
        return False

    return current_parts >= requested_parts

########NEW FILE########
__FILENAME__ = xmlutils
# Copyright 2013 IBM Corp.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from xml.dom import minidom
from xml.parsers import expat
from xml import sax
from xml.sax import expatreader


class ProtectedExpatParser(expatreader.ExpatParser):
    """An expat parser which disables DTD's and entities by default."""

    def __init__(self, forbid_dtd=True, forbid_entities=True,
                 *args, **kwargs):
        # Python 2.x old style class
        expatreader.ExpatParser.__init__(self, *args, **kwargs)
        self.forbid_dtd = forbid_dtd
        self.forbid_entities = forbid_entities

    def start_doctype_decl(self, name, sysid, pubid, has_internal_subset):
        raise ValueError("Inline DTD forbidden")

    def entity_decl(self, entityName, is_parameter_entity, value, base,
                    systemId, publicId, notationName):
        raise ValueError("<!ENTITY> entity declaration forbidden")

    def unparsed_entity_decl(self, name, base, sysid, pubid, notation_name):
        # expat 1.2
        raise ValueError("<!ENTITY> unparsed entity forbidden")

    def external_entity_ref(self, context, base, systemId, publicId):
        raise ValueError("<!ENTITY> external entity forbidden")

    def notation_decl(self, name, base, sysid, pubid):
        raise ValueError("<!ENTITY> notation forbidden")

    def reset(self):
        expatreader.ExpatParser.reset(self)
        if self.forbid_dtd:
            self._parser.StartDoctypeDeclHandler = self.start_doctype_decl
            self._parser.EndDoctypeDeclHandler = None
        if self.forbid_entities:
            self._parser.EntityDeclHandler = self.entity_decl
            self._parser.UnparsedEntityDeclHandler = self.unparsed_entity_decl
            self._parser.ExternalEntityRefHandler = self.external_entity_ref
            self._parser.NotationDeclHandler = self.notation_decl
            try:
                self._parser.SkippedEntityHandler = None
            except AttributeError:
                # some pyexpat versions do not support SkippedEntity
                pass


def safe_minidom_parse_string(xml_string):
    """Parse an XML string using minidom safely.

    """
    try:
        return minidom.parseString(xml_string, parser=ProtectedExpatParser())
    except sax.SAXParseException:
        raise expat.ExpatError()

########NEW FILE########
__FILENAME__ = exception
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Exceptions common to OpenStack projects
"""

import logging

from designate.openstack.common.gettextutils import _

_FATAL_EXCEPTION_FORMAT_ERRORS = False


class Error(Exception):
    def __init__(self, message=None):
        super(Error, self).__init__(message)


class ApiError(Error):
    def __init__(self, message='Unknown', code='Unknown'):
        self.message = message
        self.code = code
        super(ApiError, self).__init__('%s: %s' % (code, message))


class NotFound(Error):
    pass


class UnknownScheme(Error):

    msg = "Unknown scheme '%s' found in URI"

    def __init__(self, scheme):
        msg = self.__class__.msg % scheme
        super(UnknownScheme, self).__init__(msg)


class BadStoreUri(Error):

    msg = "The Store URI %s was malformed. Reason: %s"

    def __init__(self, uri, reason):
        msg = self.__class__.msg % (uri, reason)
        super(BadStoreUri, self).__init__(msg)


class Duplicate(Error):
    pass


class NotAuthorized(Error):
    pass


class NotEmpty(Error):
    pass


class Invalid(Error):
    pass


class BadInputError(Exception):
    """Error resulting from a client sending bad input to a server"""
    pass


class MissingArgumentError(Error):
    pass


class DatabaseMigrationError(Error):
    pass


class ClientConnectionError(Exception):
    """Error resulting from a client connecting to a server"""
    pass


def wrap_exception(f):
    def _wrap(*args, **kw):
        try:
            return f(*args, **kw)
        except Exception, e:
            if not isinstance(e, Error):
                #exc_type, exc_value, exc_traceback = sys.exc_info()
                logging.exception(_('Uncaught exception'))
                #logging.error(traceback.extract_stack(exc_traceback))
                raise Error(str(e))
            raise
    _wrap.func_name = f.func_name
    return _wrap


class OpenstackException(Exception):
    """
    Base Exception

    To correctly use this class, inherit from it and define
    a 'message' property. That message will get printf'd
    with the keyword arguments provided to the constructor.
    """
    message = "An unknown exception occurred"

    def __init__(self, **kwargs):
        try:
            self._error_string = self.message % kwargs

        except Exception as e:
            if _FATAL_EXCEPTION_FORMAT_ERRORS:
                raise e
            else:
                # at least get the core message out if something happened
                self._error_string = self.message

    def __str__(self):
        return self._error_string


class MalformedRequestBody(OpenstackException):
    message = "Malformed message body: %(reason)s"


class InvalidContentType(OpenstackException):
    message = "Invalid content type %(content_type)s"

########NEW FILE########
__FILENAME__ = wsgi
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Utility methods for working with WSGI servers."""

import eventlet
eventlet.patcher.monkey_patch(all=False, socket=True)

import datetime
import errno
import socket
import sys
import time

import eventlet.wsgi
from oslo.config import cfg
import routes
import routes.middleware
import webob.dec
import webob.exc
from xml.dom import minidom
from xml.parsers import expat

from designate.openstack.common.gettextutils import _
from designate.openstack.common import jsonutils
from designate.openstack.common import log as logging
from designate.openstack.common import service
from designate.openstack.common import sslutils
from designate.openstack.common import xmlutils
from designate.openstack.deprecated import exception

socket_opts = [
    cfg.IntOpt('backlog',
               default=4096,
               help="Number of backlog requests to configure the socket with"),
    cfg.IntOpt('tcp_keepidle',
               default=600,
               help="Sets the value of TCP_KEEPIDLE in seconds for each "
                    "server socket. Not supported on OS X."),
]

CONF = cfg.CONF
CONF.register_opts(socket_opts)

LOG = logging.getLogger(__name__)


def run_server(application, port):
    """Run a WSGI server with the given application."""
    sock = eventlet.listen(('0.0.0.0', port))
    eventlet.wsgi.server(sock, application)


class Service(service.Service):
    """
    Provides a Service API for wsgi servers.

    This gives us the ability to launch wsgi servers with the
    Launcher classes in service.py.
    """

    def __init__(self, application, port,
                 host='0.0.0.0', backlog=4096, threads=1000):
        self.application = application
        self._port = port
        self._host = host
        self._backlog = backlog if backlog else CONF.backlog
        super(Service, self).__init__(threads)

    def _get_socket(self, host, port, backlog):
        # TODO(dims): eventlet's green dns/socket module does not actually
        # support IPv6 in getaddrinfo(). We need to get around this in the
        # future or monitor upstream for a fix
        info = socket.getaddrinfo(host,
                                  port,
                                  socket.AF_UNSPEC,
                                  socket.SOCK_STREAM)[0]
        family = info[0]
        bind_addr = info[-1]

        sock = None
        retry_until = time.time() + 30
        while not sock and time.time() < retry_until:
            try:
                sock = eventlet.listen(bind_addr,
                                       backlog=backlog,
                                       family=family)
                if sslutils.is_enabled():
                    sock = sslutils.wrap(sock)

            except socket.error, err:
                if err.args[0] != errno.EADDRINUSE:
                    raise
                eventlet.sleep(0.1)
        if not sock:
            raise RuntimeError(_("Could not bind to %(host)s:%(port)s "
                               "after trying for 30 seconds") %
                               {'host': host, 'port': port})
        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        # sockets can hang around forever without keepalive
        sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)

        # This option isn't available in the OS X version of eventlet
        if hasattr(socket, 'TCP_KEEPIDLE'):
            sock.setsockopt(socket.IPPROTO_TCP,
                            socket.TCP_KEEPIDLE,
                            CONF.tcp_keepidle)

        return sock

    def start(self):
        """Start serving this service using the provided server instance.

        :returns: None

        """
        super(Service, self).start()
        self._socket = self._get_socket(self._host, self._port, self._backlog)
        self.tg.add_thread(self._run, self.application, self._socket)

    @property
    def backlog(self):
        return self._backlog

    @property
    def host(self):
        return self._socket.getsockname()[0] if self._socket else self._host

    @property
    def port(self):
        return self._socket.getsockname()[1] if self._socket else self._port

    def stop(self):
        """Stop serving this API.

        :returns: None

        """
        super(Service, self).stop()

    def _run(self, application, socket):
        """Start a WSGI server in a new green thread."""
        logger = logging.getLogger('eventlet.wsgi')
        eventlet.wsgi.server(socket,
                             application,
                             custom_pool=self.tg.pool,
                             log=logging.WritableLogger(logger))


class Middleware(object):
    """
    Base WSGI middleware wrapper. These classes require an application to be
    initialized that will be called next.  By default the middleware will
    simply call its wrapped app, or you can override __call__ to customize its
    behavior.
    """

    def __init__(self, application):
        self.application = application

    def process_request(self, req):
        """
        Called on each request.

        If this returns None, the next application down the stack will be
        executed. If it returns a response then that response will be returned
        and execution will stop here.
        """
        return None

    def process_response(self, response):
        """Do whatever you'd like to the response."""
        return response

    @webob.dec.wsgify
    def __call__(self, req):
        response = self.process_request(req)
        if response:
            return response
        response = req.get_response(self.application)
        return self.process_response(response)


class Debug(Middleware):
    """
    Helper class that can be inserted into any WSGI application chain
    to get information about the request and response.
    """

    @webob.dec.wsgify
    def __call__(self, req):
        print ("*" * 40) + " REQUEST ENVIRON"
        for key, value in req.environ.items():
            print key, "=", value
        print
        resp = req.get_response(self.application)

        print ("*" * 40) + " RESPONSE HEADERS"
        for (key, value) in resp.headers.iteritems():
            print key, "=", value
        print

        resp.app_iter = self.print_generator(resp.app_iter)

        return resp

    @staticmethod
    def print_generator(app_iter):
        """
        Iterator that prints the contents of a wrapper string iterator
        when iterated.
        """
        print ("*" * 40) + " BODY"
        for part in app_iter:
            sys.stdout.write(part)
            sys.stdout.flush()
            yield part
        print


class Router(object):

    """
    WSGI middleware that maps incoming requests to WSGI apps.
    """

    def __init__(self, mapper):
        """
        Create a router for the given routes.Mapper.

        Each route in `mapper` must specify a 'controller', which is a
        WSGI app to call.  You'll probably want to specify an 'action' as
        well and have your controller be a wsgi.Controller, who will route
        the request to the action method.

        Examples:
          mapper = routes.Mapper()
          sc = ServerController()

          # Explicit mapping of one route to a controller+action
          mapper.connect(None, "/svrlist", controller=sc, action="list")

          # Actions are all implicitly defined
          mapper.resource("server", "servers", controller=sc)

          # Pointing to an arbitrary WSGI app.  You can specify the
          # {path_info:.*} parameter so the target app can be handed just that
          # section of the URL.
          mapper.connect(None, "/v1.0/{path_info:.*}", controller=BlogApp())
        """
        self.map = mapper
        self._router = routes.middleware.RoutesMiddleware(self._dispatch,
                                                          self.map)

    @webob.dec.wsgify
    def __call__(self, req):
        """
        Route the incoming request to a controller based on self.map.
        If no match, return a 404.
        """
        return self._router

    @staticmethod
    @webob.dec.wsgify
    def _dispatch(req):
        """
        Called by self._router after matching the incoming request to a route
        and putting the information into req.environ.  Either returns 404
        or the routed WSGI app's response.
        """
        match = req.environ['wsgiorg.routing_args'][1]
        if not match:
            return webob.exc.HTTPNotFound()
        app = match['controller']
        return app


class Request(webob.Request):
    """Add some Openstack API-specific logic to the base webob.Request."""

    default_request_content_types = ('application/json', 'application/xml')
    default_accept_types = ('application/json', 'application/xml')
    default_accept_type = 'application/json'

    def best_match_content_type(self, supported_content_types=None):
        """Determine the requested response content-type.

        Based on the query extension then the Accept header.
        Defaults to default_accept_type if we don't find a preference

        """
        supported_content_types = (supported_content_types or
                                   self.default_accept_types)

        parts = self.path.rsplit('.', 1)
        if len(parts) > 1:
            ctype = 'application/{0}'.format(parts[1])
            if ctype in supported_content_types:
                return ctype

        bm = self.accept.best_match(supported_content_types)
        return bm or self.default_accept_type

    def get_content_type(self, allowed_content_types=None):
        """Determine content type of the request body.

        Does not do any body introspection, only checks header

        """
        if "Content-Type" not in self.headers:
            return None

        content_type = self.content_type
        allowed_content_types = (allowed_content_types or
                                 self.default_request_content_types)

        if content_type not in allowed_content_types:
            raise exception.InvalidContentType(content_type=content_type)
        return content_type


class Resource(object):
    """
    WSGI app that handles (de)serialization and controller dispatch.

    Reads routing information supplied by RoutesMiddleware and calls
    the requested action method upon its deserializer, controller,
    and serializer. Those three objects may implement any of the basic
    controller action methods (create, update, show, index, delete)
    along with any that may be specified in the api router. A 'default'
    method may also be implemented to be used in place of any
    non-implemented actions. Deserializer methods must accept a request
    argument and return a dictionary. Controller methods must accept a
    request argument. Additionally, they must also accept keyword
    arguments that represent the keys returned by the Deserializer. They
    may raise a webob.exc exception or return a dict, which will be
    serialized by requested content type.
    """
    def __init__(self, controller, deserializer=None, serializer=None):
        """
        :param controller: object that implement methods created by routes lib
        :param deserializer: object that supports webob request deserialization
                             through controller-like actions
        :param serializer: object that supports webob response serialization
                           through controller-like actions
        """
        self.controller = controller
        self.serializer = serializer or ResponseSerializer()
        self.deserializer = deserializer or RequestDeserializer()

    @webob.dec.wsgify(RequestClass=Request)
    def __call__(self, request):
        """WSGI method that controls (de)serialization and method dispatch."""

        try:
            action, action_args, accept = self.deserialize_request(request)
        except exception.InvalidContentType:
            msg = _("Unsupported Content-Type")
            return webob.exc.HTTPUnsupportedMediaType(explanation=msg)
        except exception.MalformedRequestBody:
            msg = _("Malformed request body")
            return webob.exc.HTTPBadRequest(explanation=msg)

        action_result = self.execute_action(action, request, **action_args)
        try:
            return self.serialize_response(action, action_result, accept)
        # return unserializable result (typically a webob exc)
        except Exception:
            return action_result

    def deserialize_request(self, request):
        return self.deserializer.deserialize(request)

    def serialize_response(self, action, action_result, accept):
        return self.serializer.serialize(action_result, accept, action)

    def execute_action(self, action, request, **action_args):
        return self.dispatch(self.controller, action, request, **action_args)

    def dispatch(self, obj, action, *args, **kwargs):
        """Find action-specific method on self and call it."""
        try:
            method = getattr(obj, action)
        except AttributeError:
            method = getattr(obj, 'default')

        return method(*args, **kwargs)

    def get_action_args(self, request_environment):
        """Parse dictionary created by routes library."""
        try:
            args = request_environment['wsgiorg.routing_args'][1].copy()
        except Exception:
            return {}

        try:
            del args['controller']
        except KeyError:
            pass

        try:
            del args['format']
        except KeyError:
            pass

        return args


class ActionDispatcher(object):
    """Maps method name to local methods through action name."""

    def dispatch(self, *args, **kwargs):
        """Find and call local method."""
        action = kwargs.pop('action', 'default')
        action_method = getattr(self, str(action), self.default)
        return action_method(*args, **kwargs)

    def default(self, data):
        raise NotImplementedError()


class DictSerializer(ActionDispatcher):
    """Default request body serialization"""

    def serialize(self, data, action='default'):
        return self.dispatch(data, action=action)

    def default(self, data):
        return ""


class JSONDictSerializer(DictSerializer):
    """Default JSON request body serialization"""

    def default(self, data):
        def sanitizer(obj):
            if isinstance(obj, datetime.datetime):
                _dtime = obj - datetime.timedelta(microseconds=obj.microsecond)
                return _dtime.isoformat()
            return unicode(obj)
        return jsonutils.dumps(data, default=sanitizer)


class XMLDictSerializer(DictSerializer):

    def __init__(self, metadata=None, xmlns=None):
        """
        :param metadata: information needed to deserialize xml into
                         a dictionary.
        :param xmlns: XML namespace to include with serialized xml
        """
        super(XMLDictSerializer, self).__init__()
        self.metadata = metadata or {}
        self.xmlns = xmlns

    def default(self, data):
        # We expect data to contain a single key which is the XML root.
        root_key = data.keys()[0]
        doc = minidom.Document()
        node = self._to_xml_node(doc, self.metadata, root_key, data[root_key])

        return self.to_xml_string(node)

    def to_xml_string(self, node, has_atom=False):
        self._add_xmlns(node, has_atom)
        return node.toprettyxml(indent='    ', encoding='UTF-8')

    #NOTE (ameade): the has_atom should be removed after all of the
    # xml serializers and view builders have been updated to the current
    # spec that required all responses include the xmlns:atom, the has_atom
    # flag is to prevent current tests from breaking
    def _add_xmlns(self, node, has_atom=False):
        if self.xmlns is not None:
            node.setAttribute('xmlns', self.xmlns)
        if has_atom:
            node.setAttribute('xmlns:atom', "http://www.w3.org/2005/Atom")

    def _to_xml_node(self, doc, metadata, nodename, data):
        """Recursive method to convert data members to XML nodes."""
        result = doc.createElement(nodename)

        # Set the xml namespace if one is specified
        # TODO(justinsb): We could also use prefixes on the keys
        xmlns = metadata.get('xmlns', None)
        if xmlns:
            result.setAttribute('xmlns', xmlns)

        #TODO(bcwaldon): accomplish this without a type-check
        if type(data) is list:
            collections = metadata.get('list_collections', {})
            if nodename in collections:
                metadata = collections[nodename]
                for item in data:
                    node = doc.createElement(metadata['item_name'])
                    node.setAttribute(metadata['item_key'], str(item))
                    result.appendChild(node)
                return result
            singular = metadata.get('plurals', {}).get(nodename, None)
            if singular is None:
                if nodename.endswith('s'):
                    singular = nodename[:-1]
                else:
                    singular = 'item'
            for item in data:
                node = self._to_xml_node(doc, metadata, singular, item)
                result.appendChild(node)
        #TODO(bcwaldon): accomplish this without a type-check
        elif type(data) is dict:
            collections = metadata.get('dict_collections', {})
            if nodename in collections:
                metadata = collections[nodename]
                for k, v in data.items():
                    node = doc.createElement(metadata['item_name'])
                    node.setAttribute(metadata['item_key'], str(k))
                    text = doc.createTextNode(str(v))
                    node.appendChild(text)
                    result.appendChild(node)
                return result
            attrs = metadata.get('attributes', {}).get(nodename, {})
            for k, v in data.items():
                if k in attrs:
                    result.setAttribute(k, str(v))
                else:
                    node = self._to_xml_node(doc, metadata, k, v)
                    result.appendChild(node)
        else:
            # Type is atom
            node = doc.createTextNode(str(data))
            result.appendChild(node)
        return result

    def _create_link_nodes(self, xml_doc, links):
        link_nodes = []
        for link in links:
            link_node = xml_doc.createElement('atom:link')
            link_node.setAttribute('rel', link['rel'])
            link_node.setAttribute('href', link['href'])
            if 'type' in link:
                link_node.setAttribute('type', link['type'])
            link_nodes.append(link_node)
        return link_nodes


class ResponseHeadersSerializer(ActionDispatcher):
    """Default response headers serialization"""

    def serialize(self, response, data, action):
        self.dispatch(response, data, action=action)

    def default(self, response, data):
        response.status_int = 200


class ResponseSerializer(object):
    """Encode the necessary pieces into a response object"""

    def __init__(self, body_serializers=None, headers_serializer=None):
        self.body_serializers = {
            'application/xml': XMLDictSerializer(),
            'application/json': JSONDictSerializer(),
        }
        self.body_serializers.update(body_serializers or {})

        self.headers_serializer = (headers_serializer or
                                   ResponseHeadersSerializer())

    def serialize(self, response_data, content_type, action='default'):
        """Serialize a dict into a string and wrap in a wsgi.Request object.

        :param response_data: dict produced by the Controller
        :param content_type: expected mimetype of serialized response body

        """
        response = webob.Response()
        self.serialize_headers(response, response_data, action)
        self.serialize_body(response, response_data, content_type, action)
        return response

    def serialize_headers(self, response, data, action):
        self.headers_serializer.serialize(response, data, action)

    def serialize_body(self, response, data, content_type, action):
        response.headers['Content-Type'] = content_type
        if data is not None:
            serializer = self.get_body_serializer(content_type)
            response.body = serializer.serialize(data, action)

    def get_body_serializer(self, content_type):
        try:
            return self.body_serializers[content_type]
        except (KeyError, TypeError):
            raise exception.InvalidContentType(content_type=content_type)


class RequestHeadersDeserializer(ActionDispatcher):
    """Default request headers deserializer"""

    def deserialize(self, request, action):
        return self.dispatch(request, action=action)

    def default(self, request):
        return {}


class RequestDeserializer(object):
    """Break up a Request object into more useful pieces."""

    def __init__(self, body_deserializers=None, headers_deserializer=None,
                 supported_content_types=None):

        self.supported_content_types = supported_content_types

        self.body_deserializers = {
            'application/xml': XMLDeserializer(),
            'application/json': JSONDeserializer(),
        }
        self.body_deserializers.update(body_deserializers or {})

        self.headers_deserializer = (headers_deserializer or
                                     RequestHeadersDeserializer())

    def deserialize(self, request):
        """Extract necessary pieces of the request.

        :param request: Request object
        :returns: tuple of (expected controller action name, dictionary of
                  keyword arguments to pass to the controller, the expected
                  content type of the response)

        """
        action_args = self.get_action_args(request.environ)
        action = action_args.pop('action', None)

        action_args.update(self.deserialize_headers(request, action))
        action_args.update(self.deserialize_body(request, action))

        accept = self.get_expected_content_type(request)

        return (action, action_args, accept)

    def deserialize_headers(self, request, action):
        return self.headers_deserializer.deserialize(request, action)

    def deserialize_body(self, request, action):
        if not len(request.body) > 0:
            LOG.debug(_("Empty body provided in request"))
            return {}

        try:
            content_type = request.get_content_type()
        except exception.InvalidContentType:
            LOG.debug(_("Unrecognized Content-Type provided in request"))
            raise

        if content_type is None:
            LOG.debug(_("No Content-Type provided in request"))
            return {}

        try:
            deserializer = self.get_body_deserializer(content_type)
        except exception.InvalidContentType:
            LOG.debug(_("Unable to deserialize body as provided Content-Type"))
            raise

        return deserializer.deserialize(request.body, action)

    def get_body_deserializer(self, content_type):
        try:
            return self.body_deserializers[content_type]
        except (KeyError, TypeError):
            raise exception.InvalidContentType(content_type=content_type)

    def get_expected_content_type(self, request):
        return request.best_match_content_type(self.supported_content_types)

    def get_action_args(self, request_environment):
        """Parse dictionary created by routes library."""
        try:
            args = request_environment['wsgiorg.routing_args'][1].copy()
        except Exception:
            return {}

        try:
            del args['controller']
        except KeyError:
            pass

        try:
            del args['format']
        except KeyError:
            pass

        return args


class TextDeserializer(ActionDispatcher):
    """Default request body deserialization"""

    def deserialize(self, datastring, action='default'):
        return self.dispatch(datastring, action=action)

    def default(self, datastring):
        return {}


class JSONDeserializer(TextDeserializer):

    def _from_json(self, datastring):
        try:
            return jsonutils.loads(datastring)
        except ValueError:
            msg = _("cannot understand JSON")
            raise exception.MalformedRequestBody(reason=msg)

    def default(self, datastring):
        return {'body': self._from_json(datastring)}


class XMLDeserializer(TextDeserializer):

    def __init__(self, metadata=None):
        """
        :param metadata: information needed to deserialize xml into
                         a dictionary.
        """
        super(XMLDeserializer, self).__init__()
        self.metadata = metadata or {}

    def _from_xml(self, datastring):
        plurals = set(self.metadata.get('plurals', {}))

        try:
            node = xmlutils.safe_minidom_parse_string(datastring).childNodes[0]
            return {node.nodeName: self._from_xml_node(node, plurals)}
        except expat.ExpatError:
            msg = _("cannot understand XML")
            raise exception.MalformedRequestBody(reason=msg)

    def _from_xml_node(self, node, listnames):
        """Convert a minidom node to a simple Python type.

        :param listnames: list of XML node names whose subnodes should
                          be considered list items.

        """

        if len(node.childNodes) == 1 and node.childNodes[0].nodeType == 3:
            return node.childNodes[0].nodeValue
        elif node.nodeName in listnames:
            return [self._from_xml_node(n, listnames) for n in node.childNodes]
        else:
            result = dict()
            for attr in node.attributes.keys():
                result[attr] = node.attributes[attr].nodeValue
            for child in node.childNodes:
                if child.nodeType != node.TEXT_NODE:
                    result[child.nodeName] = self._from_xml_node(child,
                                                                 listnames)
            return result

    def find_first_child_named(self, parent, name):
        """Search a nodes children for the first child with a given name"""
        for node in parent.childNodes:
            if node.nodeName == name:
                return node
        return None

    def find_children_named(self, parent, name):
        """Return all of a nodes children who have the given name"""
        for node in parent.childNodes:
            if node.nodeName == name:
                yield node

    def extract_text(self, node):
        """Get the text field contained by the given node"""
        if len(node.childNodes) == 1:
            child = node.childNodes[0]
            if child.nodeType == child.TEXT_NODE:
                return child.nodeValue
        return ""

    def default(self, datastring):
        return {'body': self._from_xml(datastring)}

########NEW FILE########
__FILENAME__ = plugin
# Copyright 2012 Bouvet ASA
#
# Author: Endre Karlson <endre.karlson@bouvet.no>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import abc
from stevedore import driver
from stevedore import enabled
from designate.openstack.common import log as logging


LOG = logging.getLogger(__name__)


class Plugin(object):
    __metaclass__ = abc.ABCMeta

    __plugin_ns__ = None

    __plugin_name__ = None
    __plugin_type__ = None

    def __init__(self):
        self.name = self.get_canonical_name()
        LOG.debug("Loaded plugin %s", self.name)

    @classmethod
    def get_canonical_name(cls):
        """
        Return the plugin name
        """
        type_ = cls.get_plugin_type()
        name = cls.get_plugin_name()
        return "%s:%s" % (type_, name)

    @classmethod
    def get_plugin_name(cls):
        return cls.__plugin_name__

    @classmethod
    def get_plugin_type(cls):
        return cls.__plugin_type__


class DriverPlugin(Plugin):
    """
    A Driver plugin is a singleton, where only a single driver will loaded
    at a time.

    For example: Storage implementations (SQLAlchemy)
    """

    @classmethod
    def get_driver(cls, name):
        """ Load a single driver """

        LOG.debug('Looking for driver %s in %s', name, cls.__plugin_ns__)

        mgr = driver.DriverManager(cls.__plugin_ns__, name)

        return mgr.driver


class ExtensionPlugin(Plugin):
    """
    Extension plugins are loaded as a group, where multiple extensions will
    be loaded and used at the same time.

    For example: Designate Sink handlers
    """

    @classmethod
    def get_extensions(cls, enabled_extensions=None):
        """ Load a series of extensions """

        LOG.debug('Looking for extensions in %s', cls.__plugin_ns__)

        def _check_func(ext):
            if enabled_extensions is None:
                # All extensions are enabled by default, if no specific list
                # is specified
                return True

            return ext.plugin.get_plugin_name() in enabled_extensions

        mgr = enabled.EnabledExtensionManager(
            cls.__plugin_ns__, check_func=_check_func,
            propagate_map_exceptions=True)

        return [e.plugin for e in mgr]

########NEW FILE########
__FILENAME__ = policy
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from oslo.config import cfg
from designate.openstack.common import log as logging
from designate.openstack.common import policy
from designate import utils
from designate import exceptions

LOG = logging.getLogger(__name__)


_ENFORCER = None


def reset():
    global _ENFORCER
    if _ENFORCER:
        _ENFORCER.clear()
    _ENFORCER = None


def set_rules(data, default_rule=None, overwrite=True):
    default_rule = default_rule or cfg.CONF.policy_default_rule
    if not _ENFORCER:
        LOG.debug("Enforcer not present, recreating at rules stage.")
        init()

    if default_rule:
        _ENFORCER.default_rule = default_rule

    msg = "Loading rules %s, default: %s, overwrite: %s"
    LOG.debug(msg, data, default_rule, overwrite)

    if isinstance(data, dict):
        rules = dict((k, policy.parse_rule(v)) for k, v in data.items())
        rules = policy.Rules(rules, default_rule)
    else:
        rules = policy.Rules.load_json(data, default_rule)

    _ENFORCER.set_rules(rules, overwrite=overwrite)


def init(default_rule=None):
    policy_files = utils.find_config(cfg.CONF.policy_file)

    if len(policy_files) == 0:
        msg = 'Unable to determine appropriate policy json file'
        raise exceptions.ConfigurationError(msg)

    LOG.info('Using policy_file found at: %s' % policy_files[0])

    with open(policy_files[0]) as fh:
        policy_string = fh.read()
    rules = policy.Rules.load_json(policy_string, default_rule=default_rule)

    global _ENFORCER
    if not _ENFORCER:
        LOG.debug("Enforcer is not present, recreating.")
        _ENFORCER = policy.Enforcer()

    _ENFORCER.set_rules(rules)


def check(rule, ctxt, target={}, do_raise=True, exc=exceptions.Forbidden):
    creds = ctxt.to_dict()

    try:
        result = _ENFORCER.enforce(rule, target, creds, do_raise, exc)
    except Exception:
        result = False
        raise
    else:
        return result
    finally:
        extra = {'policy': {'rule': rule, 'target': target}}

        if result:
            LOG.audit("Policy check succeeded for rule '%s' on target %s",
                      rule, repr(target), extra=extra)
        else:
            LOG.audit("Policy check failed for rule '%s' on target: %s",
                      rule, repr(target), extra=extra)

########NEW FILE########
__FILENAME__ = base
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import abc
from oslo.config import cfg
from designate import exceptions
from designate.plugin import DriverPlugin


class Quota(DriverPlugin):
    """ Base class for quota plugins """
    __metaclass__ = abc.ABCMeta
    __plugin_ns__ = 'designate.quota'
    __plugin_type__ = 'quota'

    def limit_check(self, context, tenant_id, **values):
        quotas = self.get_quotas(context, tenant_id)

        for resource, value in values.items():
            if resource in quotas:
                if value >= quotas[resource]:
                    raise exceptions.OverQuota()
            else:
                raise exceptions.QuotaResourceUnknown()

    def get_quotas(self, context, tenant_id):
        quotas = self.get_default_quotas(context)

        quotas.update(self._get_quotas(context, tenant_id))

        return quotas

    @abc.abstractmethod
    def _get_quotas(self, context, tenant_id):
        pass

    def get_default_quotas(self, context):
        return {
            'domains': cfg.CONF.quota_domains,
            'domain_recordsets': cfg.CONF.quota_domain_recordsets,
            'domain_records': cfg.CONF.quota_domain_records,
            'recordset_records': cfg.CONF.quota_recordset_records,
        }

    def get_quota(self, context, tenant_id, resource):
        quotas = self._get_quotas(context, tenant_id)

        if resource not in quotas:
            raise exceptions.QuotaResourceUnknown()

        return quotas[resource]

    def set_quota(self, context, tenant_id, resource, hard_limit):
        raise NotImplementedError()

    def reset_quotas(self, context, tenant_id):
        raise NotImplementedError()

########NEW FILE########
__FILENAME__ = impl_noop
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.openstack.common import log as logging
from designate.quota.base import Quota

LOG = logging.getLogger(__name__)


class NoopQuota(Quota):
    __plugin_name__ = 'noop'

    def _get_quotas(self, context, tenant_id):
        return {}

########NEW FILE########
__FILENAME__ = impl_storage
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate import exceptions
from designate.openstack.common import log as logging
from designate.quota.base import Quota
from designate.storage import api as sapi

LOG = logging.getLogger(__name__)


class StorageQuota(Quota):
    __plugin_name__ = 'storage'

    def __init__(self, storage_api=None):
        super(StorageQuota, self).__init__()

        if storage_api is None:
            storage_api = sapi.StorageAPI()

        self.storage_api = storage_api

    def _get_quotas(self, context, tenant_id):
        quotas = self.storage_api.find_quotas(context, {
            'tenant_id': tenant_id,
        })

        return dict((q['resource'], q['hard_limit']) for q in quotas)

    def get_quota(self, context, tenant_id, resource):
        context = context.deepcopy()
        context.all_tenants = True

        quota = self.storage_api.find_quota(context, {
            'tenant_id': tenant_id,
            'resource': resource,
        })

        return {resource: quota['hard_limit']}

    def set_quota(self, context, tenant_id, resource, hard_limit):
        context = context.deepcopy()
        context.all_tenants = True

        def create_quota():
            values = {
                'tenant_id': tenant_id,
                'resource': resource,
                'hard_limit': hard_limit,
            }

            with self.storage_api.create_quota(context, values):
                pass  # NOTE(kiall): No other systems need updating.

        def update_quota():
            values = {'hard_limit': hard_limit}

            with self.storage_api.update_quota(context, quota['id'], values):
                pass  # NOTE(kiall): No other systems need updating.

        if resource not in self.get_default_quotas(context).keys():
            raise exceptions.QuotaResourceUnknown("%s is not a valid quota "
                                                  "resource", resource)

        try:
            quota = self.storage_api.find_quota(context, {
                'tenant_id': tenant_id,
                'resource': resource,
            })
        except exceptions.NotFound:
            create_quota()
        else:
            update_quota()

        return {resource: hard_limit}

    def reset_quotas(self, context, tenant_id):
        context = context.deepcopy()
        context.all_tenants = True

        quotas = self.storage_api.find_quotas(context, {
            'tenant_id': tenant_id,
        })

        for quota in quotas:
            with self.storage_api.delete_quota(context, quota['id']):
                pass  # NOTE(kiall): No other systems need updating.

########NEW FILE########
__FILENAME__ = rpc
# Copyright 2013 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

__all__ = [
    'init',
    'cleanup',
    'set_defaults',
    'add_extra_exmods',
    'clear_extra_exmods',
    'get_allowed_exmods',
    'RequestContextSerializer',
    'get_client',
    'get_server',
    'get_notifier',
    'TRANSPORT_ALIASES',
]

from oslo.config import cfg
from oslo import messaging

import designate.context
import designate.exceptions
from designate.openstack.common import importutils
from designate.openstack.common import jsonutils

CONF = cfg.CONF
TRANSPORT = None
NOTIFIER = None


# NOTE: Additional entries to designate.exceptions goes here.
CONF.register_opts([
    cfg.ListOpt(
        'allowed_remote_exmods',
        default=[],
        help="Additional modules that contains allowed RPC exceptions.",
        deprecated_name='allowed_rpc_exception_modules')
])
ALLOWED_EXMODS = [
    designate.exceptions.__name__,
    'designate.backend.impl_dynect'
]
EXTRA_EXMODS = []


# NOTE(flaper87): The designate.openstack.common.rpc entries are
# for backwards compat with Havana rpc_backend configuration
# values. The designate.rpc entries are for compat with Folsom values.
TRANSPORT_ALIASES = {
    'designate.openstack.common.rpc.impl_kombu': 'rabbit',
    'designate.openstack.common.rpc.impl_qpid': 'qpid',
    'designate.openstack.common.rpc.impl_zmq': 'zmq',
    'designate.rpc.impl_kombu': 'rabbit',
    'designate.rpc.impl_qpid': 'qpid',
    'designate.rpc.impl_zmq': 'zmq',
}


def init(conf):
    global TRANSPORT, NOTIFIER
    exmods = get_allowed_exmods()
    TRANSPORT = messaging.get_transport(conf,
                                        allowed_remote_exmods=exmods,
                                        aliases=TRANSPORT_ALIASES)

    serializer = RequestContextSerializer(JsonPayloadSerializer())
    NOTIFIER = messaging.Notifier(TRANSPORT, serializer=serializer)


def initialized():
    return None not in [TRANSPORT, NOTIFIER]


def cleanup():
    global TRANSPORT, NOTIFIER
    assert TRANSPORT is not None
    assert NOTIFIER is not None
    TRANSPORT.cleanup()
    TRANSPORT = NOTIFIER = None


def set_defaults(control_exchange):
    messaging.set_transport_defaults(control_exchange)


def add_extra_exmods(*args):
    EXTRA_EXMODS.extend(args)


def clear_extra_exmods():
    del EXTRA_EXMODS[:]


def get_allowed_exmods():
    return ALLOWED_EXMODS + EXTRA_EXMODS + CONF.allowed_remote_exmods


class JsonPayloadSerializer(messaging.NoOpSerializer):
    @staticmethod
    def serialize_entity(context, entity):
        return jsonutils.to_primitive(entity, convert_instances=True)


class DesignateObjectSerializer(messaging.NoOpSerializer):
    def _process_iterable(self, context, action_fn, values):
        """Process an iterable, taking an action on each value.
        :param:context: Request context
        :param:action_fn: Action to take on each item in values
        :param:values: Iterable container of things to take action on
        :returns: A new container of the same type (except set) with
        items from values having had action applied.
        """
        iterable = values.__class__
        if iterable == set:
            # NOTE: A set can't have an unhashable value inside, such as
            # a dict. Convert sets to tuples, which is fine, since we can't
            # send them over RPC anyway.
            iterable = tuple
        return iterable([action_fn(context, value) for value in values])

    def serialize_entity(self, context, entity):
        if isinstance(entity, (tuple, list, set)):
            entity = self._process_iterable(context, self.serialize_entity,
                                            entity)
        elif hasattr(entity, 'to_primitive') and callable(entity.to_primitive):
            entity = entity.to_primitive()
        return entity

    def deserialize_entity(self, context, entity):
        if isinstance(entity, dict) and 'designate_object.name' in entity and\
                'designate_object.data' in entity:
            cls = importutils.import_class(entity['designate_object.name'])
            entity = cls.from_primitive(entity)
        elif isinstance(entity, (tuple, list, set)):
            entity = self._process_iterable(context, self.deserialize_entity,
                                            entity)
        return entity


class RequestContextSerializer(messaging.Serializer):

    def __init__(self, base):
        self._base = base

    def serialize_entity(self, context, entity):
        if not self._base:
            return entity
        return self._base.serialize_entity(context, entity)

    def deserialize_entity(self, context, entity):
        if not self._base:
            return entity
        return self._base.deserialize_entity(context, entity)

    def serialize_context(self, context):
        return context.to_dict()

    def deserialize_context(self, context):
        return designate.context.DesignateContext.from_dict(context)


def get_transport_url(url_str=None):
    return messaging.TransportURL.parse(CONF, url_str, TRANSPORT_ALIASES)


def get_client(target, version_cap=None, serializer=None):
    assert TRANSPORT is not None
    if serializer is None:
        serializer = DesignateObjectSerializer()
    serializer = RequestContextSerializer(serializer)
    return messaging.RPCClient(TRANSPORT,
                               target,
                               version_cap=version_cap,
                               serializer=serializer)


def get_server(target, endpoints, serializer=None):
    assert TRANSPORT is not None
    if serializer is None:
        serializer = DesignateObjectSerializer()
    serializer = RequestContextSerializer(serializer)
    return messaging.get_rpc_server(TRANSPORT,
                                    target,
                                    endpoints,
                                    executor='eventlet',
                                    serializer=serializer)


def get_listener(targets, endpoints, serializer=None):
    assert TRANSPORT is not None
    serializer = RequestContextSerializer(serializer)
    return messaging.get_notification_listener(TRANSPORT,
                                               targets,
                                               endpoints,
                                               executor='eventlet',
                                               serializer=serializer)


def get_notifier(service=None, host=None, publisher_id=None):
    assert NOTIFIER is not None
    if not publisher_id:
        publisher_id = "%s.%s" % (service, host or CONF.host)
    return NOTIFIER.prepare(publisher_id=publisher_id)

########NEW FILE########
__FILENAME__ = format
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import re
import jsonschema
from jsonschema import compat
import netaddr
from designate.openstack.common import log as logging

LOG = logging.getLogger(__name__)

RE_DOMAINNAME = r'^(?!.{255,})(?:(?!\-)[A-Za-z0-9_\-]{1,63}(?<!\-)\.)+$'
RE_HOSTNAME = r'^(?!.{255,})(?:(^\*|(?!\-)[A-Za-z0-9_\-]{1,63})(?<!\-)\.)+$'

# The TLD name will not end in a period.
RE_TLDNAME = r'^(?!.{255,})(?:(?!\-)[A-Za-z0-9_\-]{1,63}(?<!\-))' \
    r'(\.(?:(?!\-)[A-Za-z0-9_\-]{1,63}(?<!\-)))*$'

draft3_format_checker = jsonschema.draft3_format_checker
draft4_format_checker = jsonschema.draft4_format_checker


@draft3_format_checker.checks("ip-address")
@draft4_format_checker.checks("ipv4")
def is_ipv4(instance):
    if not isinstance(instance, compat.str_types):
        return True

    try:
        address = netaddr.IPAddress(instance, version=4)
        # netaddr happly accepts, and expands "127.0" into "127.0.0.0"
        if str(address) != instance:
            return False
    except Exception:
        return False

    if instance == '0.0.0.0':  # RFC5735
        return False

    return True


@draft3_format_checker.checks("ipv6")
@draft4_format_checker.checks("ipv6")
def is_ipv6(instance):
    if not isinstance(instance, compat.str_types):
        return True

    try:
        netaddr.IPAddress(instance, version=6)
    except Exception:
        return False

    return True


@draft3_format_checker.checks("host-name")
@draft4_format_checker.checks("hostname")
def is_hostname(instance):
    if not isinstance(instance, compat.str_types):
        return True

    if not re.match(RE_HOSTNAME, instance):
        return False

    return True


@draft3_format_checker.checks("domain-name")
@draft4_format_checker.checks("domainname")
def is_domainname(instance):
    if not isinstance(instance, compat.str_types):
        return True

    if not re.match(RE_DOMAINNAME, instance):
        return False

    return True


@draft3_format_checker.checks("tld-name")
@draft4_format_checker.checks("tldname")
def is_tldname(instance):
    if not isinstance(instance, compat.str_types):
        return True

    if not re.match(RE_TLDNAME, instance):
        return False

    return True


@draft3_format_checker.checks("email")
@draft4_format_checker.checks("email")
def is_email(instance):
    if not isinstance(instance, compat.str_types):
        return True

    # A valid email address. We use the RFC1035 version of "valid".
    if instance.count('@') != 1:
        return False

    rname = instance.replace('@', '.', 1)

    if not re.match(RE_DOMAINNAME, "%s." % rname):
        return False

    return True

########NEW FILE########
__FILENAME__ = resolvers
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import jsonschema
from designate.openstack.common import log as logging
from designate import utils


LOG = logging.getLogger(__name__)


class LocalResolver(jsonschema.RefResolver):
    def __init__(self, base_uri, referrer):
        super(LocalResolver, self).__init__(base_uri, referrer, (), True)
        self.api_version = None

    @classmethod
    def from_schema(cls, api_version, schema, *args, **kwargs):
        resolver = cls(schema.get("id", ""), schema, *args, **kwargs)
        resolver.api_version = api_version

        return resolver

    def resolve_remote(self, uri):
        LOG.debug('Loading remote schema: %s', uri)
        return utils.load_schema(self.api_version, uri)

########NEW FILE########
__FILENAME__ = validators
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from jsonschema import validators
from designate.openstack.common import log as logging
from designate.schema import _validators

LOG = logging.getLogger(__name__)

Draft3Validator = validators.extend(
    validators.Draft3Validator,
    validators={
        "type": _validators.type_draft3,
        "oneOf": _validators.oneOf_draft3,
    })

Draft4Validator = validators.extend(
    validators.Draft4Validator,
    validators={
        "type": _validators.type_draft4,
    })

########NEW FILE########
__FILENAME__ = _validators
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import datetime
import jsonschema
from jsonschema import _utils


def type_draft3(validator, types, instance, schema):
    types = _utils.ensure_list(types)

    # NOTE(kiall): A datetime object is not a string, but is still valid.
    if ('format' in schema and schema['format'] == 'date-time'
            and isinstance(instance, datetime.datetime)):
        return

    all_errors = []
    for index, type in enumerate(types):
        if type == "any":
            return
        if validator.is_type(type, "object"):
            errors = list(validator.descend(instance, type, schema_path=index))
            if not errors:
                return
            all_errors.extend(errors)
        else:
            if validator.is_type(instance, type):
                return
    else:
        yield jsonschema.ValidationError(
            _utils.types_msg(instance, types), context=all_errors,
        )


def oneOf_draft3(validator, oneOf, instance, schema):
        # Backported from Draft4 to Draft3
        subschemas = iter(oneOf)
        first_valid = next(
            (s for s in subschemas if validator.is_valid(instance, s)), None,
        )

        if first_valid is None:
            yield jsonschema.ValidationError(
                "%r is not valid under any of the given schemas." % (instance,)
            )
        else:
            more_valid = [s for s in subschemas
                          if validator.is_valid(instance, s)]
            if more_valid:
                more_valid.append(first_valid)
                reprs = ", ".join(repr(schema) for schema in more_valid)
                yield jsonschema.ValidationError(
                    "%r is valid under each of %s" % (instance, reprs)
                )


def type_draft4(validator, types, instance, schema):
    types = _utils.ensure_list(types)

    # NOTE(kiall): A datetime object is not a string, but is still valid.
    if ('format' in schema and schema['format'] == 'date-time'
            and isinstance(instance, datetime.datetime)):
        return

    if not any(validator.is_type(instance, type) for type in types):
        yield jsonschema.ValidationError(_utils.types_msg(instance, types))

########NEW FILE########
__FILENAME__ = service
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
import os
import inspect

from oslo import messaging
from oslo.config import cfg

from designate.openstack.common import service
from designate.openstack.common import log as logging
from designate.openstack.common.gettextutils import _
from designate import rpc
from designate import version

CONF = cfg.CONF

LOG = logging.getLogger(__name__)


class Service(service.Service):
    """
    Service class to be shared among the diverse service inside of Designate.

    Partially inspired by the code at cinder.service but for now without
    support for loading so called "endpoints" or "managers".
    """
    def __init__(self, host, binary, topic, service_name=None, manager=None):
        super(Service, self).__init__()

        if not rpc.initialized():
            rpc.init(CONF)

        self.host = host
        self.binary = binary
        self.topic = topic
        self.service_name = service_name

        # TODO(ekarlso): change this to be loadable via mod import or
        # stevedore?
        self.manager = manager or self

    def start(self):
        version_string = version.version_info.version_string()
        LOG.audit(_('Starting %(topic)s node (version %(version_string)s)'),
                  {'topic': self.topic, 'version_string': version_string})

        LOG.debug(_("Creating RPC server on topic '%s'") % self.topic)

        manager = self.manager or self
        endpoints = [manager]
        if hasattr(manager, 'additional_endpoints'):
            endpoints.extend(self.manager.additional_endpoints)

        target = messaging.Target(topic=self.topic, server=self.host)
        self.rpcserver = rpc.get_server(target, endpoints)
        self.rpcserver.start()

        self.notifier = rpc.get_notifier(self.service_name)

    @classmethod
    def create(cls, host=None, binary=None, topic=None, service_name=None,
               manager=None):
        """Instantiates class and passes back application object.

        :param host: defaults to CONF.host
        :param binary: defaults to basename of executable
        :param topic: defaults to bin_name - 'cinder-' part
        """
        if not host:
            host = CONF.host
        if not binary:
            binary = os.path.basename(inspect.stack()[-1][1])
        if not topic:
            name = "_".join(binary.split('-')[1:]) + '_topic'
            topic = CONF.get(name)

        service_obj = cls(host, binary, topic, service_name=service_name,
                          manager=manager)
        return service_obj

    def stop(self):
        # Try to shut the connection down, but if we get any sort of
        # errors, go ahead and ignore them.. as we're shutting down anyway
        try:
            self.rpcserver.stop()
        except Exception:
            pass
        super(Service, self).stop()


_launcher = None


def serve(server, workers=None):
    global _launcher
    if _launcher:
        raise RuntimeError(_('serve() can only be called once'))

    _launcher = service.launch(server, workers=workers)


def wait():
    try:
        _launcher.wait()
    except KeyboardInterrupt:
        _launcher.stop()
    rpc.cleanup()

########NEW FILE########
__FILENAME__ = service
# Copyright 2012 Managed I.T.
# Copyright 2014 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from oslo.config import cfg
from oslo import messaging

from designate.openstack.common import log as logging
from designate.openstack.common import service
from designate import exceptions
from designate import notification_handler
from designate import rpc

LOG = logging.getLogger(__name__)


class Service(service.Service):
    def __init__(self, *args, **kwargs):
        super(Service, self).__init__(*args, **kwargs)

        rpc.init(cfg.CONF)

        # Initialize extensions
        self.handlers = self._init_extensions()
        self.subscribers = self._get_subscribers()

    def _init_extensions(self):
        """ Loads and prepares all enabled extensions """

        enabled_notification_handlers = \
            cfg.CONF['service:sink'].enabled_notification_handlers

        notification_handlers = notification_handler.get_notification_handlers(
            enabled_notification_handlers)

        if len(notification_handlers) == 0:
            # No handlers enabled. Bail!
            raise exceptions.ConfigurationError('No designate-sink handlers '
                                                'enabled or loaded')

        return notification_handlers

    def _get_subscribers(self):
        subscriptions = {}
        for handler in self.handlers:
            for et in handler.get_event_types():
                subscriptions.setdefault(et, [])
                subscriptions[et].append(handler)
        return subscriptions

    def start(self):
        super(Service, self).start()

        # Setup notification subscriptions and start consuming
        targets = self._get_targets()

        # TODO(ekarlso): Change this is to endpoint objects rather then
        # ourselves?
        self._server = rpc.get_listener(targets, [self])
        self._server.start()

    def stop(self):
        # Try to shut the connection down, but if we get any sort of
        # errors, go ahead and ignore them.. as we're shutting down anyway
        try:
            self._server.stop()
        except Exception:
            pass

        super(Service, self).stop()

    def _get_targets(self):
        """
        Set's up subscriptions for the various exchange+topic combinations that
        we have a handler for.
        """
        targets = []
        for handler in self.handlers:
            exchange, topics = handler.get_exchange_topics()

            for topic in topics:
                target = messaging.Target(exchange=exchange, topic=topic)
                targets.append(target)
        return targets

    def _get_handler_event_types(self):
        """return a dict - keys are the event types we can handle"""
        return self.subscribers

    def info(self, context, publisher_id, event_type, payload, metadata):
        """
        Processes an incoming notification, offering each extension the
        opportunity to handle it.
        """
        # NOTE(zykes): Only bother to actually do processing if there's any
        # matching events, skips logging of things like compute.exists etc.
        if event_type in self._get_handler_event_types():
            for handler in self.handlers:
                if event_type in handler.get_event_types():
                    LOG.debug('Found handler for: %s' % event_type)
                    handler.process_notification(context, event_type, payload)

########NEW FILE########
__FILENAME__ = expressions
# Copyright 2012 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy.ext.compiler import compiles
from sqlalchemy.sql.expression import Executable, ClauseElement


class InsertFromSelect(Executable, ClauseElement):
    execution_options = \
        Executable._execution_options.union({'autocommit': True})

    def __init__(self, table, select, columns=None):
        self.table = table
        self.select = select
        self.columns = columns


@compiles(InsertFromSelect)
def visit_insert_from_select(element, compiler, **kw):
    # NOTE(kiall): SQLA 0.8.3+ has an InsertFromSelect built in:
    #              sqlalchemy.sql.expression.Insert.from_select
    #              This code can be removed once we require 0.8.3+
    table = compiler.process(element.table, asfrom=True)
    select = compiler.process(element.select)

    if element.columns is not None:

        columns = [compiler.preparer.format_column(c) for c in element.columns]
        columns = ", ".join(columns)

        return "INSERT INTO %s (%s) %s" % (
            table,
            columns,
            select
        )
    else:
        return "INSERT INTO %s %s" % (
            table,
            select
        )


# # Dialect specific compilation example, should it be needed.
# @compiles(InsertFromSelect, 'postgresql')
# def visit_insert_from_select(element, compiler, **kw):
#     ...

########NEW FILE########
__FILENAME__ = models
# Copyright 2012 Hewlett-Packard Development Company, L.P.
#
# Author: Patrick Galbraith <patg@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import Column, DateTime
from sqlalchemy.exc import IntegrityError
from sqlalchemy.orm import object_mapper
from sqlalchemy.types import CHAR
from designate.openstack.common import timeutils
from designate import exceptions


class Base(object):
    __abstract__ = True
    __table_initialized__ = False

    def save(self, session):
        """ Save this object """
        session.add(self)

        try:
            session.flush()
        except IntegrityError as e:
            non_unique_strings = (
                'duplicate entry',
                'not unique',
                'unique constraint failed'
            )

            for non_unique_string in non_unique_strings:
                if non_unique_string in str(e).lower():
                    raise exceptions.Duplicate(str(e))

            # Not a Duplicate error.. Re-raise.
            raise

    def delete(self, session):
        """ Delete this object """
        session.delete(self)
        session.flush()

    def __setitem__(self, key, value):
        setattr(self, key, value)

    def __getitem__(self, key):
        return getattr(self, key)

    def __iter__(self):
        columns = dict(object_mapper(self).columns).keys()
        # NOTE(russellb): Allow models to specify other keys that can be looked
        # up, beyond the actual db columns.  An example would be the 'name'
        # property for an Instance.
        if hasattr(self, '_extra_keys'):
            columns.extend(self._extra_keys())
        self._i = iter(columns)
        return self

    def next(self):
        n = self._i.next()
        return n, getattr(self, n)

    def update(self, values):
        """ Make the model object behave like a dict """
        for k, v in values.iteritems():
            setattr(self, k, v)

    def iteritems(self):
        """
        Make the model object behave like a dict.

        Includes attributes from joins.
        """
        local = dict(self)
        joined = dict([(k, v) for k, v in self.__dict__.iteritems()
                      if not k[0] == '_'])
        local.update(joined)
        return local.iteritems()


class SoftDeleteMixin(object):
    deleted = Column(CHAR(32), nullable=False, default="0", server_default="0")
    deleted_at = Column(DateTime, nullable=True, default=None)

    def soft_delete(self, session=None):
        """ Mark this object as deleted. """
        self.deleted = self.id.replace('-', '')
        self.deleted_at = timeutils.utcnow()

        if hasattr(self, 'status'):
            self.status = "DELETED"

        self.save(session=session)

########NEW FILE########
__FILENAME__ = session
# Copyright 2010 United States Government as represented by the
#   Administrator of the National Aeronautics and Space Administration.
#   All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

"""Session Handling for SQLAlchemy backend."""

import re
import time

import sqlalchemy
from sqlalchemy.exc import DisconnectionError, OperationalError
import sqlalchemy.orm
from sqlalchemy.pool import NullPool, StaticPool

from oslo.config import cfg
from designate.openstack.common import log as logging
from designate.openstack.common.gettextutils import _

LOG = logging.getLogger(__name__)

_MAKERS = {}
_ENGINES = {}


SQLOPTS = [
    cfg.StrOpt('database_connection',
               default='sqlite:///$state_path/designate.sqlite',
               secret=True,
               help='The database driver to use'),
    cfg.IntOpt('connection_debug', default=0,
               help='Verbosity of SQL debugging information. 0=None,'
               ' 100=Everything'),
    cfg.BoolOpt('connection_trace', default=False,
                help='Add python stack traces to SQL as comment strings'),
    cfg.BoolOpt('sqlite_synchronous', default=True,
                help='If passed, use synchronous mode for sqlite'),
    cfg.IntOpt('idle_timeout', default=3600,
               help='timeout before idle sql connections are reaped'),
    cfg.IntOpt('max_retries', default=10,
               help='maximum db connection retries during startup. '
               '(setting -1 implies an infinite retry count)'),
    cfg.IntOpt('retry_interval', default=10,
               help='interval between retries of opening a sql connection')
]


def get_session(config_group,
                autocommit=True,
                expire_on_commit=False,
                autoflush=True):
    """Return a SQLAlchemy session."""
    global _MAKERS

    if config_group not in _MAKERS:
        engine = get_engine(config_group)
        _MAKERS[config_group] = get_maker(engine,
                                          autocommit,
                                          expire_on_commit,
                                          autoflush)

    session = _MAKERS[config_group]()
    return session


def synchronous_switch_listener(dbapi_conn, connection_rec):
    """Switch sqlite connections to non-synchronous mode"""
    dbapi_conn.execute("PRAGMA synchronous = OFF")


def add_regexp_listener(dbapi_con, con_record):
    """Add REGEXP function to sqlite connections."""

    def regexp(expr, item):
        reg = re.compile(expr)
        return reg.search(unicode(item)) is not None
    dbapi_con.create_function('regexp', 2, regexp)


def ping_listener(dbapi_conn, connection_rec, connection_proxy):
    """
    Ensures that MySQL connections checked out of the
    pool are alive.

    Borrowed from:
    http://groups.google.com/group/sqlalchemy/msg/a4ce563d802c929f
    """
    try:
        dbapi_conn.cursor().execute('select 1')
    except dbapi_conn.OperationalError as ex:
        if ex.args[0] in (2006, 2013, 2014, 2045, 2055):
            LOG.warn('Got mysql server has gone away: %s', ex)
            raise DisconnectionError("Database server went away")
        else:
            raise


def is_db_connection_error(args):
    """Return True if error in connecting to db."""
    # NOTE(adam_g): This is currently MySQL specific and needs to be extended
    #               to support Postgres and others.
    conn_err_codes = ('2002', '2003', '2006')
    for err_code in conn_err_codes:
        if args.find(err_code) != -1:
            return True
    return False


def get_engine(config_group):
    """Return a SQLAlchemy engine."""
    global _ENGINES

    database_connection = cfg.CONF[config_group].database_connection

    if config_group not in _ENGINES:
        connection_dict = sqlalchemy.engine.url.make_url(
            database_connection)

        engine_args = {
            "pool_recycle": cfg.CONF[config_group].idle_timeout,
            "echo": False,
            'convert_unicode': True,
        }

        # Map our SQL debug level to SQLAlchemy's options
        if cfg.CONF[config_group].connection_debug >= 100:
            engine_args['echo'] = 'debug'
        elif cfg.CONF[config_group].connection_debug >= 50:
            engine_args['echo'] = True

        if "sqlite" in connection_dict.drivername:
            engine_args["poolclass"] = NullPool

            if database_connection == "sqlite://":
                engine_args["poolclass"] = StaticPool
                engine_args["connect_args"] = {'check_same_thread': False}

        _ENGINES[config_group] = sqlalchemy.create_engine(database_connection,
                                                          **engine_args)

        if 'mysql' in connection_dict.drivername:
            sqlalchemy.event.listen(_ENGINES[config_group],
                                    'checkout',
                                    ping_listener)
        elif "sqlite" in connection_dict.drivername:
            if not cfg.CONF[config_group].sqlite_synchronous:
                sqlalchemy.event.listen(_ENGINES[config_group],
                                        'connect',
                                        synchronous_switch_listener)
            sqlalchemy.event.listen(_ENGINES[config_group],
                                    'connect',
                                    add_regexp_listener)

        if (cfg.CONF[config_group].connection_trace and
                _ENGINES[config_group].dialect.dbapi.__name__ == 'MySQLdb'):
            import MySQLdb.cursors
            _do_query = debug_mysql_do_query()
            setattr(MySQLdb.cursors.BaseCursor, '_do_query', _do_query)

        try:
            _ENGINES[config_group].connect()
        except OperationalError as e:
            if not is_db_connection_error(e.args[0]):
                raise

            remaining = cfg.CONF[config_group].max_retries
            if remaining == -1:
                remaining = 'infinite'
            while True:
                msg = _('SQL connection failed. %s attempts left.')
                LOG.warn(msg % remaining)
                if remaining != 'infinite':
                    remaining -= 1
                time.sleep(cfg.CONF[config_group].retry_interval)
                try:
                    _ENGINES[config_group].connect()
                    break
                except OperationalError as e:
                    if (remaining != 'infinite' and remaining == 0) or \
                            not is_db_connection_error(e.args[0]):
                        raise
    return _ENGINES[config_group]


def get_maker(engine, autocommit=True, expire_on_commit=False, autoflush=True):
    """Return a SQLAlchemy sessionmaker using the given engine."""
    return sqlalchemy.orm.sessionmaker(bind=engine,
                                       autocommit=autocommit,
                                       autoflush=autoflush,
                                       expire_on_commit=expire_on_commit)


def debug_mysql_do_query():
    """Return a debug version of MySQLdb.cursors._do_query"""
    import MySQLdb.cursors
    import traceback

    old_mysql_do_query = MySQLdb.cursors.BaseCursor._do_query

    def _do_query(self, q):
        stack = ''
        for file, line, method, function in traceback.extract_stack():
            # exclude various common things from trace
            if file.endswith('session.py') and method == '_do_query':
                continue
            if file.endswith('api.py') and method == 'wrapper':
                continue
            if file.endswith('utils.py') and method == '_inner':
                continue
            if file.endswith('exception.py') and method == '_wrap':
                continue
            # nova/db/api is just a wrapper around nova/db/sqlalchemy/api
            if file.endswith('nova/db/api.py'):
                continue
            # only trace inside nova
            index = file.rfind('nova')
            if index == -1:
                continue
            stack += "File:%s:%s Method:%s() Line:%s | " \
                     % (file[index:], line, method, function)

        # strip trailing " | " from stack
        if stack:
            stack = stack[:-3]
            qq = "%s /* %s */" % (q, stack)
        else:
            qq = q
        old_mysql_do_query(self, qq)

    # return the new _do_query method
    return _do_query

########NEW FILE########
__FILENAME__ = types
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy.types import TypeDecorator, CHAR, VARCHAR
from sqlalchemy.dialects.postgresql import UUID as pgUUID
from sqlalchemy.dialects.postgresql import INET as pgINET
import uuid


class UUID(TypeDecorator):
    """Platform-independent UUID type.

    Uses Postgresql's UUID type, otherwise uses
    CHAR(32), storing as stringified hex values.

    Copied verbatim from SQLAlchemy documentation.
    """
    impl = CHAR

    def load_dialect_impl(self, dialect):
        if dialect.name == 'postgresql':
            return dialect.type_descriptor(pgUUID())
        else:
            return dialect.type_descriptor(CHAR(32))

    def process_bind_param(self, value, dialect):
        if value is None:
            return value
        elif dialect.name == 'postgresql':
            return str(value)
        else:
            if not isinstance(value, uuid.UUID):
                return "%.32x" % uuid.UUID(value)
            else:
                # hexstring
                return "%.32x" % value

    def process_result_value(self, value, dialect):
        if value is None:
            return value
        else:
            return str(uuid.UUID(value))


class Inet(TypeDecorator):
    impl = VARCHAR

    def load_dialect_impl(self, dialect):
        if dialect.name == "postgresql":
            return pgINET()
        else:
            return VARCHAR(39)  # IPv6 can be up to 39 chars

    def process_bind_param(self, value, dialect):
        if value is None:
            return value
        else:
            return str(value)

########NEW FILE########
__FILENAME__ = api
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import contextlib
from designate import storage
from designate.openstack.common import excutils


class StorageAPI(object):
    """ Storage API """

    def __init__(self):
        self.storage = storage.get_storage()

    def _extract_dict_subset(self, d, keys):
        return dict([(k, d[k]) for k in keys if k in d])

    @contextlib.contextmanager
    def create_quota(self, context, values):
        """
        Create a Quota.

        :param context: RPC Context.
        :param values: Values to create the new Quota from.
        """
        self.storage.begin()

        try:
            quota = self.storage.create_quota(context, values)
            yield quota
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    def get_quota(self, context, quota_id):
        """
        Get a Quota via ID.

        :param context: RPC Context.
        :param quota_id: Quota ID to get.
        """
        return self.storage.get_quota(context, quota_id)

    def find_quotas(self, context, criterion=None, marker=None, limit=None,
                    sort_key=None, sort_dir=None):
        """
        Find Quotas

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.find_quotas(
            context, criterion, marker, limit, sort_key, sort_dir)

    def find_quota(self, context, criterion):
        """
        Find a single Quota.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.find_quota(context, criterion)

    @contextlib.contextmanager
    def update_quota(self, context, quota_id, values):
        """
        Update a Quota via ID

        :param context: RPC Context.
        :param quota_id: Quota ID to update.
        :param values: Values to update the Quota from
        """
        self.storage.begin()

        try:
            quota = self.storage.update_quota(context, quota_id, values)
            yield quota
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    @contextlib.contextmanager
    def delete_quota(self, context, quota_id):
        """
        Delete a Quota via ID.

        :param context: RPC Context.
        :param quota_id: Delete a Quota via ID
        """
        self.storage.begin()

        try:
            yield self.storage.get_quota(context, quota_id)
            self.storage.delete_quota(context, quota_id)
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    @contextlib.contextmanager
    def create_server(self, context, values):
        """
        Create a Server.

        :param context: RPC Context.
        :param values: Values to create the new Domain from.
        """
        self.storage.begin()

        try:
            server = self.storage.create_server(context, values)
            yield server
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    def get_server(self, context, server_id):
        """
        Get a Server via ID.

        :param context: RPC Context.
        :param server_id: Server ID to get.
        """
        return self.storage.get_server(context, server_id)

    def find_servers(self, context, criterion=None, marker=None, limit=None,
                     sort_key=None, sort_dir=None):
        """
        Find Servers

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.find_servers(
            context, criterion, marker, limit, sort_key, sort_dir)

    def find_server(self, context, criterion):
        """
        Find a single Server.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.find_server(context, criterion)

    @contextlib.contextmanager
    def update_server(self, context, server_id, values):
        """
        Update a Server via ID

        :param context: RPC Context.
        :param server_id: Server ID to update.
        :param values: Values to update the Server from
        """
        self.storage.begin()

        try:
            server = self.storage.update_server(context, server_id, values)
            yield server
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    @contextlib.contextmanager
    def delete_server(self, context, server_id):
        """
        Delete a Server via ID.

        :param context: RPC Context.
        :param server_id: Delete a Server via ID
        """
        self.storage.begin()

        try:
            yield self.storage.get_server(context, server_id)
            self.storage.delete_server(context, server_id)
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    @contextlib.contextmanager
    def create_tld(self, context, values):
        """
        Create a TLD.

        :param context: RPC Context.
        :param values: Values to create the new TLD from.
        """
        self.storage.begin()

        try:
            tld = self.storage.create_tld(context, values)
            yield tld
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    def get_tld(self, context, tld_id):
        """
        Get a TLD via ID.

        :param context: RPC Context.
        :param tld_id: TLD ID to get.
        """
        return self.storage.get_tld(context, tld_id)

    def find_tlds(self, context, criterion=None, marker=None, limit=None,
                  sort_key=None, sort_dir=None):
        """
        Find TLDs

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.find_tlds(
            context, criterion, marker, limit, sort_key, sort_dir)

    def find_tld(self, context, criterion):
        """
        Find a single TLD.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.find_tld(context, criterion)

    @contextlib.contextmanager
    def update_tld(self, context, tld_id, values):
        """
        Update a TLD via ID

        :param context: RPC Context.
        :param tld_id: TLD ID to update.
        :param values: Values to update the TLD from
        """
        self.storage.begin()

        try:
            tld = self.storage.update_tld(context, tld_id, values)
            yield tld
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    @contextlib.contextmanager
    def delete_tld(self, context, tld_id):
        """
        Delete a TLD via ID.

        :param context: RPC Context.
        :param tld_id: Delete a TLD via ID
        """
        self.storage.begin()

        try:
            yield self.storage.get_tld(context, tld_id)
            self.storage.delete_tld(context, tld_id)
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    @contextlib.contextmanager
    def create_tsigkey(self, context, values):
        """
        Create a TSIG Key.

        :param context: RPC Context.
        """
        self.storage.begin()

        try:
            tsigkey = self.storage.create_tsigkey(context, values)
            yield tsigkey
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    def get_tsigkey(self, context, tsigkey_id):
        """
        Get a TSIG Key via ID.

        :param context: RPC Context.
        :param tsigkey_id: Server ID to get.
        """
        return self.storage.get_tsigkey(context, tsigkey_id)

    def find_tsigkeys(self, context, criterion=None, marker=None, limit=None,
                      sort_key=None, sort_dir=None):
        """
        Find Tsigkey

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.find_tsigkeys(
            context, criterion, marker, limit, sort_key, sort_dir)

    def find_tsigkey(self, context, criterion):
        """
        Find a single Tsigkey.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.find_tsigkey(context, criterion)

    @contextlib.contextmanager
    def update_tsigkey(self, context, tsigkey_id, values):
        """
        Update a TSIG Key via ID

        :param context: RPC Context.
        :param tsigkey_id: TSIG Key ID to update.
        :param values: Values to update the TSIG Key from
        """
        self.storage.begin()

        try:
            tsigkey = self.storage.update_tsigkey(context, tsigkey_id, values)
            yield tsigkey
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    @contextlib.contextmanager
    def delete_tsigkey(self, context, tsigkey_id):
        """
        Delete a TSIG Key via ID.

        :param context: RPC Context.
        :param tsigkey_id: Delete a TSIG Key via ID
        """
        self.storage.begin()

        try:
            yield self.storage.get_tsigkey(context, tsigkey_id)
            self.storage.delete_tsigkey(context, tsigkey_id)
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    def find_tenants(self, context):
        """
        Find all Tenants.

        :param context: RPC Context.
        """
        return self.storage.find_tenants(context)

    def get_tenant(self, context, tenant_id):
        """
        Get all Tenants.

        :param context: RPC Context.
        :param tenant_id: ID of the Tenant.
        """
        return self.storage.get_tenant(context, tenant_id)

    def count_tenants(self, context):
        """
        Count tenants

        :param context: RPC Context.
        """
        return self.storage.count_tenants(context)

    @contextlib.contextmanager
    def create_domain(self, context, values):
        """
        Create a new Domain.

        :param context: RPC Context.
        :param values: Values to create the new Domain from.
        """
        self.storage.begin()

        try:
            domain = self.storage.create_domain(context, values)
            yield domain
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    def get_domain(self, context, domain_id):
        """
        Get a Domain via its ID.

        :param context: RPC Context.
        :param domain_id: ID of the Domain.
        """
        return self.storage.get_domain(context, domain_id)

    def find_domains(self, context, criterion=None, marker=None, limit=None,
                     sort_key=None, sort_dir=None):
        """
        Find Domains

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.find_domains(
            context, criterion, marker, limit, sort_key, sort_dir)

    def find_domain(self, context, criterion):
        """
        Find a single Domain.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.find_domain(context, criterion)

    @contextlib.contextmanager
    def update_domain(self, context, domain_id, values):
        """
        Update a Domain via ID.

        :param context: RPC Context.
        :param domain_id: Values to update the Domain with
        :param values: Values to update the Domain from.
        """
        self.storage.begin()

        try:
            domain = self.storage.update_domain(context, domain_id, values)
            yield domain
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    @contextlib.contextmanager
    def delete_domain(self, context, domain_id):
        """
        Delete a Domain

        :param context: RPC Context.
        :param domain_id: Domain ID to delete.
        """
        self.storage.begin()

        try:
            yield self.storage.get_domain(context, domain_id)
            self.storage.delete_domain(context, domain_id)
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    def count_domains(self, context, criterion=None):
        """
        Count domains

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.count_domains(context, criterion)

    @contextlib.contextmanager
    def create_recordset(self, context, domain_id, values):
        """
        Create a recordset on a given Domain ID

        :param context: RPC Context.
        :param domain_id: Domain ID to create the recordset in.
        :param values: Values to create the new RecordSet from.
        """
        self.storage.begin()

        try:
            recordset = self.storage.create_recordset(
                context, domain_id, values)
            yield recordset
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    def get_recordset(self, context, recordset_id):
        """
        Get a recordset via ID

        :param context: RPC Context.
        :param recordset_id: RecordSet ID to get
        """
        return self.storage.get_recordset(context, recordset_id)

    def find_recordsets(self, context, criterion=None, marker=None, limit=None,
                        sort_key=None, sort_dir=None):
        """
        Find RecordSets.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.find_recordsets(
            context, criterion, marker, limit, sort_key, sort_dir)

    def find_recordset(self, context, criterion=None):
        """
        Find a single RecordSet.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.find_recordset(context, criterion)

    @contextlib.contextmanager
    def update_recordset(self, context, recordset_id, values):
        """
        Update a recordset via ID

        :param context: RPC Context
        :param recordset_id: RecordSet ID to update
        """
        self.storage.begin()

        try:
            recordset = self.storage.update_recordset(
                context, recordset_id, values)
            yield recordset
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    @contextlib.contextmanager
    def delete_recordset(self, context, recordset_id):
        """
        Delete a recordset

        :param context: RPC Context
        :param recordset_id: RecordSet ID to delete
        """
        self.storage.begin()

        try:
            yield self.storage.get_recordset(context, recordset_id)
            self.storage.delete_recordset(context, recordset_id)
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    def count_recordsets(self, context, criterion=None):
        """
        Count recordsets

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.count_recordsets(context, criterion)

    @contextlib.contextmanager
    def create_record(self, context, domain_id, recordset_id, values):
        """
        Create a record on a given Domain ID

        :param context: RPC Context.
        :param domain_id: Domain ID to create the record in.
        :param recordset_id: RecordSet ID to create the record in.
        :param values: Values to create the new Record from.
        """
        self.storage.begin()

        try:
            record = self.storage.create_record(
                context, domain_id, recordset_id, values)
            yield record
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    def get_record(self, context, record_id):
        """
        Get a record via ID

        :param context: RPC Context.
        :param record_id: Record ID to get
        """
        return self.storage.get_record(context, record_id)

    def find_records(self, context, criterion=None, marker=None, limit=None,
                     sort_key=None, sort_dir=None):
        """
        Find Records.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.find_records(
            context, criterion, marker, limit, sort_key, sort_dir)

    def find_record(self, context, criterion=None):
        """
        Find a single Record.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.find_record(context, criterion)

    @contextlib.contextmanager
    def update_record(self, context, record_id, values):
        """
        Update a record via ID

        :param context: RPC Context
        :param record_id: Record ID to update
        """
        self.storage.begin()

        try:
            record = self.storage.update_record(context, record_id, values)
            yield record
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    @contextlib.contextmanager
    def delete_record(self, context, record_id):
        """
        Delete a record

        :param context: RPC Context
        :param record_id: Record ID to delete
        """
        self.storage.begin()

        try:
            yield self.storage.get_record(context, record_id)
            self.storage.delete_record(context, record_id)
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    def count_records(self, context, criterion=None):
        """
        Count records

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.count_records(context, criterion)

    @contextlib.contextmanager
    def create_blacklist(self, context, values):
        """
        Create a new Blacklisted Domain.

        :param context: RPC Context.
        :param values: Values to create the new Blacklist from.
        """
        self.storage.begin()

        try:
            blacklist = self.storage.create_blacklist(context, values)
            yield blacklist
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    def get_blacklist(self, context, blacklist_id):
        """
        Get a Blacklist via its ID.

        :param context: RPC Context.
        :param blacklist_id: ID of the Blacklisted Domain.
        """
        return self.storage.get_blacklist(context, blacklist_id)

    def find_blacklists(self, context, criterion=None, marker=None, limit=None,
                        sort_key=None, sort_dir=None):
        """
        Find all Blacklisted Domains

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.find_blacklists(
            context, criterion, marker, limit, sort_key, sort_dir)

    def find_blacklist(self, context, criterion):
        """
        Find a single Blacklisted Domain.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """
        return self.storage.find_blacklist(context, criterion)

    @contextlib.contextmanager
    def update_blacklist(self, context, blacklist_id, values):
        """
        Update a Blacklisted Domain via ID.

        :param context: RPC Context.
        :param blacklist_id: Values to update the Blacklist with
        :param values: Values to update the Blacklist from.
        """
        self.storage.begin()

        try:
            blacklist = self.storage.update_blacklist(context,
                                                      blacklist_id,
                                                      values)
            yield blacklist
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    @contextlib.contextmanager
    def delete_blacklist(self, context, blacklist_id):
        """
        Delete a Blacklisted Domain

        :param context: RPC Context.
        :param blacklist_id: Blacklist ID to delete.
        """
        self.storage.begin()

        try:
            yield self.storage.get_blacklist(context, blacklist_id)
            self.storage.delete_blacklist(context, blacklist_id)
        except Exception:
            with excutils.save_and_reraise_exception():
                self.storage.rollback()
        else:
            self.storage.commit()

    def ping(self, context):
        """ Ping the Storage connection """
        return self.storage.ping(context)

########NEW FILE########
__FILENAME__ = base
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import abc
from designate.plugin import DriverPlugin


class Storage(DriverPlugin):

    """ Base class for storage plugins """
    __metaclass__ = abc.ABCMeta
    __plugin_ns__ = 'designate.storage'
    __plugin_type__ = 'storage'

    @abc.abstractmethod
    def create_quota(self, context, values):
        """
        Create a Quota.

        :param context: RPC Context.
        :param values: Values to create the new Quota from.
        """

    @abc.abstractmethod
    def get_quota(self, context, quota_id):
        """
        Get a Quota via ID.

        :param context: RPC Context.
        :param quota_id: Quota ID to get.
        """

    @abc.abstractmethod
    def find_quotas(self, context, criterion=None, marker=None,
                    limit=None, sort_key=None, sort_dir=None):
        """
        Find Quotas

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        :param marker: Resource ID from which after the requested page will
                       start after
        :param limit: Integer limit of objects of the page size after the
                      marker
        :param sort_key: Key from which to sort after.
        :param sort_dir: Direction to sort after using sort_key.
        """

    @abc.abstractmethod
    def find_quota(self, context, criterion):
        """
        Find a single Quota.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """

    @abc.abstractmethod
    def update_quota(self, context, quota_id, values):
        """
        Update a Quota via ID

        :param context: RPC Context.
        :param quota_id: Quota ID to update.
        :param values: Values to update the Quota from
        """

    @abc.abstractmethod
    def delete_quota(self, context, quota_id):
        """
        Delete a Quota via ID.

        :param context: RPC Context.
        :param quota_id: Delete a Quota via ID
        """

    @abc.abstractmethod
    def create_server(self, context, values):
        """
        Create a Server.

        :param context: RPC Context.
        :param values: Values to create the new Domain from.
        """

    @abc.abstractmethod
    def find_servers(self, context, criterion=None, marker=None,
                     limit=None, sort_key=None, sort_dir=None):
        """
        Find Servers.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        :param marker: Resource ID from which after the requested page will
                       start after
        :param limit: Integer limit of objects of the page size after the
                      marker
        :param sort_key: Key from which to sort after.
        :param sort_dir: Direction to sort after using sort_key.
        """

    @abc.abstractmethod
    def get_server(self, context, server_id):
        """
        Get a Server via ID.

        :param context: RPC Context.
        :param server_id: Server ID to get.
        """

    @abc.abstractmethod
    def update_server(self, context, server_id, values):
        """
        Update a Server via ID

        :param context: RPC Context.
        :param server_id: Server ID to update.
        :param values: Values to update the Server from
        """

    @abc.abstractmethod
    def delete_server(self, context, server_id):
        """
        Delete a Server via ID.

        :param context: RPC Context.
        :param server_id: Delete a Server via ID
        """

    @abc.abstractmethod
    def create_tld(self, context, values):
        """
        Create a TLD.

        :param context: RPC Context.
        :param values: Values to create the new TLD from.
        """

    @abc.abstractmethod
    def get_tld(self, context, tld_id):
        """
        Get a TLD via ID.

        :param context: RPC Context.
        :param tld_id: TLD ID to get.
        """

    @abc.abstractmethod
    def find_tlds(self, context, criterion=None, marker=None,
                  limit=None, sort_key=None, sort_dir=None):
        """
        Find TLDs

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        :param marker: Resource ID from which after the requested page will
                       start after
        :param limit: Integer limit of objects of the page size after the
                      marker
        :param sort_key: Key from which to sort after.
        :param sort_dir: Direction to sort after using sort_key.
        """

    @abc.abstractmethod
    def find_tld(self, context, criterion):
        """
        Find a single TLD.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        :param marker: Resource ID from which after the requested page will
                       start after
        :param limit: Integer limit of objects of the page size after the
                      marker
        :param sort_key: Key from which to sort after.
        :param sort_dir: Direction to sort after using sort_key.
        """

    @abc.abstractmethod
    def update_tld(self, context, tld_id, values):
        """
        Update a TLD via ID

        :param context: RPC Context.
        :param tld_id: TLD ID to update.
        :param values: Values to update the TLD from
        """

    @abc.abstractmethod
    def delete_tld(self, context, tld_id):
        """
        Delete a TLD via ID.

        :param context: RPC Context.
        :param tld_id: Delete a TLD via ID
        """

    @abc.abstractmethod
    def create_tsigkey(self, context, values):
        """
        Create a TSIG Key.

        :param context: RPC Context.
        """

    @abc.abstractmethod
    def find_tsigkeys(self, context, criterion=None,
                      marker=None, limit=None, sort_key=None, sort_dir=None):
        """
        Find TSIG Keys.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        :param marker: Resource ID from which after the requested page will
                       start after
        :param limit: Integer limit of objects of the page size after the
                      marker
        :param sort_key: Key from which to sort after.
        :param sort_dir: Direction to sort after using sort_key.
        """

    @abc.abstractmethod
    def get_tsigkey(self, context, tsigkey_id):
        """
        Get a TSIG Key via ID.

        :param context: RPC Context.
        :param tsigkey_id: Server ID to get.
        """

    @abc.abstractmethod
    def update_tsigkey(self, context, tsigkey_id, values):
        """
        Update a TSIG Key via ID

        :param context: RPC Context.
        :param tsigkey_id: TSIG Key ID to update.
        :param values: Values to update the TSIG Key from
        """

    @abc.abstractmethod
    def delete_tsigkey(self, context, tsigkey_id):
        """
        Delete a TSIG Key via ID.

        :param context: RPC Context.
        :param tsigkey_id: Delete a TSIG Key via ID
        """

    @abc.abstractmethod
    def find_tenants(self, context):
        """
        Find all Tenants.

        :param context: RPC Context.
        """

    @abc.abstractmethod
    def get_tenant(self, context, tenant_id):
        """
        Get all Tenants.

        :param context: RPC Context.
        :param tenant_id: ID of the Tenant.
        """

    @abc.abstractmethod
    def count_tenants(self, context):
        """
        Count tenants

        :param context: RPC Context.
        """

    @abc.abstractmethod
    def create_domain(self, context, values):
        """
        Create a new Domain.

        :param context: RPC Context.
        :param values: Values to create the new Domain from.
        """

    @abc.abstractmethod
    def get_domain(self, context, domain_id):
        """
        Get a Domain via its ID.

        :param context: RPC Context.
        :param domain_id: ID of the Domain.
        """

    @abc.abstractmethod
    def find_domains(self, context, criterion=None, marker=None,
                     limit=None, sort_key=None, sort_dir=None):
        """
        Find Domains

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        :param marker: Resource ID from which after the requested page will
                       start after
        :param limit: Integer limit of objects of the page size after the
                      marker
        :param sort_key: Key from which to sort after.
        :param sort_dir: Direction to sort after using sort_key.
        """

    @abc.abstractmethod
    def find_domain(self, context, criterion):
        """
        Find a single Domain.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """

    @abc.abstractmethod
    def update_domain(self, context, domain_id, values):
        """
        Update a Domain via ID.

        :param context: RPC Context.
        :param domain_id: Values to update the Domain with
        :param values: Values to update the Domain from.
        """

    @abc.abstractmethod
    def delete_domain(self, context, domain_id):
        """
        Delete a Domain

        :param context: RPC Context.
        :param domain_id: Domain ID to delete.
        """

    @abc.abstractmethod
    def count_domains(self, context, criterion=None):
        """
        Count domains

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """

    @abc.abstractmethod
    def create_recordset(self, context, domain_id, values):
        """
        Create a recordset on a given Domain ID

        :param context: RPC Context.
        :param domain_id: Domain ID to create the recordset in.
        :param values: Values to create the new RecordSet from.
        """

    @abc.abstractmethod
    def get_recordset(self, context, recordset_id):
        """
        Get a recordset via ID

        :param context: RPC Context.
        :param recordset_id: RecordSet ID to get
        """

    @abc.abstractmethod
    def find_recordsets(self, context, criterion=None,
                        marker=None, limit=None, sort_key=None, sort_dir=None):
        """
        Find RecordSets.

        :param context: RPC Context.
        :param domain_id: Domain ID where the recordsets reside.
        :param criterion: Criteria to filter by.
        :param marker: Resource ID from which after the requested page will
                       start after
        :param limit: Integer limit of objects of the page size after the
                      marker
        :param sort_key: Key from which to sort after.
        :param sort_dir: Direction to sort after using sort_key.
        """

    @abc.abstractmethod
    def find_recordset(self, context, criterion):
        """
        Find a single RecordSet.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """

    @abc.abstractmethod
    def update_recordset(self, context, recordset_id, values):
        """
        Update a recordset via ID

        :param context: RPC Context
        :param recordset_id: RecordSet ID to update
        """

    @abc.abstractmethod
    def delete_recordset(self, context, recordset_id):
        """
        Delete a recordset

        :param context: RPC Context
        :param recordset_id: RecordSet ID to delete
        """

    @abc.abstractmethod
    def count_recordsets(self, context, criterion=None):
        """
        Count recordsets

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """

    @abc.abstractmethod
    def create_record(self, context, domain_id, recordset_id, values):
        """
        Create a record on a given Domain ID

        :param context: RPC Context.
        :param domain_id: Domain ID to create the record in.
        :param recordset_id: RecordSet ID to create the record in.
        :param values: Values to create the new Record from.
        """

    @abc.abstractmethod
    def get_record(self, context, record_id):
        """
        Get a record via ID

        :param context: RPC Context.
        :param record_id: Record ID to get
        """

    @abc.abstractmethod
    def find_records(self, context, criterion=None, marker=None,
                     limit=None, sort_key=None, sort_dir=None):
        """
        Find Records.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        :param marker: Resource ID from which after the requested page will
                       start after
        :param limit: Integer limit of objects of the page size after the
                      marker
        :param sort_key: Key from which to sort after.
        :param sort_dir: Direction to sort after using sort_key.
        """

    @abc.abstractmethod
    def find_record(self, context, criterion):
        """
        Find a single Record.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """

    @abc.abstractmethod
    def update_record(self, context, record_id, values):
        """
        Update a record via ID

        :param context: RPC Context
        :param record_id: Record ID to update
        """

    @abc.abstractmethod
    def delete_record(self, context, record_id):
        """
        Delete a record

        :param context: RPC Context
        :param record_id: Record ID to delete
        """

    @abc.abstractmethod
    def count_records(self, context, criterion=None):
        """
        Count records

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """

    @abc.abstractmethod
    def create_blacklist(self, context, values):
        """
        Create a Blacklist.

        :param context: RPC Context.
        :param values: Values to create the new Blacklist from.
        """

    @abc.abstractmethod
    def get_blacklist(self, context, blacklist_id):
        """
        Get a Blacklist via ID.

        :param context: RPC Context.
        :param blacklist_id: Blacklist ID to get.
        """

    @abc.abstractmethod
    def find_blacklists(self, context, criterion=None, marker=None,
                        limit=None, sort_key=None, sort_dir=None):
        """
        Find Blacklists

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        :param marker: Resource ID from which after the requested page will
                       start after
        :param limit: Integer limit of objects of the page size after the
                      marker
        :param sort_key: Key from which to sort after.
        :param sort_dir: Direction to sort after using sort_key.
        """

    @abc.abstractmethod
    def find_blacklist(self, context, criterion):
        """
        Find a single Blacklist.

        :param context: RPC Context.
        :param criterion: Criteria to filter by.
        """

    @abc.abstractmethod
    def update_blacklist(self, context, blacklist_id, values):
        """
        Update a Blacklist via ID

        :param context: RPC Context.
        :param blacklist_id: Blacklist ID to update.
        :param values: Values to update the Blacklist from
        """

    @abc.abstractmethod
    def delete_blacklist(self, context, blacklist_id):
        """
        Delete a Blacklist via ID.

        :param context: RPC Context.
        :param blacklist_id: Delete a Blacklist via ID
        """

    def ping(self, context):
        """ Ping the Storage connection """
        return {
            'status': None
        }

########NEW FILE########
__FILENAME__ = manage
#!/usr/bin/env python
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2012 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
# Author: Patrick Galbraith <patg@hp.com>
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from migrate.versioning.shell import main

if __name__ == '__main__':
    main(debug='False')

########NEW FILE########
__FILENAME__ = utils
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2012 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
# Author: Patrick Galbraith <patg@hp.com>
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Various conveniences used for migration scripts
"""
from sqlalchemy.schema import Table as SqlaTable
from designate.openstack.common import log as logging

LOG = logging.getLogger(__name__)


def create_tables(tables):
    for table in tables:
        LOG.debug("Creating table %s" % table)
        table.create()


def drop_tables(tables):
    for table in tables:
        LOG.debug("Dropping table %s" % table)
        table.drop()


def Table(*args, **kwargs):
    if 'mysql_engine' not in kwargs:
        kwargs['mysql_engine'] = 'INNODB'

    return SqlaTable(*args, **kwargs)

########NEW FILE########
__FILENAME__ = 001_add_moniker_schema
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2012 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
# Author: Patrick Galbraith <patg@hp.com>
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from sqlalchemy import ForeignKey, Enum, Integer, String, DateTime, Text
from sqlalchemy.schema import Column, MetaData
from designate.openstack.common import timeutils
from designate import utils
from designate.storage.impl_sqlalchemy.migrate_repo.utils import Table
from designate.storage.impl_sqlalchemy.migrate_repo.utils import create_tables
from designate.storage.impl_sqlalchemy.migrate_repo.utils import drop_tables
from designate.sqlalchemy.types import Inet
from designate.sqlalchemy.types import UUID

meta = MetaData()

RECORD_TYPES = ['A', 'AAAA', 'CNAME', 'MX', 'SRV', 'TXT', 'NS']

servers = Table('servers', meta,
                Column('id', UUID(), default=utils.generate_uuid,
                       primary_key=True),
                Column('created_at', DateTime(), default=timeutils.utcnow),
                Column('updated_at', DateTime(), onupdate=timeutils.utcnow),
                Column('version', Integer(), default=1, nullable=False),
                Column('name', String(255), nullable=False, unique=True),
                Column('ipv4', Inet(), nullable=False, unique=True),
                Column('ipv6', Inet(), default=None, unique=True))

domains = Table('domains', meta,
                Column('id', UUID(), default=utils.generate_uuid,
                       primary_key=True),
                Column('created_at', DateTime(), default=timeutils.utcnow),
                Column('updated_at', DateTime(), onupdate=timeutils.utcnow),
                Column('version', Integer(), default=1, nullable=False),
                Column('tenant_id', String(36), default=None, nullable=True),
                Column('name', String(255), nullable=False, unique=True),
                Column('email', String(36), nullable=False),
                Column('ttl', Integer(), default=3600, nullable=False),
                Column('refresh', Integer(), default=3600, nullable=False),
                Column('retry', Integer(), default=3600, nullable=False),
                Column('expire', Integer(), default=3600, nullable=False),
                Column('minimum', Integer(), default=3600, nullable=False))

records = Table('records', meta,
                Column('id', UUID(), default=utils.generate_uuid,
                       primary_key=True),
                Column('created_at', DateTime(), default=timeutils.utcnow),
                Column('updated_at', DateTime(), onupdate=timeutils.utcnow),
                Column('version', Integer(), default=1, nullable=False),
                Column('type', Enum(name='record_types', *RECORD_TYPES),
                       nullable=False),
                Column('name', String(255), nullable=False),
                Column('data', Text(), nullable=False),
                Column('priority', Integer(), default=None),
                Column('ttl', Integer(), default=3600, nullable=False),
                Column('domain_id', UUID(), ForeignKey('domains.id'),
                       nullable=False))


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    tables = [domains, servers, records, ]
    create_tables(tables)


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    tables = [domains, servers, records, ]
    drop_tables(tables)

########NEW FILE########
__FILENAME__ = 002_add_managed_col_for_records
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import MetaData, Table, Column, Boolean, Unicode
from designate.sqlalchemy.types import UUID

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    records_table = Table('records', meta, autoload=True)

    managed_resource = Column('managed_resource', Boolean(), default=False)
    managed_resource.create(records_table, populate_default=True)

    managed_resource_type = Column('managed_resource_type', Unicode(50),
                                   default=None, nullable=True)
    managed_resource_type.create(records_table, populate_default=True)

    managed_resource_id = Column('managed_resource_id', UUID(), default=None,
                                 nullable=True)
    managed_resource_id.create(records_table, populate_default=True)


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    records_table = Table('records', meta, autoload=True)

    managed_resource_id = Column('managed_resource_id', UUID(), default=None,
                                 nullable=True)
    managed_resource_id.drop(records_table)

    managed_resource_type = Column('managed_resource_type', Unicode(50),
                                   default=None, nullable=True)
    managed_resource_type.drop(records_table)

    managed_resource = Column('managed_resource', Boolean(), default=False)
    managed_resource.drop(records_table)

########NEW FILE########
__FILENAME__ = 003_add_delete_on_cascade
# Copyright 2012 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
# Author: Patrick Galbraith <patg@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import MetaData, Table
from migrate import ForeignKeyConstraint

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    dialect = migrate_engine.url.get_dialect().name

    records = Table('records', meta, autoload=True)
    domains = Table('domains', meta, autoload=True)

    # add foreignkey if not sqlite
    if not dialect.startswith('sqlite'):
        if dialect.startswith('mysql'):
            ForeignKeyConstraint(columns=[records.c.domain_id],
                                 refcolumns=[domains.c.id],
                                 name='records_ibfk_1').drop()
        else:
            ForeignKeyConstraint(columns=[records.c.domain_id],
                                 refcolumns=[domains.c.id]).drop()

        ForeignKeyConstraint(columns=[records.c.domain_id],
                             refcolumns=[domains.c.id],
                             ondelete='CASCADE',
                             name='fkey_records_domain_id').create()


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    dialect = migrate_engine.url.get_dialect().name

    records = Table('records', meta, autoload=True)
    domains = Table('domains', meta, autoload=True)

    # add foreignkey if not sqlite
    if not dialect.startswith('sqlite'):
        ForeignKeyConstraint(columns=[records.c.domain_id],
                             refcolumns=[domains.c.id]).drop()
        ForeignKeyConstraint(columns=[records.c.domain_id],
                             refcolumns=[domains.c.id]).create()

########NEW FILE########
__FILENAME__ = 004_changed_managed_cols_for_records
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import MetaData, Table, Column, Unicode

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    records_table = Table('records', meta, autoload=True)

    records_table.c.managed_resource.alter(name='managed')

    managed_plugin_name = Column('managed_plugin_name', Unicode(50))
    managed_plugin_name.create(records_table, populate_default=True)

    managed_plugin_type = Column('managed_plugin_type', Unicode(50))
    managed_plugin_type.create(records_table, populate_default=True)


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    records_table = Table('records', meta, autoload=True)

    records_table.c.managed.alter(name='managed_resource')

    managed_plugin_name = Column('managed_resource_name', Unicode(50))
    managed_plugin_name.drop(records_table)

    managed_plugin_type = Column('managed_resource_type', Unicode(50))
    managed_plugin_type.drop(records_table)

########NEW FILE########
__FILENAME__ = 005_records_allow_null_ttl
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import MetaData, Table

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    records_table = Table('records', meta, autoload=True)
    records_table.c.ttl.alter(nullable=True, default=None)


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    records_table = Table('records', meta, autoload=True)
    records_table.c.ttl.alter(nullable=False, default=3600)

########NEW FILE########
__FILENAME__ = 006_support_ptr_records
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import MetaData, Table, Enum

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    RECORD_TYPES = ['A', 'AAAA', 'CNAME', 'MX', 'SRV', 'TXT', 'NS', 'PTR']

    records_table = Table('records', meta, autoload=True)
    records_table.c.type.alter(type=Enum(name='record_types', *RECORD_TYPES))


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    RECORD_TYPES = ['A', 'AAAA', 'CNAME', 'MX', 'SRV', 'TXT', 'NS']

    records_table = Table('records', meta, autoload=True)

    # Delete all PTR records
    records_table.filter_by(type='PTR').delete()

    # Remove PTR from the ENUM
    records_table.c.type.alter(type=Enum(name='record_types', *RECORD_TYPES))

########NEW FILE########
__FILENAME__ = 007_add_parent_domain_id_col
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import MetaData, Table, Column, ForeignKey
from designate.sqlalchemy.types import UUID

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    domains_table = Table('domains', meta, autoload=True)

    parent_domain_id = Column('parent_domain_id',
                              UUID,
                              ForeignKey('domains.id'),
                              default=None,
                              nullable=True)
    parent_domain_id.create(domains_table, populate_default=True)


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    domains_table = Table('domains', meta, autoload=True)

    parent_domain_id = Column('parent_domain_id',
                              UUID,
                              ForeignKey('domains.id'),
                              default=None,
                              nullable=True)
    parent_domain_id.drop(domains_table)

########NEW FILE########
__FILENAME__ = 008_support_spf_records
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import MetaData, Table, Enum

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    RECORD_TYPES = ['A', 'AAAA', 'CNAME', 'MX', 'SRV', 'TXT', 'SPF', 'NS',
                    'PTR']

    records_table = Table('records', meta, autoload=True)
    records_table.c.type.alter(type=Enum(name='record_types', *RECORD_TYPES))


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    RECORD_TYPES = ['A', 'AAAA', 'CNAME', 'MX', 'SRV', 'TXT', 'NS', 'PTR']

    records_table = Table('records', meta, autoload=True)

    # Delete all SPF records
    records_table.filter_by(type='SPF').delete()

    # Remove SPF from the ENUM
    records_table.c.type.alter(type=Enum(name='record_types', *RECORD_TYPES))

########NEW FILE########
__FILENAME__ = 009_add_tsigkeys_table
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import Enum, Integer, String, DateTime
from sqlalchemy.schema import Table, Column, MetaData
from designate.openstack.common import timeutils
from designate import utils
from designate.sqlalchemy.types import UUID


meta = MetaData()

TSIG_ALGORITHMS = ['hmac-md5', 'hmac-sha1', 'hmac-sha224', 'hmac-sha256',
                   'hmac-sha384', 'hmac-sha512']

tsigkeys = Table('tsigkeys', meta,
                 Column('id', UUID(), default=utils.generate_uuid,
                        primary_key=True),
                 Column('created_at', DateTime(), default=timeutils.utcnow),
                 Column('updated_at', DateTime(), onupdate=timeutils.utcnow),
                 Column('version', Integer(), default=1, nullable=False),
                 Column('name', String(255), nullable=False, unique=True),
                 Column('algorithm', Enum(name='tsig_algorithms',
                                          *TSIG_ALGORITHMS),
                        nullable=False),
                 Column('secret', String(255), nullable=False))


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    tsigkeys.create()


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    tsigkeys.drop()

########NEW FILE########
__FILENAME__ = 010_drop_server_ip_cols
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import MetaData, Table, Column
from designate.sqlalchemy.types import Inet

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    servers_table = Table('servers', meta, autoload=True)

    servers_table.c.ipv4.drop()
    servers_table.c.ipv6.drop()


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    servers_table = Table('servers', meta, autoload=True)

    ipv4 = Column('ipv4', Inet(), nullable=False, unique=True)
    ipv6 = Column('ipv6', Inet(), default=None, unique=True)

    ipv4.create(servers_table, populate_default=True)
    ipv6.create(servers_table, populate_default=True)

########NEW FILE########
__FILENAME__ = 011_support_sshfp_records
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import MetaData, Table, Enum

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    RECORD_TYPES = ['A', 'AAAA', 'CNAME', 'MX', 'SRV', 'TXT', 'SPF', 'NS',
                    'PTR', 'SSHFP']

    records_table = Table('records', meta, autoload=True)
    records_table.c.type.alter(type=Enum(name='record_types', *RECORD_TYPES))


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    RECORD_TYPES = ['A', 'AAAA', 'CNAME', 'MX', 'SRV', 'TXT', 'SPF', 'NS',
                    'PTR']

    records_table = Table('records', meta, autoload=True)

    # Delete all SSHFP records
    records_table.filter_by(type='SSHFP').delete()

    # Remove SSHFP from the ENUM
    records_table.c.type.alter(type=Enum(name='record_types', *RECORD_TYPES))

########NEW FILE########
__FILENAME__ = 012_add_serial_col
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.openstack.common import timeutils
from designate.openstack.common import log as logging
from sqlalchemy import MetaData, Table, Column, Integer

LOG = logging.getLogger(__name__)
meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    domains_table = Table('domains', meta, autoload=True)

    serial = Column('serial', Integer(), default=timeutils.utcnow_ts,
                    nullable=False, server_default="1")
    serial.create(domains_table, populate_default=True)

    # Do we have any domains?
    domain_count = domains_table.count().execute().first()[0]

    if domain_count > 0:
        LOG.warn('A sync-domains is now required in order for the API '
                 'provided, and backend provided serial numbers to align')


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    domains_table = Table('domains', meta, autoload=True)
    domains_table.c.serial.drop()

########NEW FILE########
__FILENAME__ = 013_expand_email_column
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import MetaData, Table, String

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    domains_table = Table('domains', meta, autoload=True)
    domains_table.c.email.alter(type=String(255))


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    domains_table = Table('domains', meta, autoload=True)
    domains_table.c.email.alter(type=String(36))

########NEW FILE########
__FILENAME__ = 014_add_quotas_table
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import Integer, String, DateTime, UniqueConstraint
from sqlalchemy.schema import Table, Column, MetaData
from designate.openstack.common import timeutils
from designate import utils
from designate.sqlalchemy.types import UUID


meta = MetaData()

quotas = Table('quotas', meta,
               Column('id', UUID(), default=utils.generate_uuid,
                      primary_key=True),
               Column('created_at', DateTime(), default=timeutils.utcnow),
               Column('updated_at', DateTime(), onupdate=timeutils.utcnow),
               Column('version', Integer(), default=1, nullable=False),
               Column('tenant_id', String(36), nullable=False),
               Column('resource', String(32), nullable=False),
               Column('hard_limit', Integer(), nullable=False),
               UniqueConstraint('tenant_id', 'resource', name='unique_quota'))


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    quotas.create()


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    quotas.drop()

########NEW FILE########
__FILENAME__ = 015_add_unique_record_constraint
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import hashlib
from sqlalchemy.exc import IntegrityError
from sqlalchemy.schema import Table, Column, MetaData
from sqlalchemy.types import String
from designate.openstack.common import log as logging

LOG = logging.getLogger(__name__)
meta = MetaData()


def _build_hash(r):
    md5 = hashlib.md5()
    md5.update("%s:%s:%s:%s:%s" % (r.domain_id, r.name, r.type, r.data,
                                   r.priority))
    return md5.hexdigest()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    records_table = Table('records', meta, autoload=True)

    # Add the hash column, start with allowing NULLs
    hash_column = Column('hash', String(32), nullable=True, default=None,
                         unique=True)
    hash_column.create(records_table, unique_name='unique_record')

    sync_domains = []

    # Fill out the hash values. We need to do this in a way that lets us track
    # which domains need to be re-synced, so having the DB do this directly
    # won't work.
    for record in records_table.select().execute():
        try:
            records_table.update()\
                         .where(records_table.c.id == record.id)\
                         .values(hash=_build_hash(record))\
                         .execute()
        except IntegrityError:
            if record.domain_id not in sync_domains:
                sync_domains.append(record.domain_id)
                LOG.warn("Domain '%s' needs to be synchronised" %
                         record.domain_id)

            records_table.delete()\
                         .where(records_table.c.id == record.id)\
                         .execute()

    # Finally, the column should not be nullable.
    records_table.c.hash.alter(nullable=False)


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    records_table = Table('records', meta, autoload=True)

    hash_column = Column('hash', String(32), nullable=False, unique=True)
    hash_column.drop(records_table)

########NEW FILE########
__FILENAME__ = 016_add_deleted_columns
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import MetaData, Table, Column, DateTime
from sqlalchemy.types import CHAR
from migrate.changeset.constraint import UniqueConstraint

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    domains_table = Table('domains', meta, autoload=True)

    # Create the new columns
    deleted_column = Column('deleted', CHAR(32), nullable=False, default="0",
                            server_default="0")
    deleted_column.create(domains_table, populate_default=True)

    deleted_at_column = Column('deleted_at', DateTime, nullable=True,
                               default=None)
    deleted_at_column.create(domains_table, populate_default=True)

    # Drop the old single column unique
    # NOTE(kiall): It appears this does nothing. Miration 17 has been added.
    #              leaving this here for reference.
    domains_table.c.name.alter(unique=False)

    # Add a new multi-column unique index
    constraint = UniqueConstraint('name', 'deleted', name='unique_domain_name',
                                  table=domains_table)
    constraint.create()


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    domains_table = Table('domains', meta, autoload=True)

    # Drop the multi-column unique index
    constraint = UniqueConstraint('name', 'deleted', name='unique_domain_name',
                                  table=domains_table)
    constraint.drop()

    # Revert to single column unique
    # NOTE(kiall): It appears this does nothing. Miration 17 has been added.
    #              leaving this here for reference.
    domains_table.c.name.alter(unique=True)

    # Drop the deleted columns
    deleted_column = Column('deleted', CHAR(32), nullable=True, default=None)
    deleted_column.drop(domains_table)

    deleted_at_column = Column('deleted_at', DateTime, nullable=True,
                               default=None)
    deleted_at_column.drop(domains_table)

########NEW FILE########
__FILENAME__ = 017_drop_unique_domain_name_index
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import logging
from sqlalchemy import MetaData, Table, Column, String
from sqlalchemy.sql import update
from migrate.changeset.constraint import UniqueConstraint

LOG = logging.getLogger(__name__)
meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    dialect = migrate_engine.url.get_dialect().name

    domains_table = Table('domains', meta, autoload=True)

    if dialect.startswith('sqlite'):
        # SQLite can't drop a constraint. Yay. This will be fun..

        # Create a new name column without the unique index
        name_tmp_column = Column('name_tmp', String(255))
        name_tmp_column.create(domains_table)

        # Copy the data over.
        query = update(domains_table).values(name_tmp=domains_table.c.name)
        migrate_engine.execute(query)

        # Delete the name column
        domains_table.c.name.drop()

        # Rename the name_tmp column to name
        domains_table.c.name_tmp.alter(name='name')
    elif dialect.startswith('postgresql'):
        constraint = UniqueConstraint('name', name='domains_name_key',
                                      table=domains_table)
        constraint.drop()
    else:
        constraint = UniqueConstraint('name', name='name', table=domains_table)
        constraint.drop()


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    domains_table = Table('domains', meta, autoload=True)

    constraint = UniqueConstraint('name', name='name', table=domains_table)
    constraint.create()

########NEW FILE########
__FILENAME__ = 018_add_back_unique_name_deleted_sqlite
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Patrick Galbraith <patg@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import logging
from sqlalchemy import MetaData, Table
from migrate.changeset.constraint import UniqueConstraint

LOG = logging.getLogger(__name__)
meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    dialect = migrate_engine.url.get_dialect().name

    if dialect.startswith('sqlite'):
        domains_table = Table('domains', meta, autoload=True)
        servers_table = Table('servers', meta, autoload=True)

        # Add missing multi-column unique index
        constraint = UniqueConstraint('name', 'deleted',
                                      name='unique_domain_name',
                                      table=domains_table)
        constraint.create()

        # Add a missing unique index
        constraint = UniqueConstraint('name',
                                      name='unique_server_name',
                                      table=servers_table)
        constraint.create()


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    dialect = migrate_engine.url.get_dialect().name

    if dialect.startswith('sqlite'):
        domains_table = Table('domains', meta, autoload=True)
        servers_table = Table('servers', meta, autoload=True)

        # Add a new multi-column unique index
        constraint = UniqueConstraint('name', 'deleted',
                                      name='unique_domain_name',
                                      table=domains_table)
        constraint.drop()

        # Add a missing unique index
        constraint = UniqueConstraint('name',
                                      name='unique_server_name',
                                      table=servers_table)
        constraint.drop()

########NEW FILE########
__FILENAME__ = 019_fix_deleted_column
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import MetaData, Table

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    domains_table = Table('domains', meta, autoload=True)

    # Aleady deleted domains will have an incorrect value in the deleted
    # column - fix them up.
    domains_table.update().where(domains_table.c.deleted != '0')\
                          .values(deleted=domains_table.c.id)\
                          .execute()


def downgrade(migrate_engine):
    pass

########NEW FILE########
__FILENAME__ = 020_add_description_domains_records
# Copyright 2013 Rackspace Hosting
#
# Author: Tim Simmons <tim.simmons@rackspace.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.openstack.common import log as logging
from sqlalchemy import MetaData, Table, Column, Unicode

LOG = logging.getLogger(__name__)
meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    domains_table = Table('domains', meta, autoload=True)
    records_table = Table('records', meta, autoload=True)

    #Add in description columns in domain/record databases
    domain_description = Column('description', Unicode(160),
                                nullable=True)
    domain_description.create(domains_table, populate_default=True)

    record_description = Column('description', Unicode(160),
                                nullable=True)
    record_description.create(records_table, populate_default=True)


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    domains_table = Table('domains', meta, autoload=True)
    domains_table.c.description.drop()

    record_table = Table('records', meta, autoload=True)
    record_table.c.description.drop()

########NEW FILE########
__FILENAME__ = 021_convert_to_innodb_utf8
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy.schema import MetaData

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    if migrate_engine.name == "mysql":
        tables = ['domains', 'quotas', 'records', 'servers', 'tsigkeys']

        sql = "SET foreign_key_checks = 0;"

        for table in tables:
            sql += "ALTER TABLE %s ENGINE=InnoDB;" % table
            sql += "ALTER TABLE %s CONVERT TO CHARACTER SET utf8;" % table

        sql += "SET foreign_key_checks = 1;"
        sql += "ALTER DATABASE %s DEFAULT CHARACTER SET utf8;" \
            % migrate_engine.url.database

        migrate_engine.execute(sql)


def downgrade(migrate_engine):
    # utf8/InnoDB tables are backward compatible.. No need to revert.
    pass

########NEW FILE########
__FILENAME__ = 022_add_domain_status
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Graham Hayes <graham.hayes@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.openstack.common import log as logging
from sqlalchemy import MetaData, Table, Column, Enum

LOG = logging.getLogger(__name__)
RESOURCE_STATUSES = ['ACTIVE', 'PENDING', 'DELETED']
meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    domains_table = Table('domains', meta, autoload=True)
    records_table = Table('records', meta, autoload=True)

    # Add a domain & record creation status for async backends
    domain_statuses = Enum(name='domain_statuses', metadata=meta,
                           *RESOURCE_STATUSES)
    domain_statuses.create()

    record_statuses = Enum(name='record_statuses', metadata=meta,
                           *RESOURCE_STATUSES)
    record_statuses.create()

    domain_status = Column('status', domain_statuses, nullable=False,
                           server_default='ACTIVE', default='ACTIVE')

    record_status = Column('status', record_statuses, nullable=False,
                           server_default='ACTIVE', default='ACTIVE')

    domain_status.create(domains_table, populate_default=True)
    record_status.create(records_table, populate_default=True)


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    domains_table = Table('domains', meta, autoload=True)
    records_table = Table('records', meta, autoload=True)

    domains_table.c.status.drop()
    records_table.c.status.drop()

########NEW FILE########
__FILENAME__ = 023_placeholder
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# This is a placeholder for Havana backports.
# Do not use this number for new Icehouse work. New Icehouse work starts after
# all the placeholders.
#
# See https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations
# http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html


def upgrade(migrate_engine):
    pass


def downgrade(migration_engine):
    pass

########NEW FILE########
__FILENAME__ = 024_placeholder
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# This is a placeholder for Havana backports.
# Do not use this number for new Icehouse work. New Icehouse work starts after
# all the placeholders.
#
# See https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations
# http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html


def upgrade(migrate_engine):
    pass


def downgrade(migration_engine):
    pass

########NEW FILE########
__FILENAME__ = 025_placeholder
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# This is a placeholder for Havana backports.
# Do not use this number for new Icehouse work. New Icehouse work starts after
# all the placeholders.
#
# See https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations
# http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html


def upgrade(migrate_engine):
    pass


def downgrade(migration_engine):
    pass

########NEW FILE########
__FILENAME__ = 026_placeholder
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# This is a placeholder for Havana backports.
# Do not use this number for new Icehouse work. New Icehouse work starts after
# all the placeholders.
#
# See https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations
# http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html


def upgrade(migrate_engine):
    pass


def downgrade(migration_engine):
    pass

########NEW FILE########
__FILENAME__ = 027_placeholder
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# This is a placeholder for Havana backports.
# Do not use this number for new Icehouse work. New Icehouse work starts after
# all the placeholders.
#
# See https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations
# http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html


def upgrade(migrate_engine):
    pass


def downgrade(migration_engine):
    pass

########NEW FILE########
__FILENAME__ = 028_placeholder
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# This is a placeholder for Havana backports.
# Do not use this number for new Icehouse work. New Icehouse work starts after
# all the placeholders.
#
# See https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations
# http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html


def upgrade(migrate_engine):
    pass


def downgrade(migration_engine):
    pass

########NEW FILE########
__FILENAME__ = 029_placeholder
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# This is a placeholder for Havana backports.
# Do not use this number for new Icehouse work. New Icehouse work starts after
# all the placeholders.
#
# See https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations
# http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html


def upgrade(migrate_engine):
    pass


def downgrade(migration_engine):
    pass

########NEW FILE########
__FILENAME__ = 030_placeholder
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# This is a placeholder for Havana backports.
# Do not use this number for new Icehouse work. New Icehouse work starts after
# all the placeholders.
#
# See https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations
# http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html


def upgrade(migrate_engine):
    pass


def downgrade(migration_engine):
    pass

########NEW FILE########
__FILENAME__ = 031_placeholder
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# This is a placeholder for Havana backports.
# Do not use this number for new Icehouse work. New Icehouse work starts after
# all the placeholders.
#
# See https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations
# http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html


def upgrade(migrate_engine):
    pass


def downgrade(migration_engine):
    pass

########NEW FILE########
__FILENAME__ = 032_placeholder
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# This is a placeholder for Havana backports.
# Do not use this number for new Icehouse work. New Icehouse work starts after
# all the placeholders.
#
# See https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations
# http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html


def upgrade(migrate_engine):
    pass


def downgrade(migration_engine):
    pass

########NEW FILE########
__FILENAME__ = 033_add_record_tenant_id
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.openstack.common import log as logging
from sqlalchemy import MetaData, Table, Column, String
from sqlalchemy.sql import select

LOG = logging.getLogger(__name__)
meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    domains_table = Table('domains', meta, autoload=True)
    records_table = Table('records', meta, autoload=True)

    # Add the tenant_id column
    tenant_id = Column('tenant_id', String(36), default=None, nullable=True)
    tenant_id.create(records_table, populate_default=True)

    # Populate the tenant_id column
    inner_select = select([domains_table.c.tenant_id])\
        .where(domains_table.c.id == records_table.c.domain_id)\
        .as_scalar()

    records_table.update()\
        .values(tenant_id=inner_select)\
        .execute()


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    records_table = Table('records', meta, autoload=True)

    # Drop the tenant_id column
    records_table.c.tenant_id.drop()

########NEW FILE########
__FILENAME__ = 034_add_recordsets_table
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import hashlib
from sqlalchemy import ForeignKey, Enum, Integer, String, DateTime, Unicode
from sqlalchemy import func
from sqlalchemy.sql import select
from sqlalchemy.schema import Table, Column, MetaData
from migrate import ForeignKeyConstraint
from migrate.changeset.constraint import UniqueConstraint
from designate.openstack.common import timeutils
from designate import utils
from designate.sqlalchemy.types import UUID


RECORD_TYPES = ['A', 'AAAA', 'CNAME', 'MX', 'SRV', 'TXT', 'SPF', 'NS', 'PTR',
                'SSHFP']

meta = MetaData()

recordsets_table = Table(
    'recordsets',
    meta,

    Column('id', UUID(), default=utils.generate_uuid, primary_key=True),
    Column('created_at', DateTime(), default=timeutils.utcnow),
    Column('updated_at', DateTime(), onupdate=timeutils.utcnow),
    Column('version', Integer(), default=1, nullable=False),

    Column('tenant_id', String(36), default=None, nullable=True),
    Column('domain_id', UUID, ForeignKey('domains.id'), nullable=False),
    Column('name', String(255), nullable=False),
    Column('type', Enum(name='recordset_types', *RECORD_TYPES),
           nullable=False),
    Column('ttl', Integer, default=None, nullable=True),
    Column('description', Unicode(160), nullable=True),

    UniqueConstraint('domain_id', 'name', 'type', name='unique_recordset'),

    mysql_engine='INNODB',
    mysql_charset='utf8')


def _build_hash(recordset_id, record):
    md5 = hashlib.md5()
    md5.update("%s:%s:%s" % (recordset_id, record.data, record.priority))

    return md5.hexdigest()


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    records_table = Table('records', meta, autoload=True)

    # We need to autoload the domains table for the FK to succeed.
    Table('domains', meta, autoload=True)

    # Prepare an empty dict to cache (domain_id, name, type) tuples to
    # RRSet id's
    cache = {}

    # Create the recordsets_table table
    recordsets_table.create()

    # NOTE(kiall): Since we need a unique UUID for each recordset, and need
    #              to maintain cross DB compatibility, we're stuck doing this
    #              in code rather than an
    #              INSERT INTO recordsets_table SELECT (..) FROM records;
    results = select(
        columns=[
            records_table.c.tenant_id,
            records_table.c.domain_id,
            records_table.c.name,
            records_table.c.type,
            func.min(records_table.c.ttl).label('ttl'),
            func.min(records_table.c.created_at).label('created_at'),
            func.max(records_table.c.updated_at).label('updated_at')
        ],
        group_by=[
            records_table.c.tenant_id,
            records_table.c.domain_id,
            records_table.c.name,
            records_table.c.type
        ]
    ).execute()

    for result in results:
        # Create the new RecordSet and remember it's id
        pk = recordsets_table.insert().execute(
            tenant_id=result.tenant_id,
            domain_id=result.domain_id,
            name=result.name,
            type=result.type,
            ttl=result.ttl,
            created_at=result.created_at,
            updated_at=result.updated_at
        ).inserted_primary_key[0]

        # Cache the ID for later
        cache_key = "%s.%s.%s" % (result.domain_id, result.name, result.type)
        cache[cache_key] = pk

    # Add the recordset column to the records table
    record_recordset_id = Column('recordset_id', UUID,
                                 default=None,
                                 nullable=True)
    record_recordset_id.create(records_table, populate_default=True)

    # Fetch all the records
    # TODO(kiall): Batch this..
    results = select(
        columns=[
            records_table.c.id,
            records_table.c.domain_id,
            records_table.c.name,
            records_table.c.type,
            records_table.c.data,
            records_table.c.priority
        ]
    ).execute()

    # Update each result with the approperiate recordset_id, and refresh
    # the hash column to reflect the removal of several fields.
    for result in results:
        cache_key = "%s.%s.%s" % (result.domain_id, result.name,
                                  result.type)

        recordset_id = cache[cache_key]
        new_hash = _build_hash(recordset_id, result)

        records_table.update()\
            .where(records_table.c.id == result.id)\
            .values(recordset_id=cache[cache_key], hash=new_hash)\
            .execute()

    # Now that the records.recordset_id field is populated, lets ensure the
    # column is not nullable and is a FK to the records table.
    records_table.c.recordset_id.alter(nullable=False)
    ForeignKeyConstraint(columns=[records_table.c.recordset_id],
                         refcolumns=[recordsets_table.c.id],
                         ondelete='CASCADE',
                         name='fkey_records_recordset_id').create()

    # Finally, drop the now-defunct columns from the records table
    records_table.c.name.drop()
    records_table.c.type.drop()
    records_table.c.ttl.drop()


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    raise Exception('There is no undo')

########NEW FILE########
__FILENAME__ = 035_add_unique_record_sqlite
# Copyright 2014 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import logging
from sqlalchemy import MetaData, Table
from migrate.changeset.constraint import UniqueConstraint

LOG = logging.getLogger(__name__)
meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    dialect = migrate_engine.url.get_dialect().name

    if dialect.startswith('sqlite'):
        records_table = Table('records', meta, autoload=True)

        # Add missing unique index
        constraint = UniqueConstraint('hash',
                                      name='unique_recordset',
                                      table=records_table)
        constraint.create()


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    dialect = migrate_engine.url.get_dialect().name

    if dialect.startswith('sqlite'):
        records_table = Table('records', meta, autoload=True)

        # Drop the unique index
        constraint = UniqueConstraint('hash',
                                      name='unique_recordset',
                                      table=records_table)
        constraint.drop()

########NEW FILE########
__FILENAME__ = 036_add_managed_tenant_and_region_and_extra
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Endre Karlson <endre.karlson@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# This is a placeholder for Havana backports.
# Do not use this number for new Icehouse work. New Icehouse work starts after
# all the placeholders.
#
# See https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations
# http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html
from designate.openstack.common import log as logging
from sqlalchemy import MetaData, Table, Column, Unicode


LOG = logging.getLogger(__name__)
meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    records_table = Table('records', meta, autoload=True)
    record_managed_tenant_id = Column(
        'managed_tenant_id', Unicode(36), default=None, nullable=True)
    record_managed_tenant_id.create(records_table, populate_default=True)

    record_managed_resource_region = Column(
        'managed_resource_region', Unicode(100), default=None, nullable=True)
    record_managed_resource_region.create(records_table, populate_default=True)

    record_managed_extra = Column(
        'managed_extra', Unicode(100), default=None, nullable=True)
    record_managed_extra.create(records_table, populate_default=True)


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    records_table = Table('records', meta, autoload=True)

    record_managed_tenant_id = Column(
        'managed_tenant_id', Unicode(36), default=None, nullable=True)
    record_managed_tenant_id.drop(records_table)

    record_managed_resource_region = Column(
        'managed_resource_region', Unicode(100), default=None, nullable=True)
    record_managed_resource_region.drop(records_table)

    record_extra = Column(
        'managed_extra', Unicode(100), default=None, nullable=True)
    record_extra.drop(records_table)

########NEW FILE########
__FILENAME__ = 037_add_tlds_table
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from sqlalchemy import Integer, String, DateTime, Unicode
from sqlalchemy.schema import Table, Column, MetaData
from designate.openstack.common import timeutils
from designate import utils
from designate.sqlalchemy.types import UUID
from designate.openstack.common import log as logging

LOG = logging.getLogger(__name__)
meta = MetaData()

tlds_table = Table(
    'tlds',
    meta,

    Column('id', UUID(), default=utils.generate_uuid, primary_key=True),
    Column('created_at', DateTime(), default=timeutils.utcnow),
    Column('updated_at', DateTime(), onupdate=timeutils.utcnow),
    Column('version', Integer(), default=1, nullable=False),

    Column('name', String(255), nullable=False, unique=True),
    Column('description', Unicode(160), nullable=True),

    mysql_engine='INNODB',
    mysql_charset='utf8')


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    tlds_table.create()


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    tlds_table.drop()

########NEW FILE########
__FILENAME__ = 038_add_blacklists_table
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
# Author: Betsy Luzader <betsy.luzader@rackspace.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from sqlalchemy import Integer, String, DateTime
from sqlalchemy.schema import Table, Column, MetaData
from designate.openstack.common import timeutils
from designate import utils
from designate.sqlalchemy.types import UUID

meta = MetaData()

blacklists = Table(
    'blacklists',
    meta,
    Column('id', UUID(), default=utils.generate_uuid,
           primary_key=True),
    Column('created_at', DateTime(),
           default=timeutils.utcnow),
    Column('updated_at', DateTime(),
           onupdate=timeutils.utcnow),
    Column('version', Integer(), default=1,
           nullable=False),
    Column('pattern', String(255), nullable=False,
           unique=True),
    Column('description', String(160),
           nullable=True),

    mysql_engine='INNODB',
    mysql_charset='utf8')


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    blacklists.create()


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    blacklists.drop()

########NEW FILE########
__FILENAME__ = models
# Copyright 2012 Hewlett-Packard Development Company, L.P.
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
# Modified: Patrick Galbraith <patg@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import hashlib
from oslo.config import cfg
from sqlalchemy import (Column, DateTime, String, Text, Integer, ForeignKey,
                        Enum, Boolean, Unicode, UniqueConstraint, event)
from sqlalchemy.orm import relationship, backref
from designate.openstack.common import log as logging
from designate.openstack.common import timeutils
from designate.sqlalchemy.types import UUID
from designate.sqlalchemy.models import Base as CommonBase
from designate.sqlalchemy.models import SoftDeleteMixin
from designate import utils
from sqlalchemy.ext.declarative import declarative_base

LOG = logging.getLogger(__name__)
CONF = cfg.CONF

RESOURCE_STATUSES = ['ACTIVE', 'PENDING', 'DELETED']
RECORD_TYPES = ['A', 'AAAA', 'CNAME', 'MX', 'SRV', 'TXT', 'SPF', 'NS', 'PTR',
                'SSHFP']
TSIG_ALGORITHMS = ['hmac-md5', 'hmac-sha1', 'hmac-sha224', 'hmac-sha256',
                   'hmac-sha384', 'hmac-sha512']


class Base(CommonBase):
    id = Column(UUID, default=utils.generate_uuid, primary_key=True)
    version = Column(Integer, default=1, nullable=False)
    created_at = Column(DateTime, default=timeutils.utcnow)
    updated_at = Column(DateTime, onupdate=timeutils.utcnow)

    __mapper_args__ = {
        'version_id_col': version
    }

    __table_args__ = {'mysql_engine': 'InnoDB', 'mysql_charset': 'utf8'}


Base = declarative_base(cls=Base)


class Quota(Base):
    __tablename__ = 'quotas'
    __table_args__ = (
        UniqueConstraint('tenant_id', 'resource', name='unique_quota'),
        {'mysql_engine': 'InnoDB', 'mysql_charset': 'utf8'}
    )

    tenant_id = Column(String(36), default=None, nullable=True)
    resource = Column(String(32), nullable=False)
    hard_limit = Column(Integer(), nullable=False)


class Server(Base):
    __tablename__ = 'servers'

    name = Column(String(255), nullable=False, unique=True)


class Tld(Base):
    __tablename__ = 'tlds'

    name = Column(String(255), nullable=False, unique=True)
    description = Column(Unicode(160), nullable=True)


class Domain(SoftDeleteMixin, Base):
    __tablename__ = 'domains'
    __table_args__ = (
        UniqueConstraint('name', 'deleted', name='unique_domain_name'),
        {'mysql_engine': 'InnoDB', 'mysql_charset': 'utf8'}
    )

    tenant_id = Column(String(36), default=None, nullable=True)

    name = Column(String(255), nullable=False)
    email = Column(String(255), nullable=False)
    description = Column(Unicode(160), nullable=True)
    ttl = Column(Integer, default=CONF.default_ttl, nullable=False)

    serial = Column(Integer, default=timeutils.utcnow_ts, nullable=False)
    refresh = Column(Integer, default=CONF.default_soa_refresh, nullable=False)
    retry = Column(Integer, default=CONF.default_soa_retry, nullable=False)
    expire = Column(Integer, default=CONF.default_soa_expire, nullable=False)
    minimum = Column(Integer, default=CONF.default_soa_minimum, nullable=False)
    status = Column(Enum(name='resource_statuses', *RESOURCE_STATUSES),
                    nullable=False, server_default='ACTIVE',
                    default='ACTIVE')

    recordsets = relationship('RecordSet',
                              backref=backref('domain', uselist=False),
                              cascade="all, delete-orphan",
                              passive_deletes=True)

    parent_domain_id = Column(UUID, ForeignKey('domains.id'), default=None,
                              nullable=True)


class RecordSet(Base):
    __tablename__ = 'recordsets'
    __table_args__ = (
        UniqueConstraint('domain_id', 'name', 'type', name='unique_recordset'),
        {'mysql_engine': 'InnoDB', 'mysql_charset': 'utf8'}
    )

    tenant_id = Column(String(36), default=None, nullable=True)
    domain_id = Column(UUID, ForeignKey('domains.id', ondelete='CASCADE'),
                       nullable=False)

    name = Column(String(255), nullable=False)
    type = Column(Enum(name='record_types', *RECORD_TYPES), nullable=False)
    ttl = Column(Integer, default=None, nullable=True)
    description = Column(Unicode(160), nullable=True)

    records = relationship('Record',
                           backref=backref('recordset', uselist=False),
                           cascade="all, delete-orphan",
                           passive_deletes=True)


class Record(Base):
    __tablename__ = 'records'

    tenant_id = Column(String(36), default=None, nullable=True)
    domain_id = Column(UUID, ForeignKey('domains.id', ondelete='CASCADE'),
                       nullable=False)
    recordset_id = Column(UUID,
                          ForeignKey('recordsets.id', ondelete='CASCADE'),
                          nullable=False)

    data = Column(Text, nullable=False)
    priority = Column(Integer, default=None, nullable=True)
    description = Column(Unicode(160), nullable=True)

    hash = Column(String(32), nullable=False, unique=True)

    managed = Column(Boolean, default=False)
    managed_extra = Column(Unicode(100), default=None, nullable=True)
    managed_plugin_type = Column(Unicode(50), default=None, nullable=True)
    managed_plugin_name = Column(Unicode(50), default=None, nullable=True)
    managed_resource_type = Column(Unicode(50), default=None, nullable=True)
    managed_resource_region = Column(Unicode(100), default=None, nullable=True)
    managed_resource_id = Column(UUID, default=None, nullable=True)
    managed_tenant_id = Column(Unicode(36), default=None, nullable=True)
    status = Column(Enum(name='resource_statuses', *RESOURCE_STATUSES),
                    nullable=False, server_default='ACTIVE',
                    default='ACTIVE')

    def recalculate_hash(self):
        """
        Calculates the hash of the record, used to ensure record uniqueness.
        """
        md5 = hashlib.md5()
        md5.update("%s:%s:%s" % (self.recordset_id, self.data, self.priority))

        self.hash = md5.hexdigest()


@event.listens_for(Record, "before_insert")
def recalculate_record_hash_before_insert(mapper, connection, instance):
    instance.recalculate_hash()


@event.listens_for(Record, "before_update")
def recalculate_record_hash_before_update(mapper, connection, instance):
    instance.recalculate_hash()


class TsigKey(Base):
    __tablename__ = 'tsigkeys'

    name = Column(String(255), nullable=False, unique=True)
    algorithm = Column(Enum(name='tsig_algorithms', *TSIG_ALGORITHMS),
                       nullable=False)
    secret = Column(String(255), nullable=False)


class Blacklists(Base):
    __tablename__ = 'blacklists'

    pattern = Column(String(255), nullable=False, unique=True)
    description = Column(Unicode(160), nullable=True)

########NEW FILE########
__FILENAME__ = test_service
# Copyright 2012 Hewlett-Packard Development Company, L.P. All Rights Reserved.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.tests.test_agent import AgentTestCase


class AgentServiceTest(AgentTestCase):
    def setUp(self):
        super(AgentServiceTest, self).setUp()
        self.service = self.start_service('agent')

    def test_stop(self):
        # NOTE: Start is already done by the fixture in start_service()
        self.service.stop()

########NEW FILE########
__FILENAME__ = test_middleware
# Copyright 2012 Managed I.T.
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from oslo.config import cfg
from designate.tests.test_api import ApiTestCase
from designate import context
from designate import exceptions
from designate import rpc
from designate.api import middleware


class FakeRequest(object):
    def __init__(self):
        self.headers = {}
        self.environ = {}

    def get_response(self, app):
        return "FakeResponse"


class MaintenanceMiddlewareTest(ApiTestCase):
    def test_process_request_disabled(self):
        self.config(maintenance_mode=False, group='service:api')

        request = FakeRequest()
        app = middleware.MaintenanceMiddleware({})

        # Process the request
        response = app(request)

        # Ensure request was not blocked
        self.assertEqual(response, 'FakeResponse')

    def test_process_request_enabled_reject(self):
        self.config(maintenance_mode=True, maintenance_mode_role='admin',
                    group='service:api')

        request = FakeRequest()
        request.environ['context'] = context.DesignateContext(roles=['user'])

        app = middleware.MaintenanceMiddleware({})

        # Process the request
        response = app(request)

        # Ensure request was blocked
        self.assertEqual(response.status_code, 503)

    def test_process_request_enabled_reject_no_roles(self):
        self.config(maintenance_mode=True, maintenance_mode_role='admin',
                    group='service:api')

        request = FakeRequest()
        request.environ['context'] = context.DesignateContext(roles=[])

        app = middleware.MaintenanceMiddleware({})

        # Process the request
        response = app(request)

        # Ensure request was blocked
        self.assertEqual(response.status_code, 503)

    def test_process_request_enabled_reject_no_context(self):
        self.config(maintenance_mode=True, maintenance_mode_role='admin',
                    group='service:api')

        request = FakeRequest()
        app = middleware.MaintenanceMiddleware({})

        # Process the request
        response = app(request)

        # Ensure request was blocked
        self.assertEqual(response.status_code, 503)

    def test_process_request_enabled_bypass(self):
        self.config(maintenance_mode=True, maintenance_mode_role='admin',
                    group='service:api')

        request = FakeRequest()
        request.environ['context'] = context.DesignateContext(roles=['admin'])

        app = middleware.MaintenanceMiddleware({})

        # Process the request
        response = app(request)

        # Ensure request was not blocked
        self.assertEqual(response, 'FakeResponse')


class KeystoneContextMiddlewareTest(ApiTestCase):
    def test_process_request(self):
        app = middleware.KeystoneContextMiddleware({})

        request = FakeRequest()

        request.headers = {
            'X-Auth-Token': 'AuthToken',
            'X-User-ID': 'UserID',
            'X-Tenant-ID': 'TenantID',
            'X-Roles': 'admin,Member',
        }

        # Process the request
        app.process_request(request)

        self.assertIn('context', request.environ)

        context = request.environ['context']

        self.assertFalse(context.is_admin)
        self.assertEqual('AuthToken', context.auth_token)
        self.assertEqual('UserID', context.user)
        self.assertEqual('TenantID', context.tenant)
        self.assertEqual(['admin', 'Member'], context.roles)

    def test_process_request_invalid_keystone_token(self):
        app = middleware.KeystoneContextMiddleware({})

        request = FakeRequest()

        request.headers = {
            'X-Auth-Token': 'AuthToken',
            'X-User-ID': 'UserID',
            'X-Tenant-ID': 'TenantID',
            'X-Roles': 'admin,Member',
            'X-Identity-Status': 'Invalid'
        }

        # Process the request
        response = app(request)

        self.assertEqual(response.status_code, 401)


class NoAuthContextMiddlewareTest(ApiTestCase):
    def test_process_request(self):
        app = middleware.NoAuthContextMiddleware({})

        request = FakeRequest()

        # Process the request
        app.process_request(request)

        self.assertIn('context', request.environ)

        ctxt = request.environ['context']

        self.assertIsNone(ctxt.auth_token)
        self.assertEqual('noauth-user', ctxt.user)
        self.assertEqual('noauth-project', ctxt.tenant)
        self.assertEqual(['admin'], ctxt.roles)


class FaultMiddlewareTest(ApiTestCase):
    def test_notify_of_fault(self):
        self.config(notify_api_faults=True)
        rpc.init(cfg.CONF)
        app = middleware.FaultWrapperMiddleware({})

        class RaisingRequest(FakeRequest):
            def get_response(self, request):
                raise exceptions.DuplicateDomain()

        request = RaisingRequest()
        ctxt = context.DesignateContext()
        ctxt.request_id = 'one'
        request.environ['context'] = ctxt

        # Process the request
        app(request)

        notifications = self.get_notifications()
        self.assertEqual(1, len(notifications))

        ctxt, message, priority = notifications.pop()

        self.assertEqual('ERROR', message['priority'])
        self.assertEqual('dns.api.fault', message['event_type'])
        self.assertIn('timestamp', message)
        self.assertIn('publisher_id', message)

########NEW FILE########
__FILENAME__ = test_service
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.tests.test_api import ApiTestCase
from designate.api import service


class ApiServiceTest(ApiTestCase):
    def setUp(self):
        super(ApiServiceTest, self).setUp()

        # Use a random port for the API
        self.config(api_port=0, group='service:api')

        # Bring up the Central service as if not the rpc will go into
        # AssertError since TRANSPORT is None
        self.start_service('central')

        self.service = service.Service()

    def test_start_and_stop(self):
        # NOTE: Start is already done by the fixture in start_service()
        self.service.start()
        self.service.stop()

########NEW FILE########
__FILENAME__ = test_domains
# coding=utf-8
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from mock import patch
from oslo import messaging
from designate.openstack.common import log as logging
from designate import exceptions
from designate.central import service as central_service
from designate.tests.test_api.test_v1 import ApiV1Test


LOG = logging.getLogger(__name__)


class ApiV1DomainsTest(ApiV1Test):
    def test_create_domain(self):
        # Create a server
        self.create_server()

        # Create a domain
        fixture = self.get_domain_fixture(0)

        response = self.post('domains', data=fixture)

        self.assertIn('id', response.json)
        self.assertIn('name', response.json)
        self.assertEqual(response.json['name'], fixture['name'])

    @patch.object(central_service.Service, 'create_domain')
    def test_create_domain_trailing_slash(self, mock):
        # Create a server
        self.create_server()
        self.post('domains/', data=self.get_domain_fixture(0))

        # verify that the central service is called
        self.assertTrue(mock.called)

    def test_create_domain_junk(self):
        # Create a server
        self.create_server()

        # Create a domain
        fixture = self.get_domain_fixture(0)

        # Add a junk property
        fixture['junk'] = 'Junk Field'

        # Ensure it fails with a 400
        self.post('domains', data=fixture, status_code=400)

    def test_create_domain_no_servers(self):
        # Create a domain
        fixture = self.get_domain_fixture(0)

        self.post('domains', data=fixture, status_code=500)

    @patch.object(central_service.Service, 'create_domain',
                  side_effect=messaging.MessagingTimeout())
    def test_create_domain_timeout(self, _):
        # Create a domain
        fixture = self.get_domain_fixture(0)

        self.post('domains', data=fixture, status_code=504)

    @patch.object(central_service.Service, 'create_domain',
                  side_effect=exceptions.DuplicateDomain())
    def test_create_domain_duplicate(self, _):
        # Create a domain
        fixture = self.get_domain_fixture(0)
        self.post('domains', data=fixture, status_code=409)

    def test_create_domain_null_ttl(self):
        # Create a domain
        fixture = self.get_domain_fixture(0)
        fixture['ttl'] = None
        self.post('domains', data=fixture, status_code=400)

    def test_create_domain_negative_ttl(self):
        # Create a domain
        fixture = self.get_domain_fixture(0)
        fixture['ttl'] = -1
        self.post('domains', data=fixture, status_code=400)

    def test_create_domain_utf_description(self):
        # Create a server
        self.create_server()

        # Create a domain
        fixture = self.get_domain_fixture(0)

        # Give it a UTF-8 filled description
        fixture['description'] = "utf-8:2H₂+O₂⇌2H₂O,R=4.7kΩ,⌀200mm∮E⋅da=Q,n" \
                                 ",∑f(i)=∏g(i),∀x∈ℝ:⌈x⌉"
        # Create the domain, ensuring it succeeds, thus UTF-8 is supported
        self.post('domains', data=fixture)

    def test_create_domain_description_too_long(self):
        # Create a server
        self.create_server()

        # Create a domain
        fixture = self.get_domain_fixture(0)
        fixture['description'] = "x" * 161

        #Create the domain, ensuring it fails with a 400
        self.post('domains', data=fixture, status_code=400)

    def test_create_invalid_name(self):
        # Prepare a domain
        fixture = self.get_domain_fixture(0)

        invalid_names = [
            'org',
            'example.org',
            'example.321',
        ]

        for invalid_name in invalid_names:
            fixture['name'] = invalid_name

            # Create a record
            response = self.post('domains', data=fixture, status_code=400)

            self.assertNotIn('id', response.json)

    def test_create_invalid_email(self):
        # Prepare a domain
        fixture = self.get_domain_fixture(0)

        invalid_emails = [
            'org',
            'example.org',
            'bla.example.org',
            'org.',
            'example.org.',
            'bla.example.org.',
            'bla.example.org.',
        ]

        for invalid_email in invalid_emails:
            fixture['email'] = invalid_email

            # Create a record
            response = self.post('domains', data=fixture, status_code=400)

            self.assertNotIn('id', response.json)

    def test_get_domains(self):
        response = self.get('domains')

        self.assertIn('domains', response.json)
        self.assertEqual(0, len(response.json['domains']))

        # Create a domain
        self.create_domain()

        response = self.get('domains')

        self.assertIn('domains', response.json)
        self.assertEqual(1, len(response.json['domains']))

        # Create a second domain
        self.create_domain(fixture=1)

        response = self.get('domains')

        self.assertIn('domains', response.json)
        self.assertEqual(2, len(response.json['domains']))

    @patch.object(central_service.Service, 'find_domains')
    def test_get_domains_trailing_slash(self, mock):
        self.get('domains/')

        # verify that the central service is called
        self.assertTrue(mock.called)

    @patch.object(central_service.Service, 'find_domains',
                  side_effect=messaging.MessagingTimeout())
    def test_get_domains_timeout(self, _):
        self.get('domains', status_code=504)

    def test_get_domain(self):
        # Create a domain
        domain = self.create_domain()

        response = self.get('domains/%s' % domain['id'])

        self.assertIn('id', response.json)
        self.assertEqual(response.json['id'], domain['id'])

    @patch.object(central_service.Service, 'get_domain')
    def test_get_domain_trailing_slash(self, mock):
        # Create a domain
        domain = self.create_domain()

        self.get('domains/%s/' % domain['id'])

        # verify that the central service is called
        self.assertTrue(mock.called)

    @patch.object(central_service.Service, 'get_domain',
                  side_effect=messaging.MessagingTimeout())
    def test_get_domain_timeout(self, _):
        # Create a domain
        domain = self.create_domain()

        self.get('domains/%s' % domain['id'], status_code=504)

    def test_get_domain_missing(self):
        self.get('domains/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980',
                 status_code=404)

    def test_get_domain_invalid_id(self):
        # The letter "G" is not valid in a UUID
        self.get('domains/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff9GG',
                 status_code=404)

        self.get('domains/2fdadfb1cf964259ac6bbb7b6d2ff980', status_code=404)

    def test_update_domain(self):
        # Create a domain
        domain = self.create_domain()

        data = {'email': 'prefix-%s' % domain['email']}

        response = self.put('domains/%s' % domain['id'], data=data)

        self.assertIn('id', response.json)
        self.assertEqual(response.json['id'], domain['id'])

        self.assertIn('email', response.json)
        self.assertEqual(response.json['email'], 'prefix-%s' % domain['email'])

    @patch.object(central_service.Service, 'update_domain')
    def test_update_domain_trailing_slash(self, mock):
        # Create a domain
        domain = self.create_domain()

        data = {'email': 'prefix-%s' % domain['email']}

        self.put('domains/%s/' % domain['id'], data=data)

        # verify that the central service is called
        self.assertTrue(mock.called)

    def test_update_domain_junk(self):
        # Create a domain
        domain = self.create_domain()

        data = {'email': 'prefix-%s' % domain['email'], 'junk': 'Junk Field'}

        self.put('domains/%s' % domain['id'], data=data, status_code=400)

    def test_update_domain_name_fail(self):
        # Create a domain
        domain = self.create_domain()

        data = {'name': 'renamed.com.'}

        self.put('domains/%s' % domain['id'], data=data, status_code=400)

    def test_update_domain_null_ttl(self):
        # Create a domain
        domain = self.create_domain()

        data = {'ttl': None}

        self.put('domains/%s' % domain['id'], data=data, status_code=400)

    @patch.object(central_service.Service, 'update_domain',
                  side_effect=messaging.MessagingTimeout())
    def test_update_domain_timeout(self, _):
        # Create a domain
        domain = self.create_domain()

        data = {'email': 'prefix-%s' % domain['email']}

        self.put('domains/%s' % domain['id'], data=data, status_code=504)

    @patch.object(central_service.Service, 'update_domain',
                  side_effect=exceptions.DuplicateDomain())
    def test_update_domain_duplicate(self, _):
        # Create a domain
        domain = self.create_domain()

        data = {'email': 'prefix-%s' % domain['email']}

        self.put('domains/%s' % domain['id'], data=data, status_code=409)

    def test_update_domain_missing(self):
        data = {'email': 'bla@bla.com'}

        self.put('domains/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980', data=data,
                 status_code=404)

    def test_update_domain_invalid_id(self):
        data = {'email': 'bla@bla.com'}

        # The letter "G" is not valid in a UUID
        self.put('domains/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff9GG', data=data,
                 status_code=404)

        self.put('domains/2fdadfb1cf964259ac6bbb7b6d2ff980', data=data,
                 status_code=404)

    def test_delete_domain(self):
        # Create a domain
        domain = self.create_domain()

        self.delete('domains/%s' % domain['id'])

        # Esnure we can no longer fetch the domain
        self.get('domains/%s' % domain['id'], status_code=404)

    @patch.object(central_service.Service, 'delete_domain')
    def test_delete_domain_trailing_slash(self, mock):
        # Create a domain
        domain = self.create_domain()

        self.delete('domains/%s/' % domain['id'])

        # verify that the central service is called
        self.assertTrue(mock.called)

    @patch.object(central_service.Service, 'delete_domain',
                  side_effect=messaging.MessagingTimeout())
    def test_delete_domain_timeout(self, _):
        # Create a domain
        domain = self.create_domain()

        self.delete('domains/%s' % domain['id'], status_code=504)

    def test_delete_domain_missing(self):
        self.delete('domains/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980',
                    status_code=404)

    def test_delete_domain_invalid_id(self):
        # The letter "G" is not valid in a UUID
        self.delete('domains/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff9GG',
                    status_code=404)

        self.delete('domains/2fdadfb1cf964259ac6bbb7b6d2ff980',
                    status_code=404)

########NEW FILE########
__FILENAME__ = test_records
# coding=utf-8
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from mock import patch
from oslo import messaging
from designate.openstack.common import log as logging
from designate.central import service as central_service
from designate.tests.test_api.test_v1 import ApiV1Test


LOG = logging.getLogger(__name__)


class ApiV1RecordsTest(ApiV1Test):
    def setUp(self):
        super(ApiV1RecordsTest, self).setUp()

        self.domain = self.create_domain()
        self.recordset = self.create_recordset(self.domain, 'A')

    def test_create_record(self):
        recordset_fixture = self.get_recordset_fixture(
            self.domain['name'])

        fixture = self.get_record_fixture(recordset_fixture['type'])
        fixture.update({
            'name': recordset_fixture['name'],
            'type': recordset_fixture['type'],
        })

        # Create a record
        response = self.post('domains/%s/records' % self.domain['id'],
                             data=fixture)

        self.assertIn('id', response.json)
        self.assertIn('name', response.json)
        self.assertEqual(response.json['name'], fixture['name'])

    def test_create_record_existing_recordset(self):
        fixture = self.get_record_fixture(self.recordset['type'])
        fixture.update({
            'name': self.recordset['name'],
            'type': self.recordset['type'],
        })

        # Create a record
        response = self.post('domains/%s/records' % self.domain['id'],
                             data=fixture)

        self.assertIn('id', response.json)
        self.assertIn('name', response.json)
        self.assertEqual(response.json['name'], fixture['name'])

    @patch.object(central_service.Service, 'create_record')
    def test_create_record_trailing_slash(self, mock):
        fixture = self.get_record_fixture(self.recordset['type'])
        fixture.update({
            'name': self.recordset['name'],
            'type': self.recordset['type'],
        })

        # Create a record with a trailing slash
        self.post('domains/%s/records/' % self.domain['id'],
                  data=fixture)

        # verify that the central service is called
        self.assertTrue(mock.called)

    def test_create_record_junk(self):
        fixture = self.get_record_fixture(self.recordset['type'])
        fixture.update({
            'name': self.recordset['name'],
            'type': self.recordset['type'],
        })

        # Add a junk property
        fixture['junk'] = 'Junk Field'

        # Create a record, Ensuring it fails with a 400
        self.post('domains/%s/records' % self.domain['id'], data=fixture,
                  status_code=400)

    def test_create_record_utf_description(self):
        fixture = self.get_record_fixture(self.recordset['type'])
        fixture.update({
            'name': self.recordset['name'],
            'type': self.recordset['type'],
        })

        #Add a UTF-8 riddled description
        fixture['description'] = "utf-8:2H₂+O₂⇌2H₂O,R=4.7kΩ,⌀200mm∮E⋅da=Q,n" \
                                 ",∑f(i)=∏g(i),∀x∈ℝ:⌈x⌉"

        # Create a record, Ensuring it succeeds
        self.post('domains/%s/records' % self.domain['id'], data=fixture)

    def test_create_record_description_too_long(self):
        fixture = self.get_record_fixture(self.recordset['type'])
        fixture.update({
            'name': self.recordset['name'],
            'type': self.recordset['type'],
        })

        # Add a description that is too long
        fixture['description'] = "x" * 161

        # Create a record, Ensuring it Fails with a 400
        self.post('domains/%s/records' % self.domain['id'], data=fixture,
                  status_code=400)

    def test_create_record_negative_ttl(self):
        fixture = self.get_record_fixture(self.recordset['type'])
        fixture.update({
            'name': self.recordset['name'],
            'type': self.recordset['type'],
        })

        # Set the TTL to a negative value
        fixture['ttl'] = -1

        # Create a record, Ensuring it Fails with a 400
        self.post('domains/%s/records' % self.domain['id'], data=fixture,
                  status_code=400)

    @patch.object(central_service.Service, 'create_record',
                  side_effect=messaging.MessagingTimeout())
    def test_create_record_timeout(self, _):
        fixture = self.get_record_fixture(self.recordset['type'])
        fixture.update({
            'name': self.recordset['name'],
            'type': self.recordset['type'],
        })

        # Create a record
        self.post('domains/%s/records' % self.domain['id'], data=fixture,
                  status_code=504)

    def test_create_wildcard_record(self):
        # Prepare a record
        fixture = self.get_record_fixture(self.recordset['type'])
        fixture.update({
            'name': '*.%s' % self.recordset['name'],
            'type': self.recordset['type'],
        })

        # Create a record
        response = self.post('domains/%s/records' % self.domain['id'],
                             data=fixture)

        self.assertIn('id', response.json)
        self.assertIn('name', response.json)
        self.assertEqual(response.json['name'], fixture['name'])

    def test_create_srv_record(self):
        recordset_fixture = self.get_recordset_fixture(
            self.domain['name'], 'SRV')

        fixture = self.get_record_fixture(recordset_fixture['type'])
        fixture.update({
            'name': recordset_fixture['name'],
            'type': recordset_fixture['type'],
        })

        # Create a record
        response = self.post('domains/%s/records' % self.domain['id'],
                             data=fixture)

        self.assertIn('id', response.json)
        self.assertEqual(response.json['type'], fixture['type'])
        self.assertEqual(response.json['name'], fixture['name'])
        self.assertEqual(response.json['priority'], fixture['priority'])
        self.assertEqual(response.json['data'], fixture['data'])

    def test_create_invalid_data_srv_record(self):
        recordset_fixture = self.get_recordset_fixture(
            self.domain['name'], 'SRV')

        fixture = self.get_record_fixture(recordset_fixture['type'])
        fixture.update({
            'name': recordset_fixture['name'],
            'type': recordset_fixture['type'],
        })

        invalid_datas = [
            'I 5060 sip.%s' % self.domain['name'],
            '5060 sip.%s' % self.domain['name'],
            '5060 I sip.%s' % self.domain['name'],
            '0 5060 sip',
            'sip',
            'sip.%s' % self.domain['name'],
        ]

        for invalid_data in invalid_datas:
            fixture['data'] = invalid_data
            # Attempt to create the record
            self.post('domains/%s/records' % self.domain['id'], data=fixture,
                      status_code=400)

    def test_create_invalid_name_srv_record(self):
        recordset_fixture = self.get_recordset_fixture(
            self.domain['name'], 'SRV')

        fixture = self.get_record_fixture(recordset_fixture['type'])
        fixture.update({
            'name': recordset_fixture['name'],
            'type': recordset_fixture['type'],
        })

        invalid_names = [
            '%s' % self.domain['name'],
            '_udp.%s' % self.domain['name'],
            'sip._udp.%s' % self.domain['name'],
            '_sip.udp.%s' % self.domain['name'],
        ]

        for invalid_name in invalid_names:
            fixture['name'] = invalid_name

            # Attempt to create the record
            self.post('domains/%s/records' % self.domain['id'], data=fixture,
                      status_code=400)

    def test_create_invalid_name(self):
        # Prepare a record
        fixture = self.get_record_fixture(self.recordset['type'])
        fixture.update({
            'name': self.recordset['name'],
            'type': self.recordset['type'],
        })

        invalid_names = [
            'org',
            'example.org',
            '$$.example.org',
            '*example.org.',
            '*.*.example.org.',
            'abc.*.example.org.',
        ]

        for invalid_name in invalid_names:
            fixture['name'] = invalid_name

            # Create a record
            response = self.post('domains/%s/records' % self.domain['id'],
                                 data=fixture, status_code=400)

            self.assertNotIn('id', response.json)

    def test_get_records(self):
        response = self.get('domains/%s/records' % self.domain['id'])

        self.assertIn('records', response.json)
        self.assertEqual(0, len(response.json['records']))

        # Create a record
        self.create_record(self.domain, self.recordset)

        response = self.get('domains/%s/records' % self.domain['id'])

        self.assertIn('records', response.json)
        self.assertEqual(1, len(response.json['records']))

        # Create a second record
        self.create_record(self.domain, self.recordset, fixture=1)

        response = self.get('domains/%s/records' % self.domain['id'])

        self.assertIn('records', response.json)
        self.assertEqual(2, len(response.json['records']))

    @patch.object(central_service.Service, 'find_records')
    def test_get_records_trailing_slash(self, mock):
        self.get('domains/%s/records/' % self.domain['id'])

        # verify that the central service is called
        self.assertTrue(mock.called)

    @patch.object(central_service.Service, 'find_records',
                  side_effect=messaging.MessagingTimeout())
    def test_get_records_timeout(self, _):
        self.get('domains/%s/records' % self.domain['id'],
                 status_code=504)

    def test_get_records_missing_domain(self):
        self.get('domains/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980/records',
                 status_code=404)

    def test_get_records_invalid_domain_id(self):
        self.get('domains/2fdadfb1cf964259ac6bbb7b6d2ff980/records',
                 status_code=404)

    def test_get_record(self):
        # Create a record
        record = self.create_record(self.domain, self.recordset)

        response = self.get('domains/%s/records/%s' % (self.domain['id'],
                                                       record['id']))

        self.assertIn('id', response.json)
        self.assertEqual(response.json['id'], record['id'])
        self.assertEqual(response.json['name'], self.recordset['name'])
        self.assertEqual(response.json['type'], self.recordset['type'])

    @patch.object(central_service.Service, 'get_recordset')
    def test_get_record_trailing_slash(self, mock):
        # Create a record
        record = self.create_record(self.domain, self.recordset)

        self.get('domains/%s/records/%s/' % (self.domain['id'],
                                             record['id']))

        # verify that the central service is called
        self.assertTrue(mock.called)

    def test_update_record(self):
        # Create a record
        record = self.create_record(self.domain, self.recordset)

        # Fetch another fixture to use in the update
        fixture = self.get_record_fixture(self.recordset['type'], fixture=1)

        # Update the record
        data = {'data': fixture['data']}
        response = self.put('domains/%s/records/%s' % (self.domain['id'],
                                                       record['id']),
                            data=data)

        self.assertIn('id', response.json)
        self.assertEqual(response.json['id'], record['id'])
        self.assertEqual(response.json['data'], fixture['data'])
        self.assertEqual(response.json['type'], self.recordset['type'])

    def test_update_record_ttl(self):
        # Create a record
        record = self.create_record(self.domain, self.recordset)

        # Update the record
        data = {'ttl': 100}
        response = self.put('domains/%s/records/%s' % (self.domain['id'],
                                                       record['id']),
                            data=data)

        self.assertIn('id', response.json)
        self.assertEqual(response.json['id'], record['id'])
        self.assertEqual(response.json['data'], record['data'])
        self.assertEqual(response.json['type'], self.recordset['type'])
        self.assertEqual(response.json['ttl'], 100)

    @patch.object(central_service.Service, 'update_record')
    def test_update_record_trailing_slash(self, mock):
        # Create a record
        record = self.create_record(self.domain, self.recordset)

        data = {'ttl': 100}

        self.put('domains/%s/records/%s/' % (self.domain['id'],
                                             record['id']),
                 data=data)

        # verify that the central service is called
        self.assertTrue(mock.called)

    def test_update_record_junk(self):
        # Create a record
        record = self.create_record(self.domain, self.recordset)

        data = {'ttl': 100, 'junk': 'Junk Field'}

        self.put('domains/%s/records/%s' % (self.domain['id'], record['id']),
                 data=data, status_code=400)

    def test_update_record_outside_domain_fail(self):
        # Create a record
        record = self.create_record(self.domain, self.recordset)

        data = {'name': 'test.someotherdomain.com.'}

        self.put('domains/%s/records/%s' % (self.domain['id'], record['id']),
                 data=data, status_code=400)

    @patch.object(central_service.Service, 'get_domain',
                  side_effect=messaging.MessagingTimeout())
    def test_update_record_timeout(self, _):
        # Create a record
        record = self.create_record(self.domain, self.recordset)

        data = {'name': 'test.example.org.'}

        self.put('domains/%s/records/%s' % (self.domain['id'], record['id']),
                 data=data, status_code=504)

    def test_update_record_missing(self):
        data = {'name': 'test.example.org.'}

        self.put('domains/%s/records/2fdadfb1-cf96-4259-ac6b-'
                 'bb7b6d2ff980' % self.domain['id'],
                 data=data,
                 status_code=404)

    def test_update_record_invalid_id(self):
        data = {'name': 'test.example.org.'}

        self.put('domains/%s/records/2fdadfb1cf964259ac6bbb7b6d2ff980' %
                 self.domain['id'],
                 data=data,
                 status_code=404)

    def test_update_record_missing_domain(self):
        data = {'name': 'test.example.org.'}

        self.put('domains/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980/records/'
                 '2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980',
                 data=data,
                 status_code=404)

    def test_update_record_invalid_domain_id(self):
        data = {'name': 'test.example.org.'}

        self.put('domains/2fdadfb1cf964259ac6bbb7b6d2ff980/records/'
                 '2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980',
                 data=data,
                 status_code=404)

    def test_delete_record(self):
        # Create a record
        record = self.create_record(self.domain, self.recordset)

        self.delete('domains/%s/records/%s' % (self.domain['id'],
                                               record['id']))

        # Ensure we can no longer fetch the record
        self.get('domains/%s/records/%s' % (self.domain['id'],
                                            record['id']),
                 status_code=404)

    @patch.object(central_service.Service, 'get_domain')
    def test_delete_record_trailing_slash(self, mock):
        # Create a record
        record = self.create_record(self.domain, self.recordset)

        self.delete('domains/%s/records/%s/' % (self.domain['id'],
                                                record['id']))

        # verify that the central service is called
        self.assertTrue(mock.called)

    @patch.object(central_service.Service, 'get_domain',
                  side_effect=messaging.MessagingTimeout())
    def test_delete_record_timeout(self, _):
        # Create a record
        record = self.create_record(self.domain, self.recordset)

        self.delete('domains/%s/records/%s' % (self.domain['id'],
                                               record['id']),
                    status_code=504)

    def test_delete_record_missing(self):
        self.delete('domains/%s/records/2fdadfb1-cf96-4259-ac6b-'
                    'bb7b6d2ff980' % self.domain['id'],
                    status_code=404)

    def test_delete_record_missing_domain(self):
        self.delete('domains/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980/records/'
                    '2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980',
                    status_code=404)

    def test_delete_record_invalid_domain_id(self):
        self.delete('domains/2fdadfb1cf964259ac6bbb7b6d2ff980/records/'
                    '2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980',
                    status_code=404)

########NEW FILE########
__FILENAME__ = test_servers
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from mock import patch
from oslo import messaging
from designate.openstack.common import log as logging
from designate import exceptions
from designate.central import service as central_service
from designate.tests.test_api.test_v1 import ApiV1Test


LOG = logging.getLogger(__name__)


class ApiV1ServersTest(ApiV1Test):
    def setUp(self):
        super(ApiV1ServersTest, self).setUp()

        # All Server Checks should be performed as an admin, so..
        # Override to policy to make everyone an admin.
        self.policy({'admin': '@'})

    def test_create_server(self):
        # Create a server
        fixture = self.get_server_fixture(0)

        response = self.post('servers', data=fixture)

        self.assertIn('id', response.json)
        self.assertIn('name', response.json)
        self.assertEqual(response.json['name'], fixture['name'])

    @patch.object(central_service.Service, 'create_server')
    def test_create_server_trailing_slash(self, mock):
        # Create a server with a trailing slash
        self.post('servers/', data=self.get_server_fixture(0))

        # verify that the central service is called
        self.assertTrue(mock.called)

    def test_create_server_junk(self):
        # Create a server
        fixture = self.get_server_fixture(0)

        # Add a junk property
        fixture['junk'] = 'Junk Field'

        # Ensure it fails with a 400
        self.post('servers', data=fixture, status_code=400)

    @patch.object(central_service.Service, 'create_server',
                  side_effect=messaging.MessagingTimeout())
    def test_create_server_timeout(self, _):
        # Create a server
        fixture = self.get_server_fixture(0)

        self.post('servers', data=fixture, status_code=504)

    @patch.object(central_service.Service, 'create_server',
                  side_effect=exceptions.DuplicateServer())
    def test_create_server_duplicate(self, _):
        # Create a server
        fixture = self.get_server_fixture(0)

        self.post('servers', data=fixture, status_code=409)

    def test_get_servers(self):
        response = self.get('servers')

        self.assertIn('servers', response.json)
        self.assertEqual(0, len(response.json['servers']))

        # Create a server
        self.create_server()

        response = self.get('servers')

        self.assertIn('servers', response.json)
        self.assertEqual(1, len(response.json['servers']))

        # Create a second server
        self.create_server(fixture=1)

        response = self.get('servers')

        self.assertIn('servers', response.json)
        self.assertEqual(2, len(response.json['servers']))

    @patch.object(central_service.Service, 'find_servers')
    def test_get_servers_trailing_slash(self, mock):
        self.get('servers/')

        # verify that the central service is called
        self.assertTrue(mock.called)

    @patch.object(central_service.Service, 'find_servers',
                  side_effect=messaging.MessagingTimeout())
    def test_get_servers_timeout(self, _):
        self.get('servers', status_code=504)

    def test_get_server(self):
        # Create a server
        server = self.create_server()

        response = self.get('servers/%s' % server['id'])

        self.assertIn('id', response.json)
        self.assertEqual(response.json['id'], server['id'])

    @patch.object(central_service.Service, 'get_server')
    def test_get_server_trailing_slash(self, mock):
        # Create a server
        server = self.create_server()

        self.get('servers/%s/' % server['id'])

        # verify that the central service is called
        self.assertTrue(mock.called)

    @patch.object(central_service.Service, 'get_server',
                  side_effect=messaging.MessagingTimeout())
    def test_get_server_timeout(self, _):
        # Create a server
        server = self.create_server()

        self.get('servers/%s' % server['id'], status_code=504)

    def test_get_server_missing(self):
        self.get('servers/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980',
                 status_code=404)

    def test_update_server(self):
        # Create a server
        server = self.create_server()

        data = {'name': 'test.example.org.'}

        response = self.put('servers/%s' % server['id'], data=data)

        self.assertIn('id', response.json)
        self.assertEqual(response.json['id'], server['id'])

        self.assertIn('name', response.json)
        self.assertEqual(response.json['name'], 'test.example.org.')

    @patch.object(central_service.Service, 'update_server')
    def test_update_server_trailing_slash(self, mock):
        # Create a server
        server = self.create_server()

        data = {'name': 'test.example.org.'}

        self.put('servers/%s/' % server['id'], data=data)

        # verify that the central service is called
        self.assertTrue(mock.called)

    def test_update_server_junk(self):
        # Create a server
        server = self.create_server()

        data = {'name': 'test.example.org.', 'junk': 'Junk Field'}

        self.put('servers/%s' % server['id'], data=data, status_code=400)

    @patch.object(central_service.Service, 'update_server',
                  side_effect=messaging.MessagingTimeout())
    def test_update_server_timeout(self, _):
        # Create a server
        server = self.create_server()

        data = {'name': 'test.example.org.'}

        self.put('servers/%s' % server['id'], data=data, status_code=504)

    @patch.object(central_service.Service, 'update_server',
                  side_effect=exceptions.DuplicateServer())
    def test_update_server_duplicate(self, _):
        server = self.create_server()

        data = {'name': 'test.example.org.'}

        self.put('servers/%s' % server['id'], data=data, status_code=409)

    def test_update_server_missing(self):
        data = {'name': 'test.example.org.'}

        self.get('servers/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980', data=data,
                 status_code=404)

    def test_delete_server(self):
        # Create a server
        server = self.create_server()

        # Create a second server so that we can delete the first
        # because the last remaining server is not allowed to be deleted
        server2 = self.create_server(fixture=1)

        # Now delete the server
        self.delete('servers/%s' % server['id'])

        # Ensure we can no longer fetch the deleted server
        self.get('servers/%s' % server['id'], status_code=404)

        # Also, verify we cannot delete last remaining server
        self.delete('servers/%s' % server2['id'], status_code=400)

    @patch.object(central_service.Service, 'delete_server')
    def test_delete_server_trailing_slash(self, mock):
        # Create a server
        server = self.create_server()

        self.delete('servers/%s/' % server['id'])

        # verify that the central service is called
        self.assertTrue(mock.called)

    @patch.object(central_service.Service, 'delete_server',
                  side_effect=messaging.MessagingTimeout())
    def test_delete_server_timeout(self, _):
        # Create a server
        server = self.create_server()

        self.delete('servers/%s' % server['id'], status_code=504)

    def test_delete_server_missing(self):
        self.delete('servers/9fdadfb1-cf96-4259-ac6b-bb7b6d2ff980',
                    status_code=404)

########NEW FILE########
__FILENAME__ = test_blacklists
# Copyright 2014 Rackspace Hosting
# All rights reserved
#
# Author: Betsy Luzader <betsy.luzader@rackspace.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.tests.test_api.test_v2 import ApiV2TestCase


class ApiV2BlacklistsTest(ApiV2TestCase):
    def setUp(self):
        super(ApiV2BlacklistsTest, self).setUp()

    def test_get_blacklists(self):
        # Set the policy file as this is an admin-only API
        self.policy({'find_blacklists': '@'})

        response = self.client.get('/blacklists/')

        # Check the headers are what we expect
        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Check the body structure is what we expect
        self.assertIn('blacklists', response.json)
        self.assertIn('links', response.json)
        self.assertIn('self', response.json['links'])

        # Test with 0 blacklists
        self.assertEqual(0, len(response.json['blacklists']))

        data = [self.create_blacklist(
            pattern='x-%s.org.' % i) for i in xrange(0, 10)]

        self._assert_paging(data, '/blacklists', key='blacklists')

    def test_get_blacklist(self):
        blacklist = self.create_blacklist(fixture=0)

        # Set the policy file as this is an admin-only API
        self.policy({'get_blacklist': '@'})

        response = self.client.get('/blacklists/%s' % blacklist['id'],
                                   headers=[('Accept', 'application/json')])

        # Verify the headers
        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Verify the body structure
        self.assertIn('blacklist', response.json)
        self.assertIn('links', response.json['blacklist'])
        self.assertIn('self', response.json['blacklist']['links'])

        # Verify the returned values
        self.assertIn('id', response.json['blacklist'])
        self.assertIn('created_at', response.json['blacklist'])
        self.assertIsNone(response.json['blacklist']['updated_at'])
        self.assertEqual(self.get_blacklist_fixture(0)['pattern'],
                         response.json['blacklist']['pattern'])

    def test_get_bkaclist_invalid_id(self):
        self._assert_invalid_uuid(self.client.get, '/blacklists/%s')

    def test_create_blacklist(self):
        self.policy({'create_blacklist': '@'})
        fixture = self.get_blacklist_fixture(0)
        response = self.client.post_json('/blacklists/',
                                         {'blacklist': fixture})

        # Verify the headers
        self.assertEqual(201, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Verify the body structure
        self.assertIn('blacklist', response.json)
        self.assertIn('links', response.json['blacklist'])
        self.assertIn('self', response.json['blacklist']['links'])

        # Verify the returned values
        self.assertIn('id', response.json['blacklist'])
        self.assertIn('created_at', response.json['blacklist'])
        self.assertIsNone(response.json['blacklist']['updated_at'])
        self.assertEqual(fixture['pattern'],
                         response.json['blacklist']['pattern'])

    def test_delete_blacklist(self):
        blacklist = self.create_blacklist(fixture=0)
        self.policy({'delete_blacklist': '@'})

        self.client.delete('/blacklists/%s' % blacklist['id'], status=204)

    def test_delete_bkaclist_invalid_id(self):
        self._assert_invalid_uuid(self.client.delete, '/blacklists/%s')

    def test_update_blacklist(self):
        blacklist = self.create_blacklist(fixture=0)
        self.policy({'update_blacklist': '@'})

        # Prepare the update body
        body = {'blacklist': {'description': 'prefix-%s' %
                                             blacklist['description']}}

        response = self.client.patch_json('/blacklists/%s' %
                                          blacklist['id'], body,
                                          status=200)

        # Verify the headers
        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Verify the body structure
        self.assertIn('blacklist', response.json)
        self.assertIn('links', response.json['blacklist'])
        self.assertIn('self', response.json['blacklist']['links'])

        # Verify the returned values
        self.assertIn('id', response.json['blacklist'])
        self.assertIsNotNone(response.json['blacklist']['updated_at'])
        self.assertEqual('prefix-%s' % blacklist['description'],
                         response.json['blacklist']['description'])

    def test_update_bkaclist_invalid_id(self):
        self._assert_invalid_uuid(self.client.patch_json, '/blacklists/%s')

########NEW FILE########
__FILENAME__ = test_floatingips
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Endre Karlson <endre.karlson@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.tests.test_api.test_v2 import ApiV2TestCase


"""
NOTE: Record invalidation is tested in Central tests
"""


class ApiV2ReverseFloatingIPTest(ApiV2TestCase):
    def test_get_floatingip_no_record(self):
        context = self.get_context(tenant='a')

        fip = self.network_api.fake.allocate_floatingip(context.tenant)

        response = self.client.get(
            '/reverse/floatingips/%s' % ":".join([fip['region'], fip['id']]),
            headers={'X-Test-Tenant-Id': context.tenant})

        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)
        self.assertIn('floatingip', response.json)

        #TODO(ekarlso): Remove the floatingip key - bug in v2 api
        fip_record = response.json['floatingip']
        self.assertEqual(":".join([fip['region'],
                         fip['id']]), fip_record['id'])
        self.assertEqual(fip['address'], fip_record['address'])
        self.assertEqual(None, fip_record['description'])
        self.assertEqual(None, fip_record['ptrdname'])

    def test_get_floatingip_with_record(self):
        self.create_server()

        fixture = self.get_ptr_fixture()

        context = self.get_context(tenant='a')

        fip = self.network_api.fake.allocate_floatingip(
            context.tenant)

        self.central_service.update_floatingip(
            context, fip['region'], fip['id'], fixture)

        response = self.client.get(
            '/reverse/floatingips/%s' % ":".join([fip['region'], fip['id']]),
            headers={'X-Test-Tenant-Id': context.tenant})

        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)
        self.assertIn('floatingip', response.json)

        # TODO(ekarlso): Remove the floatingip key - bug in v2 api
        fip_record = response.json['floatingip']
        self.assertEqual(":".join([fip['region'], fip['id']]),
                         fip_record['id'])
        self.assertEqual(fip['address'], fip_record['address'])
        self.assertEqual(None, fip_record['description'])
        self.assertEqual(fixture['ptrdname'], fip_record['ptrdname'])

    def test_get_floatingip_not_allocated(self):
        url = '/reverse/floatingips/foo:04580c52-b253-4eb7-8791-fbb9de9f856f'

        self._assert_exception('not_found', 404, self.client.get, url)

    def test_get_floatingip_invalid_key(self):
        url = '/reverse/floatingips/foo:bar'

        self._assert_exception('bad_request', 400, self.client.get, url)

    def test_list_floatingip_no_allocations(self):
        response = self.client.get('/reverse/floatingips')

        self.assertIn('floatingips', response.json)
        self.assertIn('links', response.json)
        self.assertEqual(0, len(response.json['floatingips']))

    def test_list_floatingip_no_record(self):
        context = self.get_context(tenant='a')

        fip = self.network_api.fake.allocate_floatingip(context.tenant)

        response = self.client.get(
            '/reverse/floatingips',
            headers={'X-Test-Tenant-Id': context.tenant})

        self.assertIn('floatingips', response.json)
        self.assertIn('links', response.json)
        self.assertEqual(1, len(response.json['floatingips']))

        fip_record = response.json['floatingips'][0]
        self.assertEqual(None, fip_record['ptrdname'])
        self.assertEqual(":".join([fip['region'], fip['id']]),
                         fip_record['id'])
        self.assertEqual(fip['address'], fip_record['address'])
        self.assertEqual(None, fip_record['description'])

    def test_list_floatingip_with_record(self):
        self.create_server()

        fixture = self.get_ptr_fixture()

        context = self.get_context(tenant='a')

        fip = self.network_api.fake.allocate_floatingip(context.tenant)

        self.central_service.update_floatingip(
            context, fip['region'], fip['id'], fixture)

        response = self.client.get(
            '/reverse/floatingips',
            headers={'X-Test-Tenant-Id': context.tenant})

        self.assertIn('floatingips', response.json)
        self.assertIn('links', response.json)
        self.assertEqual(1, len(response.json['floatingips']))

        fip_record = response.json['floatingips'][0]
        self.assertEqual(fixture['ptrdname'], fip_record['ptrdname'])
        self.assertEqual(":".join([fip['region'], fip['id']]),
                         fip_record['id'])
        self.assertEqual(fip['address'], fip_record['address'])
        self.assertEqual(None, fip_record['description'])
        self.assertEqual(fixture['ptrdname'], fip_record['ptrdname'])

    def test_set_floatingip(self):
        self.create_server()
        fixture = self.get_ptr_fixture()

        fip = self.network_api.fake.allocate_floatingip('tenant')

        response = self.client.patch_json(
            '/reverse/floatingips/%s' % ":".join([fip['region'], fip['id']]),
            {"floatingip": fixture},
            headers={'X-Test-Tenant-Id': 'tenant'})

        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)
        self.assertIn('floatingip', response.json)

        fip_record = response.json['floatingip']
        self.assertEqual(":".join([fip['region'], fip['id']]),
                         fip_record['id'])
        self.assertEqual(fip['address'], fip_record['address'])
        self.assertEqual(None, fip_record['description'])
        self.assertEqual(fixture['ptrdname'], fip_record['ptrdname'])

    def test_set_floatingip_not_allocated(self):
        fixture = self.get_ptr_fixture()

        fip = self.network_api.fake.allocate_floatingip('tenant')
        self.network_api.fake.deallocate_floatingip(fip['id'])

        url = '/reverse/floatingips/%s' % ":".join([fip['region'], fip['id']])

        self._assert_exception('not_found', 404, self.client.patch_json, url,
                               {'floatingip': fixture})

    def test_set_floatingip_invalid_ptrdname(self):
        fip = self.network_api.fake.allocate_floatingip('tenant')

        url = '/reverse/floatingips/%s' % ":".join([fip['region'], fip['id']])

        self._assert_exception('invalid_object', 400, self.client.patch_json,
                               url, {'floatingip': {'ptrdname': 'test|'}})

    def test_set_floatingip_invalid_key(self):
        url = '/reverse/floatingips/%s' % 'foo:random'
        self._assert_exception('bad_request', 400, self.client.patch_json,
                               url, {})

    def test_unset_floatingip(self):
        self.create_server()

        fixture = self.get_ptr_fixture()
        context = self.get_context(tenant='a')

        fip = self.network_api.fake.allocate_floatingip(context.tenant)

        # Unsetting via "None"
        self.central_service.update_floatingip(
            context, fip['region'], fip['id'], fixture)

        # Unset PTR ('ptrdname' is None aka null in JSON)
        response = self.client.patch_json(
            '/reverse/floatingips/%s' % ":".join([fip['region'], fip['id']]),
            {'floatingip': {'ptrdname': None}},
            headers={'X-Test-Tenant-Id': context.tenant})
        self.assertEqual(None, response.json)
        self.assertEqual(200, response.status_int)

        fip = self.central_service.get_floatingip(
            context, fip['region'], fip['id'])
        self.assertEqual(None, fip['ptrdname'])

    def test_unset_floatingip_not_allocated(self):
        self.create_server()

        fixture = self.get_ptr_fixture()
        context = self.get_context(tenant='a')

        fip = self.network_api.fake.allocate_floatingip(context.tenant)

        self.central_service.update_floatingip(
            context, fip['region'], fip['id'], fixture)

        self.network_api.fake.deallocate_floatingip(fip['id'])

        url = '/reverse/floatingips/%s' % ":".join([fip['region'], fip['id']])

        self._assert_exception('not_found', 404, self.client.patch_json, url,
                               {"floatingip": {'ptrdname': None}})

########NEW FILE########
__FILENAME__ = test_limits
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from oslo.config import cfg
from designate.tests.test_api.test_v2 import ApiV2TestCase


class ApiV2LimitsTest(ApiV2TestCase):
    def test_get_limits(self):
        response = self.client.get('/limits/')

        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)

        self.assertIn('limits', response.json)
        self.assertIn('absolute', response.json['limits'])
        self.assertIn('maxZones', response.json['limits']['absolute'])
        self.assertIn('maxZoneRecords', response.json['limits']['absolute'])

        absolutelimits = response.json['limits']['absolute']

        self.assertEqual(cfg.CONF.quota_domains, absolutelimits['maxZones'])
        self.assertEqual(cfg.CONF.quota_domain_records,
                         absolutelimits['maxZoneRecords'])

########NEW FILE########
__FILENAME__ = test_nameservers
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from mock import patch
from oslo import messaging
from designate.central import service as central_service
from designate.tests.test_api.test_v2 import ApiV2TestCase


class ApiV2NameServersTest(ApiV2TestCase):
    def setUp(self):
        super(ApiV2NameServersTest, self).setUp()

        # Create a domain
        self.domain = self.create_domain()

    def test_get_nameservers(self):
        url = '/zones/%s/nameservers' % self.domain['id']

        response = self.client.get(url)

        # Check the headers are what we expect
        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Check the body structure is what we expect
        self.assertIn('nameservers', response.json)
        self.assertIn('links', response.json)
        self.assertIn('self', response.json['links'])

        # We should start with 0 nameservers
        self.assertEqual(1, len(response.json['nameservers']))

        servers = self.central_service.get_domain_servers(
            self.admin_context, self.domain['id'])

        self.assertEqual(servers[0]['id'],
                         response.json['nameservers'][0]['id'])
        self.assertEqual(servers[0]['name'],
                         response.json['nameservers'][0]['name'])

        self.create_server(name='nsx.mydomain.com.')

        response = self.client.get(url)

        self.assertEqual(2, len(response.json['nameservers']))

    def test_get_nameservers_invalid_id(self):
        self._assert_invalid_uuid(self.client.get, '/zones/%s/nameservers')

    @patch.object(central_service.Service, 'get_domain_servers',
                  side_effect=messaging.MessagingTimeout())
    def test_get_nameservers_timeout(self, _):
        url = '/zones/ba751950-6193-11e3-949a-0800200c9a66/nameservers'

        self._assert_exception('timeout', 504, self.client.get, url)

########NEW FILE########
__FILENAME__ = test_records
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from mock import patch
from oslo import messaging
from designate import exceptions
from designate.central import service as central_service
from designate.tests.test_api.test_v2 import ApiV2TestCase


class ApiV2RecordsTest(ApiV2TestCase):
    def setUp(self):
        super(ApiV2RecordsTest, self).setUp()

        # Create a domain
        self.domain = self.create_domain()

        name = 'www.%s' % self.domain['name']
        self.rrset = self.create_recordset(self.domain, name=name)

    def test_create(self):
        # Create a zone
        fixture = self.get_record_fixture(self.rrset['type'], fixture=0)

        url = '/zones/%s/recordsets/%s/records' % (
            self.domain['id'], self.rrset['id'])

        response = self.client.post_json(url, {'record': fixture})
        self.assertIn('record', response.json)
        self.assertIn('links', response.json['record'])
        self.assertIn('self', response.json['record']['links'])

        # Check the values returned are what we expect
        self.assertIn('id', response.json['record'])
        self.assertIn('created_at', response.json['record'])
        self.assertIsNone(response.json['record']['updated_at'])

        for k in fixture:
            self.assertEqual(fixture[k], response.json['record'][k])

    def test_create_validation(self):
        fixture = self.get_record_fixture(self.rrset['type'], fixture=0)

        # Add a junk field to the wrapper
        body = {'record': fixture, 'junk': 'Junk Field'}

        url = '/zones/%s/recordsets/%s/records' % (
            self.domain['id'], self.rrset['id'])

        # Ensure it fails with a 400
        self._assert_exception('invalid_object', 400, self.client.post_json,
                               url, body)

        # Add a junk field to the body
        fixture['junk'] = 'Junk Field'
        body = {'record': fixture}

        # Ensure it fails with a 400
        self._assert_exception('invalid_object', 400, self.client.post_json,
                               url, body)

    @patch.object(central_service.Service, 'create_record',
                  side_effect=messaging.MessagingTimeout())
    def test_create_recordset_timeout(self, _):
        fixture = self.get_record_fixture(self.rrset['type'], fixture=0)

        body = {'record': fixture}

        url = '/zones/%s/recordsets/%s/records' % (
            self.domain['id'], self.rrset['id'])

        self._assert_exception('timeout', 504, self.client.post_json, url,
                               body)

    @patch.object(central_service.Service, 'create_record',
                  side_effect=exceptions.DuplicateRecord())
    def test_create_record_duplicate(self, _):
        fixture = self.get_record_fixture(self.rrset['type'], fixture=0)

        body = {'record': fixture}

        url = '/zones/%s/recordsets/%s/records' % (
            self.domain['id'], self.rrset['id'])

        self._assert_exception('duplicate_record', 409, self.client.post_json,
                               url, body)

    def test_create_record_invalid_domain(self):
        fixture = self.get_record_fixture(self.rrset['type'], fixture=0)

        body = {'record': fixture}

        url = '/zones/ba751950-6193-11e3-949a-0800200c9a66/recordsets/' \
            'ba751950-6193-11e3-949a-0800200c9a66/records'

        self._assert_exception('domain_not_found', 404, self.client.post_json,
                               url, body)

    def test_create_record_invalid_rrset(self):
        fixture = self.get_record_fixture(self.rrset['type'], fixture=0)

        body = {'record': fixture}

        url = '/zones/%s/recordsets/' \
            'ba751950-6193-11e3-949a-0800200c9a66/records' % self.domain['id']

        self._assert_exception('recordset_not_found', 404,
                               self.client.post_json, url, body)

    def test_get_records(self):
        url = '/zones/%s/recordsets/%s/records' % (
            self.domain['id'], self.rrset['id'])
        response = self.client.get(url)

        # Check the headers are what we expect
        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Check the body structure is what we expect
        self.assertIn('records', response.json)
        self.assertIn('links', response.json)
        self.assertIn('self', response.json['links'])

        # We should start with 0 recordsets
        self.assertEqual(0, len(response.json['records']))

        data = [self.create_record(self.domain, self.rrset,
                data='192.168.0.%s' % i) for i in xrange(2, 10)]

        self._assert_paging(data, url, key='records')

        self._assert_invalid_paging(data, url, key='records')

    @patch.object(central_service.Service, 'find_records',
                  side_effect=messaging.MessagingTimeout())
    def test_get_records_timeout(self, _):
        url = '/zones/ba751950-6193-11e3-949a-0800200c9a66/recordsets/' \
            'ba751950-6193-11e3-949a-0800200c9a66/records'

        self._assert_exception('timeout', 504, self.client.get, url)

    def test_get_record(self):
        # Create a record
        record = self.create_record(self.domain, self.rrset)

        url = '/zones/%s/recordsets/%s/records/%s' % (
            self.domain['id'], self.rrset['id'], record['id'])
        response = self.client.get(url)

        # Check the headers are what we expect
        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Check the body structure is what we expect
        self.assertIn('record', response.json)
        self.assertIn('links', response.json['record'])
        self.assertIn('self', response.json['record']['links'])

        # Check the values returned are what we expect
        self.assertIn('id', response.json['record'])
        self.assertIn('created_at', response.json['record'])
        self.assertIn('version', response.json['record'])
        self.assertIsNone(response.json['record']['updated_at'])
        self.assertEqual(record['data'], response.json['record']['data'])

    @patch.object(central_service.Service, 'get_record',
                  side_effect=messaging.MessagingTimeout())
    def test_get_record_timeout(self, _):
        url = '/zones/%s/recordsets/%s/records/' \
            'ba751950-6193-11e3-949a-0800200c9a66' % (
                self.domain['id'], self.rrset['id'])

        self._assert_exception('timeout', 504, self.client.get, url,
                               headers={'Accept': 'application/json'})

    @patch.object(central_service.Service, 'get_record',
                  side_effect=exceptions.RecordNotFound())
    def test_get_record_missing(self, _):
        url = '/zones/%s/recordsets/%s/records/' \
            'ba751950-6193-11e3-949a-0800200c9a66' % (
                self.domain['id'], self.rrset['id'])

        self._assert_exception('record_not_found', 404, self.client.get, url,
                               headers={'Accept': 'application/json'})

    def test_get_record_invalid_id(self):
        url = '/zones/%s/recordsets/%s/records/%s'

        self._assert_invalid_uuid(self.client.get, url)

    def test_update_record(self):
        # Create a recordset
        record = self.create_record(self.domain, self.rrset)

        # Prepare an update body
        body = {'record': {'description': 'Tester'}}

        url = '/zones/%s/recordsets/%s/records/%s' % (
            self.domain['id'], self.rrset['id'], record['id'])
        response = self.client.patch_json(url, body, status=200)

        # Check the headers are what we expect
        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Check the body structure is what we expect
        self.assertIn('record', response.json)
        self.assertIn('links', response.json['record'])
        self.assertIn('self', response.json['record']['links'])

        # Check the values returned are what we expect
        self.assertIn('id', response.json['record'])
        self.assertIsNotNone(response.json['record']['updated_at'])
        self.assertEqual('Tester', response.json['record']['description'])

    def test_update_record_validation(self):
        # NOTE: The schemas should be tested separatly to the API. So we
        #       don't need to test every variation via the API itself.
        # Create a zone
        record = self.create_record(self.domain, self.rrset)

        url = '/zones/%s/recordsets/%s/records/%s' % (
            self.domain['id'], self.rrset['id'], record['id'])

        # Prepare an update body with junk in the wrapper
        body = {'record': {'description': 'Tester'}, 'junk': 'Junk Field'}

        # Ensure it fails with a 400
        self._assert_exception('invalid_object', 400, self.client.patch_json,
                               url, body)

        # Prepare an update body with junk in the body
        body = {'record': {'description': 'Tester', 'junk': 'Junk Field'}}

        # Ensure it fails with a 400
        self._assert_exception('invalid_object', 400, self.client.patch_json,
                               url, body)

    @patch.object(central_service.Service, 'get_record',
                  side_effect=exceptions.DuplicateRecord())
    def test_update_record_duplicate(self, _):
        url = '/zones/%s/recordsets/%s/records/' \
            'ba751950-6193-11e3-949a-0800200c9a66' % (
                self.domain['id'], self.rrset['id'])

        # Prepare an update body
        body = {'record': {'description': 'Tester'}}

        # Ensure it fails with a 409
        self._assert_exception('duplicate_record', 409, self.client.patch_json,
                               url, body,
                               headers={'Accept': 'application/json'})

    @patch.object(central_service.Service, 'get_record',
                  side_effect=messaging.MessagingTimeout())
    def test_update_record_timeout(self, _):
        url = '/zones/%s/recordsets/%s/records/' \
            'ba751950-6193-11e3-949a-0800200c9a66' % (
                self.domain['id'], self.rrset['id'])

        # Prepare an update body
        body = {'record': {'description': 'Tester'}}

        # Ensure it fails with a 504
        self._assert_exception('timeout', 504, self.client.patch_json,
                               url, body)

    @patch.object(central_service.Service, 'get_record',
                  side_effect=exceptions.RecordNotFound())
    def test_update_record_missing(self, _):
        url = '/zones/%s/recordsets/%s/records/' \
            'ba751950-6193-11e3-949a-0800200c9a66' % (
                self.domain['id'], self.rrset['id'])

        # Prepare an update body
        body = {'record': {'description': 'Tester'}}

        # Ensure it fails with a 404
        self._assert_exception('record_not_found', 404, self.client.patch_json,
                               url, body)

    def test_update_record_invalid_id(self):
        url = '/zones/%s/recordsets/%s/records/%s'
        self._assert_invalid_uuid(self.client.patch_json, url)

    def test_delete_record(self):
        record = self.create_record(self.domain, self.rrset)

        url = '/zones/%s/recordsets/%s/records/%s' % (
            self.domain['id'], self.rrset['id'], record['id'])

        self.client.delete(url, status=204)

    @patch.object(central_service.Service, 'delete_record',
                  side_effect=messaging.MessagingTimeout())
    def test_delete_record_timeout(self, _):
        url = '/zones/%s/recordsets/%s/records/' \
            'ba751950-6193-11e3-949a-0800200c9a66' % (
                self.domain['id'], self.rrset['id'])

        self._assert_exception('timeout', 504, self.client.delete, url)

    @patch.object(central_service.Service, 'delete_record',
                  side_effect=exceptions.RecordNotFound())
    def test_delete_record_missing(self, _):
        url = '/zones/%s/recordsets/%s/records/' \
            'ba751950-6193-11e3-949a-0800200c9a66' % (
                self.domain['id'], self.rrset['id'])

        self._assert_exception('record_not_found', 404, self.client.delete,
                               url)

    def test_delete_record_invalid_id(self):
        url = '/zones/%s/recordsets/%s/records/%s'

        self._assert_invalid_uuid(self.client.delete, url)

########NEW FILE########
__FILENAME__ = test_recordsets
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from mock import patch
from oslo import messaging
from designate import exceptions
from designate.central import service as central_service
from designate.tests.test_api.test_v2 import ApiV2TestCase


class ApiV2RecordSetsTest(ApiV2TestCase):
    def setUp(self):
        super(ApiV2RecordSetsTest, self).setUp()

        # Create a domain
        self.domain = self.create_domain()

    def test_create_recordset(self):
        # Create a zone
        fixture = self.get_recordset_fixture(self.domain['name'], fixture=0)
        response = self.client.post_json(
            '/zones/%s/recordsets' % self.domain['id'], {'recordset': fixture})

        # Check the headers are what we expect
        self.assertEqual(201, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Check the body structure is what we expect
        self.assertIn('recordset', response.json)
        self.assertIn('links', response.json['recordset'])
        self.assertIn('self', response.json['recordset']['links'])

        # Check the values returned are what we expect
        self.assertIn('id', response.json['recordset'])
        self.assertIn('created_at', response.json['recordset'])
        self.assertIsNone(response.json['recordset']['updated_at'])

        for k in fixture:
            self.assertEqual(fixture[k], response.json['recordset'][k])

    def test_create_recordset_invalid_id(self):
        self._assert_invalid_uuid(self.client.post, '/zones/%s/recordsets')

    def test_create_recordset_validation(self):
        # NOTE: The schemas should be tested separatly to the API. So we
        #       don't need to test every variation via the API itself.
        # Fetch a fixture
        fixture = self.get_recordset_fixture(self.domain['name'], fixture=0)

        # Add a junk field to the wrapper
        body = {'recordset': fixture, 'junk': 'Junk Field'}

        url = '/zones/%s/recordsets' % self.domain['id']

        # Ensure it fails with a 400
        self._assert_exception(
            'invalid_object', 400, self.client.post_json, url, body)

        # Add a junk field to the body
        fixture['junk'] = 'Junk Field'
        body = {'recordset': fixture}

        # Ensure it fails with a 400
        self._assert_exception(
            'invalid_object', 400, self.client.post_json, url, body)

    @patch.object(central_service.Service, 'create_recordset',
                  side_effect=messaging.MessagingTimeout())
    def test_create_recordset_timeout(self, _):
        fixture = self.get_recordset_fixture(self.domain['name'], fixture=0)

        body = {'recordset': fixture}

        url = '/zones/%s/recordsets' % self.domain['id']

        self._assert_exception('timeout', 504, self.client.post_json, url,
                               body)

    @patch.object(central_service.Service, 'create_recordset',
                  side_effect=exceptions.DuplicateRecordSet())
    def test_create_recordset_duplicate(self, _):
        fixture = self.get_recordset_fixture(self.domain['name'], fixture=0)

        body = {'recordset': fixture}

        url = '/zones/%s/recordsets' % self.domain['id']

        self._assert_exception('duplicate_recordset', 409,
                               self.client.post_json, url, body)

    def test_create_recordset_invalid_domain(self):
        fixture = self.get_recordset_fixture(self.domain['name'], fixture=0)

        body = {'recordset': fixture}

        url = '/zones/ba751950-6193-11e3-949a-0800200c9a66/recordsets'

        self._assert_exception('domain_not_found', 404, self.client.post_json,
                               url, body)

    def test_recordsets_invalid_url(self):
        url = '/zones/recordsets'
        self._assert_exception('not_found', 404, self.client.get, url)
        self._assert_exception('not_found', 404, self.client.post_json, url)

        # Pecan returns a 405 for Patch and delete operations
        response = self.client.patch_json(url, status=405)
        self.assertEqual(405, response.status_int)

        response = self.client.delete(url, status=405)
        self.assertEqual(405, response.status_int)

    def test_get_recordsets(self):
        url = '/zones/%s/recordsets' % self.domain['id']

        response = self.client.get(url)

        # Check the headers are what we expect
        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Check the body structure is what we expect
        self.assertIn('recordsets', response.json)
        self.assertIn('links', response.json)
        self.assertIn('self', response.json['links'])

        # We should start with 0 recordsets
        self.assertEqual(0, len(response.json['recordsets']))

        data = [self.create_recordset(self.domain,
                name='x-%s.%s' % (i, self.domain['name']))
                for i in xrange(0, 10)]

        self._assert_paging(data, url, key='recordsets')

        self._assert_invalid_paging(data, url, key='recordsets')

    def test_get_recordsets_invalid_id(self):
        self._assert_invalid_uuid(self.client.get, '/zones/%s/recordsets')

    @patch.object(central_service.Service, 'find_recordsets',
                  side_effect=messaging.MessagingTimeout())
    def test_get_recordsets_timeout(self, _):
        url = '/zones/ba751950-6193-11e3-949a-0800200c9a66/recordsets'

        self._assert_exception('timeout', 504, self.client.get, url)

    def test_get_recordset(self):
        # Create a recordset
        recordset = self.create_recordset(self.domain)

        url = '/zones/%s/recordsets/%s' % (self.domain['id'], recordset['id'])
        response = self.client.get(url)

        # Check the headers are what we expect
        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Check the body structure is what we expect
        self.assertIn('recordset', response.json)
        self.assertIn('links', response.json['recordset'])
        self.assertIn('self', response.json['recordset']['links'])

        # Check the values returned are what we expect
        self.assertIn('id', response.json['recordset'])
        self.assertIn('created_at', response.json['recordset'])
        self.assertIsNone(response.json['recordset']['updated_at'])
        self.assertEqual(recordset['name'], response.json['recordset']['name'])
        self.assertEqual(recordset['type'], response.json['recordset']['type'])

    def test_get_recordset_invalid_id(self):
        self._assert_invalid_uuid(self.client.get, '/zones/%s/recordsets/%s')

    @patch.object(central_service.Service, 'get_recordset',
                  side_effect=messaging.MessagingTimeout())
    def test_get_recordset_timeout(self, _):
        url = '/zones/%s/recordsets/ba751950-6193-11e3-949a-0800200c9a66' % (
            self.domain['id'])

        self._assert_exception('timeout', 504, self.client.get, url,
                               headers={'Accept': 'application/json'})

    @patch.object(central_service.Service, 'get_recordset',
                  side_effect=exceptions.RecordSetNotFound())
    def test_get_recordset_missing(self, _):
        url = '/zones/%s/recordsets/ba751950-6193-11e3-949a-0800200c9a66' % (
            self.domain['id'])

        self._assert_exception('recordset_not_found', 404,
                               self.client.get, url,
                               headers={'Accept': 'application/json'})

    def test_update_recordset(self):
        # Create a recordset
        recordset = self.create_recordset(self.domain)

        # Prepare an update body
        body = {'recordset': {'description': 'Tester'}}

        url = '/zones/%s/recordsets/%s' % (recordset['domain_id'],
                                           recordset['id'])
        response = self.client.patch_json(url, body, status=200)

        # Check the headers are what we expect
        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Check the body structure is what we expect
        self.assertIn('recordset', response.json)
        self.assertIn('links', response.json['recordset'])
        self.assertIn('self', response.json['recordset']['links'])

        # Check the values returned are what we expect
        self.assertIn('id', response.json['recordset'])
        self.assertIsNotNone(response.json['recordset']['updated_at'])
        self.assertEqual('Tester', response.json['recordset']['description'])

    def test_update_recordset_invalid_id(self):
        self._assert_invalid_uuid(
            self.client.patch_json, '/zones/%s/recordsets/%s')

    def test_update_recordset_validation(self):
        # NOTE: The schemas should be tested separatly to the API. So we
        #       don't need to test every variation via the API itself.
        # Create a zone
        recordset = self.create_recordset(self.domain)

        # Prepare an update body with junk in the wrapper
        body = {'recordset': {'description': 'Tester'}, 'junk': 'Junk Field'}

        # Ensure it fails with a 400
        url = '/zones/%s/recordsets/%s' % (recordset['domain_id'],
                                           recordset['id'])

        self._assert_exception('invalid_object', 400, self.client.patch_json,
                               url, body)

        # Prepare an update body with junk in the body
        body = {'recordset': {'description': 'Tester', 'junk': 'Junk Field'}}

        # Ensure it fails with a 400
        self._assert_exception('invalid_object', 400, self.client.patch_json,
                               url, body)

    @patch.object(central_service.Service, 'get_recordset',
                  side_effect=exceptions.DuplicateRecordSet())
    def test_update_recordset_duplicate(self, _):
        # Prepare an update body
        body = {'recordset': {'description': 'Tester'}}

        # Ensure it fails with a 409
        url = ('/zones/%s/recordsets/ba751950-6193-11e3-949a-0800200c9a66'
               % (self.domain['id']))

        self._assert_exception('duplicate_recordset', 409,
                               self.client.patch_json, url, body)

    @patch.object(central_service.Service, 'get_recordset',
                  side_effect=messaging.MessagingTimeout())
    def test_update_recordset_timeout(self, _):
        # Prepare an update body
        body = {'recordset': {'description': 'Tester'}}

        # Ensure it fails with a 504
        url = ('/zones/%s/recordsets/ba751950-6193-11e3-949a-0800200c9a66'
               % (self.domain['id']))

        self._assert_exception('timeout', 504, self.client.patch_json, url,
                               body)

    @patch.object(central_service.Service, 'get_recordset',
                  side_effect=exceptions.RecordSetNotFound())
    def test_update_recordset_missing(self, _):
        # Prepare an update body
        body = {'recordset': {'description': 'Tester'}}

        # Ensure it fails with a 404
        url = ('/zones/%s/recordsets/ba751950-6193-11e3-949a-0800200c9a66'
               % (self.domain['id']))

        self._assert_exception('recordset_not_found', 404,
                               self.client.patch_json, url, body)

    def test_delete_recordset(self):
        recordset = self.create_recordset(self.domain)

        url = '/zones/%s/recordsets/%s' % (recordset['domain_id'],
                                           recordset['id'])
        self.client.delete(url, status=204)

    @patch.object(central_service.Service, 'delete_recordset',
                  side_effect=messaging.MessagingTimeout())
    def test_delete_recordset_timeout(self, _):
        url = ('/zones/%s/recordsets/ba751950-6193-11e3-949a-0800200c9a66'
               % (self.domain['id']))

        self._assert_exception('timeout', 504, self.client.delete, url)

    @patch.object(central_service.Service, 'delete_recordset',
                  side_effect=exceptions.RecordSetNotFound())
    def test_delete_recordset_missing(self, _):
        url = ('/zones/%s/recordsets/ba751950-6193-11e3-949a-0800200c9a66'
               % (self.domain['id']))

        self._assert_exception('recordset_not_found', 404,
                               self.client.delete, url)

    def test_delete_recordset_invalid_id(self):
        self._assert_invalid_uuid(
            self.client.delete, '/zones/%s/recordsets/%s')

########NEW FILE########
__FILENAME__ = test_tlds
# Copyright (c) 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from designate.tests.test_api.test_v2 import ApiV2TestCase


class ApiV2TldsTest(ApiV2TestCase):
    def setUp(self):
        super(ApiV2TldsTest, self).setUp()

    def test_create_tld(self):
        self.policy({'create_tld': '@'})
        fixture = self.get_tld_fixture(0)
        response = self.client.post_json('/tlds/', {'tld': fixture})

        # Check the headers are what we expect
        self.assertEqual(201, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Check the body structure is what we expect
        self.assertIn('tld', response.json)
        self.assertIn('links', response.json['tld'])
        self.assertIn('self', response.json['tld']['links'])

        # Check the values returned are what we expect
        self.assertIn('id', response.json['tld'])
        self.assertIn('created_at', response.json['tld'])
        self.assertIsNone(response.json['tld']['updated_at'])
        self.assertEqual(fixture['name'], response.json['tld']['name'])

    def test_create_tld_validation(self):
        self.policy({'create_tld': '@'})
        invalid_fixture = self.get_tld_fixture(-1)

        # Ensure it fails with a 400
        self._assert_exception('invalid_object', 400, self.client.post_json,
                               '/tlds', {'tld': invalid_fixture})

    def test_get_tlds(self):
        self.policy({'find_tlds': '@'})
        response = self.client.get('/tlds/')

        # Check the headers are what we expect
        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Check the body structure is what we expect
        self.assertIn('tlds', response.json)
        self.assertIn('links', response.json)
        self.assertIn('self', response.json['links'])

        # We should start with 0 tlds
        self.assertEqual(0, len(response.json['tlds']))

        data = [self.create_tld(name='tld%s.' % i) for i in 'abcdefghijklmn']
        self._assert_paging(data, '/tlds', key='tlds')

    def test_get_tld(self):
        tld = self.create_tld(fixture=0)
        self.policy({'get_tld': '@'})

        response = self.client.get('/tlds/%s' % tld['id'],
                                   headers=[('Accept', 'application/json')])

        # Check the headers are what we expect
        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Check the body structure is what we expect
        self.assertIn('tld', response.json)
        self.assertIn('links', response.json['tld'])
        self.assertIn('self', response.json['tld']['links'])

        # Check the values returned are what we expect
        self.assertIn('id', response.json['tld'])
        self.assertIn('created_at', response.json['tld'])
        self.assertIsNone(response.json['tld']['updated_at'])
        self.assertEqual(self.get_tld_fixture(0)['name'],
                         response.json['tld']['name'])

    def test_get_tld_invalid_id(self):
        self._assert_invalid_uuid(self.client.get, '/tlds/%s')

    def test_delete_tld(self):
        tld = self.create_tld(fixture=0)
        self.policy({'delete_tld': '@'})

        self.client.delete('/tlds/%s' % tld['id'], status=204)

    def test_delete_tld_invalid_id(self):
        self._assert_invalid_uuid(self.client.delete, '/tlds/%s')

    def test_update_tld(self):
        tld = self.create_tld(fixture=0)
        self.policy({'update_tld': '@'})

        # Prepare an update body
        body = {'tld': {'description': 'prefix-%s' % tld['description']}}

        response = self.client.patch_json('/tlds/%s' % tld['id'], body,
                                          status=200)

        # Check the headers are what we expect
        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Check the body structure is what we expect
        self.assertIn('tld', response.json)
        self.assertIn('links', response.json['tld'])
        self.assertIn('self', response.json['tld']['links'])

        # Check the values returned are what we expect
        self.assertIn('id', response.json['tld'])
        self.assertIsNotNone(response.json['tld']['updated_at'])
        self.assertEqual('prefix-%s' % tld['description'],
                         response.json['tld']['description'])

    def test_update_tld_invalid_id(self):
        self._assert_invalid_uuid(self.client.patch_json, '/tlds/%s')

########NEW FILE########
__FILENAME__ = test_zones
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from dns import zone as dnszone
from mock import patch
from oslo import messaging
from designate import exceptions
from designate.central import service as central_service
from designate.tests.test_api.test_v2 import ApiV2TestCase


class ApiV2ZonesTest(ApiV2TestCase):
    def setUp(self):
        super(ApiV2ZonesTest, self).setUp()

        # Create a server
        self.create_server()

        # Create the default TLDs
        self.create_default_tlds()

    def test_create_zone(self):
        # Create a zone
        fixture = self.get_domain_fixture(0)
        response = self.client.post_json('/zones/', {'zone': fixture})

        # Check the headers are what we expect
        self.assertEqual(201, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Check the body structure is what we expect
        self.assertIn('zone', response.json)
        self.assertIn('links', response.json['zone'])
        self.assertIn('self', response.json['zone']['links'])

        # Check the values returned are what we expect
        self.assertIn('id', response.json['zone'])
        self.assertIn('created_at', response.json['zone'])
        self.assertEqual('ACTIVE', response.json['zone']['status'])
        self.assertIsNone(response.json['zone']['updated_at'])

        for k in fixture:
            self.assertEqual(fixture[k], response.json['zone'][k])

    def test_create_zone_validation(self):
        # NOTE: The schemas should be tested separately to the API. So we
        #       don't need to test every variation via the API itself.
        # Fetch a fixture
        fixture = self.get_domain_fixture(0)

        # Add a junk field to the wrapper
        body = {'zone': fixture, 'junk': 'Junk Field'}

        # Ensure it fails with a 400
        self._assert_exception('invalid_object', 400, self.client.post_json,
                               '/zones', body)

        # Add a junk field to the body
        fixture['junk'] = 'Junk Field'

        # Ensure it fails with a 400
        body = {'zone': fixture}

        self._assert_exception('invalid_object', 400, self.client.post_json,
                               '/zones', body)

    def test_create_zone_body_validation(self):
        fixture = self.get_domain_fixture(0)
        # Add id to the body
        fixture['id'] = '2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980'
        # Ensure it fails with a 400
        body = {'zone': fixture}
        self._assert_exception('invalid_object', 400, self.client.post_json,
                               '/zones', body)

        fixture = self.get_domain_fixture(0)
        # Add created_at to the body
        fixture['created_at'] = '2014-03-12T19:07:53.000000'
        # Ensure it fails with a 400
        body = {'zone': fixture}
        self._assert_exception('invalid_object', 400, self.client.post_json,
                               '/zones', body)

    def test_create_zone_invalid_name(self):
        # Try to create a zone with an invalid name
        fixture = self.get_domain_fixture(-1)

        # Ensure it fails with a 400
        self._assert_exception('invalid_object', 400, self.client.post_json,
                               '/zones', {'zone': fixture})

    @patch.object(central_service.Service, 'create_domain',
                  side_effect=messaging.MessagingTimeout())
    def test_create_zone_timeout(self, _):
        fixture = self.get_domain_fixture(0)

        body = {'zone': fixture}

        self._assert_exception('timeout', 504, self.client.post_json,
                               '/zones/', body)

    @patch.object(central_service.Service, 'create_domain',
                  side_effect=exceptions.DuplicateDomain())
    def test_create_zone_duplicate(self, _):
        fixture = self.get_domain_fixture(0)

        body = {'zone': fixture}

        self._assert_exception('duplicate_domain', 409, self.client.post_json,
                               '/zones/', body)

    def test_create_zone_missing_content_type(self):
        self._assert_exception('unsupported_content_type', 415,
                               self.client.post, '/zones')

    def test_create_zone_bad_content_type(self):
        self._assert_exception(
            'unsupported_content_type', 415, self.client.post, '/zones',
            headers={'Content-type': 'test/goat'})

    def test_zone_invalid_url(self):
        url = '/zones/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980/invalid'
        self._assert_exception('not_found', 404, self.client.get, url,
                               headers={'Accept': 'application/json'})
        self._assert_exception('not_found', 404, self.client.patch_json, url)
        self._assert_exception('not_found', 404, self.client.delete, url)

        # Pecan returns a 405 for post
        response = self.client.post(url, status=405)
        self.assertEqual(405, response.status_int)

    def test_get_zones(self):
        response = self.client.get('/zones/')

        # Check the headers are what we expect
        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Check the body structure is what we expect
        self.assertIn('zones', response.json)
        self.assertIn('links', response.json)
        self.assertIn('self', response.json['links'])

        # We should start with 0 zones
        self.assertEqual(0, len(response.json['zones']))

        # We should start with 0 zones
        self.assertEqual(0, len(response.json['zones']))

        data = [self.create_domain(name='x-%s.com.' % i)
                for i in 'abcdefghij']
        self._assert_paging(data, '/zones', key='zones')

        self._assert_invalid_paging(data, '/zones', key='zones')

    @patch.object(central_service.Service, 'find_domains',
                  side_effect=messaging.MessagingTimeout())
    def test_get_zones_timeout(self, _):
        self._assert_exception('timeout', 504, self.client.get, '/zones/')

    def test_get_zone(self):
        # Create a zone
        zone = self.create_domain()

        response = self.client.get('/zones/%s' % zone['id'],
                                   headers=[('Accept', 'application/json')])

        # Check the headers are what we expect
        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Check the body structure is what we expect
        self.assertIn('zone', response.json)
        self.assertIn('links', response.json['zone'])
        self.assertIn('self', response.json['zone']['links'])

        # Check the values returned are what we expect
        self.assertIn('id', response.json['zone'])
        self.assertIn('created_at', response.json['zone'])
        self.assertEqual('ACTIVE', response.json['zone']['status'])
        self.assertIsNone(response.json['zone']['updated_at'])
        self.assertEqual(zone['name'], response.json['zone']['name'])
        self.assertEqual(zone['email'], response.json['zone']['email'])

    def test_get_zone_invalid_id(self):
        self._assert_invalid_uuid(self.client.get, '/zones/%s')

    @patch.object(central_service.Service, 'get_domain',
                  side_effect=messaging.MessagingTimeout())
    def test_get_zone_timeout(self, _):
        url = '/zones/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980'
        self._assert_exception('timeout', 504, self.client.get, url,
                               headers={'Accept': 'application/json'})

    @patch.object(central_service.Service, 'get_domain',
                  side_effect=exceptions.DomainNotFound())
    def test_get_zone_missing(self, _):
        url = '/zones/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980'
        self._assert_exception('domain_not_found', 404, self.client.get, url,
                               headers={'Accept': 'application/json'})

    def test_get_zone_missing_accept(self):
        url = '/zones/6e2146f3-87bc-4f47-adc5-4df0a5c78218'

        self._assert_exception('bad_request', 400, self.client.get, url)

    def test_get_zone_bad_accept(self):
        url = '/zones/6e2146f3-87bc-4f47-adc5-4df0a5c78218'

        self.client.get(url, headers={'Accept': 'test/goat'}, status=406)

    def test_update_zone(self):
        # Create a zone
        zone = self.create_domain()

        # Prepare an update body
        body = {'zone': {'email': 'prefix-%s' % zone['email']}}

        response = self.client.patch_json('/zones/%s' % zone['id'], body,
                                          status=200)

        # Check the headers are what we expect
        self.assertEqual(200, response.status_int)
        self.assertEqual('application/json', response.content_type)

        # Check the body structure is what we expect
        self.assertIn('zone', response.json)
        self.assertIn('links', response.json['zone'])
        self.assertIn('self', response.json['zone']['links'])
        self.assertIn('status', response.json['zone'])

        # Check the values returned are what we expect
        self.assertIn('id', response.json['zone'])
        self.assertIsNotNone(response.json['zone']['updated_at'])
        self.assertEqual('prefix-%s' % zone['email'],
                         response.json['zone']['email'])

    def test_update_zone_invalid_id(self):
        self._assert_invalid_uuid(self.client.patch_json, '/zones/%s')

    def test_update_zone_validation(self):
        # NOTE: The schemas should be tested separatly to the API. So we
        #       don't need to test every variation via the API itself.
        # Create a zone
        zone = self.create_domain()

        # Prepare an update body with junk in the wrapper
        body = {'zone': {'email': 'prefix-%s' % zone['email']},
                'junk': 'Junk Field'}

        url = '/zones/%s' % zone['id']

        # Ensure it fails with a 400
        self._assert_exception('invalid_object', 400, self.client.patch_json,
                               url, body)

        # Prepare an update body with junk in the body
        body = {'zone': {'email': 'prefix-%s' % zone['email'],
                         'junk': 'Junk Field'}}

        # Ensure it fails with a 400
        self._assert_exception('invalid_object', 400, self.client.patch_json,
                               url, body)

    @patch.object(central_service.Service, 'get_domain',
                  side_effect=exceptions.DuplicateDomain())
    def test_update_zone_duplicate(self, _):
        # Prepare an update body
        body = {'zone': {'email': 'example@example.org'}}

        url = '/zones/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980'

        # Ensure it fails with a 409
        self._assert_exception('duplicate_domain', 409, self.client.patch_json,
                               url, body)

    @patch.object(central_service.Service, 'get_domain',
                  side_effect=messaging.MessagingTimeout())
    def test_update_zone_timeout(self, _):
        # Prepare an update body
        body = {'zone': {'email': 'example@example.org'}}

        url = '/zones/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980'

        # Ensure it fails with a 504
        self._assert_exception('timeout', 504, self.client.patch_json,
                               url, body)

    @patch.object(central_service.Service, 'get_domain',
                  side_effect=exceptions.DomainNotFound())
    def test_update_zone_missing(self, _):
        # Prepare an update body
        body = {'zone': {'email': 'example@example.org'}}

        url = '/zones/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980'

        # Ensure it fails with a 404
        self._assert_exception('domain_not_found', 404, self.client.patch_json,
                               url, body)

    def test_delete_zone(self):
        zone = self.create_domain()

        self.client.delete('/zones/%s' % zone['id'], status=204)

    def test_delete_zone_invalid_id(self):
        self._assert_invalid_uuid(self.client.delete, '/zones/%s')

    @patch.object(central_service.Service, 'delete_domain',
                  side_effect=messaging.MessagingTimeout())
    def test_delete_zone_timeout(self, _):
        url = '/zones/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980'

        self._assert_exception('timeout', 504, self.client.delete, url)

    @patch.object(central_service.Service, 'delete_domain',
                  side_effect=exceptions.DomainNotFound())
    def test_delete_zone_missing(self, _):
        url = '/zones/2fdadfb1-cf96-4259-ac6b-bb7b6d2ff980'

        self._assert_exception('domain_not_found', 404, self.client.delete,
                               url)

    # Zone import/export
    def test_missing_origin(self):
        fixture = self.get_zonefile_fixture(variant='noorigin')

        self._assert_exception('bad_request', 400, self.client.post, '/zones',
                               fixture, headers={'Content-type': 'text/dns'})

    def test_missing_soa(self):
        fixture = self.get_zonefile_fixture(variant='nosoa')

        self._assert_exception('bad_request', 400, self.client.post, '/zones',
                               fixture, headers={'Content-type': 'text/dns'})

    def test_malformed_zonefile(self):
        fixture = self.get_zonefile_fixture(variant='malformed')

        self._assert_exception('bad_request', 400, self.client.post, '/zones',
                               fixture, headers={'Content-type': 'text/dns'})

    def test_import_export(self):
        # Since v2 doesn't support getting records, import and export the
        # fixture, making sure they're the same according to dnspython
        post_response = self.client.post('/zones',
                                         self.get_zonefile_fixture(),
                                         headers={'Content-type': 'text/dns'})
        get_response = self.client.get('/zones/%s' %
                                       post_response.json['zone']['id'],
                                       headers={'Accept': 'text/dns'})
        exported_zonefile = get_response.body
        imported = dnszone.from_text(self.get_zonefile_fixture())
        exported = dnszone.from_text(exported_zonefile)
        # Compare SOA emails, since zone comparison takes care of origin
        imported_soa = imported.get_rdataset(imported.origin, 'SOA')
        imported_email = imported_soa[0].rname.to_text()
        exported_soa = exported.get_rdataset(exported.origin, 'SOA')
        exported_email = exported_soa[0].rname.to_text()
        self.assertEqual(imported_email, exported_email)
        # Delete SOAs since they have, at the very least, different serials,
        # and dnspython considers that to be not equal.
        imported.delete_rdataset(imported.origin, 'SOA')
        exported.delete_rdataset(exported.origin, 'SOA')
        # Delete non-delegation NS, since they won't be the same
        imported.delete_rdataset(imported.origin, 'NS')
        exported.delete_rdataset(exported.origin, 'NS')
        self.assertEqual(imported, exported)

########NEW FILE########
__FILENAME__ = test_backends
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from testscenarios import load_tests_apply_scenarios as load_tests  # noqa
from designate import tests
from designate.tests.test_backend.test_nsd4slave import NSD4Fixture
from designate.tests.test_backend import BackendTestMixin


class BackendTestCase(tests.TestCase, BackendTestMixin):
    scenarios = [
        ('bind9', dict(backend_driver='bind9', group='service:agent')),
        ('fake', dict(backend_driver='fake', group='service:agent')),
        ('nsd4slave', dict(backend_driver='nsd4slave', group='service:agent',
                           server_fixture=NSD4Fixture)),
        ('powerdns', dict(backend_driver='powerdns', group='service:agent')),
        ('ipa', dict(backend_driver='ipa', group='service:agent'))
    ]

    def setUp(self):
        super(BackendTestCase, self).setUp()

        if hasattr(self, 'server_fixture'):
            self.useFixture(self.server_fixture())

        self.config(backend_driver=self.backend_driver, group=self.group)

########NEW FILE########
__FILENAME__ = test_ipa
# Copyright (C) 2014 Red Hat, Inc.
#
# Author: Rich Megginson <rmeggins@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import fixtures
from mock import MagicMock
from requests.auth import AuthBase
from designate import tests
from designate import utils
from designate.tests.test_backend import BackendTestMixin
from designate.openstack.common import jsonutils as json
from designate.backend import impl_ipa

ipamethods = {"dnszone_add": {}, "dnszone_mod": {},
              "dnszone_del": {}, "dnsrecord_add": {},
              "dnsrecord_mod": {}, "dnsrecord_del": {},
              }


class MockIPAAuth(AuthBase):
    def __init__(self, hostname, keytab):
        self.count = 0

    def refresh_auth(self):
        self.count += 1


class MockResponse(object):
    def __init__(self, status_code, jsontext):
        self.status_code = status_code
        self.text = jsontext


class MockRequest(object):
    def __init__(self, testcase):
        self.headers = {}
        self.myauth = MockIPAAuth("ignore", "ignore")
        self.testcase = testcase
        self.error = None
        self.needauth = False

    @property
    def auth(self):
        # always return the mock object
        return self.myauth

    @auth.setter
    def auth(self, val):
        # disallow setting
        pass

    def post(self, jsonurl, data):
        # convert json data string to dict
        ddict = json.loads(data)
        # check basic parameters
        self.testcase.assertIn('method', ddict)
        meth = ddict['method']
        self.testcase.assertIn(meth, ipamethods)
        self.testcase.assertIn('params', ddict)
        self.testcase.assertIsInstance(ddict['params'], list)
        self.testcase.assertEqual(len(ddict['params']), 2)
        self.testcase.assertIsInstance(ddict['params'][0], list)
        self.testcase.assertIsInstance(ddict['params'][1], dict)
        self.testcase.assertIn('version', ddict['params'][1])
        # check method specific parameters
        if meth.startswith('dnsrecord_'):
            self.testcase.assertEqual(len(ddict['params'][0]), 2)
            # domain params end with a .
            param1 = ddict['params'][0][0]
            self.testcase.assertEqual(param1[-1], ".")
        elif meth.startswith('dnszone_'):
            self.testcase.assertEqual(len(ddict['params'][0]), 1)
            param1 = ddict['params'][0][0]
            self.testcase.assertEqual(param1[-1], ".")

        rc = {}
        if self.needauth:
            self.needauth = False  # reset
            return MockResponse(401, json.dumps(rc))
        if self.error:
            rc['error'] = {'code': self.error}
            self.error = None  # reset
        else:
            rc['error'] = None
        return MockResponse(200, json.dumps(rc))


class IPABackendTestCase(tests.TestCase, BackendTestMixin):

    def get_record_fixture(self, recordset_type, fixture=0, values={}):
        """override to ensure all records have a recordset_id"""
        return super(IPABackendTestCase, self).get_record_fixture(
            recordset_type, fixture,
            values={
                'recordset_id': utils.generate_uuid()
            }
        )

    def setUp(self):
        super(IPABackendTestCase, self).setUp()
        self.request = MockRequest(self)
        # make requests return our mock object

        def getSession():
            return self.request

        # replace requests.Session() with our mock version
        self.useFixture(fixtures.MonkeyPatch('requests.Session', getSession))

        self.config(backend_driver='ipa', group='service:agent')
        self.backend = self.get_backend_driver()
        self.CONF['backend:ipa'].ipa_auth_driver_class = \
            "designate.tests.test_backend.test_ipa.MockIPAAuth"
        self.backend.start()
        self.central_service = self.start_service('central')
        # Since some CRUD methods in impl_ipa call central's find_servers
        # and find_records method, mock it up to return our fixture.
        self.backend.central_service.find_servers = MagicMock(
            return_value=[self.get_server_fixture()])
        self.backend.central_service.find_records = MagicMock(
            return_value=[self.get_record_fixture('A')])

    def test_create_server(self):
        context = self.get_context()
        server = self.get_server_fixture()
        self.backend.create_server(context, server)

    def test_update_server(self):
        context = self.get_context()
        server = self.get_server_fixture()
        self.backend.create_server(context, server)
        self.backend.update_server(context, server)

    def test_delete_server(self):
        context = self.get_context()
        server = self.get_server_fixture()
        self.backend.create_server(context, server)
        self.backend.delete_server(context, server)

    def test_create_domain(self):
        context = self.get_context()
        server = self.get_server_fixture()
        domain = self.get_domain_fixture()
        self.backend.create_server(context, server)
        self.backend.create_domain(context, domain)

    def test_update_domain(self):
        context = self.get_context()
        server = self.get_server_fixture()
        domain = self.get_domain_fixture()
        self.backend.create_server(context, server)
        self.backend.create_domain(context, domain)
        domain['serial'] = 123456789
        self.backend.update_domain(context, domain)

    def test_delete_domain(self):
        context = self.get_context()
        server = self.get_server_fixture()
        domain = self.get_domain_fixture()
        self.backend.create_server(context, server)
        self.backend.create_domain(context, domain)
        self.backend.delete_domain(context, domain)

    def test_create_domain_dup_domain(self):
        context = self.get_context()
        server = self.get_server_fixture()
        domain = self.get_domain_fixture()
        self.backend.create_server(context, server)
        self.request.error = impl_ipa.IPA_DUPLICATE
        self.assertRaises(impl_ipa.IPADuplicateDomain,
                          self.backend.create_domain,
                          context, domain)
        self.assertIsNone(self.request.error)

    def test_update_domain_error_no_domain(self):
        context = self.get_context()
        server = self.get_server_fixture()
        domain = self.get_domain_fixture()
        self.backend.create_server(context, server)
        self.backend.create_domain(context, domain)
        self.request.error = impl_ipa.IPA_NOT_FOUND
        self.assertRaises(impl_ipa.IPADomainNotFound,
                          self.backend.update_domain,
                          context, domain)

    def test_create_record(self):
        context = self.get_context()
        server = self.get_server_fixture()
        self.backend.create_server(context, server)
        domain = self.get_domain_fixture()
        self.backend.create_domain(context, domain)
        recordset = self.get_recordset_fixture(domain['name'], "A")
        record = self.get_record_fixture("A")
        self.backend.create_record(context, domain, recordset, record)
        self.backend.delete_domain(context, domain)

    def test_create_record_error_no_changes(self):
        context = self.get_context()
        server = self.get_server_fixture()
        self.backend.create_server(context, server)
        domain = self.get_domain_fixture()
        self.backend.create_domain(context, domain)
        recordset = self.get_recordset_fixture(domain['name'], "A")
        record = self.get_record_fixture("A")
        self.request.error = impl_ipa.IPA_NO_CHANGES
        # backend should ignore this error
        self.backend.create_record(context, domain, recordset, record)
        self.assertIsNone(self.request.error)
        self.backend.delete_domain(context, domain)

    def test_create_domain_error_no_changes(self):
        context = self.get_context()
        server = self.get_server_fixture()
        self.backend.create_server(context, server)
        domain = self.get_domain_fixture()
        self.request.error = impl_ipa.IPA_NO_CHANGES
        # backend should ignore this error
        self.backend.create_domain(context, domain)
        self.assertIsNone(self.request.error)
        self.backend.delete_domain(context, domain)

    def test_create_record_error_dup_record(self):
        context = self.get_context()
        server = self.get_server_fixture()
        self.backend.create_server(context, server)
        domain = self.get_domain_fixture()
        self.backend.create_domain(context, domain)
        recordset = self.get_recordset_fixture(domain['name'], "A")
        record = self.get_record_fixture("A")
        self.request.error = impl_ipa.IPA_DUPLICATE  # causes request to raise
        self.assertRaises(impl_ipa.IPADuplicateRecord,
                          self.backend.create_record,
                          context, domain, recordset, record)
        self.assertIsNone(self.request.error)
        self.backend.delete_domain(context, domain)

    def test_update_record_error_no_record(self):
        context = self.get_context()
        server = self.get_server_fixture()
        self.backend.create_server(context, server)
        domain = self.get_domain_fixture()
        self.backend.create_domain(context, domain)
        recordset = self.get_recordset_fixture(domain['name'], "A")
        record = self.get_record_fixture("A")
        self.request.error = impl_ipa.IPA_NOT_FOUND  # causes request to raise
        self.assertRaises(impl_ipa.IPARecordNotFound,
                          self.backend.update_record,
                          context, domain, recordset, record)
        self.assertIsNone(self.request.error)
        self.backend.delete_domain(context, domain)

    def test_update_record_unknown_error(self):
        context = self.get_context()
        server = self.get_server_fixture()
        self.backend.create_server(context, server)
        domain = self.get_domain_fixture()
        self.backend.create_domain(context, domain)
        recordset = self.get_recordset_fixture(domain['name'], "A")
        record = self.get_record_fixture("A")
        self.request.error = 1234  # causes request to raise
        self.assertRaises(impl_ipa.IPAUnknownError, self.backend.update_record,
                          context, domain, recordset, record)
        self.assertIsNone(self.request.error)
        self.backend.delete_domain(context, domain)

    def test_create_record_reauth(self):
        context = self.get_context()
        server = self.get_server_fixture()
        self.backend.create_server(context, server)
        domain = self.get_domain_fixture()
        self.backend.create_domain(context, domain)
        recordset = self.get_recordset_fixture(domain['name'], "A")
        record = self.get_record_fixture("A")
        self.request.needauth = True  # causes request to reauth
        beforecount = self.request.myauth.count
        self.backend.create_record(context, domain, recordset, record)
        self.assertFalse(self.request.needauth)
        self.assertEqual(self.request.myauth.count, (beforecount + 1))
        self.backend.delete_domain(context, domain)

    def test_create_record_reauth_fail(self):
        context = self.get_context()
        server = self.get_server_fixture()
        self.backend.create_server(context, server)
        domain = self.get_domain_fixture()
        self.backend.create_domain(context, domain)
        recordset = self.get_recordset_fixture(domain['name'], "A")
        record = self.get_record_fixture("A")
        self.request.needauth = True  # causes request to reauth
        self.backend.ntries = 0  # force exception upon retry
        self.assertRaises(impl_ipa.IPACommunicationFailure,
                          self.backend.create_record, context, domain,
                          recordset, record)
        self.assertFalse(self.request.needauth)
        self.assertNotEqual(self.backend.ntries, 0)
        self.backend.delete_domain(context, domain)

########NEW FILE########
__FILENAME__ = test_multi
# Copyright (C) 2013 eNovance SAS <licensing@enovance.com>
#
# Author: Artom Lifshitz <artom.lifshitz@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import mock
from mock import call
from mock import MagicMock
from designate import exceptions
from designate import tests
from designate.tests.test_backend import BackendTestMixin


class MultiBackendTestCase(tests.TestCase, BackendTestMixin):
    """
    Test the master/slave ordering as defined in MultiBackend.

    Test that create for tsigkeys, servers and domains is done on the master
    first, then on the slave. At the same time, test that if the slave raises
    an exception, delete is called on the master to cleanup.

    Test that delete for tsigkeys, servers and domains is done on the slave
    first, then on the master. At the same time, test that if the master raises
    an exception, create is called on the slave to cleanup.

    Test that updates and all operations on records are done on the master
    only.
    """

    def setUp(self):
        super(MultiBackendTestCase, self).setUp()
        self.config(backend_driver='multi', group='service:agent')
        self.backend = self.get_backend_driver()

        self.backends = MagicMock()
        self.backend.master = MagicMock()
        self.backend.slave = MagicMock()
        self.backends.master = self.backend.master
        self.backends.slave = self.backend.slave

    def test_create_tsigkey(self):
        context = self.get_context()
        tsigkey = self.get_tsigkey_fixture()
        self.backend.slave.create_tsigkey = MagicMock(
            side_effect=exceptions.Backend)
        self.assertRaises(exceptions.Backend, self.backend.create_tsigkey,
                          context, tsigkey)
        self.assertEqual(self.backends.mock_calls,
                         [call.master.create_tsigkey(context, tsigkey),
                          call.slave.create_tsigkey(context, tsigkey),
                          call.master.delete_tsigkey(context, tsigkey)])

    def test_update_tsigkey(self):
        context = self.get_context()
        tsigkey = self.get_tsigkey_fixture()
        self.backend.update_tsigkey(context, tsigkey)
        self.assertEqual(self.backends.mock_calls,
                         [call.master.update_tsigkey(context, tsigkey)])

    def test_delete_tsigkey(self):
        context = self.get_context()
        tsigkey = self.get_tsigkey_fixture()
        self.backend.master.delete_tsigkey = MagicMock(
            side_effect=exceptions.Backend)
        self.assertRaises(exceptions.Backend, self.backend.delete_tsigkey,
                          context, tsigkey)
        self.assertEqual(self.backends.mock_calls,
                         [call.slave.delete_tsigkey(context, tsigkey),
                          call.master.delete_tsigkey(context, tsigkey),
                          call.slave.create_tsigkey(context, tsigkey)])

    def test_create_domain(self):
        context = self.get_context()
        domain = self.get_domain_fixture()
        self.backend.slave.create_domain = MagicMock(
            side_effect=exceptions.Backend)
        self.assertRaises(exceptions.Backend, self.backend.create_domain,
                          context, domain)
        self.assertEqual(self.backends.mock_calls,
                         [call.master.create_domain(context, domain),
                          call.slave.create_domain(context, domain),
                          call.master.delete_domain(context, domain)])

    def test_update_domain(self):
        context = self.get_context()
        domain = self.get_domain_fixture()
        self.backend.update_domain(context, domain)
        self.assertEqual(self.backends.mock_calls,
                         [call.master.update_domain(context, domain)])

    def test_delete_domain(self):
        context = self.get_context()
        domain = self.get_domain_fixture()

        # Since multi's delete fetches the domain from central to be able to
        # recreate it if something goes wrong, create the domain first
        self.backend.central_service.create_server(
            self.get_admin_context(), self.get_server_fixture())
        self.backend.central_service.create_domain(context, domain)
        self.backend.master.delete_domain = MagicMock(
            side_effect=exceptions.Backend)
        self.assertRaises(exceptions.Backend, self.backend.delete_domain,
                          context, domain)
        self.assertEqual(self.backends.mock_calls,
                         [call.slave.delete_domain(context, domain),
                          call.master.delete_domain(context, domain),
                          call.slave.create_domain(context, domain)])

    def test_create_server(self):
        context = self.get_context()
        server = self.get_server_fixture()
        self.backend.slave.create_server = MagicMock(
            side_effect=exceptions.Backend)
        self.assertRaises(exceptions.Backend, self.backend.create_server,
                          context, server)
        self.assertEqual(self.backends.mock_calls,
                         [call.master.create_server(context, server),
                          call.slave.create_server(context, server),
                          call.master.delete_server(context, server)])

    def test_update_server(self):
        context = self.get_context()
        server = self.get_server_fixture()
        self.backend.update_server(context, server)
        self.assertEqual(self.backends.mock_calls,
                         [call.master.update_server(context, server)])

    def test_delete_server(self):
        context = self.get_context()
        server = self.get_server_fixture()
        self.backend.master.delete_server = MagicMock(
            side_effect=exceptions.Backend)
        self.assertRaises(exceptions.Backend, self.backend.delete_server,
                          context, server)
        self.assertEqual(self.backends.mock_calls,
                         [call.slave.delete_server(context, server),
                          call.master.delete_server(context, server),
                          call.slave.create_server(context, server)])

    def test_create_recordset(self):
        context = self.get_context()

        domain = mock.sentinel.domain
        recordset = mock.sentinel.recordset

        self.backend.create_recordset(context, domain, recordset)

        self.assertEqual(
            self.backends.mock_calls,
            [call.master.create_recordset(context, domain, recordset)])

    def test_update_recordset(self):
        context = self.get_context()

        domain = mock.sentinel.domain
        recordset = mock.sentinel.recordset

        self.backend.update_recordset(context, domain, recordset)

        self.assertEqual(
            self.backends.mock_calls,
            [call.master.update_recordset(context, domain, recordset)])

    def test_delete_recordset(self):
        context = self.get_context()

        domain = mock.sentinel.domain
        recordset = mock.sentinel.recordset

        self.backend.delete_recordset(context, domain, recordset)

        self.assertEqual(
            self.backends.mock_calls,
            [call.master.delete_recordset(context, domain, recordset)])

    def test_create_record(self):
        context = self.get_context()

        domain = mock.sentinel.domain
        recordset = mock.sentinel.recordset
        record = mock.sentinel.record

        self.backend.create_record(context, domain, recordset, record)

        self.assertEqual(
            self.backends.mock_calls,
            [call.master.create_record(context, domain, recordset, record)])

    def test_update_record(self):
        context = self.get_context()

        domain = mock.sentinel.domain
        recordset = mock.sentinel.recordset
        record = mock.sentinel.record

        self.backend.update_record(context, domain, recordset, record)

        self.assertEqual(
            self.backends.mock_calls,
            [call.master.update_record(context, domain, recordset, record)])

    def test_delete_record(self):
        context = self.get_context()

        domain = mock.sentinel.domain
        recordset = mock.sentinel.recordset
        record = mock.sentinel.record

        self.backend.delete_record(context, domain, recordset, record)

        self.assertEqual(
            self.backends.mock_calls,
            [call.master.delete_record(context, domain, recordset, record)])

    def test_ping(self):
        context = self.get_context()
        self.backend.ping(context)
        self.assertEqual(self.backends.mock_calls,
                         [call.master.ping(context),
                          call.slave.ping(context)])

    def test_start(self):
        self.backend.start()
        self.assertEqual(self.backends.mock_calls,
                         [call.master.start(), call.slave.start()])

    def test_stop(self):
        self.backend.stop()
        self.assertEqual(self.backends.mock_calls,
                         [call.slave.stop(), call.master.stop()])

########NEW FILE########
__FILENAME__ = test_nsd4slave
# Copyright (C) 2013 eNovance SAS <licensing@enovance.com>
#
# Author: Artom Lifshitz <artom.lifshitz@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import eventlet
import fixtures
from mock import MagicMock
import os
import socket
import ssl
from oslo.config import cfg

from designate import exceptions
from designate import tests
from designate.tests.test_backend import BackendTestMixin
from designate.tests import resources
from designate.backend import impl_nsd4slave


class NSD4ServerStub:
    recved_command = None
    response = 'ok'
    keyfile = os.path.join(resources.path, 'ssl', 'nsd_server.key')
    certfile = os.path.join(resources.path, 'ssl', 'nsd_server.pem')

    def handle(self, client_sock, client_addr):
        stream = client_sock.makefile()
        self.recved_command = stream.readline()
        stream.write(self.response)
        stream.flush()

    def start(self):
        self.port = 1025
        while True:
            try:
                eventlet.spawn_n(eventlet.serve,
                                 eventlet.wrap_ssl(
                                     eventlet.listen(('127.0.0.1', self.port)),
                                     keyfile=self.keyfile,
                                     certfile=self.certfile,
                                     server_side=True),
                                 self.handle)
                break
            except socket.error:
                self.port = self.port + 1

    def stop(self):
        eventlet.StopServe()


class NSD4Fixture(fixtures.Fixture):
    def setUp(self):
        super(NSD4Fixture, self).setUp()
        self.servers = [NSD4ServerStub(), NSD4ServerStub()]
        [server.start() for server in self.servers]
        impl_nsd4slave.DEFAULT_PORT = self.servers[0].port
        cfg.CONF.set_override('backend_driver', 'nsd4slave', 'service:agent')
        cfg.CONF.set_override(
            'servers', ['127.0.0.1', '127.0.0.1:%d' % self.servers[1].port],
            'backend:nsd4slave')
        keyfile = os.path.join(resources.path, 'ssl', 'nsd_control.key')
        certfile = os.path.join(resources.path, 'ssl', 'nsd_control.pem')
        cfg.CONF.set_override('keyfile', keyfile, 'backend:nsd4slave')
        cfg.CONF.set_override('certfile', certfile, 'backend:nsd4slave')
        cfg.CONF.set_override('pattern', 'test-pattern', 'backend:nsd4slave')
        self.addCleanup(self.tearDown)

    def tearDown(self):
        [server.stop() for server in self.servers]


# NOTE: We'll only test the specifics to the nsd4 backend here.
# Rest is handled via scenarios
class NSD4SlaveBackendTestCase(tests.TestCase, BackendTestMixin):
    def setUp(self):
        super(NSD4SlaveBackendTestCase, self).setUp()

        self.server_fixture = NSD4Fixture()
        self.useFixture(self.server_fixture)

        self.backend = self.get_backend_driver()

    def test_create_domain(self):
        context = self.get_context()
        domain = self.get_domain_fixture()
        self.backend.create_domain(context, domain)
        command = 'NSDCT1 addzone %s test-pattern\n' % domain['name']
        [self.assertEqual(server.recved_command, command)
         for server in self.server_fixture.servers]

    def test_delete_domain(self):
        context = self.get_context()
        domain = self.get_domain_fixture()
        self.backend.delete_domain(context, domain)
        command = 'NSDCT1 delzone %s\n' % domain['name']
        [self.assertEqual(server.recved_command, command)
         for server in self.server_fixture.servers]

    def test_server_not_ok(self):
        self.server_fixture.servers[0].response = 'goat'
        context = self.get_context()
        domain = self.get_domain_fixture()
        self.assertRaises(exceptions.NSD4SlaveBackendError,
                          self.backend.create_domain,
                          context, domain)

    def test_ssl_error(self):
        self.backend._command = MagicMock(side_effet=ssl.SSLError)
        context = self.get_context()
        domain = self.get_domain_fixture()
        self.assertRaises(exceptions.NSD4SlaveBackendError,
                          self.backend.create_domain,
                          context, domain)

    def test_socket_error(self):
        self.backend._command = MagicMock(side_effet=socket.error)
        context = self.get_context()
        domain = self.get_domain_fixture()
        self.assertRaises(exceptions.NSD4SlaveBackendError,
                          self.backend.create_domain,
                          context, domain)

########NEW FILE########
__FILENAME__ = test_powerdns
# Copyright (C) 2013 eNovance SAS <licensing@enovance.com>
#
# Author: Artom Lifshitz <artom.lifshitz@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import os
from mock import MagicMock

from designate import tests
from designate.tests import DatabaseFixture
from designate.tests.test_backend import BackendTestMixin
from designate import utils

# impl_powerdns needs to register its options before being instanciated.
# Import it and pretend to use it to avoid flake8 unused import errors.
from designate.backend import impl_powerdns
impl_powerdns

REPOSITORY = os.path.abspath(os.path.join(os.path.dirname(__file__),
                                          '..', '..',
                                          'backend', 'impl_powerdns',
                                          'migrate_repo'))


class PowerDNSBackendTestCase(tests.TestCase, BackendTestMixin):

    def get_tsigkey_fixture(self):
        return super(PowerDNSBackendTestCase, self).get_tsigkey_fixture(
            values={
                'id': utils.generate_uuid()
            }
        )

    def get_server_fixture(self):
        return super(PowerDNSBackendTestCase, self).get_server_fixture(
            values={
                'id': utils.generate_uuid()
            }
        )

    def get_domain_fixture(self):
        return super(PowerDNSBackendTestCase, self).get_domain_fixture(
            values={
                'id': utils.generate_uuid(),
                'ttl': 42,
                'serial': 42,
                'refresh': 42,
                'retry': 42,
                'expire': 42,
                'minimum': 42,
            }
        )

    def setUp(self):
        super(PowerDNSBackendTestCase, self).setUp()
        self.db_fixture = DatabaseFixture.get_fixture(REPOSITORY)
        self.useFixture(self.db_fixture)
        self.config(backend_driver='powerdns', group='service:agent')
        self.config(database_connection=self.db_fixture.url,
                    group='backend:powerdns')
        self.backend = self.get_backend_driver()
        self.backend.start()
        self.central_service = self.start_service('central')
        # Since some CRUD methods in impl_powerdns call central's find_servers
        # method, mock it up to return our fixture.
        self.backend.central_service.find_servers = MagicMock(
            return_value=[self.get_server_fixture()])

    def test_create_tsigkey(self):
        context = self.get_context()
        tsigkey = self.get_tsigkey_fixture()
        self.backend.create_tsigkey(context, tsigkey)

    def test_update_tsigkey(self):
        context = self.get_context()
        tsigkey = self.get_tsigkey_fixture()
        self.backend.create_tsigkey(context, tsigkey)
        self.backend.update_tsigkey(context, tsigkey)

    def test_delete_tsigkey(self):
        context = self.get_context()
        tsigkey = self.get_tsigkey_fixture()
        self.backend.create_tsigkey(context, tsigkey)
        self.backend.delete_tsigkey(context, tsigkey)

    def test_create_server(self):
        context = self.get_context()
        server = self.get_server_fixture()
        self.backend.create_server(context, server)

    def test_update_server(self):
        context = self.get_context()
        server = self.get_server_fixture()
        self.backend.create_server(context, server)
        self.backend.update_server(context, server)

    def test_delete_server(self):
        context = self.get_context()
        server = self.get_server_fixture()
        self.backend.create_server(context, server)
        self.backend.delete_server(context, server)

    def test_create_domain(self):
        context = self.get_context()
        server = self.get_server_fixture()
        domain = self.get_domain_fixture()
        self.backend.create_server(context, server)
        self.backend.create_domain(context, domain)

    def test_update_domain(self):
        context = self.get_context()
        server = self.get_server_fixture()
        domain = self.get_domain_fixture()
        self.backend.create_server(context, server)
        self.backend.create_domain(context, domain)
        self.backend.update_domain(context, domain)

    def test_delete_domain(self):
        context = self.get_context()
        server = self.get_server_fixture()
        domain = self.get_domain_fixture()
        self.backend.create_server(context, server)
        self.backend.create_domain(context, domain)
        self.backend.delete_domain(context, domain)

########NEW FILE########
__FILENAME__ = test_service
# -*- coding: utf-8 -*-
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import random
import testtools
from designate.openstack.common import log as logging
from designate import exceptions
from designate.tests.test_central import CentralTestCase

LOG = logging.getLogger(__name__)


class CentralServiceTest(CentralTestCase):
    def setUp(self):
        super(CentralServiceTest, self).setUp()
        self.central_service = self.start_service('central')

    def test_stop(self):
        # Test stopping the service
        self.central_service.stop()

    def test_is_valid_domain_name(self):
        self.config(max_domain_name_len=10,
                    group='service:central')

        context = self.get_context()

        self.central_service._is_valid_domain_name(context, 'valid.org.')

        with testtools.ExpectedException(exceptions.InvalidDomainName):
            self.central_service._is_valid_domain_name(context, 'example.org.')

        with testtools.ExpectedException(exceptions.InvalidDomainName):
            self.central_service._is_valid_domain_name(context, 'example.tld.')

    def test_is_valid_recordset_name(self):
        self.config(max_recordset_name_len=18,
                    group='service:central')

        context = self.get_context()

        domain = self.create_domain(name='example.org.')

        self.central_service._is_valid_recordset_name(
            context, domain, 'valid.example.org.')

        with testtools.ExpectedException(exceptions.InvalidRecordSetName):
            self.central_service._is_valid_recordset_name(
                context, domain, 'toolong.example.org.')

        with testtools.ExpectedException(exceptions.InvalidRecordSetLocation):
            self.central_service._is_valid_recordset_name(
                context, domain, 'a.example.COM.')

    def test_is_blacklisted_domain_name(self):
        # Create blacklisted zones with specific names
        self.create_blacklist(pattern='example.org.')
        self.create_blacklist(pattern='example.net.')
        self.create_blacklist(pattern='^blacklisted.org.$')
        self.create_blacklist(pattern='com.$')

        # Set the policy to reject the authz
        self.policy({'use_blacklisted_domain': '!'})

        context = self.get_context()

        result = self.central_service._is_blacklisted_domain_name(
            context, 'org.')
        self.assertFalse(result)

        # Subdomains should not be allowed from a blacklisted domain
        result = self.central_service._is_blacklisted_domain_name(
            context, 'www.example.org.')
        self.assertTrue(result)

        result = self.central_service._is_blacklisted_domain_name(
            context, 'example.org.')
        self.assertTrue(result)

        # Check for blacklisted domains containing regexps
        result = self.central_service._is_blacklisted_domain_name(
            context, 'example.net.')
        self.assertTrue(result)

        result = self.central_service._is_blacklisted_domain_name(
            context, 'example.com.')
        self.assertTrue(result)

        result = self.central_service._is_blacklisted_domain_name(
            context, 'blacklisted.org.')
        self.assertTrue(result)

    def test_is_subdomain(self):
        context = self.get_context()

        # Create a domain (using the specified domain name)
        self.create_domain(name='example.org.')

        result = self.central_service._is_subdomain(context, 'org.')
        self.assertFalse(result)

        result = self.central_service._is_subdomain(context,
                                                    'www.example.net.')
        self.assertFalse(result)

        result = self.central_service._is_subdomain(context, 'example.org.')
        self.assertFalse(result)

        result = self.central_service._is_subdomain(context,
                                                    'www.example.org.')
        self.assertTrue(result)

    def test_is_valid_recordset_placement_subdomain(self):
        context = self.get_context()

        # Create a domain (using the specified domain name)
        domain = self.create_domain(name='example.org.')
        sub_domain = self.create_domain(name='sub.example.org.')

        def _fail(domain_, name):
            with testtools.ExpectedException(
                    exceptions.InvalidRecordSetLocation):
                self.central_service._is_valid_recordset_placement_subdomain(
                    context, domain_, name)

        def _ok(domain_, name):
            self.central_service._is_valid_recordset_placement_subdomain(
                context, domain_, name)

        _fail(domain, 'record.sub.example.org.')
        _fail(domain, 'sub.example.org.')
        _ok(domain, 'example.org.')
        _ok(domain, 'record.example.org.')

        _ok(sub_domain, 'record.example.org.')

    def test_is_valid_ttl(self):
        self.policy({'use_low_ttl': '!'})
        self.config(min_ttl="100",
                    group='service:central')
        context = self.get_context()

        values = self.get_domain_fixture(1)
        values['ttl'] = 0

        with testtools.ExpectedException(exceptions.InvalidTTL):
                    self.central_service._is_valid_ttl(
                        context, values['ttl'])

    # Server Tests
    def test_create_server(self):
        values = dict(
            name='ns1.example.org.'
        )

        # Create a server
        server = self.central_service.create_server(
            self.admin_context, values=values)

        # Ensure all values have been set correctly
        self.assertIsNotNone(server['id'])
        self.assertEqual(server['name'], values['name'])

    def test_find_servers(self):
        # Ensure we have no servers to start with.
        servers = self.central_service.find_servers(self.admin_context)
        self.assertEqual(len(servers), 0)

        # Create a single server (using default values)
        self.create_server()

        # Ensure we can retrieve the newly created server
        servers = self.central_service.find_servers(self.admin_context)
        self.assertEqual(len(servers), 1)
        self.assertEqual(servers[0]['name'], 'ns1.example.org.')

        # Create a second server
        self.create_server(name='ns2.example.org.')

        # Ensure we can retrieve both servers
        servers = self.central_service.find_servers(self.admin_context)
        self.assertEqual(len(servers), 2)
        self.assertEqual(servers[0]['name'], 'ns1.example.org.')
        self.assertEqual(servers[1]['name'], 'ns2.example.org.')

    def test_get_server(self):
        # Create a server
        server_name = 'ns%d.example.org.' % random.randint(10, 1000)
        expected_server = self.create_server(name=server_name)

        # Retrieve it, and ensure it's the same
        server = self.central_service.get_server(
            self.admin_context, expected_server['id'])

        self.assertEqual(server['id'], expected_server['id'])
        self.assertEqual(server['name'], expected_server['name'])

    def test_update_server(self):
        # Create a server
        expected_server = self.create_server()

        # Update the server
        values = dict(name='prefix.%s' % expected_server['name'])
        self.central_service.update_server(
            self.admin_context, expected_server['id'], values=values)

        # Fetch the server again
        server = self.central_service.get_server(
            self.admin_context, expected_server['id'])

        # Ensure the server was updated correctly
        self.assertEqual(server['name'], 'prefix.%s' % expected_server['name'])

    def test_delete_server(self):
        # Create a server
        server = self.create_server()

        # Create a second server
        server2 = self.create_server(fixture=1)

        # Delete one server
        self.central_service.delete_server(self.admin_context, server['id'])

        # Fetch the server again, ensuring an exception is raised
        self.assertRaises(
            exceptions.ServerNotFound,
            self.central_service.get_server,
            self.admin_context, server['id'])

        # Try to delete last remaining server - expect exception
        self.assertRaises(
            exceptions.LastServerDeleteNotAllowed,
            self.central_service.delete_server, self.admin_context,
            server2['id'])

    # TLD Tests
    def test_create_tld(self):
        # Create a TLD with one label
        tld = self.create_tld(fixture=0)

        # Ensure all values have been set correctly
        self.assertIsNotNone(tld['id'])
        self.assertEqual(tld['name'], self.get_tld_fixture(fixture=0)['name'])

        # Create a TLD with more than one label
        tld = self.create_tld(fixture=1)

        # Ensure all values have been set correctly
        self.assertIsNotNone(tld['id'])
        self.assertEqual(tld['name'], self.get_tld_fixture(fixture=1)['name'])

    def test_find_tlds(self):
        # Ensure we have no tlds to start with.
        tlds = self.central_service.find_tlds(self.admin_context)
        self.assertEqual(len(tlds), 0)

        # Create a single tld
        self.create_tld(fixture=0)
        # Ensure we can retrieve the newly created tld
        tlds = self.central_service.find_tlds(self.admin_context)
        self.assertEqual(len(tlds), 1)
        self.assertEqual(tlds[0]['name'],
                         self.get_tld_fixture(fixture=0)['name'])

        # Create a second tld
        self.create_tld(fixture=1)

        # Ensure we can retrieve both tlds
        tlds = self.central_service.find_tlds(self.admin_context)
        self.assertEqual(len(tlds), 2)
        self.assertEqual(tlds[0]['name'],
                         self.get_tld_fixture(fixture=0)['name'])
        self.assertEqual(tlds[1]['name'],
                         self.get_tld_fixture(fixture=1)['name'])

    def test_get_tld(self):
        # Create a tld
        tld_name = 'ns%d.co.uk' % random.randint(10, 1000)
        expected_tld = self.create_tld(name=tld_name)

        # Retrieve it, and ensure it's the same
        tld = self.central_service.get_tld(
            self.admin_context, expected_tld['id'])

        self.assertEqual(tld['id'], expected_tld['id'])
        self.assertEqual(tld['name'], expected_tld['name'])

    def test_update_tld(self):
        # Create a tld
        expected_tld = self.create_tld(fixture=0)

        # Update the tld
        values = dict(name='prefix.%s' % expected_tld['name'])
        self.central_service.update_tld(
            self.admin_context, expected_tld['id'], values=values)

        # Fetch the tld again
        tld = self.central_service.get_tld(
            self.admin_context, expected_tld['id'])

        # Ensure the tld was updated correctly
        self.assertEqual(tld['name'], 'prefix.%s' % expected_tld['name'])

    def test_delete_tld(self):
        # Create a tld
        tld = self.create_tld(fixture=0)
        # Delete the tld
        self.central_service.delete_tld(self.admin_context, tld['id'])

        # Fetch the tld again, ensuring an exception is raised
        self.assertRaises(
            exceptions.TLDNotFound,
            self.central_service.get_tld,
            self.admin_context, tld['id'])

    # TsigKey Tests
    def test_create_tsigkey(self):
        values = self.get_tsigkey_fixture(fixture=0)

        # Create a tsigkey
        tsigkey = self.central_service.create_tsigkey(
            self.admin_context, values=values)

        # Ensure all values have been set correctly
        self.assertIsNotNone(tsigkey['id'])
        self.assertEqual(tsigkey['name'], values['name'])
        self.assertEqual(tsigkey['algorithm'], values['algorithm'])
        self.assertEqual(tsigkey['secret'], values['secret'])

    def test_find_tsigkeys(self):
        # Ensure we have no tsigkeys to start with.
        tsigkeys = self.central_service.find_tsigkeys(self.admin_context)
        self.assertEqual(len(tsigkeys), 0)

        # Create a single tsigkey (using default values)
        tsigkey_one = self.create_tsigkey()

        # Ensure we can retrieve the newly created tsigkey
        tsigkeys = self.central_service.find_tsigkeys(self.admin_context)
        self.assertEqual(len(tsigkeys), 1)
        self.assertEqual(tsigkeys[0]['name'], tsigkey_one['name'])

        # Create a second tsigkey
        tsigkey_two = self.create_tsigkey(fixture=1)

        # Ensure we can retrieve both tsigkeys
        tsigkeys = self.central_service.find_tsigkeys(self.admin_context)
        self.assertEqual(len(tsigkeys), 2)
        self.assertEqual(tsigkeys[0]['name'], tsigkey_one['name'])
        self.assertEqual(tsigkeys[1]['name'], tsigkey_two['name'])

    def test_get_tsigkey(self):
        # Create a tsigkey
        expected = self.create_tsigkey()

        # Retrieve it, and ensure it's the same
        tsigkey = self.central_service.get_tsigkey(
            self.admin_context, expected['id'])

        self.assertEqual(tsigkey['id'], expected['id'])
        self.assertEqual(tsigkey['name'], expected['name'])
        self.assertEqual(tsigkey['algorithm'], expected['algorithm'])
        self.assertEqual(tsigkey['secret'], expected['secret'])

    def test_update_tsigkey(self):
        # Create a tsigkey using default values
        expected = self.create_tsigkey()

        # Update the tsigkey
        fixture = self.get_tsigkey_fixture(fixture=1)
        values = dict(name=fixture['name'])

        self.central_service.update_tsigkey(
            self.admin_context, expected['id'], values=values)

        # Fetch the tsigkey again
        tsigkey = self.central_service.get_tsigkey(
            self.admin_context, expected['id'])

        # Ensure the tsigkey was updated correctly
        self.assertEqual(tsigkey['name'], fixture['name'])

    def test_delete_tsigkey(self):
        # Create a tsigkey
        tsigkey = self.create_tsigkey()

        # Delete the tsigkey
        self.central_service.delete_tsigkey(self.admin_context, tsigkey['id'])

        # Fetch the tsigkey again, ensuring an exception is raised
        with testtools.ExpectedException(exceptions.TsigKeyNotFound):
            self.central_service.get_tsigkey(self.admin_context, tsigkey['id'])

    # Tenant Tests
    def test_count_tenants(self):
        admin_context = self.get_admin_context()
        admin_context.all_tenants = True

        tenant_one_context = self.get_context(tenant=1)
        tenant_two_context = self.get_context(tenant=2)

        # in the beginning, there should be nothing
        tenants = self.central_service.count_tenants(admin_context)
        self.assertEqual(tenants, 0)

        # Explicitly set a tenant_id
        self.create_domain(fixture=0, context=tenant_one_context)
        self.create_domain(fixture=1, context=tenant_two_context)

        tenants = self.central_service.count_tenants(admin_context)
        self.assertEqual(tenants, 2)

    def test_count_tenants_policy_check(self):
        # Set the policy to reject the authz
        self.policy({'count_tenants': '!'})

        with testtools.ExpectedException(exceptions.Forbidden):
            self.central_service.count_tenants(self.get_context())

    # Domain Tests
    def _test_create_domain(self, values):
        # Create a server
        self.create_server()

        # Reset the list of notifications
        self.reset_notifications()

        # Create a domain
        domain = self.central_service.create_domain(
            self.admin_context, values=values)

        # Ensure all values have been set correctly
        self.assertIsNotNone(domain['id'])
        self.assertEqual(domain['name'], values['name'])
        self.assertEqual(domain['email'], values['email'])
        self.assertIn('status', domain)

        # Ensure we sent exactly 1 notification
        notifications = self.get_notifications()
        self.assertEqual(len(notifications), 1)

        # Ensure the notification wrapper contains the correct info
        ctxt, message, priority = notifications.pop()
        self.assertEqual(message['event_type'], 'dns.domain.create')
        self.assertEqual(message['priority'], 'INFO')
        self.assertIsNotNone(message['timestamp'])
        self.assertIsNotNone(message['message_id'])

        # Ensure the notification payload contains the correct info
        payload = message['payload']
        self.assertEqual(payload['id'], domain['id'])
        self.assertEqual(payload['name'], domain['name'])
        self.assertEqual(payload['tenant_id'], domain['tenant_id'])

    def test_create_domain_over_tld(self):
        values = dict(
            name='example.com',
            email='info@example.com'
        )
        self._test_create_domain(values)

    def test_idn_create_domain_over_tld(self):
        values = dict(
            name='xn--3e0b707e'
        )

        # Create the appropriate TLD
        self.central_service.create_tld(self.admin_context, values=values)

        # Test creation of a domain in 한국 (kr)
        values = dict(
            name='example.xn--3e0b707e.',
            email='info@example.xn--3e0b707e'
        )
        self._test_create_domain(values)

    def test_create_domain_over_quota(self):
        self.config(quota_domains=1)

        self.create_domain()

        with testtools.ExpectedException(exceptions.OverQuota):
            self.create_domain()

    def test_create_subdomain(self):
        # Create the Parent Domain using fixture 0
        parent_domain = self.create_domain(fixture=0)

        # Prepare values for the subdomain using fixture 1 as a base
        values = self.get_domain_fixture(1)
        values['name'] = 'www.%s' % parent_domain['name']

        # Create the subdomain
        domain = self.central_service.create_domain(
            self.admin_context, values=values)

        # Ensure all values have been set correctly
        self.assertIsNotNone(domain['id'])
        self.assertEqual(domain['parent_domain_id'], parent_domain['id'])

    def test_create_subdomain_failure(self):
        context = self.get_admin_context()

        # Explicitly set a tenant_id
        context.tenant = '1'

        # Create the Parent Domain using fixture 0
        parent_domain = self.create_domain(fixture=0, context=context)

        context = self.get_admin_context()

        # Explicitly use a different tenant_id
        context.tenant = '2'

        # Prepare values for the subdomain using fixture 1 as a base
        values = self.get_domain_fixture(1)
        values['name'] = 'www.%s' % parent_domain['name']

        # Attempt to create the subdomain
        with testtools.ExpectedException(exceptions.Forbidden):
            self.central_service.create_domain(context, values=values)

    def test_create_blacklisted_domain_success(self):
        # Create blacklisted zone using default values
        self.create_blacklist()

        # Set the policy to accept the authz
        self.policy({'use_blacklisted_domain': '@'})

        values = dict(
            name='blacklisted.com.',
            email='info@blacklisted.com'
        )

        # Create a server
        self.create_server()

        # Create a zone that is blacklisted
        domain = self.central_service.create_domain(
            self.admin_context, values=values)

        # Ensure all values have been set correctly
        self.assertIsNotNone(domain['id'])
        self.assertEqual(domain['name'], values['name'])
        self.assertEqual(domain['email'], values['email'])

    def test_create_blacklisted_domain_fail(self):
        self.create_blacklist()

        # Set the policy to reject the authz
        self.policy({'use_blacklisted_domain': '!'})

        values = dict(
            name='blacklisted.com.',
            email='info@blacklisted.com'
        )

        with testtools.ExpectedException(exceptions.InvalidDomainName):
            # Create a domain
            self.central_service.create_domain(
                self.admin_context, values=values)

    def _test_create_domain_fail(self, values, exception):

        with testtools.ExpectedException(exception):
            # Create an invalid domain
            self.central_service.create_domain(
                self.admin_context, values=values)

    def test_create_domain_invalid_tld_fail(self):
        # Create a server
        self.create_server()

        # add a tld for com
        self.create_tld(fixture=0)

        values = dict(
            name='example.com.',
            email='info@example.com'
        )

        # Create a valid domain
        self.central_service.create_domain(self.admin_context, values=values)

        values = dict(
            name='example.net.',
            email='info@example.net'
        )

        # There is no TLD for net so it should fail
        with testtools.ExpectedException(exceptions.InvalidDomainName):
            # Create an invalid domain
            self.central_service.create_domain(
                self.admin_context, values=values)

    def test_create_domain_invalid_ttl_fail(self):
        self.policy({'use_low_ttl': '!'})
        self.config(min_ttl="100",
                    group='service:central')
        context = self.get_context()

        values = self.get_domain_fixture(1)
        values['ttl'] = 0

        # Create a server
        self.create_server()

        with testtools.ExpectedException(exceptions.InvalidTTL):
                    self.central_service.create_domain(context, values=values)

    def test_create_domain_no_min_ttl(self):
        self.policy({'use_low_ttl': '!'})
        self.config(min_ttl="None",
                    group='service:central')
        values = self.get_domain_fixture(1)
        values['ttl'] = -100

        # Create a server
        self.create_server()

        #Create domain with random TTL
        domain = self.central_service.create_domain(
            self.admin_context, values=values)

        # Ensure all values have been set correctly
        self.assertEqual(domain['ttl'], values['ttl'])

    def test_find_domains(self):
        # Ensure we have no domains to start with.
        domains = self.central_service.find_domains(self.admin_context)
        self.assertEqual(len(domains), 0)

        # Create a single domain (using default values)
        self.create_domain()

        # Ensure we can retrieve the newly created domain
        domains = self.central_service.find_domains(self.admin_context)
        self.assertEqual(len(domains), 1)
        self.assertEqual(domains[0]['name'], 'example.com.')

        # Create a second domain
        self.create_domain(name='example.net.')

        # Ensure we can retrieve both domain
        domains = self.central_service.find_domains(self.admin_context)
        self.assertEqual(len(domains), 2)
        self.assertEqual(domains[0]['name'], 'example.com.')
        self.assertEqual(domains[1]['name'], 'example.net.')

    def test_find_domains_criteria(self):
        # Create a domain
        domain_name = '%d.example.com.' % random.randint(10, 1000)
        expected_domain = self.create_domain(name=domain_name)

        # Retrieve it, and ensure it's the same
        criterion = {'name': domain_name}

        domains = self.central_service.find_domains(
            self.admin_context, criterion)

        self.assertEqual(domains[0]['id'], expected_domain['id'])
        self.assertEqual(domains[0]['name'], expected_domain['name'])
        self.assertEqual(domains[0]['email'], expected_domain['email'])

    def test_find_domains_tenant_restrictions(self):
        admin_context = self.get_admin_context()
        admin_context.all_tenants = True

        tenant_one_context = self.get_context(tenant=1)
        tenant_two_context = self.get_context(tenant=2)

        # Ensure we have no domains to start with.
        domains = self.central_service.find_domains(admin_context)
        self.assertEqual(len(domains), 0)

        # Create a single domain (using default values)
        domain = self.create_domain(context=tenant_one_context)

        # Ensure admins can retrieve the newly created domain
        domains = self.central_service.find_domains(admin_context)
        self.assertEqual(len(domains), 1)
        self.assertEqual(domains[0]['name'], domain['name'])

        # Ensure tenant=1 can retrieve the newly created domain
        domains = self.central_service.find_domains(tenant_one_context)
        self.assertEqual(len(domains), 1)
        self.assertEqual(domains[0]['name'], domain['name'])

        # Ensure tenant=2 can NOT retrieve the newly created domain
        domains = self.central_service.find_domains(tenant_two_context)
        self.assertEqual(len(domains), 0)

    def test_get_domain(self):
        # Create a domain
        domain_name = '%d.example.com.' % random.randint(10, 1000)
        expected_domain = self.create_domain(name=domain_name)

        # Retrieve it, and ensure it's the same
        domain = self.central_service.get_domain(
            self.admin_context, expected_domain['id'])

        self.assertEqual(domain['id'], expected_domain['id'])
        self.assertEqual(domain['name'], expected_domain['name'])
        self.assertEqual(domain['email'], expected_domain['email'])

    def test_get_domain_servers(self):
        # Create a domain
        domain = self.create_domain()

        # Retrieve the servers list
        servers = self.central_service.get_domain_servers(
            self.admin_context, domain['id'])

        self.assertTrue(len(servers) > 0)

    def test_find_domain(self):
        # Create a domain
        domain_name = '%d.example.com.' % random.randint(10, 1000)
        expected_domain = self.create_domain(name=domain_name)

        # Retrieve it, and ensure it's the same
        criterion = {'name': domain_name}

        domain = self.central_service.find_domain(
            self.admin_context, criterion)

        self.assertEqual(domain['id'], expected_domain['id'])
        self.assertEqual(domain['name'], expected_domain['name'])
        self.assertEqual(domain['email'], expected_domain['email'])
        self.assertIn('status', domain)

    def test_update_domain(self):
        # Create a domain
        expected_domain = self.create_domain()

        # Reset the list of notifications
        self.reset_notifications()

        # Update the domain
        values = dict(email='new@example.com')

        self.central_service.update_domain(
            self.admin_context, expected_domain['id'], values=values)

        # Fetch the domain again
        domain = self.central_service.get_domain(
            self.admin_context, expected_domain['id'])

        # Ensure the domain was updated correctly
        self.assertTrue(domain['serial'] > expected_domain['serial'])
        self.assertEqual(domain['email'], 'new@example.com')

        # Ensure we sent exactly 1 notification
        notifications = self.get_notifications()
        self.assertEqual(len(notifications), 1)

        # Ensure the notification wrapper contains the correct info
        ctxt, message, priority = notifications.pop()
        self.assertEqual(message['event_type'], 'dns.domain.update')
        self.assertEqual(message['priority'], 'INFO')
        self.assertIsNotNone(message['timestamp'])
        self.assertIsNotNone(message['message_id'])

        # Ensure the notification payload contains the correct info
        payload = message['payload']
        self.assertEqual(payload['id'], domain['id'])
        self.assertEqual(payload['name'], domain['name'])
        self.assertEqual(payload['tenant_id'], domain['tenant_id'])

    def test_update_domain_without_incrementing_serial(self):
        # Create a domain
        expected_domain = self.create_domain()

        # Update the domain
        values = dict(email='new@example.com')

        self.central_service.update_domain(
            self.admin_context, expected_domain['id'], values=values,
            increment_serial=False)

        # Fetch the domain again
        domain = self.central_service.get_domain(
            self.admin_context, expected_domain['id'])

        # Ensure the domain was updated correctly
        self.assertEqual(domain['serial'], expected_domain['serial'])
        self.assertEqual(domain['email'], 'new@example.com')

    def test_update_domain_name_fail(self):
        # Create a domain
        expected_domain = self.create_domain()

        # Update the domain
        with testtools.ExpectedException(exceptions.BadRequest):
            values = dict(name='renamed-domain.com.')

            self.central_service.update_domain(
                self.admin_context, expected_domain['id'], values=values)

    def test_delete_domain(self):
        # Create a domain
        domain = self.create_domain()

        # Reset the list of notifications
        self.reset_notifications()

        # Delete the domain
        self.central_service.delete_domain(self.admin_context, domain['id'])

        # Fetch the domain again, ensuring an exception is raised
        with testtools.ExpectedException(exceptions.DomainNotFound):
            self.central_service.get_domain(self.admin_context, domain['id'])

        # Ensure we sent exactly 1 notification
        notifications = self.get_notifications()
        self.assertEqual(len(notifications), 1)

        # Ensure the notification wrapper contains the correct info
        ctxt, message, priority = notifications.pop()
        self.assertEqual(message['event_type'], 'dns.domain.delete')
        self.assertEqual(message['priority'], 'INFO')
        self.assertIsNotNone(message['timestamp'])
        self.assertIsNotNone(message['message_id'])

        # Ensure the notification payload contains the correct info
        payload = message['payload']
        self.assertEqual(payload['id'], domain['id'])
        self.assertEqual(payload['name'], domain['name'])
        self.assertEqual(payload['tenant_id'], domain['tenant_id'])

    def test_delete_parent_domain(self):
        # Create the Parent Domain using fixture 0
        parent_domain = self.create_domain(fixture=0)

        # Create the subdomain
        self.create_domain(fixture=1, name='www.%s' % parent_domain['name'])

        # Attempt to delete the parent domain
        with testtools.ExpectedException(exceptions.DomainHasSubdomain):
            self.central_service.delete_domain(
                self.admin_context, parent_domain['id'])

    def test_count_domains(self):
        # in the beginning, there should be nothing
        domains = self.central_service.count_domains(self.admin_context)
        self.assertEqual(domains, 0)

        # Create a single domain
        self.create_domain()

        # count 'em up
        domains = self.central_service.count_domains(self.admin_context)

        # well, did we get 1?
        self.assertEqual(domains, 1)

    def test_count_domains_policy_check(self):
        # Set the policy to reject the authz
        self.policy({'count_domains': '!'})

        with testtools.ExpectedException(exceptions.Forbidden):
            self.central_service.count_domains(self.get_context())

    def test_touch_domain(self):
        # Create a domain
        expected_domain = self.create_domain()

        # Touch the domain
        self.central_service.touch_domain(
            self.admin_context, expected_domain['id'])

        # Fetch the domain again
        domain = self.central_service.get_domain(
            self.admin_context, expected_domain['id'])

        # Ensure the serial was incremented
        self.assertTrue(domain['serial'] > expected_domain['serial'])

    # RecordSet Tests
    def test_create_recordset(self):
        domain = self.create_domain()

        values = dict(
            name='www.%s' % domain['name'],
            type='A'
        )

        # Create a recordset
        recordset = self.central_service.create_recordset(
            self.admin_context, domain['id'], values=values)

        # Ensure all values have been set correctly
        self.assertIsNotNone(recordset['id'])
        self.assertEqual(recordset['name'], values['name'])
        self.assertEqual(recordset['type'], values['type'])

    # def test_create_recordset_over_quota(self):
    #     self.config(quota_domain_recordsets=1)

    #     domain = self.create_domain()

    #     self.create_recordset(domain)

    #     with testtools.ExpectedException(exceptions.OverQuota):
    #         self.create_recordset(domain)

    def test_create_invalid_recordset_location_cname_at_apex(self):
        domain = self.create_domain()

        values = dict(
            name=domain['name'],
            type='CNAME'
        )

        # Attempt to create a CNAME record at the apex
        with testtools.ExpectedException(exceptions.InvalidRecordSetLocation):
            self.central_service.create_recordset(
                self.admin_context, domain['id'], values=values)

    def test_create_invalid_recordset_location_cname_sharing(self):
        domain = self.create_domain()
        expected = self.create_recordset(domain)

        values = dict(
            name=expected['name'],
            type='CNAME'
        )

        # Attempt to create a CNAME record alongside another record
        with testtools.ExpectedException(exceptions.InvalidRecordSetLocation):
            self.central_service.create_recordset(
                self.admin_context, domain['id'], values=values)

    def test_create_invalid_recordset_location_wrong_domain(self):
        domain = self.create_domain()
        other_domain = self.create_domain(fixture=1)

        values = dict(
            name=other_domain['name'],
            type='A'
        )

        # Attempt to create a record in the incorrect domain
        with testtools.ExpectedException(exceptions.InvalidRecordSetLocation):
            self.central_service.create_recordset(
                self.admin_context, domain['id'], values=values)

    def test_create_invalid_recordset_ttl(self):
        self.policy({'use_low_ttl': '!'})
        self.config(min_ttl="100",
                    group='service:central')
        domain = self.create_domain()

        values = dict(
            name='www.%s' % domain['name'],
            type='A',
            ttl=10
        )

        # Attempt to create a A record under the TTL
        with testtools.ExpectedException(exceptions.InvalidTTL):
            self.central_service.create_recordset(
                self.admin_context, domain['id'], values=values)

    def test_create_recordset_no_min_ttl(self):
        self.policy({'use_low_ttl': '!'})
        self.config(min_ttl="None",
                    group='service:central')
        domain = self.create_domain()

        values = dict(
            name='www.%s' % domain['name'],
            type='A',
            ttl=10
        )

        recordset = self.central_service.create_recordset(
            self.admin_context, domain['id'], values=values)
        self.assertEqual(recordset['ttl'], values['ttl'])

    def test_get_recordset(self):
        domain = self.create_domain()

        # Create a recordset
        expected = self.create_recordset(domain)

        # Retrieve it, and ensure it's the same
        recordset = self.central_service.get_recordset(
            self.admin_context, domain['id'], expected['id'])

        self.assertEqual(recordset['id'], expected['id'])
        self.assertEqual(recordset['name'], expected['name'])
        self.assertEqual(recordset['type'], expected['type'])

    def test_get_recordset_incorrect_domain_id(self):
        domain = self.create_domain()
        other_domain = self.create_domain(fixture=1)

        # Create a recordset
        expected = self.create_recordset(domain)

        # Ensure we get a 404 if we use the incorrect domain_id
        with testtools.ExpectedException(exceptions.RecordSetNotFound):
            self.central_service.get_recordset(
                self.admin_context, other_domain['id'], expected['id'])

    def test_find_recordsets(self):
        domain = self.create_domain()

        criterion = {'domain_id': domain['id']}

        # Ensure we have no recordsets to start with.
        recordsets = self.central_service.find_recordsets(
            self.admin_context, criterion)

        self.assertEqual(len(recordsets), 0)

        # Create a single recordset (using default values)
        self.create_recordset(domain, name='www.%s' % domain['name'])

        # Ensure we can retrieve the newly created recordset
        recordsets = self.central_service.find_recordsets(
            self.admin_context, criterion)

        self.assertEqual(len(recordsets), 1)
        self.assertEqual(recordsets[0]['name'], 'www.%s' % domain['name'])

        # Create a second recordset
        self.create_recordset(domain, name='mail.%s' % domain['name'])

        # Ensure we can retrieve both recordsets
        recordsets = self.central_service.find_recordsets(
            self.admin_context, criterion)

        self.assertEqual(len(recordsets), 2)
        self.assertEqual(recordsets[0]['name'], 'www.%s' % domain['name'])
        self.assertEqual(recordsets[1]['name'], 'mail.%s' % domain['name'])

    def test_find_recordset(self):
        domain = self.create_domain()

        # Create a recordset
        expected = self.create_recordset(domain)

        # Retrieve it, and ensure it's the same
        criterion = {'domain_id': domain['id'], 'name': expected['name']}

        recordset = self.central_service.find_recordset(
            self.admin_context, criterion)

        self.assertEqual(recordset['id'], expected['id'])
        self.assertEqual(recordset['name'], expected['name'])

    def test_update_recordset(self):
        domain = self.create_domain()

        # Create a recordset
        expected = self.create_recordset(domain)

        # Update the recordset
        values = dict(ttl=1800)
        self.central_service.update_recordset(
            self.admin_context, domain['id'], expected['id'], values=values)

        # Fetch the recordset again
        recordset = self.central_service.get_recordset(
            self.admin_context, domain['id'], expected['id'])

        # Ensure the record was updated correctly
        self.assertEqual(recordset['ttl'], 1800)

    def test_update_recordset_without_incrementing_serial(self):
        domain = self.create_domain()

        # Create a recordset
        expected = self.create_recordset(domain)

        # Fetch the domain so we have the latest serial number
        domain_before = self.central_service.get_domain(
            self.admin_context, domain['id'])

        # Update the recordset
        values = dict(ttl=1800)
        self.central_service.update_recordset(
            self.admin_context, domain['id'], expected['id'], values,
            increment_serial=False)

        # Fetch the recordset again
        recordset = self.central_service.get_recordset(
            self.admin_context, domain['id'], expected['id'])

        # Ensure the recordset was updated correctly
        self.assertEqual(recordset['ttl'], 1800)

        # Ensure the domains serial number was not updated
        domain_after = self.central_service.get_domain(
            self.admin_context, domain['id'])

        self.assertEqual(domain_before['serial'], domain_after['serial'])

    def test_update_recordset_incorrect_domain_id(self):
        domain = self.create_domain()
        other_domain = self.create_domain(fixture=1)

        # Create a recordset
        expected = self.create_recordset(domain)

        # Update the recordset
        values = dict(ttl=1800)

        # Ensure we get a 404 if we use the incorrect domain_id
        with testtools.ExpectedException(exceptions.RecordSetNotFound):
            self.central_service.update_recordset(
                self.admin_context, other_domain['id'], expected['id'],
                values=values)

    def test_delete_recordset(self):
        domain = self.create_domain()

        # Create a recordset
        recordset = self.create_recordset(domain)

        # Delete the recordset
        self.central_service.delete_recordset(
            self.admin_context, domain['id'], recordset['id'])

        # Fetch the recordset again, ensuring an exception is raised
        with testtools.ExpectedException(exceptions.RecordSetNotFound):
            self.central_service.get_recordset(
                self.admin_context, domain['id'], recordset['id'])

    def test_delete_recordset_without_incrementing_serial(self):
        domain = self.create_domain()

        # Create a recordset
        recordset = self.create_recordset(domain)

        # Fetch the domain so we have the latest serial number
        domain_before = self.central_service.get_domain(
            self.admin_context, domain['id'])

        # Delete the recordset
        self.central_service.delete_recordset(
            self.admin_context, domain['id'], recordset['id'],
            increment_serial=False)

        # Fetch the record again, ensuring an exception is raised
        with testtools.ExpectedException(exceptions.RecordSetNotFound):
            self.central_service.get_recordset(
                self.admin_context, domain['id'], recordset['id'])

        # Ensure the domains serial number was not updated
        domain_after = self.central_service.get_domain(
            self.admin_context, domain['id'])

        self.assertEqual(domain_before['serial'], domain_after['serial'])

    def test_delete_recordset_incorrect_domain_id(self):
        domain = self.create_domain()
        other_domain = self.create_domain(fixture=1)

        # Create a recordset
        recordset = self.create_recordset(domain)

        # Ensure we get a 404 if we use the incorrect domain_id
        with testtools.ExpectedException(exceptions.RecordSetNotFound):
            self.central_service.delete_recordset(
                self.admin_context, other_domain['id'], recordset['id'])

    def test_count_recordsets(self):
        # in the beginning, there should be nothing
        recordsets = self.central_service.count_recordsets(self.admin_context)
        self.assertEqual(recordsets, 0)

        # Create a domain to put our recordset in
        domain = self.create_domain()

        # Create a recordset
        self.create_recordset(domain)

        # We should have 1 recordset now
        recordsets = self.central_service.count_recordsets(self.admin_context)
        self.assertEqual(recordsets, 1)

    def test_count_recordsets_policy_check(self):
        # Set the policy to reject the authz
        self.policy({'count_recordsets': '!'})

        with testtools.ExpectedException(exceptions.Forbidden):
            self.central_service.count_recordsets(self.get_context())

    # Record Tests
    def test_create_record(self):
        domain = self.create_domain()
        recordset = self.create_recordset(domain, type='A')

        values = dict(
            data='127.0.0.1'
        )

        # Create a record
        record = self.central_service.create_record(
            self.admin_context, domain['id'], recordset['id'], values=values)

        # Ensure all values have been set correctly
        self.assertIsNotNone(record['id'])
        self.assertEqual(record['data'], values['data'])
        self.assertIn('status', record)

    def test_create_record_over_quota(self):
        self.config(quota_domain_records=1)

        domain = self.create_domain()
        recordset = self.create_recordset(domain)

        self.create_record(domain, recordset)

        with testtools.ExpectedException(exceptions.OverQuota):
            self.create_record(domain, recordset)

    def test_create_record_without_incrementing_serial(self):
        domain = self.create_domain()
        recordset = self.create_recordset(domain, type='A')

        values = dict(
            data='127.0.0.1'
        )

        # Create a record
        self.central_service.create_record(
            self.admin_context, domain['id'], recordset['id'], values=values,
            increment_serial=False)

        # Ensure the domains serial number was not updated
        updated_domain = self.central_service.get_domain(
            self.admin_context, domain['id'])

        self.assertEqual(domain['serial'], updated_domain['serial'])

    def test_get_record(self):
        domain = self.create_domain()
        recordset = self.create_recordset(domain)

        # Create a record
        expected = self.create_record(domain, recordset)

        # Retrieve it, and ensure it's the same
        record = self.central_service.get_record(
            self.admin_context, domain['id'], recordset['id'], expected['id'])

        self.assertEqual(record['id'], expected['id'])
        self.assertEqual(record['data'], expected['data'])
        self.assertIn('status', record)

    def test_get_record_incorrect_domain_id(self):
        domain = self.create_domain()
        recordset = self.create_recordset(domain)
        other_domain = self.create_domain(fixture=1)

        # Create a record
        expected = self.create_record(domain, recordset)

        # Ensure we get a 404 if we use the incorrect domain_id
        with testtools.ExpectedException(exceptions.RecordNotFound):
            self.central_service.get_record(
                self.admin_context, other_domain['id'], recordset['id'],
                expected['id'])

    def test_get_record_incorrect_recordset_id(self):
        domain = self.create_domain()
        recordset = self.create_recordset(domain)
        other_recordset = self.create_recordset(domain, fixture=1)

        # Create a record
        expected = self.create_record(domain, recordset)

        # Ensure we get a 404 if we use the incorrect recordset_id
        with testtools.ExpectedException(exceptions.RecordNotFound):
            self.central_service.get_record(
                self.admin_context, domain['id'], other_recordset['id'],
                expected['id'])

    def test_find_records(self):
        domain = self.create_domain()
        recordset = self.create_recordset(domain)

        criterion = {
            'domain_id': domain['id'],
            'recordset_id': recordset['id']
        }

        # Ensure we have no records to start with.
        records = self.central_service.find_records(
            self.admin_context, criterion)

        self.assertEqual(len(records), 0)

        # Create a single record (using default values)
        expected_one = self.create_record(domain, recordset)

        # Ensure we can retrieve the newly created record
        records = self.central_service.find_records(
            self.admin_context, criterion)

        self.assertEqual(len(records), 1)
        self.assertEqual(records[0]['data'], expected_one['data'])

        # Create a second record
        expected_two = self.create_record(domain, recordset, fixture=1)

        # Ensure we can retrieve both records
        records = self.central_service.find_records(
            self.admin_context, criterion)

        self.assertEqual(len(records), 2)
        self.assertEqual(records[0]['data'], expected_one['data'])
        self.assertEqual(records[1]['data'], expected_two['data'])

    def test_find_record(self):
        domain = self.create_domain()
        recordset = self.create_recordset(domain)

        # Create a record
        expected = self.create_record(domain, recordset)

        # Retrieve it, and ensure it's the same
        criterion = {
            'domain_id': domain['id'],
            'recordset_id': recordset['id'],
            'data': expected['data']
        }

        record = self.central_service.find_record(
            self.admin_context, criterion)

        self.assertEqual(record['id'], expected['id'])
        self.assertEqual(record['data'], expected['data'])
        self.assertIn('status', record)

    def test_update_record(self):
        domain = self.create_domain()
        recordset = self.create_recordset(domain, 'A')

        # Create a record
        expected = self.create_record(domain, recordset)

        # Update the record
        values = dict(data='127.0.0.2')
        self.central_service.update_record(
            self.admin_context, domain['id'], recordset['id'], expected['id'],
            values=values)

        # Fetch the record again
        record = self.central_service.get_record(
            self.admin_context, domain['id'], recordset['id'], expected['id'])

        # Ensure the record was updated correctly
        self.assertEqual(record['data'], '127.0.0.2')

    def test_update_record_without_incrementing_serial(self):
        domain = self.create_domain()
        recordset = self.create_recordset(domain, 'A')

        # Create a record
        expected = self.create_record(domain, recordset)

        # Fetch the domain so we have the latest serial number
        domain_before = self.central_service.get_domain(
            self.admin_context, domain['id'])

        # Update the record
        values = dict(data='127.0.0.2')

        self.central_service.update_record(
            self.admin_context, domain['id'], recordset['id'], expected['id'],
            values, increment_serial=False)

        # Fetch the record again
        record = self.central_service.get_record(
            self.admin_context, domain['id'], recordset['id'], expected['id'])

        # Ensure the record was updated correctly
        self.assertEqual(record['data'], '127.0.0.2')

        # Ensure the domains serial number was not updated
        domain_after = self.central_service.get_domain(
            self.admin_context, domain['id'])

        self.assertEqual(domain_before['serial'], domain_after['serial'])

    def test_update_record_incorrect_domain_id(self):
        domain = self.create_domain()
        recordset = self.create_recordset(domain, 'A')
        other_domain = self.create_domain(fixture=1)

        # Create a record
        expected = self.create_record(domain, recordset)

        # Update the record
        values = dict(data='127.0.0.2')

        # Ensure we get a 404 if we use the incorrect domain_id
        with testtools.ExpectedException(exceptions.RecordNotFound):
            self.central_service.update_record(
                self.admin_context, other_domain['id'], recordset['id'],
                expected['id'], values=values)

    def test_update_record_incorrect_recordset_id(self):
        domain = self.create_domain()
        recordset = self.create_recordset(domain, 'A')
        other_recordset = self.create_recordset(domain, 'A', fixture=1)

        # Create a record
        expected = self.create_record(domain, recordset)

        # Update the record
        values = dict(data='127.0.0.2')

        # Ensure we get a 404 if we use the incorrect domain_id
        with testtools.ExpectedException(exceptions.RecordNotFound):
            self.central_service.update_record(
                self.admin_context, domain['id'], other_recordset['id'],
                expected['id'], values=values)

    def test_delete_record(self):
        domain = self.create_domain()
        recordset = self.create_recordset(domain)

        # Create a record
        record = self.create_record(domain, recordset)

        # Delete the record
        self.central_service.delete_record(
            self.admin_context, domain['id'], recordset['id'], record['id'])

        # Fetch the record again, ensuring an exception is raised
        with testtools.ExpectedException(exceptions.RecordNotFound):
            self.central_service.get_record(
                self.admin_context, domain['id'], recordset['id'],
                record['id'])

    def test_delete_record_without_incrementing_serial(self):
        domain = self.create_domain()
        recordset = self.create_recordset(domain)

        # Create a record
        record = self.create_record(domain, recordset)

        # Fetch the domain so we have the latest serial number
        domain_before = self.central_service.get_domain(
            self.admin_context, domain['id'])

        # Delete the record
        self.central_service.delete_record(
            self.admin_context, domain['id'], recordset['id'], record['id'],
            increment_serial=False)

        # Fetch the record again, ensuring an exception is raised
        with testtools.ExpectedException(exceptions.RecordNotFound):
            self.central_service.get_record(
                self.admin_context, domain['id'], recordset['id'],
                record['id'])

        # Ensure the domains serial number was not updated
        domain_after = self.central_service.get_domain(
            self.admin_context, domain['id'])

        self.assertEqual(domain_before['serial'], domain_after['serial'])

    def test_delete_record_incorrect_domain_id(self):
        domain = self.create_domain()
        recordset = self.create_recordset(domain)
        other_domain = self.create_domain(fixture=1)

        # Create a record
        record = self.create_record(domain, recordset)

        # Ensure we get a 404 if we use the incorrect domain_id
        with testtools.ExpectedException(exceptions.RecordNotFound):
            self.central_service.delete_record(
                self.admin_context, other_domain['id'], recordset['id'],
                record['id'])

    def test_delete_record_incorrect_recordset_id(self):
        domain = self.create_domain()
        recordset = self.create_recordset(domain)
        other_recordset = self.create_recordset(domain, fixture=1)

        # Create a record
        record = self.create_record(domain, recordset)

        # Ensure we get a 404 if we use the incorrect recordset_id
        with testtools.ExpectedException(exceptions.RecordNotFound):
            self.central_service.delete_record(
                self.admin_context, domain['id'], other_recordset['id'],
                record['id'])

    def test_count_records(self):
        # in the beginning, there should be nothing
        records = self.central_service.count_records(self.admin_context)
        self.assertEqual(records, 0)

        # Create a domain and recordset to put our record in
        domain = self.create_domain()
        recordset = self.create_recordset(domain)

        # Create a record
        self.create_record(domain, recordset)

        # we should have 1 record now
        records = self.central_service.count_records(self.admin_context)
        self.assertEqual(records, 1)

    def test_count_records_policy_check(self):
        # Set the policy to reject the authz
        self.policy({'count_records': '!'})

        with testtools.ExpectedException(exceptions.Forbidden):
            self.central_service.count_records(self.get_context())

    def test_get_floatingip_no_record(self):
        self.create_server()

        context = self.get_context(tenant='a')

        fip = self.network_api.fake.allocate_floatingip(context.tenant)

        fip_ptr = self.central_service.get_floatingip(
            context, fip['region'], fip['id'])

        self.assertEqual(fip['region'], fip_ptr['region'])
        self.assertEqual(fip['id'], fip_ptr['id'])
        self.assertEqual(fip['address'], fip_ptr['address'])
        self.assertEqual(None, fip_ptr['ptrdname'])

    def test_get_floatingip_with_record(self):
        self.create_server()

        context = self.get_context(tenant='a')

        fixture = self.get_ptr_fixture()

        fip = self.network_api.fake.allocate_floatingip(context.tenant)

        expected = self.central_service.update_floatingip(
            context, fip['region'], fip['id'], fixture)

        actual = self.central_service.get_floatingip(
            context, fip['region'], fip['id'])
        self.assertEqual(expected, actual)

        self.assertEqual(expected, actual)

    def test_get_floatingip_not_allocated(self):
        context = self.get_context(tenant='a')

        fip = self.network_api.fake.allocate_floatingip(context.tenant)
        self.network_api.fake.deallocate_floatingip(fip['id'])

        with testtools.ExpectedException(exceptions.NotFound):
            self.central_service.get_floatingip(
                context, fip['region'], fip['id'])

    def test_get_floatingip_deallocated_and_invalidate(self):
        self.create_server()

        context_a = self.get_context(tenant='a')
        elevated_a = context_a.elevated()
        elevated_a.all_tenants = True

        context_b = self.get_context(tenant='b')

        fixture = self.get_ptr_fixture()

        # First allocate and create a FIP as tenant a
        fip = self.network_api.fake.allocate_floatingip(context_a.tenant)

        self.central_service.update_floatingip(
            context_a, fip['region'], fip['id'], fixture)

        self.network_api.fake.deallocate_floatingip(fip['id'])

        with testtools.ExpectedException(exceptions.NotFound):
            self.central_service.get_floatingip(
                context_a, fip['region'], fip['id'])

        # Ensure that the record is still in DB (No invalidation)
        criterion = {
            'managed_resource_id': fip['id'],
            'managed_tenant_id': context_a.tenant}
        self.central_service.find_record(elevated_a, criterion)

        # Now give the fip id to tenant 'b' and see that it get's deleted
        self.network_api.fake.allocate_floatingip(
            context_b.tenant, fip['id'])

        # There should be a fip returned with ptrdname of None
        fip_ptr = self.central_service.get_floatingip(
            context_b, fip['region'], fip['id'])
        self.assertEqual(None, fip_ptr['ptrdname'])

        # Ensure that the old record for tenant a for the fip now owned by
        # tenant b is gone
        with testtools.ExpectedException(exceptions.RecordNotFound):
            self.central_service.find_record(elevated_a, criterion)

    def test_list_floatingips_no_allocations(self):
        context = self.get_context(tenant='a')

        fips = self.central_service.list_floatingips(context)

        self.assertEqual(0, len(fips))

    def test_list_floatingips_no_record(self):
        context = self.get_context(tenant='a')

        fip = self.network_api.fake.allocate_floatingip(context.tenant)

        fips = self.central_service.list_floatingips(context)

        self.assertEqual(1, len(fips))
        self.assertEqual(None, fips[0]['ptrdname'])
        self.assertEqual(fip['id'], fips[0]['id'])
        self.assertEqual(fip['region'], fips[0]['region'])
        self.assertEqual(fip['address'], fips[0]['address'])
        self.assertEqual(None, fips[0]['description'])

    def test_list_floatingips_with_record(self):
        self.create_server()

        context = self.get_context(tenant='a')

        fixture = self.get_ptr_fixture()

        fip = self.network_api.fake.allocate_floatingip(context.tenant)

        fip_ptr = self.central_service.update_floatingip(
            context, fip['region'], fip['id'], fixture)

        fips = self.central_service.list_floatingips(context)

        self.assertEqual(1, len(fips))
        self.assertEqual(fip_ptr['ptrdname'], fips[0]['ptrdname'])
        self.assertEqual(fip_ptr['id'], fips[0]['id'])
        self.assertEqual(fip_ptr['region'], fips[0]['region'])
        self.assertEqual(fip_ptr['address'], fips[0]['address'])
        self.assertEqual(fip_ptr['description'], fips[0]['description'])

    def test_list_floatingips_deallocated_and_invalidate(self):
        self.create_server()

        context_a = self.get_context(tenant='a')
        elevated_a = context_a.elevated()
        elevated_a.all_tenants = True

        context_b = self.get_context(tenant='b')

        fixture = self.get_ptr_fixture()

        # First allocate and create a FIP as tenant a
        fip = self.network_api.fake.allocate_floatingip(context_a.tenant)

        self.central_service.update_floatingip(
            context_a, fip['region'], fip['id'], fixture)

        self.network_api.fake.deallocate_floatingip(fip['id'])

        fips = self.central_service.list_floatingips(context_a)
        self.assertEqual([], fips)

        # Ensure that the record is still in DB (No invalidation)
        criterion = {
            'managed_resource_id': fip['id'],
            'managed_tenant_id': context_a.tenant}
        self.central_service.find_record(elevated_a, criterion)

        # Now give the fip id to tenant 'b' and see that it get's deleted
        self.network_api.fake.allocate_floatingip(
            context_b.tenant, fip['id'])

        # There should be a fip returned with ptrdname of None
        fips = self.central_service.list_floatingips(context_b)
        self.assertEqual(1, len(fips))
        self.assertEqual(None, fips[0]['ptrdname'])

        # Ensure that the old record for tenant a for the fip now owned by
        # tenant b is gone
        with testtools.ExpectedException(exceptions.RecordNotFound):
            self.central_service.find_record(elevated_a, criterion)

    def test_set_floatingip(self):
        self.create_server()

        context = self.get_context(tenant='a')

        fixture = self.get_ptr_fixture()

        fip = self.network_api.fake.allocate_floatingip(context.tenant)

        fip_ptr = self.central_service.update_floatingip(
            context, fip['region'], fip['id'], fixture)

        self.assertEqual(fixture['ptrdname'], fip_ptr['ptrdname'])
        self.assertEqual(fip['address'], fip_ptr['address'])
        self.assertEqual(None, fip_ptr['description'])
        self.assertIsNotNone(fip_ptr['ttl'])

    def test_set_floatingip_removes_old_rrset_and_record(self):
        self.create_server()

        context_a = self.get_context(tenant='a')
        elevated_a = context_a.elevated()
        elevated_a.all_tenants = True

        context_b = self.get_context(tenant='b')

        fixture = self.get_ptr_fixture()

        # Test that re-setting as tenant a an already set floatingip leaves
        # only 1 record
        fip = self.network_api.fake.allocate_floatingip(context_a.tenant)

        self.central_service.update_floatingip(
            context_a, fip['region'], fip['id'], fixture)

        fixture2 = self.get_ptr_fixture(fixture=1)
        self.central_service.update_floatingip(
            context_a, fip['region'], fip['id'], fixture2)

        count = self.central_service.count_records(
            elevated_a, {'managed_resource_id': fip['id']})

        self.assertEqual(1, count)

        self.network_api.fake.deallocate_floatingip(fip['id'])

        # Now test that tenant b allocating the same fip and setting a ptr
        # deletes any records
        fip = self.network_api.fake.allocate_floatingip(
            context_b.tenant, fip['id'])

        self.central_service.update_floatingip(
            context_b, fip['region'], fip['id'], fixture)

        count = self.central_service.count_records(
            elevated_a, {'managed_resource_id': fip['id']})

        self.assertEqual(1, count)

    def test_set_floatingip_not_allocated(self):
        context = self.get_context(tenant='a')
        fixture = self.get_ptr_fixture()

        fip = self.network_api.fake.allocate_floatingip(context.tenant)
        self.network_api.fake.deallocate_floatingip(fip['id'])

        # If one attempts to assign a de-allocated FIP or not-owned it should
        # fail with BadRequest
        with testtools.ExpectedException(exceptions.NotFound):
            fixture = self.central_service.update_floatingip(
                context, fip['region'], fip['id'], fixture)

    def test_unset_floatingip(self):
        self.create_server()

        context = self.get_context(tenant='a')

        fixture = self.get_ptr_fixture()

        fip = self.network_api.fake.allocate_floatingip(context.tenant)

        fip_ptr = self.central_service.update_floatingip(
            context, fip['region'], fip['id'], fixture)

        self.assertEqual(fixture['ptrdname'], fip_ptr['ptrdname'])
        self.assertEqual(fip['address'], fip_ptr['address'])
        self.assertEqual(None, fip_ptr['description'])
        self.assertIsNotNone(fip_ptr['ttl'])

        self.central_service.update_floatingip(
            context, fip['region'], fip['id'], {'ptrdname': None})

        self.central_service.get_floatingip(
            context, fip['region'], fip['id'])

    # Blacklist Tests
    def test_create_blacklist(self):
        values = self.get_blacklist_fixture(fixture=0)

        blacklist = self.create_blacklist(fixture=0)

        # Verify all values have been set correctly
        self.assertIsNotNone(blacklist['id'])
        self.assertEqual(blacklist['pattern'], values['pattern'])
        self.assertEqual(blacklist['description'], values['description'])

    def test_get_blacklist(self):
        # Create a blacklisted zone
        expected = self.create_blacklist(fixture=0)

        # Retrieve it, and verify it is the same
        blacklist = self.central_service.get_blacklist(
            self.admin_context, expected['id'])

        self.assertEqual(blacklist['id'], expected['id'])
        self.assertEqual(blacklist['pattern'], expected['pattern'])
        self.assertEqual(blacklist['description'], expected['description'])

    def test_find_blacklists(self):
        # Verify there are no blacklisted zones to start with
        blacklists = self.central_service.find_blacklists(
            self.admin_context)

        self.assertEqual(len(blacklists), 0)

        # Create a single blacklisted zone
        self.create_blacklist()

        # Verify we can retrieve the newly created blacklist
        blacklists = self.central_service.find_blacklists(
            self.admin_context)
        values1 = self.get_blacklist_fixture(fixture=0)

        self.assertEqual(len(blacklists), 1)
        self.assertEqual(blacklists[0]['pattern'], values1['pattern'])

        # Create a second blacklisted zone
        self.create_blacklist(fixture=1)

        # Verify we can retrieve both blacklisted zones
        blacklists = self.central_service.find_blacklists(
            self.admin_context)

        values2 = self.get_blacklist_fixture(fixture=1)

        self.assertEqual(len(blacklists), 2)
        self.assertEqual(blacklists[0]['pattern'], values1['pattern'])
        self.assertEqual(blacklists[1]['pattern'], values2['pattern'])

    def test_find_blacklist(self):
        #Create a blacklisted zone
        expected = self.create_blacklist(fixture=0)

        # Retrieve the newly created blacklist
        blacklist = self.central_service.find_blacklist(
            self.admin_context, {'id': expected['id']})

        self.assertEqual(blacklist['pattern'], expected['pattern'])
        self.assertEqual(blacklist['description'], expected['description'])

    def test_update_blacklist(self):
        # Create a blacklisted zone
        expected = self.create_blacklist(fixture=0)
        new_comment = "This is a different comment."

        # Update the blacklist
        updated_values = dict(
            description=new_comment
        )
        self.central_service.update_blacklist(self.admin_context,
                                              expected['id'],
                                              updated_values)

        # Fetch the blacklist
        blacklist = self.central_service.get_blacklist(self.admin_context,
                                                       expected['id'])

        # Verify that the record was updated correctly
        self.assertEqual(blacklist['description'], new_comment)

    def test_delete_blacklist(self):
        # Create a blacklisted zone
        blacklist = self.create_blacklist()

        # Delete the blacklist
        self.central_service.delete_blacklist(self.admin_context,
                                              blacklist['id'])

        # Try to fetch the blacklist to verify an exception is raised
        with testtools.ExpectedException(exceptions.BlacklistNotFound):
            self.central_service.get_blacklist(self.admin_context,
                                               blacklist['id'])

########NEW FILE########
__FILENAME__ = _test_service_ipa
# -*- coding: utf-8 -*-
# Copyright 2014 Red Hat, Inc.
#
# Author: Rich Megginson <rmeggins@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import unittest

from designate.openstack.common import log as logging
import designate.tests.test_central.test_service
from designate import utils
from designate import exceptions
import testtools
from designate.backend import impl_ipa

LOG = logging.getLogger(__name__)


class CentralServiceTestIPA(designate.tests.test_central.
                            test_service.CentralServiceTest):

    test_config_file_name = "designate-ipa-test.conf"

    def start_service(self, svc_name, *args, **kw):
        """
        override here so we can make sure central is set up correctly
        for live ipa testing
        """
        if svc_name == 'central':
            self.config(backend_driver='ipa', group='service:central')
            # options in the test_config file can override the
            # defaults set elsewhere
            test_config = utils.find_config(self.test_config_file_name)
            self.CONF([], project='designate',
                      default_config_files=test_config)
        return super(CentralServiceTestIPA, self).start_service(svc_name,
                                                                *args, **kw)

    def setUp(self):
        super(CentralServiceTestIPA, self).setUp()
        # go directly through storage api to bypass tenant/policy checks
        save_all_tenants = self.admin_context.all_tenants
        self.admin_context.all_tenants = True
        self.startdomains = self.central_service.storage_api.\
            find_domains(self.admin_context)
        LOG.debug("%s.setUp: startdomains %d" % (self.__class__,
                                                 len(self.startdomains)))
        self.admin_context.all_tenants = save_all_tenants

    def tearDown(self):
        # delete domains
        # go directly through storage api to bypass tenant/policy checks
        self.admin_context.all_tenants = True
        domains = self.central_service.storage_api.\
            find_domains(self.admin_context)
        LOG.debug("%s.tearDown: domains %d" % (self.__class__,
                                               len(self.startdomains)))
        for domain in domains:
            if domain in self.startdomains:
                continue
            # go directly to backend - front end domains will be
            # removed when the database fixture is reset
            self.central_service.backend.delete_domain(self.admin_context,
                                                       domain)

        super(CentralServiceTestIPA, self).tearDown()

    def assertRecordsEqual(self, rec1, rec2):
        rec1dict = dict(rec1.iteritems())
        rec2dict = dict(rec2.iteritems())
        self.assertEqual(rec1dict, rec2dict)

    def test_delete_recordset_extra(self):
        domain = self.create_domain()

        # Create a recordset
        recsetA = self.create_recordset(domain, 'A')
        recsetMX = self.create_recordset(domain, 'MX')

        # create two records in recsetA
        recA0 = self.create_record(domain, recsetA, fixture=0)
        recA1 = self.create_record(domain, recsetA, fixture=1)

        # create two records in recsetMX
        recMX0 = self.create_record(domain, recsetMX, fixture=0)
        recMX1 = self.create_record(domain, recsetMX, fixture=1)

        # verify two records in each recset
        criterion = {
            'domain_id': domain['id'],
            'recordset_id': recsetA['id']
        }

        records = self.central_service.find_records(
            self.admin_context, criterion)

        self.assertEqual(len(records), 2)
        self.assertRecordsEqual(recA0, records[0])
        self.assertRecordsEqual(recA1, records[1])

        criterion['recordset_id'] = recsetMX['id']

        records = self.central_service.find_records(
            self.admin_context, criterion)

        self.assertEqual(len(records), 2)
        self.assertRecordsEqual(recMX0, records[0])
        self.assertRecordsEqual(recMX1, records[1])

        # Delete recsetA
        self.central_service.delete_recordset(
            self.admin_context, domain['id'], recsetA['id'])

        # Fetch the recordset again, ensuring an exception is raised
        with testtools.ExpectedException(exceptions.RecordSetNotFound):
            self.central_service.get_recordset(
                self.admin_context, domain['id'], recsetA['id'])

        # should be no records left in recsetA
        # however, that doesn't appear to be how
        # designate currently works, at least in
        # this particular test
        delete_recset_deletes_recs = False
        if delete_recset_deletes_recs:
            criterion['recordset_id'] = recsetA['id']

            records = self.central_service.find_records(
                self.admin_context, criterion)

            self.assertEqual(len(records), 0)

        # verify two records in recsetMX
        criterion['recordset_id'] = recsetMX['id']

        records = self.central_service.find_records(
            self.admin_context, criterion)

        self.assertEqual(len(records), 2)

        # Delete recsetMX
        self.central_service.delete_recordset(
            self.admin_context, domain['id'], recsetMX['id'])

    def test_create_domain_no_min_ttl(self):
        """Override - ipa does not allow negative ttl values -
        instead, check for proper error
        """
        self.policy({'use_low_ttl': '!'})
        self.config(min_ttl="None",
                    group='service:central')
        values = self.get_domain_fixture(1)
        values['ttl'] = -100

        # Create a server
        self.create_server()

        # Create domain with negative TTL
        with testtools.ExpectedException(impl_ipa.IPAInvalidData):
            self.central_service.create_domain(
                self.admin_context, values=values)

    @unittest.skip("this is currently broken in IPA")
    def test_idn_create_domain_over_tld(self):
        pass

    @unittest.skip("not supported in IPA")
    def test_create_tsigkey(self):
        pass

    @unittest.skip("not supported in IPA")
    def test_get_tsigkey(self):
        pass

    @unittest.skip("not supported in IPA")
    def test_find_tsigkeys(self):
        pass

    @unittest.skip("not supported in IPA")
    def test_delete_tsigkey(self):
        pass

    @unittest.skip("not supported in IPA")
    def test_update_tsigkey(self):
        pass

########NEW FILE########
__FILENAME__ = test_context
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.tests import TestCase
from designate import context


class TestDesignateContext(TestCase):
    def test_deepcopy(self):
        orig = context.DesignateContext(user='12345', tenant='54321')
        copy = orig.deepcopy()

        self.assertEqual(orig.to_dict(), copy.to_dict())

    def test_elevated(self):
        ctxt = context.DesignateContext(user='12345', tenant='54321')
        admin_ctxt = ctxt.elevated()

        self.assertFalse(ctxt.is_admin)
        self.assertTrue(admin_ctxt.is_admin)

########NEW FILE########
__FILENAME__ = test_neutron
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Endre Karlson <endre.karlson@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate import exceptions
from designate.network_api import get_network_api
from designate.tests import TestCase

from neutronclient.v2_0 import client as clientv20
from neutronclient.common import exceptions as neutron_exceptions

from oslo.config import cfg
from mock import patch
import testtools


cfg.CONF.import_group('network_api:neutron', 'designate.network_api.neutron')


class NeutronAPITest(TestCase):
    def setUp(self):
        super(NeutronAPITest, self).setUp()
        self.config(endpoints=['RegionOne|http://localhost:9696'],
                    group='network_api:neutron')
        self.api = get_network_api('neutron')

    @patch.object(clientv20.Client, 'list_floatingips',
                  side_effect=neutron_exceptions.Unauthorized)
    def test_unauthorized_returns_empty(self, _):
        context = self.get_context(tenant='a', auth_token='test')

        fips = self.api.list_floatingips(context)
        self.assertEqual(0, len(fips))

    @patch.object(clientv20.Client, 'list_floatingips',
                  side_effect=neutron_exceptions.NeutronException)
    def test_communication_failure(self, _):
        context = self.get_context(tenant='a', auth_token='test')

        with testtools.ExpectedException(
                exceptions.NeutronCommunicationFailure):
            self.api.list_floatingips(context)

########NEW FILE########
__FILENAME__ = test_neutron
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.openstack.common import log as logging
from designate.tests import TestCase
from designate.notification_handler.neutron import NeutronFloatingHandler
from designate.tests.test_notification_handler import \
    NotificationHandlerMixin

LOG = logging.getLogger(__name__)


class NeutronFloatingHandlerTest(TestCase, NotificationHandlerMixin):
    def setUp(self):
        super(NeutronFloatingHandlerTest, self).setUp()

        self.central_service = self.start_service('central')

        domain = self.create_domain()
        self.domain_id = domain['id']
        self.config(domain_id=domain['id'], group='handler:neutron_floatingip')

        self.plugin = NeutronFloatingHandler()

    def test_floatingip_associate(self):
        event_type = 'floatingip.update.end'
        fixture = self.get_notification_fixture(
            'neutron', event_type + '_associate')

        self.assertIn(event_type, self.plugin.get_event_types())

        criterion = {'domain_id': self.domain_id}

        # Ensure we start with 0 records
        records = self.central_service.find_records(self.admin_context,
                                                    criterion)

        self.assertEqual(0, len(records))

        self.plugin.process_notification(
            self.admin_context, event_type, fixture['payload'])

        # Ensure we now have exactly 1 record
        records = self.central_service.find_records(self.admin_context,
                                                    criterion)

        self.assertEqual(1, len(records))

    def test_floatingip_disassociate(self):
        start_event_type = 'floatingip.update.end'
        start_fixture = self.get_notification_fixture(
            'neutron', start_event_type + '_associate')
        self.plugin.process_notification(self.admin_context,
                                         start_event_type,
                                         start_fixture['payload'])

        event_type = 'floatingip.update.end'
        fixture = self.get_notification_fixture(
            'neutron', event_type + '_disassociate')

        self.assertIn(event_type, self.plugin.get_event_types())

        criterion = {'domain_id': self.domain_id}

        # Ensure we start with at least 1 record
        records = self.central_service.find_records(self.admin_context,
                                                    criterion)

        self.assertEqual(1, len(records))

        self.plugin.process_notification(
            self.admin_context, event_type, fixture['payload'])

        # Ensure we now have exactly 0 records
        records = self.central_service.find_records(self.admin_context,
                                                    criterion)

        self.assertEqual(0, len(records))

    def test_floatingip_delete(self):
        start_event_type = 'floatingip.update.end'
        start_fixture = self.get_notification_fixture(
            'neutron', start_event_type + '_associate')
        self.plugin.process_notification(self.admin_context,
                                         start_event_type,
                                         start_fixture['payload'])

        event_type = 'floatingip.delete.start'
        fixture = self.get_notification_fixture(
            'neutron', event_type)

        self.assertIn(event_type, self.plugin.get_event_types())

        criterion = {'domain_id': self.domain_id}

        # Ensure we start with at least 1 record
        records = self.central_service.find_records(self.admin_context,
                                                    criterion)
        self.assertEqual(1, len(records))

        self.plugin.process_notification(
            self.admin_context, event_type, fixture['payload'])

        # Ensure we now have exactly 0 records
        records = self.central_service.find_records(self.admin_context,
                                                    criterion)

        self.assertEqual(0, len(records))

########NEW FILE########
__FILENAME__ = test_nova
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.openstack.common import log as logging
from designate.tests import TestCase
from designate.notification_handler.nova import NovaFixedHandler
from designate.tests.test_notification_handler import \
    NotificationHandlerMixin

LOG = logging.getLogger(__name__)


class NovaFixedHandlerTest(TestCase, NotificationHandlerMixin):
    def setUp(self):
        super(NovaFixedHandlerTest, self).setUp()

        self.central_service = self.start_service('central')

        domain = self.create_domain()
        self.domain_id = domain['id']
        self.config(domain_id=domain['id'], group='handler:nova_fixed')

        self.plugin = NovaFixedHandler()

    def test_instance_create_end(self):
        event_type = 'compute.instance.create.end'
        fixture = self.get_notification_fixture('nova', event_type)

        self.assertIn(event_type, self.plugin.get_event_types())

        criterion = {'domain_id': self.domain_id}

        # Ensure we start with 0 records
        records = self.central_service.find_records(self.admin_context,
                                                    criterion)

        self.assertEqual(0, len(records))

        self.plugin.process_notification(
            self.admin_context, event_type, fixture['payload'])

        # Ensure we now have exactly 1 record
        records = self.central_service.find_records(self.admin_context,
                                                    criterion)

        self.assertEqual(1, len(records))

    def test_instance_delete_start(self):
        # Prepare for the test
        start_event_type = 'compute.instance.create.end'
        start_fixture = self.get_notification_fixture('nova', start_event_type)

        self.plugin.process_notification(self.admin_context,
                                         start_event_type,
                                         start_fixture['payload'])

        # Now - Onto the real test
        event_type = 'compute.instance.delete.start'
        fixture = self.get_notification_fixture('nova', event_type)

        self.assertIn(event_type, self.plugin.get_event_types())

        criterion = {'domain_id': self.domain_id}

        # Ensure we start with at least 1 record
        records = self.central_service.find_records(self.admin_context,
                                                    criterion)

        self.assertEqual(1, len(records))

        self.plugin.process_notification(
            self.admin_context, event_type, fixture['payload'])

        # Ensure we now have exactly 0 records
        records = self.central_service.find_records(self.admin_context,
                                                    criterion)

        self.assertEqual(0, len(records))

########NEW FILE########
__FILENAME__ = test_quota
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from testscenarios import load_tests_apply_scenarios as load_tests  # noqa
import testtools
from oslo.config import cfg
from designate.openstack.common import log as logging
from designate import quota
from designate import tests
from designate import exceptions


LOG = logging.getLogger(__name__)


class QuotaTestCase(tests.TestCase):
    scenarios = [
        ('noop', dict(quota_driver='noop')),
        ('storage', dict(quota_driver='storage'))
    ]

    def setUp(self):
        super(QuotaTestCase, self).setUp()
        self.config(quota_driver=self.quota_driver)
        self.quota = quota.get_quota()

    def test_get_quotas(self):
        context = self.get_admin_context()

        quotas = self.quota.get_quotas(context, 'DefaultQuotaTenant')

        self.assertIsNotNone(quotas)
        self.assertEqual({
            'domains': cfg.CONF.quota_domains,
            'domain_recordsets': cfg.CONF.quota_domain_recordsets,
            'domain_records': cfg.CONF.quota_domain_records,
            'recordset_records': cfg.CONF.quota_recordset_records,
        }, quotas)

    def test_limit_check_unknown(self):
        context = self.get_admin_context()

        with testtools.ExpectedException(exceptions.QuotaResourceUnknown):
            self.quota.limit_check(context, 'tenant_id', unknown=0)

        with testtools.ExpectedException(exceptions.QuotaResourceUnknown):
            self.quota.limit_check(context, 'tenant_id', unknown=0, domains=0)

    def test_limit_check_under(self):
        context = self.get_admin_context()

        self.quota.limit_check(context, 'tenant_id', domains=0)
        self.quota.limit_check(context, 'tenant_id', domain_records=0)
        self.quota.limit_check(context, 'tenant_id', domains=0,
                               domain_records=0)

        self.quota.limit_check(context, 'tenant_id',
                               domains=(cfg.CONF.quota_domains - 1))
        self.quota.limit_check(
            context,
            'tenant_id',
            domain_records=(cfg.CONF.quota_domain_records - 1))

    def test_limit_check_at(self):
        context = self.get_admin_context()

        with testtools.ExpectedException(exceptions.OverQuota):
            self.quota.limit_check(context, 'tenant_id',
                                   domains=cfg.CONF.quota_domains)

        with testtools.ExpectedException(exceptions.OverQuota):
            self.quota.limit_check(
                context,
                'tenant_id',
                domain_records=cfg.CONF.quota_domain_records)

    def test_limit_check_over(self):
        context = self.get_admin_context()

        with testtools.ExpectedException(exceptions.OverQuota):
            self.quota.limit_check(context, 'tenant_id', domains=99999)

        with testtools.ExpectedException(exceptions.OverQuota):
            self.quota.limit_check(context, 'tenant_id', domain_records=99999)

        with testtools.ExpectedException(exceptions.OverQuota):
            self.quota.limit_check(context, 'tenant_id', domains=99999,
                                   domain_records=99999)

        with testtools.ExpectedException(exceptions.OverQuota):
            self.quota.limit_check(context, 'tenant_id', domains=99999,
                                   domain_records=0)

        with testtools.ExpectedException(exceptions.OverQuota):
            self.quota.limit_check(context, 'tenant_id', domains=0,
                                   domain_records=99999)

########NEW FILE########
__FILENAME__ = test_storage
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate import quota
from designate import tests
from designate.openstack.common import log as logging

LOG = logging.getLogger(__name__)


class StorageQuotaTest(tests.TestCase):
    def setUp(self):
        super(StorageQuotaTest, self).setUp()
        self.config(quota_driver='storage')
        self.quota = quota.get_quota()

    def test_set_quota_create(self):
        context = self.get_admin_context()
        context.all_tenants = True

        quota = self.quota.set_quota(context, 'tenant_id', 'domains', 1500)

        self.assertEqual(quota, {'domains': 1500})

        # Drop into the storage layer directly to ensure the quota was created
        # successfully
        criterion = {
            'tenant_id': 'tenant_id',
            'resource': 'domains'
        }

        quota = self.quota.storage_api.find_quota(context, criterion)

        self.assertEqual(quota['tenant_id'], 'tenant_id')
        self.assertEqual(quota['resource'], 'domains')
        self.assertEqual(quota['hard_limit'], 1500)

    def test_set_quota_update(self):
        context = self.get_admin_context()
        context.all_tenants = True

        # First up, Create the quota
        self.quota.set_quota(context, 'tenant_id', 'domains', 1500)

        # Next, update the quota
        self.quota.set_quota(context, 'tenant_id', 'domains', 1234)

        # Drop into the storage layer directly to ensure the quota was updated
        # successfully
        criterion = {
            'tenant_id': 'tenant_id',
            'resource': 'domains'
        }

        quota = self.quota.storage_api.find_quota(context, criterion)

        self.assertEqual(quota['tenant_id'], 'tenant_id')
        self.assertEqual(quota['resource'], 'domains')
        self.assertEqual(quota['hard_limit'], 1234)

    def test_reset_quotas(self):
        context = self.get_admin_context()
        context.all_tenants = True

        # First up, Create a domains quota
        self.quota.set_quota(context, 'tenant_id', 'domains', 1500)

        # Then, Create a domain_records quota
        self.quota.set_quota(context, 'tenant_id', 'domain_records', 800)

        # Now, Reset the tenants quota
        self.quota.reset_quotas(context, 'tenant_id')

        # Drop into the storage layer directly to ensure the tenant has no
        # specific quotas registed.
        criterion = {
            'tenant_id': 'tenant_id'
        }

        quotas = self.quota.storage_api.find_quotas(context, criterion)
        self.assertEqual(0, len(quotas))

########NEW FILE########
__FILENAME__ = test_v2
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.openstack.common import log as logging
from designate import schema
from designate.tests import TestCase

LOG = logging.getLogger(__name__)


class SchemasV2Test(TestCase):
    def test_recordset(self):
        validator = schema.Schema('v2', 'recordset')

        # Pass Expected
        validator.validate({
            'recordset': {
                'id': 'b22d09e0-efa3-11e2-b778-0800200c9a66',
                'zone_id': 'b22d09e0-efa3-11e2-b778-0800200c9a66',
                'name': 'example.com.',
                'type': 'A'
            }
        })

        # Pass Expected
        validator.validate({
            'recordset': {
                'id': 'b22d09e0-efa3-11e2-b778-0800200c9a66',
                'zone_id': 'b22d09e0-efa3-11e2-b778-0800200c9a66',
                'name': 'example.com.',
                'type': 'MX'
            }
        })

########NEW FILE########
__FILENAME__ = test_format
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.tests import TestCase
from designate.openstack.common import log as logging
from designate.schema import format

LOG = logging.getLogger(__name__)


class SchemaFormatTest(TestCase):
    def test_is_ipv4(self):
        valid_ipaddresses = [
            '0.0.0.1',
            '127.0.0.1',
            '10.0.0.1',
            '192.0.2.2',
        ]

        invalid_ipaddresses = [
            '0.0.0.0',
            '0.0.0.256',
            '0.0.256.0',
            '0.256.0.0',
            '256.0.0.0',
            '127.0.0',
            '127.0.0.999',
            '127.0.0.256',
            '127.0..1',
            '-1.0.0.1',
            '1.0.-0.1',
            '1.0.0.-1',
            'ABCDEF',
            'ABC/DEF',
            'ABC\\DEF',
        ]

        for ipaddress in valid_ipaddresses:
            self.assertTrue(format.is_ipv4(ipaddress))

        for ipaddress in invalid_ipaddresses:
            self.assertFalse(format.is_ipv4(ipaddress))

    def test_is_hostname(self):
        valid_hostnames = [
            'example.com.',
            'www.example.com.',
            '*.example.com.',
            '12345.example.com.',
            '192-0-2-1.example.com.',
            'ip192-0-2-1.example.com.',
            'www.ip192-0-2-1.example.com.',
            'ip192-0-2-1.www.example.com.',
            'abc-123.example.com.',
            '_tcp.example.com.',
            '_service._tcp.example.com.',
            ('1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.8.b.d.0.1.0.0.2'
             '.ip6.arpa.'),
            '1.1.1.1.in-addr.arpa.',
            'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.',
            ('abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghi.'),
        ]

        invalid_hostnames = [
            '**.example.com.',
            '*.*.example.org.',
            'a.*.example.org.',
            # Exceeds single lable length limit
            ('abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijkL'
             '.'),
            ('abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijkL'
             '.'),
            # Exceeds total length limit
            ('abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopq.'),
            # Empty label part
            'abc..def.',
            '..',
            # Invalid character
            'abc$.def.',
            'abc.def$.',
            # Labels must not start with a -
            '-abc.',
            'abc.-def.',
            'abc.-def.ghi.',
            # Labels must not end with a -
            'abc-.',
            'abc.def-.',
            'abc.def-.ghi.',
            # Labels must not start or end with a -
            '-abc-.',
            'abc.-def-.',
            'abc.-def-.ghi.',
        ]

        for hostname in valid_hostnames:
            self.assertTrue(format.is_hostname(hostname))

        for hostname in invalid_hostnames:
            self.assertFalse(format.is_hostname(hostname))

    def test_is_domainname(self):
        valid_domainnames = [
            'example.com.',
            'www.example.com.',
            '12345.example.com.',
            '192-0-2-1.example.com.',
            'ip192-0-2-1.example.com.',
            'www.ip192-0-2-1.example.com.',
            'ip192-0-2-1.www.example.com.',
            'abc-123.example.com.',
            '_tcp.example.com.',
            '_service._tcp.example.com.',
            ('1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.8.b.d.0.1.0.0.2'
             '.ip6.arpa.'),
            '1.1.1.1.in-addr.arpa.',
            'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.',
            ('abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghi.'),
        ]

        invalid_domainnames = [
            '*.example.com.',
            '**.example.com.',
            '*.*.example.org.',
            'a.*.example.org.',
            # Exceeds single lable length limit
            ('abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijkL'
             '.'),
            ('abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijkL'
             '.'),
            ('abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijkL'
             '.'),
            # Exceeds total length limit
            ('abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopq.'),
            # Empty label part
            'abc..def.',
            '..',
            # Invalid character
            'abc$.def.',
            'abc.def$.',
            # Labels must not start with a -
            '-abc.',
            'abc.-def.',
            'abc.-def.ghi.',
            # Labels must not end with a -
            'abc-.',
            'abc.def-.',
            'abc.def-.ghi.',
            # Labels must not start or end with a -
            '-abc-.',
            'abc.-def-.',
            'abc.-def-.ghi.',
        ]

        for domainname in valid_domainnames:
            self.assertTrue(format.is_domainname(domainname))

        for domainname in invalid_domainnames:
            self.assertFalse(format.is_domainname(domainname))

    def test_is_email(self):
        valid_emails = [
            'user@example.com',
            'user@emea.example.com',
            'user@example.com',
            'first.last@example.com',
        ]

        invalid_emails = [
            # We use the email addr for the SOA RNAME field, this means the
            # entire address, excluding the @ must be chacracters valid
            # as a DNS name. i.e. + and % addressing is invalid.
            'user+plus@example.com',
            'user%example.org@example.com',
            'example.org',
            '@example.org',
            'user@*.example.org',
            'user',
            'user@',
            'user+plus',
            'user+plus@',
            'user%example.org',
            'user%example.org@',
            'user@example.org.',
            # Exceeds total length limit
            ('user@fghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijk.'
             'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopq.'),
            # Exceeds single lable length limit
            ('user@abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefg'
             'hijkL.'),
            ('user@abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefg'
             'hijk.abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefg'
             'hijkL.'),
            # Exceeds single lable length limit in username part
            ('abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijkL'
             '@example.com.'),
        ]

        for email in valid_emails:
            LOG.debug('Expecting success for: %s' % email)
            self.assertTrue(format.is_email(email))

        for email in invalid_emails:
            LOG.debug('Expecting failure for: %s' % email)
            self.assertFalse(format.is_email(email))

########NEW FILE########
__FILENAME__ = test_api
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Author: Kiall Mac Innes <kiall@hp.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import mock
import testtools
from designate.openstack.common import log as logging
from designate.tests import TestCase
from designate.storage import api as storage_api

LOG = logging.getLogger(__name__)


class SentinelException(Exception):
    pass


class StorageAPITest(TestCase):
    def setUp(self):
        super(StorageAPITest, self).setUp()
        self.storage_api = storage_api.StorageAPI()
        self.storage_mock = mock.Mock()
        self.storage_api.storage = self.storage_mock

    def _set_side_effect(self, method, side_effect):
        methodc = getattr(self.storage_mock, method)
        methodc.side_effect = side_effect

    def _assert_called_with(self, method, *args, **kwargs):
        methodc = getattr(self.storage_mock, method)
        methodc.assert_called_with(*args, **kwargs)

    def _assert_has_calls(self, method, *args, **kwargs):
        methodc = getattr(self.storage_mock, method)
        methodc.assert_has_calls(*args, **kwargs)

    def _assert_call_count(self, method, call_count):
        methodc = getattr(self.storage_mock, method)
        self.assertEqual(methodc.call_count, call_count)

    # Quota Tests
    def test_create_quota(self):
        context = mock.sentinel.context
        values = mock.sentinel.values
        quota = mock.sentinel.quota

        self._set_side_effect('create_quota', [quota])

        with self.storage_api.create_quota(context, values) as q:
            self.assertEqual(quota, q)

        self._assert_called_with('create_quota', context, values)

    def test_create_quota_failure(self):
        context = mock.sentinel.context
        values = mock.sentinel.values

        self._set_side_effect('create_quota', [{'id': 12345}])

        with testtools.ExpectedException(SentinelException):
            with self.storage_api.create_quota(context, values):
                raise SentinelException('Something Went Wrong')

        self._assert_called_with('begin')
        self._assert_called_with('rollback')
        self._assert_called_with('create_quota', context, values)

    def test_get_quota(self):
        context = mock.sentinel.context
        quota_id = mock.sentinel.quota_id
        quota = mock.sentinel.quota

        self._set_side_effect('get_quota', [quota])

        result = self.storage_api.get_quota(context, quota_id)
        self._assert_called_with('get_quota', context, quota_id)
        self.assertEqual(quota, result)

    def test_find_quotas(self):
        context = mock.sentinel.context
        criterion = mock.sentinel.criterion
        marker = mock.sentinel.marker
        limit = mock.sentinel.limit
        sort_key = mock.sentinel.sort_key
        sort_dir = mock.sentinel.sort_dir
        quota = mock.sentinel.quota

        self._set_side_effect('find_quotas', [[quota]])

        result = self.storage_api.find_quotas(
            context, criterion,
            marker, limit, sort_key, sort_dir)
        self._assert_called_with(
            'find_quotas', context, criterion,
            marker, limit, sort_key, sort_dir)
        self.assertEqual([quota], result)

    def test_find_quota(self):
        context = mock.sentinel.context
        criterion = mock.sentinel.criterion
        quota = mock.sentinel.quota

        self._set_side_effect('find_quota', [quota])

        result = self.storage_api.find_quota(context, criterion)
        self._assert_called_with('find_quota', context, criterion)
        self.assertEqual(quota, result)

    def test_update_quota(self):
        context = mock.sentinel.context
        values = mock.sentinel.values

        with self.storage_api.update_quota(context, 123, values):
            pass

        self._assert_called_with('update_quota', context, 123, values)

    def test_update_quota_failure(self):
        context = mock.sentinel.context
        values = {'test': 2}

        self._set_side_effect('get_quota', [{'id': 123, 'test': 1}])

        with testtools.ExpectedException(SentinelException):
            with self.storage_api.update_quota(context, 123, values):
                raise SentinelException('Something Went Wrong')

        self._assert_called_with('begin')
        self._assert_called_with('rollback')
        self._assert_called_with('update_quota', context, 123, values)

    def test_delete_quota(self):
        context = mock.sentinel.context
        quota = mock.sentinel.quota

        self._set_side_effect('get_quota', [quota])

        with self.storage_api.delete_quota(context, 123) as q:
            self.assertEqual(quota, q)

        self._assert_called_with('delete_quota', context, 123)

    def test_delete_quota_failure(self):
        context = mock.sentinel.context
        quota = mock.sentinel.quota

        self._set_side_effect('get_quota', [quota])

        with testtools.ExpectedException(SentinelException):
            with self.storage_api.delete_quota(context, 123):
                raise SentinelException('Something Went Wrong')

        self._assert_called_with('begin')
        self._assert_called_with('rollback')
        self._assert_call_count('delete_quota', 0)

    # Server Tests
    def test_create_server(self):
        context = mock.sentinel.context
        values = mock.sentinel.values
        server = mock.sentinel.server

        self._set_side_effect('create_server', [server])

        with self.storage_api.create_server(context, values) as q:
            self.assertEqual(server, q)

        self._assert_called_with('create_server', context, values)

    def test_create_server_failure(self):
        context = mock.sentinel.context
        values = mock.sentinel.values

        self._set_side_effect('create_server', [{'id': 12345}])

        with testtools.ExpectedException(SentinelException):
            with self.storage_api.create_server(context, values):
                raise SentinelException('Something Went Wrong')

        self._assert_called_with('begin')
        self._assert_called_with('rollback')
        self._assert_called_with('create_server', context, values)

    def test_get_server(self):
        context = mock.sentinel.context
        server_id = mock.sentinel.server_id
        server = mock.sentinel.server

        self._set_side_effect('get_server', [server])

        result = self.storage_api.get_server(context, server_id)
        self._assert_called_with('get_server', context, server_id)
        self.assertEqual(server, result)

    def test_find_servers(self):
        context = mock.sentinel.context
        criterion = mock.sentinel.criterion
        marker = mock.sentinel.marker
        limit = mock.sentinel.limit
        sort_key = mock.sentinel.sort_key
        sort_dir = mock.sentinel.sort_dir

        server = mock.sentinel.server

        self._set_side_effect('find_servers', [[server]])

        result = self.storage_api.find_servers(
            context, criterion,
            marker, limit, sort_key, sort_dir)
        self._assert_called_with(
            'find_servers', context, criterion,
            marker, limit, sort_key, sort_dir)
        self.assertEqual([server], result)

    def test_find_server(self):
        context = mock.sentinel.context
        criterion = mock.sentinel.criterion
        server = mock.sentinel.server

        self._set_side_effect('find_server', [server])

        result = self.storage_api.find_server(context, criterion)
        self._assert_called_with('find_server', context, criterion)
        self.assertEqual(server, result)

    def test_update_server(self):
        context = mock.sentinel.context
        values = mock.sentinel.values

        with self.storage_api.update_server(context, 123, values):
            pass

        self._assert_called_with('update_server', context, 123, values)

    def test_update_server_failure(self):
        context = mock.sentinel.context
        values = {'test': 2}

        self._set_side_effect('get_server', [{'id': 123, 'test': 1}])

        with testtools.ExpectedException(SentinelException):
            with self.storage_api.update_server(context, 123, values):
                raise SentinelException('Something Went Wrong')

        self._assert_called_with('begin')
        self._assert_called_with('rollback')
        self._assert_called_with('update_server', context, 123, values)

    def test_delete_server(self):
        context = mock.sentinel.context
        server = mock.sentinel.server

        self._set_side_effect('get_server', [server])

        with self.storage_api.delete_server(context, 123) as q:
            self.assertEqual(server, q)

        self._assert_called_with('delete_server', context, 123)

    def test_delete_server_failure(self):
        context = mock.sentinel.context
        server = mock.sentinel.server

        self._set_side_effect('get_server', [server])

        with testtools.ExpectedException(SentinelException):
            with self.storage_api.delete_server(context, 123):
                raise SentinelException('Something Went Wrong')

        self._assert_called_with('begin')
        self._assert_called_with('rollback')
        self._assert_call_count('delete_server', 0)

    # Tsigkey Tests
    def test_create_tsigkey(self):
        context = mock.sentinel.context
        values = mock.sentinel.values
        tsigkey = mock.sentinel.tsigkey

        self._set_side_effect('create_tsigkey', [tsigkey])

        with self.storage_api.create_tsigkey(context, values) as q:
            self.assertEqual(tsigkey, q)

        self._assert_called_with('create_tsigkey', context, values)

    def test_create_tsigkey_failure(self):
        context = mock.sentinel.context
        values = mock.sentinel.values

        self._set_side_effect('create_tsigkey', [{'id': 12345}])

        with testtools.ExpectedException(SentinelException):
            with self.storage_api.create_tsigkey(context, values):
                raise SentinelException('Something Went Wrong')

        self._assert_called_with('begin')
        self._assert_called_with('rollback')
        self._assert_called_with('create_tsigkey', context, values)

    def test_get_tsigkey(self):
        context = mock.sentinel.context
        tsigkey_id = mock.sentinel.tsigkey_id
        tsigkey = mock.sentinel.tsigkey

        self._set_side_effect('get_tsigkey', [tsigkey])

        result = self.storage_api.get_tsigkey(context, tsigkey_id)
        self._assert_called_with('get_tsigkey', context, tsigkey_id)
        self.assertEqual(tsigkey, result)

    def test_find_tsigkeys(self):
        context = mock.sentinel.context
        criterion = mock.sentinel.criterion
        marker = mock.sentinel.marker
        limit = mock.sentinel.limit
        sort_key = mock.sentinel.sort_key
        sort_dir = mock.sentinel.sort_dir
        tsigkey = mock.sentinel.tsigkey

        self._set_side_effect('find_tsigkeys', [[tsigkey]])

        result = self.storage_api.find_tsigkeys(
            context, criterion, marker, limit, sort_key, sort_dir)
        self._assert_called_with(
            'find_tsigkeys', context, criterion,
            marker, limit, sort_key, sort_dir)
        self.assertEqual([tsigkey], result)

    def test_find_tsigkey(self):
        context = mock.sentinel.context
        criterion = mock.sentinel.criterion
        tsigkey = mock.sentinel.tsigkey

        self._set_side_effect('find_tsigkey', [tsigkey])

        result = self.storage_api.find_tsigkey(context, criterion)
        self._assert_called_with('find_tsigkey', context, criterion)
        self.assertEqual(tsigkey, result)

    def test_update_tsigkey(self):
        context = mock.sentinel.context
        values = mock.sentinel.values

        with self.storage_api.update_tsigkey(context, 123, values):
            pass

        self._assert_called_with('update_tsigkey', context, 123, values)

    def test_update_tsigkey_failure(self):
        context = mock.sentinel.context
        values = {'test': 2}

        self._set_side_effect('get_tsigkey', [{'id': 123, 'test': 1}])

        with testtools.ExpectedException(SentinelException):
            with self.storage_api.update_tsigkey(context, 123, values):
                raise SentinelException('Something Went Wrong')

        self._assert_called_with('begin')
        self._assert_called_with('rollback')
        self._assert_called_with('update_tsigkey', context, 123, values)

    def test_delete_tsigkey(self):
        context = mock.sentinel.context
        tsigkey = mock.sentinel.tsigkey

        self._set_side_effect('get_tsigkey', [tsigkey])

        with self.storage_api.delete_tsigkey(context, 123) as q:
            self.assertEqual(tsigkey, q)

        self._assert_called_with('delete_tsigkey', context, 123)

    def test_delete_tsigkey_failure(self):
        context = mock.sentinel.context
        tsigkey = mock.sentinel.tsigkey

        self._set_side_effect('get_tsigkey', [tsigkey])

        with testtools.ExpectedException(SentinelException):
            with self.storage_api.delete_tsigkey(context, 123):
                raise SentinelException('Something Went Wrong')

        self._assert_called_with('begin')
        self._assert_called_with('rollback')
        self._assert_call_count('delete_tsigkey', 0)

    # Tenant Tests
    def test_find_tenants(self):
        context = mock.sentinel.context
        tenant = mock.sentinel.tenant

        self._set_side_effect('find_tenants', [[tenant]])

        result = self.storage_api.find_tenants(context)
        self._assert_called_with('find_tenants', context)
        self.assertEqual([tenant], result)

    def test_get_tenant(self):
        context = mock.sentinel.context
        tenant = mock.sentinel.tenant

        self._set_side_effect('get_tenant', [tenant])

        result = self.storage_api.get_tenant(context, 123)
        self._assert_called_with('get_tenant', context, 123)
        self.assertEqual(tenant, result)

    def test_count_tenants(self):
        context = mock.sentinel.context

        self._set_side_effect('count_tenants', [1])

        result = self.storage_api.count_tenants(context)
        self._assert_called_with('count_tenants', context)
        self.assertEqual(1, result)

    # Domain Tests
    def test_create_domain(self):
        context = mock.sentinel.context
        values = mock.sentinel.values
        domain = mock.sentinel.domain

        self._set_side_effect('create_domain', [domain])

        with self.storage_api.create_domain(context, values) as q:
            self.assertEqual(domain, q)

        self._assert_called_with('create_domain', context, values)

    def test_create_domain_failure(self):
        context = mock.sentinel.context
        values = mock.sentinel.values

        self._set_side_effect('create_domain', [{'id': 12345}])

        with testtools.ExpectedException(SentinelException):
            with self.storage_api.create_domain(context, values):
                raise SentinelException('Something Went Wrong')

        self._assert_called_with('begin')
        self._assert_called_with('rollback')
        self._assert_called_with('create_domain', context, values)

    def test_get_domain(self):
        context = mock.sentinel.context
        domain_id = mock.sentinel.domain_id
        domain = mock.sentinel.domain

        self._set_side_effect('get_domain', [domain])

        result = self.storage_api.get_domain(context, domain_id)
        self._assert_called_with('get_domain', context, domain_id)
        self.assertEqual(domain, result)

    def test_find_domains(self):
        context = mock.sentinel.context
        criterion = mock.sentinel.criterion
        marker = mock.sentinel.marker
        limit = mock.sentinel.limit
        sort_key = mock.sentinel.sort_key
        sort_dir = mock.sentinel.sort_dir
        domain = mock.sentinel.domain

        self._set_side_effect('find_domains', [[domain]])

        result = self.storage_api.find_domains(
            context, criterion,
            marker, limit, sort_key, sort_dir)
        self._assert_called_with(
            'find_domains', context, criterion,
            marker, limit, sort_key, sort_dir)
        self.assertEqual([domain], result)

    def test_find_domain(self):
        context = mock.sentinel.context
        criterion = mock.sentinel.criterion
        domain = mock.sentinel.domain

        self._set_side_effect('find_domain', [domain])

        result = self.storage_api.find_domain(context, criterion)
        self._assert_called_with('find_domain', context, criterion)
        self.assertEqual(domain, result)

    def test_update_domain(self):
        context = mock.sentinel.context
        values = mock.sentinel.values

        with self.storage_api.update_domain(context, 123, values):
            pass

        self._assert_called_with('update_domain', context, 123, values)

    def test_update_domain_failure(self):
        context = mock.sentinel.context
        values = {'test': 2}

        self._set_side_effect('get_domain', [{'id': 123, 'test': 1}])

        with testtools.ExpectedException(SentinelException):
            with self.storage_api.update_domain(context, 123, values):
                raise SentinelException('Something Went Wrong')

        self._assert_called_with('begin')
        self._assert_called_with('rollback')
        self._assert_called_with('update_domain', context, 123, values)

    def test_delete_domain(self):
        context = mock.sentinel.context
        domain = mock.sentinel.domain

        self._set_side_effect('get_domain', [domain])

        with self.storage_api.delete_domain(context, 123) as q:
            self.assertEqual(domain, q)

        self._assert_called_with('delete_domain', context, 123)

    def test_delete_domain_failure(self):
        context = mock.sentinel.context
        domain = mock.sentinel.domain

        self._set_side_effect('get_domain', [domain])

        with testtools.ExpectedException(SentinelException):
            with self.storage_api.delete_domain(context, 123):
                raise SentinelException('Something Went Wrong')

        self._assert_called_with('begin')
        self._assert_called_with('rollback')
        self._assert_call_count('delete_domain', 0)

    # RecordSet Tests
    def test_create_recordset(self):
        context = mock.sentinel.context
        values = mock.sentinel.values
        recordset = mock.sentinel.recordset

        self._set_side_effect('create_recordset', [recordset])

        with self.storage_api.create_recordset(context, 123, values) as q:
            self.assertEqual(recordset, q)

        self._assert_called_with('create_recordset', context, 123, values)

    def test_create_recordset_failure(self):
        context = mock.sentinel.context
        values = mock.sentinel.values

        self._set_side_effect('create_recordset', [{'id': 12345}])

        with testtools.ExpectedException(SentinelException):
            with self.storage_api.create_recordset(context, 123, values):
                raise SentinelException('Something Went Wrong')

        self._assert_called_with('begin')
        self._assert_called_with('rollback')
        self._assert_called_with('create_recordset', context, 123, values)

    def test_get_recordset(self):
        context = mock.sentinel.context
        recordset_id = mock.sentinel.recordset_id
        recordset = mock.sentinel.recordset

        self._set_side_effect('get_recordset', [recordset])

        result = self.storage_api.get_recordset(context, recordset_id)
        self._assert_called_with('get_recordset', context, recordset_id)
        self.assertEqual(recordset, result)

    def test_find_recordsets(self):
        context = mock.sentinel.context
        criterion = mock.sentinel.criterion
        marker = mock.sentinel.marker
        limit = mock.sentinel.limit
        sort_key = mock.sentinel.sort_key
        sort_dir = mock.sentinel.sort_dir
        recordset = mock.sentinel.recordset

        self._set_side_effect('find_recordsets', [[recordset]])

        result = self.storage_api.find_recordsets(
            context, criterion,
            marker, limit, sort_key, sort_dir)
        self._assert_called_with(
            'find_recordsets', context, criterion,
            marker, limit, sort_key, sort_dir)
        self.assertEqual([recordset], result)

    def test_find_recordset(self):
        context = mock.sentinel.context
        criterion = mock.sentinel.criterion
        recordset = mock.sentinel.recordset

        self._set_side_effect('find_recordset', [recordset])

        result = self.storage_api.find_recordset(context, criterion)
        self._assert_called_with('find_recordset', context, criterion)
        self.assertEqual(recordset, result)

    def test_update_recordset(self):
        context = mock.sentinel.context
        values = mock.sentinel.values

        with self.storage_api.update_recordset(context, 123, values):
            pass

        self._assert_called_with('update_recordset', context, 123, values)

    def test_update_recordset_failure(self):
        context = mock.sentinel.context
        values = {'test': 2}

        self._set_side_effect('get_recordset', [{'id': 123, 'test': 1}])

        with testtools.ExpectedException(SentinelException):
            with self.storage_api.update_recordset(context, 123, values):
                raise SentinelException('Something Went Wrong')

        self._assert_called_with('begin')
        self._assert_called_with('rollback')
        self._assert_called_with('update_recordset', context, 123, values)

    def test_delete_recordset(self):
        context = mock.sentinel.context
        recordset = mock.sentinel.recordset

        self._set_side_effect('get_recordset', [recordset])

        with self.storage_api.delete_recordset(context, 123) as q:
            self.assertEqual(recordset, q)

        self._assert_called_with('delete_recordset', context, 123)

    def test_delete_recordset_failure(self):
        context = mock.sentinel.context
        recordset = mock.sentinel.recordset

        self._set_side_effect('get_recordset', [recordset])

        with testtools.ExpectedException(SentinelException):
            with self.storage_api.delete_recordset(context, 123):
                raise SentinelException('Something Went Wrong')

        self._assert_called_with('begin')
        self._assert_called_with('rollback')
        self._assert_call_count('delete_recordset', 0)

    # Record Tests
    def test_create_record(self):
        context = mock.sentinel.context
        values = mock.sentinel.values
        record = mock.sentinel.record

        self._set_side_effect('create_record', [record])

        with self.storage_api.create_record(context, 123, 321, values) as q:
            self.assertEqual(record, q)

        self._assert_called_with('create_record', context, 123, 321, values)

    def test_create_record_failure(self):
        context = mock.sentinel.context
        values = mock.sentinel.values

        self._set_side_effect('create_record', [{'id': 12345}])

        with testtools.ExpectedException(SentinelException):
            with self.storage_api.create_record(context, 123, 321, values):
                raise SentinelException('Something Went Wrong')

        self._assert_called_with('begin')
        self._assert_called_with('rollback')
        self._assert_called_with('create_record', context, 123, 321, values)

    def test_get_record(self):
        context = mock.sentinel.context
        record_id = mock.sentinel.record_id
        record = mock.sentinel.record

        self._set_side_effect('get_record', [record])

        result = self.storage_api.get_record(context, record_id)
        self._assert_called_with('get_record', context, record_id)
        self.assertEqual(record, result)

    def test_find_records(self):
        context = mock.sentinel.context
        criterion = mock.sentinel.criterion
        marker = mock.sentinel.marker
        limit = mock.sentinel.limit
        sort_key = mock.sentinel.sort_key
        sort_dir = mock.sentinel.sort_dir
        record = mock.sentinel.record

        self._set_side_effect('find_records', [[record]])

        result = self.storage_api.find_records(
            context, criterion,
            marker, limit, sort_key, sort_dir)
        self._assert_called_with(
            'find_records', context, criterion,
            marker, limit, sort_key, sort_dir)

        self.assertEqual([record], result)

    def test_find_record(self):
        context = mock.sentinel.context
        criterion = mock.sentinel.criterion
        record = mock.sentinel.record

        self._set_side_effect('find_record', [record])

        result = self.storage_api.find_record(context, criterion)
        self._assert_called_with('find_record', context, criterion)

        self.assertEqual(record, result)

    def test_update_record(self):
        context = mock.sentinel.context
        record_id = mock.sentinel.record_id
        values = mock.sentinel.values

        with self.storage_api.update_record(context, record_id, values):
            pass

        self._assert_called_with('update_record', context, record_id, values)

    def test_update_record_failure(self):
        context = mock.sentinel.context
        record_id = mock.sentinel.record_id
        values = {'test': 2}

        self._set_side_effect('get_record', [{'id': record_id, 'test': 1}])

        with testtools.ExpectedException(SentinelException):
            with self.storage_api.update_record(context, record_id, values):
                raise SentinelException('Something Went Wrong')

        self._assert_called_with('begin')
        self._assert_called_with('rollback')
        self._assert_called_with('update_record', context, record_id, values)

    def test_delete_record(self):
        context = mock.sentinel.context
        record = mock.sentinel.record

        self._set_side_effect('get_record', [record])

        with self.storage_api.delete_record(context, 123) as q:
            self.assertEqual(record, q)

        self._assert_called_with('delete_record', context, 123)

    def test_delete_record_failure(self):
        context = mock.sentinel.context
        record = mock.sentinel.record

        self._set_side_effect('get_record', [record])

        with testtools.ExpectedException(SentinelException):
            with self.storage_api.delete_record(context, 123):
                raise SentinelException('Something Went Wrong')

        self._assert_called_with('begin')
        self._assert_called_with('rollback')
        self._assert_call_count('delete_record', 0)

########NEW FILE########
__FILENAME__ = test_sqlalchemy
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.openstack.common import log as logging
from designate import storage
from designate.tests import TestCase
from designate.tests.test_storage import StorageTestCase

LOG = logging.getLogger(__name__)


class SqlalchemyStorageTest(StorageTestCase, TestCase):
    def setUp(self):
        super(SqlalchemyStorageTest, self).setUp()

        self.config(
            storage_driver='sqlalchemy',
            group='service:central'
        )

        self.storage = storage.get_storage()

########NEW FILE########
__FILENAME__ = test_utils
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import os
import tempfile
import testtools
from jinja2 import Template
from designate.tests import TestCase
from designate import exceptions
from designate import utils


class TestUtils(TestCase):
    def test_resource_string(self):
        name = ['templates', 'bind9-config.jinja2']

        resource_string = utils.resource_string(*name)

        self.assertIsNotNone(resource_string)

    def test_resource_string_missing(self):
        name = 'invalid.jinja2'

        with testtools.ExpectedException(exceptions.ResourceNotFound):
            utils.resource_string(name)

    def test_load_schema(self):
        schema = utils.load_schema('v1', 'domain')

        self.assertIsInstance(schema, dict)

    def test_load_schema_missing(self):
        with testtools.ExpectedException(exceptions.ResourceNotFound):
            utils.load_schema('v1', 'missing')

    def test_load_template(self):
        name = 'bind9-config.jinja2'

        template = utils.load_template(name)

        self.assertIsInstance(template, Template)

    def test_load_template_missing(self):
        name = 'invalid.jinja2'

        with testtools.ExpectedException(exceptions.ResourceNotFound):
            utils.load_template(name)

    def test_render_template(self):
        template = Template("Hello {{name}}")

        result = utils.render_template(template, name="World")

        self.assertEqual('Hello World', result)

    def test_render_template_to_file(self):
        output_path = tempfile.mktemp()

        template = Template("Hello {{name}}")

        utils.render_template_to_file(template, output_path=output_path,
                                      name="World")

        self.assertTrue(os.path.exists(output_path))

        try:
            with open(output_path, 'r') as fh:
                self.assertEqual('Hello World', fh.read())
        finally:
            os.unlink(output_path)

########NEW FILE########
__FILENAME__ = utils
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import copy
import json
import functools
import inspect
import os
import pkg_resources
import uuid

from jinja2 import Template
from oslo.config import cfg

from designate import exceptions
from designate.openstack.common import log as logging
from designate.openstack.common import processutils
from designate.openstack.common import timeutils

LOG = logging.getLogger(__name__)


cfg.CONF.register_opts([
    cfg.StrOpt('root-helper',
               default='sudo designate-rootwrap /etc/designate/rootwrap.conf')
])


def find_config(config_path):
    """
    Find a configuration file using the given hint.

    Code nabbed from cinder.

    :param config_path: Full or relative path to the config.
    :returns: List of config paths
    """
    possible_locations = [
        config_path,
        os.path.join(cfg.CONF.pybasedir, "etc", "designate", config_path),
        os.path.join(cfg.CONF.pybasedir, "etc", config_path),
        os.path.join(cfg.CONF.pybasedir, config_path),
        "/etc/designate/%s" % config_path,
    ]

    found_locations = []

    for path in possible_locations:
        LOG.debug('Searching for configuration at path: %s' % path)
        if os.path.exists(path):
            LOG.debug('Found configuration at path: %s' % path)
            found_locations.append(os.path.abspath(path))

    return found_locations


def read_config(prog, argv):
    config_files = find_config('%s.conf' % prog)

    cfg.CONF(argv[1:], project='designate', prog=prog,
             default_config_files=config_files)


def resource_string(*args):
    if len(args) == 0:
        raise ValueError()

    resource_path = os.path.join('resources', *args)

    if not pkg_resources.resource_exists('designate', resource_path):
        raise exceptions.ResourceNotFound('Could not find the requested '
                                          'resource: %s' % resource_path)

    return pkg_resources.resource_string('designate', resource_path)


def load_schema(version, name):
    schema_string = resource_string('schemas', version, '%s.json' % name)

    return json.loads(schema_string)


def load_template(template_name):
    template_string = resource_string('templates', template_name)

    return Template(template_string)


def render_template(template, **template_context):
    if not isinstance(template, Template):
        template = load_template(template)

    return template.render(**template_context)


def render_template_to_file(template_name, output_path, makedirs=True,
                            **template_context):
    output_folder = os.path.dirname(output_path)

    # Create the output folder tree if necessary
    if makedirs and not os.path.exists(output_folder):
        os.makedirs(output_folder)

    # Render the template
    content = render_template(template_name, **template_context)

    with open(output_path, 'w') as output_fh:
        output_fh.write(content)


def execute(*cmd, **kw):
    root_helper = kw.pop('root_helper', cfg.CONF.root_helper)
    run_as_root = kw.pop('run_as_root', True)
    return processutils.execute(*cmd, run_as_root=run_as_root,
                                root_helper=root_helper, **kw)


def get_item_properties(item, fields, mixed_case_fields=[], formatters={}):
    """Return a tuple containing the item properties.

    :param item: a single item resource (e.g. Server, Tenant, etc)
    :param fields: tuple of strings with the desired field names
    :param mixed_case_fields: tuple of field names to preserve case
    :param formatters: dictionary mapping field names to callables
        to format the values
    """
    row = []

    for field in fields:
        if field in formatters:
            row.append(formatters[field](item))
        else:
            if field in mixed_case_fields:
                field_name = field.replace(' ', '_')
            else:
                field_name = field.lower().replace(' ', '_')
            if not hasattr(item, field_name) and \
                    (isinstance(item, dict) and field_name in item):
                data = item[field_name]
            else:
                data = getattr(item, field_name, '')
            if data is None:
                data = ''
            row.append(data)
    return tuple(row)


def get_columns(data):
    """
    Some row's might have variable count of columns, ensure that we have the
    same.

    :param data: Results in [{}, {]}]
    """
    columns = set()

    def _seen(col):
        columns.add(str(col))

    map(lambda item: map(_seen, item.keys()), data)
    return list(columns)


def increment_serial(serial=0):
    # This provides for *roughly* unix timestamp based serial numbers
    new_serial = timeutils.utcnow_ts()

    if new_serial <= serial:
        new_serial = serial + 1

    return new_serial


def quote_string(string):
    inparts = string.split(' ')
    outparts = []
    tmp = None

    for part in inparts:
        if part == '':
            continue
        elif part[0] == '"' and part[-1:] == '"' and part[-2:] != '\\"':
            # Handle Quoted Words
            outparts.append(part.strip('"'))
        elif part[0] == '"':
            # Handle Start of Quoted Sentance
            tmp = part[1:]
        elif tmp is not None and part[-1:] == '"' and part[-2:] != '\\"':
            # Handle End of Quoted Sentance
            tmp += " " + part.strip('"')
            outparts.append(tmp)
            tmp = None
        elif tmp is not None:
            # Handle Middle of Quoted Sentance
            tmp += " " + part
        else:
            # Handle Standalone words
            outparts.append(part)

    if tmp is not None:
        # Handle unclosed quoted strings
        outparts.append(tmp)

    # This looks odd, but both calls are necessary to ensure the end results
    # is always consistent.
    outparts = [o.replace('\\"', '"') for o in outparts]
    outparts = [o.replace('"', '\\"') for o in outparts]

    return '"' + '" "'.join(outparts) + '"'


def deep_dict_merge(a, b):
    if not isinstance(b, dict):
        return b

    result = copy.deepcopy(a)

    for k, v in b.iteritems():
        if k in result and isinstance(result[k], dict):
                result[k] = deep_dict_merge(result[k], v)
        else:
            result[k] = copy.deepcopy(v)

    return result


def generate_uuid():
    return str(uuid.uuid4())


def is_uuid_like(val):
    """Returns validation of a value as a UUID.

    For our purposes, a UUID is a canonical form string:
    aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa

    """
    try:
        return str(uuid.UUID(val)) == val
    except (TypeError, ValueError, AttributeError):
        return False


def validate_uuid(*check):
    """
    A wrapper to ensure that API controller methods arguments are valid UUID's.

    Usage:
    @validate_uuid('zone_id')
    def get_all(self, zone_id):
        return {}
    """
    def inner(f):
        def wrapper(*args, **kwargs):
            arg_spec = inspect.getargspec(f).args

            # Ensure that we have the exact number of parameters that the
            # function expects.  This handles URLs like
            # /v2/zones/<UUID - valid or invalid>/invalid
            # get, patch and delete return a 404, but Pecan returns a 405
            # for a POST at the same URL
            if (len(arg_spec) != len(args)):
                raise exceptions.NotFound()

            # Ensure that we have non-empty parameters in the cases where we
            # have sub controllers - i.e. controllers at the 2nd level
            # This is for URLs like /v2/zones/nameservers
            # Ideally Pecan should be handling these cases, but until then
            # we handle those cases here.
            if (len(args) <= len(check)):
                raise exceptions.NotFound()

            for name in check:
                pos = arg_spec.index(name)
                if not is_uuid_like(args[pos]):
                    msg = 'Invalid UUID %s: %s' % (name, args[pos])
                    raise exceptions.InvalidUUID(msg)
            return f(*args, **kwargs)
        return functools.wraps(f)(wrapper)
    return inner

########NEW FILE########
__FILENAME__ = version
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import pbr.version

version_info = pbr.version.VersionInfo('designate')

########NEW FILE########
__FILENAME__ = wsgi
# Copyright 2012 Managed I.T.
#
# Author: Kiall Mac Innes <kiall@managedit.ie>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from designate.openstack.deprecated import wsgi


class Middleware(wsgi.Middleware):
    @classmethod
    def factory(cls, global_config, **local_conf):
        """ Used for paste app factories in paste.deploy config files """

        def _factory(app):
            return cls(app, **local_conf)

        return _factory

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# designate documentation build configuration file, created by
# sphinx-quickstart on Wed Oct 31 18:58:17 2012.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.viewcode', 'sphinxcontrib.httpdomain']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'designate'
copyright = u'2012, Managed I.T.'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
from designate.version import version_info as designate_version
version = designate_version.canonical_version_string()
# The full version, including alpha/beta/rc tags.
release = designate_version.version_string_with_vcs()

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = []

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'designatedoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'designate.tex', u'Designate Documentation',
   u'Managed I.T.', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
#    ('index', 'designate', u'Designate Documentation',
#     [u'Managed I.T.'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'designate', u'Designate Documentation',
   u'Managed I.T.', 'designate', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

########NEW FILE########
__FILENAME__ = install_venv
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
# Copyright 2010 OpenStack Foundation
# Copyright 2013 IBM Corp.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os
import sys

import install_venv_common as install_venv  # noqa


def print_help(venv, root):
    help = """
    Designate development environment setup is complete.

    Designate development uses virtualenv to track and manage Python
    dependencies while in development and testing.

    To activate the Designate virtualenv for the extent of your current shell
    session you can run:

    $ source %s/bin/activate

    Or, if you prefer, you can run commands in the virtualenv on a case by case
    basis by running:

    $ %s/tools/with_venv.sh <your command>
    """
    print(help % (venv, root))


def main(argv):
    root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))

    if os.environ.get('tools_path'):
        root = os.environ['tools_path']
    venv = os.path.join(root, '.venv')
    if os.environ.get('venv'):
        venv = os.environ['venv']

    pip_requires = os.path.join(root, 'requirements.txt')
    test_requires = os.path.join(root, 'test-requirements.txt')
    py_version = "python%s.%s" % (sys.version_info[0], sys.version_info[1])
    project = 'Designate'
    install = install_venv.InstallVenv(root, venv, pip_requires, test_requires,
                                       py_version, project)
    options = install.parse_args(argv)
    install.check_python_version()
    install.check_dependencies()
    install.create_virtualenv(no_site_packages=options.no_site_packages)
    install.install_dependencies()
    print_help(venv, root)

if __name__ == '__main__':
    main(sys.argv)

########NEW FILE########
__FILENAME__ = install_venv_common
# Copyright 2013 OpenStack Foundation
# Copyright 2013 IBM Corp.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Provides methods needed by installation script for OpenStack development
virtual environments.

Since this script is used to bootstrap a virtualenv from the system's Python
environment, it should be kept strictly compatible with Python 2.6.

Synced in from openstack-common
"""

from __future__ import print_function

import optparse
import os
import subprocess
import sys


class InstallVenv(object):

    def __init__(self, root, venv, requirements,
                 test_requirements, py_version,
                 project):
        self.root = root
        self.venv = venv
        self.requirements = requirements
        self.test_requirements = test_requirements
        self.py_version = py_version
        self.project = project

    def die(self, message, *args):
        print(message % args, file=sys.stderr)
        sys.exit(1)

    def check_python_version(self):
        if sys.version_info < (2, 6):
            self.die("Need Python Version >= 2.6")

    def run_command_with_code(self, cmd, redirect_output=True,
                              check_exit_code=True):
        """Runs a command in an out-of-process shell.

        Returns the output of that command. Working directory is self.root.
        """
        if redirect_output:
            stdout = subprocess.PIPE
        else:
            stdout = None

        proc = subprocess.Popen(cmd, cwd=self.root, stdout=stdout)
        output = proc.communicate()[0]
        if check_exit_code and proc.returncode != 0:
            self.die('Command "%s" failed.\n%s', ' '.join(cmd), output)
        return (output, proc.returncode)

    def run_command(self, cmd, redirect_output=True, check_exit_code=True):
        return self.run_command_with_code(cmd, redirect_output,
                                          check_exit_code)[0]

    def get_distro(self):
        if (os.path.exists('/etc/fedora-release') or
                os.path.exists('/etc/redhat-release')):
            return Fedora(
                self.root, self.venv, self.requirements,
                self.test_requirements, self.py_version, self.project)
        else:
            return Distro(
                self.root, self.venv, self.requirements,
                self.test_requirements, self.py_version, self.project)

    def check_dependencies(self):
        self.get_distro().install_virtualenv()

    def create_virtualenv(self, no_site_packages=True):
        """Creates the virtual environment and installs PIP.

        Creates the virtual environment and installs PIP only into the
        virtual environment.
        """
        if not os.path.isdir(self.venv):
            print('Creating venv...', end=' ')
            if no_site_packages:
                self.run_command(['virtualenv', '-q', '--no-site-packages',
                                 self.venv])
            else:
                self.run_command(['virtualenv', '-q', self.venv])
            print('done.')
        else:
            print("venv already exists...")
            pass

    def pip_install(self, *args):
        self.run_command(['tools/with_venv.sh',
                         'pip', 'install', '--upgrade'] + list(args),
                         redirect_output=False)

    def install_dependencies(self):
        print('Installing dependencies with pip (this can take a while)...')

        # First things first, make sure our venv has the latest pip and
        # setuptools and pbr
        self.pip_install('pip>=1.4')
        self.pip_install('setuptools')
        self.pip_install('pbr')

        self.pip_install('-r', self.requirements, '-r', self.test_requirements)

    def parse_args(self, argv):
        """Parses command-line arguments."""
        parser = optparse.OptionParser()
        parser.add_option('-n', '--no-site-packages',
                          action='store_true',
                          help="Do not inherit packages from global Python "
                               "install")
        return parser.parse_args(argv[1:])[0]


class Distro(InstallVenv):

    def check_cmd(self, cmd):
        return bool(self.run_command(['which', cmd],
                    check_exit_code=False).strip())

    def install_virtualenv(self):
        if self.check_cmd('virtualenv'):
            return

        if self.check_cmd('easy_install'):
            print('Installing virtualenv via easy_install...', end=' ')
            if self.run_command(['easy_install', 'virtualenv']):
                print('Succeeded')
                return
            else:
                print('Failed')

        self.die('ERROR: virtualenv not found.\n\n%s development'
                 ' requires virtualenv, please install it using your'
                 ' favorite package management tool' % self.project)


class Fedora(Distro):
    """This covers all Fedora-based distributions.

    Includes: Fedora, RHEL, CentOS, Scientific Linux
    """

    def check_pkg(self, pkg):
        return self.run_command_with_code(['rpm', '-q', pkg],
                                          check_exit_code=False)[1] == 0

    def install_virtualenv(self):
        if self.check_cmd('virtualenv'):
            return

        if not self.check_pkg('python-virtualenv'):
            self.die("Please install 'python-virtualenv'.")

        super(Fedora, self).install_virtualenv()

########NEW FILE########
