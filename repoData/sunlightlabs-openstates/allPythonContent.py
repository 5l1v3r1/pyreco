__FILENAME__ = billy_settings
import os

from os.path import abspath, dirname, join

SCRAPER_PATHS=[os.path.join(os.getcwd(), 'openstates')]
MONGO_HOST = 'localhost'
MONGO_PORT = 27017
MONGO_DATABASE = 'fiftystates'

BILLY_MANUAL_DATA_DIR = os.environ.get('BILLY_MANUAL_DATA_DIR')
if BILLY_MANUAL_DATA_DIR is None:
    BILLY_MANUAL_DATA_DIR = os.path.join(
        os.path.abspath(os.path.dirname(__file__)),
        "manual_data",
    )

LEGISLATOR_FILTERS = {
    "billy.importers.filters.single_space_filter": [
        "full_name",
        "first_name",
        "last_name",
        "middle_name",
    ],
    "billy.importers.filters.phone_filter": [
        "office_phone",
        "phone",
        "offices.phone",
        "offices.fax",
    ],
    "billy.importers.filters.email_filter": [
        "offices.email",
    ],
}

BILL_FILTERS = {
    "billy.importers.filters.single_space_filter": [
        "actions.action",
        "title",
    ]
}

EVENT_FILTERS = {
    "billy.importers.filters.single_space_filter": [
        "description",
        "participants.participant",
        "related_bills.bill_id",
        "related_bills.description",
    ]
}


try:
    from billy_local import *
except ImportError:
    pass

########NEW FILE########
__FILENAME__ = actions
import re
from collections import defaultdict

from billy.models import db
import logbook


logger = logbook.Logger('bathawk.actions')


class Actions(object):
    '''From this object, you can reference:
      * unmatched
      * matched
      * actions_list
    '''

    def __init__(self, abbr):
        self.abbr = abbr
        self._build()

    def _build(self):

        self.patterns = []

        # Get lists of un/matched actions.
        actions_list = filter(None, self._get_list())
        actions = set(actions_list)
        matched = set()
        unmatched = set(actions)
        for pattern in self.patterns:
            matched_pattern = set(filter(pattern.search, unmatched))
            matched |= matched_pattern
            unmatched -= matched

        self.unmatched = unmatched
        self.matched = matched
        self.list = actions_list

    def _get_list(self, only_other=False):
        '''Yield actions currently categorized as 'other'.
        '''
        meta = db.metadata.find_one(self.abbr)
        session = meta['terms'][-1]['sessions'][-1]
        msg = 'Retrieving actions for %r session %r'
        logger.info(msg % (self.abbr, session))
        action_ids = defaultdict(list)
        for bill in db.bills.find({'state': self.abbr,
                                   #'session': session
                                   },
                                  fields=['actions']):
            for action in bill['actions']:
                if only_other and action['type'] != ['other']:
                    continue
                action_text = action['action']
                action_ids[action_text].append(bill['_id'])
                yield action_text
        self.action_ids = action_ids


########NEW FILE########
__FILENAME__ = bathawk
# -*- coding: utf-8 -*-
import re
import pydoc
import types
import copy
import pprint
import itertools
import sre_constants
import webbrowser
import subprocess
import functools
import collections
import pprint
import datetime
import importlib
from code import InteractiveConsole
from operator import itemgetter
from os.path import abspath, dirname, join

import logbook
from clint.textui import puts, indent
from clint.textui.colored import red, green, cyan, magenta, yellow

from pymongo import Connection

from categories import categories
from billy.models import db

import batshell
from batshell import command, xsel_enabled

logger = logbook.Logger('bathawk.batshell')
HERE = dirname(abspath(__file__))


class BathawkShell(batshell.Shell):
    '''Beware!
    '''
    ps1 = '(batshell) '
    banner = yellow('''
      _==/           i     i           \==_
     /XX/            |\___/|            \XX\\
   /XXXX\            |XXXXX|            /XXXX\\
  |XXXXXX\_         _XXXXXXX_         _/XXXXXX|
 XXXXXXXXXXXxxxxxxxXXXXXXXXXXXxxxxxxxXXXXXXXXXXX
|XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX|
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
|XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX|
 XXXXXX/^^^^"\XXXXXXXXXXXXXXXXXXXXX/^^^^^\XXXXXX
  |XXX|       \XXX/^^\XXXXX/^^\XXX/       |XXX|
    \XX\       \X/    \XXX/    \X/       /XX/
       "\       "      \X/      "       /"
''')
    banner += cyan('\n\nWelcome to bashell. '
                       'Type h for a list of commands.')

    def __init__(self, actions_object, abbr, *args, **kwargs):

        # Initialize a batshell with our bat commands.
        batshell.Shell.__init__(self, commands=BatCommands(actions_object))

        # The stuff below might be better placed on the commands object.
        # Store abbreviations and this state's actions helper object in the shell.
        self.abbr = abbr
        self.actions = actions_object

        # Store the state's categorizer in the shell.
        try:
            actions_module = importlib.import_module('openstates.%s.actions' % abbr)
            categorizer = getattr(actions_module, 'Categorizer', None)
            if categorizer is not None:
                self.locals['categorizer'] = categorizer()
        except ImportError:
            pass


class GameState(dict):
    '''Keep track of the state of the matching game. This is only in its
    own class for organizational reasons.
    '''

    def __init__(self, actions_unmatched):

        self.db = Connection().bathawk

        self['created'] = datetime.datetime.now()

        self['current_rgx'] = None

        # All the regexes added during this game.
        self['regexes'] = set()

        # Mapping of regexes to matching types.
        self['types'] = collections.defaultdict(set)

        # Keep track of pattens tested so far.
        self['tested_regexes'] = set()

        # Keep track of how many actions each regex matched.
        self['match_counts'] = {}

        # The master actions list for this state.
        self._actions = actions_unmatched

    def save(self):
        data = copy.copy(self)
        data['tested_regexes'] = list(self['tested_regexes'])
        data['regexes'] = list(self['regexes'])
        data['types'] = [(k, list(v)) for (k, v) in self['types'].items()]
        data['match_counts'] = self['match_counts'].items()
        self.db.games.save(data)

    def load(self, game):
        self['_id'] = game['_id']
        self['current_rgx'] = game['current_rgx']
        self['tested_regexes'] = set(game['tested_regexes'])
        self['regexes'] = set(game['regexes'])

        # My 2.6 linter doesn't doesn't like dict comprehensions...
        self['match_counts'] = {
            rgx: len(filter(re.compile(rgx).search, self._actions))
            for rgx in self['regexes']
            }
        types = collections.defaultdict(set)
        for k, v in game['types']:
            types[k] = set(v)
        self['types'] = types

    def reset(self):
        return GameState(self._actions)

    def matched_actions(self):
        sre_type = type(re.compile(r''))
        ret = collections.defaultdict(list)
        for action in self._actions:
            for rgx in self['regexes']:
                if isinstance(rgx, sre_type):
                    m = rgx.search(action)
                else:
                    m = re.search(rgx, action)
                if m:
                    ret[rgx].append((action, m))
        return ret

    def unmatched_actions(self):
        sre_type = type(re.compile(r''))
        ret = []
        rgxs = self['regexes']
        for action in self._actions:
            matches = []
            for rgx in self['regexes']:
                if isinstance(rgx, sre_type):
                    m = rgx.search(action)
                else:
                    m = re.search(rgx, action)
                if m:
                    matches.append(m)
            if not matches:
                ret.append(action)
        return ret


class BatCommands(batshell.ShellCommands):
    '''The commands accessible from the Bathawk shell.
    '''

    def __init__(self, actions):
        batshell.ShellCommands.__init__(self)
        self.actions = actions
        self._test_list = actions.unmatched

        # How many lines to show.
        self.show = 30
        self.matched = []
        self.show_matches_start = 0
        self.show_actions_start = 0

        self.game_state = GameState(actions.unmatched)

    @command('r', lex_args=False)
    def test_regex(self, argtext):
        '''Test a regex to see how many actions match.
        '''
        try:
            rgx = re.compile(argtext)
        except sre_constants.error as e:
            msg = red('Bad regex: ') + green(repr(argtext)) + ' You have failed the bat-test.'
            puts(msg)
            print e
            return

        puts('Testing ' + green(argtext))
        matched = []
        for action in self._test_list:
            m = re.search(argtext, action)
            if m:
                matched.append([action, m])
        if not matched:
            with indent(4, quote=' >'):
                puts(red('Aw, snap!') + ' ' + cyan('No matches found!'))
                return
        self.show_matches_start = 0

        total_matches = len(filter(rgx.search, self.actions.list))

        # Update game state.
        self.game_state['current_rgx'] = argtext
        self.game_state['tested_regexes'].add(argtext)
        self.game_state['match_counts'][argtext] = total_matches

        with indent(4, quote=' >'):
            puts('Found ' + red(total_matches) + ' matches:')
        self._print_matches(matched[:self.show])
        self.matched = matched

        # Copy the pattern to the clipboard.
        if xsel_enabled:
            p = subprocess.Popen(['xsel', '-bi'], stdin=subprocess.PIPE)
            p.communicate(input=argtext)

    @command('sw')
    def switch_test_list(self, target):
        '''Switch the regex tester to test against matched,
        unmatched, or all ('list') actions.
        '''
        if not hasattr(self.actions, target):
            logger.warning("Failure! The argument should be 'matched', "
                           "'unmatched', or 'list' for all actions.")
            return
        self._test_list = getattr(self.actions, target)
        logger.info('Switched regex tester over to %r.' % target)

    def _print_matches(self, matched):
        actions = map(itemgetter(0), matched)
        padding = max(map(len, actions))
        self.printed_matched = matched
        for i, (action, match) in enumerate(matched):
            group = match.group()
            colored_action = action.replace(group, str(green(group)))

            groupdict = match.groupdict()
            vals = [str(cyan(i)).ljust(5),
                    '[%s]' % magenta(self.actions.action_ids[action][-1]),
                    colored_action.ljust(padding),
                    repr(groupdict)]
            puts(' '.join(vals))

    @command('m')
    def show_20_matches(self, line):
        '''Show first 20 matches.
        '''
        search = functools.partial(re.search, self.game_state['current_rgx'])
        text = '\n'.join(filter(search, self.actions.list))
        self.pager(text.encode('utf-8'))

    @command('#')
    def show(self, line):
        '''How many matches or actions to show at a time.
        '''
        number = int(line)
        self.show = number

    @command('a')
    def show_actions(self, line=None):
        '''List the first 10 actions.
        '''
        text = '\n'.join(self.actions.unmatched)
        self.pager(text.encode('utf-8'))

    @command('as')
    def show_actions_sorted(self, line=None):
        '''Show actions in alphabetical order.
        '''
        self.show_actions_start = 0
        text = '\n'.join(sorted(list(self.actions.unmatched)))
        self.pager(text.encode('utf-8'))

    @command('c')
    def categories(self):
        '''Print available openstates action categories.
        '''
        padding = max(map(len, map(itemgetter(0), categories)))
        with indent(4, quote=' >'):
            for i, (category, description) in enumerate(categories):
                i = str(yellow(i)).rjust(5)
                category = green(category.ljust(padding))
                puts('(%s) %s %s' % (i, category, description))

    @command('o')
    def hyperlink(self, index):
        '''Given a number representing an index in the
        most recent display of matches, print a hyperlink
        to the bill on localhost.
        '''
        index = int(index)
        action, groupdict = self.printed_matched[index]
        _id = self.actions.action_ids[action][-1]
        bill = db.bills.find_one({'_all_ids': _id})
        url = 'http:localhost:8000/{state}/bills/{session}/{bill_id}/'.format(**bill)
        colored_url = cyan(url)
        puts('View this bill: ' + colored_url)
        webbrowser.open(url)

    @command('j')
    def bill_json(self, index):
        '''Pretty print the bill's actions json.
        '''
        index = int(index)
        action, groupdict = self.printed_matched[index]
        _id = self.actions.action_ids[action][-1]
        bill = db.bills.find_one({'_all_ids': _id})
        url = 'http:localhost:8000/{state}/bills/{session}/{bill_id}/'.format(**bill)
        colored_url = cyan(url)
        puts('View this bill: ' + colored_url)
        pprint.pprint(bill['actions'])

    # -----------------------------------------------------------------------
    # Game commands.
    @command('g')
    def start_game(self):
        '''Resume a saved game.'''
        games = list(self.game_state.db.games.find())
        if games:
            tmpl = '%s: created %s, # regexes: %s'
            with indent(4):
                for i, game in enumerate(games):
                    created = game['created'].strftime('%m/%d/%Y')
                    puts(tmpl % (str(yellow(i)).rjust(5),
                                  green(created),
                                  cyan(str(len(game['regexes'])))
                                  ))
            msg = 'Enter the game number to resume (empty for new game): '
            index = raw_input(msg)
            if not index:
                self.game_state = self.game_state.reset()
                puts(yellow('New game.'))
            else:
                game = games[int(index)]
                self.game_state.load(game)
                msg = 'Resumed saved game %r!'
                puts(yellow(msg % game['created'].strftime('%m/%d/%Y')))
        else:
            puts(yellow('No saved games found. New game.'))

    @command('pg')
    def purge_games(self):
        '''Purge one or more saved games.'''
        games = list(self.game_state.db.games.find())
        if games:
            tmpl = '%s: created %s, # regexes: %s'
            with indent(4):
                for i, game in enumerate(games):
                    created = game['created'].strftime('%m/%d/%Y')
                    puts(tmpl % (str(yellow(i)).rjust(5),
                                  green(created),
                                  cyan(str(len(game['regexes'])))
                                  ))
            msg = 'Enter the game number to resume (empty for new game): '
            indexes = map(int, raw_input(msg).split())
            for i in indexes:
                game = games[i]
                self.game_state.db.games.remove(game)
                created = game['created'].strftime('%m/%d/%Y')
                logger.info('removed game %s' % created)
        else:
            puts(colore.red('No games to purge.'))

    @command('rt')
    def show_tested_regexes(self):
        '''Show tested patterns.
        '''
        game_state = self.game_state
        tmpl = '%s: (%s) %s'
        regexes = sorted(game_state['tested_regexes'], key=hash)
        sre_type = type(re.compile(r''))
        for i, rgx in enumerate(regexes):
            if isinstance(rgx, sre_type):
                rgx = '%s  ... flags=%d' % (rgx.pattern, rgx.flags)
            match_count = game_state['match_counts'].get(rgx, '?')
            puts(tmpl % (str(cyan(i)).rjust(5),
                         str(red(match_count)).rjust(5),
                         green(rgx)))

    @command()
    def save(self):
        '''Manually save the game state'''
        self.game_state.save()

    @command('rs')
    def show_added_regexes(self):
        '''Show added patterns.
        '''
        matched_count = sum(self.game_state['match_counts'].values())
        percent = matched_count / float(len(self.game_state._actions))
        puts(red(percent) + '% matched')
        game_state = self.game_state
        tmpl = '%s: (%s) %s'
        regexes = sorted(game_state['regexes'], key=hash)
        sre_type = type(re.compile(r''))
        for i, rgx in enumerate(regexes):
            if isinstance(rgx, sre_type):
                rgx = '%s  ... flags=%d' % (rgx.pattern, rgx.flags)
            match_count = game_state['match_counts'].get(rgx, '?')
            puts(tmpl % (str(cyan(i)).rjust(5),
                         str(red(match_count)).rjust(5),
                         green(rgx)))

    @command('rp')
    def purge_patterns(self):
        '''Purge regexes from the collections.
        '''
        game_state = self.game_state
        regexes = sorted(game_state['regexes'], key=hash)
        self.show_added_regexes()
        indexes = raw_input('Enter numbers of regexes to purge: ')
        indexes = map(int, indexes.split())
        regexes = map(regexes.__getitem__, indexes)
        for rgx in regexes:
            self.game_state['regexes'] -= set([rgx])
            logger.info('removed regex: %r' % rgx)

    @command('ra')
    def add_regex(self):
        '''Add a regex to the stored regexes.
        '''
        rgx = self.game_state['current_rgx']
        self.game_state['regexes'].add(rgx)
        puts(cyan('Added ') + green(rgx))
        self.game_state.save()

    @command('assoc')
    def assoc_rgx_with_types(self, index=None):
        '''Associate a rgx with action categories.
        '''
        if index is not None:
            index = int(index)
            regex = sorted(self.game_state['regexes'], key=hash)[index]
        else:
            regex = self.game_state['current_rgx']
        self.categories()
        types = raw_input('Type numbers of categories to apply: ')
        types = map(categories.__getitem__, map(int, types.split()))
        types = set(map(itemgetter(0), types))
        self.game_state['types'][regex] |= types
        types = ', '.join(types)
        puts(green(regex) + ' --> ' + yellow(types))

    @command('s')
    def print_summary(self):
        '''Display how many actions are currently matched or not.
        '''
        pprint.pprint(self.game_state)
        # unmatched_len = len(self.actions.unmatched)
        # unmatched = red('%d' % unmatched_len)
        # total_len = len(self.actions.list)
        # total = cyan('%d' % total_len)
        # message = 'There are %s unmatched actions out of %s total actions (%s).'
        # percentage = 1.0 * unmatched_len / total_len
        # percentage = green(percentage)
        # puts(message % (unmatched, total, percentage))

    @command('uu')
    def show_unmatched_actions_unsorted(self):
        '''List the first 10 actions.
        '''
        text = '\n'.join(self.game_state.unmatched_actions())
        self.pager(text.encode('utf-8'))

    @command('u')
    def show_unmatched_actions_sorted(self):
        '''Show actions in alphabetical order.

        To eliminate actions with types from the view, pass a flag (any text).
        '''
        self.show_actions_start = 0
        text = '\n'.join(sorted(list(self.game_state.unmatched_actions())))
        self.pager(text.encode('utf-8'))

    @command('im')
    def import_state_action_rules(self):
        '''Load the rule defs from a state and add them to this
        game's regexes.
        '''
        import importlib
        abbr = self.shell.abbr
        actions = importlib.import_module('openstates.%s.actions' % abbr)
        for rule in actions.Categorizer.rules:
            for rgx in rule.regexes:
                if hasattr(rgx, 'pattern'):
                    rgx = FlagDecompiler.add_flags_to_regex(rgx)
                self.game_state['regexes'].add(rgx)
                self.game_state['types'][rgx] |= rule.types
        rule_count = len(actions.Categorizer.rules)
        puts(yellow('Imported %d rule(s).' % rule_count))

    @command('dump')
    def dump_patterns(self):
        '''Dump the accumulated regexs into acopy/pasteable snippet.
        '''
        from billy.scrape.actions import Rule
        rules = []
        regexes = self.game_state['regexes']
        grouper = self.game_state['types'].get
        for types, regexes in itertools.groupby(regexes, grouper):
            rules.append((list(regexes), list(types or [])))

        pprint.pprint(rules)

    @command('b')
    def show_breakdown_all(self):
        '''List unmatched actions and show the count for each.
        '''
        unmatched = self.game_state.unmatched_actions()
        counts = collections.Counter(self.game_state._actions)
        items = sorted(counts.items(), key=itemgetter(1))
        actions = []
        for action, count in items:
            if action in unmatched:
                action = '[%s]' % action
            action = str(count).ljust(5) + action
            actions.append(action)
        self.pager('\n'.join(actions).encode('utf8'))

    @command('bu')
    def show_breakdown_unmatched(self):
        '''List unmatched actions and show the count for each.
        '''
        counts = collections.Counter(self.game_state.unmatched_actions())
        items = sorted(counts.items(), key=itemgetter(1))
        self.pager('\n'.join(str(count).ljust(5) + action for (action, count) in items).encode('utf8'))

    @command('md')
    def metadata(self):
        pprint.pprint(db.metadata.find_one(self.shell.abbr))

    @command()
    def allcategorized(self, line):
        ret = []
        for action in self.game_state._actions:
            ret.append(self.shell.locals['categorizer'].categorize(action))
        import pdb;pdb.set_trace()


class FlagDecompiler(object):
    '''A helper class to convert compile regexes into strings with
    inline flags. Helpful for editing, dumping regexes as text.
    '''
    flag_letters = 'IULMXS'
    flag_vals = [getattr(re, flag) for flag in flag_letters]
    letter_to_int = dict(zip(flag_letters, flag_vals))
    int_to_letter = dict(zip(flag_vals, flag_letters.lower()))
    int_to_letters = {}
    for r in range(7):
        for combo in itertools.combinations(flag_vals, r=r):
            letters = frozenset(map(int_to_letter.get, combo))
            int_to_letters[sum(combo)] = letters

    @classmethod
    def add_flags_to_regex(cls, compiled_rgx):
        # Get existing inline flags.
        rgx = compiled_rgx.pattern
        inline_flags = re.search(r'\(\?([iulmxs])\)', rgx)
        if inline_flags:
            inline_flags = set(inline_flags.group(1))

        if compiled_rgx.flags:
            letters = cls.int_to_letters[compiled_rgx.flags]
            if inline_flags:
                letters = set(letters) - inline_flags
                if letters:
                    return '(?%s)%s' % (''.join(letters), rgx)
        return rgx


########NEW FILE########
__FILENAME__ = batshell
# -*- coding: utf-8 -*-
import re
import sys
import pdb
import pydoc
import copy
import shlex
import types
import pprint
import itertools
import sre_constants
import webbrowser
import subprocess
import traceback
from code import InteractiveConsole
from operator import itemgetter
from os.path import abspath, dirname, join

import logbook
from clint.textui import puts, indent, colored
from clint.textui.colored import red, green, cyan, magenta, yellow

logger = logbook.Logger('batshell')
HERE = dirname(abspath(__file__))
pager = pydoc.getpager()

try:
    subprocess.check_call("echo 'test' | xsel -pi", shell=True)
except subprocess.CalledProcessError:
    xsel_enabled = False
    logger.warning(u'✄ Proceeding without xsel ☹')
    logger.info('Please install xsel to automatically '
                'copy tested regexes to the clipboard.')
else:
    xsel_enabled = True
    logger.info(u'✄ xsel is enabled! ☺')


def command(*aliases, **kwargs):
    def decorator(f):
        f.is_command = True
        f.aliases = aliases or kwargs.get('aliases', [])
        f.lex_args = kwargs.get('lex_args', True)
        return f
    return decorator


class ShellCommands(object):

    def __init__(self):
        self.pager = pydoc.getpager()
        self.command_map = self.as_map()

    def as_map(self):
        commands = {}
        for name in dir(self):
            attr = getattr(self, name)
            if getattr(attr, 'is_command', False):
                commands[name] = attr
                if isinstance(attr, types.MethodType):
                    for alias in getattr(attr, 'aliases', []):
                        commands[alias] = attr
        return commands

    @command('h')
    def help(self, command_name=None):
        '''Show help on the specified commands, otherwise a list of commands.
        '''
        if command_name:
            command = self.command_map[command_name]
            help(command)
        else:
            command_map = self.command_map

            def fmt(cmd):
                command_name = green(cmd.__name__)
                aliases = yellow(', '.join(cmd.aliases))
                return str('(%s) %s:' % (aliases, command_name))
            commands = {cmd: fmt(cmd) for cmd in command_map.values()}
            shame = red('[No docstring found. For shame!]')
            for cmd in commands:
                puts(commands[cmd])
                with indent(4):
                    docstring = cmd.__doc__ or shame
                    puts(docstring)

    @command('i')
    def inspect(self):
        '''In case you want to look inside the shell's guts.
        '''
        pdb.set_trace()

    @command('q')
    def quit(self):
        '''Quit the batshell. You have failed!
        '''
        import sys
        sys.exit(1)

    @command(aliases=['pp'], lex_args=False)
    def prettyprint(self, expression):
        '''Pretty print something--can handle expressions:
        >>> pp 1 + 3, "cow"
        '''
        mylocals = copy.copy(self.shell.locals)
        exec 'print_val = ' + expression in mylocals
        pprint.pprint(mylocals['print_val'])


class Shell(InteractiveConsole):

    ps1 = None

    def __init__(self, commands=None, *args, **kwargs):
        InteractiveConsole.__init__(self, *args, **kwargs)
        self.last_line = None
        if commands is None:
            commands = ShellCommands()

        # Keep a references to the shell on the commands.
        commands.shell = self
        command_map = commands.as_map()
        keys = sorted(command_map, key=len, reverse=True)
        self.command_regex = '^(?P<cmd>%s)(\s+(?P<args>.*))?$' % '|'.join(keys)
        self.commands = commands
        self.command_map = command_map
        self.logger = logbook.Logger(self.ps1 or 'logger')

    def push(self, line):

        if not line:
            if not self.last_line:
                return
            line = self.last_line
        self.last_line = line

        # Call the custom command if given.
        m = re.search(self.command_regex, line)
        if m:
            command_name = m.group('cmd')
            command = self.command_map[command_name]
            args = []
            if m.group('args') is not None:
                argtext = str(m.group('args'))
                if command.lex_args:
                    # Lex the args text.
                    args += shlex.split(argtext)
                else:
                    # Pass the raw text.
                    args.append(argtext)

            if command_name in ('q', 'quit'):
                return command(*args)

            try:
                ret = command(*args)
            except:
                # The command encountered an error.
                traceback.print_exc(file=sys.stdout)
                return
            else:
                # Command succeeded; inject the result back into the shell.
                if ret:
                    self.locals['ret'] = ret
                    msg = 'Result of function has been assigned to name "ret"'
                    self.logger.info(msg)
            return

        if xsel_enabled:
            p = subprocess.Popen(['xsel', '-bi'], stdin=subprocess.PIPE)
            p.communicate(input=line)

        InteractiveConsole.push(self, line)

    def interact(self, *args, **kwargs):
        sys.ps1 = self.ps1
        puts(self.banner)

        try:
            import readline
        except ImportError:
            pass
        InteractiveConsole.interact(self, *args, **kwargs)

########NEW FILE########
__FILENAME__ = broken_progress
import webbrowser
import itertools
import operator
from billy.core import db


def main(abbr, session=None):

    spec = {
        'state': abbr,
        'action_dates.signed': {'$ne': None},
        '$or': [
            {'action_dates.passed_upper': None},
            {'action_dates.passed_lower': None}
            ]
        }

    if session is not None:
        spec['session'] = session

    bills = db.bills.find(spec)
    yield bills.count()
    bills = list(bills)
    yield bills
    if not session:
        grouper = operator.itemgetter('session')
        for session, _bills in itertools.groupby(bills, key=grouper):
            yield session, len(list(_bills))


if __name__ == '__main__':
    import sys

    from clint.textui import puts, indent
    from clint.textui.colored import green, yellow, red

    if not sys.argv[1:]:
        for meta in db.metadata.find():
            abbr = meta['_id']
            data = main(abbr)
            bogus = data.next()
            bills = data.next()
            if not bogus:
                puts(green('%s: ok!' % abbr))
                continue
            msg = '%s: Found %s bills with progress gaps...'
            puts(msg % (green(abbr), red(str(bogus))))
            with indent(2):
                for session, total in data:
                    vals = (yellow(session), red(total))
                    puts('|-%s: %s' % vals)

    else:
        abbr = sys.argv[1]
        data = main(abbr)
        bogus = data.next()
        bills = data.next()
        if not bogus:
            puts(green('%s: ok!' % abbr))
            sys.exit(1)
        msg = '%s: Found %s bills with progress gaps...'
        puts(msg % (green(abbr), red(str(bogus))))
        with indent(2):
            for session, total in data:
                vals = (yellow(session), red(total))
                puts('|-%s: %s' % vals)

        raw_input("Press enter to view the offending bills: ")

        for bill in bills:
            url = 'http://localhost:8000/{state}/bills/{session}/{bill_id}/'.format(**bill)
            print bill['bill_id'], url
            print bill['action_dates']
            webbrowser.open(url)
            import pdb;pdb.set_trace()

########NEW FILE########
__FILENAME__ = categories
categories = [
    ('bill:introduced',
     'Bill is introduced or prefiled'),

    ('bill:passed',
     'Bill has passed a chamber'),

    ('bill:failed',
     'Bill has failed to pass a chamber'),

    ('bill:withdrawn',
     'Bill has been withdrawn from consideration'),

    ('bill:veto_override:passed',
     'The chamber attempted a veto override and succeeded'),

    ('bill:veto_override:failed',
     'The chamber attempted a veto override and failed'),

    ('bill:reading:1',
     'A bill has undergone its first reading'),

    ('bill:reading:2',
     'A bill has undergone its second reading'),

    ('bill:reading:3',
     'A bill has undergone its third (or final) reading'),

    ('bill:filed',
     ('A bill has been filed (for states where this is '
      'a separate event from bill:introduced')),

    ('bill:substituted',
     ('A bill has been replaced with a substituted wholesale '
      '(called hoghousing in some states')),

    ('governor:received',
     'The bill has been transmitted to the governor for consideration'),

    ('governor:signed',
     'The bill has signed into law by the governor'),

    ('governor:vetoed',
     'The bill has been vetoed by the governor'),

    ('governor:vetoed:line-item',
     'The governor has issued a line-item (partial) veto'),

    ('amendment:introduced',
     'An amendment has been offered on the bill'),

    ('amendment:passed',
     'The bill has been amended'),

    ('amendment:failed',
     'An offered amendment has failed'),

    ('amendment:amended',
     'An offered amendment has been amended (seen in Texas)'),

    ('amendment:withdrawn',
     'An offered amendment has been withdrawn'),

    ('amendment:tabled',
     ('An amendment has been \'laid on the table\' '
      '(generally preventing further consideration)')),

    ('committee:referred',
     'The bill has been referred to a committee'),

    ('committee:passed',
     'The bill has been passed out of a committee'),

    ('committee:passed:favorable',
     'The bill has been passed out of a committee with a favorable report'),

    ('committee:passed:unfavorable',
     'The bill has been passed out of a committee with an unfavorable report'),

    ('committee:failed',
     'The bill has failed to make it out of committee other'),
    ]

########NEW FILE########
__FILENAME__ = categorize
import re
from functools import partial
from collections import namedtuple, defaultdict
from types import MethodType


class Rule(namedtuple('Rule', 'regexes types stop attrs')):
    '''If anyh of ``regexes`` matches the action text, the resulting
    action's types should include ``types``.

    If stop is true, no other rules should be tested after this one;
    in other words, this rule conclusively determines the action's
    types and attrs.

    The resulting action should contain ``attrs``, which basically
    enables overwriting certain attributes, like the chamber if
    the action was listed in the wrong column.
    '''
    def __new__(_cls, regexes, types=None, stop=False, **kwargs):
        'Create new instance of Rule(regex, types, attrs, stop)'

        # Regexes can be a string or a sequence.
        if isinstance(regexes, basestring):
            regexes = set([regexes])
        regexes = set(regexes or [])

        # Types can be a string or a sequence.
        if isinstance(types, basestring):
            types = set([types])
        types = set(types or [])

        return tuple.__new__(_cls, (regexes, types, stop, kwargs))


class BaseCategorizer(object):
    '''A class that exposes a main categorizer function
    and before and after hooks, in case a state requires specific
    steps that make use of action or category info. The return
    value is a 2-tuple of category types and a dictionary of
    attributes to overwrite on the target action object.
    '''
    rules = []

    def __init__(self):
        before_funcs = []
        after_funcs = []
        for name in dir(self):
            attr = getattr(self, name)
            if isinstance(attr, MethodType):
                # func = partial(attr, self)
                func = attr
                if getattr(attr, 'before', None):
                    before_funcs.append(func)
                if getattr(attr, 'after', None):
                    after_funcs.append(func)
        self._before_funcs = before_funcs
        self._after_funcs = after_funcs

    def categorize(self, text):

        whitespace = partial(re.sub, '\s{1,4}', '\s{,4}')

        # Run the before hook.
        text = self.before_categorize(text)
        for func in self._before_funcs:
            text = func(text)

        types = set()
        attrs = defaultdict(set)
        for rule in self.rules:

            for regex in rule.regexes:

                # Try to match the regex.
                m = re.search(whitespace(regex), text)
                if m or (regex in text):
                    # If so, apply its associated types to this action.
                    types |= rule.types

                    # Also add its specified attrs.
                    for k, v in m.groupdict().items():
                        attrs[k].add(v)

                    for k, v in rule.attrs.items():
                        attrs[k].add(v)

                    # Break if the rule says so, otherwise
                    # continue testing against other rules.
                    if rule.stop is True:
                        break

        # Returns types, attrs
        return_val = (list(types), attrs)
        return_val = self.after_categorize(return_val)
        for func in self._after_funcs:
            return_val = func(*return_val)
        return self.finalize(return_val)

    def before_categorize(self, text):
        '''A precategorization hook. Takes/returns text.
        '''
        return text

    def after_categorize(self, return_val):
        '''A post-categorization hook. Takes, returns
        a tuple like (types, attrs), where types is a sequence
        of categories (e.g., bill:passed), and attrs is a
        dictionary of addition attributes that can be used to
        augment the action (or whatever).
        '''
        return return_val

    def finalize(self, return_val):
        '''Before the types and attrs get passed to the
        importer they need to be altered by converting lists to
        sets, etc.
        '''
        types, attrs = return_val
        _attrs = {}

        # Get rid of defaultdict.
        for k, v in attrs.items():

            # Skip empties.
            if not v:
                continue
            else:
                v = filter(None, v)

            # Get rid of sets.
            if isinstance(v, set):
                v = list(v)

            # Some vals should be strings, not seqs.
            if k == 'actor' and len(v) == 1:
                v = v.pop()

            _attrs[k] = v

        return types, _attrs


def after_categorize(f):
    '''A decorator to mark a function to be run
    before categorization has happened.
    '''
    f.after = True
    return f


def before_categorize(f):
    '''A decorator to mark a function to be run
    before categorization has happened.
    '''
    f.before = True
    return f

########NEW FILE########
__FILENAME__ = run
import sys

from bathawk import BathawkShell
from actions import Actions


def main(_, abbr):
    actions = Actions(abbr)
    shell = BathawkShell(actions, abbr)
    shell.interact(banner='')

if __name__ == '__main__':
    main(*sys.argv)

########NEW FILE########
__FILENAME__ = tn
patterns = []
sub_patterns = {}

########NEW FILE########
__FILENAME__ = bill_bayes
#!/usr/bin/env python
import itertools
import collections

from nltk.corpus import stopwords
from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader
from nltk.classify import NaiveBayesClassifier
import nltk.classify.util
from nltk.collocations import BigramCollocationFinder
from nltk.metrics import BigramAssocMeasures
from nltk.probability import FreqDist, ConditionalFreqDist

stopset = set(stopwords.words('english'))
stopset.add('member')
stopset.add('california')


def most_informative_words(corpus, categories=['dem', 'rep'], count=2500):
    fd = FreqDist()
    cond_fd = ConditionalFreqDist()
    word_counts = {}

    for cat in categories:
        for word in corpus.words(categories=[cat]):
            word = word.lower().strip(".!?:,/ ")
            if not word.isalpha() or word in stopset:
                continue
            fd.inc(word)
            cond_fd[cat].inc(word)

        word_counts[cat] = cond_fd[cat].N()

    total_word_count = sum(word_counts.values())

    word_scores = collections.defaultdict(int)
    for word, freq in fd.iteritems():
        for cat in categories:
            cat_word_score = BigramAssocMeasures.chi_sq(
                cond_fd[cat][word],
                (freq, word_counts[cat]),
                total_word_count)
            word_scores[word] += cat_word_score

    informative_words = sorted(word_scores.iteritems(),
                               key=lambda (w, s): s,
                               reverse=True)[:count]
    return set([w for w, s in informative_words])


def word_feats(words):
    feats = {}
    for word in words:
        word = word.lower().strip(".!?:,/ ")
        if word in best_words:
            feats[word] = True
    return feats


def bigram_feats(words):
    filtered_words = []
    for word in words:
        word = word.lower().strip(".!?:,/ ")
        if word in best_words:
            filtered_words.append(word)

    bigram_finder = BigramCollocationFinder.from_words(filtered_words)
    bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 200)
    return dict([(ngram, True) for ngram in itertools.chain(filtered_words,
                                                            bigrams)])


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('directory',
                        help="the bill directory")
    parser.add_argument('--bigrams', action='store_true', dest='bigrams',
                        default=False, help='use bigrams')
    args = parser.parse_args()

    if args.bigrams:
        featurizer = bigram_feats
    else:
        featurizer = word_feats

    corpus = CategorizedPlaintextCorpusReader(
        root=args.directory,
        fileids=".*/.*\.txt",
        cat_pattern=r'(dem|rep)/')

    best_words = most_informative_words(corpus)

    dem_ids = corpus.fileids(categories=['dem'])
    rep_ids = corpus.fileids(categories=['rep'])

    dem_feats = [(featurizer(corpus.words(fileids=[f])), 'dem')
                 for f in dem_ids]
    rep_feats = [(featurizer(corpus.words(fileids=[f])), 'rep')
                 for f in rep_ids]

    dem_cutoff = len(dem_feats) * 5 / 6
    rep_cutoff = len(rep_feats) * 5 / 6

    train_feats = dem_feats[:dem_cutoff] + rep_feats[:rep_cutoff]
    test_feats = dem_feats[dem_cutoff:] + rep_feats[rep_cutoff:]
    print 'training on %d instances, testing on %d instances' % (
        len(train_feats), len(test_feats))

    classifier = NaiveBayesClassifier.train(train_feats)
    print 'accuracy:', nltk.classify.util.accuracy(classifier, test_feats)

    classifier.show_most_informative_features(10)

########NEW FILE########
__FILENAME__ = pagerank
from billy import db, utils
from billy.core import settings

import numpy


def generate_leg_indexes(abbr, term, chamber):
    leg_indexes = {}
    elemMatch = {settings.LEVEL_FIELD: abbr, 'chamber': chamber,
                 'type': 'member', 'term': term}
    i = 0
    for leg in db.legislators.find({'$or':
                                    [{'roles': {'$elemMatch': elemMatch}},
                                     {('old_roles.%s' % term):
                                      {'$elemMatch': elemMatch}}]}):
        leg_indexes[leg['leg_id']] = i
        i += 1
    return leg_indexes


def generate_adjacency_matrix(abbr, session, chamber, leg_indexes,
                              primary_sponsor_type='LEAD_AUTHOR'):
    size = len(leg_indexes)
    matrix = numpy.zeros((size, size))

    for bill in db.bills.find({settings.LEVEL_FIELD: abbr, 'session': session,
                               'chamber': chamber}):
        try:
            for author in bill['sponsors']:
                if author['type'] == primary_sponsor_type:
                    primary_sponsor_id = leg_indexes[author['leg_id']]
                    break
            else:
                continue
        except KeyError:
            continue
        except IndexError:
            continue

        for sponsor in bill['sponsors'][1:]:
            if sponsor['type'] == primary_sponsor_type:
                continue

            try:
                sponsor_id = leg_indexes[sponsor['leg_id']]
            except KeyError:
                continue

            matrix[primary_sponsor_id, sponsor_id] += 1

    return matrix


def pagerank(matrix, d_factor=0.85):
    """
    Calculate the pagerank vector of a given adjacency matrix (using
    the power method).

    :param matrix: an adjacency matrix
    :param d_factor: the damping factor
    """
    size = len(matrix)
    epsilon = 0.0001
    matrix = matrix.copy()

    # Divide each column by its number of outgoing links
    for i in xrange(0, size):
        col_sum = matrix[:, i].sum()
        if col_sum:
            matrix[:, i] /= col_sum

    e = ((1.0 - d_factor) / size) * numpy.ones((size, size))
    matrix = d_factor * matrix + e

    result = numpy.ones(size) / size
    prev = numpy.ones(size) / size
    iteration = 0

    while True:
        result = numpy.dot(matrix, result)
        result /= result.sum()
        diff = numpy.abs(result - prev).sum()
        print "Iteration %d, change %f" % (iteration, diff)
        if diff < epsilon:
            break
        prev = result
        iteration += 1

    return result


def legislator_pagerank(abbr, session, chamber, d_factor=0.85):
    term = utils.term_for_session(abbr, session)
    leg_indexes = generate_leg_indexes(abbr, term, chamber)
    adjacency_matrix = generate_adjacency_matrix(abbr, session, chamber,
                                                 leg_indexes)
    result = pagerank(adjacency_matrix, d_factor)

    for leg_id in leg_indexes.keys():
        leg_indexes[leg_id] = result[leg_indexes[leg_id]]

    return leg_indexes


if __name__ == '__main__':
    import sys
    import csv
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('abbr')
    parser.add_argument('session')
    parser.add_argument('chamber')
    args = parser.parse_args()

    pr = legislator_pagerank(args.abbr, args.session, args.chamber)
    out = csv.writer(sys.stdout)

    for (leg_id, value) in pr.iteritems():
        leg = db.legislators.find_one({'_id': leg_id})
        out.writerow((leg_id, leg['full_name'], value))

########NEW FILE########
__FILENAME__ = wnominate
#!/usr/bin/env python
import os
import csv
import sys
import tempfile
import subprocess

from billy import db, utils
from billy.core import settings


def vote_csv(abbr, session, chamber, out=sys.stdout):
    term = utils.term_for_session(abbr, session)

    votes = {}
    legislators = {}

    elemMatch = {settings.LEVEL_FIELD: abbr, 'chamber': chamber,
                 'type': 'member', 'term': term}

    for leg in db.legislators.find({'$or':
                                    [{'roles': {'$elemMatch': elemMatch}},
                                     {('old_roles.%s' % term):
                                      {'$elemMatch': elemMatch}}]}):
        votes[leg['leg_id']] = []
        legislators[leg['leg_id']] = leg

    for bill in db.bills.find({settings.LEVEL_FIELD: abbr, 'chamber': chamber,
                               'session': session}):
        for vote in bill['votes']:
            if 'committee' in vote and vote['committee']:
                continue
            if vote['chamber'] != chamber:
                continue

            seen = set()

            for yv in vote['yes_votes']:
                leg_id = yv['leg_id']
                if leg_id:
                    seen.add(leg_id)
                    try:
                        votes[leg_id].append(1)
                    except KeyError:
                        continue
            for nv in vote['no_votes']:
                leg_id = nv['leg_id']
                if leg_id:
                    seen.add(leg_id)
                    try:
                        votes[leg_id].append(6)
                    except KeyError:
                        continue

            for leg_id in set(votes.keys()) - seen:
                votes[leg_id].append(9)

    out = csv.writer(out)

    for (leg_id, vs) in votes.iteritems():
        leg = legislators[leg_id]

        try:
            party = leg['old_roles'][term][0]['party']
        except KeyError:
            party = leg['party']

        row = [leg['full_name'].encode('ascii', 'replace'), leg['leg_id'],
               party]
        for vote in vs:
            row.append(str(vote))

        out.writerow(row)


def wnominate(abbr, session, chamber, polarity, r_bin="R",
              out_file=None):
    (fd, filename) = tempfile.mkstemp('.csv')
    with os.fdopen(fd, 'w') as out:
        vote_csv(abbr, session, chamber, out)

    if not out_file:
        (result_fd, out_file) = tempfile.mkstemp('.csv')
        os.close(result_fd)

    r_src_path = os.path.join(os.path.dirname(__file__), 'calc_wnominate.R')

    with open('/dev/null', 'w') as devnull:
        subprocess.check_call([r_bin, "-f", r_src_path, "--args",
                               filename, out_file, polarity],
                              stdout=devnull, stderr=devnull)

    results = {}
    with open(out_file) as f:
        c = csv.DictReader(f)
        for row in c:
            try:
                res = float(row['coord1D'])
            except ValueError:
                res = None
            results[row['leg_id']] = res

    os.remove(filename)

    return results


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('abbr')
    parser.add_argument('session')
    parser.add_argument('chamber')
    parser.add_argument('polarity')
    parser.add_argument('out_file')
    args = parser.parse_args()

    wnominate(args.abbr, args.session, args.chamber, args.polarity,
              out_file=args.out_file)

########NEW FILE########
__FILENAME__ = condor
#

from billy import db

state = "oh"


def get_state_breakdown(abbr):
    legs = db.legislators.find({"state": abbr})
    repub, dem, other = [], [], []
    for leg in legs:
        roles = leg['roles']
        if len(roles) <= 0:
            continue

        if not "party" in roles[0]:
            continue

        parta = roles[0]['party'].strip()
        if parta == "Republican":
            repub.append(leg)
        elif parta == "Democratic":
            dem.append(leg)
        else:
            other.append(leg)

    rC = float(len(repub))
    dC = float(len(dem))
    oC = float(len(other))
    tC = (rC + dC + oC)
    rP = rC / tC
    dP = dC / tC
    oP = oC / tC

    return (
        (rP, dP, oP),
        (rC, dC, oC),
        (repub, dem, other)
    )


def grok_committee(cid):
    ctty = db.committees.find_one({"_id": cid})
    if ctty is None:
        return

    (pcts, cts, raw) = get_state_breakdown(ctty['state'])

    rC, dC, oC = cts
    rP, dP, oP = pcts

    missing = 0.0
    total = 0.0

    rCount = 0.0
    dCount = 0.0
    oCount = 0.0

    for member in ctty['members']:
        total += 1
        if "leg_id" in member:
            leg = db.legislators.find_one({"_id": member['leg_id']})
            if leg is None:
                missing += 1
                continue

            roles = leg['roles']
            if len(roles) <= 0:
                missing += 1
                continue

            if not "party" in roles[0]:
                missing += 1
                continue

            parta = roles[0]['party'].strip()
            if parta == "Republican":
                rCount += 1
            elif parta == "Democratic":
                dCount += 1
            else:
                oCount += 1
        else:
            missing += 1

    if total == 0:
        return None

    rPct = rCount / total
    dPct = dCount / total
    oPct = oCount / total

    rD = rPct - rP
    dD = dPct - dP
    oD = oPct - oP

    pErr = missing / total

    # print "Ctty:", rPct, dPct, oPct
    # print "CoW: ", rP, dP, oP
    # print "Delt:", rD, dD, oD

    return (rD, dD, oD, pErr)


def digest_state(state):
    ctties = db.committees.find({"state": state})

    raw = []

    for c in ctties:
        ctty_stuff = grok_committee(c['_id'])
        if ctty_stuff is None:
            continue

        rD, dD, oD, pE = ctty_stuff
        raw.append({
            "err": pE,
            "repub": rD,
            "dem": dD,
            "other": oD,
            "cid": c['_id']
        })

    demSkew = sorted(raw, key=lambda x: x['dem'])
    repSkew = sorted(raw, key=lambda x: x['repub'])
    othSkew = sorted(raw, key=lambda x: x['other'])
    allSkew = sorted(raw, key=lambda x: (abs(x['dem']) + abs(x['repub']) + abs(x['other'])))

    return (demSkew, repSkew, othSkew, allSkew)

states = [x['_id'] for x in db.metadata.find()]

for state in states:
    print digest_state(state)

########NEW FILE########
__FILENAME__ = debug
'''
Mission
========

Accurately guess when a bill is currently in a particular committee.
Also, accurately guess which committee and bill has been in and how
long the bill spent in those committees.

The problem
===============

Many states provide actions when a bill is referred, but not when
it is reported.

Possible Solution
==================

On a state-by-state basis, create lists of actions that necessarily
or at least probably indicate that previously referred bill is probably
out of the committee it was referred to. To elaborate, when a bill passes
the introduced chamber, it's a good bet that it has been reported following
its initial committee referral.
'''
import itertools
from collections import defaultdict
import logbook

from billy.models import db


logger = logbook.Logger('elf-owl')


class RecursiveDotDict(dict):
    def __getattr__(self, name):
        attr = self[name]
        if isinstance(attr, dict):
            attr = RecursiveDotDict(attr)
        return attr


class Action(RecursiveDotDict):

    @property
    def types(self):
        return set(self.type)

    @property
    def id(self):
        bill = self.bill
        return bill['_id'], bill['actions'].index(self), self['actor']


class Referral(dict):

    def __init__(self, referring_action=None, reporting_action=None):
        self['referring_action'] = referring_action
        self['reporting_action'] = reporting_action
        self['status'] = 'pending'

    def set_reporting_action(self, action):
        self['reporting_action'] = action
        self['status'] = 'complete'


class BaseDetectorThingy(object):

    referred_types = set(['committee:referred'])
    reported_types = set([
        'committee:passed',
        'committee:passed:favorable',
        'committee:passed:unfavorable',
        ])
    reported_inferred_types = set([
        'reading:2:passed'
        'bill:passed',
        ])

    def __init__(self, bill):
        action_cls = type('Action', (Action,), dict(bill=bill))
        self.referrals = defaultdict(list)
        self.bill = bill
        actions = map(action_cls, bill['actions'])

        self.actions = actions

    def pending_referrals(self):
        '''All the referrals that haven't (ostensibly) concluded
        yet. Yield them in reverse order though.
        '''
        referrals = sorted(
            itertools.chain.from_iterable(self.referrals.values()),
            key=lambda ref: ref['referring_action']['date'],
            reverse=True)
        for referral in referrals:
            if not referral['reporting_action']:
                yield referral

    def check(self):
        for k, v in self.referrals.items():
            done = []
            v = list(v)
            while v:
                x = v.pop()
                if x in done:
                    import pdb;pdb.set_trace()
                done.append(x)


    def get_referrals(self):
        referred_types = self.referred_types
        reported_types = self.reported_types
        reported_inferred_types = self.reported_inferred_types
        bogus_referrals = []
        for action in self.actions:
            # self.check()

            if action.types & (reported_types | reported_inferred_types):
                logger.debug('REPORTED %r %r' % (action['date'], action['action']))

                # Close the most recent one.
                if self.referrals:
                    pending = self.pending_referrals()
                    recent = next(pending)

                    recent['reporting_action'] = action

                    # The rest are marked complete without
                    # a reporting action, per the assumption
                    # that a bill can only be referred to one
                    # committe at a time.
                    for referral in pending:
                        referral['status'] = 'complete'

                else:
                    logger.warning('No referral: %r' % action['action'])

            if action.types & referred_types:
                logger.debug('REFERRED %r, %r' % (action['date'], action['action']))

                # Is the committee unambigiuously identified in this action?
                committee_ids = [obj['id'] for obj in action.related_entities
                                 if obj['type'] == 'committee']
                committee_ids = filter(None, committee_ids)

                # Make sure there's only one tagged committee
                # in the referring action.
                try:
                    assert len(committee_ids) == 1
                except AssertionError:
                    if len(committee_ids) == 0:
                        msg = "No committees captured in action: %r"
                    elif len(committee_ids) > 1:
                        msg = "Ambiguous referring committee: %r"
                    logger.warning(msg % action.action)
                    bogus_referrals.append(Referral(action))
                    continue

                # A list of all referrals of this bill to this committee.
                key = committee_ids.pop(), action['actor']
                committee_referrals = self.referrals[key]
                committee_referrals.append(Referral(action))

        for key, value in self.referrals.items():
            import pprint
            pprint.pprint(value)
            # import pdb;pdb.set_trace()


def main(abbr, *args):

    bills = db.bills.find({
        'state': abbr,
        'actions.type': 'committee:refereed',
        'actions.type': 'committee:passed'
        })

    for bb in bills:
        print '\n\n-------', bb['bill_id'], '------\n\n'
        dt = BaseDetectorThingy(bb)
        dt.get_referrals()

        tags = ['committee:referred', 'committee:passed'
                'committee:passed:favorable',
                'committee:passed:unfavorable',]
        acs = filter(lambda a: set(tags) & set(a['type']), bb['actions'])
        for ac in acs:
            print 'text: %(action)s, tags: %(type)r' % ac
        import pdb;pdb.set_trace()


if __name__ == '__main__':
    import sys
    main(*sys.argv[1:])
########NEW FILE########
__FILENAME__ = batshell
# -*- coding: utf-8 -*-
import re
import sys
import pdb
import pydoc
import copy
import shlex
import types
import pprint
import itertools
import sre_constants
import webbrowser
import subprocess
import traceback
from code import InteractiveConsole
from operator import itemgetter
from os.path import abspath, dirname, join

import logbook
from clint.textui import puts, indent, colored
from clint.textui.colored import red, green, cyan, magenta, yellow

logger = logbook.Logger('batshell')
HERE = dirname(abspath(__file__))
pager = pydoc.getpager()

try:
    subprocess.check_call("echo 'test' | xsel -pi", shell=True)
except subprocess.CalledProcessError:
    xsel_enabled = False
    logger.warning(u'✄ Proceeding without xsel ☹')
    logger.info('Please install xsel to automatically '
                'copy tested regexes to the clipboard.')
else:
    xsel_enabled = True
    logger.info(u'✄ xsel is enabled! ☺')


def command(*aliases, **kwargs):
    def decorator(f):
        f.is_command = True
        f.aliases = aliases or kwargs.get('aliases', [])
        f.lex_args = kwargs.get('lex_args', True)
        return f
    return decorator


class ShellCommands(object):

    def __init__(self):
        self.command_map = self.as_map()

    def as_map(self):
        commands = {}
        for name in dir(self):
            attr = getattr(self, name)
            if getattr(attr, 'is_command', False):
                commands[name] = attr
                if isinstance(attr, types.MethodType):
                    for alias in getattr(attr, 'aliases', []):
                        commands[alias] = attr
        return commands

    @command('h')
    def help(self, command_name=None):
        '''Show help on the specified commands, otherwise a list of commands.
        '''
        if command_name:
            command = self.command_map[command_name]
            help(command)
        else:
            command_map = self.command_map

            def fmt(cmd):
                command_name = green(cmd.__name__)
                aliases = yellow(', '.join(cmd.aliases))
                return str('(%s) %s:' % (aliases, command_name))
            commands = {cmd: fmt(cmd) for cmd in command_map.values()}
            shame = red('[No docstring found. For shame!]')
            for cmd in commands:
                puts(commands[cmd])
                with indent(4):
                    docstring = cmd.__doc__ or shame
                    puts(docstring)

    @command('i')
    def inspect(self):
        '''In case you want to look inside the shell's guts.
        '''
        pdb.set_trace()

    @command('q')
    def quit(self):
        '''Quit the batshell. You have failed!
        '''
        import sys
        sys.exit(1)

    @command(aliases=['pp'], lex_args=False)
    def prettyprint(self, expression):
        '''Pretty print something--can handle expressions:
        >>> pp 1 + 3, "cow"
        '''
        mylocals = copy.copy(self.shell.locals)
        exec 'print_val = ' + expression in mylocals
        pprint.pprint(mylocals['print_val'])

    def inject(self, **kwargs):
        '''Inject vars into the shell's local scope.
        '''
        self.shell.locals.update(**kwargs)


class Shell(InteractiveConsole):

    ps1 = None

    def __init__(self, commands=None, *args, **kwargs):
        InteractiveConsole.__init__(self, *args, **kwargs)
        self.last_line = None
        if commands is None:
            commands = ShellCommands()

        # Keep a references to the shell on the commands.
        commands.shell = self
        command_map = commands.as_map()
        keys = sorted(command_map, key=len, reverse=True)
        self.command_regex = '^(?P<cmd>%s)(\s+(?P<args>.*))?$' % '|'.join(keys)
        self.commands = commands
        self.command_map = command_map
        self.logger = logbook.Logger(self.ps1 or 'logger')

    def push(self, line):

        if not line:
            if not self.last_line:
                return
            line = self.last_line
        self.last_line = line

        # Call the custom command if given.
        m = re.search(self.command_regex, line)
        if m:
            command_name = m.group('cmd')
            command = self.command_map[command_name]
            args = []
            if m.group('args') is not None:
                argtext = str(m.group('args'))
                if command.lex_args:
                    # Lex the args text.
                    args += shlex.split(argtext)
                else:
                    # Pass the raw text.
                    args.append(argtext)

            if command_name in ('q', 'quit'):
                return command(*args)

            try:
                ret = command(*args)
            except:
                # The command encountered an error.
                traceback.print_exc(file=sys.stdout)
                return
            else:
                # Command succeeded; inject the result back into the shell.
                if ret:
                    self.locals['ret'] = ret
                    msg = 'Result of function has been assigned to name "ret"'
                    self.logger.info(msg)
            return

        if xsel_enabled:
            p = subprocess.Popen(['xsel', '-bi'], stdin=subprocess.PIPE)
            p.communicate(input=line)

        InteractiveConsole.push(self, line)

    def interact(self, *args, **kwargs):
        sys.ps1 = self.ps1
        puts(self.banner)

        try:
            import readline
        except ImportError:
            pass
        InteractiveConsole.interact(self, *args, **kwargs)

########NEW FILE########
__FILENAME__ = bs
import json
import re

from clint.textui import colored
from selenium import webdriver
from batshell import Shell, ShellCommands, command
import logbook

from billy.core import mdb, db


def nth(n, iterable):
    it = iter(iterable)
    for i in xrange(n):
        res = next(it)
    return res


class BillyShell(Shell):
    ps1 = '(billy) '
    banner = colored.yellow('''
_|        _|  _|  _|
_|_|_|        _|  _|  _|    _|
_|    _|  _|  _|  _|  _|    _|
_|    _|  _|  _|  _|  _|    _|
_|_|_|    _|  _|  _|    _|_|_|
                            _|
                        _|_|''')
    banner += colored.cyan('\n\nWelcome to billy shell. '
                           'Type h for a list of commands.')


class BillyCommands(ShellCommands):

    @command()
    def sessions(self):
        '''List sessions for this state.
        '''
        for session in self.metadata['session_details']:
            print session

    @command(aliases=['b'], lex_args=False)
    def bills(self, line):
        '''Search bills in mongo.
        * If the first argument is a mongo spec, return a mongo cursor.
          e.g. `b {"state": "ny"}`
        * If the argument is a mongo id, fetch that bill.
          e.g. `b NYL012345`
        '''
        # Is the first arg a spec? If so, query mongo with it.
        if line.startswith('{') and line.endswith('}'):
            line = json.loads(line)
            return mdb.bills.find(line)

        if re.match(r'[A-Z]{2}B\d+', line):
            return mdb.bills.find_one(line)

    @command(aliases=['l'], lex_args=False)
    def legislators(self, line):
        '''`l {"state": "ny"}`
        * If the first argument is a mongo spec, return a mongo cursor.
        * If the argument is a mongo id, fetch that object.
        '''
        # Is the first arg a spec? If so, query mongo with it.
        if line.startswith('{') and line.endswith('}'):
            line = json.loads(line)
            return mdb.legislators.find(line)

        if re.match(r'[A-Z]{2}L\d+', line):
            return mdb.legislators.find_one(line)

    @command(aliases=['c'], lex_args=False)
    def committees(self, line):
        '''
        * If the first argument is a mongo spec, return a mongo cursor.
        * If the argument is a mongo id, fetch that object.
        '''
        # Is the first arg a spec? If so, query mongo with it.
        if line.startswith('{') and line.endswith('}'):
            line = json.loads(line)
            return mdb.committees.find(line)

        if re.match(r'[A-Z]{2}C\d+', line):
            return mdb.committees.find_one(line)

    @command('pb', lex_args=False)
    def random_bill_public_local(self, argtext=None, cache={}):
        '''Page through bills one at a time.
        '''
        do_query = False
        if argtext is not None:
            do_query = True
            if argtext.startswith('{') and argtext.endswith('}'):
                spec = json.loads(argtext)

        elif not hasattr(self, 'bills_cursor'):
            do_query = True
            spec = {}
        else:
            spec = {}

        if do_query:
            if 'state' not in spec:
                spec['state'] = self.abbr
                cache['state'] = self.abbr

            self.logger.info('Mongo query: %r' % spec)
            self.bills_cursor = mdb.bills.find(spec)

        bill = self.bills_cursor.next()
        self.logger.info(bill['_id'])
        url = 'http://localhost:8000/{abbr}/bills/{session}/{bill_id}/'
        url = url.format(abbr=spec.get('state', cache['state']), **bill)

        # Let user reference with the bill in the shell.
        self.inject(x=bill)

        if not hasattr(self, 'browser'):
            self.browser = webdriver.Firefox()
        self.browser.get(url)


def main():
    import sys
    commands = BillyCommands()
    commands.logger = logbook.Logger('fruitbat')
    metadata = None
    if 1 < len(sys.argv):
        abbr = sys.argv[1]
        commands.abbr = abbr
        metadata = mdb.metadata.find_one(abbr)
        commands.metadata = metadata

    shell = BillyShell(commands=commands)
    if metadata is not None:
        shell.metadata = metadata
    commands.inject(mdb=mdb, db=db)
    shell.interact(banner='')


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = app
from flask import Flask, render_template, request
from billy.core import settings, db
from pymongo import Connection

connection = Connection('localhost', 27017)
nudb = connection.harpy

app = Flask(__name__)


@app.route("/")
def index():
    return render_template("index.html", **{
        "subjects": settings.BILLY_SUBJECTS
    })


@app.route("/who")
def who():
    subject = request.args.get('subject', '')
    yinz = nudb.interests.find({"subjects.%s" % (subject): {"$exists": True}})
    els = [(x, db.legislators.find_one(x['_id'])) for x in yinz]
    return render_template('who.html', els=els)


if __name__ == '__main__':
    app.run(debug=True)

########NEW FILE########
__FILENAME__ = entities
import re
import os
import json
import hashlib
import logging
import datetime
import urlparse
import collections
from os.path import join, dirname, abspath
from operator import itemgetter

import pymongo

from billy.core import db
from billy.utils import metadata
from billy.core import settings

from trie_utils import Trie, trie_add, trie_scan
from utils import cd, cartcat, clean_html



# ---------------------------------------------------------------------------
#
# ---------------------------------------------------------------------------

host = settings.MONGO_HOST
port = settings.MONGO_PORT

conn = pymongo.Connection(host, port)
feed_db = conn.newsblogs


class BogusEntry(Exception):
    '''Raised when an entry lacks a required attribute, like 'link'.'''


def new_entry_id(entry, cache={}):
    '''Generate an entry id using the hash value of the title and link.
    '''
    s = (entry['link'] + entry['title']).encode('ascii', 'ignore')
    return hashlib.md5(s).hexdigest()


PATH = dirname(abspath(__file__))
DATA = settings.BILLY_DATA_DIR


class Extractor(object):

    trie_terms = {
        'legislators': cartcat(

            [u'Senator',
             u'Senate member',
             u'Senate Member',
             u'Assemblymember',
             u'Assembly Member',
             u'Assembly member',
             u'Assemblyman',
             u'Assemblywoman',
             u'Assembly person',
             u'Assemblymember',
             u'Assembly Member',
             u'Assembly member',
             u'Assemblyman',
             u'Assemblywoman',
             u'Assembly person',
             u'Representative',
             u'Rep.',
             u'Sen.',
             u'Council member',
             u'Councilman',
             u'Councilwoman',
             u'Councilperson'],

            [u' {legislator[last_name]}',
             u' {legislator[full_name]}'])

            + [u' {legislator[first_name]} {legislator[last_name]}'],

        'bills': [
            ('bill_id', lambda s: s.upper().replace('.', ''))
            ]

        }

    def __init__(self, abbr):
        self.entrycount = 0
        self.abbr = abbr
        self._assemble_ban_data()

        logger = logging.getLogger('billu.extractor.' + abbr)
        self.logger = logger

    @property
    def metadata(self):
        return metadata(self.abbr)

    def extract_bill(self, m, collection=db.bills, cache={}):
        '''Given a match object m, return the _id of the related bill.
        '''
        def squish(bill_id):
            bill_id = ''.join(bill_id.split())
            bill_id = bill_id.upper().replace('.', '')
            return bill_id

        bill_id = squish(m.group())

        try:
            ids = cache['ids']
        except KeyError:
            # Get a list of (bill_id, _id) tuples like ('SJC23', 'CAB000123')
            ids = collection.find({'state': self.abbr}, {'bill_id': 1})
            ids = dict((squish(r['bill_id']), r['_id']) for r in ids)

            # Cache it in the method.
            cache['ids'] = ids

        if bill_id in ids:
            return ids[bill_id]

    def committee_variations(self, committee):
        '''Compute likely variations for a committee
        Standing Committee on Rules
         - Rules Committee
         - Committee on Rules
         - Senate Rules
         - Senate Rules Committee
         - Rules (useless)
        '''
        name = committee['committee']
        chamber = committee['chamber']
        if chamber != 'joint':
            chamber_name = self.metadata['chambers'][chamber]['name']
        else:
            chamber_name = 'Joint'

        # Arts
        raw = re.sub(r'(Standing|Joint|Select) Committee on ', '', name)
        raw = re.sub(r'\s+Committee$', '', raw)

        # Committee on Arts
        committee_on_1 = 'Committee on ' + raw

        # Arts Committee
        short1 = raw + ' Committee'

        # Assembly Arts Committee
        if not short1.startswith(chamber_name):
            short2 = chamber_name + ' ' + short1
        else:
            short2 = short1

        # Assembly Committee on Arts
        committee_on_2 = chamber_name + ' ' + committee_on_1

        phrases = [name, committee_on_1, committee_on_2, short1, short2]

        # "select Committee weirdness"
        phrases += ['Select ' + committee_on_1]

        # Adjust for ampersand usage like "Senate Public Employment &
        # Retirement Committee"
        phrases += [p.replace(' and ', ' & ') for p in phrases]

        # Exclude phrases less than two words in length.
        return set(filter(lambda s: ' ' in s, phrases))

    @property
    def trie(self):
        try:
            return self._trie
        except AttributeError:
            return self.build_trie()

    def build_trie(self):
        '''Interpolate values from this state's mongo records
        into the trie_terms strings. Create a new list of formatted
        strings to use in building the trie, then build.
        '''
        trie = Trie()
        trie_terms = self.trie_terms
        abbr = self.abbr

        for collection_name in trie_terms:
            trie_data = []
            collection = getattr(db, collection_name)
            cursor = collection.find({'state': abbr})
            self.logger.info('compiling %d %r trie term values' % (
                cursor.count(), collection_name))

            for record in cursor:
                k = collection_name.rstrip('s')
                vals = {k: record}

                for term in trie_terms[collection_name]:

                    if isinstance(term, basestring):
                        trie_add_args = (term.format(**vals),
                                         [collection_name, record['_id']])
                        trie_data.append(trie_add_args)

                    elif isinstance(term, tuple):
                        k, func = term
                        trie_add_args = (func(record[k]),
                                         [collection_name, record['_id']])
                        trie_data.append(trie_add_args)

            self.logger.info('adding %d %s terms to the trie' % \
                (len(trie_data), collection_name))

            trie = trie_add(trie, trie_data)

        if hasattr(self, 'committee_variations'):

            committee_variations = self.committee_variations
            trie_data = []
            records = db.committees.find({'state': abbr},
                                         {'committee': 1, 'subcommittee': 1,
                                          'chamber': 1})
            self.logger.info('Computing name variations for %d records' % \
                                                            records.count())
            for c in records:
                for variation in committee_variations(c):
                    trie_add_args = (variation, ['committees', c['_id']])
                    trie_data.append(trie_add_args)

        self.logger.info('adding %d \'committees\' terms to the trie' % \
                                                            len(trie_data))

        trie = trie_add(trie, trie_data)
        self._trie = trie
        return trie

    def scan_feed(self, entries):

        for entry in entries:
            self.entrycount += 1
            yield self.scan_entry(entry)

    def scan_entry(self, entry):
        '''Test an entry against the trie to see if any entities
        are found.
        '''
        # Search the trie.
        matches = []
        try:
            summary = clean_html(entry['summary'])
        except KeyError:
            # This entry has no summary. Skip.
            return entry, []
        matches += trie_scan(self.trie, summary)

        return entry, matches

    def process_entry(self, entry):
        '''Given an entry, add a mongo id and other top-level
        attributes, then run it through scan_feed to recognize
        any entities mentioned.
        '''
        abbr = self.abbr
        third = itemgetter(2)

        entry, matches = self.scan_entry(entry)
        matches = self.extract_entities(matches)

        ids = map(third, matches)
        strings = [m.group() for m, _, _ in matches]
        assert len(ids) == len(strings)

        # Add references and save in mongo.
        entry['state'] = abbr  # list probably wiser
        entry['entity_ids'] = ids or []
        entry['entity_strings'] = strings or []
        entry['save_time'] = datetime.datetime.utcnow()

        try:
            entry['_id'] = new_entry_id(entry)
        except BogusEntry:
            # This entry appears to be malformed somehow. Skip.
            msg = 'Skipping malformed feed: %s'
            msg = msg % repr(entry)[:100] + '...'
            self.logger.info(msg)
            return

        entry['_type'] = 'feedentry'

        try:
            entry['summary'] = clean_html(entry['summary'])
        except KeyError:
            return
        try:
            entry['summary_detail']['value'] = clean_html(
                entry['summary_detail']['value'])
        except KeyError:
            pass

        # Kill any keys that contain dots.
        entry = dict((k, v) for (k, v) in entry.items() if '.' not in k)

        # Bail if the feed contains any banned key-value pairs.
        entry_set = self._dictitems_to_set(entry)
        for keyval_set in self._banned_keyvals:
            if entry_set & keyval_set:
                msg = 'Skipped story containing banned key values: %r'
                self.logger.info(msg % keyval_set)
                return

        # Skip any entries that are missing required keys:
        required = set('summary source host link published_parsed'.split())
        if required not in set(entry):
            if 'links' not in entry:
                msg = 'Skipped story lacking required keys: %r'
                self.logger.info(msg % (required - set(entry)))
                return
            else:
                source = entry['links'][-1].get('href')
                if source:
                    host = urlparse.urlparse(entry['links'][0]['href']).netloc
                    entry['source'] = source
                    entry['host'] = host
                else:
                    msg = 'Skipped story lacking required keys: %r'
                    self.logger.info(msg % (required - set(entry)))
                    return

        # Save
        msg = 'Found %d related entities in %r'
        if ids:
            self.logger.info(msg % (len(ids), entry['title']))
        else:
            self.logger.debug(msg % (len(ids), entry['title']))
        return entry
        # feed_db.entries.save(entry)

    def process_feed(self, entries):
        # Find matching entities in the feed.
        for entry, matches in self.scan_feed(entries):
            self.process_entry(entry, matches)

    def process_all_feeds(self):
        '''Note to self...possible problems with entries getting
        overwritten?
        '''
        abbr = self.abbr
        STATE_DATA = join(DATA, abbr, 'feeds')
        STATE_DATA_RAW = join(STATE_DATA, 'raw')
        _process_feed = self.process_feed

        with cd(STATE_DATA_RAW):
            for fn in os.listdir('.'):
                with open(fn) as f:
                    entries = json.load(f)
                    _process_feed(entries)

    def extract_entities(self, matches):

        funcs = {}
        for collection_name, method in (('bills', 'extract_bill'),
                                        ('legislators', 'extract_legislator'),
                                        ('committees', 'extract_committees')):

            try:
                funcs[collection_name] = getattr(self, method)
            except AttributeError:
                pass

        processed = []
        for m in matches:

            if len(m) == 2:
                match, collection_name = m
                extractor = funcs.get(collection_name)
                if extractor:
                    _id = extractor(match)
                    processed.append(m + [_id])
            else:
                processed.append(m)

        return processed

    def _assemble_ban_data(self):
        '''Go through this state's file in newblogs/skip, parse each line
        into a JSON object, and store them in the Extractor.
        '''
        here = dirname(abspath(__file__))
        skipfile = join(here,  'skip', '%s.txt' % self.abbr)
        banned_keyvals = []
        try:
            with open(skipfile) as f:
                for line in filter(None, f):
                    data = json.loads(line)
                    data = self._dictitems_to_set(data)
                    banned_keyvals.append(set(data))
        except IOError:
            pass
        self._banned_keyvals = banned_keyvals

    def _dictitems_to_set(self, dict_):
        return set((k, v) for (k, v) in dict_.items()
                   if isinstance(v, collections.Hashable))

'''
To-do:
DONE - Make trie-scan return a pseudo-match object that has same
interface as re.matchobjects.

DONE - Handle A.B. 200 variations for bills.

DONE-ish... Tune committee regexes.

- Add chambers the entry is relevant to so we can query by state
and chamber in addition to entity.

Investigate other jargon and buzz phrase usage i.e.:
 - speaker of the house
 - committee chair

'''

if __name__ == '__main__':

    import sys
    from os.path import dirname, abspath

    PATH = dirname(abspath(__file__))

    if sys.argv[1:]:
        states = sys.argv[1:]
    else:
        filenames = os.listdir(join(PATH, 'urls'))
        filenames = [s.replace('.txt', '') for s in filenames]
        states = filter(lambda s: '~' not in s, filenames)

    stats = {}
    for abbr in states:
        ex = Extractor(abbr)
        ex.process_all_feeds()
        stats[ex.abbr] = ex.entrycount

    # logger = logging.getLogger('mysql-update')
    # logger.setLevel(logging.INFO)

    # ch = logging.StreamHandler()
    # formatter = logging.Formatter('%(asctime)s - %(message)s',
    #                               datefmt='%H:%M:%S')
    # ch.setFormatter(formatter)
    # logger.addHandler(ch)

    # for abbr, count in stats.items():
    #     logger.info('%s - scanned %d feed entries' % (abbr, count))

########NEW FILE########
__FILENAME__ = models
'''The basic problem this rewrite tries to address is a lack of good reporting
on what happens during each scrape. It would be useful to know how many new
entries with relevant entities were seen on each scrape, which feeds never
have useful entries, which states or particular legislators don't have many
entries, etc. These things will help in curating and debugging the news feed
scraping so we can figure out whether some states need more links or more
specific patterns added to identify bills, committees, legislators.

Note: Script needs to blast the feed cache at the beginning of each run.

'''
import time
import datetime
import traceback
import hashlib
import shutil

import feedparser

import scrapelib
from scrapelib.cache import FileCache
from billy.core import logging
from billy.core import feeds_db


USER_AGENT = ('Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:10.0.2) '
              'Gecko/20100101 Firefox/10.0.2')

FASTMODE = True

# This assumes the script will be run with openstates as the cwd.
FEEDS_CACHE = 'cache/feeds'
ENTRIES_CACHE = 'cache/entries'


def _request_defaults(kwargs):
    '''Return defaults for a requests session.
    '''
    request_defaults = {
        #'proxies': {"http": "localhost:8001"},
        'timeout': 5.0,
        'headers': {
            'Accept': ('text/html,application/xhtml+xml,application/'
                       'xml;q=0.9,*/*;q=0.8'),
            'Accept-Encoding': 'gzip, deflate',
            'Accept-Language': 'en-us,en;q=0.5',
            'Connection': 'keep-alive',
            },
        'user_agent': USER_AGENT,
        'follow_robots': False,
        }
    request_defaults.update(kwargs)
    return request_defaults


class Feed(object):
    '''This model handles fetching the rss feed and recording any errors
    that occur for post-mortem reporting. It also has an instance-level
    report dictionary that gets augmented each time one of the feed's
    entries is scanned for relevant entities.
    '''

    request_defaults = dict(
        cache_obj=FileCache(FEEDS_CACHE),
        requests_per_minute=0,
        cache_write_only=False)

    session = scrapelib.Scraper(
        **_request_defaults(request_defaults))
    logger = logging.getLogger('billy.feed-model')

    def __init__(self, url, jurisdiction):
        self.url = url
        self.jurisdiction = jurisdiction

        self.succeeded = None
        self.default_report = {
            'entries': {
                'count': 0,
                'new': 0,
                'old': 0,
                'relevant': 0,
                },
            'entities': {
                'count' : 0,
                }
            }
        self.report = {
            'url': url,

            # The info is stored under the jurisdiction key
            # to avoid over-writing data for feeds with national scope that
            # are scanned for multiple jursidictions.
            jurisdiction: self.default_report
            }


        # Make sure this feed has a mongo id.
        self._initial_save()

    @staticmethod
    def blast_cache(self):
        '''Remove the scrapelib.Scraper fastmode cache for feed retrieval.
        Done before a scrape, but not before multiple jurisdictions in a
        single run, in case a feed of national scope needs to get processed
        for each state.
        '''
        shutil.rmtree(FEEDS_CACHE)

    def _initial_save(self):
        '''Perform the initial save (to get us the mongo_id if none exists yet.
        '''
        spec = dict(url=self.url)
        update = {'$set': spec}
        self.logger.debug('feed._initial_save %r' % self.url)
        doc = feeds_db.feeds.find_and_modify(
            spec, update, upsert=True, new=True)
        self.mongo_id = doc['_id']

    def _get_feed(self):
        '''Try to fetch the feed and parse it. If the fetch fails, log
        the exception. Finally, update the report with details of the
        success/failure of the fetch.
        '''
        try:
            text = self.session.get(self.url).text
        except Exception:
            tb = traceback.format_exc()
            self._handle_fetch_exception(tb)
            self._update_report_after_fetch()
        else:
            self.succeeded = True

            # XXX: This will fail if the text isn't a valid feed.
            data = feedparser.parse(text)
            self._data = data
            self._update_report_after_fetch()
            return data

    @property
    def data(self):
        '''The parsed feed contents.
        '''
        data = getattr(self, '_data', None)
        return data or self._get_feed() or {}

    def is_valid(self):
        '''Does this hot garbage contain the keys we expect?
        '''
        return 'title' in self.data.get('feed', {})

    def _handle_fetch_exception(self, _traceback):
        '''If the fetch fails, log the exception and store the traceback for
        the report.
        '''
        self.traceback = _traceback
        self.logger.exception(_traceback)
        self.succeeded = False

    def _update_report_after_fetch(self):
        '''Update the feed's report with whether the fetch operation
        succeeded or failed, including a formatted traceback if it failed.
        '''
        last_fetch = {
            'succeeded': self.succeeded,
            'datetime': datetime.datetime.utcnow()
            }
        if not self.succeeded:
            last_fetch['traceback'] = self.traceback
        self.report[self.jurisdiction].update(last_fetch=last_fetch)

    def entries(self):
        '''A generator of wrapped entries for this feed.
        '''
        data = self.data or {}
        entries = data.get('entries', [])
        for entry in entries:
            yield Entry(entry, feed=self)

    def serializable(self):
        '''Returns metadata about the feed (url, etc) and report information
        that can be saved in mongo.
        '''
        return {'$set': self.report}

    def finish_report(self):
        '''Extra stuff to go in the report goes here.
        '''

    def save(self):
        '''Update the feed record with the latest report.
        '''
        if not self.is_valid():
            return
        spec = dict(url=self.url)
        update = {'$set': self.report}
        self.logger.debug('feed.finish_report %r' % self.url)
        feeds_db.feeds.find_and_modify(spec, update, upsert=True, new=True)
        self.logger.info('feed.save: %r' % self.url)


class Entry(object):
    '''Wrap a parsed feed entry dictionary thingy from feedparser.
    '''
    request_defaults = dict(
        cache_obj=FileCache(ENTRIES_CACHE),
        requests_per_minute=0,
        cache_write_only=False)

    session = scrapelib.Scraper(**_request_defaults(request_defaults))
    logger = logging.getLogger('billy.entry-model')

    def __init__(self, entry, feed):
        self.entry = entry
        self.feed = feed
        self.report = {
            'entities': {
                'count' : 0,
                }
            }

        # Whether a fetch of the full text was tried and succeeded.
        self.tried = False
        self.succeeded = None

    def is_valid(self):
        '''Does this hot garbage contain the keys we expect?
        '''
        valid = set(['summary', 'link', 'title'])
        return valid < set(self.entry)

    @staticmethod
    def blast_cache(self):
        '''Just in case you want to blast the entries cache.
        '''
        shutil.rmtree(ENTRIES_CACHE)

    def mongo_id(self):
        '''Get a unique mongo id based on this entry's url and title.
        '''
        s = self.entry['link'] + self.entry['title']
        s = s.encode('ascii', 'ignore')
        return hashlib.md5(s).hexdigest()

    def is_new(self):
        '''Guess whether this entry is new (i.e., previously unseen)
        or old.
        '''
        mongo_id = self.mongo_id()
        if feeds_db.entries.find_one(mongo_id) is None:
            is_new = True
        else:
            is_new = False
        self.logger.debug('is_new? %r --> %r' % (mongo_id, is_new))
        return is_new

    def _get_full_text(self):
        '''Just for experimenting at this point. Fetch the full text,
        log any exception the occurs, and store the details regarding the
        outcome of the fetch on the object.
        '''
        self.logger.debug('entry GET %r' % self.entry.link)
        try:
            html = self.session.get(self.entry.link).text
        except Exception:
            tb = traceback.format_exc()
            self._handle_fetch_exception(tb)
            return

        self.succeeded = True
        self.tried = True
        self.html = html

        self._update_report_after_fetch()

        return html

    def _handle_fetch_exception(self, _traceback):
        '''If the fetch failed, log the failre and store the traceback
        object for the report.
        '''
        self.traceback = _traceback
        self.logger.exception(_traceback)
        self.succeeded = False

    def _update_report_after_fetch(self):
        '''After fetching the entry's full text (if at all), update
        the entry's report with the outcome of the fetch operation, including
        a traceback if it failed.
        '''
        report = {
            'url': self.url,
            'entity_count': len(self['entity_ids'])
            }

        if self.tried:
            last_fetch = {
                'succeeded': self.succeeded,
                'datetime': datetime.datetime.utcnow()
                }
            if not self.succeeded:
                last_fetch['traceback'] = self.traceback
            report.update(last_fetch=last_fetch)
        self.report.update(report)

    def serializable(self):
        '''Replace date objects with datetime objects that can be
        json serialized.
        '''
        # Add the feed's id to make the entry and its feed joinable.
        ret = {}
        ret['feed_id'] = self.feed.mongo_id

        # Convert unserializable timestructs into datetimes.
        for k, v in self.entry.items():
            if isinstance(v, time.struct_time):
                t = time.mktime(self.entry[k])
                dt = datetime.datetime.fromtimestamp(t)
                ret[k] = dt
            elif '.' not in k:
                ret[k] = v

        return ret

    def save_if_entities_found(self):
        '''If the entry is previously unseen and the extractor finds entities
        have been mentioned, save, otherwise do nothing.
        '''
        if self.is_valid() and self.is_new() and self.entry['entity_ids']:
            feeds_db.entries.save(self.serializable())
            msg = 'found %d entities: %r'
            args = (len(self.entry['entity_ids']), self.entry.link)
            self.logger.debug(msg % args)

    def finish_report(self, abbr):
        '''After attempting to extract entities, update the report and the
        report of this entry's feed with relevant information.

        Two things happen in this function: the entry's report gets updated,
        and the report object on the entry's feed gets updated.

        The feed's default report for a jurisdiction has this basic shape:
            {
            'entries': {
                'count': 0,
                'new': 0,
                'old': 0,
                'relevant': 0,
                },
            'entities': {
                'count' : 0,
                }
            }

        `abbr` is the jurisdiction abbreviation this info will be stored under
        in the feed's report object.
        '''
        # Update the feed's report.
        feed_report = self.feed.report
        report = feed_report.get(abbr, self.feed.default_report)

        report['entries']['count'] += 1

        # If this is a new entry...
        if self.is_new():
            report['entries']['new'] += 1
            if self.entry['entity_ids']:
                report['entries']['relevant'] += 1
            report['entities']['count'] += len(self.entry['entity_ids'])
            self.report['entities']['count'] += len(self.entry['entity_ids'])
        else:
            report['entries']['old'] += 1

########NEW FILE########
__FILENAME__ = scrape
import os
import sys
from os.path import dirname, abspath, join
import shutil

from billy.core import logging

from models import Feed
from entities import Extractor

if __name__ == '__main__':

    level = logging.DEBUG

    logging.getLogger('billy.feed-model').setLevel(level)
    logging.getLogger('billy.entry-model').setLevel(level)
    logging.getLogger('billu.extractor').setLevel(level)

    # The path where the news/blogs code and urls files are located.
    PATH = dirname(abspath(__file__))

    #
    filenames = os.listdir(join(PATH, 'urls'))
    filenames = filter(lambda s: '~' not in s, filenames)

    for urls_filename in filenames:
        abbr = urls_filename.lower().replace('.txt', '')

        # If abbrs are specified on the command line, scrape only those.
        if sys.argv[1:] and (abbr not in sys.argv[1:]):
            continue

        with open(join(PATH, 'urls', urls_filename)) as urls:
            urls = urls.read().splitlines()
            ignored = lambda url: not url.strip().startswith('#')
            urls = filter(ignored, urls)
            urls = filter(None, urls)

        # Path to scraped feed data for this state.
        STATE_FEED_DATA = join('data', 'feeds')

        try:
            shutil.rmtree(STATE_FEED_DATA)
        except OSError:
            pass

        try:
            os.makedirs(STATE_FEED_DATA)
        except OSError:
            pass

        extractor = Extractor(abbr)
        for url in urls:
            feed = Feed(url, jurisdiction=abbr)
            if not feed.is_valid():
                continue

            for entry in feed.entries():
                if entry.is_valid():
                    extractor.process_entry(entry.entry)
                    entry.finish_report(abbr)
                    entry.save_if_entities_found()
            feed.finish_report()
            feed.save()

########NEW FILE########
__FILENAME__ = blast_feeds_collection
from billy.core import feeds_db


if __name__ == '__main__':
    feeds_db.feeds.remove()

########NEW FILE########
__FILENAME__ = trie_utils
import functools
import re
from operator import itemgetter

from utils import CachedAttr

class Trie(dict):

    @CachedAttr
    def finditer(self):
        return functools.partial(re.finditer, '|'.join(self))


class PseudoMatch(object):
    '''A fake match object that provides the same basic interface
    as _sre.SRE_Match.'''

    def __init__(self, group, start, end):
        self._group = group
        self._start = start
        self._end = end

    def group(self):
        return self._group

    def start(self):
        return self._start

    def end(self):
        return self._end

    def _tuple(self):
        return (self._group, self._start, self._end)

    def __repr__(self):
        return 'PseudoMatch(group=%r, start=%r, end=%r)' % self._tuple()


def trie_add(trie, seq_value_2tuples, terminus=0):
    '''Given a trie (or rather, a dict), add the match terms into the
    trie.
    '''
    for seq, value in seq_value_2tuples:

        this = trie
        w_len = len(seq) - 1
        for i, c in enumerate(seq):

            if c in ",. '&[]":
                continue

            try:
                this = this[c]
            except KeyError:
                this[c] = {}
                this = this[c]

            if i == w_len:
                this[terminus] = value

    return trie


def trie_scan(trie, s,
         _match=PseudoMatch,
         second=itemgetter(1)):
    '''
    Finds all matches for `s` in `trie`.
    '''

    res = []
    match = []

    this = trie
    in_match = False

    for i, c in enumerate(s):

        if c in ",. '&[]":
            if in_match:
                match.append((i, c))
            continue

        if c in this:
            this = this[c]
            match.append((i, c))
            in_match = True
            if 0 in this:
                _matchobj = _match(group=''.join(map(second, match)),
                                   start=match[0][0], end=match[-1][0])
                res.append([_matchobj] + this[0])

        else:
            in_match = False
            if match:
                match = []

            this = trie
            if c in this:
                this = this[c]
                match.append((i, c))
                in_match = True

    # Remove any matches that are enclosed in bigger matches.
    prev = None
    for tpl in reversed(res):
        match, _, _ = tpl
        start, end = match.start, match.end

        if prev:
            a = prev._start <= match._start
            b = match._end <= prev._end
            c = match._group in prev._group
            if a and b and c:
                res.remove(tpl)

        prev = match

    return res


# def trie_scan(trie, string, _match=PseudoMatch,
#               second=itemgetter(1)):

#     this = trie
#     match = []
#     spans = []

#     for matchobj in trie.finditer(string):

#         pos = matchobj.start()
#         this = trie
#         match = []

#         while True:

#             try:
#                 char = string[pos]
#             except IndexError:
#                 break

#             if char in ",. '&[]":
#                 match.append((pos, char))
#                 pos += 1
#                 continue

#             try:
#                 this = this[char]
#             except KeyError:
#                 break
#             else:
#                 match.append((pos, char))
#                 if 0 in this:
#                     start = matchobj.start()
#                     end = pos
#                     pseudo_match = _match(group=''.join(map(second, match)),
#                                           start=start, end=end)

#                     # Don't yeild a match if this match is contained in a
#                     # larger match.
#                     _break = False
#                     for _start, _end in spans:
#                         if (_start <= start) and (end <= _end):
#                             _break = True
#                     if _break:
#                         break

#                     spans.append((start, end))
#                     yield [pseudo_match] + this[0]
#                     break
#                 else:
#                     pos += 1

########NEW FILE########
__FILENAME__ = utils
import re
import os
import contextlib
import itertools
from functools import partial
import operator


class CachedAttr(object):
    '''Computes attr value and caches it in the instance.'''

    def __init__(self, method, name=None):
        self.method = method
        self.name = name or method.__name__

    def __get__(self, inst, cls):
        if inst is None:
            return self
        result = self.method(inst)
        setattr(inst, self.name, result)
        return result


@contextlib.contextmanager
def cd(path):
    '''Creates the path if it doesn't exist'''
    old_dir = os.getcwd()
    try:
        os.makedirs(path)
    except OSError:
        pass
    os.chdir(path)
    try:
        yield
    finally:
        os.chdir(old_dir)


def cartcat(s_list1, s_list2):
    '''Given two lists of strings, take the cartesian product
    of the lists and concat each resulting 2-tuple.'''
    prod = itertools.product(s_list1, s_list2)
    return map(partial(apply, operator.add), prod)


def clean_html(html):
    """
    Remove HTML markup from the given string. Borrowed from nltk.
    """
    # First we remove inline JavaScript/CSS:
    cleaned = re.sub(r"(?is)<(script|style).*?>.*?(</\1>)", "", html.strip())
    # Then we remove html comments. This has to be done before removing regular
    # tags since comments can contain '>' characters.
    cleaned = re.sub(r"(?s)<!--(.*?)-->[\n]?", "", cleaned)
    # Next we can remove the remaining tags:
    cleaned = re.sub(r"(?s)<.*?>", " ", cleaned)
    # Finally, we deal with whitespace
    cleaned = re.sub(r"&nbsp;", " ", cleaned)
    cleaned = re.sub(r"  ", " ", cleaned)
    cleaned = re.sub(r"  ", " ", cleaned)
    return cleaned.strip()
########NEW FILE########
__FILENAME__ = build_all
'''
Build an index for each state (er, territory/place/region/shapefile)
in mongo.
'''
from subprocess import check_call
from billy import db

from .build_index import build_index

def main():
    for abbr in db.metadata.distinct('_id'):
        check_call('')

if __name__ == '__main__':
    main()
########NEW FILE########
__FILENAME__ = build_index
from os.path import join, abspath, dirname
import json

from jinja2 import Template
import jsindex
from operator import itemgetter


templates = {

    'committees': Template(
        '{{obj.committee}} {{obj.subcommittee}}'),

    'legislators': Template(
        '{{obj.full_name}} {{obj.district}} {{obj.party}}'),

    'bills': Template('''
        {{obj.bill_id}} {{obj.title}}
        {% for subject in obj.subjects %}
            {{ subject }}
        {% endfor %}
        ''')
    }

if __name__ == '__main__':
    from billy.models import db
    index = jsindex.IndexBuilder()

    cname = 'legislators'
    storekeys = ['full_name', '_type', 'chamber', 'district', 'party',
                 'state', '_id', 'photo_url']
    coll = getattr(db, cname)
    spec = {'state': 'ca', 'active': True}
    objects = coll.find(spec)
    print 'adding', objects.count(), cname, 'with spec %r' % spec
    renderer = lambda obj: templates[cname].render(obj=obj)
    index.add(cname[0], objects, renderer, all_substrs=True, storekeys=storekeys)

    cname = 'committees'
    storekeys = ['committee', 'chamber', '_type', 'state', '_id', 'members']
    coll = getattr(db, cname)
    spec = {'state': 'ca'}
    objects = coll.find(spec)
    print 'adding', objects.count(), cname, 'with spec %r' % spec
    renderer = lambda obj: templates[cname].render(obj=obj)
    index.add(cname[0], objects, renderer, all_substrs=True, storekeys=storekeys)

    spec.update(session='20112012')
    storekeys = ['bill_id', 'title', '_type', 'subjects', 'type', 'scraped_subjects',
                 'state', '_id', 'session']
    objects = db.bills.find(spec)
    print 'adding', objects.count(), 'bills', 'with spec %r' % spec
    renderer = lambda obj: templates['bills'].render(obj=obj)
    index.add('b', objects, renderer, substrs=True, storekeys=storekeys)

    jd = index.jsondata()
    js = index.as_json(showsizes=True)

    ROOT = 'build/index/'

    # I hate doing this.
    HERE = dirname(abspath(__file__))
    index_dir = join(HERE, ROOT)

    for stem, stem_id in index.stem2id.items():
        import ipdb;ipdb.set_trace()
        results = index.index[stem_id]
        second = itemgetter(2)
        types = map(second, results)
        bills_count = types.count('B')
        committees_count = types.count('C')
        legislators_count = types.count('L')
        data = dict(
            bills_count=bills_count,
            committees_count=committees_count,
            legislators_count=legislators_count,
            results=list(results))
        path = join(index_dir, stem)
        with open(path, 'w') as f:
            json.dump(data, f)

########NEW FILE########
__FILENAME__ = download
import os
import sys
import requests
from billy.models import db


def main(abbr, session=None):
    try:
        os.makedirs('billtext/%s' % abbr)
    except OSError:
        pass
    spec = {'state': abbr}
    if session is not None:
        spec.update(session=session)
    for bill in db.bills.find(spec):
        with open('billtext/%s/%s' % (abbr, bill['_id']), 'w') as f:
            try:
                url = bill['versions'][0]['url']
                print 'trying', url
                resp = requests.get(url)
            except KeyboardInterrupt:
                import pdb;pdb.set_trace()
            except Exception as e:
                print 'failed with', e
                pass
            f.write(resp.text.encode(resp.encoding))

if __name__ == '__main__':
    import sys
    if 2 < len(sys.argv):
        session = sys.argv[2]
    else:
        session = None
    main(sys.argv[1], session)
########NEW FILE########
__FILENAME__ = js
import sys
import jsonjinja
from jsonjinja.utils import get_runtime_javascript


# env = jsonjinja.Environment(loader=jsonjinja.DictLoader({
#     'results': '''\
# <div>
# {% if person.length %}
# <h2>Legislators ({{ person_count }})</h2>
# <ul>
# {% for leg in person %}
#     <li>
#     <div class='content'>
#     <i class='icon-user'></i>
#     <h3>
#         [{{leg.chamber}}] <a href='http://openstates.org/{{leg.state}}/legislators/{{leg._id}}/'>{{ leg.full_name }}</a>
#     </h3> ({{leg.party}}--{{leg.district}})
#     </div>
#     </li>
# {% endfor %}
# </ul>
# {% else %}
#  <h2>No Legislators Found</h2>
# {% endif %}

# {#
# {% if person.length %}
#     {% for leg in person %}
#     <li>
#     <i class='icon-user'></i>
#     <div class='content'>
#     <h3>{{ leg.full_name }}</h3> ({{leg.party}}--{{leg.district}})
#     <div class="row-fluid">
#         {% for role in leg.roles %}
#             {% if role.committee %}
#                 <div class="span4">{{role.committee}}</div>
#             {% endif %}
#         {% endfor %}
#     </div>
#     </div>
#     </li>
#     {% endfor %}
# {% endif %}
# #}


# {% if committee %}
# <h2>Committees ({{ committee_count }})</h2>
# <ul>
# {% for c in committee %}
#     <div class='content'>
#     <i class='icon-lock'></i>
#     <li><h3>
#         [{{c.chamber}}]
#         <a href='http://openstates.org/{{c.state}}/committees/{{c._id}}/'>{{ c.committee }}</a>
#     </h3></li>
#     </div>
# {% endfor %}
# </ul>
# {% else %}
#  <h2>No Committees Found</h2>
# {% endif %}

# {% if bill %}
# <h2>Bills ({{ bill_count }})</h2>
# <ul>
# {% for b in bill %}
#     <li>
#         <div class='content'>
#         <i class='icon-file'></i>
#         <h3>
#             <a href='http://openstates.org/{{b.state}}/bills/{{b.session}}/{{b.bill_id}}'>
#             {{ b.bill_id }}
#             </a>
#         </h3>
#         <p>{{ b.title }}</p>
#         </div>
#         <p>
#         {% for subject in b.type %}
#             <span class='label'>{{ subject }}</span>
#         {% endfor %}
#         {% for subject in b.subjects %}
#             <span class='label'>{{ subject }}</span>
#         {% endfor %}
#         {% for subject in b.scraped_subjects %}
#             <span class='label'>{{ subject }}</span>
#         {% endfor %}
#         </p>
#     </li>
# {% endfor %}
# </ul>
# </div>
# {% else %}
#  <h2>No Bills Found</h2>
# {% endif %}
# '''}))


loader = jsonjinja.FileSystemLoader('templates')
env = jsonjinja.Environment(loader=loader)

print get_runtime_javascript()
print 'jsonjinja.addTemplates('
env.compile_javascript_templates(stream=sys.stdout)
print ');'
# print 'document.write(jsonjinja.getTemplate("test.html").render({seq: ["cow", 2, 33, "pig"], title: "Jab"}));'

########NEW FILE########
__FILENAME__ = jsindex
'''
The index is a mapping of work stems to lists of objects whose attributes
contain the strings. The results are type, object pairs.

Basic workflow to build the search index:

For each term--
  - reduce it to a set of words
  - subtract stopwords
  - stem each word and add the term's id to the search index
  - add the removed tail to another set corresponding to each stem.

If the result is not huge, expand the search index to include fragments
of stemmed words as well.
'''
import sys
import json
import collections
import itertools
import time
import datetime
import re
import nltk


class IndexBuilder(object):

    def __init__(self):
        self.stem2id = {}
        self.index = collections.defaultdict(lambda: set())
        self.tails = collections.defaultdict(lambda: set())
        self.stemmer = nltk.stem.porter.PorterStemmer()
        self.stopwords = set(nltk.corpus.stopwords.words('english'))
        self.stem_word = self.stemmer.stem_word
        self.f = lambda _s: len(_s) > 1
        self.new_id = itertools.count()
        self.objects = {}

    def get_stem_id(self, stem):

        # Get the stem's id (normalize stems)
        try:
            stem_id = self.stem2id[stem]
        except KeyError:
            stem_id = self.stem2id[stem] = self.new_id.next()

        return stem_id

    def _add_word(self, word, object_id, do_stem=True):

        # Get the stem and tail.
        if do_stem:
            stem = self.stem_word(word)
            tail = word.replace(stem, '', 1)
        else:
            stem = word

        stem_id = self.get_stem_id(stem)

        # Augment the index collection.
        self.index[stem_id].add(object_id)

        # Augment the tail collection.
        if do_stem and tail:
            self.tails[stem_id].add(tail)

        # If stem diffs from word, add word as well.
        if stem != word:
            stem_id = self.get_stem_id(word)
            self.index[stem_id].add(object_id)

    def add(self, object_type, objects, text_func=None, substrs=False,
            all_substrs=False, storekeys=None):

        object_store = self.objects
        #more_text = self.more_text
        for obj in objects:

            id_ = obj['_id']
            if storekeys:
                obj_ = dict(zip(storekeys, map(obj.get, storekeys)))
            else:
                obj_ = obj
            object_store[id_] = obj_

            text = text_func(obj)

            #text += more_text(obj)

            words = set(filter(self.f, re.findall(r'\w+', text.lower())))
            words = words - self.stopwords
            add_word = self._add_word

            for w in words:

                add_word(w, id_)

                if all_substrs:
                    for substring in substrings(w):
                        add_word(substring, id_, do_stem=False)

                elif substrs:
                    for substring in substrings(w, from_beginning_only=True):
                        add_word(substring, id_, do_stem=False)

    @property
    def id2stem(self):
        return dict(t[::-1] for t in self.stem2id.items())

    def jsondata(self):

        return {
            'stem2id': self.stem2id,
            'id2stem': self.id2stem,
            'index': dict((k, list(v)) for (k, v) in self.index.items()),
            'tails': dict((k, list(v)) for (k, v) in self.tails.items()),
            'objects': self.objects
            }

    def as_json(self, showsizes=False):
        data = self.jsondata()
        if showsizes:
            from utils import humanize_bytes
            for k, v in data.items():
                js = json.dumps(v, cls=JSONDateEncoder)
                print 'size of', k, humanize_bytes(sys.getsizeof(js))
        return data

    def dump(self, fp):
        json.dump(self.jsondata(), fp, cls=JSONDateEncoder)

    def query(self, word):
        stem = self.stem_word(word)
        stem_id = self.stem2id[stem]
        return self.index[stem_id]

    def qq(self, word):
        stem = self.stem_word(word)
        stem_id = self.stem2id[stem]
        pairs = self.index[stem_id]
        objects = self.objects
        for type_, id_ in pairs:
            yield objects[type_][id_]

    def more_text(self, obj, clean_html=nltk.clean_html):
        try:
            with open('billtext/{state}/{_id}'.format(**obj)) as f:
                html = f.read().decode('utf-8')
                return clean_html(html)
        except IOError:
            return ''


class JSONDateEncoder(json.JSONEncoder):
    """
    JSONEncoder that encodes datetime objects as Unix timestamps.
    """
    def default(self, obj):
        if isinstance(obj, datetime.datetime):
            return time.mktime(obj.utctimetuple())
        elif isinstance(obj, datetime.date):
            return time.mktime(obj.timetuple())

        return json.JSONEncoder.default(self, obj)


def substrings(word, from_beginning_only=False):
    '''A generator of all substrings in `word`
    greater than 1 character in length.'''
    w_len = len(word)
    w_len_plus_1 = w_len + 1
    i = 0
    while i < w_len:
        j = i + 2
        while j < w_len_plus_1:
            yield word[i:j]
            j += 1
        if from_beginning_only:
            return
        i += 1


def trie_add(values, trie=None, terminus=0):
    '''Given a trie (or rather, a dict), add the match terms into the
    trie.
    '''
    if trie is None:
        trie = {}

    for value in values:

        this = trie
        w_len = len(value) - 1
        for i, c in enumerate(value):

            try:
                this = this[c]
            except KeyError:
                this[c] = {}
                this = this[c]

            if i == w_len:
                this[terminus] = value

    return trie

########NEW FILE########
__FILENAME__ = utils
from __future__ import division


def humanize_bytes(bytes, precision=1):
    """Return a humanized string representation of a number of bytes.

    Assumes `from __future__ import division`.

    >>> humanize_bytes(1)
    '1 byte'
    >>> humanize_bytes(1024)
    '1.0 kB'
    >>> humanize_bytes(1024*123)
    '123.0 kB'
    >>> humanize_bytes(1024*12342)
    '12.1 MB'
    >>> humanize_bytes(1024*12342,2)
    '12.05 MB'
    >>> humanize_bytes(1024*1234,2)
    '1.21 MB'
    >>> humanize_bytes(1024*1234*1111,2)
    '1.31 GB'
    >>> humanize_bytes(1024*1234*1111,1)
    '1.3 GB'
    """
    abbrevs = (
        (1 << 50L, 'PB'),
        (1 << 40L, 'TB'),
        (1 << 30L, 'GB'),
        (1 << 20L, 'MB'),
        (1 << 10L, 'kB'),
        (1, 'bytes')
    )
    if bytes == 1:
        return '1 byte'
    for factor, suffix in abbrevs:
        if bytes >= factor:
            break
    return '%.*f %s' % (precision, bytes / factor, suffix)

## end of http://code.activestate.com/recipes/577081/ }}}
########NEW FILE########
__FILENAME__ = bills
import re
import datetime

import lxml.html

from billy.scrape import NoDataForPeriod
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote


class AKBillScraper(BillScraper):
    jurisdiction = 'ak'

    _fiscal_dept_mapping = {
        'ADM': 'Administration',
        'CED': 'Commerce, Community & Economic Development',
        'COR': 'Corrections',
        'CRT': 'Court System',
        'EED': 'Education and Early Development',
        'DEC': 'Environmental Conservation ',
        'DFG': 'Fish and Game',
        'GOV': "Governor's Office",
        'DHS': 'Health and Social Services',
        'LWF': 'Labor and Workforce Development',
        'LAW': 'Law',
        'LEG': 'Legislative Agency',
        'MVA': "Military and Veterans' Affairs",
        'DNR': 'Natural Resources',
        'DPS': 'Public Safety',
        'REV': 'Revenue',
        'DOT': 'Transportation and Public Facilities',
        'UA': 'University of Alaska',
        'ALL': 'All Departments'}

    _comm_vote_type = {
        'DP': 'Do Pass',
        'DNP': 'Do Not Pass',
        'NR': 'No Recommendation',
        'AM': 'Amend'}

    _comm_mapping = {
        'CRA': 'Community & Regional Affairs',
        'EDC': 'Education',
        'FIN': 'Finance',
        'HSS': 'Health & Social Services',
        'JUD': 'Judiciary',
        'L&C': 'Labor & Commerce',
        'RES': 'Resources',
        'RLS': 'Rules',
        'STA': 'State Affairs',
        'TRA': 'Transportation',
        'EDT': 'Economic Development, Trade & Tourism',
        'ENE': 'Energy',
        'FSH': 'Fisheries',
        'MLV': 'Military & Veterans',
        'WTR': 'World Trade',
        'ARR': 'Administrative Regulation Review',
        'ASC': 'Armed Services Committee',
        'BUD': 'Legislative Budget & Audit',
        'ECR': 'Higher Education/Career Readiness Task Force',
        'EFF': 'Education Fuding District Cost Factor Committee',
        'ETH': 'Select Committee on Legislative Ethics',
        'LEC': 'Legislative Council',
    }

    _comm_re = re.compile(r'^(%s)\s' % '|'.join(_comm_mapping.keys()))
    _current_comm = None

    def scrape(self, chamber, session):
        if chamber == 'upper':
            bill_abbrs = ('SB', 'SR', 'SCR', 'SJR')
        elif chamber == 'lower':
            bill_abbrs = ('HB', 'HR', 'HCR', 'HJR')
        bill_types = {'B': 'bill', 'R': 'resolution', 'JR': 'joint resolution',
                      'CR': 'concurrent resolution'}

        for abbr in bill_abbrs:
            bill_type = bill_types[abbr[1:]]
            bill_list_url = ('http://www.legis.state.ak.us/basis/range_multi'
                             '.asp?session=%s&bill1=%s1&bill2=%s999' %
                             (session, abbr, abbr)
                            )
            doc = lxml.html.fromstring(self.urlopen(bill_list_url))
            doc.make_links_absolute(bill_list_url)
            for bill_link in doc.xpath('//table[@align="center"]//tr/td[1]//a'):
                bill_url = bill_link.get('href')
                bill_id = bill_link.text.replace(' ', '')
                self.scrape_bill(chamber, session, bill_id, bill_type,
                                 bill_url)


    def scrape_bill(self, chamber, session, bill_id, bill_type, url):
        doc = lxml.html.fromstring(self.urlopen(url))
        doc.make_links_absolute(url)

        title = doc.xpath('//b[text()="TITLE:"]')
        if title:
            title = title[0].tail.strip()
        else:
            self.warning("skipping bill %s, no information" % url)
            return

        bill = Bill(session, chamber, bill_id, title, type=bill_type)
        bill.add_source(url)

        # Get sponsors
        spons_str = doc.xpath('//b[contains(text(), "SPONSOR")]')[0].tail.strip()
        sponsors_match = re.match(
            '(SENATOR|REPRESENTATIVE)\([Ss]\) ([^,]+(,[^,]+){0,})',
            spons_str)
        if sponsors_match:
            sponsors = sponsors_match.group(2).split(',')
            sponsor = sponsors[0].strip()

            if sponsor:
                bill.add_sponsor('primary', sponsors[0])

            for sponsor in sponsors[1:]:
                sponsor = sponsor.strip()
                if sponsor:
                    bill.add_sponsor('cosponsor', sponsor)
        else:
            # Committee sponsorship
            spons_str = spons_str.strip()

            if re.match(r' BY REQUEST OF THE GOVERNOR$', spons_str):
                spons_str = re.sub(r' BY REQUEST OF THE GOVERNOR$',
                                   '', spons_str).title()
                spons_str = (spons_str +
                             " Committee (by request of the governor)")

            if spons_str:
                bill.add_sponsor('primary', spons_str)

        # Get actions from second myth table
        self._current_comm = None
        act_rows = doc.xpath('(//table[@class="myth"])[2]//tr')[1:]
        for row in act_rows:
            date, journal, raw_chamber, action = row.xpath('td')

            act_date = datetime.datetime.strptime(date.text_content().strip(),
                                                  '%m/%d/%y')
            raw_chamber = raw_chamber.text_content().strip()
            action = action.text_content().strip()

            if raw_chamber == "(H)":
                act_chamber = "lower"
            elif raw_chamber == "(S)":
                act_chamber = "upper"

            if re.match("\w+ Y(\d+)", action):
                vote_href = journal.xpath('.//a/@href')
                if vote_href:
                    self.parse_vote(bill, action, act_chamber, act_date,
                                    vote_href[0])

            action, atype = self.clean_action(action)

            match = re.match('^Prefile released (\d+/\d+/\d+)$', action)
            if match:
                action = 'Prefile released'
                act_date = datetime.datetime.strptime(match.group(1),
                                                '%m/%d/%y')

            bill.add_action(act_chamber, action, act_date, type=atype)

        # Get subjects
        bill['subjects'] = []
        for subj in doc.xpath('//a[contains(@href, "subject")]/text()'):
            bill['subjects'].append(subj.strip())

        # Get versions
        text_list_url = "http://www.legis.state.ak.us/"\
            "basis/get_fulltext.asp?session=%s&bill=%s" % (
            session, bill_id)
        bill.add_source(text_list_url)

        text_doc = lxml.html.fromstring(self.urlopen(text_list_url))
        text_doc.make_links_absolute(text_list_url)
        for link in text_doc.xpath('//a[contains(@href, "get_bill_text")]'):
            name = link.xpath('../preceding-sibling::td/text()')[0].strip()
            text_url = link.get('href')
            bill.add_version(name, text_url, mimetype="text/html")

        # Get documents
        doc_list_url = "http://www.legis.state.ak.us/"\
                "basis/get_documents.asp?session=%s&bill=%s" % (
                    session, bill_id )
        doc_list = lxml.html.fromstring(self.urlopen(doc_list_url))
        doc_list.make_links_absolute(doc_list_url)
        bill.add_source(doc_list_url)
        for href in doc_list.xpath('//a[contains(@href, "get_documents")][@onclick]'):
            h_name = href.text_content()
            h_href = href.attrib['href']
            bill.add_document(h_name, h_href)


        self.save_bill(bill)

    def parse_vote(self, bill, action, act_chamber, act_date, url,
        re_vote_text = re.compile(r'The question (?:being|to be reconsidered):\s*"(.*?\?)"', re.S),
        re_header=re.compile(r'\d{2}-\d{2}-\d{4}\s{10,}\w{,20} Journal\s{10,}\d{,6}\s{,4}')):

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        # Find all chunks of text representing voting reports.
        votes_text = doc.xpath('//pre')[1].text_content()
        votes_text = re_vote_text.split(votes_text)
        votes_data = zip(votes_text[1::2], votes_text[2::2])

        # Process each.
        for motion, text in votes_data:

            yes = no = other = 0

            tally = re.findall(r'\b([YNEA])[A-Z]+:\s{,3}(\d{,3})', text)
            for vtype, vcount in tally:
                vcount = int(vcount) if vcount != '-' else 0
                if vtype == 'Y':
                    yes = vcount
                elif vtype == 'N':
                    no = vcount
                else:
                    other += vcount

            vote = Vote(act_chamber, act_date, motion, yes > no, yes, no, other)

            # In lengthy documents, the "header" can be repeated in the middle
            # of content. This regex gets rid of it.
            vote_lines = re_header.sub('', text)
            vote_lines = vote_lines.split('\r\n')

            vote_type = None
            for vote_list in vote_lines:
                if vote_list.startswith('Yeas: '):
                    vote_list, vote_type = vote_list[6:], vote.yes
                elif vote_list.startswith('Nays: '):
                    vote_list, vote_type = vote_list[6:], vote.no
                elif vote_list.startswith('Excused: '):
                    vote_list, vote_type = vote_list[9:], vote.other
                elif vote_list.startswith('Absent: '):
                    vote_list, vote_type = vote_list[9:], vote.other
                elif vote_list.strip() == '':
                    vote_type = None
                if vote_type:
                    for name in vote_list.split(','):
                        name = name.strip()
                        if name:
                            vote_type(name)

            vote.add_source(url)
            bill.add_vote(vote)

    def clean_action(self, action):
        # Clean up some acronyms
        match = re.match(r'^FN(\d+): (ZERO|INDETERMINATE)?\((\w+)\)', action)
        if match:
            num = match.group(1)

            if match.group(2) == 'ZERO':
                impact = 'No fiscal impact'
            elif match.group(2) == 'INDETERMINATE':
                impact = 'Indeterminate fiscal impact'
            else:
                impact = ''

            dept = match.group(3)
            dept = self._fiscal_dept_mapping.get(dept, dept)

            action = "Fiscal Note %s: %s (%s)" % (num, impact, dept)

        match = self._comm_re.match(action)
        if match:
            self._current_comm = match.group(1)

        match = re.match(r'^(DP|DNP|NR|AM):\s(.*)$', action)
        if match:
            vtype = self._comm_vote_type[match.group(1)]

            action = "%s %s: %s" % (self._current_comm, vtype,
                                    match.group(2))

        match = re.match(r'^COSPONSOR\(S\): (.*)$', action)
        if match:
            action = "Cosponsors added: %s" % match.group(1)

        match = re.match('^([A-Z]{3,3}), ([A-Z]{3,3})$', action)
        if match:
            action = "REFERRED TO %s and %s" % (
                self._comm_mapping[match.group(1)],
                self._comm_mapping[match.group(2)])

        match = re.match('^([A-Z]{3,3})$', action)
        if match:
            action = 'REFERRED TO %s' % self._comm_mapping[action]

        match = re.match('^REFERRED TO (.*)$', action)
        if match:
            comms = match.group(1).title().replace(' And ', ' and ')
            action = "REFERRED TO %s" % comms

        action = re.sub(r'\s+', ' ', action)

        action = action.replace('PREFILE RELEASED', 'Prefile released')

        atype = []
        if 'READ THE FIRST TIME' in action:
            atype.append('bill:introduced')
            atype.append('bill:reading:1')
            action = action.replace('READ THE FIRST TIME',
                                'Read the first time')
        if 'READ THE SECOND TIME' in action:
            atype.append('bill:reading:2')
            action = action.replace('READ THE SECOND TIME',
                                'Read the second time')
        if 'READ THE THIRD TIME' in action:
            atype.append('bill:reading:3')
            action = action.replace('READ THE THIRD TIME',
                                'Read the third time')
        if 'TRANSMITTED TO GOVERNOR' in action:
            atype.append('governor:received')
            action = action.replace('TRANSMITTED TO GOVERNOR',
                                'Transmitted to Governor')
        if 'SIGNED INTO LAW' in action:
            atype.append('governor:signed')
            action = action.replace('SIGNED INTO LAW', 'Signed into law')
        if 'Do Pass' in action:
            atype.append('committee:passed')
        if 'Do Not Pass' in action:
            atype.append('committee:failed')
        if action.startswith('PASSED'):
            atype.append('bill:passed')
        if 'REFERRED TO' in action:
            atype.append('committee:referred')
            action = action.replace('REFERRED TO', 'Referred to')

        return action, atype

########NEW FILE########
__FILENAME__ = committees
from billy.scrape.committees import CommitteeScraper, Committee

import lxml.html


class AKCommitteeScraper(CommitteeScraper):
    jurisdiction = 'ak'

    def scrape(self, chamber, term):
        url = ("http://www.legis.state.ak.us/basis/commbr_info.asp"
               "?session=%s" % term)

        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        chamber_abbrev = {'upper': 'S', 'lower': 'H'}[chamber]

        for link in page.xpath("//a[contains(@href, 'comm=')]"):
            name = link.text.strip().title()

            if name.startswith('Conference Committee'):
                continue

            url = link.attrib['href']
            if ('comm=%s' % chamber_abbrev) in url:
                self.scrape_committee(chamber, name, url)

    def scrape_committee(self, chamber, name, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)

        if page.xpath("//h3[. = 'Joint Committee']"):
            chamber = 'joint'

        subcommittee = page.xpath("//h3[@align='center']/text()")[0]
        if not "Subcommittee" in subcommittee:
            subcommittee = None

        comm = Committee(chamber, name, subcommittee=subcommittee)
        comm.add_source(url)

        for link in page.xpath("//a[contains(@href, 'member=')]"):
            member = link.text.strip()

            mtype = link.xpath("string(../preceding-sibling::td[1])")
            mtype = mtype.strip(": \r\n\t").lower()

            comm.add_member(member, mtype)

        if not comm['members']:
            self.warning('not saving %s, appears to be empty' % name)
        else:
            self.save_committee(comm)

########NEW FILE########
__FILENAME__ = events
import re
import datetime

from billy.scrape import NoDataForPeriod
from billy.scrape.events import Event, EventScraper

import pytz
import lxml.html

exclude_slugs = [ "TBA" ]
formats = [
    "%b %d %A %I:%M %p %Y",
    "%B %d %A %I:%M %p %Y"
]

replacements = {
    "Sept" : "Sep"
}

now = datetime.datetime.now()

class AKEventScraper(EventScraper):
    jurisdiction = 'ak'

    _tz = pytz.timezone('US/Alaska')

    def scrape(self, chamber, session):
        if session != '28':
            raise NoDataForPeriod(session)

        if chamber == 'other':
            return

        year = now.year

        # Full calendar year
        date1 = '0101' + str(year)[2:]
        date2 = '1231' + str(year)[2:]

        url = ("http://www.legis.state.ak.us/basis/"
               "get_hearing.asp?session=%s&Chamb=B&Date1=%s&Date2=%s&"
               "Comty=&Root=&Sel=1&Button=Display" % (
                   session, date1, date2))

        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        path = "//font[starts-with(., '(H)') or starts-with(., '(S)')]"
        for font in page.xpath(path):
            match = re.match(r'^\((H|S)\)(.+)$', font.text)

            chamber = {'H': 'lower', 'S': 'upper'}[match.group(1)]
            comm = match.group(2).strip().title()

            next_row = font.xpath("../../following-sibling::tr[1]")[0]

            when = next_row.xpath("string(td[1]/font)").strip()
            when = re.sub("\s+", " ", when)
            when = "%s %s" % (when, year)

            continu = False
            for slug in exclude_slugs:
                if slug in when:
                    continu = True

            for repl in replacements:
                if repl in when:
                    when = when.replace(repl, replacements[repl])

            if continu:
                continue

            parsed_when = None
            for fmt in formats:
                try:
                    parsed_when = datetime.datetime.strptime(when, fmt)
                    break
                except ValueError:
                    pass

            if not parsed_when:
                raise

            when = parsed_when
            if when < now:
                self.warning("Dropping an event at %s. Be careful!" % (
                    when
                ))
                continue

            when = self._tz.localize(when)

            where = next_row.xpath("string(td[2]/font)").strip()

            description = "Committee Meeting\n"
            description += comm

            links = font.xpath(
                "../../td/font/a[contains(@href, 'get_documents')]")
            if links:
                agenda_link = links[0]
                event['link'] = agenda_link.attrib['href']

            cur_node = font.getparent().getparent()
            bills = []
            while cur_node is not None and cur_node.xpath(".//hr") == []:
                bills += cur_node.xpath(
                    ".//a[contains(@href, 'get_complete_bill')]/text()")
                cur_node = cur_node.getnext()


            event = Event(session, when, 'committee:meeting',
                          description, location=where)

            event.add_source(url)
            for bill in bills:
                event.add_related_bill(bill,
                                       description='Related Bill',
                                       type='consideration')

            event.add_participant('host',
                                  comm,
                                  participant_type='committee',
                                  chamber=chamber)
            self.save_event(event)

########NEW FILE########
__FILENAME__ = legislators
import re

from billy.scrape import NoDataForPeriod
from billy.scrape.legislators import LegislatorScraper, Legislator

import lxml.html


class AKLegislatorScraper(LegislatorScraper):
    jurisdiction = 'ak'
    latest_only = True

    def scrape(self, chamber, term):
        if chamber == 'upper':
            chamber_abbr = 'S'
            url = 'http://senate.legis.state.ak.us/'
            search = 'senator'
        else:
            chamber_abbr = 'H'
            url = 'http://house.legis.state.ak.us/'
            search = 'rep'

        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        seen = set()
        for link in page.xpath("//a[contains(@href, '%s')]" % search):
            name = link.text_content()
            # Members of the leadership are linked twice three times:
            # one image link and two text links. Don't double/triple
            # scrape them
            if not name or link.attrib['href'] in seen:
                continue
            seen.add(link.attrib['href'])

            self.scrape_legislator(chamber, term,
                                   link.xpath('string()').strip(),
                                   link.attrib['href'])

    def scrape_address(self, leg, doc, id):
        text = doc.xpath('//div[@id="{0}"]'.format(id))[0].text_content()
        address = ''
        for line in text.splitlines():
            if line == 'Session Contact':
                addr_type = 'capitol'
                addr_name = 'Session Office'
            elif line == 'Interim Contact':
                addr_type = 'district'
                addr_name = 'Interim Office'
            elif line.startswith('Phone:'):
                phone = line.split(': ')[-1]
            elif line.startswith('Fax:'):
                fax = line.split(': ')[-1]
            elif line:
                address += (line + '\n')

        kwargs = {}
        if fax:
            kwargs['fax'] = fax
        if phone:
            kwargs['phone'] = phone

        if address.strip() != ',':
            leg.add_office(addr_type, addr_name, address=address.strip(),
                           **kwargs)


    def scrape_legislator(self, chamber, term, name, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)

        name = re.sub(r'\s+', ' ', name)

        info = page.xpath('string(//div[@id = "fullpage"])')

        district = re.search(r'District: ([\w\d]+)', info)
        if district is None:
            maddr = page.xpath("//div[@id='fullpage']//a[contains(@href, 'mailto')]")
            if maddr == []:
                return   # Needed for http://senate.legis.state.ak.us/senator.php?id=cog ..
            maddr = maddr[0]
            district = maddr.getnext().tail
            # This hack needed for http://house.legis.state.ak.us/rep.php?id=dru
            # please remove as soon as this is alive.
        else:
            district = district.group(1)

        party = re.search(r'Party: (.+)', info).group(1).strip()
        email = re.search(r'Email: ([\w_]+@legis\.state\.ak\.us)',
                          info)

        if email is None:
            email = re.search(r'Email: (.+@akleg\.gov)',
                              info)

        email = email.group(1)

        # for consistency
        if party == 'Democrat':
            party = 'Democratic'

        leg = Legislator(term, chamber, district, name, party=party,
                         email=email, url=url)
        self.scrape_address(leg, page, 'bioleft')
        self.scrape_address(leg, page, 'bioright')
        leg.add_source(url)

        self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = bills
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

import re
import datetime
import lxml.html

_action_re = (
    ('Introduced', 'bill:introduced'),
    ('(Forwarded|Delivered) to Governor', 'governor:received'),
    ('Amendment (?:.*)Offered', 'amendment:introduced'),
    ('Substitute (?:.*)Offered', 'amendment:introduced'),
    ('Amendment (?:.*)adopted', 'amendment:passed'),
    ('Amendment lost', 'amendment:failed'),
    ('Read for the first time and referred to',
       ['bill:reading:1', 'committee:referred']),
    ('(r|R)eferred to', 'committee:referred'),
    ('Read for the second time', 'bill:reading:2'),
    ('(S|s)ubstitute adopted', 'bill:substituted'),
    ('(m|M)otion to Adopt (?:.*)adopted', 'amendment:passed'),
    ('(m|M)otion to (t|T)able (?:.*)adopted', 'amendment:passed'),
    ('(m|M)otion to Adopt (?:.*)lost', 'amendment:failed'),
    ('(m|M)otion to Read a Third Time and Pass adopted', 'bill:passed'),
    ('(m|M)otion to Concur In and Adopt adopted', 'bill:passed'),
    ('Third Reading Passed', 'bill:passed'),
    ('Reported from', 'committee:passed'),
    ('Indefinitely Postponed', 'bill:failed'),
    ('Passed Second House', 'bill:passed'),
    # memorial resolutions can pass w/o debate
    ('Joint Rule 11', ['bill:introduced', 'bill:passed']),
    ('Lost in', 'bill:failed'),
    ('Favorable from', 'committee:passed:favorable'),
)

def _categorize_action(action):
    for pattern, types in _action_re:
        if re.findall(pattern, action):
            return types
    return 'other'

class ALBillScraper(BillScraper):

    jurisdiction = 'al'

    def refresh_session(self):
        url = ('http://alisondb.legislature.state.al.us/acas/ACASLoginFire.asp'
               '?SESSION=%s') % self.session_id
        html = self.urlopen(url)

    def scrape(self, chamber, session):
        self.session_id = self.metadata['session_details'][session]['internal_id']
        self.base_doc_url = 'http://alisondb.legislature.state.al.us/acas/searchableinstruments/%s/PrintFiles/' % session

        chamber_piece = {'upper': 'Senate',
                         'lower': 'House+of+Representatives'}[chamber]
        # resolutions
        res_url = ('http://alisondb.legislature.state.al.us/acas/SESSResosBySe'
                   'lectedMatterTransResults.asp?WhichResos=%s&TransCodes='
                   '{All}&LegDay={All}') % chamber_piece
        self.scrape_for_bill_type(chamber, session, res_url)

        bill_url = ('http://alisondb.legislature.state.al.us/acas/SESSBillsByS'
                    'electedMatterTransResults.asp?TransCodes={All}'
                    '&LegDay={All}&WhichBills=%s') % chamber_piece
        self.scrape_for_bill_type(chamber, session, bill_url)


    def scrape_for_bill_type(self, chamber, session, url):

        self.refresh_session()

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        # bills are all their own table with cellspacing=4 (skip first)
        bill_tables = doc.xpath('//table[@cellspacing="4"]')
        for bt in bill_tables[1:]:

            # each table has 3 rows: detail row, description, blank
            details, desc, _ = bt.xpath('tr')

            # first <tr> has img, button, sponsor, topic, current house
            #   current status, committee, committee2, last action
            tds = details.xpath('td')
            if len(tds) == 9:
                # middle _, _ is chamber, last action
                _, button, sponsor, subject, _, _, com1, com2, _ = tds
            elif len(tds) == 8:
                # middle _ is last action
                _, button, sponsor, subject, _, com1, com2, _ = tds
            else:
                self.warning('invalid row: (tds=%s) %s', len(tds),
                             details.text_content())
                continue

            # contains script tag that has a document.write that writes the
            # bill_id, we have to pull that out (gross, but only way)
            script_text = button.text_content()
            # skip SBIR/HBIR
            if 'SBIR' in script_text or 'HBIR' in script_text:
                continue

            """ script text looks like:
               document.write("<input type=button id=BTN71139 name=BTN71139 style='font-weight:normal' value='SB1'");
               document.write(" onClick=\"javascript:instrumentSelected(this,'71139','SB1','ON','ON','ON','");
               document.write(status + "','OFF','SB1-int.pdf,,','SB1-int.pdf,,')\">");
            """

            oid, bill_id, fnotes = re.findall(r"instrumentSelected\(this,'(\d+)','(\w+)','ON','ON','(ON|OFF)'",
                                              script_text)[0]
            second_piece = re.findall(r"status \+ \"','(ON|OFF)','([^,]*),([^,]*),([^,]*)\'", script_text)
            if second_piece:
                amend, intver, engver, enrver = second_piece[0]
            else:
                intver = engver = enrver = None

            sponsor = sponsor.text_content()
            subject = subject.text_content()
            com1 = com1.text_content()
            com2 = com2.text_content()
            desc = desc.text_content()

            if 'B' in bill_id:
                bill_type = 'bill'
            elif 'JR' in bill_id:
                bill_type = 'joint resolution'
            elif 'R' in bill_id:
                bill_type = 'resolution'

            # title is missing on a few bills
            title = desc.strip()
            if not title:
                return

            # create bill
            bill = Bill(session, chamber, bill_id, title, type=bill_type)
            if subject:
                bill['subjects'] = [subject]

            if fnotes == 'ON':
                bill.add_document('fiscal notes', 'http://alisondb.legislature.state.al.us/acas/ACTIONFiscalNotesFrameMac.asp?OID=%s&LABEL=%s' %
                                  (oid, bill_id))

            self.get_sponsors(bill, oid)
            self.get_actions(bill, oid)

            # craft bill URLs
            if intver:
                bill.add_version('introduced', self.base_doc_url + intver,
                                 mimetype='application/pdf')
            if engver:
                bill.add_version('engrossed', self.base_doc_url + engver,
                                 mimetype='application/pdf')
            if enrver:
                bill.add_version('enrolled', self.base_doc_url + enrver,
                                 mimetype='application/pdf')

            self.save_bill(bill)


    def get_actions(self, bill, oid):
        url = 'http://alisondb.legislature.state.al.us/acas/ACTIONHistoryResultsMac.asp?OID=%s&LABEL=%s' % (oid, bill['bill_id'])

        bill.add_source(url)

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        for row in doc.xpath('//tr[@valign="top"]'):
            # date, amend/subst, matter, committee, nay, yea, abs, vote
            tds = row.xpath('td')

            # only change date if it exists (actions w/o date get old date)
            if tds[0].text_content():
                date = datetime.datetime.strptime(tds[0].text_content(),
                                                  '%m/%d/%Y')
            scraped_chamber = tds[1].text_content().strip()
            if scraped_chamber == 'S':
                action_chamber = 'upper'
            elif scraped_chamber == 'H':
                action_chamber = 'lower'
            else:
                action_chamber = 'executive'

            amendment = tds[2].xpath('.//input/@value')
            if amendment:
                amendment = amendment[0]
                bill.add_document('amendment ' + amendment,
                                  self.base_doc_url + amendment + '.pdf')
            else:
                amendment = None

            action = tds[3].text_content().strip()

            if action:
                atype = _categorize_action(action)
                bill.add_action(action_chamber, action, date,
                                type=atype, amendment=amendment)

            # pulling values out of javascript
            vote_button = tds[-1].xpath('input')
            if vote_button:
                vote_js = vote_button[0].get('onclick')
                moid, vote, body, inst = re.match(".*\('(\d+)','(\d+)','(\d+)','(\w+)'", vote_js).groups()
                self.scrape_vote(bill, moid, vote, body, inst, action,
                                 action_chamber)



    def get_sponsors(self, bill, oid):
        url = "http://alisondb.legislature.state.al.us/acas/ACTIONSponsorsResultsMac.asp?OID=%s&LABEL=%s" % (oid, bill['bill_id'])

        bill.add_source(url)

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        # primary sponsors
        for cs in doc.xpath('//table[2]/tr/td[1]/table/tr/td/text()'):
            if cs:
                bill.add_sponsor('primary', cs)
        # cosponsors in really weird table layout (likely to break)
        for cs in doc.xpath('//table[2]/tr/td[2]/table/tr/td/text()'):
            if cs:
                bill.add_sponsor('cosponsor', cs)


    def scrape_vote(self, bill, moid, vote_id, body, inst, motion, chamber):
        url = "http://alisondb.legislature.state.al.us/acas/GetRollCallVoteResults.asp?MOID=%s&VOTE=%s&BODY=%s&INST=%s&SESS=%s" % (
            moid, vote_id, body, inst, self.session_id)
        doc = lxml.html.fromstring(self.urlopen(url))

        voters = {'Y': [], 'N': [], 'P': [], 'A': []}

        leg_tds = doc.xpath('//td[@width="33%"]')
        for td in leg_tds:
            name = td.text
            two_after = td.xpath('following-sibling::td')[1].text
            if name == 'Total Yea:':
                total_yea = int(two_after)
            elif name == 'Total Nay:':
                total_nay = int(two_after)
            elif name == 'Total Abs:':
                total_abs = int(two_after)
            elif name == 'Legislative Date:':
                vote_date = datetime.datetime.strptime(two_after, '%m/%d/%Y')
            # lines to ignore
            elif name in ('Legislative Day:', 'Vote ID:'):
                pass
            elif 'Vacant' in name:
                pass
            else:
                # add legislator to list of voters
                voters[two_after].append(name)

        # TODO: passed is faked
        total_other = total_abs + len(voters['P'])
        vote = Vote(chamber, vote_date, motion, total_yea > total_nay,
                    total_yea, total_nay, total_other)
        vote.add_source(url)
        for member in voters['Y']:
            vote.yes(member)
        for member in voters['N']:
            vote.no(member)
        for member in (voters['A'] + voters['P']):
            vote.other(member)

        bill.add_vote(vote)

########NEW FILE########
__FILENAME__ = legislators
import re
from billy.scrape.legislators import LegislatorScraper, Legislator

import lxml.html

class ALLegislatorScraper(LegislatorScraper):
    jurisdiction = 'al'

    def scrape(self, chamber, term):
        urls = {'upper': 'http://www.legislature.state.al.us/senate/senators/senateroster_alpha.html',
                'lower': 'http://www.legislature.state.al.us/house/representatives/houseroster_alpha.html'}
        party_dict = {'(D)': 'Democratic', '(R)': 'Republican', 
                      '(I)': 'Independent'}

        url = urls[chamber]

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        for row in doc.xpath('//strong[starts-with(text(), "MEMBERS")]/following-sibling::table/tr')[1:]:
            name, party, district, office, phone = row.getchildren()

            # if the name column contains a link it isn't vacant
            link = name.xpath('a')
            if link and link[0].text.lower() != 'vacant':
                name = name.text_content().strip()

                party = party_dict[party.text_content().strip()]
                district = district.text_content().strip().replace('(', '').replace(')', '')
                office = office.text_content().strip()
                phone = phone.text_content().strip()
                leg_url = link[0].get('href')

                office_address = 'Room %s\n11 S. Union Street\nMontgomery, AL 36130' % office

                leg = Legislator(term, chamber, district, name,
                                 party=party, url=leg_url)
                if leg_url.startswith('http://'):
                    self.get_details(leg, term, leg_url)
                leg.add_office('capitol', 'Capitol Office',
                               address=office_address, phone=phone)

                leg.add_source(url)
                self.save_legislator(leg)

    def get_details(self, leg, term, url):
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        photo = doc.xpath('//img[@height="250"]/@src')
        if photo:
            leg['photo_url'] = photo[0]
        email = doc.xpath('//a[starts-with(@href, "mailto")]/@href')
        if email:
            leg['email'] = email[0].strip('mailto:')

        # find lis after the strong (or p) containing Committees
        coms = doc.xpath('//strong[contains(text(), "Committees")]/following::li')
        if not coms:
            coms = doc.xpath('//p[contains(text(), "Committees")]/following::li')
        if not coms:
            self.warning('no committees?')
        for com in coms:
            com = com.text_content()
            if '(' in com:
                com, position = com.split('(')
                position = position.replace(')', '').lower().strip()
            else:
                position = 'member'
            com = com.replace('Committee', '')
            # fix their nonsense
            com = com.replace(" No.","")
            com = com.replace("Veterans Affairs", "Veterans' Affairs")
            com = com.replace("Boards, Agencies, and", "Boards, Agencies and")
            com = com.replace("Means / Education", "Means Education")
            com = com.replace("Ethics, and Elections", "Ethics and Elections")
            com = com.replace("Taxation, ", "Taxation ")
            com = com.replace("Municiple ", "Municipal ")
            com = com.strip()
            leg.add_role('committee member', term=leg['roles'][0]['term'],
                         chamber=leg['roles'][0]['chamber'], committee=com,
                         position=position)
        leg.add_source(url)

########NEW FILE########
__FILENAME__ = bills
import re
import csv
import StringIO
import datetime

from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

import lxml.html


def unicode_csv_reader(unicode_csv_data, dialect=csv.excel, **kwargs):
    # csv.py doesn't do Unicode; encode temporarily as UTF-8:
    csv_reader = csv.reader(utf_8_encoder(unicode_csv_data),
                            dialect=dialect, **kwargs)
    for row in csv_reader:
        # decode UTF-8 back to Unicode, cell by cell:
        yield [unicode(cell, 'utf-8') for cell in row]


def utf_8_encoder(unicode_csv_data):
    for line in unicode_csv_data:
        yield line.encode('utf-8')


class ARBillScraper(BillScraper):
    jurisdiction = 'ar'

    def scrape(self, chamber, session):
        self.bills = {}

        self.slug = self.metadata['session_details'][session]['slug']

        self.scrape_bill(chamber, session)
        self.scrape_actions()

        for bill in self.bills.itervalues():
            self.save_bill(bill)

    def scrape_bill(self, chamber, session):
        url = "ftp://www.arkleg.state.ar.us/dfadooas/LegislativeMeasures.txt"
        page = self.urlopen(url)
        page = unicode_csv_reader(StringIO.StringIO(page), delimiter='|')

        for row in page:
            bill_chamber = {'H': 'lower', 'S': 'upper'}[row[0]]
            if bill_chamber != chamber:
                continue

            bill_id = "%s%s %s" % (row[0], row[1], row[2])

            type_spec = re.match(r'(H|S)([A-Z]+)\s', bill_id).group(2)
            bill_type = {
                'B': 'bill',
                'R': 'resolution',
                'JR': 'joint resolution',
                'CR': 'concurrent resolution',
                'MR': 'memorial resolution',
                'CMR': 'concurrent memorial resolution'}[type_spec]

            if row[-1] != self.slug:
                continue

            bill = Bill(session, chamber, bill_id, row[3], type=bill_type)
            bill.add_source(url)

            primary = row[11]
            if not primary:
                primary = row[12]
            bill.add_sponsor('primary', primary)

            version_url = ("ftp://www.arkleg.state.ar.us/Bills/"
                           "%s/Public/%s.pdf" % (
                               session, bill_id.replace(' ', '')))
            bill.add_version(bill_id, version_url, mimetype='application/pdf')

            self.scrape_bill_page(bill)

            self.bills[bill_id] = bill

    def scrape_actions(self):
        url = "ftp://www.arkleg.state.ar.us/dfadooas/ChamberActions.txt"
        page = self.urlopen(url)
        page = csv.reader(StringIO.StringIO(page))

        for row in page:
            bill_id = "%s%s %s" % (row[1], row[2], row[3])

            if bill_id not in self.bills:
                continue
            # different term
            if row[-2] != self.slug:
                continue

            # Commas aren't escaped, but only one field (the action) can
            # contain them so we can work around it by using both positive
            # and negative offsets
            bill_id = "%s%s %s" % (row[1], row[2], row[3])
            actor = {'HU': 'lower', 'SU': 'upper'}[row[-5].upper()]
            date = datetime.datetime.strptime(row[6], "%Y-%m-%d %H:%M:%S")
            action = ','.join(row[7:-5])

            action_type = []
            if action.startswith('Filed'):
                action_type.append('bill:introduced')
            elif (action.startswith('Read first time') or
                  action.startswith('Read the first time')):
                action_type.append('bill:reading:1')
            if re.match('Read the first time, .*, read the second time', action):
                action_type.append('bill:reading:2')
            elif action.startswith('Read the third time and passed'):
                action_type.append('bill:passed')
                action_type.append('bill:reading:3')
            elif action.startswith('Read the third time'):
                action_type.append('bill:reading:3')
            elif action.startswith('DELIVERED TO GOVERNOR'):
                action_type.append('governor:received')

            if 'referred to' in action:
                action_type.append('committee:referred')

            if 'Returned by the Committee' in action:
                if 'recommendation that it Do Pass' in action:
                    action_type.append('committee:passed:favorable')
                else:
                    action_type.append('committee:passed')

            if re.match(r'Amendment No\. \d+ read and adopted', action):
                action_type.append('amendment:introduced')
                action_type.append('amendment:passed')

            if not action:
                action = '[No text provided]'
            self.bills[bill_id].add_action(actor, action, date,
                                           type=action_type or ['other'])

    def scrape_bill_page(self, bill):
        # We need to scrape each bill page in order to grab associated votes.
        # It's still more efficient to get the rest of the data we're
        # interested in from the CSVs, though, because their site splits
        # other info (e.g. actions) across many pages
        for t in self.metadata['terms']:
            if bill['session'] in t['sessions']:
                term_year = t['start_year']
                break
        measureno = bill['bill_id'].replace(' ', '')
        url = ("http://www.arkleg.state.ar.us/assembly/%s/%s/"
               "Pages/BillInformation.aspx?measureno=%s" % (
                   term_year, self.slug, measureno))
        bill.add_source(url)

        page = lxml.html.fromstring(self.urlopen(url))
        page.make_links_absolute(url)

        for link in page.xpath("//a[contains(@href, 'Amendments')]"):
            num = link.xpath("string(../../td[2])")
            name = "Amendment %s" % num
            bill.add_document(name, link.attrib['href'])

        try:
            cosponsor_link = page.xpath(
                "//a[contains(@href, 'CoSponsors')]")[0]
            self.scrape_cosponsors(bill, cosponsor_link.attrib['href'])
        except IndexError:
            # No cosponsor link is OK
            pass

    #     hist_link = page.xpath("//a[contains(@href, 'BillStatusHistory')]")[0]
    #     self.scrape_votes(bill, hist_link.attrib['href'])

    # def scrape_votes(self, bill, url):
    #     page = lxml.html.fromstring(self.urlopen(url))
    #     page.make_links_absolute(url)

        for link in page.xpath("//a[contains(@href, 'votes.aspx')]"):
            date = link.xpath("string(../../td[2])")
            date = datetime.datetime.strptime(date, "%m/%d/%Y %I:%M:%S %p")

            motion = link.xpath("string(../../td[3])")

            self.scrape_vote(bill, date, motion, link.attrib['href'])

    def scrape_vote(self, bill, date, motion, url):
        page = self.urlopen(url)

        if 'not yet official' in page:
            # Sometimes they link to vote pages before they go live
            return

        page = lxml.html.fromstring(page)

        if url.endswith('Senate'):
            actor = 'upper'
        else:
            actor = 'lower'

        count_path = "string(//td[@align = 'center' and contains(., '%s: ')])"
        yes_count = int(page.xpath(count_path % "Yeas").split()[-1])
        no_count = int(page.xpath(count_path % "Nays").split()[-1])
        other_count = int(page.xpath(count_path % "Non Voting").split()[-1])
        other_count += int(page.xpath(count_path % "Present").split()[-1])

        passed = yes_count > no_count + other_count
        vote = Vote(actor, date, motion, passed, yes_count,
                    no_count, other_count)
        vote.add_source(url)

        xpath = (
            '//*[contains(@class, "ms-standardheader")]/'
            'following-sibling::table')
        divs = page.xpath(xpath)
        votevals = 'yes no other other'.split()
        for (voteval, div) in zip(votevals, divs):
            for a in div.xpath('.//a'):
                name = a.text_content().strip()
                if not name:
                    continue
                getattr(vote, voteval)(name)
        bill.add_vote(vote)

    def scrape_cosponsors(self, bill, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)

########NEW FILE########
__FILENAME__ = committees
import re

from billy.utils import urlescape
from billy.scrape.committees import CommitteeScraper, Committee

import lxml.html

COMM_TYPES = {'joint': 'Joint',
              # 'task_force': 'Task Force',
              'upper': 'Senate',
              'lower': 'House'}


class ARCommitteeScraper(CommitteeScraper):
    jurisdiction = 'ar'
    latest_only = True

    _seen = set()

    def scrape(self, chamber, term):

        # Get start year of term.
        for termdict in self.metadata['terms']:
            if termdict['name'] == term:
                break
        start_year = termdict['start_year']

        base_url = ("http://www.arkleg.state.ar.us/assembly/%s/%sR/"
                    "Pages/Committees.aspx?committeetype=")
        base_url = base_url % (start_year, start_year)

        for chamber, url_ext in COMM_TYPES.iteritems():
            chamber_url = urlescape(base_url + url_ext)
            page = self.urlopen(chamber_url)
            page = lxml.html.fromstring(page)
            page.make_links_absolute(chamber_url)

            for a in page.xpath('//td[@class="dxtl dxtl__B0"]/a'):
                if a.attrib.get('colspan') == '2':
                    # colspan=2 signals a subcommittee, but it's easier
                    # to pick those up from links on the committee page,
                    # so we do that in scrape_committee() and skip
                    # it here
                    continue

                name = re.sub(r'\s*-\s*(SENATE|HOUSE)$', '', a.text).strip()

                comm_url = urlescape(a.attrib['href'])
                if chamber == 'task_force':
                    chamber = 'joint'

                self.scrape_committee(chamber, name, comm_url)

    def _fix_committee_case(self, subcommittee):
        '''Properly capitalize the committee name.
        '''
        subcommittee = re.sub(
            r'^(HOUSE|SENATE|JOINT)\s+', '', subcommittee.strip().title())

        # Fix roman numeral capping.
        replacer = lambda m: m.group().upper()
        subcommittee = re.sub(
            r'\b(VII|VII|III|IIV|IV|II|VI|IX|V|X)\b', replacer, subcommittee,
            flags=re.I)
        return subcommittee

    def _fix_committee_name(self, committee, parent=None, subcommittee=False):
        '''Remove committee acronyms from committee names. We want to
        remove them from the beginning of names, and at the end:

        ARKANSAS LEGISLATIVE COUNCIL (ALC)
        ALC/EXECUTIVE SUBCOMMITTEE
        ALC-ADMINISTRATIVE RULES & REGULATIONS

        Also strip the main committee name from the committee.
        '''
        junk = ['ALC', 'ALC-JBC', 'JBC', 'JPR']

        if subcommittee:
            junk += [
                'AGING\s+&\s+LEGISLATIVE\s+AFFAIRS\s*-',
                'AGRICULTURE\s*-',
                'CITY, COUNTY & LOCAL AFFAIRS\s*-',
                'EDUCATION\s*-',
                'JUDICIARY\s*-',
                'PUBLIC HEALTH\s*-',
                'STATE AGENCIES & GOV. AFFAIRS\s*-',
                'TRANSPORTATION\s*-',
                ]

        if parent:
            junk += [parent]

        committee = committee.strip()
        for rgx in sorted(junk, key=len, reverse=True):
            match = re.match(rgx, committee, flags=re.I)
            if match:
                committee = committee.replace(match.group(), '', 1).strip()
                break

        # Kill "(ALC)" in "Blah Blah (ALC)"
        rgx = '\(?(%s)\)?' % '|'.join(sorted(junk, key=len, reverse=True))
        committee = re.sub(rgx, '', committee, flags=re.I)
        committee = committee.strip(' \/-\n\r\t')

        # Fix commas with no space.
        def replacer(matchobj):
            return matchobj.group(1) + ', ' + matchobj.group(2)
        rgx = r'(?i)([a-z]),([a-z])'
        committee = re.sub(rgx, replacer, committee)

        # Fix subcom.
        committee = committee.replace('Subcom.', 'Subcommittee')
        return committee

    def scrape_committee(self, chamber, name, url, subcommittee=None):
        name = self._fix_committee_name(name)
        name = self._fix_committee_case(name)

        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        # Get the subcommittee name.
        xpath = '//div[@class="ms-WPBody"]//table//tr/td/b/text()'

        if subcommittee:
            subcommittee = page.xpath(xpath)
            if subcommittee:
                subcommittee = page.xpath(xpath).pop(0)
                subcommittee = self._fix_committee_name(
                    subcommittee, parent=name, subcommittee=True)
                subcommittee = self._fix_committee_case(subcommittee)
            else:
                subcommittee = None

        # Dedupe.
        if (chamber, name, subcommittee) in self._seen:
            return
        self._seen.add((chamber, name, subcommittee))

        comm = Committee(chamber, name, subcommittee=subcommittee)
        comm.add_source(url)

        for tr in page.xpath('//table[@class="dxgvTable"]/tr[position()>1]'):
            if tr.xpath('string(td[1])').strip():
                mtype = tr.xpath('string(td[1])').strip()
            else:
                mtype = 'member'

            member = tr.xpath('string(td[3])').split()
            title = member[0]
            member = ' '.join(member[1:])

            if title == 'Senator':
                mchamber = 'upper'
            elif title == 'Representative':
                mchamber = 'lower'
            else:
                # skip non-legislative members
                continue

            comm.add_member(member, mtype, chamber=mchamber)

        for a in page.xpath('//ul/li/a'):
            sub_name = a.text.strip()
            sub_url = urlescape(a.attrib['href'])
            self.scrape_committee(chamber, name, sub_url,
                                  subcommittee=sub_name)

        if not comm['members']:
            self.warning('not saving empty committee %s' % name)
        else:
            self.save_committee(comm)

########NEW FILE########
__FILENAME__ = events
import re
import csv
import datetime

try:
    import cStringIO as StringIO
except ImportError:
    import StringIO

from billy.scrape import NoDataForPeriod
from billy.scrape.events import EventScraper, Event

# ftp://www.arkleg.state.ar.us/dfadooas/ReadMeScheduledMeetings.txt
TIMECODES = {
    "12:34 PM": "Upon Recess of the House",
    "12:36 PM": "10 Minutes Upon Adjournment of",
    "12:37 PM": "Upon Adjournment of Afternoon Joint Budget Committee",
    "12:38 PM": "15 Minutes Upon Adjournment of Senate",
    "12:39 PM": "15 Minutes Upon Adjournment of House",
    "12:40 PM": "Upon Adjournment of Senate",
    "12:41 PM": "Upon Adjournment of House",
    "12:42 PM": "Upon Adjournment of",
    "12:43 PM": "Upon Adjournment of Both Chambers",
    "12:44 PM": "10 Minutes upon Adjournment",
    "12:46 PM": "Upon Adjournment of House Rules",
    "12:47 PM": "Rescheduled",
    "12:48 PM": "Upon Adjournment of Joint Budget",
    "12:49 PM": "15 Minutes upon Adjournment",
    "12:50 PM": "30 Minutes upon Adjournment",
    "12:51 PM": "1 Hour prior to Senate convening",
    "12:52 PM": "1 Hour prior to House convening",
    "12:53 PM": "30 Minutes prior to Senate convening",
    "12:54 PM": "30 Minutes prior to House convening",
    "12:55 PM": "Meeting Cancelled",
    "12:56 PM": "No Meeting Scheduled",
    "12:57 PM": "Call of Chair",
    "12:58 PM": "To Be Announced",
    "12:59 PM": "Upon Adjournment",
}


class AREventScraper(EventScraper):
    jurisdiction = 'ar'

    def scrape(self, chamber, session):
        if chamber == 'other':
            return

        url = "ftp://www.arkleg.state.ar.us/dfadooas/ScheduledMeetings.txt"
        page = self.urlopen(url)
        page = csv.reader(StringIO.StringIO(page.bytes), delimiter='|')

        for row in page:
            desc = row[7].strip()

            match = re.match(r'^(.*)- (HOUSE|SENATE)$', desc)
            if match:
                comm_chamber = {'HOUSE': 'lower',
                                'SENATE': 'upper'}[match.group(2)]
                if comm_chamber != chamber:
                    continue

                comm = match.group(1).strip()
                comm = re.sub(r'\s+', ' ', comm)
                location = row[5].strip() or 'Unknown'
                when = datetime.datetime.strptime(row[2], '%Y-%m-%d %H:%M:%S')

                event = Event(session, when, 'committee:meeting',
                              "%s MEETING" % comm,
                              location=location)
                event.add_source(url)

                event.add_participant('host', comm, 'committee',
                                      chamber=chamber)

                time = row[3].strip()
                if time in TIMECODES:
                    event['notes'] = TIMECODES[time]

                self.save_event(event)

########NEW FILE########
__FILENAME__ = legislators
import re

from billy.utils import urlescape
from billy.scrape import NoDataForPeriod
from billy.scrape.legislators import LegislatorScraper, Legislator

import lxml.html


class ARLegislatorScraper(LegislatorScraper):
    jurisdiction = 'ar'
    latest_only = True

    def scrape(self, chamber, term):

        # Get start year of term.
        for termdict in self.metadata['terms']:
            if termdict['name'] == term:
                break
        start_year = termdict['start_year']

        url = ('http://www.arkleg.state.ar.us/assembly/%s/%sR/Pages/'
               'LegislatorSearchResults.aspx?member=&committee=All&chamber=')
        url = url % (start_year, start_year)
        page = self.urlopen(url)
        root = lxml.html.fromstring(page)

        for a in root.xpath('//table[@class="dxgvTable"]'
                            '/tr[contains(@class, "dxgvDataRow")]'
                            '/td[1]/a'):
            member_url = urlescape(a.attrib['href'])
            self.scrape_member(chamber, term, member_url)

    def scrape_member(self, chamber, term, member_url):
        page = self.urlopen(member_url)
        root = lxml.html.fromstring(page)

        name_and_party = root.xpath(
            'string(//td[@class="SiteNames"])').split()

        title = name_and_party[0]
        if title == 'Representative':
            chamber = 'lower'
        elif title == 'Senator':
            chamber = 'upper'

        full_name = ' '.join(name_and_party[1:-1])

        party = name_and_party[-1]
        if party == '(R)':
            party = 'Republican'
        elif party == '(D)':
            party = 'Democratic'
        elif party == '(G)':
            party = 'Green'
        else:
            raise Exception('unknown party: %s' % party)

        img = root.xpath('//img[@class="SitePhotos"]')[0]
        photo_url = img.attrib['src']

        # Need to figure out a cleaner method for this later
        info_box = root.xpath('string(//table[@class="InfoTable"])')
        district = re.search(r'District(.+)\r', info_box).group(1)

        leg = Legislator(term, chamber, district, full_name, party=party,
                         photo_url=photo_url, url=member_url)
        leg.add_source(member_url)

        try:
            phone = re.search(r'Phone(.+)\r', info_box).group(1)
        except AttributeError:
            phone = None
        address = root.xpath('//nobr/text()')[0].replace(u'\xa0', ' ')
        leg.add_office('district', 'District Office', address=address,
                       phone=phone)

        try:
            leg['email'] = re.search(r'Email(.+)\r', info_box).group(1)
        except AttributeError:
            pass

        try:
            leg['occupation'] = re.search(
                r'Occupation(.+)\r', info_box).group(1)
        except AttributeError:
            pass

        self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = action_utils
###########################################################
# committee actions
###########################################################

committee_actions = {
    ("REREF GOVOP", "REREF JUD",
     "REREF WM","DISC/S/C")          : "committee:referred",
    ("FAILED",)                      : "committee:failed",
    ("PASSED", "C&P", "PFC",
     "PFC W/FL")                     : "committee:passed",
    ("C&P AS AM BY AP",
     "C&P AS AM BY EN", "C&P AS AM BY APPROP",
     "C&P AS AM BY GO", "C&P AS AM BY HE",
     "C&P AS AM BY JU", "C&P AS AM BY TR",
     "C&P AS AM BY WM", "C&P AS AM BY GOVOP") : "committee:passed", # "committee:amended"],
    ("AMEND C&P", "AM C&P ON RECON",
     "AM C&P ON REREF", "PFCA W/FL",
     "PFCA")                         : "committee:passed", # "committee:amended"
    ("DP", "DP ON RECON",
     "DP ON REREFER", "DP W/MIN RPT",
     "DP/PFC", "DPA/PFC W/FL",
     "DP/PFCA")                      : "committee:passed:favorable",
    ("DPA", "DPA CORRECTED",
     "DPA ON RECON", "DPA ON REREFER",
     "DPA/PFC", "DPA/PFC W/FL",
     "DPA/PFCA", "DPA/PFCA W/FL"): "committee:passed:favorable", # "committee:amended"],
    ("DNP",)                         : "committee:passed:unfavorable",
    ("DPA/SE", "DPA/SE ON RECON",
     "DPA/SE ON REREF")              : ["committee:passed:favorable",
                                        "bill:substituted"],
    ("DISC/HELD", "HELD ON RECON",
     "HELD", "HELD 1 WK",
     "HELD INDEF")                   : "other", # "committee:held"]
}
###########################################################
# Generic actions hopefully classified correctly
###########################################################

generic_actions= {
    ("introduced",)                 : "bill:introduced",
    ("PASSED",)                     : "bill:passed",
    #there are several different fail motions so if the action contains 'FAILED'
    ("FAILED",)                     : "bill:failed",

    ("VETO OVERRIDE: PASSED",)      : "bill_veto_override:passed",
    ("VETO OVERRIDE: FAILED",)      : "bill_veto_override:failed",

    ("TRANSMITTED TO: GOVERNOR")    : "governor:received",
    ("SIGNED",)                     : "governor:signed",
    ("VETOED",)                     : "governor:vetoed",

    ("AMENDMENT: INTRODUCED",)      : "amendment:introduced",
    ("AMENDMENT: PASSED",)          : "amendment:passed",
    ("AMENDMENT", "FAILED")         : "amendment:failed",
    ("FURTHER AMENDED")             : "amendment:amended",
#    ("AMENDMENT", "WITHDRAWN")       : "amendement:withdrawn",
    ("REREF GOVOP", "REREF JUD",
     "REREF WM", "DISC/S/C",
     "REC REREF TO COM",
     "RECOMMIT TO COM")             : "committee:referred",
    # THIRD READ AND FINAL READ
    ("THIRD READ:",)                : "bill:reading:3",
    ("DPA/SE", "DPA/SE ON RECON",
     "DPA/SE ON REREF")              : "bill:substituted",
    ("HOUSE FINAL READ:",
     "SENATE FINAL READ:")          : "other",
}

###########################################################
# get_action_type()
###########################################################
def get_action_type(abbrv, group=None):
    """
    best attempt at classifying committee actions
    """
    if group == 'COMMITTEES:':
        actions = committee_actions
    else:
        actions = generic_actions

    for key in actions:
        if abbrv in key:
            return actions[key]
    return 'other'

###########################################################
# get_action
###########################################################
def get_verbose_action(abbr):
    """
    get_action('PFCA W/FL') -->
    'proper for consideration amended with recommendation for a floor amendment'
    """
    try:
        return common_abbrv[abbr]
    except KeyError:
        return abbr

###########################################################
# annottated abbreviations
###########################################################

common_abbrv = {
    # amended
    'AM C&P ON RECON': 'amended constitutional and in proper form on reconsideration',
    'AM C&P ON REREF': 'amended constitutional and in proper form on rereferral',
    'AMEND C&P': 'amended constitutional and in proper form',

    'C&P': 'constitutional and in proper form',
    # amended
    'C&P AS AM BY AP': 'constitutional and in proper form as amended by the committee on App',
    'C&P AS AM BY APPR': 'constitutional and in proper form as amended by Appropriations',
    'C&P AS AM BY EN': 'constitutional and in proper form as amended by the Committee on ENV',
    'C&P AS AM BY GO': 'constitutional and in proper form as amended by GovOp',
    'C&P AS AM BY HE': 'constitutional and in proper form as amended by HE',
    'C&P AS AM BY JU': 'constitutional and in proper form as amended by Jud',
    'C&P AS AM BY TR': 'constitutional and in proper form as amended by TR',
    'C&P AS AM BY WM': 'constitutional and in proper form as amended by the Committee on Way & Means',
    'C&P AS AM GOVOP': 'Constitutional and in proper form as amended by Government Operations',
    # reconsideration
    'C&P ON RECON': 'constitutional and in proper form on reconsideration',
    'C&P ON REREF': 'constitutional and in proper form on rereferral',
    # amended
    'C&P W/FL': 'constitutional and in proper form with a floor amendment',
    # caucus actions are ceremonial and not actually part of the legislative process
    # but if neither caucus concurs the likelyhood of a bill getting to the committee
    # of the whole or third read is not good.
    'CAUCUS': 'Caucus',
    #
    'CONCUR': 'recommend to concur',
    'CONCUR FAILED': 'motion to concur failed',
    'DISC PETITION': 'discharge petition',
    'DISC/HELD': 'discussed and held',
    'DISC/ONLY': 'discussion only',
    'DISC/S/C': 'discussd and assigned to subcommittee',
    # committee:passed:unfavorable
    'DNP': 'do not pass',
    # committee:passed:favorable from here down a ways
    'DP': 'do pass',
    'DP ON RECON': 'do pass on reconsideration',
    'DP ON REREFER': 'do passed on rereferral',
    'DP W/MIN RPT': 'do pass with minority report',
    'DP/PFC': 'do pass and proper for consideration',
    'DP/PFC W/FL': 'do pass and proper for consideration with recommendation for a floor amendment',
    'DP/PFCA': 'do pass and proper for consideration amended',
    # favorable and amended from here down #
    'DPA': 'do pass amended',
    'DPA CORRECTED': 'do pass amended corrected',
    'DPA ON RECON': 'do pass amended on reconsideration',
    'DPA ON REREFER': 'do pass amended on rereferral',
    'DPA/PFC': 'do pass amended and proper for consideration',
    'DPA/PFC W/FL': 'do pass amended and proper for consideration with recommendation for a floor amendment',
    'DPA/PFCA': 'do pass amended and proper for consideration amended',
    'DPA/PFCA W/FL': 'do pass amended and proper for consideration with recommendation for a floor amendment',
    # strike everything is like amended in the nature of a substitue
    'DPA/SE': 'do pass amended/strike-everything',
    'DPA/SE CORRECTED': 'do pass amended/strike everything corrected',
    'DPA/SE ON RECON': 'do pass amended/strike everything on reconsideration',
    'DPA/SE ON REREF': 'do pass amended/strike everything on rereferral',
    # failed #
    'FAILED': 'failed to pass',
    'FAILED BY S/V 0': 'failed by standing vote', # does this mean there is no vote?
    'FAILED ON RECON': 'failed on reconsideration', # after a succesful motion to reconsider
    # in the house a bill is first read and assigned to committee #
    'FIRST': 'First Reading',
    # amendment amended??? or just another amendment? #
    'FURTHER AMENDED': 'further amended',

    'HELD': 'held',
    'HELD 1 WK': 'held one week',
    'HELD INDEF': 'held indefinitely',
    'HELD ON RECON': 'held on reconsideration',

    'None': 'No Action',

    'NOT CONCUR': 'rec not concur',
    # not read?
    'NOT HEARD': 'not heard',

    'NOT IN ORDER': 'not in order',

    'PASSED': 'Passed',
    # rules committee action #
    'PFC': 'proper for consideration',

    'PFC W/FL': 'proper for consideration with recommendation for a floor amendment',
    'PFCA': 'proper for consideration amended',
    'PFCA W/FL': 'proper for consideration amended with recommendation for a floor amendment',

    'POSTPONE INDEFI': 'postponed indefinitely',
    # vote actions
    'REC REREF TO COM': 'recommend rereferral to committee',
    'RECOMMIT TO COM': 'recommit to committee',
    # not sure when this would take place? I would like to see it in action
    'REMOVAL REQ': 'removal request from Rules Committee',
    # committee:refered
    'REREF GOVOP': 'rereferred to GovOp',
    'REREF JUD': 'rereferred to Judiciary',
    'REREF WM': 'rereferred to Ways & Means',

    'RET FOR CON': 'returned for consideration',

    'RET ON CAL': 'retained on the Calendar',
    'RETAINED': 'retained',

    'RULE 8J PROPER': 'proper legislation and deemed not derogatory or insulting',

    'S/C': 'subcommittee',
    'S/C REPORTED': 'subcommittee reported',

    'W/D': 'withdrawn',
}

########NEW FILE########
__FILENAME__ = bills
import re

from billy.scrape import NoDataForPeriod
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote
from . import utils
from .action_utils import get_action_type, get_verbose_action

from lxml import html

BASE_URL = 'http://www.azleg.gov/'

# map to type and official_type (maybe we can figure out PB/PZ and add names
SPONSOR_TYPES = {'P': ('primary', 'P'),
                 'C': ('cosponsor', 'C'),
                 'PB': ('primary', 'PB'),
                 'PZ': ('primary', 'PZ'),
                 'CZ': ('cosponsor', 'CZ')}

# This string of hot garbage appears when a document hasn't been posted yet.
hot_garbage_404_fail = ('The Requested Document Has Not Been '
                        'Posted To The Web Site Yet.')

class AZBillScraper(BillScraper):
    def accept_response(self, response):
        normal = super(AZBillScraper, self).accept_response(response)
        return normal or response.status_code == 500

    """
    Arizona Bill Scraper.
    """
    jurisdiction = 'az'
    def get_session_id(self, session):
        """
        returns the session id for a given session
        """
        return self.metadata['session_details'][session]['session_id']

    def scrape_bill(self, chamber, session, bill_id):
        """
        Scrapes documents, actions, vote counts and votes for
        a given bill.
        """
        session_id = self.get_session_id(session)
        url = BASE_URL + 'DocumentsForBill.asp?Bill_Number=%s&Session_ID=%s' % (
                                           bill_id.replace(' ', ''), session_id)

        docs_for_bill = self.urlopen(url)

        if hot_garbage_404_fail in docs_for_bill:
            # Bailing here will prevent the bill from being saved, which
            # occurs in the scrape_actions method below.
            return

        root = html.fromstring(docs_for_bill)
        bill_title = root.xpath(
                        '//div[@class="ContentPageTitle"]')[1].text.strip()
        b_type = utils.get_bill_type(bill_id)
        bill = Bill(session, chamber, bill_id, bill_title, type=b_type)
        bill.add_source(url)
        path = '//tr[contains(td/font/text(), "%s")]'
        link_path = '//tr[contains(td/a/@href, "%s")]'
        link_path2 = '//tr[contains(td/font/a/@href, "%s")]'
        # versions
        for href in root.xpath("//a[contains(@href, 'pdf')]"):
            version_url = href.attrib['href']
            if "bills" in version_url.lower():
                name = list(href.getparent().getparent().getparent())
                name = name[1].text_content()
                bill.add_version(href.text_content(), version_url,
                                 on_duplicate='use_old',
                                 mimetype='application/pdf')

        #fact sheets and summary
        rows = root.xpath(link_path2 % '/summary/')
        for row in rows:
            tds = row.xpath('td')
            fact_sheet = tds[1].text_content().strip()
            fact_sheet_url = tds[1].xpath('string(font/a/@href)') or \
                             tds[2].xpath('string(font/a/@href)')
            bill.add_document(fact_sheet, fact_sheet_url, type="summary")

        #agendas
        # skipping revised, cancelled, date, time and room from agendas
        # but how to get the agenda type cleanly? meaning whether it is
        # house or senate?
        rows = root.xpath(link_path % '/agendas')
        for row in rows:
            tds = row.xpath('td')
            agenda_committee = tds[0].text_content().strip()
            agenda_html = tds[7].xpath('string(a/@href)').strip()
            if agenda_html == '':
                agenda_html = tds[6].xpath('string(a/@href)').strip()
            bill.add_document(agenda_committee, agenda_html)

        # House Calendars
        # skipping calendar number, modified, date
        rows = root.xpath(link_path % '/calendar/h')
        for row in rows:
            tds = row.xpath('td')
            calendar_name = tds[0].text_content().strip()
            calendar_html = tds[5].xpath('string(a/@href)')
            bill.add_document(calendar_name, calendar_html,
                              type='house calendar')
        # Senate Calendars
        # skipping calendar number, modified, date
        rows = root.xpath(link_path % '/calendar/s')
        for row in rows:
            tds = row.xpath('td')
            calendar_name = tds[0].text_content().strip()
            calendar_html = tds[5].xpath('string(a/@href)')
            bill.add_document(calendar_name, calendar_html,
                              type='senate calendar')
        # amendments
        rows = root.xpath(path % 'AMENDMENT:')
        for row in rows:
            tds = row.xpath('td')
            amendment_title = tds[1].text_content().strip()
            amendment_link = tds[2].xpath('string(font/a/@href)')
            bill.add_document(amendment_title, amendment_link,
                              type='amendment')

        # videos
        # http://azleg.granicus.com/MediaPlayer.php?view_id=13&clip_id=7684
        rows = root.xpath(link_path % '&clip_id')
        for row in rows:
            tds = row.xpath('td')
            video_title = tds[1].text_content().strip()
            video_link = tds[2].xpath('string(a/@href)')
            video_date = tds[0].text_content().strip()
            bill.add_document(video_title, video_link, date=video_date,
                              type='video')

        self.scrape_actions(chamber, session, bill)

    def scrape_actions(self, chamber, session, bill):
        """
        Scrape the actions for a given bill
        """
        ses_num = utils.legislature_to_number(session)
        bill_id = bill['bill_id'].replace(' ', '')
        action_url = BASE_URL + 'FormatDocument.asp?inDoc=/legtext/%s/bills/%so.asp' % (ses_num, bill_id.lower())
        action_page = self.urlopen(action_url)

        if hot_garbage_404_fail in action_page:
            # This bill has no actions yet, but that
            # happened frequently with pre-filed bills
            # before the 2013 session, so skipping probably
            # isn't the thing to do.
            self.save_bill(bill)
            return

        bill.add_source(action_url)
        root = html.fromstring(action_page)
        base_table = root.xpath('//table[@class="ContentAreaBackground"]')[0]
        # common xpaths
        table_path = '//table[contains(tr/td/b/text(), "%s")]'

        #sponsors
        sponsors = base_table.xpath('//sponsor')
        for sponsor in sponsors:
            name = sponsor.text.strip()
            # sponsor.xpath('string(ancestor::td[1]/following-sibling::td[1]/text())').strip()
            s_type = sponsor.getparent().getparent().getnext().text_content().strip()
            s_type, o_type = SPONSOR_TYPES[s_type]
            bill.add_sponsor(s_type, name, official_type=o_type)

        #titles
        table = base_table.xpath(table_path % 'TITLE')
        if table:
            for row in table[0].iterchildren('tr'):
                title = row[1].text_content().strip()
                if title != bill['title']:
                    bill.add_title(title)

        for table in base_table.xpath('tr/td/table'):
            action = table.xpath('string(tr[1]/td[1])').strip()
            if action == '':
                action = table.xpath('string(tr[1])').strip()
            if (action.endswith('FIRST READ:') or
                action.endswith('SECOND READ:') or 'WAIVED' in action):

                rows = table.xpath('tr')
                for row in rows:
                    action = row[0].text_content().strip()[:-1]
                    actor = 'lower' if action.startswith('H') else 'upper'
                    date = utils.get_date(row[1])
                    # bill:introduced
                    if (action.endswith('FIRST READ') or
                        action.endswith('FIRST WAIVED')):
                        if actor == chamber:
                            a_type = ['bill:introduced', 'bill:reading:1']
                        else:
                            a_type = 'bill:reading:1'
                        bill.add_action(actor, action, date, type=a_type)
                    else:
                        a_type = 'bill:reading:2'
                        bill.add_action(actor, action, date, type=a_type)
                continue

            elif action == 'COMMITTEES:':
                # committee assignments
                rows = table.xpath('tr')[1:]
                for row in rows:
                    # First add the committee assigned action
                    meta_tag = row.xpath('.//meta')[0]
                    h_or_s = meta_tag.get('name')[0] # @name is HCOMMITTEE OR SCOMMITTEE
                    committee = meta_tag.get('content') # @content is committee abbrv
                    #actor is house or senate referring the bill to committee
                    actor = 'lower' if h_or_s.lower() == 'h' else 'upper'
                    act = 'assigned to committee: ' + \
                        utils.get_committee_name(committee, actor)
                    date = utils.get_date(row[1])
                    bill.add_action(actor, act, date, type='committee:referred')
                    # now lets see if there is a vote
                    vote_url = row[0].xpath('string(a/@href)')
                    if vote_url:
                        date = utils.get_date(row[3])
                        act = row[5].text_content().strip()
                        a_type = get_action_type(act, 'COMMITTEES:')
                        act = get_verbose_action(act)
                        bill.add_action(actor,
                                        utils.get_committee_name(committee, actor) + ":" + act,
                                        date, type=a_type, abbrv=committee)
                        self.scrape_votes(actor, vote_url, bill, date,
                                            motion='committee: ' + act,
                                            committees=committee,
                                            type='other')
                    elif len(row) == 5:
                        # probably senate rules committee
                        date = utils.get_date(row[3])
                        if date == '':
                            date = utils.get_date(row[1])
                        act = row[4].text_content().strip()
                        a_type = get_action_type(act, 'COMMITTEES:')
                        act = get_verbose_action(act)
                        bill.add_action(actor,
                                        utils.get_committee_name(
                                            committee, actor) +
                                        ":" + act, date,
                                        type=a_type, abbrv=committee)
                continue

            elif 'CAUCUS' in action:
                rows = table.xpath('tr')[0:2]
                for row in rows:
                    actor = utils.get_actor(row, chamber)
                    action = row[0].text_content().strip()
                    if action.endswith(':'):
                        action = action[:-1]
                    if len(row) != 3:
                        self.warning('skipping row: %s' %
                                     row.text_content())
                        continue
                    result = row[2].text_content().strip()
                    # majority caucus Y|N
                    action = action + " recommends to concur: " + result
                    date = utils.get_date(row[1])
                    bill.add_action(actor, action, date, concur=result,
                                    type='other')
                continue

        # transmit to house or senate
            elif 'TRANSMIT TO' in action:
                rows = table.xpath('tr')
                for row in rows:
                    action = row[0].text_content().strip()[:-1]
                    actor = 'upper' if action.endswith('HOUSE') else 'lower'
                    date = utils.get_date(row[1])
                    bill.add_action(actor, action, date, type='other')
                continue

            # Committee of the whole actions
            elif 'COW ACTION' in action:
                rows = table.xpath('tr')
                actor = utils.get_actor(rows[0], chamber)
                if 'SIT COW ACTION' in action:
                    act = rows[0][-1].text_content().strip()
                    date = utils.get_date(rows[0][1])
                else:
                    act = rows[1][2].text_content().strip()
                    date = utils.get_date(rows[1][1])
                action = action + " " + get_verbose_action(act) # COW ACTION 1 DPA
                bill.add_action(actor, action, date, type='other')
                if rows[1][0].text_content().strip() == 'Vote Detail':
                    vote_url = rows[1][0].xpath('string(a/@href)')
                    self.scrape_votes(actor, vote_url, bill, date,
                                            motion=action, type='other',
                                            extra=act)
                continue
            # AMENDMENTS
            elif 'AMENDMENTS' in action:
                rows = table.xpath('tr')[1:]
                for row in rows:
                    act = row.text_content().strip()
                    if act == '':
                        continue
                    if 'passed' in act or 'adopted' in act:
                        a_type = 'amendment:passed'
                    elif 'failed' in act:
                        a_type = 'amendment:failed'
                    elif 'withdrawn' in act:
                        a_type = 'amendment:withdrawn'
                    else:
                        a_type = 'other'
                    # actor and date will same as previous action
                    bill.add_action(actor, act, date, type=a_type)
                continue
        # CONFERENCE COMMITTEE
        # http://www.azleg.gov/FormatDocument.asp?inDoc=/legtext/49Leg/2r/bills/hb2083o.asp

            # MISCELLANEOUS MOTION

            # MOTION TO RECONSIDER
            elif action == 'MOTION TO RECONSIDER:':
                date = utils.get_date(table[1][1])
                if date:
                    if table[1][0].text_content().strip() == 'Vote Detail':
                        vote_url = table[1][0].xpath('string(a/@href)')
                        bill.add_action(actor, action, date, type=a_type)
                        self.scrape_votes(actor, vote_url, bill, date,
                                          motion='motion to reconsider',
                                            type='other')
                    else:
                        action = table[-1][1].text_content().strip()
                        bill.add_action(actor, action, date, type='other')
                continue

            elif (action.endswith('FINAL READ:') or
                  action.endswith('THIRD READ:')):
                # house|senate final and third read
                rows = table.xpath('tr')
                # need to find out if third read took place in house or senate
                # if an ancestor table contains 'TRANSMIT TO' then the action
                # is taking place in that chamber, else it is in chamber
                actor = utils.get_actor(rows[0], chamber)
                # get a dict of keys from the header and values from the row
                k_rows = utils.get_rows(rows[1:], rows[0])
                action = rows[0][0].text_content().strip()
                for row in k_rows:
                    a_type = [get_action_type(action, 'Generic')]
                    if row[action].text_content().strip() == 'Vote Detail':
                        vote_url = row.pop(action).xpath('string(a/@href)')
                        vote_date = utils.get_date(row.pop('DATE'))
                        try:
                            passed = row.pop('RESULT').text_content().strip()
                        except KeyError:
                            passed = row.pop('2/3 VOTE').text_content().strip()

                        # leaves vote counts, ammended, emergency, two-thirds
                        # and possibly rfe left in k_rows. get the vote counts
                        # from scrape votes and pass ammended and emergency
                        # as kwargs to sort them in scrap_votes
                        pass_fail = {'PASSED': 'bill:passed',
                                        'FAILED': 'bill:failed'}[passed]
                        a_type.append(pass_fail)
                        bill.add_action(actor, action, vote_date,
                                        type=a_type)
                        row['type'] = 'passage'
                        self.scrape_votes(actor, vote_url, bill, vote_date,
                                            passed=passed, motion=action,
                                            **row)
                    else:
                        date = utils.get_date(row.pop('DATE'))
                        if date:
                            bill.add_action(actor, action, date, type=a_type)
                continue
            elif 'TRANSMITTED TO' in action:
                # transmitted to Governor or secretary of the state
                # SoS if it goes to voters as a proposition and memorials, etc
                rows = table.xpath('tr')
                actor = utils.get_actor(rows[0], chamber)
                # actor is the actor from the previous statement because it is
                # never transmitted to G or S without third or final read
                sent_to = rows[0][1].text_content().strip()
                date = utils.get_date(rows[0][2])
                a_type = 'governor:received' if sent_to[0] == 'G' else 'other'
                bill.add_action(actor, "TRANSMITTED TO " + sent_to, date,
                                type=a_type)
                # See if the actor is the governor and whether he signed
                # the bill or vetoed it
                act, date, chapter, version = '', '', '', ''
                for row in rows[1:]:
                    if row[0].text_content().strip() == 'ACTION:':
                        act = row[1].text_content().strip()
                        date = utils.get_date(row[2])
                    elif row[0].text_content().strip() == 'CHAPTER:':
                        chapter = row[1].text_content().strip()
                    elif row[0].text_content().strip() == 'CHAPTERED VERSION:':
                        version = row[1].text_content().strip()
                    elif row[0].text_content().strip() == 'TRANSMITTED VERSION:':
                        version = row[1].text_content().strip()
                if act and sent_to == 'GOVERNOR':
                    a_type = 'governor:signed' if act == 'SIGNED' else 'governor:vetoed'
                    if chapter:
                        bill.add_action(sent_to.lower(), act, date,
                                        type=a_type, chapter=chapter,
                                        chaptered_version=version)
                    else:
                        bill.add_action(sent_to.lower(), act, date,
                                            type=a_type)
                continue

        # this is probably only important for historical legislation
            elif 'FINAL DISPOSITION' in action:
                rows = table.xpath('tr')
                if rows:
                    disposition = rows[0][1].text_content().strip()
                    bill['final_disposition'] = disposition
        bill = self.sort_bill_actions(bill)
        self.save_bill(bill)

    def scrape(self, chamber, session):
        try:
            session_id = self.get_session_id(session)
        except KeyError:
            raise NoDataForPeriod(session)
        view = {'lower':'allhouse', 'upper':'allsenate'}[chamber]
        url = BASE_URL + 'Bills.asp?view=%s&Session_ID=%s' % (view, session_id)

        bills_index = self.urlopen(url)
        root = html.fromstring(bills_index)
        bill_links = root.xpath('//div/table/tr[3]/td[4]/table/tr/td/' +
                    'table[2]/tr[2]/td/table/tr/td[2]/table/tr/td//a')
        for link in bill_links:
            bill_id = link.text.strip()
            bill_id = " ".join(re.split('([A-Z]*)([0-9]*)', bill_id)).strip()
            self.scrape_bill(chamber, session, bill_id)

    def scrape_votes(self, chamber, url, bill, date, **kwargs):
        """
        Scrapes the votes from a vote detail page with the legislator's names
        this handles all of the votes and expects the following keyword
        arguments: motion
        an Arizona Vote object will have the following additional fields:
        additional vote counts:
            +not_voting, +excused, +absent, +present
        additional vote lists
            +NV, +EX, +AB, +P
        this depends on the chamber and the committee
        """
        o_args = {}
        passed = '' # to test if we need to compare vote counts later
        v_type = kwargs.pop('type')
        if 'passed' in kwargs:
            passed = {'PASSED': True, 'FAILED': False}[kwargs.pop('passed')]
        if 'AMEND' in kwargs:
            o_args['amended'] = kwargs.pop('AMEND').text_content().strip()
        if 'motion' in kwargs:
            motion = kwargs.pop('motion')
        if 'EMER' in kwargs and kwargs['EMER'].text_content().strip():
            o_args['EMER'] = kwargs.pop('EMER').text_content().strip()
        if '2/3 VOTE' in kwargs and kwargs['2/3 VOTE'].text_content().strip():
            o_args['2/3 VOTE'] = kwargs.pop('2/3 VOTE').text_content().strip()
        if 'committee' in kwargs:
            o_args['committee'] = utils.get_committee_name(kwargs.pop('committee'),
                                                            chamber)
        if 'committees' in kwargs:
            o_args['committee'] = utils.get_committee_name(kwargs.pop('committees'),
                                                            chamber)
        vote_page = self.urlopen(url)
        root = html.fromstring(vote_page)
        vote_table = root.xpath('/html/body/div/table/tr[3]/td[4]/table/tr/td/table/tr/td/table')[0]
        vote_count = vote_table.xpath('following-sibling::p/following-sibling::text()')
        vote_string = vote_count[0].replace(u'\xa0', '').strip()
        v_count = re.compile(r'\b[A-Z]*\s*[A-z]*:\s\d*')
        v_list = v_count.findall(vote_string)
        o_count = 0
        for x in v_list:
            k, v = x.split(':')
            # make NOT VOTING not_voting
            k = k.strip().replace(' ', '_').lower()
            v = int(v.strip())
            if k == 'ayes':
                yes_count = int(v)
            elif k == 'nays':
                no_count = int(v)
            else:
                o_args.update({str(k):v})
                o_count += int(v)
        if passed == '':
            passed = yes_count > no_count
            if ('committee' not in o_args) and ('committees' not in o_args):
                if chamber == 'upper' and passed:
                    if 'EMER' in o_args or '2/3 VOTE' in o_args:
                        passed = yes_count > 20
                    else:
                        passed = yes_count > 16
                elif chamber == 'lower' and passed:
                    if 'EMER' in o_args or '2/3 VOTE' in o_args:
                        passed = yes_count > 40
                    else:
                        passed = yes_count > 31

        vote = Vote(chamber, date, motion, passed, yes_count, no_count,
                    o_count, type=v_type, **o_args)
        vote.add_source(url)
        # grab all the tables descendant tds
        tds = vote_table.xpath('descendant::td')
        # pair 'em up
        matched = [ tds[y:y+2] for y in range(0, len(tds), 2) ]
        for name, v in iter(matched):
            v = v.text_content().strip()
            name = name.text_content().strip()
            if name == 'Member Name':
                continue
            if v == 'Y':
                vote.yes(name)
            elif v == 'N':
                vote.no(name)
            else:
                if v in vote:
                    vote[v].append(name)
                else:
                    vote[v] = [name]
                vote.other(name)

        # Warn if the stated other_vote count doesn't add up.
        if vote['other_count'] != len(vote['other_votes']):
            self.warning("Other votes count on webpage didn't match "
                         "len(other_votes)...using length instead.")
            vote['other_count'] = len(vote['other_votes'])

        bill.add_vote(vote)

    def sort_bill_actions(self, bill):
        actions = bill['actions']
        actions_list = []
        out_of_order = []
        new_list = []
        if not actions:
            return bill
        action_date = actions[0]['date']
        actions[0]['action'] = actions[0]['action'].lower()
        actions_list.append(actions[0])
        # seperate the actions that are out of order
        for action in actions[1:]:
            if action['date'] < action_date:
                out_of_order.append(action)
            else:
                actions_list.append(action)
                action_date = action['date']
            action['action'] = action['action'].lower()
        action_date = actions_list[0]['date']


        for action in actions_list:
            # this takes care of the actions in beween
            for act in out_of_order:
                if act['date'] < action_date:
                    o_index = out_of_order.index(act)
                    new_list.append(out_of_order.pop(o_index))
                if act['date'] >= action_date and act['date'] < action['date']:
                    o_index = out_of_order.index(act)
                    new_list.append(out_of_order.pop(o_index))
            new_list.append(action)

            for act in out_of_order:
                if act['date'] == action['date']:
                    o_index = out_of_order.index(act)
                    new_list.append(out_of_order.pop(o_index))

        if out_of_order != []:
            self.log("Unable to sort " + bill['bill_id'])
            return bill
        else:
            bill['actions'] = new_list
            return bill

########NEW FILE########
__FILENAME__ = build_metadata
from billy.scrape import Scraper
import datetime
import re
from lxml import etree, html

word_key = (
    ('fifty', '50'),
    ('fiftieth', '50'),
    ('forty', '40'),
    ('first', '1'),
    ('second', '2'),
    ('third', '3'),
    ('fourth', '4'),
    ('fifth', '5'),
    ('sixth', '6'),
    ('seventh', '7'),
    ('eighth', '8'),
    ('ninth', '9'),
    ('tenth', '10'),
    ('eleventh', '11'),
    ('twelth', '12'),
    ('regular', 'r'),
    ('special', 's'),
)

# borrowed from django.contrib.humanize.templatetags
def ordinal(value):
    """
    Converts an integer to its ordinal as a string. 1 is '1st', 2 is '2nd',
    3 is '3rd', etc. Works for any integer.
    """
    try:
        value = int(value)
    except (TypeError, ValueError):
        return value
    t = ('th', 'st', 'nd', 'rd', 'th', 'th', 'th', 'th', 'th', 'th')
    if value % 100 in (11, 12, 13): # special case
        return u"%d%s" % (value, t[0])
    return u'%d%s' % (value, t[value % 10])

def get_session_name(leg):
    l = leg.lower().replace('-', ' ').split()
    session = [x[1] for y in l for x in word_key if x[0] == y]
    try:
        if len(session) == 4:
            one = ordinal(int(session[0]) + int(session[1]))
            two = ordinal(session[2])
        else:
            one = ordinal(int(session[0]))
            two = ordinal(int(session[1]))
        three = {'s': 'special', 'r': 'regular'}[session[-1]]
        return "%s-%s-%s" % (one, two, three)
    except IndexError:
        return None

def get_date(d):
    if d:
        d = datetime.datetime.strptime(d, '%Y-%m-%dT%H:%M:%S').date()
        return '%d, %d, %d' % (d.year, d.month, d.day)
    else:
        return ''

class AZTermScraper(Scraper):
    state = 'az'

    def scrape_session_details(self):
        """
        writes the terms and session details to session_detail.py
        still needs some hand work to make sure that regular sessions are
        where they should be.
        """
        url = 'http://www.azleg.gov/xml/sessions.asp'
        page = self.urlopen(url)
        root = etree.fromstring(page)
        session_file = open('session_details.py', 'w')
        terms = """
        {'name': '%s',
         'sessions': [
            %s
         ],
         'start_year': %s, 'end_year': %s
        },
        """
        term_list = {}
        detail = """
                 '%s':
                    {'type': '%s', 'session_id': %s,
                     'verbose_name': '%s',
                     'start_date': datetime.date(%s),
                     'end_date': datetime.date(%s)},\n"""
        sessions = root.xpath('//session')
        sessions = sorted(sessions, key=lambda x: x.get('Sine_Die_Date') or
                                        "%s" % datetime.datetime.today())
        terms_text = ""
        details_text = ""
        for session in sessions:
            session_type = 'primary' if re.search('Regular', session.get('Session_Full_Name')) else 'special'
            start_date = get_date(session.get('Session_Start_Date', None))
            end_date = get_date(session.get('Sine_Die_Date', None))
            session_name = get_session_name(session.get('Session_Full_Name'))
            details_text += detail % (session_name,
                                      session_type,
                                      session.get('Session_ID'),
                                      session.get('Session_Full_Name'),
                                      start_date,
                                      end_date,)
            try:
                s_name = session_name[0:2]
            except TypeError:
                s_name = 'misc'
            if s_name in term_list:
                term_list[s_name]['sessions'] += "                '%s',\n" % session_name
                if end_date[0:4] > term_list[s_name]['end_date']:
                    term_list[s_name]['end_date'] = end_date[0:4]
                if start_date[0:4] < term_list[s_name]['start_date']:
                    term_list[s_name]['start_date'] = start_date[0:4]
            else:
                term_list[s_name] = {}
                term_list[s_name]['sessions'] = "'%s',\n" % session_name
                term_list[s_name]['end_date'] = end_date[0:4]
                term_list[s_name]['start_date'] = start_date[0:4]

        for key in sorted(term_list.keys()):
            session_file.write(terms % (
                    key, term_list[key]['sessions'],
                    term_list[key]['start_date'],
                    term_list[key]['end_date']
                ))
        session_file.write(details_text)
        session_file.close()

if __name__ == '__main__':
    from . import metadata
    scraper = AZTermScraper(metadata)
    scraper.scrape_session_details()

########NEW FILE########
__FILENAME__ = committees
import re
import datetime
import urlparse

import lxml
from scrapelib import HTTPError
from billy.scrape import NoDataForPeriod
from billy.scrape.committees import CommitteeScraper, Committee

from . import utils

base_url = 'http://www.azleg.gov/'


class AZCommitteeScraper(CommitteeScraper):
    jurisdiction = 'az'

    def get_session_for_term(self, term):
        # ideally this should be either first or second regular session
        # and probably first and second when applicable
        for t in self.metadata['terms']:
            if t['name'] == term:
                session = t['sessions'][-1]
                if re.search('regular', session):
                    return session
                else:
                    return t['sessions'][0]

    def get_session_id(self, session):
        return self.metadata['session_details'][session]['session_id']

    def scrape(self, chamber, term):
        self.validate_term(term)
        session = self.get_session_for_term(term)
        try:
            session_id = self.get_session_id(session)
        except KeyError:
            raise NoDataForPeriod

        url = 'http://www.azleg.gov/StandingCom.asp'
        html = self.get(url).text
        doc = lxml.html.fromstring(html)

        chamber_name = dict(
            upper="Senate",
            lower="House of Representatives")[chamber]
        xpath = '//strong[contains(text(), "%s")]/../../following-sibling::tr/td'
        tds = doc.xpath(xpath % chamber_name)
        for td in tds:
            name = td.text_content().strip()
            source_url = td.xpath('a/@href')[0]
            query = urlparse.urlparse(source_url).query
            params = dict(urlparse.parse_qsl(query))
            c_id = params['Committee_ID']
            session_id = params['Session_ID']

            c = Committee(chamber, name, session=session, az_committee_id=c_id)

            c.add_source(source_url)
            #for some reason they don't always have any info on the committees'
            try:
                self.scrape_com_info(session, session_id, c_id, c)
            except HTTPError:
                pass

            if not c['members']:
                msg = 'No members found: not saving {committee}.'
                self.logger.warning(msg.format(**c))
                continue
            self.save_committee(c)

    def scrape_com_info(self, session, session_id, committee_id, committee):
        url = base_url + 'CommitteeInfo.asp?Committee_ID=%s&Session_ID=%s' % (committee_id,
                                                                    session_id)

        page = self.urlopen(url)
        committee.add_source(url)
        root = lxml.html.fromstring(page)
        p = '//table/tr/td[1]/a/ancestor::tr[1]'
        rows = root.xpath(p)
        #need to skip the first row cause its a link to the home page
        for row in rows[1:]:
            name = row[0].text_content().strip()
            role = row[1].text_content().strip()
            committee.add_member(name, role)

    def scrape_index(self, chamber, session, session_id, committee_type):
        url = base_url + 'xml/committees.asp?session=%s&type=%s' % (session_id,
                                                                 committee_type)
        page = self.urlopen(url)
        root = etree.fromstring(page.bytes, etree.XMLParser(recover=True))

        body = '//body[@Body="%s"]/committee' % {'upper': 'S',
                                                 'lower': 'H'}[chamber]
        # TODO need to and make sure to add sub committees
        for com in root.xpath(body):
            c_id, name, short_name, sub = com.values()
            c = Committee(chamber, name, short_name=short_name,
                          session=session, az_committee_id=c_id)
            c.add_source(url)
            self.scrape_com_info(session, session_id, c_id, c)
            if c['members']:
                self.save_committee(c)
            else:
                msg = 'No members found: not saving {committee}.'
                self.logger.warning(msg.format(**c))


########NEW FILE########
__FILENAME__ = events
import datetime
import re

from lxml import html

from billy.scrape import NoDataForPeriod
from billy.scrape.events import EventScraper, Event


class AZEventScraper(EventScraper):
    """
    Arizona Event Scraper, gets interim committee, agendas, floor calendars
    and floor activity events
    """
    jurisdiction = 'az'

    _chamber_short = {'upper': 'S', 'lower': 'H'}
    _chamber_long = {'upper': 'Senate', 'lower': 'House'}

    def get_session_id(self, session):
        """
        returns the session id for a given session
        """
        return self.metadata['session_details'][session]['session_id']

    def scrape(self, chamber, session):
        if chamber == "other":
            return
        """
        given a chamber and session returns the events
        """
        try:
            session_id = self.get_session_id(session)
        except KeyError:
            raise NoDataForPeriod(session)

        # this will only work on the latest regular or special session
        # 103 is fortyninth - ninth special session 102 is session_ID for
        # fiftieth
        # we can get old events with some hassle but we cant change what has
        # already happened so why bother?
        if session_id == 103 or session_id < 102:
            raise NoDataForPeriod(session)

        # http://www.azleg.gov/CommitteeAgendas.asp?Body=H
        self.scrape_committee_agendas(chamber, session)
        # http://www.azleg.gov/InterimCommittees.asp
        # self.scrape_interim_events(chamber, session)

    def scrape_committee_agendas(self, chamber, session):
        """
        Scrape upper or lower committee agendas
        """
        # could use &ShowAll=ON doesn't seem to work though
        url = 'http://www.azleg.gov/CommitteeAgendas.asp?Body=%s' % \
                                          self._chamber_short[chamber]
        html_ = self.urlopen(url)
        doc = html.fromstring(html_)
        if chamber == 'upper':
            event_table = doc.xpath('//table[@id="body"]/tr/td/table[2]/tr'
                                     '/td/table/tr/td/table')[0]
        else:
            event_table = doc.xpath('//table[@id="body"]/tr/td/table[2]/tr'
                                     '/td/table/tr/td/table/tr/td/table')[0]
        for row in event_table.xpath('tr')[2:]:
            # Agenda Date, Committee, Revised, Addendum, Cancelled, Time, Room,
            # HTML Document, PDF Document for house
            # Agenda Date, Committee, Revised, Cancelled, Time, Room,
            # HTML Document, PDF Document for senate
            text = [x.text_content().strip() for x in row.xpath('td')]
            when, committee = text[0:2]
            if chamber == 'upper':
                time, room = text[4:6]
                link = row[6].xpath('string(a/@href)')
            else:
                time, room = text[5:7]
                link = row[7].xpath('string(a/@href)')
            if 'NOT MEETING' in time or 'CANCELLED' in time:
                continue
            time = re.match('(\d+:\d+ (A|P))', time)
            if time:
                when = "%s %sM" % (text[0], time.group(0))
                when = datetime.datetime.strptime(when, '%m/%d/%Y %I:%M %p')
            else:
                when = text[0]
                when = datetime.datetime.strptime(when, '%m/%d/%Y')

            title = "Committee Meeting:\n%s %s %s\n" % (
                                              self._chamber_long[chamber],
                                              committee, room)
            agenda_info = self.parse_agenda(chamber, link)

            description = agenda_info['description']
            member_list = agenda_info['member_list']
            related_bills = agenda_info['related_bills']

            event = Event(session, when, 'committee:meeting', title,
                          location=room, link=link, details=description,
                          related_bills=related_bills)
            event.add_participant('host', committee, 'committee',
                                  chamber=chamber)

            event['participants'].extend(member_list)
            event.add_source(url)
            event.add_source(link)
            # print event['when'].timetuple()
            # import ipdb;ipdb.set_trace()
            self.save_event(event)

    def parse_agenda(self, chamber, url):
        """
        parses the agenda detail and returns the description, participants, and
        any other useful info
        self.parse_agenda(url)--> (desc='', who=[], meeting_type='', other={})
        """
        html_ = self.urlopen(url)
        doc = html.fromstring(html_)

        # Related bills
        related_bills = []
        for tr in doc.xpath('//h3[contains(., "Bills")]/../../../../tr'):
            related_bill = {}
            bill_id = tr[1].text_content().strip()
            if not bill_id or bill_id[0] not in 'HS':
                continue
            related_bill['bill_id'] = bill_id
            try:
                description = tr[3].text_content().strip()
            except IndexError:
                continue
            description = re.sub(r'\s+', ' ', description)
            related_bill['description'] = description
            related_bill['type'] = 'consideration'
            related_bills.append(related_bill)

        xpaths = (
            '//div[@class="Section1"]',
            '//div[@class="WordSection1"]',
            )
        for xpath in xpaths:
            try:
                div = doc.xpath(xpath)[0]
            except IndexError:
                continue

        # probably committee + meeting_type?
        meeting_type = div.xpath('string(//p'
                                 '[contains(a/@name, "Joint_Meeting")])')
        members = doc.xpath('//p[contains(a/@name, "Members")]')
        if members:
            members = members[0]
        else:
            members = doc.xpath('//p[contains(span/a/@name, "Members")]')[0]
        other = {}
        member_list = []
        while members.tag == 'p':
            text = members.text_content().strip().replace(u'\xa0', u' ')
            if text == '':
                break
            names = re.split(r'\s{5,}', text)
            if names:
                for name in names:
                    name = re.sub(r'\s+', ' ', name)
                    if ',' in name:
                        name, role = name.split(',')
                        role = role.lower()
                    else:
                        role = None
                    if name == 'SENATORS' or name == 'Members':
                        continue
                    if role in ['chair', 'chairman']:
                        role = 'chair'
                    else:
                        role = 'participant'
                    person = {"type": role,
                              "participant": name,
                              "participant_type": 'legislator',
                              "chamber": chamber}
                    member_list.append(person)
            members = members.getnext()
        description = ""
        agenda_items = div.xpath('//p[contains(a/@name, "AgendaItems")]'
                                '/following-sibling::table[1]')
        if agenda_items:
            agenda_items = [tr.text_content().strip().replace("\r\n", "")
                            for tr in agenda_items[0].getchildren()
                            if tr.text_content().strip()]
            description = ",\n".join(agenda_items)
        bill_list = div.xpath('//p[contains(a/@name, "Agenda_Bills")]'
                                '/following-sibling::table[1]')
        if bill_list:
            try:
                bill_list = [tr[1].text_content().strip() + " " +
                         tr[3].text_content().strip().replace("\r\n", "")
                         for tr in bill_list[0].xpath('tr')
                         if tr.text_content().strip()]
            except IndexError:
                bill_list = [tr.text_content().strip().replace("\r\n", "")
                            for tr in bill_list[0].getchildren()
                            if tr.text_content().strip()]

            bill_list = ",\n".join(bill_list)
            description = description + bill_list

        return {
            "description": description,
            "member_list": member_list,
            "meeting_type": meeting_type,
            "agenda_items": agenda_items,
            "related_bills": related_bills,
            "other": other
        }

    def scrape_interim_events(self, chamber, session):
        """
        Scrapes the events for interim committees
        """
        session_id = self.get_session_id(session)
        url = 'http://www.azleg.gov/InterimCommittees.asp?Session_ID=%s' % session_id
        # common xpaths
        agendas_path = '//table[contains(' \
                       'tr/td/div[@class="ContentPageTitle"]/text(), "%s")]'

        html_ = self.urlopen(url)
        doc = html.fromstring(html_)
        table = doc.xpath(agendas_path % "Interim Committee Agendas")
        if table:
            rows = table[0].xpath('tr')
            for row in rows[2:]:
                pass

########NEW FILE########
__FILENAME__ = legislators
from billy.scrape import NoDataForPeriod
from billy.scrape.legislators import LegislatorScraper, Legislator
from lxml import html

import re, datetime

class AZLegislatorScraper(LegislatorScraper):
    jurisdiction = 'az'
    parties = {
        'R': 'Republican',
        'D': 'Democratic',
        'L': 'Libertarian',
        'I': 'Independent',
        'G': 'Green'
    }

    def get_party(self, abbr):
        return self.parties[abbr]

    def get_session_id(self, session):
        return self.metadata['session_details'][session]['session_id']

    def get_session_for_term(self, term):
        # ideally this should be either first or second regular session
        # and probably first and second when applicable
        for t in self.metadata['terms']:
            if t['name'] == term:
                session = t['sessions'][-1]
                if re.search('regular', session):
                    return session
                else:
                    return t['sessions'][0]

    def scrape(self, chamber, term):
        self.validate_term(term)
        session = self.get_session_for_term(term)
        try:
            session_id = self.get_session_id(session)
        except KeyError:
            raise NoDataForPeriod(session)

        body = {'lower': 'H', 'upper': 'S'}[chamber]
        url = 'http://www.azleg.gov/MemberRoster.asp?Session_ID=%s&body=%s' % (
                                                               session_id, body)
        page = self.urlopen(url)
        root = html.fromstring(page)
        path = '//table[@id="%s"]/tr' % {'H': 'house', 'S': 'senate'}[body]
        roster = root.xpath(path)[1:]
        for row in roster:
            position = ''
            vacated = ''
            name, district, party, email, room, phone, fax = row.xpath('td')

            if email.attrib.get('class') == 'vacantmember':
                continue  # Skip any vacant members.

            link = name.xpath('string(a/@href)')
            link = "http://www.azleg.gov" + link
            if len(name) == 1:
                name = name.text_content().strip()
            else:
                position = name.tail.strip()
                name = name[0].text_content().strip()

            linkpage = self.urlopen(link)
            linkroot = html.fromstring(linkpage)
            linkroot.make_links_absolute(link)

            photos = linkroot.xpath("//img[@name='memberphoto']")

            if len(photos) != 1:
                raise Exception

            photo_url = photos[0].attrib['src']

            district = district.text_content()
            party = party.text_content().strip()
            email = email.text_content().strip()

            if ('Vacated' in email or 'Resigned' in email or 
                'Removed' in email):
                # comment out the following 'continue' for historical
                # legislative sessions
                # for the current session, if a legislator has left we will
                # skip him/her to keep from overwriting their information
                continue
                vacated = re.search('[0-9]*/[0-9]*/\d{4}', email).group()
                email = ''

            party = self.get_party(party)
            room = room.text_content().strip()
            if chamber == 'lower':
                address = "House of Representatives\n"
            else:
                address = "Senate\n"
            address = address + "1700 West Washington\n Room " + room  \
                              + "\nPhoenix, AZ 85007"

            phone = phone.text_content().strip()
            if not phone.startswith('602'):
                phone = "602-" + phone
            fax = fax.text_content().strip()
            if not fax.startswith('602'):
                fax = "602-" + fax
            if vacated:
                end_date = datetime.datetime.strptime(vacated, '%m/%d/%Y')
                leg = Legislator( term, chamber, district, full_name=name,
                                  party=party, url=link)
                leg['roles'][0]['end_date'] = end_date
            else:
                leg = Legislator(term, chamber, district, full_name=name,
                                 party=party, email=email, url=link,
                                 photo_url=photo_url)

            leg.add_office('capitol', 'Capitol Office', address=address,
                           phone=phone, fax=fax)

            if position:
                leg.add_role( position, term, chamber=chamber,
                             district=district, party=party)

            leg.add_source(url)

            #Probably just get this from the committee scraper
            #self.scrape_member_page(link, session, chamber, leg)
            self.save_legislator(leg)

    def scrape_member_page(self, url, session, chamber, leg):
        html = self.urlopen(url)
        root = html.fromstring(html)
        #get the committee membership
        c = root.xpath('//td/div/strong[contains(text(), "Committee")]')
        for row in c.xpath('ancestor::table[1]')[1:]:
            name = row[0].text_content().strip()
            role = row[1].text_content().strip()
            leg.add_role(role, session, chamber=chamber, committee=name)

        leg.add_source(url)

########NEW FILE########
__FILENAME__ = utils
import re, datetime
doc_for_bills_url = 'http://www.azleg.gov/DocumentsForBill.asp?Bill_Number=%s&Session_ID=%s'
base_url = 'http://www.azleg.gov/'
select_session_url = 'http://www.azleg/SelectSession.asp.html'

def parse_link_id(link):
    """
    extracts the div[@id] from the links on the DocumentsForBill pages
    """
    return link.get('href')[link.get('href').find("'") + 1 : link.get('href').rfind("'")]

def get_bill_type(bill_id):
    """
    bill_id = 'SJR2204'
    get_bill_type(bill_id) --> 'joint resolution'
    """
    prefix = re.match('([a-z]*)', bill_id.lower()).group()
    if prefix in bill_types:
        return bill_types[prefix]
    else:
        return 'bill'

def legislature_to_number(leg):
    """
    Takes a full session and splits it down to the values for
    FormatDocument.asp.

    session = '49th-1st-regular'
    legislature_to_number(session) --> '49Leg/1s'
    """
    l = leg.lower().split('-')
    return '%sLeg/%s%s' % (l[0][0:2], l[1][0], l[2][0])

def get_date(elem):
    """
    Returns the date object or an empty string, silly but it will really save
    some typing since a table might have a date field or it might be empty
    """
    try:
        return_date = datetime.datetime.strptime(elem.text_content().strip(), '%m/%d/%y')
    except ValueError:
        return_date = ''
    return return_date

def img_check(elem):
    """
    Checks if the cell contains an image and returns true or false
    used to see if a calendar was modified revised or cancelled.
    """
    img = elem.xpath('img')
    if img:
        return 'Y'
    else:
        text = elem.text_content().strip()
        if text:
            return 'Y'
        else:
            return 'N'

def get_rows(rows, header):
    """
    takes the rows and header and returns a dict for each row with { key : <td> }
    """
    header = [x.text_content().strip() for x in header]
    keyed_rows = []
    for r in rows:
        dict_row = {}
        for k,v in zip(header, r.xpath('td')):
            dict_row.update({k:v})
        keyed_rows.append(dict_row)
    return keyed_rows

def get_actor(tr, chamber):
    """
    gets the actor of a given action based on presence of a 'TRANSMIT TO' action
    """
    actor = tr[0].text_content().strip()
    if actor.startswith('H') or actor.startswith('S'):
        actor = actor[0]
        return {'H': 'lower', 'S': 'upper'}[actor]
    else:
        h_or_s = tr.xpath('ancestor::table[1]/preceding-sibling::' +
                                  'table/tr/td/b[contains(text(), "TRANSMIT TO")]')
        if h_or_s:
            # actor is the last B element
            h_or_s = h_or_s[-1].text_content().strip()
            actor = 'upper' if h_or_s.endswith('SENATE:') else 'lower'
        else:
            actor = chamber
        return actor

def get_committee_name(abbrv, chamber):
    try:
        return com_names[chamber][abbrv]
    except KeyError:
        return abbrv

com_names = {
    'lower': {'APPROP': 'Appropriations',
              'AW': 'Agriculture and Water',
              'BI': 'Banking and Insurance',
              'COM': 'Commerce',
              'ED': 'Education',
              'ENR': 'Energy and Natural Resources',
              'ENV': 'Environment',
              'ERA': 'Employment and Regulatory Affairs',
              'GOV': 'Government',
              'HEIR': 'Higher Education, Innovation and Reform',
              'HHS': 'Health and Human Services',
              'JUD': 'Judiciary',
              'MAPS': 'Military Affairs and Public Safety',
              'RULES': 'Rules',
              'TI': 'Technology and Infrastructure',
              'TRANS': 'Transportation',
              'WM': 'Ways and Means'},
    'upper': {'APPROP': 'Appropriations',
              'BI': 'Banking and Insurance',
              'BSFSS': 'Border Security, Federalism and States Sovereignty',
              'CE': 'Commerce and Energy',
              'ED': 'Education',
              'EDJC': 'Economic Development and Jobs Creation',
              'FIN': 'Finance',
              'GR': 'Government Reform',
              'HMLR': 'Healthcare and Medical Liability Reform',
              'JUD': 'Judiciary',
              'NRT': 'Natural Resources and Transportation',
              'PSHS': 'Public Safety and Human Services',
              'RULES': 'Rules',
              'SUB APPROP HW': 'Appropriations',
              'SUB APPROP RIEN': 'Appropriations',
              'SUB APPROP TCJ': 'Appropriations',
              'VMA': 'Veterans and Military Affairs',
              'WLRD': 'Water, Land Use and Rural Development'}}

bill_types = {
    'sb': 'bill',
    'sm': 'memorial',
    'sr': 'resolution',
    'scr': 'concurrent resolution',
    'scm': 'concurrent memorial',
    'scj': 'joint resolution',
    'hb': 'bill',
    'hm': 'memorial',
    'hr': 'resolution',
    'hcr': 'concurrent resolution',
    'hcm': 'concurrent memorial',
    'hjr': 'joint resolution',
    'mis': 'miscellaneous'
}

########NEW FILE########
__FILENAME__ = actions
from billy.scrape.actions import Rule, BaseCategorizer


# These are regex patterns that map to action categories.
_categorizer_rules = (

    Rule((r'\(Ayes (?P<yes_votes>\d+)\.\s+Noes\s+'
          r'(?P<no_votes>\d+)\.( Page \S+\.)?\)')),

    Rule(r'^Introduced', 'bill:introduced'),

    Rule(r'(?i)Referred to (?P<committees>.+)', 'committee:referred'),
    Rule(r'(?i)Referred to (?P<committees>.+?)(\.\s+suspense)',
         'committee:referred'),
    Rule(r're-refer to Standing (?P<committees>[^.]+)\.',
         'committee:referred'),

    Rule(r'Read first time\.', 'bill:reading:1'),
    Rule(r'Read second time and amended',
          ['bill:reading:2']),
    Rule(r'Read third time', 'bill:reading:3'),
    Rule(r'Read third time. Refused passage\.',
         'bill:failed'),
    Rule([r'(?i)read third time.{,5}passed',
          r'(?i)Read third time.+?Passed'],
         ['bill:passed', 'bill:reading:3']),

    Rule(r'Approved by the Governor', 'governor:signed'),
    Rule(r'Approved by the Governor with item veto',
         'governor:vetoed:line-item'),
    Rule('Vetoed by Governor', 'governor:vetoed'),
    Rule(r'To Governor', 'governor:received'),

    Rule(r'amendments concurred in', 'amendment:passed'),
    Rule(r'refused to concur in Assembly amendments', 'amendment:failed'),

    Rule(r'Failed passage in committee', 'committee:failed'),
    Rule(r'(?i)From committee', 'committee:passed'),
    Rule(r'(?i)From committee: Do pass', 'committee:passed:favorable'),
    Rule(r'From committee with author\'s amendments', 'committee:passed'),

    # Resolutions
    Rule(r'Adopted', 'bill:passed'),
    Rule(r'Read', 'bill:reading:1'),
    Rule(r'^From committee: Be adopted', 'committee:passed:favorable'),
    )


class CACategorizer(BaseCategorizer):
    rules = _categorizer_rules

########NEW FILE########
__FILENAME__ = bills
import re
import os
import operator
import itertools

from lxml import etree, html
import pytz
from sqlalchemy.orm import sessionmaker
from sqlalchemy import create_engine

from billy.core import settings
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote
from .models import CABill
from .actions import CACategorizer


SPONSOR_TYPES = {'LEAD_AUTHOR': 'primary',
                 'COAUTHOR': 'cosponsor',
                 'PRINCIPAL_COAUTHOR': 'primary'}

def clean_title(s):
    # replace smart quote characters
    s = s.replace(u'\xe2\u20ac\u201c', '-')

    # Cesar Chavez e
    s = s.replace(u'\xc3\xa9', u'\u00E9')
    # Cesar Chavez a
    s = s.replace(u'\xc3\xa1', u'\u00E1')
    s = s.replace(u'\xe2\u20ac\u201c', u'\u2013')

    s = re.sub(ur'[\u2018\u2019]', "'", s)
    s = re.sub(ur'[\u201C\u201D]', '"', s)
    s = re.sub(u'\u00e2\u20ac\u2122', u"'", s)
    s = re.sub(ur'\xe2\u20ac\u02dc', "'", s)
    return s


# Committee codes used in action chamber text.
committee_data_upper = [
    #('CZ09',  'Standing Committee on Floor Analyses'),
    ('Standing Committee on Governance and Finance',
      'CS73', [u'Gov. & F.']),

    ('Standing Committee on Energy, Utilities and Communications',
      'CS71', [u'E. U. & C.', u'E., U. & C', 'E., U., & C.']),

    ('Standing Committee on Education',
      'CS44', [u'ED.']),

    ('Standing Committee on Appropriations',
      'CS61', [u'APPR.']),

    ('Standing Committee on Labor and Industrial Relations',
      'CS51', [u'L. & I.R.']),

    ('Standing Committee on Elections and Constitutional Amendments',
      'CS45', [u'E. & C.A.']),

    ('Standing Committee on Environmental Quality',
      'CS64', [u'E.Q.']),

    ('Standing Committee on Natural Resources And Water',
      'CS55', [u'N.R. & W.']),

    ('Standing Committee on Public Employment and Retirement',
      'CS56', [u'P.E. & R.']),

    ('Standing Committee on Governmental Organization',
      'CS48', [u'G.O.']),

    ('Standing Committee on Insurance',
      'CS70', [u'INS.']),

    ('Standing Committee on Public Safety',
      'CS72', [u'PUB. S.']),

    ('Standing Committee on Judiciary',
      'CS53', [u'JUD.']),

    ('Standing Committee on Health',
      'CS60', [u'HEALTH.']),

    ('Standing Committee on Transportation and Housing',
      'CS59', [u'T. & H.']),

    ('Standing Committee on Business, Professions and Economic Development',
      'CS42', [u'B., P. & E.D.']),

    ('Standing Committee on Agriculture',
      'CS40', [u'AGRI.']),

    ('Standing Committee on Banking and Financial Institutions',
      'CS69', [u'B. & F.I.']),

    ('Standing Committee on Veterans Affairs',
      'CS66', [u'V.A.']),

    ('Standing Committee on Budget and Fiscal Review',
      'CS62', [u'B. & F.R.']),

    ('Standing Committee on Human Services',
      'CS74', [u'HUM. S.', u'HUMAN S.']),

    ('Standing Committee on Rules',
      'CS58', [u'RLS.']),
    ]

committee_data_lower = [
    # LOWER
    ('Standing Committee on Rules',
      'CX20', [u'RLS.']),
    #('assembly floor analysis', 'CZ01', []),
    ('Standing Committee on Revenue and Taxation',
      'CX19', [u'REV. & TAX']),

    ('Standing Committee on Natural Resources',
      'CX16', [u'NAT. RES.']),

    ('Standing Committee on Appropriations',
      'CX25', [u'APPR.']),

    ('Standing Committee on Insurance',
      'CX28', ['INS.']),

    ('Standing Committee on Utilities and Commerce',
      'CX23', [u'U. & C.']),

    ('Standing Committee on Education',
      'CX03', [u'ED.']),

    ('Standing Committee on Public Safety',
      'CX18', [u'PUB. S.']),

    ('Standing Committee on Elections and Redistricting',
      'CX04', [u'E. & R.']),

    ('Standing Committee on Judiciary',
      'CX13', [u'JUD.', 'Jud.']),
    ('Standing Committee on Higher Education',
      'CX09', [u'HIGHER ED.']),

    ('Standing Committee on Health',
      'CX08', [u'HEALTH']),

    ('Standing Committee on Human Services',
      'CX11', [u'HUM. S.', u'HUMAN S.']),

    ('Standing Committee on Arts, Entertainment, Sports, Tourism, and Internet Media',
      'CX37', [u'A.,E.,S.,T., & I.M.']),

    ('Standing Committee on Transportation',
      'CX22', [u'TRANS.']),

    ('Standing Committee on Business, Professions and Consumer Protection',
      'CX33', [u'B.,P. & C.P.', 'B., P. & C.P.', u'B. & P.']),

    ('Standing Committee on Water, Parks and Wildlife',
      'CX24', [u'W., P. & W']),

    ('Standing Committee on Local Government',
      'CX15', [u'L. GOV.', 'L. Gov.']),

    ('Standing Committee on Aging and Long Term Care',
      'CX31', [u'AGING & L.T.C.']),

    ('Standing Committee on Labor and Employment',
      'CX14', [u'L. & E.']),

    ('Standing Committee on Governmental Organization',
      'CX07', [u'G.O.']),

    ('Standing Committee on Public Employees, Retirement and Social Security',
      'CX17', [u'P.E., R. & S.S.']),

    ('Standing Committee on Veterans Affairs',
      'CX38', [u'V.A.']),

    ('Standing Committee on Housing and Community Development',
      'CX10', [u'H. & C.D.']),

    ('Standing Committee on Environmental Safety and Toxic Materials',
      'CX05', [u'E.S. & T.M.']),

    ('Standing Committee on Agriculture',
      'CX01', [u'AGRI.']),

    ('Standing Committee on Banking and Finance',
      'CX27', [u'B. & F.']),

    ('Standing Committee on Jobs, Economic Development and the Economy',
      'CX34', [u'J., E.D. & E.']),

    ('Standing Committee on Accountability and Administrative Review',
      'CX02', [u'A. & A.R.']),

    ('Standing Committee on Budget',
      'CX29', [u'BUDGET.'])
    ]

committee_data_both = committee_data_upper + committee_data_lower


def slugify(s):
    return re.sub(r'[ ,.]', '', s)


def get_committee_code_data():
    return dict((t[1], t[0]) for t in committee_data_both)


def get_committee_abbr_data():
    _committee_abbr_to_name_upper = {}
    _committee_abbr_to_name_lower = {}
    for name, code, abbrs in committee_data_upper:
        for abbr in abbrs:
            _committee_abbr_to_name_upper[slugify(abbr).lower()] = name

    for name, code, abbrs in committee_data_lower:
        for abbr in abbrs:
            _committee_abbr_to_name_lower[slugify(abbr).lower()] = name

    committee_data = {'upper': _committee_abbr_to_name_upper,
                      'lower': _committee_abbr_to_name_lower}
    return committee_data


def get_committee_name_regex():
    _committee_abbrs = map(operator.itemgetter(2), committee_data_both)
    _committee_abbrs = itertools.chain.from_iterable(_committee_abbrs)
    _committee_abbrs = sorted(_committee_abbrs, reverse=True, key=len)
    _committee_abbrs = map(slugify, _committee_abbrs)
    #_committee_abbrs = map(re.escape, _committee_abbrs)
    _committee_abbr_regex = ['%s' % '[ .,]*'.join(list(abbr)) for abbr in _committee_abbrs]
    _committee_abbr_regex = re.compile('Com\.\s+on\s+(%s)\.?' % '|'.join(_committee_abbr_regex))
    return _committee_abbr_regex


class CABillScraper(BillScraper):
    jurisdiction = 'ca'

    categorizer = CACategorizer()

    _tz = pytz.timezone('US/Pacific')

    def __init__(self, metadata, host='localhost', user=None, pw=None,
                 db='capublic', **kwargs):
        super(CABillScraper, self).__init__(metadata, **kwargs)

        if user is None:
            user = os.environ.get('MYSQL_USER',
                                  getattr(settings, 'MYSQL_USER', ''))
        if pw is None:
            pw = os.environ.get('MYSQL_PASSWORD',
                                getattr(settings, 'MYSQL_PASSWORD', ''))

        if (user is not None) and (pw is not None):
            conn_str = 'mysql://%s:%s@' % (user, pw)
        else:
            conn_str = 'mysql://'
        conn_str = '%s%s/%s?charset=utf8' % (
            conn_str, host, db)
        self.engine = create_engine(conn_str)
        self.Session = sessionmaker(bind=self.engine)
        self.session = self.Session()

    def committee_code_to_name(self, code,
        committee_code_to_name=get_committee_code_data()):
        '''Need to map committee codes to names.
        '''
        return committee_code_to_name[code]

    def committee_abbr_to_name(self, chamber, abbr,
            committee_abbr_to_name=get_committee_abbr_data(),
            slugify=slugify):
        abbr = slugify(abbr).lower()
        try:
            return committee_abbr_to_name[chamber][slugify(abbr)]
        except KeyError:
            try:
                other_chamber = {'upper': 'lower', 'lower': 'upper'}[chamber]
            except KeyError:
                raise KeyError
            return committee_abbr_to_name[other_chamber][slugify(abbr)]

    def scrape(self, chamber, session):
        self.validate_session(session)

        bill_types = {
            'lower': {
                'AB': 'bill',
                'ACA': 'constitutional amendment',
                'ACR': 'concurrent resolution',
                'AJR': 'joint resolution',
                'HR': 'resolution',
                },
            'upper': {
                'SB': 'bill',
                'SCA': 'constitutional amendment',
                'SCR': 'concurrent resolution',
                'SR': 'resolution',
                }
            }

        for abbr, type_ in bill_types[chamber].items():
            self.scrape_bill_type(chamber, session, type_, abbr)

    def scrape_bill_type(self, chamber, session, bill_type, type_abbr,
            committee_abbr_regex=get_committee_name_regex()):

        if chamber == 'upper':
            chamber_name = 'SENATE'
        else:
            chamber_name = 'ASSEMBLY'

        bills = self.session.query(CABill).filter_by(
            session_year=session).filter_by(
            measure_type=type_abbr)

        for bill in bills:
            bill_session = session
            if bill.session_num != '0':
                bill_session += ' Special Session %s' % bill.session_num

            bill_id = bill.short_bill_id

            fsbill = Bill(bill_session, chamber, bill_id, '')

            # # Construct session for web query, going from '20092010' to '0910'
            # source_session = session[2:4] + session[6:8]

            # # Turn 'AB 10' into 'ab_10'
            # source_num = "%s_%s" % (bill.measure_type.lower(),
            #                         bill.measure_num)

            # Construct a fake source url
            source_url = ('http://leginfo.legislature.ca.gov/faces/'
                          'billNavClient.xhtml?bill_id=%s') % bill.bill_id

            fsbill.add_source(source_url)
            fsbill.add_version(bill_id, source_url, 'text/html')

            title = ''
            type_ = ['bill']
            subject = ''
            all_titles = set()

            # Get digest test (aka "summary") from latest version.
            if bill.versions:
                version = bill.versions[-1]
                nsmap = version.xml.nsmap
                xpath = '//caml:DigestText/xhtml:p'
                els = version.xml.xpath(xpath, namespaces=nsmap)
                chunks = []
                for el in els:
                    t = etree_text_content(el)
                    t = re.sub(r'\s+', ' ', t)
                    t = re.sub(r'\)(\S)', lambda m: ') %s' % m.group(1), t)
                    chunks.append(t)
                summary = '\n\n'.join(chunks)

            for version in bill.versions:
                if not version.bill_xml:
                    continue

                # CA is inconsistent in that some bills have a short title
                # that is longer, more descriptive than title.
                if bill.measure_type in ('AB', 'SB'):
                    impact_clause = clean_title(version.title)
                    title = clean_title(version.short_title)
                else:
                    impact_clause = None
                    if len(version.title) < len(version.short_title) and \
                            not version.title.lower().startswith('an act'):
                        title = clean_title(version.short_title)
                    else:
                        title = clean_title(version.title)

                if title:
                    all_titles.add(title)

                type_ = [bill_type]

                if version.appropriation == 'Yes':
                    type_.append('appropriation')
                if version.fiscal_committee == 'Yes':
                    type_.append('fiscal committee')
                if version.local_program == 'Yes':
                    type_.append('local program')
                if version.urgency == 'Yes':
                    type_.append('urgency')
                if version.taxlevy == 'Yes':
                    type_.append('tax levy')

                if version.subject:
                    subject = clean_title(version.subject)

            if not title:
                self.warning("Couldn't find title for %s, skipping" % bill_id)
                continue

            fsbill['title'] = title
            fsbill['summary'] = summary
            fsbill['type'] = type_
            fsbill['subjects'] = filter(None, [subject])
            fsbill['impact_clause'] = impact_clause

            # We don't want the current title in alternate_titles
            all_titles.remove(title)

            fsbill['alternate_titles'] = list(all_titles)

            for author in version.authors:
                if author.house == chamber_name:
                    fsbill.add_sponsor(SPONSOR_TYPES[author.contribution],
                                       author.name,
                                       official_type=author.contribution)

            seen_actions = set()
            for action in bill.actions:
                if not action.action:
                    # NULL action text seems to be an error on CA's part,
                    # unless it has some meaning I'm missing
                    continue
                actor = action.actor or chamber
                actor = actor.strip()
                match = re.match(r'(Assembly|Senate)($| \(Floor)', actor)
                if match:
                    actor = {'Assembly': 'lower',
                             'Senate': 'upper'}[match.group(1)]
                elif actor.startswith('Governor'):
                    actor = 'other'
                else:
                    def replacer(matchobj):
                        if matchobj:
                            return {'Assembly': 'lower',
                                    'Senate': 'upper'}[matchobj.group()]
                        else:
                            return matchobj.group()

                    actor = re.sub(r'^(Assembly|Senate)', replacer, actor)

                type_ = []

                act_str = action.action
                act_str = re.sub(r'\s+', ' ', act_str)

                attrs = self.categorizer.categorize(act_str)

                # Add in the committee strings of the related committees, if any.
                kwargs = attrs
                matched_abbrs = committee_abbr_regex.findall(action.action)

                if 'Com. on' in action.action and not matched_abbrs:
                    msg = 'Failed to extract committee abbr from %r.'
                    self.logger.warning(msg % action.action)

                if matched_abbrs:
                    committees = []
                    for abbr in matched_abbrs:
                        try:
                            name = self.committee_abbr_to_name(chamber, abbr)
                        except KeyError:
                            msg = ('Mapping contains no committee name for '
                                   'abbreviation %r. Action text was %r.')
                            args = (abbr, action.action)
                            raise KeyError(msg % args)
                        else:
                            committees.append(name)

                    committees = filter(None, committees)
                    kwargs['committees'] = committees

                    code = re.search(r'C[SXZ]\d+', actor)
                    if code is not None:
                        code = code.group()
                        kwargs['actor_info'] = {'committee_code': code}

                    assert len(committees) == len(matched_abbrs)
                    for committee, abbr in zip(committees, matched_abbrs):
                        act_str = act_str.replace('Com. on ' + abbr, committee)
                        act_str = act_str.replace(abbr, committee)

                changed = False
                for string in ['upper', 'lower', 'joint']:
                    if actor.startswith(string):
                        actor = string
                        changed = True
                        break
                if not changed:
                    actor = 'other'
                if actor != action.actor:
                    actor_info = kwargs.get('actor_info', {})
                    actor_info['details'] = action.actor
                    kwargs['actor_info'] = actor_info

                # Add strings for related legislators, if any.
                rgx = '(?:senator|assembly[mwp][^ .,:;]+)\s+[^ .,:;]+'
                legislators = re.findall(rgx, action.action, re.I)
                if legislators:
                    kwargs['legislators'] = legislators

                date = action.action_date.date()
                if (actor, act_str, date) in seen_actions:
                    continue
                fsbill.add_action(actor, act_str, date, **kwargs)
                seen_actions.add((actor, act_str, date))

            for vote in bill.votes:
                if vote.vote_result == '(PASS)':
                    result = True
                else:
                    result = False

                full_loc = vote.location.description
                first_part = full_loc.split(' ')[0].lower()
                if first_part in ['asm', 'assembly']:
                    vote_chamber = 'lower'
                    vote_location = ' '.join(full_loc.split(' ')[1:])
                elif first_part.startswith('sen'):
                    vote_chamber = 'upper'
                    vote_location = ' '.join(full_loc.split(' ')[1:])
                else:
                    raise ScrapeError("Bad location: %s" % full_loc)

                if vote.motion:
                    motion = vote.motion.motion_text or ''
                else:
                    motion = ''

                if "Third Reading" in motion or "3rd Reading" in motion:
                    vtype = 'passage'
                elif "Do Pass" in motion:
                    vtype = 'passage'
                else:
                    vtype = 'other'

                motion = motion.strip()

                # Why did it take until 2.7 to get a flags argument on re.sub?
                motion = re.compile(r'(\w+)( Extraordinary)? Session$',
                                    re.IGNORECASE).sub('', motion)
                motion = re.compile(r'^(Senate|Assembly) ',
                                    re.IGNORECASE).sub('', motion)
                motion = re.sub(r'^(SCR|SJR|SB|AB|AJR|ACR)\s?\d+ \w+\.?  ',
                                '', motion)
                motion = re.sub(r' \(\w+\)$', '', motion)
                motion = re.sub(r'(SCR|SB|AB|AJR|ACR)\s?\d+ \w+\.?$',
                                '', motion)
                motion = re.sub(r'(SCR|SJR|SB|AB|AJR|ACR)\s?\d+ \w+\.? '
                                r'Urgency Clause$',
                                '(Urgency Clause)', motion)
                motion = re.sub(r'\s+', ' ', motion)

                if not motion:
                    self.warning("Got blank motion on vote for %s" % bill_id)
                    continue

                fsvote = Vote(vote_chamber,
                              self._tz.localize(vote.vote_date_time),
                              motion,
                              result,
                              int(vote.ayes),
                              int(vote.noes),
                              int(vote.abstain),
                              threshold=vote.threshold,
                              type_=vtype)

                if vote_location != 'Floor':
                    fsvote['committee'] = vote_location

                for record in vote.votes:
                    if record.vote_code == 'AYE':
                        fsvote.yes(record.legislator_name)
                    elif record.vote_code.startswith('NO'):
                        fsvote.no(record.legislator_name)
                    else:
                        fsvote.other(record.legislator_name)

                for s in ('yes', 'no', 'other'):
                    # Kill dupe votes.
                    key = s + '_votes'
                    fsvote[key] = list(set(fsvote[key]))

                # In a small percentage of bills, the integer vote counts
                # are inaccurate, so let's ignore them.
                for k in ('yes', 'no', 'other'):
                    fsvote[k + '_count'] = len(fsvote[k + '_votes'])

                fsbill.add_vote(fsvote)

            self.save_bill(fsbill)


def etree_text_content(el):
    el = html.fromstring(etree.tostring(el))
    return el.text_content()

########NEW FILE########
__FILENAME__ = committees
# -*- coding: utf-8 -*-
import re
import collections
from operator import methodcaller

import lxml.html
import scrapelib
import requests.exceptions

from billy.scrape.committees import CommitteeScraper, Committee
from .utils import Urls


strip = methodcaller('strip')


def clean(s):
    s = s.strip(u'\xa0 \n\t').replace(u'\xa0', ' ')
    s = re.sub(r'[\s+\xa0]', ' ', s)
    return s.strip()


class CACommitteeScraper(CommitteeScraper):

    jurisdiction = 'ca'

    urls = {'upper': 'http://senate.ca.gov/committees',
            'lower': 'http://assembly.ca.gov/committees'}

    base_urls = {'upper': 'http://senate.ca.gov/',
                 'lower': 'http://assembly.ca.gov/'}

    def scrape(self, chamber, term):
        if chamber == 'lower':
            self.scrape_lower(chamber, term)
        elif chamber == 'upper':
            self.scrape_upper(chamber, term)


    def scrape_lower(self, chamber, term):
        url = self.urls[chamber]
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(self.base_urls[chamber])

        committee_types = {'upper': ['Standing', 'Select', 'Joint'],
                           'lower': ['Standing', 'Select']}

        for type_ in committee_types[chamber]:

            if type_ == 'Joint':
                _chamber = type_.lower()
            else:
                _chamber = chamber

            for xpath in [
                '//div[contains(@class, "view-view-%sCommittee")]' % type_,
                '//div[contains(@id, "block-views-view_StandingCommittee-block_1")]',
                '//div[contains(@class, "views-field-title")]',
                ]:
                div = doc.xpath(xpath)
                if div:
                    break

            div = div[0]
            committees = div.xpath('descendant::span[@class="field-content"]/a/text()')
            committees = map(strip, committees)
            urls = div.xpath('descendant::span[@class="field-content"]/a/@href')

            for c, _url in zip(committees, urls):

                if 'autism' in _url:
                    # The autism page takes a stunning 10 minutes to respond
                    # with a 403. Skip it.
                    continue

                if c.endswith('Committee'):
                    if type_ not in c:
                        c = '%s %s' % (type_, c)
                elif ('Subcommittee' not in c and 'Committee on' not in c):
                    c = '%s Committee on %s' % (type_, c)
                else:
                    if type_ not in c:
                        c = '%s %s' % (type_, c)

                c = Committee(_chamber, c)
                c.add_source(_url)
                c.add_source(url)
                for member, role, kw in self.scrape_membernames(c, _url,
                        chamber, term):
                    c.add_member(member, role, **kw)

                _found = False
                if len(c['members']) == 0:
                    for member, role, kw in self.scrape_membernames(c,
                            _url + '/membersstaff', chamber, term):
                        _found = True
                        c.add_member(member, role, **kw)
                    if _found:
                        source = _url + '/membersstaff'
                        c.add_source(source)

                if len(c['members']) == 0:
                    # Some committees weren't staff in early
                    # 2013; opting to skip rather than blow
                    # up the whole scrape.
                    return
                    cname = c['committee']
                    msg = '%r must have at least one member.'
                    raise ValueError(msg % cname)

                if c['members']:
                    self.save_committee(c)

        # Subcommittees
        div = doc.xpath('//div[contains(@class, "view-view-SubCommittee")]')[0]
        for subcom in div.xpath('div/div[@class="item-list"]'):
            committee = subcom.xpath('h4/text()')[0]
            names = subcom.xpath('descendant::a/text()')
            names = map(strip, names)
            urls = subcom.xpath('descendant::a/@href')
            committee = 'Standing Committee on ' + committee
            for n, _url in zip(names, urls):
                c = Committee(chamber, committee, subcommittee=n)
                c.add_source(_url)
                c.add_source(url)

                for member, role, kw in self.scrape_membernames(c, _url,
                        chamber, term):
                    c.add_member(member, role, **kw)

                _found = False
                if len(c['members']) == 0:
                    for member, role, kw in self.scrape_membernames(c,
                            _url + '/membersstaff', chamber, term):
                        _found = True
                        c.add_member(member, role, **kw)
                    if _found:
                        source = _url + '/membersstaff'
                        c.add_source(source)

                if len(c['members']) == 0:
                    # Some committees weren't staff in early
                    # 2013; opting to skip rather than blow
                    # up the whole scrape.
                    return
                    cname = c['committee']
                    msg = '%r must have at least one member.'
                    raise ValueError(msg % cname)

                if c['members']:
                    self.save_committee(c)

    def scrape_membernames(self, committee, url, chamber, term):
        '''Scrape the member names from this page.
        '''

        # Special-case senate subcomittees.
        if url == 'http://sbud.senate.ca.gov/subcommittees1':
            return self.scrape_members_senate_subcommittees(
                committee, url, chamber, term)

        # Many of the urls don't actually display members. Swap them for ones
        # that do.
        corrected_urls = (('http://autism.senate.ca.gov',
                 'http://autism.senate.ca.gov/committeemembers1'),

                ('Sub Committee on Sustainable School Facilities',
                 'http://sedn.senate.ca.gov/substainableschoolfacilities'),

                ('Sustainable School Facilities',
                 'http://sedn.senate.ca.gov/substainableschoolfacilities'),

                ('Sub Committee on Education Policy Research',
                 'http://sedn.senate.ca.gov/policyresearch'),

                ('Education Policy Research',
                 'http://sedn.senate.ca.gov/policyresearch'))

        corrected_urls = dict(corrected_urls)

        cname = committee['subcommittee']
        for key in url, cname:
            if key in corrected_urls:
                url = corrected_urls[key]
                #committee['sources'].pop()
                committee.add_source(url)
                break

        # Now actually try to get the names.
        try:
            html = self.urlopen(url)
        except (scrapelib.HTTPError, requests.exceptions.ConnectionError):
            self.warning('Bogus committee page link: %r' % url)
            return []

        doc = lxml.html.fromstring(html)

        links = doc.xpath('//a')
        names = Membernames.extract(links)
        names = Membernames.scrub(names)
        return names

    def scrape_members_senate_subcommittees(self, committee, url, chamber,
                                            term, cache={}):

        if cache:
            names = cache[committee['subcommittee']]
            return Membernames.scrub(names)

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        # Commence horrific regex-based hackery to get subcommittee members.
        text = doc.xpath('//div[@class="content"]')[0].text_content()
        chunks = re.split(r'\s*Subcommittee.*', text)
        namelists = []
        for c in chunks:
            names = re.sub(r'\s*Members\s*', '', c)
            names = re.split(r'\s*(,|and)\s*', names)
            names = filter(lambda s: s not in [',', 'and'], names)
            names = map(clean, names)
            if filter(None, names):
                namelists.append(names)

        committee_names = doc.xpath('//div[@class="content"]/h3/text()')
        committee_names = filter(None, map(clean, committee_names))
        for _committee, _names in zip(committee_names, namelists):
            if _committee:
                cache[_committee] = _names

        names = cache[committee['subcommittee']]
        return Membernames.scrub(names)

    def scrape_upper(self, chamber, term):
        for committee_type in SenateCommitteePage(self):
            for senate_committee in committee_type:
                comm = senate_committee.get_committee_obj()
                self.save_committee(comm)

class Membernames(object):

    @staticmethod
    def extract(links):
        '''Given an lxml.xpath result, extract a list of member names.
        '''
        href_rgxs = (r'(sd|a)\d+\.(senate|assembly)\.ca\.gov/$',
                     r'(senate|assembly)\.ca\.gov/(sd|a)\d+$',
                     r'(sd|a)\d+$',
                     r'dist\d+[.]',
                     r'cssrc[.]us/web/\d+',
                     r'/Wagner')

        res = collections.defaultdict(list)
        for a in links:
            try:
                href = a.attrib['href']
            except KeyError:
                continue
            for rgx in href_rgxs:
                if re.search(rgx, href, re.M):
                    res[href].append(a.text_content().strip())
        vals = [' '.join(set(lst)) for lst in res.values()]
        return [re.sub('\s+', ' ', s) for s in vals]

    @staticmethod
    def scrub(names):
        '''Separate names from roles and chambers, etc.
        '''
        role_rgxs = [r'(.+?)\s+\((.+?)\)',
                     r'(.+?),\s+(?![III])(.+)',
                     r'(.+?),\s+(?![JS]r.)(.+)',
                     ur'(.+?)\s*[-–]\s+(.+)']

        res = []
        for name in names:

            name = clean(name)

            role = 'member'
            for rgx in role_rgxs:
                m = re.match(rgx, name)
                if m:
                    name, role = m.groups()
                    break

            # Special case for Isadore hall. This entire hot mess needs
            # a re-write at some point.
            if role == 'III':
                role = 'member'

            kw = {}
            for s, ch in (('Senator', 'upper'),
                          ('Assemblymember', 'lower')):
                if s in name:
                    kw = {'chamber': ch}
            name = re.sub(r'^(Senator|Assemblymember)', '', name)
            name = name.strip()

            if name:
                if 'Sanator' in name:
                    name = name.replace('Sanator', 'Senator')
                if name.endswith(' (Chair'):
                    role = 'chair'
                    name = name.replace(' (Chair', '')
                name.strip(',').strip()
                res.append((name, role, kw))

        return res


# -----------------------------------------------------------------------------
# Senate classes.
# -----------------------------------------------------------------------------
class SenateCommitteePage(object):
    '''The senate re-did their committee page in 2014. This class is an
    iterator over each group of committees (Standing, Select, Sub, etc.)
    '''
    urls = dict(index='http://senate.ca.gov/committees')

    def __init__(self, scraper):
        self.urls = Urls(scraper, self.urls)

    def __iter__(self):
        xpath = '//div[contains(@class, "block-views")]'
        for el in self.urls.index.xpath(xpath):
            yield SenateCommitteeGroup(self.urls, el)


class SenateCommitteeGroup(object):
    '''An iterator of the committees within this group.
    '''
    def __init__(self, urls, div):
        self.urls = urls
        self.div = div

    def get_type(self):
        return self.div.xpath('./h2/text()').pop()

    def __iter__(self):
        xpath = 'div[@class="content"]//li[contains(@class, "views-row")]'
        type_ = self.get_type()

        # Join committees currently get scraped in the Assembly scraper.
        if type_ == 'Joint Committees':
            return

        for li in self.div.xpath(xpath):
            yield SenateCommittee(self.urls, type_, li)


class SenateCommittee(object):
    '''Helper to get info about a given committee.
    '''
    def __init__(self, urls, type_, li):
        self.urls = urls
        self.type_ = type_
        self.li = li

    def get_type(self):
        return self.type_.replace(' Committees', '')

    def get_name(self):
        name = self.li.xpath(".//a")[0].text_content()
        type_ = self.get_type()
        if type_ == 'Sub-Committees':
            return name
        return '%s Committee on %s' % (self.get_type(), name)

    def get_url(self):
        return self.li.xpath(".//a")[0].attrib['href']

    def get_parent_name(self):
        '''Get the name of the parent committee if this is a subcommittee.
        '''
        parent = self.li.xpath("../../h3/text()")
        if parent:
           return 'Standing Committee on ' + parent[0]

    def get_committee_obj(self):
        name = self.get_name()
        url = self.get_url()
        parent_name = self.get_parent_name()

        if parent_name is not None:
            subcommittee = name
            committee_name = parent_name
        else:
            subcommittee = None
            committee_name = name

        self.committee = Committee(
            'upper', committee_name, subcommittee=subcommittee)

        self.add_members()
        self.add_sources()
        return self.committee

    def add_members(self):
        url = self.get_url()
        self.urls.add(detail=url)
        for name, role in SenateMembers(self.urls):
            if name.strip():
                self.committee.add_member(name, role)

    def add_sources(self):
        for url in self.urls:
            self.committee.add_source(url.url)


class SenateMembers(object):

    def __init__(self, urls):
        self.urls = urls

    def get_a_list(self):
        xpath = '//h2/following-sibling::p//a'
        a_list = self.urls.detail.xpath(xpath)
        if a_list:
            return a_list

        xpath = '//div[@class="content"]'
        a_list = self.urls.detail.xpath(xpath)[0].xpath('.//a')
        return a_list

    def get_name_role(self, text):
        role = 'member'
        rgxs = [r"\((.+?)\)", r"\((.+)"]
        for rgx in rgxs:
            role_match = re.search(rgx, text)
            if role_match:
                role = role_match.group(1)
                text = re.sub(rgx, '', text).strip()
        return text, role

    def __iter__(self):
        for a in self.get_a_list():
            yield self.get_name_role(a.text_content())


########NEW FILE########
__FILENAME__ = download
'''
This file defines functions for importing the CA database dumps in mysql.

The workflow is:
 - Drop & recreate the local capublic database.
 - Inspect the FTP site with regex and determine which files have been updated, if any.
 - For each such file, unzip it & call import.
'''
import sys
import os
import re
import glob
import os.path
import zipfile
import subprocess
import logging
import urllib
from datetime import datetime
from os.path import join, split
from functools import partial
from collections import namedtuple

import MySQLdb
import _mysql_exceptions

from billy.core import settings


MYSQL_USER = getattr(settings, 'MYSQL_USER', 'root')
MYSQL_USER = os.environ.get('MYSQL_USER', MYSQL_USER)

MYSQL_PASSWORD = getattr(settings, 'MYSQL_PASSWORD', '')
MYSQL_PASSWORD = os.environ.get('MYSQL_PASSWORD', MYSQL_PASSWORD)

BASE_URL = 'ftp://www.leginfo.ca.gov/pub/bill/'


# ----------------------------------------------------------------------------
# Logging config
logger = logging.getLogger('billy.ca-update')
# logger.setLevel(logging.INFO)

# ch = logging.StreamHandler()
# formatter = logging.Formatter('%(asctime)s - %(message)s',
#                               datefmt='%H:%M:%S')
# ch.setFormatter(formatter)
# logger.addHandler(ch)

# ---------------------------------------------------------------------------
# Miscellaneous db admin commands.


def clean_text(s):
    # replace smart quote characters
    s = re.sub(ur'[\u2018\u2019]', "'", s)
    s = re.sub(ur'[\u201C\u201D]', '"', s)
    s = s.replace(u'\xe2\u20ac\u02dc', "'")
    return s


def db_drop():
    '''Drop the database.'''
    logger.info('dropping capublic...')

    try:
        connection = MySQLdb.connect(user=MYSQL_USER, passwd=MYSQL_PASSWORD, db='capublic')
    except _mysql_exceptions.OperationalError:
        # The database doesn't exist.
        logger.info('...no such database. Bailing.')
        return

    connection.autocommit(True)
    cursor = connection.cursor()

    cursor.execute('DROP DATABASE IF EXISTS capublic;')

    connection.close()
    logger.info('...done.')



# ---------------------------------------------------------------------------
# Functions for updating the data.
def load_bill_versions(connection):
    '''
    Given a data folder, read its BILL_VERSION_TBL.dat file in python,
    construct individual REPLACE statements and execute them one at
    a time. This method is slower that letting mysql do the import,
    but doesn't fail mysteriously.
    '''
    DatRow = namedtuple('DatRow', [
                      'bill_version_id', 'bill_id', 'version_num',
                      'bill_version_action_date', 'bill_version_action',
                      'request_num', 'subject', 'vote_required',
                      'appropriation', 'fiscal_committee', 'local_program',
                      'substantive_changes', 'urgency', 'taxlevy',
                      'bill_xml', 'active_flg', 'trans_uid', 'trans_update'])

    def dat_row_2_tuple(row):
        '''Convert a row in the bill_version_tbl.dat file into a
        namedtuple.
        '''
        cells = row.split('\t')
        res = []
        for cell in cells:
            if cell.startswith('`') and cell.endswith('`'):
                res.append(cell[1:-1])
            elif cell == 'NULL':
                res.append(None)
            else:
                res.append(cell)
        return DatRow(*res)

    sql = '''
        REPLACE INTO capublic.bill_version_tbl (
            BILL_VERSION_ID,
            BILL_ID,
            VERSION_NUM,
            BILL_VERSION_ACTION_DATE,
            BILL_VERSION_ACTION,
            REQUEST_NUM,
            SUBJECT,
            VOTE_REQUIRED,
            APPROPRIATION,
            FISCAL_COMMITTEE,
            LOCAL_PROGRAM,
            SUBSTANTIVE_CHANGES,
            URGENCY,
            TAXLEVY,
            BILL_XML,
            ACTIVE_FLG,
            TRANS_UID,
            TRANS_UPDATE)

        VALUES (%s)
        '''
    sql = sql % ', '.join(['%s'] * 18)

    cursor = connection.cursor()
    with open('BILL_VERSION_TBL.dat') as f:
        for row in f:
            # The files are supposedly already in utf-8, but with
            # copious bogus characters.
            row = clean_text(row.decode('utf-8')).encode('utf-8')
            row = dat_row_2_tuple(row)
            with open(row.bill_xml) as f:
                text = f.read().decode('utf-8')
                text = clean_text(text).encode('utf-8')
                row = row._replace(bill_xml=text)
                cursor.execute(sql, tuple(row))

    cursor.close()


def load(folder, sql_name=partial(re.compile(r'\.dat$').sub, '.sql')):
    '''
    Import into mysql any .dat files located in `folder`.

    First get a list of filenames like *.dat, then for each, execute
    the corresponding .sql file after swapping out windows paths for
    `folder`.

    This function doesn't bother to delete the imported data files
    afterwards; they'll be overwritten within a week, and leaving them
    around makes testing easier (they're huge).
    '''

    logger.info('Loading data from %s...' % folder)
    os.chdir(folder)

    connection = MySQLdb.connect(user=MYSQL_USER, passwd=MYSQL_PASSWORD,
                                 db='capublic', local_infile=1)
    connection.autocommit(True)

    filenames = glob.glob('*.dat')

    for filename in filenames:

        # The corresponding sql file is in data/ca/dbadmin
        _, filename = split(filename)
        sql_filename = join('../pubinfo_load', sql_name(filename).lower())
        with open(sql_filename) as f:

            # Swap out windows paths.
            script = f.read().replace(r'c:\\pubinfo\\', folder)

        _, sql_filename = split(sql_filename)
        logger.info('loading ' + sql_filename)
        if sql_filename == 'bill_version_tbl.sql':
            logger.info('inserting xml files (slow)')
            load_bill_versions(connection)
        else:
            cursor = connection.cursor()
            cursor.execute(script)
            cursor.close()

    connection.close()
    os.chdir('..')
    logging.info('...Done loading from %s' % folder)


def delete_session(session_year):
    '''
    This is the python equivalent (or at least, is supposed to be)
    of the deleteSession.bat file included in the pubinfo_load.zip file.

    It deletes all the entries for the specified session.
    Used before the weekly import of the new database dump on Sunday.
    '''
    tables = {
        'bill_id': [
            'bill_detail_vote_tbl',
            'bill_history_tbl',
            'bill_summary_vote_tbl',
            'bill_analysis_tbl',
            'bill_tbl',
            'committee_hearing_tbl',
            'daily_file_tbl'
            ],

        'bill_version_id': [
            'bill_version_authors_tbl',
            'bill_version_tbl'
            ],

        'session_year': [
            'legislator_tbl',
            'location_code_tbl'
            ]
        }

    logger.info('Deleting all data for session year %s...' % session_year)

    connection = MySQLdb.connect(user=MYSQL_USER, passwd=MYSQL_PASSWORD,
                                 db='capublic')
    connection.autocommit(True)
    cursor = connection.cursor()

    for token, names in tables.items():
        for table_name in names:
            sql = ("DELETE FROM capublic.{table_name} "
                   "where {token} like '{session_year}%';")
            sql = sql.format(**locals())
            logger.debug('executing sql: "%s"' % sql)
            cursor.execute(sql)

    cursor.close()
    connection.close()
    logger.info('...done deleting session data.')


def db_create():
    '''Create the database'''

    logger.info('Creating capublic...')

    dirname = get_zip('pubinfo_load.zip')
    os.chdir(dirname)

    with open('capublic.sql') as f:
        # Note: apparently MySQLdb can't execute compound SQL statements,
        # so we have to split them up.
        sql_statements = f.read().split(';')

    connection = MySQLdb.connect(user=MYSQL_USER, passwd=MYSQL_PASSWORD)
    connection.autocommit(True)
    cursor = connection.cursor()

    # MySQL warns in OCD fashion when executing statements relating to
    # a table that doesn't exist yet. Shush, mysql...
    import warnings
    warnings.filterwarnings('ignore', 'Unknown table.*')

    for sql in sql_statements:
        cursor.execute(sql)

    cursor.close()
    connection.close()
    os.chdir('..')


def get_contents():
    resp = {}
    for line in urllib.urlopen(BASE_URL).read().splitlines()[1:]:
        date, filename = re.match('[drwx-]{10}\s+\d\s+\d{3}\s+\d{3}\s+\d+ (\w+\s+\d+\s+\d+:?\d*) (\w+.\w+)', line).groups()
        date = date.replace('  ', ' ')
        try:
            date = datetime.strptime(date, '%b %d %Y')
        except ValueError:
            date = datetime.strptime(date, '%b %d %H:%M')
            date = date.replace(year=datetime.now().year)
        resp[filename] = date
    return resp

def _check_call(*args):
    logging.info('calling ' + ' '.join(args))
    subprocess.check_call(args)

def get_zip(filename):
    dirname = filename.replace('.zip', '')
    _check_call('wget', BASE_URL + filename)
    _check_call('rm', '-rf', dirname)
    _check_call('unzip', filename, '-d', dirname)
    _check_call('rm', '-rf', filename)
    return dirname

def get_current_year(contents):
    newest_file = '2000'
    newest_file_date = datetime(2000, 1, 1)
    files_to_get = []
    dirnames = []

    # get file for latest year
    for filename, date in contents.items():
        date_part = filename.replace('pubinfo_', '').replace('.zip', '')
        if date_part.startswith('20') and filename > newest_file:
            newest_file = filename
            newest_file_date = date
    files_to_get.append(newest_file)

    # get files for days since last update
    days = ('pubinfo_Mon.zip', 'pubinfo_Tue.zip', 'pubinfo_Wed.zip', 'pubinfo_Thu.zip',
            'pubinfo_Fri.zip', 'pubinfo_Sat.zip')
    for dayfile in days:
        if contents[dayfile] > newest_file_date:
            files_to_get.append(dayfile)

    for file in files_to_get:
        dirname = get_zip(file)
        load(dirname)

if __name__ == '__main__':
    db_drop()
    db_create()
    contents = get_contents()
    get_current_year(contents)

########NEW FILE########
__FILENAME__ = events
import os
import re
from collections import defaultdict

from billy.core import settings
from billy.scrape.events import EventScraper, Event
from .models import CACommitteeHearing, CALocation

from sqlalchemy.orm import sessionmaker
from sqlalchemy import create_engine

import pytz


class CAEventScraper(EventScraper):
    jurisdiction = 'ca'

    _tz = pytz.timezone('US/Pacific')

    def __init__(self, metadata, host='127.0.0.1', user='', pw='',
                 db='capublic', **kwargs):
        super(CAEventScraper, self).__init__(metadata, **kwargs)

        if not user:
            user = os.environ.get('MYSQL_USER',
                                  getattr(settings, 'MYSQL_USER', ''))
        if not pw:
            pw = os.environ.get('MYSQL_PASSWORD',
                                getattr(settings, 'MYSQL_PASSWORD', ''))

        if user and pw:
            conn_str = 'mysql://%s:%s@' % (user, pw)
        else:
            conn_str = 'mysql://'
        conn_str = '%s%s/%s?charset=utf8' % (
            conn_str, host, db)
        self.engine = create_engine(conn_str)
        self.Session = sessionmaker(bind=self.engine)
        self.session = self.Session()

    def scrape(self, chamber, session):
        grouped_hearings = defaultdict(list)

        for hearing in self.session.query(CACommitteeHearing):
            location = self.session.query(CALocation).filter_by(
                location_code=hearing.location_code)[0].description

            date = self._tz.localize(hearing.hearing_date)

            chamber_abbr = location[0:3]
            event_chamber = {'Asm': 'lower', 'Sen': 'upper'}[chamber_abbr]

            if event_chamber != chamber:
                continue

            grouped_hearings[(location, date)].append(hearing)

        for ((location, date), hearings) in grouped_hearings.iteritems():

            # Get list of bill_ids from the database.
            bill_ids = [hearing.bill_id for hearing in hearings]
            bills = ["%s %s" % re.match(r'\d+([^\d]+)(\d+)', bill).groups()
                     for bill in bill_ids]

            # Dereference the committee_nr number and get display name.
            msg = 'More than one committee meeting at (location, date) %r'
            msg = msg % ((location, date),)
            assert len(set(hearing.committee_nr for hearing in hearings)) == 1, msg
            committee_name = _committee_nr[hearings.pop().committee_nr]

            desc = 'Committee Meeting: ' + committee_name
            event = Event(session, date, 'committee:meeting', desc,
                          location=committee_name)
            for bill_id in bills:
                if 'B' in bill_id:
                    type_ = 'bill'
                else:
                    type_ = 'resolution'
                event.add_related_bill(bill_id, type=type_,
                                       description='consideration')

            event.add_participant('host', committee_name + ' Committee',
                                  'committee', chamber=chamber)
            event.add_source('ftp://www.leginfo.ca.gov/pub/bill/')

            self.save_event(event)

# A mapping of committee_nr numbers to committee names they
# (probably) represent, based on direct correlation they bore
# to hearing locations that resemble committee names in
# the location_code_tbl in the db dump.
_committee_nr = {
    1L: u'Assembly Agriculture',
    2L: u'Assembly Accountability and Administrative Review',
    3L: u'Assembly Education',
    4L: u'Assembly Elections and Redistricting',
    5L: u'Assembly Environmental Safety and Toxic Materials',
    6L: u'Assembly Budget X1',
    7L: u'Assembly Governmental Organization',
    8L: u'Assembly Health',
    9L: u'Assembly Higher Education',
    10L: u'Assembly Housing and Community Development',
    11L: u'Assembly Human Services',
    13L: u'Assembly Judiciary',
    14L: u'Assembly Labor and Employment',
    15L: u'Assembly Local Government',
    16L: u'Assembly Natural Resources',
    17L: u'Assembly Public Employees, Retirement/Soc Sec',
    18L: u'Assembly Public Safety',
    19L: u'Assembly Revenue and Taxation',
    20L: u'Assembly Rules',
    22L: u'Assembly Transportation',
    23L: u'Assembly Utilities and Commerce',
    24L: u'Assembly Water, Parks and Wildlife',
    25L: u'Assembly Appropriations',
    27L: u'Assembly Banking and Finance',
    28L: u'Assembly Insurance',
    29L: u'Assembly Budget',
    31L: u'Assembly Aging and Long Term Care',
    33L: u'Assembly Business, Professions and Consumer Protection ',
    34L: u'Assembly Jobs, Economic Development, and the Economy',
    37L: u'Assembly Arts, Entertainment, Sports, Tourism, and Internet Media',
    38L: u'Assembly Veterans Affairs',
    40L: u'Senate Agriculture',
    42L: u'Senate Business, Professions and Economic Development',
    44L: u'Senate Education',
    45L: u'Senate Elections and Constitutional Amendments',
    48L: u'Senate Governmental Organization',
    51L: u'Senate Labor and Industrial Relations',
    53L: u'Senate Judiciary',
    55L: u'Senate Natural Resources and Water',
    56L: u'Senate Public Employment and Retirement',
    58L: u'Senate Rules',
    59L: u'Senate Transportation and Housing',
    60L: u'Senate Health',
    61L: u'Senate Appropriations',
    62L: u'Senate Budget and Fiscal Review',
    64L: u'Senate Environmental Quality',
    66L: u'Senate Veterans Affairs',
    69L: u'Senate Banking and Financial Institutions',
    70L: u'Senate Insurance',
    71L: u'Senate Energy, Utilities and Communications',
    72L: u'Senate Public Safety',
    73L: u'Senate Governance and Finance',
    74L: u'Senate Human Services'
 }

########NEW FILE########
__FILENAME__ = legislators
import re
import collections
from operator import methodcaller

import lxml.html

from billy.scrape.legislators import LegislatorScraper, Legislator


def parse_address(s, split=re.compile(r'[;,]\s{,3}').split):
    '''
    Extract address fields from text.
    '''
    # If the address isn't formatted correctly, skip for now.
    if ';' not in s:
        return []

    fields = 'city zip phone'.split()
    vals = split(s)
    res = []
    while True:
        try:
            _field = fields.pop()
            _value = vals.pop()
        except IndexError:
            break
        else:
            if _value.strip():
                res.append((_field, _value))
    if vals:
        res.append(('street', ', '.join(vals)))
    return res


class CALegislatorScraper(LegislatorScraper):

    jurisdiction = 'ca'

    urls = {'upper': 'http://senate.ca.gov/senators',
            'lower': 'http://assembly.ca.gov/assemblymembers'}

    def scrape(self, chamber, term):

        url = self.urls[chamber]
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        rows = doc.xpath('//table/tbody/tr')

        parse = self.parse_legislator
        for tr in rows:
            legislator = parse(tr, term, chamber)
            if legislator is None:
                continue
            # Try to spot on the janky ways the site expresses vacant seats.
            if legislator['full_name'].startswith('Vacant'):
                continue
            if '[ Vacant ]' in legislator['full_name']:
                continue
            if 'Vacant ' in legislator['full_name']:
                continue
            if 'Vacant, ' in legislator['full_name']:
                continue
            fullname = legislator['full_name']
            if not legislator['first_name'] and fullname.endswith('Vacant'):
                continue
            legislator.add_source(url)
            legislator['full_name'] = legislator['full_name'].strip()
            self.save_legislator(legislator)

    def parse_legislator(self, tr, term, chamber):
        '''
        Given a tr element, get specific data from it.
        '''

        strip = methodcaller('strip')

        xpath = 'td[contains(@class, "views-field-field-%s-%s")]%s'

        xp = {
            'url':       [('lname-value-1', '/a/@href'),
                          ('member-lname-value-1', '/a/@href')],
            'district':  [('district-value', '/text()')],
            'party':     [('party-value', '/text()')],
            'full_name': [('feedbackurl-value', '/a/text()')],
            'address':   [('feedbackurl-value', '/p/text()'),
                          ('feedbackurl-value', '/p/font/text()')]
            }

        titles = {'upper': 'senator', 'lower': 'member'}

        funcs = {
            'full_name': lambda s: s.replace('Contact Senator', '').strip(),
            'address': parse_address,
            }

        rubberstamp = lambda _: _
        tr_xpath = tr.xpath
        res = collections.defaultdict(list)
        for k, xpath_info in xp.items():
            for vals in xpath_info:
                f = funcs.get(k, rubberstamp)
                vals = (titles[chamber],) + vals
                vals = map(f, map(strip, tr_xpath(xpath % vals)))

                res[k].extend(vals)

        # Photo.
        try:
            res['photo_url'] = tr_xpath('td/p/img/@src')[0]
        except IndexError:
            pass

        # Addresses.
        addresses = res['address']
        try:
            addresses = map(dict, filter(None, addresses))
        except ValueError:
            # Sometimes legislators only have one address, in which
            # case this awful hack is helpful.
            addresses = map(dict, filter(None, [addresses]))

        for address in addresses[:]:

            # Toss results that don't have required keys.
            if not set(['street', 'city', 'zip']) < set(address):
                if address in addresses:
                    addresses.remove(address)

        # Re-key the addresses
        offices = []
        if addresses:
            # Mariko Yamada's addresses wouldn't parse correctly as of
            # 3/23/2013, so here we're forced to test whether any
            # addresses were even found.
            addresses[0].update(type='capitol', name='Capitol Office')
            offices.append(addresses[0])

            for office in addresses[1:]:
                office.update(type='district', name='District Office')
                offices.append(office)

            for office in offices:
                street = office['street']
                street = '%s\n%s, %s %s' % (street, office['city'], 'CA',
                                            office['zip'])
                office['address'] = street
                office['fax'] = None
                office['email'] = None

                del office['street'], office['city'], office['zip']

        res['offices'] = offices
        del res['address']

        # Remove junk from assembly member names.
        junk = 'Contact Assembly Member '

        try:
            res['full_name'] = res['full_name'].pop().replace(junk, '')
        except IndexError:
            return

        # Normalize party.
        for party in res['party'][:]:
            if party:
                if party == 'Democrat':
                    party = 'Democratic'
                res['party'] = party
                break
            else:
                res['party'] = None

        # Mariko Yamada also didn't have a url that lxml would parse
        # as of 3/22/2013.
        if res['url']:
            res['url'] = res['url'].pop()
        else:
            del res['url']

        # strip leading zero
        res['district'] = str(int(res['district'].pop()))

        # Add a source for the url.
        leg = Legislator(term, chamber, **res)
        leg.update(**res)

        return leg

########NEW FILE########
__FILENAME__ = models
from sqlalchemy import (Table, Column, Integer, String, ForeignKey,
                        DateTime, Numeric, desc, UnicodeText)
from sqlalchemy.sql import and_
from sqlalchemy.orm import backref, relation
from sqlalchemy.ext.declarative import declarative_base

from lxml import etree

Base = declarative_base()


class CABill(Base):
    __tablename__ = "bill_tbl"

    bill_id = Column(String(20), primary_key=True)
    session_year = Column(String(8))
    session_num = Column(String(2))
    measure_type = Column(String(4))
    measure_num = Column(Integer)
    measure_state = Column(String(40))
    chapter_year = Column(String(4))
    chapter_type = Column(String(10))
    chapter_session_num = Column(String(2))
    chapter_num = Column(String(10))
    latest_bill_version_id = Column(String(30))
    active_flg = Column(String(1))
    trans_uid = Column(String(30))
    trans_update = Column(DateTime)
    current_location = Column(String(200))
    current_secondary_loc = Column(String(60))
    current_house = Column(String(60))
    current_status = Column(String(60))

    actions = relation('CABillAction', backref=backref('bill'),
                        order_by="CABillAction.bill_history_id")

    versions = relation('CABillVersion', backref=backref('bill'),
                        order_by='desc(CABillVersion.version_num)')

    votes = relation('CAVoteSummary', backref=backref('bill'),
                     order_by='CAVoteSummary.vote_date_time')

    @property
    def short_bill_id(self):
        return "%s%d" % (self.measure_type, self.measure_num)


class CABillVersion(Base):
    __tablename__ = "bill_version_tbl"

    bill_version_id = Column(String(30), primary_key=True)
    bill_id = Column(String(19), ForeignKey(CABill.bill_id))
    version_num = Column(Integer)
    bill_version_action_date = Column(DateTime)
    bill_version_action = Column(String(100))
    request_num = Column(String(10))
    subject = Column(String(1000))
    vote_required = Column(String(100))
    appropriation = Column(String(3))
    fiscal_committee = Column(String(3))
    local_program = Column(String(3))
    substantive_changes = Column(String(3))
    urgency = Column(String(3))
    taxlevy = Column(String(3))
    bill_xml = Column(UnicodeText)
    active_flg = Column(String(1))
    trans_uid = Column(String(30))
    trans_update = Column(DateTime)

    @property
    def xml(self):
        if not '_xml' in self.__dict__:
            self._xml = etree.fromstring(self.bill_xml.encode('utf-8'),
                                         etree.XMLParser(recover=True))
        return self._xml

    @property
    def title(self):
        text = self.xml.xpath("string(//*[local-name() = 'Title'])") or ''
        return text.strip()

    @property
    def short_title(self):
        text = self.xml.xpath("string(//*[local-name() = 'Subject'])") or ''
        return text.strip()


class CABillVersionAuthor(Base):
    __tablename__ = "bill_version_authors_tbl"

    # Note: the primary_keys here are a lie - the actual table has no pk
    # but SQLAlchemy seems to demand one. Furthermore, I get strange
    # exceptions when trying to use bill_version_id as part of a
    # composite primary key.

    bill_version_id = Column(String(30),
                             ForeignKey(CABillVersion.bill_version_id))
    type = Column(String(15))
    house = Column(String(100))
    name = Column(String(100), primary_key=True)
    contribution = Column(String(100))
    committee_members = Column(String(2000))
    active_flg = Column(String(1))
    trans_uid = Column(String(30))
    trans_update = Column(DateTime, primary_key=True)
    primary_author_flg = Column(String(1))

    version = relation(CABillVersion, backref=backref('authors'))


class CABillAction(Base):
    __tablename__ = "bill_history_tbl"

    bill_id = Column(String(20), ForeignKey(CABill.bill_id))
    bill_history_id = Column(Numeric, primary_key=True)
    action_date = Column(DateTime)
    action = Column(String(2000))
    trans_uid = Column(String(20))
    trans_update_dt = Column(DateTime)
    action_sequence = Column(Integer)
    action_code = Column(String(5))
    action_status = Column(String(60))
    primary_location = Column(String(60))
    secondary_location = Column(String(60))
    ternary_location = Column(String(60))
    end_status = Column(String(60))

    @property
    def actor(self):
        # TODO: replace committee codes w/ names

        if not self.primary_location:
            return None

        actor = self.primary_location

        if self.secondary_location:
            actor += " (%s" % self.secondary_location

            if self.ternary_location:
                actor += " %s" % self.ternary_location

            actor += ")"

        return actor


class CALegislator(Base):
    __tablename__ = 'legislator_tbl'

    district = Column(String(5), primary_key=True)
    session_year = Column(String(8), primary_key=True)
    legislator_name = Column(String(30), primary_key=True)
    house_type = Column(String(1), primary_key=True)
    author_name = Column(String(200))
    first_name = Column(String(30))
    last_name = Column(String(30))
    middle_initial = Column(String(1))
    name_suffix = Column(String(12))
    name_title = Column(String(34))
    web_name_title = Column(String(34))
    party = Column(String(4))
    active_flg = Column(String(1))
    trans_uid = Column(String(30))
    trans_update = Column(DateTime)


class CAMotion(Base):
    __tablename__ = "bill_motion_tbl"

    motion_id = Column(Integer, primary_key=True)
    motion_text = Column(String(250))
    trans_uid = Column(String(30))
    trans_update = Column(DateTime)


class CALocation(Base):
    __tablename__ = "location_code_tbl"

    session_year = Column(String(8), primary_key=True)
    location_code = Column(String(6), primary_key=True)
    location_type = Column(String(1), primary_key=True)
    consent_calendar_code = Column(String(2), primary_key=True)
    description = Column(String(60))
    long_description = Column(String(200))
    active_flg = Column(String(1))
    trans_uid = Column(String(30))
    trans_update = Column(DateTime)


class CAVoteSummary(Base):
    __tablename__ = "bill_summary_vote_tbl"

    bill_id = Column(String(20), ForeignKey(CABill.bill_id), primary_key=True)
    location_code = Column(String(6), ForeignKey(CALocation.location_code), primary_key=True)
    vote_date_time = Column(DateTime, primary_key=True)
    vote_date_seq = Column(Integer, primary_key=True)
    motion_id = Column(Integer, ForeignKey(CAMotion.motion_id), primary_key=True)
    ayes = Column(Integer)
    noes = Column(Integer)
    abstain = Column(Integer)
    vote_result = Column(String(6))
    trans_uid = Column(String(30))
    trans_update = Column(DateTime, primary_key=True)

    motion = relation(CAMotion)
    location = relation(CALocation)

    @property
    def threshold(self):
        # This may not always be true...
        if self.location_code != "AFLOOR" and self.location_code != "SFLOOR":
            return '1/2'

        # Get the associated bill version (probably?)
        version = filter(lambda v:
                            v.bill_version_action_date <= self.vote_date_time,
                        self.bill.versions)[0]

        if version.vote_required == 'Majority':
            return '1/2'
        else:
            return '2/3'


class CAVoteDetail(Base):
    __tablename__ = "bill_detail_vote_tbl"

    bill_id = Column(String(20), ForeignKey(CABill.bill_id),
                     ForeignKey(CAVoteSummary.bill_id), primary_key=True)
    location_code = Column(String(6), ForeignKey(CAVoteSummary.location_code),
                           primary_key=True)
    legislator_name = Column(String(50), primary_key=True)
    vote_date_time = Column(DateTime, ForeignKey(CAVoteSummary.vote_date_time),
                            primary_key=True)
    vote_date_seq = Column(Integer, ForeignKey(CAVoteSummary.vote_date_seq),
                           primary_key=True)
    vote_code = Column(String(5), primary_key=True)
    motion_id = Column(Integer, ForeignKey(CAVoteSummary.motion_id),
                       primary_key=True)
    trans_uid = Column(String(30), primary_key=True)
    trans_update = Column(DateTime, primary_key=True)

    bill = relation(CABill, backref=backref('detail_votes'))
    summary = relation(
        CAVoteSummary,
        primaryjoin=and_(CAVoteSummary.bill_id == bill_id,
                         CAVoteSummary.location_code == location_code,
                         CAVoteSummary.vote_date_time == vote_date_time,
                         CAVoteSummary.vote_date_seq == vote_date_seq,
                         CAVoteSummary.motion_id == motion_id),
        backref=backref('votes'))


class CACommitteeHearing(Base):
    __tablename__ = "committee_hearing_tbl"

    bill_id = Column(String(20), ForeignKey(CABill.bill_id),
                     ForeignKey(CAVoteSummary.bill_id), primary_key=True)
    committee_type = Column(String(2), primary_key=True)
    committee_nr = Column(Integer, primary_key=True)
    hearing_date = Column(DateTime, primary_key=True)
    location_code = Column(String(6), primary_key=True)
    trans_uid = Column(String(30), primary_key=True)
    trans_update_date = Column(DateTime, primary_key=True)

    bill = relation(CABill, backref=backref('committee_hearings'))

########NEW FILE########
__FILENAME__ = utils
import collections

import lxml.html


class CachedAttr(object):
    '''Computes attribute value and caches it in instance.

    Example:
        class MyClass(object):
            def myMethod(self):
                # ...
            myMethod = CachedAttribute(myMethod)
    Use "del inst.myMethod" to clear cache.'''

    def __init__(self, method, name=None):
        self.method = method
        self.name = name or method.__name__

    def __get__(self, inst, cls):
        if inst is None:
            return self
        result = self.method(inst)
        setattr(inst, self.name, result)
        return result


class UrlData(object):
    '''Given a url, its nickname, and a scraper instance,
    provide the parsed lxml doc, the raw html, and the url
    '''
    def __init__(self, name, url, scraper, urls_object):
        '''urls_object is a reference back to the Urls container.
        '''
        self.url = url
        self.name = name
        self.scraper = scraper
        self.urls_object = urls_object

    def __repr__(self):
        return 'UrlData(url=%r)' % self.url

    @CachedAttr
    def text(self):
        text = self.scraper.urlopen(self.url)
        self.urls_object.validate(self.name, self.url, text)
        return text

    @CachedAttr
    def resp(self):
        '''Return the decoded html or xml or whatever. sometimes
        necessary for a quick "if 'page not found' in html:..."
        '''
        return self.text.response

    @CachedAttr
    def doc(self):
        '''Return the page's lxml doc.
        '''
        doc = lxml.html.fromstring(self.text)
        doc.make_links_absolute(self.url)
        return doc

    @CachedAttr
    def xpath(self):
        return self.doc.xpath

    @CachedAttr
    def pdf_to_lxml(self):
        filename, resp = self.scraper.urlretrieve(self.url)
        text = convert_pdf(filename, 'html')
        return lxml.html.fromstring(text)

    @CachedAttr
    def etree(self):
        '''Return the documents element tree.
        '''
        return lxml.etree.fromstring(self.text)


class UrlsMeta(type):
    '''This metaclass aggregates the validator functions marked
    using the Urls.validate decorator.
    '''
    def __new__(meta, name, bases, attrs):
        '''Just aggregates the validator methods into a defaultdict
        and stores them on cls._validators.
        '''
        validators = collections.defaultdict(set)
        for attr in attrs.values():
            if hasattr(attr, 'validates'):
                validators[attr.validates].add(attr)
        attrs['_validators'] = validators
        cls = type.__new__(meta, name, bases, attrs)
        return cls


class Urls(object):
    '''Contains urls we need to fetch during this scrape.
    '''
    __metaclass__ = UrlsMeta

    def __init__(self, scraper, urls):
        '''Sets a UrlData object on the instance for each named url given.
        '''
        self.urls = urls
        self.scraper = scraper
        for name, url in urls.items():
            url = UrlData(name, url, scraper, urls_object=self)
            setattr(self, name, url)

    def __repr__(self):
        return '%s(%r)' % (self.__class__.__name__, self.urls)

    def __iter__(self):
        '''A generator of this object's UrlData members.
        '''
        for name in self.urls:
            yield getattr(self, name)

    def add(self, **name_to_url_map):
        for name, url in name_to_url_map.items():
            url_data = UrlData(name, url, self.scraper, urls_object=self)
            self.urls[name] = url
            setattr(self, name, url_data)

    @staticmethod
    def validates(name, retry=False):
        '''A decorator to mark validator functions for use on a particular
        named url. Use like so:

        @Urls.validates('history')
        def must_have_actions(self, url, text):
            'Skip bill that hasn't been introduced yet.'
            if 'no actions yet' in text:
                raise Skip('Bill had no actions yet.')
        '''
        def decorator(method):
            method.validates = name
            method.retry = retry
            return method
        return decorator

    def validate(self, name, url, text):
        '''Run each validator function for the named url and its text.
        '''
        for validator in self._validators[name]:
            try:
                validator(self, url, text)
            except Exception as e:
                if validator.retry:
                    validator(self, url, text)
                else:
                    raise e

########NEW FILE########
__FILENAME__ = actions
'''

'''
import re
from billy.scrape.actions import Rule, BaseCategorizer


committees = [
    u'Agriculture, Livestock (?:and|&) Natural Resources',
    u'Finance',
    u'Joint Budget Committee',
    u'Appropriations',
    u'Health (?:and|&) Environment',
    u'Transportation',
    u'Education',
    u'Agriculture, Livestock, (?:and|&) Natural Resources',
    u'Judiciary',
    u'Legal Services',
    u'State, Veterans (?:and|&) Military Affairs',
    u'Economic (?:and|&) Business Development',
    u'Local Government',
    u'Congressional Redistricting',
    u'Legislative Council',
    u'State Veterans, (?:and|&) Military Affairs',
    u'Health (?:and|&) Environment',
    u'Legislative Audit',
    u'Capital Development',
    u'State, Veterans, (?:and|&) Military Affairs',
    u'State, Veterans, (?:and|&) Military Affairs',
    u'Executive Committee of Legislative Council',
    u'Health (?:and|&) Environment',
    u'Finance',
    u'Appropriations',
    u'Agriculture, Natural Resources (?:and|&) Energy',
    u'Judiciary',
    u'Business, Labor (?:and|&) Technology',
    u'Health (?:and|&) Human Services',
    u'State, Veterans (?:and|&) Military Affairs',
    u'Local Government',
    u'Legislative Audit',
    u'Executive Committee of Legislative Council',
    u'Transportation',
    u'Health (?:and|&) Human Services',
    u'Education',
    u'Legislative Council',
    u'Legal Services',
    u'Capital Development',
    u'Transportation (?:and|&) Energy',
    u'Joint Budget Committee',
    u'Business, Labor, (?:and|&) Technology',
    u'State, Veterans, (?:and|&) Military Affairs'
    ]


rules = (
    Rule('^House', actor='lower'),
    Rule('^Senate', actor='upper'),
    Rule('^Introduced in Senate', actor='upper'),
    Rule('^Introduced in House', actor='lower'),
    Rule('^Governor', actor='governor'),

    Rule('Governor Action - Partial Veto', 'governor:vetoed:line-item'),
    Rule('Sent to the Governor', 'governor:received'),
    Rule('Governor Action - Signed', 'governor:signed'),
    Rule('Governor Action - Vetoed', 'governor:vetoed'),

    Rule(r'^Introduced', 'bill:introduced'),
    Rule(r'Assigned to (?P<committees>.+)'),

    Rule(u'(?i)refer (un)?amended to (?P<committees>.+)',
         [u'committee:referred']),
    Rule(u'(?i)\S+ Committee on (?P<committees>.+?) Refer (un)amended'),
    Rule(u'Second Reading Passed', [u'bill:reading:2']),
    Rule(u'Third Reading Passed', ['bill:reading:3', 'bill:passed'])
    )

committees_rgx = '(%s)' % '|'.join(
    sorted(committees, key=len, reverse=True))


class Categorizer(BaseCategorizer):
    rules = rules

    def categorize(self, text):
        '''Wrap categorize and add boilerplate committees.
        '''
        attrs = BaseCategorizer.categorize(self, text)
        if 'committees' in attrs:
            committees = attrs['committees']
            for committee in re.findall(committees_rgx, text, re.I):
                if committee not in committees:
                    committees.append(committee)
        return attrs

    def post_categorize(self, attrs):
        res = set()
        if 'legislators' in attrs:
            for text in attrs['legislators']:
                rgx = r'(,\s+(?![a-z]\.)|\s+and\s+)'
                legs = re.split(rgx, text)
                legs = filter(lambda x: x not in [', ', ' and '], legs)
                res |= set(legs)
        attrs['legislators'] = list(res)

        res = set()
        if 'committees' in attrs:
            for text in attrs['committees']:
                for committee in text.split(' + '):
                    res.add(committee.strip())
        attrs['committees'] = list(res)
        return attrs

########NEW FILE########
__FILENAME__ = bills
import datetime as dt
import re
import lxml.html
import scrapelib
from urlparse import urlparse

from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

from .actions import Categorizer


CO_URL_BASE = "http://www.leg.state.co.us"
#                     ^^^ Yes, this is actually required


class COBillScraper(BillScraper):
    """
    This scraper is a bit bigger then some of the others, but it's just
    a standard billy scraper. Methods are documented just because this
    does both bill and vote scraping, and can be a bit overwhelming.
    """

    jurisdiction = 'co'
    categorizer = Categorizer()

    def get_bill_folder(self, session, chamber):
        """
        This returns a URL to the bill "folder" - a list of all the bills for that
        session and chamber. If the URL looks funny, it's because CO has made
        some interesting technical choices.
        """
        chamber_id = "(bf-3)" if chamber == "House" else "(bf-2)"
        url = CO_URL_BASE + "/CLICS/CLICS" + session \
            + "/csl.nsf/" + chamber_id + "?OpenView&Count=20000000"
        return url

    def read_td(self, td_node):
        return td_node[0].text
        #      ^^^^^^^^^^ font

    def parse_all_votes(self, bill_vote_url):
        """
        This will parse a vote page, with a list of who voted which way on what
        bill. This is invoked from `parse_votes', and invoking it directly may
        or may not be very useful.
        """
        ret = {"meta": {}, 'votes': {}}
        ret['meta']['url'] = bill_vote_url
        vote_html = self.urlopen(bill_vote_url)
        bill_vote_page = lxml.html.fromstring(vote_html)
        bill_vote_page.make_links_absolute(bill_vote_url)
        nodes = bill_vote_page.xpath('//table/tr')

        # The first tr is funky and we can ignore it.
        nodes = nodes[1:]

        inVoteSection = False

        for line in nodes:
            to_parse = line.text_content()
            if to_parse == "VOTE":
                inVoteSection = True
                continue
            if not inVoteSection:
                # there are some very header-esque fields if we
                # grab accross the row
                metainf = [a.strip() for a in to_parse.split(":", 1)]
                ret['meta'][metainf[0]] = metainf[1]
            else:
                # OK. We've either got a vote, or the final line.
                if not line[0].text_content().strip() == "":
                    # We've got an ending line
                    # They look like:
                    #    Final     YES: 7     NO: 6     EXC: 0     ABS: 0\
                    #        FINAL ACTION: PASS
                    #
                    to_parse = to_parse.replace("FINAL ACTION",
                        "FINAL_ACTION").replace(":", "")
                    if re.match("^FINAL.*", to_parse):
                        to_parse = to_parse[len("FINAL"):]

                    passage_actions = to_parse.split("  ")
                    final_score = {}
                    for item in passage_actions:
                        if item == "":
                            continue
                        item = item.strip()
                        keys = item.split(" ", 1)
                        final_score[keys[0]] = keys[1]

                    # XXX: Verify we can't do this with recursive splits
                    # it now looks like:
                    # ['Final', 'YES:', '7', 'NO:', '6', 'EXC:', '0',
                    #   'ABS:', '0', 'FINAL_ACTION:', 'PASS']
                    #passage_actions = passage_actions[1:]
                    #il = iter(passage_actions)
                    #final_score = dict(zip(il,il))

                    if not "FINAL_ACTION" in final_score:
                        final_score["FINAL_ACTION"] = False

                    ret['result'] = final_score
                else:
                    # We've got a vote.
                    person = line[1].text_content()  # <div><font>
                    vote = line[2].text_content()
                    if person.strip() != "":
                        ret['votes'][person] = vote
        return ret

    def parse_votes(self, bill_vote_url):
        """
        This will parse all the votes on a given bill - basically, it looks on the
        page of all votes, and invokes `parse_all_votes' for each href it finds

        We do some minor sanity checking, so the caller will be able to ensure the
        bill IDs match exactly before going ahead with saving the data to the DB
        """
        ret = {}

        vote_html = self.urlopen(bill_vote_url)
        bill_vote_page = lxml.html.fromstring(vote_html)
        bill_vote_page.make_links_absolute(bill_vote_url)
        nodes = bill_vote_page.xpath('//b/font')
        title = nodes[0].text
        ret['sanity-check'] = title[title.find("-") + 1:].strip()
        ret['votes'] = []
        # Just in case we need to add some sanity checking
        # votes = bill_vote_page.xpath('//table/tr/td/a')

        lines = bill_vote_page.xpath('//table/tr/td')

        date = "unknown"
        ctty = "unknown"

        # We can throw out the headers
        lines = lines[2:]

        for line in lines:
            try:
                ctty = line[0][0][0].text_content()
                date = line[0][0][1].text_content()
            except IndexError:
                # We have a vote line for the previous line
                try:
                    vote_url = line.xpath('a')[0].attrib['href']
                    vote_page = vote_url
                    vote_dict = self.parse_all_votes(vote_page)

                    vote_dict['meta']['x-parent-date'] = date
                    vote_dict['meta']['x-parent-ctty'] = ctty

                    ret['votes'].append(vote_dict)
                except KeyError:
                    pass
                except IndexError:
                    pass

        return ret

    def get_vote_url(self, billid, session):
        """
        URL generator for getting the base vote pages. The links use all sorts of
        JS bouncing, so this will just fetch the end page.
        """
        return CO_URL_BASE + \
            "/CLICS%5CCLICS" + session + \
            "%5Ccommsumm.nsf/IndSumm?OpenView&StartKey=" + billid + "&Count=4"

    def parse_versions(self, bill_versions_url):
        """
        Parse a bill versions page for all the versions
        """
        try:
            versions_html = self.urlopen(bill_versions_url)
            bill_versions_page = lxml.html.fromstring(versions_html)
            bill_versions_page.make_links_absolute(bill_versions_url)
        except scrapelib.HTTPError:  # XXX: Hack for deleted pages - 404s
            return []

        url = re.findall("var url=\"(?P<url>.+)\"", versions_html)[0]

        trs = bill_versions_page.xpath('//form//table//tr')[3:]
        cols = {
            "type": 0,
            "pdf": 1,
            "wpd": 2
        }

        pdfs = bill_versions_page.xpath("//a/font[contains(text(), 'pdf')]")
        cur_version = bill_versions_page.xpath("//a//font[contains(text(), 'Current PDF')]")
        return [{
            "name": x.text,
            "mimetype": "application/pdf",
            "link": CO_URL_BASE + url + (
                re.findall(
                    "_doClick\('(?P<slug>.+)'",
                    x.getparent().attrib['onclick']
                )[0]
            )
        } for x in cur_version + pdfs]

    def parse_history(self, bill_history_url):
        """
        Parse a bill history page for as much data as we can gleen, such as when
        the bill was introduced (as well as a number of other things)
        """
        # their URL is actually a shim. let's get the target GET param
        url = urlparse(bill_history_url)
        get_info = url[4].split("&")
        # This is a bit of a mess. XXX: Better way to get args using
        # url[lib|parse] ?

        clean_args = {}
        ret = []

        for arg in get_info:
            try:
                key, value = (
                    arg[:arg.index("=")],
                    arg[arg.index("=") + 1:]
                )
                clean_args[key] = value
            except ValueError:
                pass

        # </mess>

        try:
            bill_history_url = CO_URL_BASE + clean_args['target']
            # We're wrapping it because guessing at the URL
            # param is not really great.

        except KeyError:
            return ret

        try:
            history_html = self.urlopen(bill_history_url)
            bill_history_page = lxml.html.fromstring(history_html)
            bill_history_page.make_links_absolute(bill_history_url)
        except scrapelib.HTTPError:  # XXX: Hack for deleted pages - 404s
            return []

        nodes = bill_history_page.xpath('//form/b/font')

        actions = nodes[3].text_content()

        for action in actions.split('\n'):
            if action.strip() == "":
                continue

            date_string = action[:action.find(" ")]
            if ":" in date_string:
                date_string = action[:action.find(":")]
            if "No" == date_string:  # XXX Remove me
            # as soon as sanity is on:
            # http://www.leg.state.co.us/clics/clics2012a/csl.nsf/billsummary/C150552896590FA587257961006E7C0B?opendocument
                continue

            date_time = dt.datetime.strptime(date_string, "%m/%d/%Y")
            action = action[action.find(" ") + 1:]
            ret.append((action, date_time))

        return ret

    def scrape_bill_sheet(self, session, chamber):
        """
        Scrape the bill sheet (the page full of bills and other small bits of data)
        """
        sheet_url = self.get_bill_folder(session, chamber)

        bill_chamber = {"Senate": "upper", "House": "lower"}[chamber]

        index = {
            "id": 0,
            "title_sponsor": 1,
            "version": 2,
            "history": 3,
            "votes": 7
        }

        sheet_html = self.urlopen(sheet_url)
        sheet_page = lxml.html.fromstring(sheet_html)
        sheet_page.make_links_absolute(sheet_url)

        bills = sheet_page.xpath('//table/tr')

        for bill in bills:
            bill_id = self.read_td(bill[index["id"]][0])

            if bill_id == None:
                # Every other entry is null for some reason
                continue

            dot_loc = bill_id.find('.')
            if dot_loc != -1:
                # budget bills are missing the .pdf, don't truncate
                bill_id = bill_id[:dot_loc]
            title_and_sponsor = bill[index["title_sponsor"]][0]

            bill_title = title_and_sponsor.text
            bill_title_and_sponsor = title_and_sponsor.text_content()
            if bill_title is None:
                continue  # Odd ...

            sponsors = bill_title_and_sponsor.replace(bill_title, "").\
                replace(" & ...", "").split("--")

            cats = {
                "SB": "bill",
                "HB": "bill",
                "HR": "resolution",
                "SR": "resolution",
                "SCR": "concurrent resolution",
                "HCR": "concurrent resolution",
                "SJR": "joint resolution",
                "HJR": "joint resolution",
                "SM": "memorial",
                "HM": "memorial"
            }

            bill_type = None

            for cat in cats:
                if bill_id[:len(cat)] == cat:
                    bill_type = cats[cat]

            b = Bill(session, bill_chamber, bill_id, bill_title,
                     type=bill_type)

            b.add_source(sheet_url)

            versions_url = \
                bill[index["version"]].xpath('font/a')[0].attrib["href"]
            versions_url = versions_url
            versions = self.parse_versions(versions_url)

            for version in versions:
                b.add_version(version['name'], version['link'],
                    mimetype=version['mimetype'])

            bill_history_href = bill[index["history"]][0][0].attrib['href']

            history = self.parse_history(bill_history_href)
            b.add_source(bill_history_href)

            chamber_map = dict(Senate='upper', House='lower')
            for action, date in history:
                action_actor = chamber_map.get(chamber, chamber)
                attrs = dict(actor=action_actor, action=action, date=date)
                attrs.update(self.categorizer.categorize(action))
                b.add_action(**attrs)

            for sponsor in sponsors:
                if sponsor != None and sponsor != "(NONE)" and \
                   sponsor != "":
                    if "&" in sponsor:
                        for sponsor in [x.strip() for x in sponsor.split("&")]:
                            b.add_sponsor("primary", sponsor)
                    else:
                        b.add_sponsor("primary", sponsor)

            # Now that we have history, let's see if we can't grab some
            # votes

            bill_vote_href, = bill.xpath(".//a[contains(text(), 'Votes')]")
            bill_vote_href = bill_vote_href.attrib['href']
            #bill_vote_href = self.get_vote_url(bill_id, session)
            votes = self.parse_votes(bill_vote_href)

            if (votes['sanity-check'] == 'This site only supports frames '
                    'compatible browsers!'):
                votes['votes'] = []
            elif votes['sanity-check'] != bill_id:
                self.warning("XXX: READ ME! Sanity check failed!")
                self.warning(" -> Scraped ID: " + votes['sanity-check'])
                self.warning(" -> 'Real' ID:  " + bill_id)
                assert votes['sanity-check'] == bill_id

            for vote in votes['votes']:
                filed_votes = vote['votes']
                passage = vote['meta']
                result = vote['result']

                composite_time = "%s %s" % (
                    passage['x-parent-date'],
                    passage['TIME']
                )
                # It's now like: 04/01/2011 02:10:14 PM
                pydate = dt.datetime.strptime(composite_time,
                    "%m/%d/%Y %I:%M:%S %p")
                hasHouse = "House" in passage['x-parent-ctty']
                hasSenate = "Senate" in passage['x-parent-ctty']

                if hasHouse and hasSenate:
                    actor = "joint"
                elif hasHouse:
                    actor = "lower"
                else:
                    actor = "upper"

                other = (int(result['EXC']) + int(result['ABS']))
                # OK, sometimes the Other count is wrong.
                local_other = 0
                for voter in filed_votes:
                    l_vote = filed_votes[voter].lower().strip()
                    if l_vote != "yes" and l_vote != "no":
                        local_other = local_other + 1

                if local_other != other:
                    self.warning( \
                        "XXX: !!!WARNING!!! - resetting the 'OTHER' VOTES")
                    self.warning(" -> Old: %s // New: %s" % (
                        other, local_other
                    ))
                    other = local_other

                passed = (result['FINAL_ACTION'] == "PASS")
                if passage['MOTION'].strip() == "":
                    continue

                if "without objection" in passage['MOTION'].lower():
                    passed = True

                v = Vote(actor, pydate, passage['MOTION'],
                         passed,
                         int(result['YES']), int(result['NO']),
                         other,
                         moved=passage['MOVED'],
                         seconded=passage['SECONDED'])

                v.add_source(vote['meta']['url'])
                # v.add_source( bill_vote_href )

                # XXX: Add more stuff to kwargs, we have a ton of data
                seen = set([])
                for voter in filed_votes:
                    who = voter
                    if who in seen:
                        raise Exception("Seeing the double-thing. - bug #702")
                    seen.add(who)

                    vote = filed_votes[who]
                    if vote.lower() == "yes":
                        v.yes(who)
                    elif vote.lower() == "no":
                        v.no(who)
                    else:
                        v.other(who)
                b.add_vote(v)
            self.save_bill(b)

    def scrape(self, chamber, session):
        """
        Entry point when invoking this from billy (or really whatever else)
        """
        chamber = {'lower': 'House', 'upper': 'Senate'}[chamber]
        self.scrape_bill_sheet(session, chamber)

########NEW FILE########
__FILENAME__ = committees
import re
import lxml.html

from billy.scrape.committees import CommitteeScraper, Committee

COMMITTEE_URL = ("http://www.colorado.gov/cs/Satellite?c=Page&"
    "childpagename=CGA-LegislativeCouncil%2FCLCLayout&"
    "cid=1245677985421&pagename=CLCWrapper")


class COCommitteeScraper(CommitteeScraper):
    jurisdiction = "co"

    def lxmlize(self, url):
        text = self.urlopen(url)
        page = lxml.html.fromstring(text)
        page.make_links_absolute(url)
        return page, text

    def scrape_page(self, a, chamber, term):
        page, text = self.lxmlize(a.attrib['href'])
        committee = a.text_content()
        twitter_ids = re.findall("setUser\('(.*)'\)", text)
        twitter_id = twitter_ids[0] if twitter_ids != [] else None
        roles = {
            ", Chair": "chair",
            ", Vice-Chair": "member"
        }

        committee = Committee(chamber, committee,
                              twitter=twitter_id)

        committee.add_source(a.attrib['href'])

        tables = page.xpath("//table[@width='545' or @width='540']")
        added = False

        seen_people = set([])
        for table in tables:
            people = table.xpath(
                ".//a[contains(@href, 'MemberDetailPage')]/text()")
            for person in [x.strip() for x in people]:
                role = "member"
                for flag in roles:
                    if person.endswith(flag):
                        role = roles[flag]
                        person = person[:-len(flag)].strip()
                if person in seen_people:
                    continue

                if person in "":
                    continue

                seen_people.add(person)
                committee.add_member(person, role)
                added = True

        if added:
            self.save_committee(committee)
            return

        tables = page.xpath("//table[@width='466']")
        added = False
        seen_people = set([])
        for table in tables:
            if "committee members" in table.text_content().lower():
                for person in table.xpath(".//td/text()"):
                    person = person.strip()
                    if person != "":
                        if person in seen_people:
                            continue
                        seen_people.add(person)
                        committee.add_member(person, "member")
                        added = True

        if added:
            self.save_committee(committee)
            return

        self.warning("Unable to scrape!")

    def scrape(self, term, chambers):
        page, _ = self.lxmlize(COMMITTEE_URL)
        chamber = "other"
        for div in page.xpath("//div[@id='Content_COITArticleDetail']"):
            hrefs = div.xpath(
                ".//a[contains(@href,'childpagename=CGA-LegislativeCouncil')]")

            if hrefs == []:
                div_txt = div.text_content().lower()
                flags = {
                    "house": "lower",
                    "senate": "upper",
                    "joint": "joint"
                }
                for flag in flags:
                    if flag in div_txt:
                        chamber = flags[flag]

            for a in hrefs:
                cchamber = chamber
                text = a.text_content().strip()
                for flag in flags:
                    if flag in text:
                        cchamber = flags[flag]
                self.scrape_page(a, cchamber, term)

########NEW FILE########
__FILENAME__ = legislators
from billy.scrape import ScrapeError, NoDataForPeriod
from billy.scrape.legislators import LegislatorScraper, Legislator
from billy.scrape.committees  import Committee

import lxml.html
import re, contextlib

CO_BASE_URL = "http://www.leg.state.co.us/"

CTTY_BLACKLIST = [ # Invalid HTML causes us to snag these tags. Super annoying.
    "Top",
    "State Home",
    "Colorado Legislature"
]


def clean_committee(name):
    committee_name = name.replace("&", " and ")
    return re.sub("\s+", " ", committee_name).strip()


def clean_input( line ):
    if line != None:
        return re.sub( " +", " ", re.sub( "(\n|\r)+", " ", line ))


class COLegislatorScraper(LegislatorScraper):
    jurisdiction = 'co'

    def get_district_list(self, chamber, session ):
        session = session[:4] + "A"

        chamber = {
            "upper" : "%5Ce.%20Senate%20Districts%20&%20Members",
            "lower" : "h.%20House%20Districts%20&%20Members"
        }[chamber]

        url = "http://www.leg.state.co.us/clics/clics" + session + \
            "/directory.nsf/Pink%20Book/" + chamber + "?OpenView&Start=1"
        return url

    def scrape_directory(self, next_page, chamber, session):
        ret = {}
        html = self.urlopen(next_page)
        page = lxml.html.fromstring(html)
        # Alright. We'll get all the districts.
        dID = page.xpath( "//div[@id='viewBody']" )[0] # should be only one
        distr = dID.xpath( "./table/tr/td/b/font/a" ) # What a mess...
        for d in distr:
            url = CO_BASE_URL + d.attrib['href']
            if "Save Conflict" in d.text:
                continue

            ret[d.text] = url

        nextPage = page.xpath( "//table/tr" )
        navBar = nextPage[0]
        np = CO_BASE_URL + navBar[len(navBar) - 1][0].attrib['href']
        if not next_page == np:
            subnodes = self.scrape_directory( np, chamber, session )
            for node in subnodes:
                ret[node] = subnodes[node]
        return ret

    def normalize_party( self, party_id ):
        try:
            return { "R" : "Republican", "D" : "Democratic" }[party_id]
        except KeyError as e:
            return "Other"

    def parse_homepage( self, hp_url ):
        image_base = "http://www.state.co.us/gov_dir/leg_dir/senate/members/"
        ret = []
        obj = {}
        image = ""
        html = self.urlopen(hp_url)
        page = lxml.html.fromstring(html)
        try:
            email = page.xpath("//a[contains(@href, 'mailto')]")[0]
            email = email.attrib['href']
            email = email.split(":", 1)[1]
            obj['email'] = email
        except IndexError:
            pass
        infoblock = page.xpath("//div[@align='center']")
        info = infoblock[0].text_content()

        number = re.findall("(\d{3})(-|\))?(\d{3})-(\d{4})", info)
        if len(number) > 0:
            number = number[0]
            number = "%s %s %s" % (
                number[0],
                number[2],
                number[3]
            )
            obj['number'] = number
        ctty_apptmts = [clean_input(x) for x in
                         page.xpath("//a[contains(@href, 'CLC')]//font/text()")]

        new = []
        for entry in ctty_apptmts:
            if "--" in entry:
                ctty, _ = entry.split("--")
                new.append(ctty.strip())

        ctty_apptmts = filter(lambda x: x.strip() != "" and
                              x not in CTTY_BLACKLIST, new)

        image = hp_url[:-3] + "jpg"
        obj.update({
            "ctty"  : ctty_apptmts,
            "photo" : image
        })
        return obj

    def process_person( self, p_url ):
        ret = { "homepage" : p_url }

        html = self.urlopen(p_url)
        page = lxml.html.fromstring(html)
        page.make_links_absolute(p_url)

        info = page.xpath( '//table/tr' )[1]
        tds = {
            "name"  : 0,
            "dist"  : 1,
            "party" : 3,
            "occup" : 4,
            "cont"  : 6
        }

        party_id = info[tds['party']].text_content()

        person_name = clean_input(info[tds['name']].text_content())
        person_name = clean_input(re.sub( '\(.*$', '', person_name).strip())
        occupation  = clean_input(info[tds['occup']].text_content())

        urls = page.xpath( '//a' )
        ret['photo_url'] = ""
        home_page = page.xpath("//a[contains(text(), 'Home Page')]")

        if home_page != []:
            home_page = home_page[0]
            ret['homepage'] = home_page.attrib['href'].strip()
            homepage = self.parse_homepage(
                home_page.attrib['href'].strip() )

            ret['ctty'] = homepage['ctty']
            ret['photo_url'] = homepage['photo']
            if "email" in homepage:
                ret['email'] = homepage['email']
            if "number" in homepage:
                ret['number'] = homepage['number']

        ret['party'] = self.normalize_party(party_id)
        ret['name']  = person_name
        ret['occupation'] = occupation
        return ret

    def scrape(self, chamber, session):
        url = self.get_district_list(chamber, session)
        people_pages = self.scrape_directory( url, chamber, session )

        for person in people_pages:
            district = person
            p_url = people_pages[district]
            metainf = self.process_person( p_url )

            p = Legislator( session, chamber, district, metainf['name'],
                party=metainf['party'],
                # some additional things the website provides:
                occupation=metainf['occupation'],
                photo_url=metainf['photo_url'],
                url=metainf['homepage'])
            if "email" in metainf:
                p['email'] = metainf['email']
            if "number" in metainf:
                p.add_office('capitol', 'Capitol Office',
                             phone=metainf['number'],
                             address='200 E. Colfax\nDenver, CO 80203'
                            )

            p.add_source( p_url )
            self.save_legislator( p )

########NEW FILE########
__FILENAME__ = votes
from billy.scrape.votes import VoteScraper, Vote
from billy.scrape.utils import convert_pdf
import datetime
import subprocess
import lxml
import os
import re

journals = "http://www.leg.state.co.us/CLICS/CLICS%s/csljournals.nsf/" \
    "jouNav?Openform&%s"

date_re = re.compile(
    r"(?i).*(?P<dt>(monday|tuesday|wednesday|thursday|friday|saturday|sunday)"
    ".*, \d{4}).*"
)
vote_re = re.compile((r"\s*"
           "YES\s*(?P<yes_count>\d+)\s*"
           "NO\s*(?P<no_count>\d+)\s*"
           "EXCUSED\s*(?P<excused_count>\d+)\s*"
           "ABSENT\s*(?P<abs_count>\d+).*"))
votes_re = r"(?P<name>\w+(\s\w\.)?)\s+(?P<vote>Y|N|A|E|-)"

class COVoteScraper(VoteScraper):
    jurisdiction = 'co'

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def scrape_house(self, session):
        url = journals % (session, 'House')
        page = self.lxmlize(url)
        hrefs = page.xpath("//font//a")

        for href in hrefs:
            (path, response) = self.urlretrieve(href.attrib['href'])
            data = convert_pdf(path, type='text')

            in_vote = False
            cur_vote = {}
            known_date = None
            cur_vote_count = None
            in_question = False
            cur_question = None
            cur_bill_id = None

            for line in data.split("\n"):
                if known_date is None:
                     dt = date_re.findall(line)
                     if dt != []:
                        dt, dow = dt[0]
                        known_date = datetime.datetime.strptime(dt,
                            "%A, %B %d, %Y")

                non_std = False
                if re.match("(\s+)?\d+.*", line) is None:
                    non_std = True
                    l = line.lower().strip()
                    skip = False
                    blacklist = [
                        "house",
                        "page",
                        "general assembly",
                        "state of colorado",
                        "session",
                        "legislative day"
                    ]
                    for thing in blacklist:
                        if thing in l:
                            skip = True
                    if skip:
                        continue

                found = re.findall(
                    "(?P<bill_id>(H|S|SJ|HJ)(B|M|R)\d{2}-\d{4})",
                    line
                )
                if found != []:
                    found = found[0]
                    cur_bill_id, chamber, typ = found

                try:
                    if not non_std:
                        _, line = line.strip().split(" ", 1)
                    line = line.strip()
                except ValueError:
                    in_vote = False
                    in_question = False
                    continue

                if in_question:
                    cur_question += " " + line
                    continue

                if ("The question being" in line) or \
                   ("On motion of" in line) or \
                   ("the following" in line) or \
                   ("moved that the" in line):
                    cur_question = line
                    in_question = True


                if in_vote:
                    if line == "":
                        likely_garbage = True

                    likely_garbage = False
                    if "co-sponsor" in line.lower():
                        likely_garbage = True

                    if 'the speaker' in line.lower():
                        likely_garbage = True

                    votes = re.findall(votes_re, line)
                    if likely_garbage:
                        votes = []

                    for person, _, v in votes:
                        cur_vote[person] = v

                    last_line = False
                    for who, _, vote in votes:
                        if who.lower() == "speaker":
                            last_line = True

                    if votes == [] or last_line:
                        in_vote = False
                        # save vote
                        yes, no, other = cur_vote_count
                        if cur_bill_id is None or cur_question is None:
                            continue

                        bc = {
                            "H": "lower",
                            "S": "upper",
                            "J": "joint"
                        }[cur_bill_id[0].upper()]

                        vote = Vote('lower',
                                    known_date,
                                    cur_question,
                                    (yes > no),
                                    yes,
                                    no,
                                    other,
                                    session=session,
                                    bill_id=cur_bill_id,
                                    bill_chamber=bc)

                        vote.add_source(href.attrib['href'])
                        vote.add_source(url)

                        for person in cur_vote:
                            if person is None:
                                continue

                            vot = cur_vote[person]

                            if person.endswith("Y"):
                                vot = "Y"
                                person = person[:-1]
                            if person.endswith("N"):
                                vot = "N"
                                person = person[:-1]
                            if person.endswith("E"):
                                vot = "E"
                                person = person[:-1]

                            if vot == 'Y':
                                vote.yes(person)
                            elif vot == 'N':
                                vote.no(person)
                            elif vot == 'E' or vot == '-':
                                vote.other(person)

                        self.save_vote(vote)

                        cur_vote = {}
                        in_question = False
                        cur_question = None
                        in_vote = False
                        cur_vote_count = None
                        continue

                summ = vote_re.findall(line)
                if summ == []:
                    continue
                summ = summ[0]
                yes, no, exc, ab = summ
                yes, no, exc, ab = \
                        int(yes), int(no), int(exc), int(ab)
                other = exc + ab
                cur_vote_count = (yes, no, other)
                in_vote = True
                continue
            os.unlink(path)

    def scrape_senate(self, session):
        url = journals % (session, 'Senate')
        page = self.lxmlize(url)
        hrefs = page.xpath("//font//a")

        for href in hrefs:
            (path, response) = self.urlretrieve(href.attrib['href'])
            data = convert_pdf(path, type='text')

            cur_bill_id = None
            cur_vote_count = None
            in_vote = False
            cur_question = None
            in_question = False
            known_date = None
            cur_vote = {}

            for line in data.split("\n"):
                if not known_date:
                    dt = date_re.findall(line)
                    if dt != []:
                        dt, dow = dt[0]
                        known_date = datetime.datetime.strptime(dt,
                            "%A, %B %d, %Y")

                if in_question:
                    line = line.strip()
                    if re.match("\d+", line):
                        in_question = False
                        continue
                    try:
                        line, _ = line.rsplit(" ", 1)
                        cur_question += line
                    except ValueError:
                        in_question = False
                        continue

                    cur_question += line
                if not in_vote:
                    summ = vote_re.findall(line)
                    if summ != []:
                        cur_vote = {}
                        cur_vote_count = summ[0]
                        in_vote = True
                        continue

                    if ("The question being" in line) or \
                       ("On motion of" in line) or \
                       ("the following" in line) or \
                       ("moved that the" in line):
                        cur_question, _ = line.strip().rsplit(" ", 1)
                        in_question = True

                    if line.strip() == "":
                        continue
                    first = line[0]
                    if first != " ":
                        if " " not in line:
                            # wtf
                            continue

                        bill_id, kruft = line.split(" ", 1)
                        if len(bill_id) < 3:
                            continue
                        if bill_id[0] != "H" and bill_id[0] != "S":
                            continue
                        if bill_id[1] not in ['B', 'J', 'R', 'M']:
                            continue

                        cur_bill_id = bill_id
                else:
                    line = line.strip()
                    try:
                        line, lineno = line.rsplit(" ", 1)
                    except ValueError:
                        in_vote = False
                        if cur_question is None:
                            continue

                        if cur_bill_id is None:
                            continue

                        yes, no, exc, ab = cur_vote_count
                        other = int(exc) + int(ab)
                        yes, no, other = int(yes), int(no), int(other)

                        bc = {'H': 'lower', 'S': 'upper'}[cur_bill_id[0]]

                        vote = Vote('upper',
                                    known_date,
                                    cur_question,
                                    (yes > no),
                                    yes,
                                    no,
                                    other,
                                    session=session,
                                    bill_id=cur_bill_id,
                                    bill_chamber=bc)
                        for person in cur_vote:
                            if person is None:
                                continue

                            howvote = cur_vote[person]

                            if person.endswith("Y"):
                                howvote = "Y"
                                person = person[:-1]
                            if person.endswith("N"):
                                howvote = "N"
                                person = person[:-1]
                            if person.endswith("E"):
                                howvote = "E"
                                person = person[:-1]

                            howvote = howvote.upper()
                            if howvote == 'Y':
                                vote.yes(person)
                            elif howvote == 'N':
                                vote.no(person)
                            else:
                                vote.other(person)
                        vote.add_source(href.attrib['href'])
                        self.save_vote(vote)

                        cur_vote, cur_question, cur_vote_count = (
                            None, None, None)
                        continue

                    votes = re.findall(votes_re, line)

                    for person in votes:
                        name, li, vot = person
                        cur_vote[name] = vot

            os.unlink(path)

    def scrape(self, chamber, session):
        if chamber == 'upper':
            self.scrape_senate(session)
        if chamber == 'lower':
            self.scrape_house(session)

########NEW FILE########
__FILENAME__ = bills
import re
import datetime
from operator import itemgetter
from collections import defaultdict

from billy.scrape import NoDataForPeriod
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote
from .utils import parse_directory_listing, open_csv

import lxml.html


class SkipBill(Exception):
    pass


class CTBillScraper(BillScraper):
    jurisdiction = 'ct'
    latest_only = True

    def scrape(self, session, chambers):
        self.bills = {}
        self._committee_names = {}
        self._introducers = defaultdict(set)
        self._subjects = defaultdict(list)

        self.scrape_committee_names()
        self.scrape_subjects()
        self.scrape_introducers('upper')
        self.scrape_introducers('lower')
        self.scrape_bill_info(session, chambers)
        for chamber in chambers:
            self.scrape_versions(chamber, session)
        self.scrape_bill_history()

        for bill in self.bills.itervalues():
            self.save_bill(bill)

    def scrape_bill_info(self, session, chambers):
        info_url = "ftp://ftp.cga.ct.gov/pub/data/bill_info.csv"
        data = self.urlopen(info_url)
        page = open_csv(data)

        chamber_map = {'H': 'lower', 'S': 'upper'}

        for row in page:
            bill_id = row['bill_num']
            chamber = chamber_map[bill_id[0]]

            if not chamber in chambers:
                continue

            # assert that the bill data is from this session, CT is tricky
            assert row['sess_year'] == session

            if re.match(r'^(S|H)J', bill_id):
                bill_type = 'joint resolution'
            elif re.match(r'^(S|H)R', bill_id):
                bill_type = 'resolution'
            else:
                bill_type = 'bill'

            bill = Bill(session, chamber, bill_id,
                        row['bill_title'],
                        type=bill_type)
            bill.add_source(info_url)

            for introducer in self._introducers[bill_id]:
                bill.add_sponsor('primary', introducer,
                                 official_type='introducer')

            try:
                self.scrape_bill_page(bill)

                bill['subjects'] = self._subjects[bill_id]

                self.bills[bill_id] = bill
            except SkipBill:
                self.warning('no such bill: ' + bill_id)
                pass

    def scrape_bill_page(self, bill):
        url = ("http://www.cga.ct.gov/asp/cgabillstatus/cgabillstatus.asp?selBillType=Bill"
               "&bill_num=%s&which_year=%s" % (bill['bill_id'], bill['session']))
        page = self.urlopen(url)
        if 'not found in Database' in page:
            raise SkipBill()
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        bill.add_source(url)

        spon_type = 'primary'
        if not bill['sponsors']:
            for sponsor in page.xpath('//td[contains(string(), "Introduced by:")]')[1].xpath('text()'):
                sponsor = sponsor.replace('Introduced by:', '').strip()
                if sponsor:
                    bill.add_sponsor(spon_type, sponsor,
                                     official_type='introducer')
                spon_type = 'cosponsor'


        for link in page.xpath("//a[contains(@href, '/FN/')]"):
            bill.add_document(link.text.strip(), link.attrib['href'])

        for link in page.xpath("//a[contains(@href, '/BA/')]"):
            bill.add_document(link.text.strip(), link.attrib['href'])

        for link in page.xpath("//a[contains(@href, 'VOTE')]"):
            # 2011 HJ 31 has a blank vote, others might too
            if link.text:
                self.scrape_vote(bill, link.text.strip(),
                                 link.attrib['href'])

    def scrape_vote(self, bill, name, url):
        if "VOTE/H" in url:
            vote_chamber = 'lower'
            cols = (1, 5, 9, 13)
            name_offset = 3
            yes_offset = 0
            no_offset = 1
        else:
            vote_chamber = 'upper'
            cols = (1, 6)
            name_offset = 4
            yes_offset = 1
            no_offset = 2

        page = self.urlopen(url)
        if 'BUDGET ADDRESS' in page:
            return

        page = lxml.html.fromstring(page)

        yes_count = page.xpath(
            "string(//span[contains(., 'Those voting Yea')])")
        yes_count = int(re.match(r'[^\d]*(\d+)[^\d]*', yes_count).group(1))

        no_count = page.xpath(
            "string(//span[contains(., 'Those voting Nay')])")
        no_count = int(re.match(r'[^\d]*(\d+)[^\d]*', no_count).group(1))

        other_count = page.xpath(
            "string(//span[contains(., 'Those absent')])")
        other_count = int(
            re.match(r'[^\d]*(\d+)[^\d]*', other_count).group(1))

        need_count = page.xpath(
            "string(//span[contains(., 'Necessary for')])")
        need_count = int(
            re.match(r'[^\d]*(\d+)[^\d]*', need_count).group(1))

        date = page.xpath("string(//span[contains(., 'Taken on')])")
        date = re.match(r'.*Taken\s+on\s+(\d+/\s?\d+)', date).group(1)
        date = date.replace(' ', '')
        date = datetime.datetime.strptime(date + " " + bill['session'],
                                          "%m/%d %Y").date()

        vote = Vote(vote_chamber, date, name, yes_count > need_count,
                    yes_count, no_count, other_count)
        vote.add_source(url)

        table = page.xpath("//table")[0]
        for row in table.xpath("tr"):
            for i in cols:
                name = row.xpath("string(td[%d])" % (
                    i + name_offset)).strip()

                if not name or name == 'VACANT':
                    continue

                if "Y" in row.xpath("string(td[%d])" %
                                    (i + yes_offset)):
                    vote.yes(name)
                elif "N" in row.xpath("string(td[%d])" %
                                      (i + no_offset)):
                    vote.no(name)
                else:
                    vote.other(name)

        bill.add_vote(vote)


    def scrape_bill_history(self):
        history_url = "ftp://ftp.cga.ct.gov/pub/data/bill_history.csv"
        page = self.urlopen(history_url)
        page = open_csv(page)

        action_rows = defaultdict(list)

        for row in page:
            bill_id = row['bill_num']

            if bill_id in self.bills:
                action_rows[bill_id].append(row)

        for (bill_id, actions) in action_rows.iteritems():
            bill = self.bills[bill_id]

            actions.sort(key=itemgetter('act_date'))
            act_chamber = bill['chamber']

            for row in actions:
                date = row['act_date']
                date = datetime.datetime.strptime(
                    date, "%Y-%m-%d %H:%M:%S").date()

                action = row['act_desc'].strip()
                act_type = []

                match = re.search('COMM(ITTEE|\.) ON$', action)
                if match:
                    comm_code = row['qual1']
                    comm_name = self._committee_names.get(comm_code,
                                                          comm_code)
                    action = "%s %s" % (action, comm_name)
                    act_type.append('committee:referred')
                elif row['qual1']:
                    if bill['session'] in row['qual1']:
                        action += ' (%s' % row['qual1']
                        if row['qual2']:
                            action += ' %s)' % row['qual2']
                    else:
                        action += ' %s' % row['qual1']

                match = re.search(r'REFERRED TO OLR, OFA (.*)',
                                  action)
                if match:
                    action = ('REFERRED TO Office of Legislative Research'
                              ' AND Office of Fiscal Analysis %s' % (
                                  match.group(1)))

                if (re.match(r'^ADOPTED, (HOUSE|SENATE)', action) or
                    re.match(r'^(HOUSE|SENATE) PASSED', action)):
                    act_type.append('bill:passed')

                match = re.match(r'^Joint ((Un)?[Ff]avorable)', action)
                if match:
                    act_type.append('committee:passed:%s' %
                                    match.group(1).lower())

                if not act_type:
                    act_type = ['other']

                bill.add_action(act_chamber, action, date,
                                type=act_type)

                if 'TRANS.TO HOUSE' in action or action == 'SENATE PASSED':
                    act_chamber = 'lower'

                if ('TRANSMITTED TO SENATE' in action or
                    action == 'HOUSE PASSED'):
                    act_chamber = 'upper'

    def scrape_versions(self, chamber, session):
        chamber_letter = {'upper': 's', 'lower': 'h'}[chamber]
        versions_url = "ftp://ftp.cga.ct.gov/%s/tob/%s/" % (
            session, chamber_letter)

        page = self.urlopen(versions_url)
        files = parse_directory_listing(page)

        for f in files:
            match = re.match(r'^\d{4,4}([A-Z]+-\d{5,5})-(R\d\d)',
                             f.filename)
            bill_id = match.group(1).replace('-', '')

            try:
                bill = self.bills[bill_id]
            except KeyError:
                continue

            url = versions_url + f.filename
            bill.add_version(match.group(2), url, mimetype='text/html')

    def scrape_subjects(self):
        info_url = "ftp://ftp.cga.ct.gov/pub/data/subject.csv"
        data = self.urlopen(info_url)
        page = open_csv(data)

        for row in page:
            self._subjects[row['bill_num']].append(row['subj_desc'])

    def scrape_committee_names(self):
        comm_url = "ftp://ftp.cga.ct.gov/pub/data/committee.csv"
        page = self.urlopen(comm_url)
        page = open_csv(page)

        for row in page:
            comm_code = row['comm_code'].strip()
            comm_name = row['comm_name'].strip()
            comm_name = re.sub(r' Committee$', '', comm_name)
            self._committee_names[comm_code] = comm_name

    def scrape_introducers(self, chamber):
        chamber_letter = {'upper': 's', 'lower': 'h'}[chamber]
        url = "http://www.cga.ct.gov/asp/menu/%slist.asp" % chamber_letter

        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        for link in page.xpath("//a[contains(@href, 'MemberBills')]"):
            name = link.xpath("string(../../td[1])").strip()
            name = re.match("^S?\d+\s+-\s+(.*)$", name).group(1)
            # we encode the URL here because there are weird characters that
            # cause problems
            self.scrape_introducer(name, link.attrib['href'].encode('utf8'))

    def scrape_introducer(self, name, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)

        for link in page.xpath("//a[contains(@href, 'billstatus')]"):
            bill_id = link.text.strip()
            self._introducers[bill_id].add(name)

########NEW FILE########
__FILENAME__ = events
import unicodecsv
import datetime
import chardet

from billy.scrape import NoDataForPeriod
from billy.scrape.events import EventScraper, Event

import pytz
import lxml.html

from .utils import open_csv

class CTEventScraper(EventScraper):
    jurisdiction = 'ct'

    _tz = pytz.timezone('US/Eastern')

    def __init__(self, *args, **kwargs):
        super(CTEventScraper, self).__init__(*args, **kwargs)

    def scrape(self, chamber, session):
        if chamber != 'other':
            # All CT committees are joint
            return

        for (code, name) in self.get_comm_codes():
            self.scrape_committee_events(session, code, name)

    def scrape_committee_events(self, session, code, name):
        url = ("http://www.cga.ct.gov/asp/menu/"
               "CGACommCal.asp?comm_code=%s" % code)
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        cal_table = page.xpath(
            "//table[contains(@summary, 'Calendar')]")[0]

        date_str = None
        for row in cal_table.xpath("tr[2]//tr"):
            col1 = row.xpath("string(td[1])").strip()
            col2 = row.xpath("string(td[2])").strip()

            if not col1:
                if col2 == "No Meetings Scheduled":
                    return
                # If col1 is empty then this is a date header
                date_str = col2
            else:
                # Otherwise, this is a committee event row
                ical_feed = row.xpath(".//a[contains(@href, 'newcal')]")[0]
                when = date_str + " " + col1
                when = datetime.datetime.strptime(
                    when, "%A, %B %d, %Y %I:%M %p")
                when = self._tz.localize(when)

                location = row.xpath("string(td[3])").strip()
                guid = row.xpath("td/a")[0].attrib['href']

                event = Event(session,
                              when,
                              'committee meeting',
                              col2,
                              location,
                              _guid=guid,
                              _ical_feed=ical_feed.attrib['href'])
                event.add_source(url)
                event.add_participant('host', name, 'committee',
                                      chamber='joint')

                self.save_event(event)

    def get_comm_codes(self):
        url = "ftp://ftp.cga.ct.gov/pub/data/committee.csv"
        page = self.urlopen(url)
        page = open_csv(page)
        return [(row['comm_code'].strip(),
                 row['comm_name'].strip())
                for row in page]

########NEW FILE########
__FILENAME__ = legislators
import re

from billy.scrape import NoDataForPeriod
from billy.scrape.legislators import LegislatorScraper, Legislator
from .utils import open_csv


class CTLegislatorScraper(LegislatorScraper):
    jurisdiction = 'ct'
    latest_only = True

    _committee_names = {}

    #def __init__(self, *args, **kwargs):
        #super(CTLegislatorScraper, self).__init__(*args, **kwargs)
        #self._scrape_committee_names()

    def scrape(self, term, chambers):
        leg_url = "ftp://ftp.cga.ct.gov/pub/data/LegislatorDatabase.csv"
        data = self.urlopen(leg_url)
        page = open_csv(data)

        for row in page:
            chamber = {'H': 'lower', 'S': 'upper'}[row['office code']]
            if chamber not in chambers:
                continue

            district = row['dist'].lstrip('0')

            name = row['first name']
            mid = row['middle initial'].strip()
            if mid:
                name += " %s" % mid
            name += " %s" % row['last name']
            suffix = row['suffix'].strip()
            if suffix:
                name += " %s" % suffix

            party = row['party']
            if party == 'Democrat':
                party = 'Democratic'

            leg = Legislator(term, chamber, district,
                             name, first_name=row['first name'],
                             last_name=row['last name'],
                             middle_name=row['middle initial'],
                             suffixes=row['suffix'],
                             party=party,
                             email=row['email'],
                             url=row['URL'],
                             office_phone=row['capitol phone'])

            office_address = "%s, Room %s\nHartford, CT 06106-1591" % (
                row['capitol street address'], row['room number'])
            leg.add_office('capitol', 'Capitol Office',
                           address=office_address, phone=row['capitol phone'])
            # skipping home address for now
            leg.add_source(leg_url)

            for comm in row['committee member1'].split(';'):
                if comm:
                    if ' (' in comm:
                        comm, role = comm.split(' (')
                        role = role.strip(')').lower()
                    else:
                        role = 'member'
                    comm = comm.strip()
                    if comm == '':
                        continue

                    leg.add_role('committee member', term,
                                 chamber='joint',
                                 committee=comm,
                                 position=role)

            self.save_legislator(leg)

    def _scrape_committee_names(self):
        comm_url = "ftp://ftp.cga.ct.gov/pub/data/committee.csv"
        page = self.urlopen(comm_url)
        page = open_csv(page)

        for row in page:
            comm_code = row['comm_code'].strip()
            comm_name = row['comm_name'].strip()
            comm_name = re.sub(r' Committee$', '', comm_name)
            self._committee_names[comm_code] = comm_name

########NEW FILE########
__FILENAME__ = utils
import re
import datetime
import collections
import chardet
import unicodecsv
try:
    import cStringIO as StringIO
except ImportError:
    import StringIO


def open_csv(data):
    char_encoding = chardet.detect(data.bytes)['encoding']
    return unicodecsv.DictReader(StringIO.StringIO(data.bytes),
                                 encoding=char_encoding)


Listing = collections.namedtuple('Listing', 'mtime size filename')


def parse_directory_listing(text):
    files = []

    dir_re = r'^(\d\d-\d\d-\d\d\s+\d\d:\d\d(AM|PM))\s+(\d+)\s+(.*\.htm)\s+$'
    for match in re.finditer(dir_re, text, re.MULTILINE):
        mtime = datetime.datetime.strptime(match.group(1),
                                           "%m-%d-%y %I:%M%p")
        files.append(Listing(mtime=mtime, size=int(match.group(3)),
                             filename=match.group(4)))

    return files

########NEW FILE########
__FILENAME__ = bills
import re
import datetime
import lxml.html

import scrapelib

from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

def extract_int(text):
    return int(text.replace(u'\xc2', '').strip())

def convert_date(text):
    long_date = re.findall('\d{1,2}-\d{1,2}-\d{4}', text)
    try:
        if long_date:
            return datetime.datetime.strptime(long_date[0], '%m-%d-%Y')
        short_date = re.findall('\d{1,2}-\d{1,2}-\d{2}', text)
        if short_date:
            return datetime.datetime.strptime(short_date[0], '%m-%d-%y')

        return datetime.datetime.strptime(text, '%A, %B %d, %Y')
    except ValueError:
        return None


class DCBillScraper(BillScraper):
    jurisdiction = 'dc'

    def scrape_bill(self, bill):
        bill_url = 'http://dcclims1.dccouncil.us/lims/legislation.aspx?LegNo=' + bill['bill_id']

        bill.add_source(bill_url)

        bill_html = self.urlopen(bill_url)
        doc = lxml.html.fromstring(bill_html)
        doc.make_links_absolute(bill_url)

        # get versions
        for link in doc.xpath('//a[starts-with(@id, "DocumentRepeater")]'):
            try:
                bill.add_version(link.text, link.get('href'),
                                 mimetype='application/pdf')
            except ValueError as e:
                self.warning('duplicate version on %s' % bill_url)

        # sponsors
        introduced_by = doc.get_element_by_id('IntroducedBy').text
        if introduced_by:
            for sponsor in introduced_by.split(', '):
                bill.add_sponsor('primary', sponsor.strip())

        requested_by = doc.get_element_by_id('RequestedBy').text
        if requested_by:
            stype = 'primary' if not introduced_by else 'cosponsor'
            bill.add_sponsor(stype, requested_by.strip(),
                             official_type='requestor')

        cosponsored_by = doc.get_element_by_id('CoSponsoredBy').text
        if cosponsored_by:
            for cosponsor in cosponsored_by.split(','):
                bill.add_sponsor('cosponsor', cosponsor.strip())

        # actions
        actions = (
            ('Introduction', 'DateIntroduction', 'bill:introduced'),
            ('Committee Action', 'DateCommitteeAction', 'other'),
            ('First Vote', 'DateFirstVote', 'bill:reading:1'),
            ('Final Vote', 'DateFinalVote', 'bill:reading:3'),
            ('Third Vote', 'DateThirdVote', ['bill:reading:3']),
            ('Reconsideration', 'DateReconsideration', 'other'),
            ('Transmitted to Mayor', 'DateTransmittedMayor', 'governor:received'),
            ('Signed by Mayor', 'DateSigned', 'governor:signed'),
            ('Returned by Mayor', 'DateReturned', 'other'),
            ('Veto Override', 'DateOverride', 'bill:veto_override:passed'),
            ('Enacted', 'DateEnactment', 'other'),
            ('Vetoed by Mayor', 'DateVeto', 'governor:vetoed'),
            ('Transmitted to Congress', 'DateTransmittedCongress', 'other'),
            ('Re-transmitted to Congress', 'DateReTransmitted', 'other'),
        )

        subactions = (
            ('WITHDRAWN BY', 'Withdrawn', 'bill:withdrawn'),
            ('TABLED', 'Tabled', 'other'),
            ('DEEMED APPROVED', 'Deemed approved without council action', 'bill:passed'),
            ('DEEMED DISAPPROVED', 'Deemed disapproved without council action', 'bill:failed'),
        )

        for action, elem_id, atype in actions:
            date = doc.get_element_by_id(elem_id).text
            if date:

                # check if the action starts with a subaction prefix
                for prefix, sub_action, subatype in subactions:
                    if date.startswith(prefix):
                        date = convert_date(date)
                        if date:
                            bill.add_action('upper', sub_action, date,
                                            type=subatype)
                        break

                # actions that mean nothing happened
                else:
                    if date not in ('Not Signed', 'NOT CONSIDERED',
                                  'NOTCONSIDERED'):
                        actor = ('mayor' if action.endswith('by Mayor')
                                 else 'upper')
                        date = convert_date(date)
                        if not isinstance(date, datetime.datetime):
                            self.warning('could not convert %s %s [%s]' %
                                         (action, date, bill['bill_id']))
                        else:
                            bill.add_action(actor, action, date, type=atype)

        # votes
        vote_tds = doc.xpath('//td[starts-with(@id, "VoteTypeRepeater")]')
        for td in vote_tds:
            vote_type = td.text
            vote_type_id = re.search(r"LoadVotingInfo\(this\.id, '(\d+)'",
                                     td.get('onclick')).groups()[0]
            # some votes randomly break
            try:
                self.scrape_vote(bill, vote_type_id, vote_type)
            except scrapelib.HTTPError as e:
                self.warning(str(e))

        bill['actions'] = sorted(bill['actions'], key=lambda b:b['date'])
        self.save_bill(bill)


    def scrape_vote(self, bill, vote_type_id, vote_type):
        base_url = 'http://dcclims1.dccouncil.us/lims/voting.aspx?VoteTypeID=%s&LegID=%s'
        url = base_url % (vote_type_id, bill['bill_id'])

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        vote_date = convert_date(doc.get_element_by_id('VoteDate').text)

        # check if voice vote / approved boxes have an 'x'
        voice = (doc.xpath('//span[@id="VoteTypeVoice"]/b/text()')[0] ==
                 'x')
        passed = (doc.xpath('//span[@id="VoteResultApproved"]/b/text()')[0]
                  == 'x')

        yes_count = extract_int(doc.xpath(
            '//span[@id="VoteCount1"]/b/text()')[0])
        no_count = extract_int(doc.xpath(
            '//span[@id="VoteCount2"]/b/text()')[0])

        other_count = 0
        for n in xrange(3, 9):
            other_count += extract_int(doc.xpath(
                '//span[@id="VoteCount%s"]/b/text()' % n)[0])

        vote = Vote('upper', vote_date, vote_type, passed, yes_count,
                    no_count, other_count, voice_vote=voice)

        vote.add_source(url)

        # members are only text on page in a <u> tag
        for member_u in doc.xpath('//u'):
            member = member_u.text
            # normalize case
            vote_text = member_u.xpath('../../i/text()')[0].upper()
            if 'YES' in vote_text:
                vote.yes(member)
            elif 'NO' in vote_text:
                vote.no(member)
            else:
                vote.other(member)
        bill.add_vote(vote)

    def scrape(self, session, chambers):
        url = 'http://dcclims1.dccouncil.us/lims/print/list.aspx?FullPage=True&Period=' + session

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        rows = doc.xpath('//table/tr')
        # skip first row
        for row in rows[1:]:
            bill_id, title, _ = row.xpath('td/text()')
            title = title.replace(u'\xe2\x80\x99', "'")  # smart apostrophe

            if bill_id.startswith('B'):
                type = 'bill'
            elif bill_id.startswith('CER'):
                type = 'resolution'
            elif bill_id.startswith('CA'):
                type = 'contract'
            elif bill_id.startswith('PR'):
                type = 'resolution'
            # don't collect these
            elif (bill_id.startswith('GBM') or
                  bill_id.startswith('HFA') or
                  bill_id.startswith('IG')):
                continue
            else:
                # will break if type isn't known
                raise ValueError('unknown bill type: %s' % bill_id)

            bill = Bill(session, 'upper', bill_id, title, type=type)
            self.scrape_bill(bill)

########NEW FILE########
__FILENAME__ = committees
from billy.scrape.committees import CommitteeScraper, Committee
import lxml.html
import re


class DCCommitteeScraper(CommitteeScraper):
    jurisdiction = 'dc'

    def scrape(self, term, chambers):
        com_url = 'http://www.dccouncil.washington.dc.us/committees'
        data = self.urlopen(com_url)
        doc = lxml.html.fromstring(data)
        doc.make_links_absolute(com_url)

        urls = set(doc.xpath('//a[contains(@href, "committee-on")]/@href'))
        for url in urls:
            data = self.urlopen(url)
            doc = lxml.html.fromstring(data)

            try:
                name = doc.xpath('//h1/text()')[0].replace('Committee on ', '')
            except IndexError:
                name = doc.xpath('//h2/text()')[0].replace('Committee on ', '')


            # skip link to Committees page
            if name == 'Committees':
                continue

            com = Committee('upper', name)

            for chair in doc.xpath('//h3[text()="Committee Chair"]/following-sibling::p'):
                com.add_member(chair.text_content(), role='chairperson')

            for member in doc.xpath('//h3[text()="Councilmembers"]/following-sibling::ul//a'):
                com.add_member(member.text_content(), role='member')

            com.add_source(url)
            self.save_committee(com)

########NEW FILE########
__FILENAME__ = legislators
from billy.scrape.legislators import LegislatorScraper, Legislator
import lxml.html

def get_field(doc, key):
    # get text_content of parent of the element containing the key
    elem = doc.xpath('//div[@id="member-info"]/p/strong[text()="%s"]/..' % key)
    if elem:
        return elem[0].text_content().replace(key, '').strip()
    else:
        return ''


class DCLegislatorScraper(LegislatorScraper):
    jurisdiction = 'dc'

    def scrape(self, term, chambers):
        council_url = 'http://www.dccouncil.washington.dc.us/council'
        data = self.urlopen(council_url)
        doc = lxml.html.fromstring(data)
        doc.make_links_absolute(council_url)
        # page should have 13 unique council URLs
        urls = set(doc.xpath('//a[contains(@href, "dccouncil.us/council/")]/@href'))
        print '\n'.join(urls)
        assert len(urls) <= 13, "should have 13 unique councilmember URLs"

        for url in urls:
            data = self.urlopen(url)
            doc = lxml.html.fromstring(data)
            doc.make_links_absolute(url)

            descriptor = doc.xpath('//p[@class="head-descriptor"]/text()')[0]
            name = doc.xpath('//h2/text()')[0]
            if 'Chairman' in descriptor:
                district = 'Chairman'
            else:
                district = get_field(doc, 'Represents: ')

            if not district:
                district = 'At-Large'

            # party
            party = get_field(doc, "Political Affiliation:")
            if 'Democratic' in party:
                party = 'Democratic'
            else:
                party = 'Independent'

            photo_url = doc.xpath('//div[@id="member-thumb"]/img/@src')
            if photo_url:
                photo_url = photo_url[0]
            else:
                photo_url = ''

            office_address = get_field(doc, "Office:")
            phone = get_field(doc, "Tel:")
            if phone.endswith('| Fax:'):
                fax = None
                phone = phone.strip('| Fax:') or None
            else:
                phone, fax = phone.split(' | Fax: ')

            email = doc.xpath('//a[starts-with(text(), "Send an email")]/@href')[0].split(':')[1]

            legislator = Legislator(term, 'upper', district, name,
                                    party=party, url=url, email=email,
                                    photo_url=photo_url)
            legislator.add_office('capitol', 'Council Office',
                                  address=office_address, phone=phone,
                                  fax=fax)
            legislator.add_source(url)
            self.save_legislator(legislator)

########NEW FILE########
__FILENAME__ = actions
import re
from billy.scrape.actions import Rule, BaseCategorizer


rules = (
    Rule(r'(?P<yes_votes>\d+)\s+YES\s+(?P<no_votes>\d+)'
         r'\s+NO\s+(?P<not_voting>.+?)\s+NOT VOTING\s+(?P<absent>.+?)\s+'
         r'ABSENT\s+(?P<vacant>.+?) VACANT'),
    Rule([u'Amendment (?P<bills>.+?) -\s+Laid On Table'], ['amendment:tabled']),
    Rule([u'Favorable'], ['committee:passed:favorable']),
    Rule([u'(?i)Amendment (?P<bills>.+?) defeated'], ['amendment:failed']),
    Rule([u'(?i)introduced and adopted in lieu of (?P<bills>.+)'],
         ['bill:introduced']),
    Rule([u'(?i)assigned to (?P<committees>.+?) Committee in'],
         ['committee:referred', 'bill:introduced']),
    Rule([u'Signed by Governor'], ['governor:signed']),
    Rule([u'(?i)Amendment (?P<bills>[\w\s]+?) Introduced'],
         ['amendment:introduced']),
    Rule([u'Amendment (?P<bills>.+?) -  Passed'], ['amendment:passed']),
    Rule([u'^Passed by'], ['bill:passed']),
    Rule([u'^Defeated'], ['bill:failed']),
    Rule([u'(?i)unfavorable'], ['committee:passed:unfavorable']),
    Rule([u'Reported Out of Committee \((?P<committees>.+?)\)'],
         ['committee:passed']),
    Rule([u'Vetoed by Governor'], ['governor:vetoed']),
    Rule([u'(?i)Amendment (?P<bills>.+?)\s+-\s+Introduced'],
         ['amendment:introduced']),
    Rule([u'(?i)Amendment (?P<bills>[\w\s]+?) Passed'], ['amendment:passed']),
    Rule([u'Amendment (?P<bills>.+?) -  Defeated by House of .+?\. Votes: Defeated'],
         ['amendment:failed']),
    Rule([u'^Introduced'], ['bill:introduced']),
    Rule([u'Amendment (?P<bills>.+?) -  Defeated in House'], ['amendment:failed']),
    Rule([u'^Passed in House'], ['bill:passed'])
    )


class Categorizer(BaseCategorizer):
    rules = rules

    def post_categorize(self, attrs):
        res = set()
        if 'legislators' in attrs:
            for text in attrs['legislators']:
                rgx = r'(,\s+(?![a-z]\.)|\s+and\s+)'
                legs = re.split(rgx, text)
                legs = filter(lambda x: x not in [', ', ' and '], legs)
                res |= set(legs)
        attrs['legislators'] = list(res)

        res = set()
        if 'committees' in attrs:
            for text in attrs['committees']:
                text = text.strip()
                res.add(text)
        attrs['committees'] = list(res)
        return attrs


def get_actor(action_text, chamber, rgxs=(
        (re.compile(r'(in|by) senate', re.I), 'upper'),
        (re.compile(r'(in|by) house', re.I), 'lower'),
        (re.compile(r'by governor', re.I), 'governor'),
        )):
    '''Guess the actor for a particular action.
    '''
    for r, actor in rgxs:
        m = r.search(action_text)
        if m:
            return actor
    return chamber

########NEW FILE########
__FILENAME__ = bills
import re
import itertools
import logging
from urlparse import urlparse
from datetime import datetime
from operator import methodcaller
from functools import partial
from collections import defaultdict

import lxml.html

from billy.scrape import ScrapeError
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote
import scrapelib

import actions


class BillIdParseError(ScrapeError):
    '''
    Raised when function `parse_bill_id` returns a string that
    doesn't remotely resmble a valid bill_id.
    '''
    pass


class WeirdDataError(ScrapeError):
    '''Raised when unexpected format is encountered, probably due to
    manual entry.
    '''
    logger = logging.getLogger('billy')

    def __init__(self, message):
        Exception.__init__(self, message)
        self.logger.warning(message)


def get_text(doc, i, xpath):
    '''
    A shortcut to get stripped text content given 1) a document of
    type lxml.html.HtmlElement, 2) an xpath, and 3) optionaly
    an index `i` that defaults to zero.
    '''
    if not i:
        i = 0
    return doc.xpath(xpath)[i].text_content().strip()


def slugify(s):
    '''
    Turn a phrase like "Current Status:" into "current_status".
    '''
    return s.lower().replace(' ', '_').rstrip(':')


def parse_votestring(v, strptime=datetime.strptime,
    re_date=re.compile('\d{,2}/\d{,2}/\d{,4} [\d:]{,8} [AP]M'),
    chambers={'House': 'lower', 'Senate': 'upper'}):
    '''
    Parse contents of the string on the main bill detail page
    describing a vote.
    '''
    motion, _ = v.split(':', 1)
    motion = motion.strip()

    m = re_date.search(v)
    if m is None:
        raise WeirdDataError('Got unexpected date format: %s' % v)
    date = strptime(m.group(), '%m/%d/%Y %I:%M:%S %p')

    chamber, _ = v.split(' ', 1)
    chamber = chambers[chamber.strip()]

    passed = ('Passed' in v)

    return dict(date=date, chamber=chamber, passed=passed, motion=motion)


def extract_bill_id(bill_id, fns=(
    partial(re.compile(r'( w/.A \d{1,3}.{,200},?)+$', re.I).sub, ''),
    partial(re.compile(r'^.{,20}for ', re.I).sub, '')),
    is_valid=re.compile(r'[A-Z]{2,4} \d{1,6}$').match):
    '''
    Given a bill id string from the website's index pages, removes
    characters indicating amendments and substitutions, e.g.:

        SB 137 w/SA 1 --> SB 137
        SB 112 w/SA 1 + HA 1 --> SB 112
        SS 1 for SB 156 --> SB 156

    Complains if the result doesn't look like a normal bill id.
    '''
    _bill_id = bill_id

    for f in fns:
        bill_id = f(bill_id)

    if not is_valid(bill_id):
        msg = 'Not a valid bill id: "%s" ' % _bill_id
        raise BillIdParseError(msg)

    return bill_id


class DEBillScraper(BillScraper):
    '''
    Scrapes bills for the current session. Delware's website
    (http://legis.delaware.gov/) lists archival data separately and
    with a different (though similar) format.
    See http://legis.delaware.gov/LEGISLATURE.NSF/LookUp/LIS_archives?opendocument
    '''

    jurisdiction = 'de'

    categorizer = actions.Categorizer()

    legislation_types = {
        'House Bill': 'bill',
        'House Concurrent Resolution': 'concurrent resolution',
        'House Joint Resolution': 'joint resolution',
        'House Resolution': 'resolution',
        'Senate Bill': 'bill',
        'Senate Concurrent Resolution': 'concurrent resolution',
        'Senate Joint Resolution': 'joint resolution',
        'Senate Resolution': 'resolution',
        'Senate Nominations': 'nomination',
        }

    def _url_2_lxml(self, url, base_url='{0.scheme}://{0.netloc}'.format):
        '''
        Fetch the url and parse with lxml.
        '''
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        urldata = urlparse(url)
        doc.make_links_absolute(base_url(urldata))
        return doc

    def _cleanup_sponsors(self, string, chamber,

        # Splits at ampersands and commas.
        re_amp=re.compile(r'[,&]'),

        # Changes "Sen. Jones" into "Jones"
        re_title=re.compile(r'(Sen|Rep)s?[.;]\s?'),

        # Map to clean up sponsor name data.
        name_map={
            '{ NONE...}': '',
            },

        # Mapping of member designations to related chamber type.
        chamber_map={
            'Sen': 'upper',
            'Rep': 'lower',
            },

        chain=itertools.chain.from_iterable,
        replace=methodcaller('replace', '&nbsp', ''),
        strip=methodcaller('strip'),

        splitter=re.compile('(?:[,;] NewLine|(?<!Reps); |on behalf of all \w+)')):
        '''
        Sponsor names are sometimes joined with an ampersand,
        are '{ NONE...}', or contain '&nbsp'. This helper removes
        that stuff and returns a list minus any non-name strings found.
        '''

        # "Briggs King" is a deplorable hack to work around DE's
        # not-easily-parseable sponsorship strings.
        tokenizer = r'(?:(?:[A-Z]\.){,5} )?(?:Briggs King|[A-Z][^.]\S{,50})'
        tokenize = partial(re.findall, tokenizer)

        for string in splitter.split(string):

            # Override the chamber based on presence of "Sens." or "Rep."
            m = re_title.search(string)
            if m:
                chamber = chamber_map[m.group(1)]

            # Remove junk.
            names = re_amp.split(string)
            names = map(lambda s: re_title.sub('', s), names)
            names = map(replace, names)
            names = filter(None, [name_map.get(n, n) for n in names])
            names = map(tokenize, names)

            for n in chain(names):
                yield (strip(n), chamber)

    def _get_urls(self, chamber, session):
        '''
        A generator of urls to legislation types listed on the
        index_url page.
        '''
        re_count = re.compile(r'count=\d+', re.I)

        index_url = ('http://legis.delaware.gov/LIS/lis%s.nsf'
                     '/Legislation/?openview' % session)

        chamber_map = {'Senate': 'upper',
                       'House': 'lower'}

        legislation_types = self.legislation_types

        index_page = self._url_2_lxml(index_url)

        # The last link on the "all legis'n" page pertains to senate
        # nominations, so skip it.
        index_links = index_page.xpath('//a[contains(@href, "Expand=")]')
        index_links = index_links[:-1]

        for el in index_links:

            type_raw = el.xpath('../following-sibling::td')[0].text_content()
            type_ = legislation_types[type_raw]

            # Skip any links that aren't for this chamber.
            _chamber, _ = type_raw.split(' ', 1)
            if chamber != chamber_map[_chamber]:
                continue

            # Tweak the url to ask the server for 10000 results (i.e., all)
            url = el.attrib['href']
            url = re_count.sub('count=10000', url)

            # Get the index page.
            doc = self._url_2_lxml(url)

            for el in doc.xpath('//a[contains(@href, "OpenDocument")]'):

                title = el.xpath('./../../../td[4]')[0].text_content()

                url = el.attrib['href']

                bill_kwargs = {'type': type_,
                               'bill_id': extract_bill_id(el.text),
                               'chamber': chamber,
                               'session': session,
                               'title': title}

                yield (url, bill_kwargs)

    def scrape(self, chamber, session):

        scrape_bill = self.scrape_bill
        for url, kw in self._get_urls(chamber, session):
            try:
                scrape_bill(url, kw)
            except WeirdDataError as exc:
                continue

    def scrape_bill(self, url, kw,
                    re_amendment=re.compile(r'(^[A-Z]A \d{1,3}) to'),
                    re_substitution=re.compile(r'(^[A-Z]S \d{1,2}) for'),
                    re_digits=re.compile(r'\d{,5}'),
                    actions_get_actor=actions.get_actor):

        bill = Bill(**kw)
        bill.add_source(url)

        #---------------------------------------------------------------------
        # A few helpers.
        _url_2_lxml = self._url_2_lxml
        _cleanup_sponsors = self._cleanup_sponsors

        # Shortcut function partial to get text at a particular xpath:
        doc = _url_2_lxml(url)
        _get_text = partial(get_text, doc, 0)

        # Get session number--needed for fetching related documents (see below).
        xpath = '//font[contains(., "General Assembly") and @face="Arial"]'
        session_num = doc.xpath(xpath)[0].text_content()
        session_num = re_digits.match(session_num).group()

        #---------------------------------------------------------------------
        # Sponsors
        chamber = bill['chamber']

        sponsor_types = {
            'Additional Sponsor(s):': 'cosponsor',
            'CoSponsors:': 'cosponsor',
            'Primary Sponsor:': 'primary'}

        xpath = '//font[contains(., "Sponsor") and @color="#008080"]'
        headings = doc.xpath(xpath + '/text()')
        sponsors = doc.xpath(xpath + '/../../following-sibling::td/font/text()')

        for h, s in zip(headings, sponsors):

            names = _cleanup_sponsors(s, chamber)
            type_ = sponsor_types[h.strip()]

            if names:
                for name, _chamber in names:
                    bill.add_sponsor(type_, name, chamber=_chamber)

        #---------------------------------------------------------------------
        # Versions

        tmp = '/'.join([
            'http://www.legis.delaware.gov',
            'LIS/lis{session_num}.nsf/vwLegislation',
            '{moniker}/$file/{filename}{format_}?open'])

        documents = self.scrape_documents(source=url,
                                     docname="introduced",
                                     filename="Legis",
                                     tmp=tmp,
                                     session_num=session_num)

        for d in documents:
            bill.add_version(**d)

        # If bill is a substitution, add the original as a version.
        names = doc.xpath('//*[contains(text(), "Substituted '
                          'Legislation for Bill:")]/text()')
        urls = doc.xpath('//*[contains(text(), "Substituted '
                          'Legislation for Bill:")]'
                         '/following-sibling::a/@href')

        for name, url in zip(names, urls):

            name = re_substitution.match(name).group(1)
            bill.add_version(name, url,
                             description='original bill')

        #---------------------------------------------------------------------
        # Actions
        actions = doc.xpath('//font[contains(., "Actions History")]'
                            '/../following-sibling::table/descendant::td[2]')
        actions = actions[0].text_content()
        actions = filter(None, actions.splitlines())

        for a in reversed(actions):
            date, action = a.split(' - ', 1)
            try:
                date = datetime.strptime(date, '%b %d, %Y')
            except ValueError:
                date = datetime.strptime(date, '%B %d, %Y')  # XXX: ugh.

            actor = actions_get_actor(action, bill['chamber'])
            attrs = dict(actor=actor, action=action, date=date)
            attrs.update(**self.categorizer.categorize(action))
            bill.add_action(**attrs)

        #---------------------------------------------------------------------
        # Votes
        xpaths = [
            '//*[contains(text(), "vote:")]/following-sibling::a/@href',
            '//font[contains(., "vote:")]/a/@href']
        for xpath in xpaths:
            vote_urls = doc.xpath(xpath)
            if vote_urls:
                break

        for url in vote_urls:
            vote = self.scrape_vote(url)
            if vote:
                bill.add_vote(vote)

        #---------------------------------------------------------------------
        # Amendments
        xpath = ("//font[contains(., 'Amendments')]/"
                 "../../../td[2]/font/a")

        tmp = ('http://www.legis.delaware.gov/LIS/lis{session_num}.nsf/'
               'vwLegislation/{id_}/$file/{filename}{format_}?open')

        for source, id_ in zip(doc.xpath(xpath + '/@href'),
                               doc.xpath(xpath + '/text()')):

            match = re_amendment.match(id_)
            if match is None:
                match = re.search('/?([A-Z]A \\d{1,3}) to', id_)
            short_id = match.group(1)

            documents = self.scrape_documents(
                source=source,
                docname='amendment (%s)' % short_id,
                filename='Legis',
                tmp=tmp, session_num=session_num,
                id_=id_)

            for d in documents:
                bill.add_document(**d)

        #---------------------------------------------------------------------
        # Add any related "Engrossments".
        # See www.ncsl.org/documents/legismgt/ILP/98Tab3Pt4.pdf for
        # an explanation of the engrossment process in DE.
        source = doc.xpath('//img[@alt="Engrossment"]/../@href')

        if source:

            tmp = '/'.join([
                'http://www.legis.delaware.gov',
                'LIS/lis{session_num}.nsf/EngrossmentsforLookup',
                '{moniker}/$file/{filename}{format_}?open'])

            documents = self.scrape_documents(
                source=source[0],
                docname="Engrossment",
                filename="Engross",
                tmp=tmp,
                session_num=session_num,
                id_=bill['bill_id'])

            for d in documents:
                bill.add_version(**d)

        # --------------------------------------------------------------------
        # Add any fiscal notes.
        source = doc.xpath("//img[@alt='Fiscal Note']/../@href")

        if source:

            tmp = '/'.join([
                'http://www.legis.delaware.gov',
                'LIS/lis{session_num}.nsf/FiscalforLookup',
                '{docnum}/$file/{filename}{format_}?open'])

            documents = self.scrape_documents(
                source=source[0],
                docname="Fiscal Note",
                filename="Fiscal",
                tmp=tmp,
                session_num=session_num)

            for d in documents:
                bill.add_document(**d)

        #---------------------------------------------------------------------
        # Extra fields

        # Helper to get the first td sibling of certain nodes.
        tmp = '//font[contains(., "%s")]/../../../td[2]'
        first_sibling_text = lambda heading: _get_text(tmp % heading)

        extra_fields = {
            # A long description of the legislation.
            "summary": "Synopsis",
            # Codification details for enacted legislation.
            "volume_chapter": "Volume Chapter",
            # Presumably the date of approval/veto.
            "date_governor_acted": "Date Governor Acted",
            "fiscal_notes": "Fiscal Notes",
        }

        for key, name in extra_fields.iteritems():
            try:
                bill[key] = first_sibling_text(name)
            except IndexError:
                # xpath lookup failed.
                pass

        if bill['title'].strip() == "":
            if bill['bill_id'] != "HB 130" and bill['session'] != '147':
                raise Exception("bill title is empty")
            bill['title'] = bill['summary']
            # This added to help hack around the page that's missing
            # the bill title.

        self.save_bill(bill)

    def scrape_vote(self, url,
                    re_digit=re.compile(r'\d{1,3}'),
                    re_totals=re.compile(
                        r'(?:Yes|No|Not Voting|Absent):\s{,3}(\d{,3})', re.I)):
        namespaces = {"re": "http://exslt.org/regular-expressions"}
        try:
            html = self.urlopen(url)
            doc = lxml.html.fromstring(html)
        except scrapelib.HTTPError as e:
            known_fail_links = [
                "http://legis.delaware.gov/LIS/lis146.nsf/7712cf7cc0e9227a852568470077336f/cdfd8149e79c2bb385257a24006e9f7a?OpenDocument"
            ]
            if "404" in str(e.response):
                # XXX: Ugh, ok, so there's no way (that I could find quickly)
                #      to get the _actual_ response (just "ok") from the object.
                #      As a result, this. Forgive me.
                #            -PRT
                if url in known_fail_links:
                    msg = 'Recieved a bogus 22/404 return code. Skipping vote.'
                    self.warning(msg)
                    return
            raise

        if 'Committee Report' in lxml.html.tostring(doc):
            # This was a committee vote with weird formatting.
            self.info('Skipping committee report.')
            return

        xpath = ("//font[re:match(., '^(Yes|No|Not Voting|Absent):', 'i')]"
                 "/ancestor::tr[1]")

        # Get the vote tallies.
        try:
            totals = doc.xpath(xpath, namespaces=namespaces)
            totals = totals[0].text_content()

        except IndexError:
            # Here the vote page didn't have have the typical format.
            # Maybe it's a hand edited page. Log and try to parse
            # the vitals from plain text.
            self.warning('Found an unusual votes page at url: "%s"' % url)
            totals = re_totals.findall(doc.text_content())
            if len(totals) == 4:
                self.warning('...was able to parse vote tallies from "%s"' % url)

        else:
            totals = re_digit.findall(totals)

        try:
            yes_count, no_count, abstentions, absent = map(int, totals)

        except ValueError:
            # There were'nt any votes listed on this page. This is probably
            # a "voice vote" lacking actual vote tallies.
            yes_count, no_count, other_count = 0, 0, 0

        else:
            other_count = abstentions + absent

        font_text = [s.strip() for s in doc.xpath('//font/text()')]
        date_index = font_text.index('Date:')
        date_string = font_text[date_index + 2]
        date = datetime.strptime(date_string, '%m/%d/%Y %H:%M %p')
        passed = True if font_text[date_index + 4] else False
        counts = defaultdict(int)
        for key, string in [
            ('yes_count', 'Yes:'),
            ('no_count', 'No:'),
            ('absent_count', 'Absent:'),
            ('not_voting', 'Not Voting:')]:
            try:
                counts[key] = int(font_text[font_text.index(string) + 2])
            except ValueError:
                continue
        counts['other_count'] = counts['absent_count'] + counts['not_voting']

        chamber_string = doc.xpath('string(//b/u/font/text())').lower()
        if 'senate' in chamber_string:
            chamber = 'upper'
        elif 'house' in chamber_string:
            chamber = 'lower'

        for xpath in (
            'string(//td/b/text())',
            'string(//td/b/font/text())',
            'string(//form/b/font/text())'):
            motion = doc.xpath(xpath)
            if motion:
                break
            # Will fail at validictory level if no motion found.

        # Create the vote object.
        vote = Vote(chamber, date, motion, passed,
                    counts['yes_count'], counts['no_count'],
                    counts['other_count'])

        # Add source.
        vote.add_source(url)

        # Get the "vote type"
        el = doc.xpath('//font[contains(., "Vote Type:")]')[0]
        try:
            vote_type = el.xpath('following-sibling::font[1]/text()')[0]
        except IndexError:
            vote_type = el.xpath('../following-sibling::font[1]/text()')[0]

        vote['vote_type'] = vote_type

        # Get an iterator like: name1, vote1, name2, vote2, ...
        xpath = ("//font[re:match(., '^[A-Z]$')]"
                 "/../../../descendant::td/font/text()")
        data = doc.xpath(xpath, namespaces=namespaces)
        data = filter(lambda s: s.strip(), data)

        # Handle the rare case where not all names have corresponding
        # text indicating vote value. See e.g. session 146 HB10.
        data_len = len(data) / 2
        tally = sum(v for (k, v) in vote.items() if '_count' in k)

        if (0 < data_len) and ((data_len) != tally):
            xpath = ("//font[re:match(., '^[A-Z]$')]/ancestor::table")
            els = doc.xpath(xpath, namespaces=namespaces)[-1]
            els = els.xpath('descendant::td')
            data = [e.text_content().strip() for e in els]

        data = iter(data)

        # Add names and vote values.
        vote_map = {
            'Y': 'yes',
            'N': 'no',
            }

        while True:

            try:
                name = data.next()
                _vote = data.next()

                # Evidently, the motion for vote can be rescinded before
                # the vote is cast, perhaps due to a quorum failure.
                # (See the Senate vote (1/26/2011) for HB 10 w/HA 1.) In
                # this rare case, values in the vote col are whitespace. Skip.
                if not _vote.strip():
                    continue

                _vote = vote_map.get(_vote, 'other')
                getattr(vote, _vote)(name)

            except StopIteration:
                break

        return vote

    def scrape_documents(self, source, docname, filename, tmp, session_num,
                         re_docnum=re.compile(r'var F?docnum="(.+?)"'),
                         re_moniker=re.compile(r'var moniker="(.+?)"'),
                         **kwargs):
        '''
        Returns a generator like [{'name': 'docname', 'url': 'docurl'}, ...]
        '''
        source = source.replace(' ', '+')

        try:
            _doc = self._url_2_lxml(source)
        except scrapelib.HTTPError:
            # Grrr...there was a dead link posted. Warn and skip.
            msg = 'Related document download failed (dead link): %r' % source
            self.warning(msg)
            return

        if _doc.xpath('//font[contains(., "DRAFT INFORMATION")]'):
            # This amendment is apparently still in draft form or can't
            # be viewed for security reasons, but we can still link to
            # its status page.
            yield dict(name=docname, url=source)
            return

        # The full-text urls are generated using onlick javascript and
        # window-level vars named "moniker" and "docnum".
        if docname == "Fiscal Note":
            xpath = '//script[contains(., "var Fdocnum")]'
        else:
            xpath = '//script[contains(., "var docnum")]'
        script_text = _doc.xpath(xpath)[0].text_content()
        docnum = re_docnum.search(script_text).group(1)
        moniker = re_moniker.search(script_text).group(1)

        # Mimetypes.
        formats = ['.html', '.pdf', '.docx', '.Docx']
        mimetypes = {
            '.html': 'text/html',
            '.pdf': 'application/pdf',
            '.docx': 'application/msword'
            }

        for format_ in formats:

            el =_doc.xpath('//font[contains(., "%s%s")]' % (filename, format_))

            if not el:
                continue

            format_ = format_.lower()
            _kwargs = kwargs.copy()
            _kwargs.update(**locals())

            if format_.lower() == '.docx':
                _kwargs['filename'] = docnum
            else:
                _kwargs['filename'] = _kwargs['filename'].lower()

            url = tmp.format(**_kwargs).replace(' ', '+')

            try:
                self.urlopen(url)
            except scrapelib.HTTPError:
                msg = 'Could\'t fetch %s version at url: "%s".'
                self.warning(msg % (format_, url))
            else:
                yield dict(name=docname, url=url,
                           source=source, mimetype=mimetypes[format_])

########NEW FILE########
__FILENAME__ = committees
import re
import lxml.html

from billy.scrape.committees import CommitteeScraper, Committee


class DECommitteeScraper(CommitteeScraper):
    jurisdiction = "de"

    def scrape(self, chamber, term):

        urls = {
            'upper': 'http://legis.delaware.gov/LIS/LIS%s.nsf/SCommittees',
            'lower': 'http://legis.delaware.gov/LIS/LIS%s.nsf/HCommittees'
        }

        # Mapping of term names to session numbers (see metatdata).
        term2session = {"2013-2014": "147", "2011-2012": "146"}

        session = term2session[term]

        url = urls[chamber] % (session,)
        self.log(url)
        page = lxml.html.fromstring(self.urlopen(url))
        page.make_links_absolute(url)

        committees = {}

        for row in page.xpath('//td[@width="96%"]/table/tr[@valign="top"]'):
            link = row.xpath('td/font/a[contains(@href, "opendocument")]')[0]
            committees[link.text] = link.attrib['href']
            self.log(link.attrib['href'])

        for c in committees:
            url = committees[c]
            page = lxml.html.fromstring(self.urlopen(url))
            page.make_links_absolute(url)
            committee = Committee(chamber, c)
            committee.add_source(url)

            for tr in page.xpath('//td[@width="96%"]/table/tr'):
                role_section = tr.xpath('td/b/font')
                if(len(role_section) > 0):
                    role = re.sub(r's?:$', '', role_section[0].text).lower()
                    for member in tr.xpath('td/font/a'):
                        name = re.sub('\s+', ' ', member.text)
                        committee.add_member(name, role)

            self.save_committee(committee)

########NEW FILE########
__FILENAME__ = events
import datetime as dt

from billy.scrape.events import Event, EventScraper

import pytz
import lxml.html

chamber_urls = {
    "other" : [],
    "lower" : [ "http://legis.delaware.gov/LIS/lis146.nsf/House+Meeting+Notice/?openview&count=2000" ],
    "upper" : [ "http://legis.delaware.gov/LIS/lis146.nsf/Senate+Meeting+Notice/?openview&count=2000" ]
}
chambers = {
    "Senate" : "upper",
    "House"  : "lower",
    "Joint"  : "joint"
}

class DEEventScraper(EventScraper):
    jurisdiction = 'de'

    _tz = pytz.timezone('US/Eastern')

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def scrape_meeting_notice(self, chamber, session, url):
        page = self.lxmlize(url)
        bits = page.xpath("//td[@width='96%']/table/tr")
        metainf = {}
        for bit in bits:
            info = bit.xpath(".//td")
            key = info[0].text_content().strip()
            val = info[1].text_content().strip()
            if key[-1:] == ":":
                key = key[:-1]
            metainf[key] = val
        date_time_lbl = "Date/Time"
        # 04/25/2012 03:00:00 PM
        fmt = "%m/%d/%Y %I:%M:%S %p"
        metainf[date_time_lbl] = dt.datetime.strptime(metainf[date_time_lbl],
                                                     fmt)
        event = Event(session,
                      metainf[date_time_lbl],
                      "committee:meeting",
                      "Committee Meeting",
                      chamber=chambers[metainf['Chamber']],
                      location=metainf['Room'],
                      chairman=metainf['Chairman'])
        event.add_participant("host", metainf['Committee'], 'committee',
                              chamber=chambers[metainf['Chamber']])
        event.add_source(url)

        agenda = page.xpath("//td[@width='96%']//font[@face='Arial']")
        agenda = [ a.text_content().strip() for a in agenda ]
        if "" in agenda:
            agenda.remove("")
        for item in agenda:
            string = item.split()
            string = string[:2]
            fChar = string[0][0]
            watch = [ "H", "S" ]
            if fChar in watch:
                try:
                    bNo = int(string[1])
                except ValueError:
                    continue
                except IndexError:
                    continue
                bill_id = "%s %s" % ( string[0], string[1] )
                event.add_related_bill(
                    bill_id,
                    description=item,
                    type="consideration"
                )

        self.save_event(event)

    def scrape(self, chamber, session):
        self.log(chamber)
        urls = chamber_urls[chamber]
        for url in urls:
            page = self.lxmlize(url)
            events = page.xpath("//a[contains(@href, 'OpenDocument')]")
            for event in events:
                self.scrape_meeting_notice(chamber, session,
                                           event.attrib['href'])

########NEW FILE########
__FILENAME__ = legislators
from collections import defaultdict
from urlparse import urlunsplit
from urllib import urlencode
from operator import methodcaller
import re

import lxml.html

from billy.scrape.legislators import LegislatorScraper, Legislator


class DELegislatorScraper(LegislatorScraper):
    jurisdiction = 'de'

    def scrape(self, chamber, term, text=methodcaller('text_content'),
               re_spaces=re.compile(r'\s{,5}')):

        url = {
            'upper': 'http://legis.delaware.gov/legislature.nsf/sen?openview&nav=senate',
            'lower': 'http://legis.delaware.gov/legislature.nsf/Reps?openview&Count=75&nav=house&count=75',
            }[chamber]

        doc = lxml.html.fromstring(self.urlopen(url))
        doc.make_links_absolute(url)

        # Sneak into the main table...
        xpath = '//font[contains(., "Leadership Position")]/ancestor::table[1]'
        table = doc.xpath(xpath)[0]

        # Skip the first tr (headings)
        trs = table.xpath('tr')[1:]

        for tr in trs:

            bio_url = tr.xpath('descendant::a/@href')[0]
            name, _, district = map(text, tr.xpath("td"))
            name = ' '.join(re_spaces.split(name))

            leg = self.scrape_bio(term, chamber, district, name, bio_url)
            leg.add_source(bio_url, page="legislator detail page")
            leg.add_source(url, page="legislator list page")
            self.save_legislator(leg)

    def scrape_bio(self, term, chamber, district, name, url):
        # this opens the committee section without having to do another request
        url += '&TableRow=1.5.5'
        doc = lxml.html.fromstring(self.urlopen(url))
        doc.make_links_absolute(url)

        # party is in one of these
        party = doc.xpath('//div[@align="center"]/b/font[@size="2"]/text()')
        if '(D)' in party:
            party = 'Democratic'
        elif '(R)' in party:
            party = 'Republican'

        leg = Legislator(term, chamber, district, name, party=party, url=url)

        photo_url = doc.xpath('//img[contains(@src, "FieldElemFormat")]/@src')
        if photo_url:
            leg['photo_url'] = photo_url[0]

        contact_info = self.scrape_contact_info(doc)
        leg.update(contact_info)
        return leg

    def scrape_contact_info(self, doc):
        office_names = ['Legislative Hall Office', 'Outside Office']
        office_types = ['capitol', 'district']
        xpath = '//u[contains(., "Office")]/ancestor::table/tr[2]/td'
        data = zip(doc.xpath(xpath)[::2], office_names, office_types)
        info = {}

        # Email
        xpath = '//font[contains(., "E-mail Address")]/../font[2]'
        email = doc.xpath(xpath)[0].text_content()

        # If multiple email addresses listed, only take the official
        # noone@state.de.us address.
        emails = re.split(r'(?:\n| or |;|\s+)', email)
        for email in filter(None, emails):
            if email.strip():
                info['email'] = email.strip()
                break

        # Offices
        offices = []
        for (td, name, type_) in data:
            office = dict(name=name, type=type_, phone=None,
                          fax=None, email=None, address=None)

            chunks = td.text_content().strip().split('\n\n')
            chunks = [s.strip() for s in chunks]
            chunks = filter(None, chunks)
            if len(chunks) == 1:
                if ':' in chunks[0].splitlines()[0]:
                    # It's just phone numbers with no actual address.
                    numbers = [chunks[0]]
                    office['address'] = None
                else:
                    office['address'] = chunks[0]
            else:
                if not chunks:
                    # This office has no data.
                    continue
                address = chunks.pop(0)
                numbers = chunks
                office['address'] = address
            for number in numbers:
                for line in number.splitlines():
                    if not line.strip():
                        continue
                    for key in ('phone', 'fax'):
                        if key in line.lower():
                            break
                    number = re.search('\(\d{3}\) \d{3}\-\d{4}', line)
                    if number:
                        number = number.group()
                        office[key] = number
                offices.append(office)

        return dict(info, offices=offices)

########NEW FILE########
__FILENAME__ = bills
import os
import re
import datetime

from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote
from billy.scrape.utils import convert_pdf

import lxml.html


class FLBillScraper(BillScraper):
    jurisdiction = 'fl'

    def scrape(self, chamber, session):
        self.validate_session(session)

        cname = {'upper': 'Senate', 'lower': 'House'}[chamber]
        url = "http://flsenate.gov/Session/Bills/%s?chamber=%s"
        url = url % (session, cname)
        while True:
            html = self.urlopen(url)
            page = lxml.html.fromstring(html)
            page.make_links_absolute(url)

            link_path = ("//a[contains(@href, '/Session/Bill/%s')"
                         "and starts-with(., '%s')]" % (
                             session, cname[0]))
            for link in page.xpath(link_path):
                bill_id = link.text.strip()
                title = link.xpath(
                    "string(../following-sibling::td[1])").strip()
                sponsor = link.xpath(
                    "string(../following-sibling::td[2])").strip()
                url = link.attrib['href'] + '/ByCategory'
                self.scrape_bill(chamber, session, bill_id, title,
                                 sponsor, url)

            try:
                next_link = page.xpath("//a[contains(., 'Next')]")[0]
                url = next_link.attrib['href']
            except (KeyError, IndexError):
                self.logger.info('Hit last page of search results.')
                break

    def accept_response(self, response):
        normal = super(FLBillScraper, self).accept_response(response)
        bill_check = True
        text_check = True

        if not response.url.lower().endswith('pdf'):
            if response.url.startswith("http://flsenate.gov/Session/Bill/20"):
                bill_check = "tabBodyVoteHistory" in response.text

            text_check = \
                    'he page you have requested has encountered an error.' \
                    not in response.text

        valid = (normal and
                bill_check and
                text_check)
        if not valid:
            raise TimSucksError('Response was invalid. Timsucks.')
        return valid

    def scrape_bill(self, chamber, session, bill_id, title, sponsor, url):
        try:
            html = self.urlopen(url)
        except TimSucksError:
            return
        page = lxml.html.fromstring(html)
        page.make_links_absolute(url)

        bill = Bill(session, chamber, bill_id, title)
        bill.add_source(url)

        bill.add_sponsor('primary', sponsor)

        next_href = url + '/?Tab=BillHistory'
        html = self.urlopen(next_href)
        hist_page = lxml.html.fromstring(html)
        hist_page.make_links_absolute(url)

        try:
            hist_table = hist_page.xpath(
                "//div[@id = 'tabBodyBillHistory']//table")[0]
        except IndexError:
            self.warning('no tabBodyBillHistory in %s, attempting to '
                         'refetch once' % url)
            html = self.urlopen(url)
            hist_page = lxml.html.fromstring(next_href)
            hist_page.make_links_absolute(next_href)

            hist_table = hist_page.xpath(
                "//div[@id = 'tabBodyBillHistory']//table")[0]

        # now try and get second h1
        bill_type_h2 = page.xpath('//h2/text()')[-1]
        if re.findall('[SH]B', bill_type_h2):
            bill_type = 'bill'
        elif re.findall('[SH]PB', bill_type_h2):
            bill_type = 'proposed bill'
        elif re.findall('[SH]R', bill_type_h2):
            bill_type = 'resolution'
        elif re.findall('[SH]JR', bill_type_h2):
            bill_type = 'joint resolution'
        elif re.findall('[SH]CR', bill_type_h2):
            bill_type = 'concurrent resolution'
        elif re.findall('[SH]M', bill_type_h2):
            bill_type = 'memorial'
        elif re.findall('\s+Senate \d+', bill_type_h2):
            bill_type = 'bill'
        else:
            raise Exception('Failed to identify bill type.')

        bill['type'] = [bill_type]

        for tr in hist_table.xpath("tbody/tr"):
            date = tr.xpath("string(td[1])")
            date = datetime.datetime.strptime(
                date, "%m/%d/%Y").date()

            actor = tr.xpath("string(td[2])")
            actor = {'Senate': 'upper', 'House': 'lower'}.get(
                actor, actor)

            if not actor:
                continue

            act_text = tr.xpath("string(td[3])").strip()
            for action in act_text.split(u'\u2022'):
                action = action.strip()
                if not action:
                    continue

                action = re.sub(r'-(H|S)J\s+(\d+)$', '',
                                action)

                atype = []
                if action.startswith('Referred to'):
                    atype.append('committee:referred')
                elif action.startswith('Favorable by'):
                    atype.append('committee:passed')
                elif action == "Filed":
                    atype.append("bill:filed")
                elif action.startswith("Withdrawn"):
                    atype.append("bill:withdrawn")
                elif action.startswith("Died"):
                    atype.append("bill:failed")
                elif action.startswith('Introduced'):
                    atype.append('bill:introduced')
                elif action.startswith('Read 2nd time'):
                    atype.append('bill:reading:2')
                elif action.startswith('Read 3rd time'):
                    atype.append('bill:reading:3')
                elif action.startswith('Adopted'):
                    atype.append('bill:passed')
                elif action.startswith('CS passed'):
                    atype.append('bill:passed')

                bill.add_action(actor, action, date, type=atype)

        try:
            version_table = page.xpath(
                "//div[@id = 'tabBodyBillText']/table")[0]
            for tr in version_table.xpath("tbody/tr"):
                name = tr.xpath("string(td[1])").strip()
                version_url = tr.xpath("td/a[1]")[0].attrib['href']
                if version_url.endswith('PDF'):
                    mimetype = 'application/pdf'
                elif version_url.endswith('HTML'):
                    mimetype = 'text/html'
                bill.add_version(name, version_url, mimetype=mimetype)
        except IndexError:
            self.log("No version table for %s" % bill_id)

        try:
            analysis_table = page.xpath(
                "//div[@id = 'tabBodyAnalyses']/table")[0]
            for tr in analysis_table.xpath("tbody/tr"):
                name = tr.xpath("string(td[1])").strip()
                name += " -- " + tr.xpath("string(td[3])").strip()
                date = tr.xpath("string(td[4])").strip()
                if date:
                    name += " (%s)" % date
                analysis_url = tr.xpath("td/a")[0].attrib['href']
                bill.add_document(name, analysis_url)
        except IndexError:
            self.log("No analysis table for %s" % bill_id)

        next_href = url + '/?Tab=VoteHistory'
        html = self.urlopen(next_href)
        vote_page = lxml.html.fromstring(html)
        vote_page.make_links_absolute(url)

        vote_tables = vote_page.xpath(
            "//div[@id = 'tabBodyVoteHistory']//table")

        for vote_table in vote_tables:
            for tr in vote_table.xpath("tbody/tr"):
                vote_date = tr.xpath("string(td[3])").strip()
                if vote_date.isalpha():
                    vote_date = tr.xpath("string(td[2])").strip()
                version = tr.xpath("string(td[1])").strip().split()
                version_chamber = version[0]

                vote_chamber = chamber
                try:
                    vote_date = datetime.datetime.strptime(
                        vote_date, "%m/%d/%Y %H:%M %p").date()
                except ValueError:
                    msg = 'Got bogus vote date: %r'
                    self.logger.warning(msg % vote_date)

                vote_url = tr.xpath("td[4]/a")[0].attrib['href']
                self.scrape_vote(bill, vote_chamber, vote_date,
                                 vote_url)
        else:
            self.log("No vote table for %s" % bill_id)

        self.save_bill(bill)

    def scrape_vote(self, bill, chamber, date, url):

        (path, resp) = self.urlretrieve(url)
        text = convert_pdf(path, 'text')
        os.remove(path)

        try:
            motion = text.split('\n')[4].strip()
        except IndexError:
            return

        try:
            yes_count = int(re.search(r'Yeas - (\d+)', text).group(1))
        except AttributeError:
            return

        no_count = int(re.search(r'Nays - (\d+)', text).group(1))
        other_count = int(re.search(r'Not Voting - (\d+)', text).group(1))
        passed = yes_count > (no_count + other_count)

        vote = Vote(chamber, date, motion, passed, yes_count, no_count,
                    other_count)
        vote.add_source(url)

        y, n, o = 0, 0, 0
        break_outter = False

        for line in text.split('\n')[9:]:
            if break_outter:
                break

            if 'after roll call' in line:
                break
            if 'Indication of Vote' in line:
                break
            if 'Presiding' in line:
                continue

            for col in re.split(r'-\d+', line):
                col = col.strip()
                if not col:
                    continue

                match = re.match(r'(Y|N|EX|\*)\s+(.+)$', col)

                if match:
                    name = match.group(2)
                    if '=' in name:
                        continue
                    if match.group(1) == 'Y':
                        vote.yes(name)
                    elif match.group(1) == 'N':
                        vote.no(name)
                    else:
                        vote.other(name)
                else:
                    if "PAIR" in line:
                        break_outter = True
                        break
                    vote.other(col.strip())

        # vote.validate()
        if not vote['motion']:
            vote['motion'] = '[No motion given.]'

        bill.add_vote(vote)


class TimSucksError(Exception):
    '''Raise if response is invalid.
    '''

########NEW FILE########
__FILENAME__ = committees
import re
from itertools import chain

from billy.scrape import NoDataForPeriod
from billy.scrape.committees import CommitteeScraper, Committee

import lxml.html


class FLCommitteeScraper(CommitteeScraper):
    jurisdiction = 'fl'

    def scrape(self, chamber, term):
        self.validate_term(term, latest_only=True)

        if chamber == 'upper':
            self.scrape_upper_committees()
        else:
            self.scrape_lower_committees()

    def scrape_upper_committees(self):
        url = "http://flsenate.gov/Committees/"
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        path = "//a[contains(@href, 'Committees/Show')]"
        for link in page.xpath(path):
            comm_name = link.text.strip()

            if comm_name.startswith('Joint'):
                continue

            if 'Subcommittee on' in comm_name:
                comm_name, sub_name = comm_name.split(' Subcommittee on ')
            else:
                comm_name, sub_name = comm_name, None

            comm = Committee('upper', comm_name, sub_name)
            self.scrape_upper_committee(comm, link.attrib['href'])
            if comm['members']:
                self.save_committee(comm)

    def scrape_upper_committee(self, comm, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        comm.add_source(url)

        path = "//a[contains(@href, 'Senators')]/name"
        seen = set()
        for name in page.xpath(path):
            dt = name.xpath("../../preceding-sibling::dt")
            if dt:
                mtype = dt[0].text.strip(': \r\n\t').lower()
            else:
                mtype = 'member'

            member = re.sub(r'\s+', ' ', name.text.strip())
            if (member, mtype) not in seen:
                comm.add_member(member, mtype)
                seen.add((member, mtype))

    def scrape_lower_committees(self):
        url = ("http://www.myfloridahouse.gov/Sections/Committees/"
               "committees.aspx")
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        for link in page.xpath("//a[contains(@href, 'CommitteeId')]"):
            comm_name = link.text.strip()

            if 'Committee' in comm_name:
                parent = re.sub(r'Committee$', '', comm_name).strip()
                sub = None
            else:
                sub = re.sub(r'Subcommittee$', '', comm_name).strip()

            comm = Committee('lower', parent, sub)
            self.scrape_lower_committee(comm, link.get('href'))
            if comm['members']:
                self.save_committee(comm)

        for link in page.xpath('//a[contains(@href, "committees/joint")]/@href'):
            self.scrape_joint_committee(link)

    def scrape_joint_committee(self, url):
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        name = doc.xpath('//h1/text()') or doc.xpath('//h2/text()')
        name = name[0].strip()

        comm = Committee('joint', name)
        comm.add_source(url)

        members = chain(doc.xpath('//a[contains(@href, "MemberId")]'),
                        doc.xpath('//a[contains(@href, "Senators")]'))

        seen = set()
        for a in members:
            parent_content = a.getparent().text_content()
            if ':' in parent_content:
                title = parent_content.split(':')[0].strip()
            else:
                title = 'member'

            name = a.text.split(' (')[0].strip()
            if (name, title) not in seen:
                comm.add_member(name, title)
                seen.add((name, title))

        if comm['members']:
            self.save_committee(comm)

    def scrape_lower_committee(self, comm, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        comm.add_source(url)

        for link in page.xpath("//p[@class='committeelinks']/a[contains(@href, 'MemberId')]"):
            # strip off spaces and everything in [R/D]
            name = link.text.strip().split(' [')[0]
            # membership type span follows link
            mtype = link.getnext().text_content().strip()
            if not mtype:
                mtype = 'member'

            comm.add_member(name, mtype)

########NEW FILE########
__FILENAME__ = events
import re
import datetime

from billy.scrape.events import EventScraper, Event

import feedparser


class FLEventScraper(EventScraper):
    jurisdiction = 'fl'

    def scrape(self, chamber, session):
        self.scrape_upper_events(session)

    def scrape_upper_events(self, session):
        url = "http://flsenate.gov/Session/DailyCalendarRSS.cfm?format=rss"
        page = self.urlopen(url)
        feed = feedparser.parse(page)

        for entry in feed['entries']:
            if 'Committee' not in entry['summary']:
                continue

            date = datetime.datetime(*entry['updated_parsed'][:6])
            match = re.match(r'(\d+):(\d+)', entry['title'])
            if match:
                when = datetime.datetime(date.year, date.month,
                                         date.day,
                                         int(match.group(1)),
                                         int(match.group(2)),
                                         0)

                desc = entry['summary'].split(' - ')[0]
                location = entry['summary'].split(' - ')[1]

                event = Event(session, when, 'committee:meeting',
                              desc, location)
                event.add_source(url)

                self.save_event(event)

########NEW FILE########
__FILENAME__ = legislators
import re
import urlparse

from billy.scrape.legislators import LegislatorScraper, Legislator

import lxml.html


class FLLegislatorScraper(LegislatorScraper):
    jurisdiction = 'fl'
    latest_only = True

    def scrape(self, chamber, term):
        if chamber == 'upper':
            self.scrape_senators(term)
        else:
            self.scrape_reps(term)

    def scrape_sen_offices(self, leg, leg_url):
        doc = lxml.html.fromstring(self.urlopen(leg_url))
        email = doc.xpath('//a[contains(@href, "mailto:")]')[0].get('href').split(':')[-1]
        leg['email'] = email

        # order of this page is
        # h3 - District Offices
        # p - office info
        # h4 - Legislative Assistant
        # p - legislative assistant
        # (repeat)
        # h3 - Tallahassee Office
        # p - office info
        skip_next_p = False
        office_type = 'district'
        office_name = 'District Office'
        els = iter(doc.xpath('//h4[contains(text(), "District Office")]/following-sibling::*'))

        while True:
            try:
                elem = next(els)
            except StopIteration:
                break

            # Skip legislative assistants and secretaries.
            if elem.tag == 'h5':
                while elem.tag != 'h4':
                    try:
                        elem = next(els)
                    except StopIteration:
                        break

            if elem.tag == 'p' and elem.text_content().strip():
                # not skipping, parse the office
                address = []
                phone = None
                fax = None
                for line in elem.xpath('text()'):
                    line = line.strip()
                    if line.startswith('('):
                        phone = line
                    elif line.startswith('FAX '):
                        fax = line[4:]
                    elif line.startswith(('Senate VOIP', 'Statewide')):
                        continue
                    else:
                        address.append(line)
                # done parsing address
                leg.add_office(office_type, office_name,
                               address='\n'.join(address), phone=phone,
                               fax=fax)

            elif elem.tag == 'h4' and 'Tallahassee Office' in elem.text:
                office_type = 'capitol'
                office_name = 'Tallahassee Office'

    def scrape_senators(self, term):
        url = "http://www.flsenate.gov/Senators/"
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        for link in page.xpath("//a[contains(@href, 'Senators/s')]"):
            name = link.text.strip()
            name = re.sub(r'\s+', ' ', name)
            leg_url = link.get('href')
            leg_doc = lxml.html.fromstring(self.urlopen(leg_url))
            leg_doc.make_links_absolute(leg_url)

            if 'Vacant' in name:
                continue

            # Special case - name_tools gets confused
            # by 'JD', thinking it is a suffix instead of a first name
            if name == 'Alexander, JD':
                name = 'JD Alexander'
            elif name == 'Vacant':
                name = 'Vacant Seat'

            district = link.xpath("string(../../td[1])")
            party = link.xpath("string(../../td[2])")

            # for consistency
            if party == 'Democrat':
                party = 'Democratic'

            if term != '2013-2014':
                raise ValueError('Please change the senate photo_url string.')
            photo_url = leg_doc.xpath('//div[@id="sidebar"]//img/@src').pop()

            leg = Legislator(term, 'upper', district, name,
                             party=party, photo_url=photo_url, url=leg_url)
            leg.add_source(url)
            leg.add_source(leg_url)

            self.scrape_sen_offices(leg, leg_url)

            self.save_legislator(leg)

    def scrape_rep_office(self, leg, doc, name):
        pieces = [x.tail.strip() for x in
                  doc.xpath('//strong[text()="%s"]/following-sibling::br' %
                            name)]
        if not pieces:
            return
        address = []
        for piece in pieces:
            if piece.startswith('Phone:'):
                # 'Phone: \r\n        (303) 222-2222'
                if re.search(r'\d+', piece):
                    phone = piece.split(None, 1)[1]
                else:
                    phone = None
            else:
                piece = re.sub(r'\s+', ' ', piece)
                address.append(piece)

        office = dict(name=name, address='\n'.join(address))

        # Phone
        if phone is not None:
            office['phone'] = phone

        # Type
        if 'Capitol' in name:
            office['type'] = 'capitol'
        elif 'District' in name:
            office['type'] = 'district'

        leg.add_office(**office)

    def scrape_reps(self, term):
        url = ("http://www.flhouse.gov/Sections/Representatives/"
               "representatives.aspx")

        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        for div in page.xpath('//div[@id="rep_icondocks2"]'):
            link = div.xpath('.//div[@class="membername"]/a')[0]
            name = link.text_content().strip()

            if 'Vacant' in name or 'Resigned' in name:
                continue

            party = div.xpath('.//div[@class="partyname"]/text()')[0].strip()
            if party == 'Democrat':
                party = 'Democratic'

            district = div.xpath('.//div[@class="districtnumber"]/text()')[0].strip()

            leg_url = link.get('href')
            split_url = urlparse.urlsplit(leg_url)
            member_id = urlparse.parse_qs(split_url.query)['MemberId'][0]
            photo_url = ("http://www.flhouse.gov/FileStores/Web/"
                         "Imaging/Member/%s.jpg" % member_id)

            leg = Legislator(term, 'lower', district, name,
                             party=party, photo_url=photo_url, url=leg_url)

            # offices
            leg_doc = lxml.html.fromstring(self.urlopen(leg_url))
            self.scrape_rep_office(leg, leg_doc, 'Capitol Office')
            self.scrape_rep_office(leg, leg_doc, 'District Office')

            leg.add_source(url)
            leg.add_source(leg_url)
            self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = bills
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote
from collections import defaultdict
from .util import get_client, get_url, backoff

#         Methods (7):
#            GetLegislationDetail(xs:int LegislationId, )
#
#            GetLegislationDetailByDescription(ns2:DocumentType DocumentType,
#                                              xs:int Number, xs:int SessionId)
#
#            GetLegislationForSession(xs:int SessionId, )
#
#            GetLegislationRange(ns2:LegislationIndexRangeSet Range, )
#
#            GetLegislationRanges(xs:int SessionId,
#                           ns2:DocumentType DocumentType, xs:int RangeSize, )
#
#            GetLegislationSearchResultsPaged(ns2:LegislationSearchConstraints
#                                               Constraints, xs:int PageSize,
#                                               xs:int StartIndex, )
#            GetTitles()


member_cache = {}
SOURCE_URL = "http://www.legis.ga.gov/Legislation/en-US/display/{session}/{bid}"


class GABillScraper(BillScraper):
    jurisdiction = 'ga'
    lservice = get_client("Legislation").service
    vservice = get_client("Votes").service
    mservice = get_client("Members").service
    lsource = get_url("Legislation")
    msource = get_url("Members")
    vsource = get_url("Votes")

    def get_member(self, member_id):
        if member_id in member_cache:
            return member_cache[member_id]

        mem = backoff(self.mservice.GetMember, member_id)
        member_cache[member_id] = mem
        return mem

    def scrape(self, session, chambers):
        sid = self.metadata['session_details'][session]['_guid']
        legislation = backoff(
            self.lservice.GetLegislationForSession,
            sid
        )['LegislationIndex']
        for leg in legislation:
            lid = leg['Id']
            instrument = backoff(self.lservice.GetLegislationDetail, lid)
            history = [x for x in instrument['StatusHistory'][0]]
            actions = reversed([{
                "code": x['Code'],
                "action": x['Description'],
                "_guid": x['Id'],
                "date": x['Date']
            } for x in history])

            guid = instrument['Id']

            bill_type = instrument['DocumentType']
            chamber = {
                "H": "lower",
                "S": "upper",
                "J": "joint"
            }[bill_type[0]]  # XXX: This is a bit of a hack.

            bill_id = "%s %s" % (
                bill_type,
                instrument['Number'],
            )
            if instrument['Suffix']:
                bill_id += instrument['Suffix']

            title = instrument['Caption']
            description = instrument['Summary']

            if title is None:
                continue

            bill = Bill(
                session,
                chamber,
                bill_id,
                title,
                description=description,
                _guid=guid
            )

            if instrument['Votes']:
                for vote_ in instrument['Votes']:
                    _, vote_ = vote_
                    vote_ = backoff(self.vservice.GetVote, vote_[0]['VoteId'])

                    vote = Vote(
                        {"House": "lower", "Senate": "upper"}[vote_['Branch']],
                        vote_['Date'],
                        vote_['Caption'] or "Vote on Bill",
                        (vote_['Yeas'] > vote_['Nays']),
                        vote_['Yeas'],
                        vote_['Nays'],
                        (vote_['Excused'] + vote_['NotVoting']),
                        session=session,
                        bill_id=bill_id,
                        bill_chamber=chamber)

                    vote.add_source(self.vsource)

                    methods = {"Yea": vote.yes, "Nay": vote.no,}

                    for vdetail in vote_['Votes'][0]:
                        whom = vdetail['Member']
                        how = vdetail['MemberVoted']
                        try:
                            m = methods[how]
                        except KeyError:
                            m = vote.other
                        m(whom['Name'])

                    bill.add_vote(vote)


            types = {
                "HI": ["other"],
                "SI": ["other"],
                "HH": ["other"],
                "SH": ["other"],
                "HPF": ["bill:introduced"],
                "HDSAS": ["other"],
                "SPF": ["bill:introduced"],
                "HSR": ["bill:reading:2"],
                "SSR": ["bill:reading:2"],
                "HFR": ["bill:reading:1"],
                "SFR": ["bill:reading:1"],
                "HRECM": ["bill:withdrawn", "committee:referred"],
                "SRECM": ["bill:withdrawn", "committee:referred"],
                "SW&C": ["bill:withdrawn", "committee:referred"],
                "HW&C": ["bill:withdrawn", "committee:referred"],
                "HRA": ["bill:passed"],
                "SRA": ["bill:passed"],
                "HPA": ["bill:passed"],
                "HRECO": ["other"],
                "SPA": ["bill:passed"],
                "HTABL": ["other"],  # "House Tabled" - what is this?
                "SDHAS": ["other"],
                "HCFR": ["committee:passed:favorable"],
                "SCFR": ["committee:passed:favorable"],
                "HRAR": ["committee:referred"],
                "SRAR": ["committee:referred"],
                "STR": ["bill:reading:3"],
                "SAHAS": ["other"],
                "SE": ["bill:passed"],
                "SR": ["committee:referred"],
                "HTRL": ["bill:reading:3", "bill:failed"],
                "HTR": ["bill:reading:3"],
                "S3RLT": ["bill:reading:3", "bill:failed"],
                "HASAS": ["other"],
                "S3RPP": ["other"],
                "STAB": ["other"],
                "SRECO": ["other"],
                "SAPPT": ["other"],
                "HCA": ["other"],
                "HNOM": ["other"],
                "HTT": ["other"],
                "STT": ["other"],
                "SRECP": ["other"],
                "SCRA": ["other"],
                "SNOM": ["other"],
                "S2R": ["bill:reading:2"],
                "H2R": ["bill:reading:2"],
                "SENG": ["bill:passed"],
                "HENG": ["bill:passed"],
                "HPOST": ["other"],
                "HCAP": ["other"],
                "SDSG": ["governor:signed"],
                "SSG": ["governor:received"],
                "Signed Gov": ["governor:signed"],
                "HDSG": ["governor:signed"],
                "HSG": ["governor:received"],
                "EFF": ["other"],
                "HRP": ["other"],
                "STH": ["other"],
                "HTS": ["other"],
            }

            ccommittees = defaultdict(list)
            committees = instrument['Committees']
            if committees:
                for committee in committees[0]:
                    ccommittees[{
                        "House": "lower",
                        "Senate": "upper",
                    }[committee['Type']]].append(committee['Name'])

            for action in actions:
                chamber = {
                    "H": "lower",
                    "S": "upper",
                    "E": "other",  # Effective Date
                }[action['code'][0]]

                try:
                    _types = types[action['code']]
                except KeyError:
                    self.debug(action)
                    _types = ["other"]

                committees = []
                if any(('committee' in x for x in _types)):
                    committees = [str(x) for x in ccommittees.get(chamber, [])]

                bill.add_action(chamber, action['action'], action['date'], _types,
                                committees=committees,
                                _code=action['code'],
                                _code_id=action['_guid'])

            sponsors = []
            if instrument['Authors']:
                sponsors = instrument['Authors']['Sponsorship']
                if 'Sponsors' in instrument and instrument['Sponsors']:
                    sponsors += instrument['Sponsors']['Sponsorship']

            sponsors = [
                (x['Type'], self.get_member(x['MemberId'])) for x in sponsors
            ]

            for typ, sponsor in sponsors:
                name = "{First} {Last}".format(**dict(sponsor['Name']))
                bill.add_sponsor(
                    'primary' if 'Author' in typ else 'seconday',
                     name
                )

            for version in instrument['Versions']['DocumentDescription']:
                name, url, doc_id, version_id = [
                    version[x] for x in [
                        'Description',
                        'Url',
                        'Id',
                        'Version'
                    ]
                ]
                bill.add_version(
                    name,
                    url,
                    mimetype='application/pdf',
                    _internal_document_id=doc_id,
                    _version_id=version_id
                )

            bill.add_source(self.msource)
            bill.add_source(self.lsource)
            bill.add_source(SOURCE_URL.format(**{
                "session": session,
                "bid": guid,
            }))
            self.save_bill(bill)

########NEW FILE########
__FILENAME__ = committees
import time

from billy.scrape.committees import CommitteeScraper, Committee
from .util import get_client, get_url, backoff


CTTIE_URL = ("http://www.house.ga.gov/COMMITTEES/en-US/committee.aspx?"
             "Committee={cttie}&Session={sid}")


class GACommitteeScraper(CommitteeScraper):
    jurisdiction = 'ga'
    latest_only = True

    cservice = get_client("Committees").service
    csource = get_url("Committees")
    ctty_cache = {}

    def scrape_session(self, term, chambers, session):
        sid = self.metadata['session_details'][session]['_guid']
        committees = backoff(self.cservice.GetCommitteesBySession, sid)

        #if committees.strip() == "":
        #    return  # If we get here, it's a problem.
        # Commenting this out for future debugging. - PRT

        committees = committees['CommitteeListing']
        for committee in committees:
            cid = committee['Id']
            committee = backoff(self.cservice.GetCommittee, cid)
            name, typ, guid, code, description = [committee[x] for x in [
                'Name', 'Type', 'Id', 'Code', 'Description'
            ]]
            chamber = {
                "House": "lower",
                "Senate": "upper",
                "Joint": "joint"
            }[typ]
            ctty = None
            if code in self.ctty_cache:
                ctty = self.ctty_cache[code]
                if (ctty['chamber'] != chamber) and (description and
                        'joint' in description.lower()):
                    ctty['chamber'] = 'joint'
                else:
                    ctty = None

            if ctty is None:
                ctty = Committee(
                    chamber,
                    name,
                    code=code,
                    _guid=guid,
                    description=description
                )
                self.ctty_cache[code] = ctty

            members = committee['Members']['CommitteeMember']
            for member in members:
                name = "{First} {Last}".format(**dict(member['Member']['Name']))
                role = member['Role']
                ctty.add_member(name, role, _guid=member['Member']['Id'])

            ctty.add_source(self.csource)
            ctty.add_source(CTTIE_URL.format(**{
                "sid": sid,
                "cttie": guid,
            }))
            self.save_committee(ctty)

    def scrape(self, term, chambers):
        for t in self.metadata['terms']:
            if t['name'] == term:
                for session in t['sessions']:
                    self.scrape_session(term, chambers, session)

########NEW FILE########
__FILENAME__ = legislators
import time

from billy.scrape.legislators import LegislatorScraper, Legislator
from .util import get_client, get_url, backoff

import lxml


HOMEPAGE_URLS = {
    "lower": ("http://www.house.ga.gov/Representatives/en-US/"
             "member.aspx?Member={code}&Session={sid}"),
    "upper": ("http://www.senate.ga.gov/SENATORS/en-US/"
              "member.aspx?Member={code}&Session={sid}")
}


class GALegislatorScraper(LegislatorScraper):
    jurisdiction = 'ga'
    sservice = get_client("Members").service
    ssource = get_url("Members")

    def clean_list(self, dirty_list):
        new_list = []
        for x in dirty_list:
            if x is None:
                new_list.append(x)
            else:
                new_list.append(x.strip())
        return new_list

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def scrape_homepage(self, url, kwargs):
        url = url.format(**kwargs)
        page = self.lxmlize(url)
        images = page.xpath("//img[contains(@src, 'SiteCollectionImages')]")

        if len(images) != 1:
            raise Exception

        return url, images[0].attrib['src']

    def scrape_session(self, term, chambers, session):
        session = self.metadata['session_details'][session]
        sid = session['_guid']
        members = backoff(
            self.sservice.GetMembersBySession,
            sid
        )['MemberListing']

        for member in members:
            guid = member['Id']
            member_info = backoff(self.sservice.GetMember, guid)
            nick_name, first_name, middle_name, last_name = (
                member_info['Name'][x] for x in [
                    'Nickname', 'First', 'Middle', 'Last'
                ]
            )

            first_name = nick_name if nick_name else first_name

            if middle_name:
                full_name = "%s %s %s" % (first_name, middle_name, last_name)
            else:
                full_name = "%s %s" % (first_name, last_name)

            legislative_service = []
            for leg_service in member_info['SessionsInService']['LegislativeService']:
                if leg_service['Session']['Id'] == sid:
                    legislative_service = leg_service

            if legislative_service:
                party = legislative_service['Party']

                if party == 'Democrat':
                    party = 'Democratic'

                if party.strip() == '':
                    party = 'other'

                chamber, district = (
                    legislative_service['District'][x] for x in [
                        'Type', 'Number'
                    ]
                )

                chamber = {
                    "House": 'lower',
                    "Senate": 'upper'
                }[chamber]

                url, photo = self.scrape_homepage(HOMEPAGE_URLS[chamber],
                                                  {"code": guid, "sid": sid})

            else:
                raise Exception("Something very bad is going on with the "
                                "Legislative service")


            legislator = Legislator(
                term,
                chamber,
                str(district),
                full_name,
                party=party,
                last_name=last_name,
                first_name=first_name,
                url=url,
                photo_url=photo,
                _guid=guid
            )

            capital_address = self.clean_list([
                member_info['Address'][x] for x in [
                    'Street', 'City', 'State', 'Zip'
                ]
            ])

            capital_address = (" ".join(
                addr_component for addr_component
                    in capital_address if addr_component
            )).strip()

            capital_contact_info = self.clean_list([
                member_info['Address'][x] for x in [
                    'Email', 'Phone', 'Fax'
                ]
            ])

            # Sometimes email is set to a long cryptic string.
            # If it doesn't have a @ character, simply set it to None
            # examples:
            # 01X5dvct3G1lV6RQ7I9o926Q==&c=xT8jBs5X4S7ZX2TOajTx2W7CBprTaVlpcvUvHEv78GI=
            # 01X5dvct3G1lV6RQ7I9o926Q==&c=eSH9vpfdy3XJ989Gpw4MOdUa3n55NTA8ev58RPJuzA8=

            if capital_contact_info[0] and '@' not in capital_contact_info[0]:
                capital_contact_info[0] = None

            # if we have more than 2 chars (eg state)
            # or a phone/fax/email address record the info
            if len(capital_address) > 2 or not capital_contact_info.count(None) == 3:
                if (capital_contact_info[0] \
                        and 'quickrxdrugs@yahoo.com' in capital_contact_info[0]):
                    self.warning("XXX: GA SITE WAS HACKED.")
                    capital_contact_info[1] = None

                if capital_address.strip() != "":
                    legislator.add_office(
                        'capitol',
                        'Capitol Address',
                        address=capital_address,
                        phone=capital_contact_info[1],
                        fax=capital_contact_info[2],
                        email=capital_contact_info[0]
                    )

            district_address = self.clean_list([
                member_info['DistrictAddress'][x] for x in [
                    'Street', 'City', 'State', 'Zip'
                ]
            ])

            district_contact_info = self.clean_list([
                member_info['DistrictAddress'][x] for x in [
                    'Email', 'Phone', 'Fax'
                ]
            ])

            # Same issue with district email. See above comment
            if district_contact_info[0] and '@' not in district_contact_info[0]:
                district_contact_info[0] = None

            district_address = (
                " ".join(
                    addr_component for addr_component
                        in district_address if addr_component
                )).strip()

            if len(capital_address) > 2 or not capital_contact_info.count(None) == 3:
                if (district_contact_info[1] and \
                        'quickrxdrugs@yahoo.com' in district_contact_info[1]):
                    self.warning("XXX: GA SITE WAS HACKED.")
                    district_contact_info[1] = None

                if district_address.strip() != "":
                    legislator.add_office(
                        'district',
                        'District Address',
                        address=district_address,
                        phone=district_contact_info[1],
                        fax=district_contact_info[2],
                        email=district_contact_info[0]
                    )

            legislator.add_source(self.ssource)
            legislator.add_source(HOMEPAGE_URLS[chamber].format(
                **{"code": guid, "sid": sid}))

            self.save_legislator(legislator)

    def scrape(self, term, chambers):
        for t in self.metadata['terms']:
            if t['name'] == term:
                for session in t['sessions']:
                    self.scrape_session(term, chambers, session)

########NEW FILE########
__FILENAME__ = util
from suds.client import Client
import logging
import socket
import urllib2
import time
import suds

logging.getLogger('suds').setLevel(logging.WARNING)
log = logging.getLogger('billy')


url = 'http://webservices.legis.ga.gov/GGAServices/%s/Service.svc?wsdl'


def get_client(service):
    client = backoff(Client, get_url(service))
    return client


def get_url(service):
    return url % (service)


def backoff(function, *args, **kwargs):
    retries = 5
    nice = 0

    def _():
        time.sleep(1)  # Seems like their server can't handle the load.
        return function(*args, **kwargs)

    for attempt in range(retries):
        try:
            return _()
        except (socket.timeout, urllib2.URLError, suds.WebFault) as e:
            if "This Roll Call Vote is not published." in e.message:
                raise ValueError("Roll Call Vote isn't published")

            backoff = ((attempt + 1) * 15)
            log.warning(
                "[attempt %s]: Connection broke. Backing off for %s seconds." % (
                    attempt,
                    backoff
                )
            )
            log.info(str(e))
            time.sleep(backoff)

    raise ValueError(
        "The server's not playing nice. We can't keep slamming it."
    )

########NEW FILE########
__FILENAME__ = bills
import datetime as dt
import lxml.html
import re

from .utils import get_short_codes
from urlparse import urlparse

from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

HI_URL_BASE = "http://capitol.hawaii.gov"
SHORT_CODES = "%s/committees/committees.aspx?chamber=all" % (HI_URL_BASE)

def create_bill_report_url( chamber, year, bill_type ):
    cname = { "upper" : "s", "lower" : "h" }[chamber]
    bill_slug = {
        "bill" : "intro%sb" % ( cname ),
        "cr"   : "%sCR" % ( cname.upper() ),
        "r"    : "%sR"  % ( cname.upper() )
    }

    return HI_URL_BASE + "/report.aspx?type=" + bill_slug[bill_type] + \
        "&year=" + year

def categorize_action(action):
    classifiers = (
        ('Pass(ed)? First Reading', 'bill:reading:1'),
        ('Introduced and Pass(ed)? First Reading',
             ['bill:introduced', 'bill:reading:1']),
        ('Introduced', 'bill:introduced'),
        #('The committee\(s\) recommends that the measure be deferred', ?
        ('Re(-re)?ferred to ', 'committee:referred'),
        ('Passed Second Reading .* referred to the committee',
         ['bill:reading:2', 'committee:referred']),
        ('.* that the measure be PASSED', 'committee:passed:favorable'),
        ('Received from (House|Senate)', 'bill:introduced'),
        ('Floor amendment .* offered', 'amendment:introduced'),
        ('Floor amendment adopted', 'amendment:passed'),
        ('Floor amendment failed', 'amendment:failed'),
        ('.*Passed Third Reading', 'bill:passed'),
        ('Enrolled to Governor', 'governor:received'),
        ('Act ', 'governor:signed'),
        # these are for resolutions
        ('Offered', 'bill:introduced'),
        ('Adopted', 'bill:passed'),
    )
    ctty = None
    for pattern, types in classifiers:
        if re.match(pattern, action):
            if "committee:referred" in types:
                ctty = re.findall(r'\w+', re.sub(pattern, "", action))
            return (types, ctty)
    # return other by default
    return ('other', ctty)

def split_specific_votes(voters):
    if voters is None or voters.startswith('none'):
        return []
    elif voters.startswith('Senator(s)'):
        voters = voters.replace('Senator(s) ', '')
    elif voters.startswith('Representative(s)'):
        voters = voters.replace('Representative(s)', '')
    return voters.split(', ')

class HIBillScraper(BillScraper):

    jurisdiction = 'hi'

    def parse_bill_metainf_table( self, metainf_table ):
        def _sponsor_interceptor(line):
            return [ guy.strip() for guy in line.split(",") ]

        interceptors = {
            "Introducer(s)" : _sponsor_interceptor
        }

        ret = {}
        for tr in metainf_table:
            row = tr.xpath( "td" )
            key   = row[0].text_content().strip()
            value = row[1].text_content().strip()
            if key[-1:] == ":":
                key = key[:-1]
            if key in interceptors:
                value = interceptors[key](value)
            ret[key] = value
        return ret

    def parse_bill_actions_table(self, bill, action_table):
        for action in action_table.xpath('*')[1:]:
            date   = action[0].text_content()
            date   = dt.datetime.strptime(date, "%m/%d/%Y")
            actor  = action[1].text_content()
            string = action[2].text_content()
            actor = {
                "S" : "upper",
                "H" : "lower",
                "D" : "Data Systems",
                "$" : "Appropriation measure",
                "ConAm" : "Constitutional Amendment"
            }[actor]
            act_type, committees = categorize_action(string)
            # XXX: Translate short-code to full committee name for the
            #      matcher.

            real_committees = []

            if committees:
                for committee in committees:
                    try:
                        committee = self.short_ids[committee]['name']
                        real_committees.append(committee)
                    except KeyError:
                        pass

            bill.add_action(actor, string, date,
                            type=act_type, committees=real_committees)

            vote = self.parse_vote(string)
            if vote:
                v, motion = vote
                vote = Vote(actor, date, motion, 'passed' in string.lower(),
                    int( v['n_yes'] or 0 ),
                    int( v['n_no'] or 0 ),
                    int( v['n_excused'] or 0))

                def _add_votes( attrib, v, vote ):
                    for voter in split_specific_votes(v):
                        getattr(vote, attrib)(voter)

                _add_votes('yes',   v['yes'],      vote)
                _add_votes('yes',   v['yes_resv'], vote)
                _add_votes('no',    v['no'],       vote)
                _add_votes('other', v['excused'],  vote)

                bill.add_vote(vote)

    def parse_bill_versions_table(self, bill, versions):
        versions = versions.xpath("./*")
        if len(versions) > 1:
            versions = versions[1:]

        if versions == []:
            raise Exception("Missing bill versions.")

        for version in versions:
            tds = version.xpath("./*")
            if 'No other versions' in tds[0].text_content():
                return

            http_href = tds[0].xpath("./a")
            name = http_href[0].text_content().strip()
            # category  = tds[1].text_content().strip()
            pdf_href  = tds[1].xpath("./a")

            http_link = http_href[0].attrib['href']
            pdf_link  = pdf_href[0].attrib['href']

            bill.add_version(name, http_link, mimetype="text/html")
            bill.add_version(name, pdf_link, mimetype="application/pdf")

    def scrape_bill(self, session, chamber, bill_type, url):
        bill_html = self.urlopen(url)
        bill_page = lxml.html.fromstring(bill_html)
        scraped_bill_id = bill_page.xpath(
            "//a[contains(@id, 'LinkButtonMeasure')]")[0].text_content()
        bill_id = scraped_bill_id.split(' ')[0]
        versions = bill_page.xpath( "//table[contains(@id, 'GridViewVersions')]" )[0]

        tables = bill_page.xpath("//table")
        metainf_table = bill_page.xpath('//div[contains(@id, "itemPlaceholder")]//table[1]')[0]
        action_table  = bill_page.xpath('//div[contains(@id, "UpdatePanel1")]//table[1]')[0]

        meta  = self.parse_bill_metainf_table(metainf_table)

        subs = [ s.strip() for s in meta['Report Title'].split(";") ]
        if "" in subs:
            subs.remove("")

        b = Bill(session, chamber, bill_id, title=meta['Measure Title'],
                 summary=meta['Description'],
                 referral=meta['Current Referral'],
                 subjects=subs,
                 type=bill_type)
        b.add_source(url)

        companion = meta['Companion'].strip()
        if companion:
            b['companion'] = companion

        for sponsor in meta['Introducer(s)']:
            b.add_sponsor(type='primary', name=sponsor)

        actions  = self.parse_bill_actions_table(b, action_table)
        versions = self.parse_bill_versions_table(b, versions)

        self.save_bill(b)

    def parse_vote(self, action):
        pattern = (r"(?P<n_yes>\d+) Aye\(?s\)?(:\s+(?P<yes>.*?))?;\s+"
                   "Aye\(?s\)? with reservations:\s+(?P<yes_resv>.*?);?"
                   "(?P<n_no>\d*) No\(?es\)?:\s+(?P<no>.*?);?"
                   "(\s+and\s+)?"
                   "(?P<n_excused>\d*) Excused: (?P<excused>.*)")
        result = re.search(pattern, action)
        if result is None:
            return None
        result = result.groupdict()
        motion = action.split('.')[0] + '.'
        return result, motion

    def scrape_type(self, chamber, session, billtype):
        session_urlslug = \
            self.metadata['session_details'][session]['_scraped_name']
        report_page_url = create_bill_report_url(chamber, session_urlslug,
                                                 billtype)
        billy_billtype = {
            "bill" : "bill",
            "cr"   : "concurrent resolution",
            "r"    : "resolution"
        }[billtype]

        list_html = self.urlopen(report_page_url)
        list_page = lxml.html.fromstring(list_html)
        for bill_url in list_page.xpath("//a[@class='report']"):
            bill_url = HI_URL_BASE + bill_url.attrib['href']
            self.scrape_bill(session, chamber, billy_billtype, bill_url)


    def scrape(self, session, chamber):
        get_short_codes(self)
        bill_types = ["bill", "cr", "r"]
        for typ in bill_types:
            self.scrape_type(session, chamber, typ)

########NEW FILE########
__FILENAME__ = events
import datetime as dt

from billy.scrape import NoDataForPeriod
from billy.scrape.events import Event, EventScraper

from .utils import get_short_codes
from requests import HTTPError

import lxml.html
import pytz


URL = "http://www.capitol.hawaii.gov/upcominghearings.aspx"


class HIEventScraper(EventScraper):
    jurisdiction = 'hi'

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def get_related_bills(self, href):
        ret = []

        try:
            page = self.lxmlize(href)
        except HTTPError:
            return ret

        bills = page.xpath(".//a[contains(@href, 'Bills')]")
        for bill in bills:
            try:
                row = bill.iterancestors(tag='tr').next()
            except StopIteration:
                continue
            tds = row.xpath("./td")
            descr = tds[1].text_content()
            ret.append({"bill_id": bill.text_content(),
                        "type": "consideration",
                        "descr": descr})

        return ret

    def scrape(self, session, chambers):
        get_short_codes(self)

        page = self.lxmlize(URL)
        table = page.xpath(
            "//table[@id='ctl00_ContentPlaceHolderCol1_GridView1']")[0]

        for event in table.xpath(".//tr")[1:]:
            tds = event.xpath("./td")
            committee = tds[0].text_content().strip()
            bills = [x.text_content() for x in tds[1].xpath(".//a")]
            descr = [x.text_content() for x in tds[1].xpath(".//span")]
            if len(descr) != 1:
                raise Exception
            descr = descr[0]
            when = tds[2].text_content().strip()
            where = tds[3].text_content().strip()
            notice = tds[4].xpath(".//a")[0]
            notice_href = notice.attrib['href']
            notice_name = notice.text
            when = dt.datetime.strptime(when, "%m/%d/%Y %I:%M %p")

            event = Event(session, when, 'committee:meeting', descr,
                          location=where)

            if "/" in committee:
                committees = committee.split("/")
            else:
                committees = [committee,]

            for committee in committees:
                if "INFO" not in committee:
                    committee = self.short_ids[committee]
                else:
                    committee = {
                        "chamber": "joint",
                        "name": committee,
                    }

                event.add_participant('host', committee['name'], 'committee',
                                      chamber=committee['chamber'])

            event.add_source(URL)
            event.add_document(notice_name,
                               notice_href,
                               mimetype='text/html')

            for bill in self.get_related_bills(notice_href):
                event.add_related_bill(
                    bill['bill_id'],
                    description=bill['descr'],
                    type=bill['type']
                )

            self.save_event(event)

########NEW FILE########
__FILENAME__ = legislators
from billy.scrape import ScrapeError, NoDataForPeriod
from billy.scrape.legislators import LegislatorScraper, Legislator
from billy.scrape.committees  import Committee

import lxml.html
import re, contextlib


HI_BASE_URL = "http://capitol.hawaii.gov"


def get_legislator_listing_url(chamber):
    chamber = {"lower": "H",
               "upper": "S"}[chamber]

    return "%s/members/legislators.aspx?chamber=%s" % (HI_BASE_URL, chamber)


class HILegislatorScraper(LegislatorScraper):
    jurisdiction = 'hi'

    def get_page( self, url ):
        html = self.urlopen(url)
        page = lxml.html.fromstring(html)
        return ( page, html )

    def scrape_homepage( self, url ):
        page, html = self.get_page( url )
        ret = { "source" : url, 'ctty' : [] }

        table = page.xpath(
            "//table[@id='ctl00_ContentPlaceHolderCol1_GridViewMemberof']")
        if len(table) > 0:
            table = table[0]
        else:
            table = None

        chamber = page.xpath("//span[contains(@id, 'LabelChamber')]")
        if chamber == []:
            raise Exception("Can't find the chamber label")

        chamber = chamber[0].text_content()
        ret['chamber'] = chamber

        if table:
            cttys = table.xpath( "./tr/td/a" )
            for ctty in cttys:
                ret['ctty'].append({
                    "name" : ctty.text,
                    "page" : "%s/%s" % (HI_BASE_URL, ctty.attrib['href']),
                })
        return ret

    def scrape_leg_page( self, url ):
        page, html = self.get_page(url)
        people = page.xpath( \
            "//table[@id='ctl00_ContentPlaceHolderCol1_GridView1']")[0]
        people = people.xpath('./tr')[1:]
        display_order = {
            "image"    : 0,
            "contact"  : 1,
            "district" : 2
        }

        ret = []

        for person in people:
            image    = person[display_order["image"]]
            contact  = person[display_order["contact"]]
            district = person[display_order["district"]]
            metainf  = self.scrape_contact_info( contact )
            district = self.scrape_district_info( district )
            homepage = self.scrape_homepage( metainf['homepage'] )

            image = "%s/%s" % (
                HI_BASE_URL,
                image.xpath("./*/img")[0].attrib['src']
            )

            pmeta = {
                "image"    : image,
                "source"   : [ url ],
                "district" : district,
                "chamber": None
            }

            if homepage != None:
                pmeta['source'].append(homepage['source'])
                pmeta['ctty'] = homepage['ctty']
                pmeta['chamber'] = homepage['chamber']

            if pmeta['chamber'] is None:
                raise Exception("No chamber found.")

            for meta in metainf:
                pmeta[meta] = metainf[meta]

            ret.append(pmeta)
        return ret

    def br_split( self, contact ):
        cel = []
        els = [ cel ]

        # krufty HTML requires stupid hacks
        elements = contact.xpath("./*")
        for element in elements:
            if element.tag == "br":
                cel = []
                els.append(cel)
            else:
                cel.append( element )
        return els

    def scrape_district_info( self, district ):
        return district[2].text_content()

    def scrape_contact_info( self, contact ):
        homepage = "%s/%s" % ( # XXX: Dispatch a read on this page
            HI_BASE_URL,
            contact.xpath("./a")[0].attrib['href']
        )

        els = self.br_split( contact )

        def _scrape_title( els ):
            return els[0].text_content()

        def _scrape_name( els ):
            lName = els[0].text_content()
            fName = els[2].text_content()
            return "%s %s" % ( fName, lName )

        def _scrape_party( els ):
            party = {
                "(D)" : "Democratic",
                "(R)" : "Republican"
            }

            try:
                return party[els[4].text_content()]
            except KeyError:
                return "Other"

        def _scrape_addr( els ):
            room_number = els[1].text_content()
            slug        = els[0].text_content()
            return "%s %s" % ( slug, room_number )

        def _scrape_room( els ):
            return els[1].text_content()

        def _scrape_phone( els ):
            return els[1].text_content()

        def _scrape_fax( els ):
            return els[1].text_content()

        def _scrape_email( els ):
            return els[1].text_content()

        contact_entries = {
            "title" : ( 0, _scrape_title ),
            "name"  : ( 1, _scrape_name ),
            "party" : ( 1, _scrape_party ),
            "addr"  : ( 2, _scrape_addr ),
            "room"  : ( 2, _scrape_room ),
            "phone" : ( 3, _scrape_phone ),
            "fax"   : ( 4, _scrape_fax ),
            "email" : ( 5, _scrape_email )
        }

        ret = {
            "homepage" : homepage
        }

        for entry in contact_entries:
            index, callback = contact_entries[entry]
            ret[entry] = callback( els[index] )
        return ret

    def scrape(self, chamber, session):
        metainf = self.scrape_leg_page(get_legislator_listing_url(chamber))
        for leg in metainf:
            chamber = {"House": "lower",
                       "Senate": "upper"}[leg['chamber']]
            p = Legislator( session, chamber, leg['district'], leg['name'],
                party=leg['party'],
                # some additional things the website provides:
                photo_url=leg['image'],
                url=leg['homepage'],
                email=leg['email'])
            p.add_office('capitol', 'Capitol Office', address=leg['addr'],
                         phone=leg['phone'], fax=leg['fax'] or None)

            for source in leg['source']:
                p.add_source( source )

            try:
                for ctty in leg['ctty']:
                    flag='Joint Legislative'
                    if ctty['name'][:len(flag)] == flag:
                        ctty_chamber = "joint"
                    else:
                        ctty_chamber = chamber

                    p.add_role( 'committee member',
                        term=session,
                        chamber=ctty_chamber,
                        committee=ctty['name'],
                        position="member")
            except KeyError:
                self.log( "XXX: Warning, %s has no scraped Commities" %
                    leg['name'] )

            self.save_legislator( p )

########NEW FILE########
__FILENAME__ = utils
import lxml

HI_URL_BASE = "http://capitol.hawaii.gov"
SHORT_CODES = "%s/committees/committees.aspx?chamber=all" % (HI_URL_BASE)


def get_short_codes(scraper):
    list_html = scraper.urlopen(SHORT_CODES)
    list_page = lxml.html.fromstring(list_html)
    rows = list_page.xpath(
        "//table[@id='ctl00_ContentPlaceHolderCol1_GridView1']/tr")
    scraper.short_ids = {
        "CONF": {
            "chamber": "joint",
            "name": "Conference Committee",
        },
    }

    for row in rows:
        tds = row.xpath("./td")
        short = tds[0]
        clong = tds[1]
        chamber = clong.xpath("./span")[0].text_content()
        clong = clong.xpath("./a")[0]
        short_id = short.text_content().strip()
        ctty_name = clong.text_content().strip()
        chamber = "joint"
        if "house" in chamber.lower():
            chamber = 'lower'
        elif "senate" in chamber.lower():
            chamber = 'upper'

        scraper.short_ids[short_id] = {
            "chamber": chamber,
            "name": ctty_name
        }

########NEW FILE########
__FILENAME__ = bills
import re
import datetime
from collections import defaultdict

from billy.scrape.bills import BillScraper, Bill

import lxml.html

from .scraper import InvalidHTTPSScraper


def get_popup_url(link):
    onclick = link.attrib['onclick']
    return re.match(r'openWin\("(.*)"\)$', onclick).group(1)


class IABillScraper(InvalidHTTPSScraper, BillScraper):
    jurisdiction = 'ia'

    _subjects = defaultdict(list)

    def _build_subject_map(self, session):
        # if already built a subject map, skip doing it again
        if self._subjects:
            return

        session_id = self.metadata['session_details'][session]['number']
        url = ('http://coolice.legis.state.ia.us/Cool-ICE/default.asp?'
               'Category=BillInfo&Service=DspGASI&ga=%s&frame=y') % session_id
        doc = lxml.html.fromstring(self.urlopen(url))

        # get all subjects from dropdown
        for option in doc.xpath('//select[@name="SelectOrig"]/option')[1:]:
            # if it has a "See also" strip that part off
            subject = option.text.strip().split(' - See also')[0]
            # skip sub-subjects
            if subject.startswith('--'):
                continue

            value = option.get('value')

            # open the subject url and get all bill_ids
            subj_url = ('http://coolice.legis.state.ia.us/Cool-ICE/default.asp'
                        '?Category=BillInfo&Service=DsplData&var=gasi&ga=%s&'
                        'typ=o&key=%s') % (session_id, value)
            subj_doc = lxml.html.fromstring(self.urlopen(subj_url))
            bill_ids = subj_doc.xpath('//td[@width="10%"]/a/text()')
            for bill_id in bill_ids:
                self._subjects[bill_id.replace(' ', '')].append(subject)

    def scrape(self, chamber, session):

        self._build_subject_map(session)

        session_id = self.metadata['session_details'][session]['number']
        bill_offset = "697"  # Try both. We need a good bill page to scrape
        bill_offset = "27"   # from. Check for "HF " + bill_offset

        url = ("http://coolice.legis.state.ia.us/Cool-ICE/default.asp?"
               "category=billinfo&service=Billbook&frm=2&hbill=HF%s%20"
               "%20%20%20&cham=House&amend=%20%20%20%20%20%20&am2nd=%20"
               "%20%20%20%20%20&am3rd=%20%20%20%20%20%20&version=red;"
               "%20%20%20%20&menu=true&ga=" % (bill_offset)) + session_id
        page = lxml.html.fromstring(self.urlopen(url))
        page.make_links_absolute(url)

        if chamber == 'upper':
            bname = 'sbill'
        else:
            bname = 'hbill'

        for option in page.xpath("//select[@name = '%s']/option" % bname):
            bill_id = option.text.strip()
            if bill_id == 'Pick One':
                continue

            bill_url = option.attrib['value'].strip() + '&frm=2'

            self.scrape_bill(chamber, session, bill_id, bill_url)

    def scrape_bill(self, chamber, session, bill_id, url):
        sidebar = lxml.html.fromstring(self.urlopen(url))

        try:
            hist_url = get_popup_url(
                sidebar.xpath("//a[contains(., 'Bill History')]")[0])
        except IndexError:
            # where is it?
            return

        page = lxml.html.fromstring(self.urlopen(hist_url))
        page.make_links_absolute(hist_url)

        title = page.xpath("string(//table[2]/tr[4])").strip()
        if title == '':
            self.warning("URL: %s gives us an *EMPTY* bill. Aborting." % url)
            return

        if title.lower().startswith("in"):
            title = page.xpath("string(//table[2]/tr[3])").strip()

        if 'HR' in bill_id or 'SR' in bill_id:
            bill_type = ['resolution']
        elif 'HJR' in bill_id or 'SJR' in bill_id:
            bill_type = ['joint resolution']
        elif 'HCR' in bill_id or 'SCR' in bill_id:
            bill_type = ['concurrent resolution']
        else:
            bill_type = ['bill']

        bill = Bill(session, chamber, bill_id, title, type=bill_type)
        bill.add_source(hist_url)

        # get pieces of version_link
        vpieces = sidebar.xpath('//a[contains(string(.), "HTML")]/@href')
        if vpieces:
            version_base, version_type, version_end = vpieces[0].rsplit('/', 2)
            versions = [o.strip() for o in
                        sidebar.xpath("//select[@name='BVer']/option/text()")]
            # if there are no options, put version_type in one
            if not versions:
                versions = [version_type]

            for version_name in versions:
                version_url = '/'.join((version_base, version_name,
                                        version_end))
                bill.add_version(version_name, version_url,
                                 mimetype='text/html')
        else:
            bill.add_version('Introduced',
                sidebar.xpath('//a[contains(string(.), "PDF")]/@href')[0],
                             mimetype='application/pdf'
                            )

        sponsors = page.xpath("string(//table[2]/tr[3])").strip()
        sponsor_re = r'[\w-]+(?:, [A-Z]\.)?(?:,|(?: and)|\.$)'
        for sponsor in re.findall(sponsor_re, sponsors):
            sponsor = sponsor.replace(' and', '').strip(' .,')

            # a few sponsors get mangled by our regex
            sponsor = {
                'Means': 'Ways & Means',
                'Iowa': 'Economic Growth/Rebuild Iowa',
                'Safety': 'Public Safety',
                'Resources': 'Human Resources',
                'Affairs': 'Veterans Affairs',
                'Protection': 'Environmental Protection',
                'Government': 'State Government',
                'Boef': 'De Boef'}.get(sponsor, sponsor)

            bill.add_sponsor('primary', sponsor)

        for tr in page.xpath("//table[3]/tr"):
            date = tr.xpath("string(td[contains(text(), ', 20')])").strip()
            if date.startswith("***"):
                continue
            elif "No history is recorded at this time." in date:
                return
            if date == "":
                continue

            date = datetime.datetime.strptime(date, "%B %d, %Y").date()

            action = tr.xpath("string(td[2])").strip()
            action = re.sub(r'\s+', ' ', action)

            # Capture any amendment links.
            version_urls = set(version['url'] for version in bill['versions'])
            if 'amendment' in action.lower():
                for anchor in tr.xpath('td[2]/a'):
                    if '-' in anchor.text:
                        url = anchor.attrib['href']
                        if url not in version_urls:
                            bill.add_version(anchor.text, url, mimetype='text/html')
                            version_urls.add(url)

            if 'S.J.' in action or 'SCS' in action:
                actor = 'upper'
            elif 'H.J.' in action or 'HCS' in action:
                actor = 'lower'
            else:
                actor = "other"

            action = re.sub(r'(H|S)\.J\.\s+\d+\.$', '', action).strip()

            if action.startswith('Introduced'):
                atype = ['bill:introduced']
                if ', referred to' in action:
                    atype.append('committee:referred')
            elif action.startswith('Read first time'):
                atype = 'bill:reading:1'
            elif action.startswith('Referred to'):
                atype = 'committee:referred'
            elif action.startswith('Sent to Governor'):
                atype = 'governor:received'
            elif action.startswith('Reported Signed by Governor'):
                atype = 'governor:signed'
            elif action.startswith('Signed by Governor'):
                atype = 'governor:signed'
            elif action.startswith('Vetoed by Governor'):
                atype = 'governor:vetoed'
            elif action.startswith('Item veto'):
                atype = 'governor:vetoed:line-item'
            elif re.match(r'Passed (House|Senate)', action):
                atype = 'bill:passed'
            elif re.match(r'Amendment (S|H)-\d+ filed', action):
                atype = ['amendment:introduced']
                if ', adopted' in action:
                    atype.append('amendment:passed')
            elif re.match(r'Amendment (S|H)-\d+( as amended,)? adopted',
                          action):
                atype = 'amendment:passed'
            elif re.match('Amendment (S|N)-\d+ lost', action):
                atype = 'amendment:failed'
            elif action.startswith('Resolution filed'):
                atype = 'bill:introduced'
            elif action.startswith('Resolution adopted'):
                atype = 'bill:passed'
            elif (action.startswith('Committee report') and
                  action.endswith('passage.')):
                  atype = 'committee:passed'
            elif action.startswith('Withdrawn'):
                atype = 'bill:withdrawn'
            else:
                atype = 'other'

            if action.strip() == "":
                continue

            bill.add_action(actor, action, date, type=atype)

        bill['subjects'] = self._subjects[bill_id]
        self.save_bill(bill)

########NEW FILE########
__FILENAME__ = events
import re
import datetime

from billy.scrape.events import EventScraper, Event
from .scraper import InvalidHTTPSScraper

import lxml.html


class IAEventScraper(InvalidHTTPSScraper, EventScraper):
    jurisdiction = 'ia'

    def scrape(self, chamber, session):
        if chamber == 'other':
            return

        today = datetime.date.today()
        start_date = today - datetime.timedelta(days=10)
        end_date = today + datetime.timedelta(days=10)

        if chamber == 'upper':
            chamber_abbrev = 'S'
        else:
            chamber_abbrev = 'H'

        url = ("http://www.legis.iowa.gov/Schedules/meetingsList"
               "Chamber.aspx?chamber=%s&bDate=%02d/%02d/"
               "%d&eDate=%02d/%02d/%d" % (chamber_abbrev,
                                          start_date.month,
                                          start_date.day,
                                          start_date.year,
                                          end_date.month,
                                          end_date.day,
                                          end_date.year))

        page = lxml.html.fromstring(self.urlopen(url))
        page.make_links_absolute(url)
        for link in page.xpath("//a[contains(@id, 'linkCommittee')]"):
            comm = link.text.strip()
            desc = comm + " Committee Hearing"
            location = link.xpath("string(../../td[3])")

            when = link.xpath("string(../../td[1])").strip()
            if 'cancelled' in when.lower() or "upon" in when.lower():
                continue
            if "To Be Determined" in when:
                continue

            if 'AM' in when:
                when = when.split('AM')[0] + " AM"
            else:
                when = when.split('PM')[0] + " PM"

            junk = ['Reception']
            for key in junk:
                when = when.replace(key, '')

            when = re.sub("\s+", " ", when).strip()
            if "tbd" in when.lower():
                # OK. This is a partial date of some sort.
                when = datetime.datetime.strptime(
                    when,
                    "%m/%d/%Y TIME - TBD %p"
                )
            else:
                try:
                    when = datetime.datetime.strptime(when, "%m/%d/%Y %I:%M %p")
                except ValueError:
                    when = datetime.datetime.strptime(when, "%m/%d/%Y %I %p")

            event = Event(session, when, 'committee:meeting',
                          desc, location)
            event.add_source(url)
            event.add_participant('host', comm, 'committee', chamber=chamber)
            self.save_event(event)

########NEW FILE########
__FILENAME__ = legislators
import re

from billy.scrape import NoDataForPeriod
from billy.scrape.legislators import LegislatorScraper, Legislator
from .scraper import InvalidHTTPSScraper

import lxml.html


class IALegislatorScraper(InvalidHTTPSScraper, LegislatorScraper):
    jurisdiction = 'ia'

    def scrape(self, chamber, term):
        self.validate_term(term, latest_only=True)
        session_id = self.metadata['session_details'][term]['number']

        if chamber == 'upper':
            chamber_name = 'senate'
        else:
            chamber_name = 'house'

        url = "https://www.legis.iowa.gov/legislators/%s" % chamber_name
        page = lxml.html.fromstring(self.urlopen(url))
        page.make_links_absolute(url)
        table = page.xpath('//table[@id="sortableTable"]')[0]
        for link in table.xpath(".//a[contains(@href, 'legislator')]"):
            name = link.text.strip()
            leg_url = link.get('href')
            district = link.xpath("string(../../td[3])")
            party = link.xpath("string(../../td[4])")
            email = link.xpath("string(../../td[5])")

            if party == 'Democrat':
                party = 'Democratic'

            pid = re.search("personID=(\d+)", link.attrib['href']).group(1)
            photo_url = ("http://www.legis.iowa.gov/getPhotoPeople.aspx"
                         "?GA=%s&PID=%s" % (session_id, pid))

            leg = Legislator(term, chamber, district, name, party=party,
                             photo_url=photo_url, url=url)
            leg.add_source(url)

            leg_page = lxml.html.fromstring(self.urlopen(link.attrib['href']))

            office_data = {
                "email": "ctl00_cphMainContent_divEmailLegis",
                "home_phone": "ctl00_cphMainContent_divPhoneHome",
                "home_addr": "ctl00_cphMainContent_divAddrHome",
                "office_phone": "ctl00_cphMainContent_divPhoneCapitol",
            }
            metainf = {}

            for attr in office_data:
                path = office_data[attr]
                info = leg_page.xpath("//div[@id='%s']" % path)
                if len(info) != 1:
                    continue
                info = info[0]

                _, data = [x.text_content() for x in info.xpath("./span")]
                data = data.strip()
                if data == "":
                    continue

                metainf[attr] = data

            if "home_phone" in metainf or "home_addr" in metainf:
                home_args = {}
                if "home_phone" in metainf:
                    home_args['phone'] = metainf['home_phone']
                if "home_addr" in metainf:
                    home_args['address'] = metainf['home_addr']
                leg.add_office('district',
                               'Home Office',
                               **home_args)

            if "email" in metainf or "office_phone" in metainf:
                cap_args = {}

                if "email" in metainf:
                    cap_args['email'] = metainf['email']
                if "office_phone" in metainf:
                    cap_args['phone'] = metainf['office_phone']

                leg.add_office('capitol',
                               'Capitol Office',
                               **cap_args)


            comm_path = "//a[contains(@href, 'committee')]"
            for comm_link in leg_page.xpath(comm_path):
                comm = comm_link.text.strip()

                match = re.search(r'\((.+)\)$', comm)
                if match:
                    comm = re.sub(r'\((.+)\)$', '', comm).strip()
                    mtype = match.group(1).lower()
                else:
                    mtype = 'member'

                if comm.endswith('Appropriations Subcommittee'):
                    sub = re.match('^(.+) Appropriations Subcommittee$',
                                   comm).group(1)
                    leg.add_role('committee member', term, chamber=chamber,
                                 committee='Appropriations',
                                 subcommittee=sub,
                                 position=mtype)
                else:
                    leg.add_role('committee member', term, chamber=chamber,
                                 committee=comm,
                                 position=mtype)

            self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = scraper
from scrapelib import Scraper

class InvalidHTTPSScraper(Scraper):
    def request(self, *args, **kwargs):
        return super(InvalidHTTPSScraper, self).request(
            *args, verify=False, **kwargs)

########NEW FILE########
__FILENAME__ = votes
# -*- coding: utf8 -*-
from datetime import datetime
import re
import collections

import lxml.etree

from billy.scrape.utils import convert_pdf
from billy.scrape.votes import VoteScraper, Vote
from .scraper import InvalidHTTPSScraper


class IAVoteScraper(InvalidHTTPSScraper, VoteScraper):
    jurisdiction = 'ia'

    def scrape(self, chamber, session):

        getattr(self, 'scrape_%s' % chamber)(session)

    def scrape_lower(self, session):
        url = 'https://www.legis.iowa.gov/Legislation/journalIndex_House.aspx'
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        urls = doc.xpath('//a[contains(@href, "DOCS")]/@href')[::-1]
        for url in urls:
            _, filename = url.rsplit('/', 1)
            try:
                date = datetime.strptime(filename, '%m-%d-%Y.pdf')
            except ValueError:
                msg = "%s doesn't smell like a date. Skipping."
                self.logger.info(msg % filename)
            self.scrape_journal(url, 'lower', session, date)

    def scrape_upper(self, session):
        url = 'https://www.legis.iowa.gov/Legislation/journalIndex_Senate.aspx'
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        urls = doc.xpath('//a[contains(@href, "DOCS")]/@href')[::-1]
        for url in urls:
            _, filename = url.rsplit('/', 1)
            try:
                date = datetime.strptime(filename, '%m-%d-%Y.pdf')
            except ValueError:
                msg = "%s doesn't smell like a date. Skipping."
                self.logger.info(msg % filename)
            self.scrape_journal(url, 'upper', session, date)

    def _journal_lines(self, etree):
        '''A generator of text lines. Skip crap.
        '''
        for page in etree:
            for text in page.xpath('text')[3:]:
                yield text

    def scrape_journal(self, url, chamber, session, date):

        filename, response = self.urlretrieve(url)
        self.logger.info('Saved journal to %r' % filename)
        xml = convert_pdf(filename)
        try:
            et = lxml.etree.fromstring(xml)
        except lxml.etree.XMLSyntaxError:
            self.logger.warning('Skipping invalid pdf: %r' % filename)
            return

        lines = self._journal_lines(et)
        while True:
            try:
                line = next(lines)
            except StopIteration:
                break

            text = gettext(line)

            # Go through with vote parse if any of
            # these conditions match.
            if 'Shall' in text:
                if 'bill pass?' in text:
                    pass
                elif 'resolution' in text:
                    pass
                elif 'amendment' in text:
                    pass
                else:
                    continue
            else:
                continue

            # Get the bill_id.
            bill_id = None
            for line in lines:
                text += gettext(line)
                m = re.search(r'\(\s*([A-Z\.]+\s+\d+)\s*\)',  text)
                if m:
                    bill_id = m.group(1)
                    break

            motion = text.strip()
            motion = re.sub(r'\s+', ' ', motion)
            if "(" in motion:
                motion, _ = motion.rsplit('(', 1)
            motion = motion.replace('"', '')
            motion = motion.replace(u'“', '')
            motion = motion.replace(u'\u201d', '')
            motion = motion.replace(u' ,', ',')
            motion = motion.strip()
            motion = re.sub(r'[SH].\d+', lambda m: ' %s ' % m.group(), motion)
            motion = re.sub(r'On the question\s*', '', motion, flags=re.I)

            for word, letter in (('Senate', 'S'),
                                 ('House', 'H'),
                                 ('File', 'F')):

                if bill_id is None:
                    return

                bill_id = bill_id.replace(word, letter)

            bill_chamber = dict(h='lower', s='upper')[bill_id.lower()[0]]
            self.current_id = bill_id
            votes = self.parse_votes(lines)
            totals = filter(lambda x: isinstance(x, int), votes.values())
            passed = (1.0 * votes['yes_count'] / sum(totals)) >= 0.5
            vote = Vote(motion=motion,
                        passed=passed,
                        chamber=chamber, date=date,
                        session=session, bill_id=bill_id,
                        bill_chamber=bill_chamber,
                        **votes)
            vote.update(votes)
            vote.add_source(url)
            self.save_vote(vote)

    def parse_votes(self, lines):

        counts = collections.defaultdict(list)
        DONE = 1
        boundaries = [

            # Senate journal.
            ('Yeas', 'yes'),
            ('Nays', 'no'),
            ('Absent', 'other'),
            ('Present', 'skip'),
            ('Amendment', DONE),
            ('Resolution', DONE),
            ('Bill', DONE),

            # House journal.
            ('The ayes were', 'yes'),
            ('The yeas were', 'yes'),
            ('The nays were', 'no'),
            ('Absent or not voting', 'other'),
            ('The bill', DONE),
            ('The committee', DONE),
            ('The resolution', DONE),
            ('The motion', DONE),
            ('The joint resolution', DONE),
            ('Under the', DONE)]

        def is_boundary(text, patterns={}):
            for blurb, key in boundaries:
                if text.strip().startswith(blurb):
                    return key

        while True:
            line = next(lines)
            text = gettext(line)
            if is_boundary(text):
                break

        while True:
            key = is_boundary(text)
            if key is DONE:
                break

            # Get the vote count.
            m = re.search(r'\d+', text)
            if not m:
                if 'none' in text:
                    votecount = 0
            else:
                votecount = int(m.group())
            if key != 'skip':
                counts['%s_count' % key] = votecount

            # Get the voter names.
            while True:
                line = next(lines)
                text = gettext(line)
                if is_boundary(text):
                    break
                elif not text.strip() or text.strip().isdigit():
                    continue
                else:
                    for name in self.split_names(text):
                        counts['%s_votes' % key].append(name.strip())

        return counts

    def split_names(self, text):
        junk = ['Presiding', 'Mr. Speaker', 'Spkr.', '.']
        text = text.strip()
        chunks = text.split()[::-1]
        name = [chunks.pop()]
        names = []
        while chunks:
            chunk = chunks.pop()
            if len(chunk) < 3:
                name.append(chunk)
            elif name[-1] in ('Mr.', 'Van', 'De', 'Vander'):
                name.append(chunk)
            else:
                name = ' '.join(name).strip(',')
                if name and (name not in names) and (name not in junk):
                    names.append(name)

                # Seed the next loop.
                name = [chunk]

        # Similar changes to the final name in the sequence.
        name = ' '.join(name).strip(',')
        if names and len(name) < 3:
            names[-1] += ' %s' % name
        elif name and (name not in names) and (name not in junk):
            names.append(name)
        return names


def _get_chunks(el, buff=None, until=None):
    tagmap = {'br': '\n'}
    buff = buff or []

    # Tag, text, tail, recur...
    yield tagmap.get(el.tag, '')
    yield el.text or ''
    if el.text == until:
        return
    for kid in el:
        for text in _get_chunks(kid, until=until):
            yield text
            if text == until:
                return
    if el.tail:
        yield el.tail
        if el.tail == until:
            return
    if el.tag == 'text':
        yield '\n'


def gettext(el):
    '''Join the chunks, then split and rejoin to normalize the whitespace.
    '''
    return ' '.join(''.join(_get_chunks(el)).split())

########NEW FILE########
__FILENAME__ = bills
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote
import re
import datetime
from collections import defaultdict
import lxml.html

BILLS_URL = 'http://legislature.idaho.gov/legislation/%s/minidata.htm'
BILL_URL = 'http://legislature.idaho.gov/legislation/%s/%s.htm'

_CHAMBERS = {'upper':'Senate', 'lower':'House'}

_BILL_TYPES = {'CR':'concurrent resolution',
              'JM': 'joint memorial', 'JR': 'joint resolution',
              'P': 'proclamation', 'R': 'resolution'}
_COMMITTEES = { 'lower': {'Loc Gov':'Local Government',
                     'Jud':'Judiciary, Rules and Administration',
                     'Res/Con':'Resources and Conservation',
                     'Com/HuRes':'Commerce and Human Resources',
                     'Transp':'Transportation and Defense',
                     'St Aff': 'State Affairs',
                     'Rev/Tax':'Revenues and Taxation',
                     'Health/Wel':'Health and Welfare',
                     'Env':'Environment, Energy and Technology',
                     'Bus':'Business', 'Educ':'Education',
                     'Agric Aff':'Agricultural Affairs',
                     'Approp': 'Appropriations','W/M': 'Ways and Means'},
                'upper': {'Agric Aff': 'Agricultural Affairs',
                     'Com/HuRes':'Commerce and Human Resources',
                     'Educ': 'Education', 'Fin':'Finance',
                     'Health/Wel':'Health and Welfare',
                     'Jud': 'Judiciary and Rules',
                     'Loc Gov': 'Local Government and Taxation',
                     'Res/Env': 'Resources and Environment',
                     'St Aff': 'State Affairs', 'Transp': 'Transportation'}
                }

# a full list of the abbreviations and definitions can be found at:
# http://legislature.idaho.gov/sessioninfo/glossary.htm
# background on bill to law can be found at:
# http://legislature.idaho.gov/about/jointrules.htm
_ACTIONS = (
     # bill:reading:1
     (r'(\w+) intro - (\d)\w+ rdg - to (\w+/?\s?\w+\s?\w+)',
      lambda mch, ch: ["bill:introduced", "bill:reading:1", "committee:referred"] \
                        if mch.groups()[2] in _COMMITTEES[ch] else ["bill:introduced", "bill:reading:1"] ),
     # committee actions
     (r'rpt prt - to\s(\w+/?\s?\w+)',
      lambda mch, ch: ["committee:referred"] if mch.groups()[0] in _COMMITTEES[ch] \
                                             else "other"),
     # it is difficult to figure out which committee passed/reported out a bill
     # but i guess we at least know that only committees report out
     (r'rpt out - rec d/p', "committee:passed:favorable"),
     (r'^rpt out', 'committee:passed'),


    (r'^Reported Signed by Governor', "governor:signed"),
    (r'^Signed by Governor', "governor:signed"),

     # I dont recall seeing a 2nd rdg by itself
     (r'^1st rdg - to 2nd rdg', "bill:reading:2"),
     # second to third will count as a third read if there is no
     # explicit third reading action
     (r'2nd rdg - to 3rd rdg', "bill:reading:3"),
     (r'^3rd rdg$', "bill:reading:3"),
     (r'.*Third Time.*PASSED.*', ["bill:reading:3", "bill:passed"]),
     # bill:reading:3, bill:passed
     (r'^3rd rdg as amen - (ADOPTED|PASSED)', ["bill:reading:3", "bill:passed"]),
     (r'^3rd rdg - (ADOPTED|PASSED)', ["bill:reading:3", "bill:passed"]),
     (r'^Read Third Time in Full .* (ADOPTED|PASSED).*', [
         "bill:reading:3", "bill:passed"]),
     (r'^.*read three times - (ADOPTED|PASSED).*', [
         "bill:reading:3", "bill:passed"]),
     (r'^.*Read in full .* (ADOPTED|PASSED).*', [
         "bill:reading:3", "bill:passed"]),
     # bill:reading:3, bill:failed
     (r'^3rd rdg as amen - (FAILED)', ["bill:reading:3", "bill:failed"]),
     (r'^3rd rdg - (FAILED)', ["bill:reading:3", "bill:failed"]),
     # rules suspended
     (r'^Rls susp - (ADOPTED|PASSED|FAILED)', lambda mch, ch: {
                                                       'ADOPTED': "bill:passed",
                                                        'PASSED': "bill:passed",
                                                        'FAILED': "bill:failed"
                                                   }[mch.groups()[0]]),
     (r'^to governor', "governor:received"),
     (r'^Governor signed', "governor:signed"),
)
def get_action(actor, text):
    # the biggest issue with actions is that some lines seem to indicate more
    # than one action
    print text

    for pattern, action in _ACTIONS:
        match = re.match(pattern, text, re.I)
        if match:
            if callable(action):
                return action(match, actor)
            else:
                return action
    return "other"

def get_bill_type(bill_id):
    suffix = bill_id.split(' ')[0]
    if len(suffix) == 1:
        return 'bill'
    else:
        return _BILL_TYPES[suffix[1:]]

class IDBillScraper(BillScraper):
    jurisdiction = 'id'

    # the following are only used for parsing legislation from 2008 and earlier
    vote = None
    in_vote = False
    ayes = False
    nays = False
    other = False
    last_date = None


    def scrape_subjects(self, session):
        self._subjects = defaultdict(list)

        url = 'http://legislature.idaho.gov/legislation/%s/topicind.htm' % session
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        # loop through anchors
        anchors = doc.xpath('//td[@width="95%"]//a')
        for a in anchors:
            # if anchor has a name, that's the subject
            if a.get('name'):
                subject = a.get('name')
            # if anchor is a link to a bill, save that reference
            elif 'legislation' in a.get('href'):
                self._subjects[a.text].append(subject)


    def scrape(self, chamber, session):
        """
        Scrapes all the bills for a given session and chamber
        """

        #url = BILLS_URL % session
        if int(session[:4]) < 2009:
            self.scrape_pre_2009(chamber, session)
        else:
            self.scrape_subjects(session)
            self.scrape_post_2009(chamber, session)

    def scrape_post_2009(self, chamber, session):
        "scrapes legislation for 2009 and above"
        url = BILLS_URL % session
        bill_index = self.urlopen(url)
        html = lxml.html.fromstring(bill_index)
        # I check for rows with an id that contains 'bill' and startswith
        # 'H' or 'S' to make sure I dont get any links from the menus
        # might not be necessary
        bill_rows = html.xpath('//tr[contains(@id, "bill") and '\
                               'starts-with(descendant::td/a/text(), "%s")]'\
                               % _CHAMBERS[chamber][0])
        for row in bill_rows:
            matches = re.match(r'([A-Z]*)([0-9]+)',
                                        row[0].text_content().strip())
            bill_id = " ".join(matches.groups()).strip()
            short_title = row[1].text_content().strip()
            self.scrape_bill(chamber, session, bill_id, short_title)

    def scrape_pre_2009(self, chamber, session):
        """scrapes legislation from 2008 and below."""
        url = BILLS_URL + 'l'
        url = url % session
        bill_index = self.urlopen(url)
        html = lxml.html.fromstring(bill_index)
        html.make_links_absolute(url)
        links = html.xpath('//a')
        exprs = r'(%s[A-Z]*)([0-9]+)' % _CHAMBERS[chamber][0]
        for link in links:
            matches = re.match(exprs, link.text)
            if matches:
                bill_id = " ".join(matches.groups())
                short_title = link.tail[:link.tail.index('..')]
                self.scrape_pre_2009_bill(chamber, session, bill_id, short_title)

    def scrape_bill(self, chamber, session, bill_id, short_title=None):
        """
        Scrapes documents, actions, vote counts and votes for
        bills from the 2009 session and above.
        """
        url = BILL_URL % (session, bill_id.replace(' ', ''))
        bill_page = self.urlopen(url)
        html = lxml.html.fromstring(bill_page)
        html.make_links_absolute('http://legislature.idaho.gov/legislation/%s/' % session)
        bill_tables = html.xpath('./body/table/tr/td[2]')[0].xpath('.//table')
        title = bill_tables[1].text_content().strip()
        bill_type = get_bill_type(bill_id)
        bill = Bill(session, chamber, bill_id, title, type=bill_type)
        bill.add_source(url)
        bill['subjects'] = self._subjects[bill_id.replace(' ', '')]

        if short_title and bill['title'].lower() != short_title.lower():
            bill.add_title(short_title)

        # documents
        doc_links = html.xpath('//span/a')
        for link in doc_links:
            name = link.text_content().strip()
            href = link.get('href')
            if 'Engrossment' in name or 'Bill Text' in name:
                bill.add_version(name, href, mimetype='application/pdf')
            else:
                bill.add_document(name, href)

        def _split(string):
            return re.split(r"\w+[,|AND]\s+", string)

        # sponsors range from a committee to one legislator to a group of legs
        sponsor_lists = bill_tables[0].text_content().split('by')
        if len(sponsor_lists) > 1:
            for sponsors in sponsor_lists[1:]:
                for person in _split(sponsors):
                    person = person.strip()
                    if person != "":
                        bill.add_sponsor('primary', person)

        actor = chamber
        last_date = None
        for row in bill_tables[2]:
            # lots of empty rows
            if len(row) == 1:
                continue
            _, date, action, _ = [x.text_content().strip() for x in row]

            if date:
                last_date = date
            else:
                date = last_date

            date = datetime.datetime.strptime(date+ '/' + session[0:4],
                                              "%m/%d/%Y")
            if action.startswith('House'):
                actor = 'lower'
            elif action.startswith('Senate'):
                actor = 'upper'

            # votes
            if 'AYES' in action or 'NAYS' in action:
                vote = self.parse_vote(actor, date, row[2])
                vote.add_source(url)
                bill.add_vote(vote)
            # some td's text is seperated by br elements
            if len(row[2]):
                action = "".join(row[2].itertext())
            action = action.replace(u'\xa0', ' ').strip()
            atype = get_action(actor, action)
            bill.add_action(actor, action, date, type=atype)
            # after voice vote/roll call and some actions the bill is sent
            # 'to House' or 'to Senate'
            if 'to House' in action:
                actor = 'lower'
            elif 'to Senate' in action:
                actor = 'upper'
        self.save_bill(bill)

    def scrape_pre_2009_bill(self, chamber, session, bill_id, short_title=''):
        """bills from 2008 and below are in a 'pre' element and is simpler to
        parse them as text"""
        url = 'http://legislature.idaho.gov/legislation/%s/%s.html' % (session, bill_id.replace(' ', ''))
        bill_page = self.urlopen(url)
        html = lxml.html.fromstring(bill_page)
        text = html.xpath('//pre')[0].text.split('\r\n')

        # title
        title = " - ".join([ x.strip() for x in text[1].split('-') if x.isupper() ])
        # bill type
        bill_type = get_bill_type(bill_id)

        bill = Bill(session, chamber, bill_id, title, type=bill_type)
        # sponsors
        sponsors = text[0].split('by')[-1]
        for sponsor in sponsors.split(','):
            bill.add_sponsor('primary', sponsor)

        actor = chamber
        self.flag() # clear last bills vote flags
        self.vote = None #

        for line in text:

            if re.match(r'^\d\d/\d\d', line):
                date = date = datetime.datetime.strptime(line[0:5] + '/' + session[0:4],
                                              "%m/%d/%Y")
                self.last_date = date
                action_text = line[5:].strip()
                # actor
                if action_text.lower().startswith('house') or \
                   action_text.lower().startswith('senate'):
                    actor = {'H':'lower', 'S':'upper'}[action_text[0]]

                action = get_action(actor, action_text)
                bill.add_action(actor,action_text, date, type=action)
                if "bill:passed" in action or "bill:failed" in action:
                    passed = False if 'FAILED' in action_text else True
                    votes = re.search(r'(\d+)-(\d+)-(\d+)', action_text)
                    if votes:
                        yes, no, other = votes.groups()
                        self.in_vote = True
                        self.vote = Vote(chamber, date, action_text, passed,
                                     int(yes), int(no), int(other))
            else:
                date = self.last_date
                # nothing to do if its not a vote
                if "Floor Sponsor" in line:
                    self.in_vote = False
                    if self.vote:
                        bill.add_vote(self.vote)
                        self.vote = None

                if not self.in_vote:
                    continue
                if 'AYES --' in line:
                    self.flag(ayes=True)
                elif 'NAYS --' in line:
                    self.flag(nays=True)
                elif 'Absent and excused' in line:
                    self.flag(other=True)

                if self.ayes:
                    for name in line.replace('AYES --', '').split(','):
                        name = name.strip()
                        if name:
                            self.vote.yes(name)

                if self.nays:
                    for name in line.replace('NAYS --', '').split(','):
                        name = name.strip()
                        if name:
                            self.vote.no(name)

                if self.other:
                    for name in line.replace('Absent and excused --', '').split(','):
                        name = name.strip()
                        if name:
                            self.vote.other(name)

        self.save_bill(bill)

    def parse_vote(self, actor, date, row):
        """
        takes the actor, date and row element and returns a Vote object
        """
        spans = row.xpath('.//span')
        motion = row.text
        passed, yes_count, no_count, other_count = spans[0].text_content().split('-')
        yes_votes = [ name for name in
                      spans[1].tail.replace(u'\xa0--\xa0', '').split(',')
                      if name ] if spans[1].tail else []

        no_votes = [ name for name in
                     spans[2].tail.replace(u'\xa0--\xa0', '').split(',')
                     if name ]
        other_votes = []
        for span in spans[3:]:
            if span.text.startswith(('Absent', 'Excused')):
                other_votes += [name.strip() for name in
                                span.tail.replace(u'\xa0--\xa0', '').split(',')
                                if name]
        for key, val in {'adopted': True, 'passed': True, 'failed':False}.items():
            if key in passed.lower():
                passed = val
                break
        vote = Vote(actor, date, motion, passed, int(yes_count), int(no_count),
                    int(other_count))
        for name in yes_votes:
            if name and name != 'None':
                vote.yes(name)
        for name in no_votes:
            if name and name != 'None':
                vote.no(name)
        for name in other_votes:
            if name and name != 'None':
                vote.other(name)
        return vote

    def flag(self, ayes=False, nays=False, other=False):
        """ help to keep track of where we are at parsing votes from text"""
        self.ayes = ayes
        self.nays = nays
        self.other = other

########NEW FILE########
__FILENAME__ = committees
from billy.scrape.committees import CommitteeScraper, Committee
import lxml.html


_COMMITTEE_URL = 'http://legislature.idaho.gov/%s/committees.cfm' # house/senate
_JOINT_URL = 'http://legislature.idaho.gov/about/jointcommittees.htm'

_CHAMBERS = {'upper':'senate', 'lower':'house'}
_REV_CHAMBERS = {'senate':'upper', 'house':'lower'}
_TD_ONE = ('committee', 'description', 'office_hours', 'secretary', 'office_phone')
_TD_TWO = ('committee', 'office_hours', 'secretary', 'office_phone')


def clean_name(name):
    return name.replace(u'\xa0', ' ')


class IDCommitteeScraper(CommitteeScraper):
    """Currently only committees from the latest regular session are
    available through html. Membership for prior terms are available via the
    committee minutes .pdf files at
    http://legislature.idaho.gov/sessioninfo/2009/standingcommittees/committeeminutesindex.htm
    and the pdfs I have encountered from Idaho convert to html
    consistantly, so we could get membership and committee minutes if we really
    want."""
    jurisdiction = 'id'

    def get_jfac(self, name, url):
        """gets membership info for the Joint Finance and Appropriations
        Committee."""
        jfac_page = self.urlopen(url)
        html = lxml.html.fromstring(jfac_page)
        table = html.xpath('body/table/tr/td[2]/table')[0]
        committee = Committee('joint', name)
        for row in table.xpath('tr')[1:]:
            senate, house = row.xpath('td/strong')
            senate = senate.text.replace(u'\xa0', ' ')
            house = house.text.replace(u'\xa0', ' ')
            if ',' in senate:
                committee.add_member(*senate.split(','), chamber='upper')
            else:
                committee.add_member(senate, chamber='upper')
            if ',' in house:
                committee.add_member(*house.split(','), chamber='lower')
            else:
                committee.add_member(house, chamber='lower')

        committee.add_source(url)
        self.save_committee(committee)

    def get_jlfc(self, name, url):
        """Gets info for the Joint Legislative Oversight Committee"""
        jlfc_page = self.urlopen(url)
        html = lxml.html.fromstring(jlfc_page)
        committee = Committee('joint', name)
        member_path = '//h3[contains(text(), "%s")]/following-sibling::p[1]'
        for chamber in ('Senate', 'House'):
            members = html.xpath(member_path % chamber)[0]\
                          .text_content().split('\r\n')
            for member in members:
                if member.strip():
                    committee.add_member(*member.replace(u'\xa0', ' ').split(','),
                                     chamber=_REV_CHAMBERS[chamber.lower()])
        committee.add_source(url)
        self.save_committee(committee)

    def get_jmfc(self, name, url):
        """Gets the Joint Millennium Fund Committee info"""
        jfmc_page = self.urlopen(url)
        html = lxml.html.fromstring(jfmc_page)
        committee = Committee('joint', name)
        table = html.xpath('//table')[2]
        for row in table.xpath('tbody/tr'):
            senate, house = [ td.text.replace('\r\n', ' ').replace(u'\xa0', ' ') \
                              for td in row.xpath('td') ]

            sen_data = senate.strip('Sen.').strip().split(',')
            hou_data = house.strip('Rep.').strip().split(',')

            if len(sen_data) > 1 and sen_data[1].strip() != "":
                committee.add_member(*sen_data)
            if len(hou_data) > 1 and hou_data[1].strip() != "":
                committee.add_member(*hou_data)

        committee.add_source(url)
        self.save_committee(committee)

    def scrape_committees(self, chamber):
        url = _COMMITTEE_URL % _CHAMBERS[chamber]
        page = self.urlopen(url)
        html = lxml.html.fromstring(page)
        table = html.xpath('body/table/tr/td[2]/table')[0]

        for row in table.xpath('tr')[1:]:
            # committee name, description, hours of operation,
            # secretary and office_phone
            text = list(row[0].itertext())
            if len(text) > 4:
                com = dict(zip(_TD_ONE, text))
            else:
                com = dict(zip(_TD_TWO, text))
            committee = Committee(chamber, **com)
            committee.add_source(url)

            # membership
            for td in row[1:]:
                if td.text:
                    leg = td.text.replace(u'\xa0', ' ').strip()
                    if leg:
                        committee.add_member(leg)

                for elem in td:
                    position, leg = elem.text, elem.tail
                    if position and leg:
                        leg = leg.replace(u'\xa0', ' ').strip()
                        if leg:
                            committee.add_member(leg, role=position)
                    elif leg:
                        leg = leg.replace(u'\xa0', ' ').strip()
                        if leg:
                            committee.add_member(leg)
            self.save_committee(committee)

    def scrape_joint_committees(self):
        url = 'http://legislature.idaho.gov/about/jointcommittees.htm'
        page = self.urlopen(url)
        html = lxml.html.fromstring(page)
        html.make_links_absolute(url)
        joint_li = html.xpath('//td[contains(h1, "Joint")]/ul/li')
        for li in joint_li:
            name, url = li[0].text, li[0].get('href')
            if 'Joint Finance-Appropriations Committee' in name:
                self.get_jfac(name, url)
            elif 'Joint Legislative Oversight Committee' in name:
                self.get_jlfc(name, url)
            elif name == 'Joint Millennium Fund Committee':
                self.get_jmfc(name, url)
            elif name == 'Economic Outlook and Revenue Assessment Committee':
                committee = Committee('joint', name)
                committee.add_source(url)
                # no membership available
                #self.save_committee(committee)
            else:
                self.log('Unknown committee: %s %s' % (name, url))


    def scrape(self, chamber, term):
        """
        Scrapes Idaho committees for the latest term.
        """
        self.validate_term(term, latest_only=True)

        self.scrape_committees(chamber)
        self.scrape_joint_committees()

########NEW FILE########
__FILENAME__ = legislators
from billy.scrape.legislators import LegislatorScraper, Legislator
import re
import datetime
import lxml.html

_BASE_URL = 'http://legislature.idaho.gov/%s/membership.cfm'

_CHAMBERS = {'upper':'Senate', 'lower':'House'}
_PARTY = {
        '(R)': 'Republican',
        '(D)': 'Democratic',
    }
_PHONE_NUMBERS = {'hom':'phone_number',
                  'bus':'business_phone',
                  'fax':'fax_number'}

class IDLegislatorScraper(LegislatorScraper):
    """Legislator data seems to be available for the current term only."""
    jurisdiction = 'id'

    def scrape_sub(self, chamber, term, district, sub_url):
        "Scrape basic info for a legislator's substitute."
        page = self.urlopen(sub_url)
        html = lxml.html.fromstring(page)
        html.make_links_absolute(sub_url)
        # substitute info div#MAINS35
        div = html.xpath('//div[contains(@id, "MAINS")]')[0]
        leg = {}
        leg['img_url'] = div[0][0].get('src')
        subfor = div[1][0].text.replace(u'\xa0', ' ').replace(': ', '')
        full_name = div[1][2].text.replace(u'\xa0', ' ')
        party = _PARTY[div[1][2].tail.strip()]
        leg['contact_form'] = div[1][3].xpath('string(a/@href)')
        leg = Legislator(term, chamber, district.strip(), full_name, party, **leg)
        leg['roles'][0] = {'chamber': chamber, 'state': self.state,
                           'term': term, 'role':'substitute',
                           'legislator': subfor[subfor.rindex('for'):],
                           'district': district.replace('District', '').strip(),
                           'party': party,
                           'start_date':None, 'end_date':None}
        leg.add_source(sub_url)
        self.save_legislator(leg)

    def scrape(self, chamber, term):
        """
        Scrapes legislators for the current term only
        """
        self.validate_term(term, latest_only=True)
        url = _BASE_URL % _CHAMBERS[chamber].lower()
        index = self.urlopen(url)
        html = lxml.html.fromstring(index)
        html.make_links_absolute(url)
        base_table = html.xpath('body/table/tr/td[2]/table[2]')
        district = None # keep track of district for substitutes
        for row in base_table[0].xpath('tr'):
            img_url = row.xpath('string(.//img/@src)')
            contact_form, additional_info_url = row.xpath('.//a/@href')
            if "Substitute" in row.text_content():
                # it seems like the sub always follows the person who he/she
                # is filling in for.
                # most sub info is provided at the additional info url
                self.scrape_sub(chamber, term, district, additional_info_url)
                continue
            else:
                full_name = " ".join(row[1][0].text_content().replace(u'\xa0', ' ').split())
                party = _PARTY[row[1][0].tail.strip()]

            pieces = [ x.strip() for x in row.itertext() if x ][6:]

            # the first index will either be a role or the district
            role = None
            if 'District' in pieces[0]:
                district = pieces.pop(0)
            else:
                role = pieces.pop(0)
                district = pieces.pop(0)

            looking_for = [
                "home",
                "fax"
            ]

            metainf = {
                "office": pieces[0]
            }

            for bit in pieces:
                for thing in looking_for:
                    if thing in bit.lower():
                        metainf[thing] = bit.split(" ", 1)[-1].strip()

            leg = Legislator(term, chamber,
                             district.replace('District', '').strip(),
                             full_name,
                             party=party)

            kwargs = {}
            if "office" in metainf:
                kwargs['address'] = metainf['office']
            if "fax" in metainf:
                kwargs['fax'] = metainf['fax']

            leg.add_office('district',
                           'District Office',
                            **kwargs)

            leg.add_source(url)
            leg['photo_url'] = img_url
            leg['contact_form'] = contact_form
            leg['url'] = additional_info_url
            leg['address'] = pieces.pop(0)

            # at this point 'pieces' still contains phone numbers and profession
            # and committee membership
            # skip committee membership, pick 'em up in IDCommitteeScraper
            end = -1
            if 'Committees:' in pieces:
                end = pieces.index('Committees:')
            for prop in pieces[:end]:
                # phone numbers
                if prop.lower()[0:3] in _PHONE_NUMBERS:
                    leg[ _PHONE_NUMBERS[ prop.lower()[0:3] ] ] = prop
                # profession
                else:
                    leg['profession'] = prop

            self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = bills
# -*- coding: utf-8 -*-
import re
import os
from collections import defaultdict
import datetime
import lxml.html
from urllib import urlencode

from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote
from billy.scrape.utils import convert_pdf

def group(lst, n):
    # from http://code.activestate.com/recipes/303060-group-a-list-into-sequential-n-tuples/
    for i in range(0, len(lst), n):
        val = lst[i:i+n]
        if len(val) == n:
            yield tuple(val)


TITLE_REMOVING_PATTERN = re.compile(".*(Rep|Sen). (.+)$")

SPONSOR_REFINE_PATTERN = re.compile(r'^Added (?P<spontype>.+) (?P<title>Rep|Sen)\. (?P<name>.+)')
SPONSOR_TYPE_REFINEMENTS = {
    'Chief Co-Sponsor': 'cosponsor',
    'as Chief Co-Sponsor': 'cosponsor',
    'Alternate Chief Co-Sponsor': 'cosponsor',
    'as Alternate Chief Co-Sponsor': 'cosponsor',
    'as Co-Sponsor': 'cosponsor',
    'Alternate Co-Sponsor':  'cosponsor',
    'as Alternate Co-Sponsor':  'cosponsor',
    'Co-Sponsor': 'cosponsor',
}


VERSION_TYPES = ('Introduced', 'Engrossed', 'Enrolled', 'Re-Enrolled')
FULLTEXT_DOCUMENT_TYPES = ('Public Act', "Governor's Message", )
# not as common, but maybe should just be added to FULLTEXT_DOCUMENT_TYPES?
# Amendatory Veto Motion \d{3}
# Conference Committee Report \d{3}

DOC_TYPES = {
    'B': 'bill',
    'R': 'resolution',
    'JR': 'joint resolution',
    'JRCA': 'constitutional amendment',
}

_action_classifiers = ( # see http://openstates.org/categorization/
    (re.compile(r'Amendment No. \d+ Filed'), 'amendment:introduced'),
    (re.compile(r'Amendment No. \d+ Tabled'), 'amendment:failed'),
    (re.compile(r'Amendment No. \d+ Adopted'), 'amendment:passed'),
    (re.compile(r'(Pref|F)iled with'), 'bill:filed'),
    (re.compile(r'Arrived? in'), 'bill:introduced'),
    (re.compile(r'First Reading'), 'bill:reading:1'),
    (re.compile(r'(Recalled to )?Second Reading'), 'bill:reading:2'),
    (re.compile(r'(Re-r|R)eferred to'), 'committee:referred'),
    (re.compile(r'(Re-a|A)ssigned to'), 'committee:referred'),
    (re.compile(r'Sent to the Governor'), 'governor:received'),
    (re.compile(r'Governor Approved'), 'governor:signed'),
    (re.compile(r'Governor Vetoed'), 'governor:vetoed'),
    (re.compile(r'Governor Item'), 'governor:vetoed:line-item'),
    (re.compile(r'Governor Amendatory Veto'), 'governor:vetoed'),
    (re.compile(r'Do Pass'), 'committee:passed'),
    (re.compile(r'Recommends Be Adopted'), 'committee:passed:favorable'),
    (re.compile(r'Be Adopted'), 'committee:passed:favorable'),
    (re.compile(r'Third Reading .+? Passed'), ['bill:reading:3', 'bill:passed']),
    (re.compile(r'Third Reading .+? Lost'), ['bill:reading:3', 'bill:failed']),
    (re.compile(r'Third Reading'), 'bill:reading:3'),
    (re.compile(r'Resolution Adopted'), 'bill:passed'),
    (re.compile(r'Resolution Lost'), 'bill:failed'),
    (re.compile(r'Session Sine Die',), 'bill:failed'),
    (re.compile(r'Tabled'), 'bill:withdrawn'),
)

OTHER_FREQUENT_ACTION_PATTERNS_WHICH_ARE_CURRENTLY_UNCLASSIFIED = [
    r'Accept Amendatory Veto - (House|Senate) (Passed|Lost) \d+-\d+\d+.?',
    r'Amendatory Veto Motion - (.+)',
    r'Balanced Budget Note (.+)',
    r'Effective Date(\s+.+ \d{4})?(;.+)?',
    r'To .*Subcommittee',
    r'Note Requested',
    r'Note Filed',
    r'^Public Act',
    r'Appeal Ruling of Chair',
    r'Added .*Sponsor',
    r'Remove(d)? .*Sponsor',
    r'Sponsor Removed',
    r'Sponsor Changed',
    r'^Chief .*Sponsor',
    r'^Co-Sponsor',
    r'Deadline Extended.+9\(b\)',
    r'Amendment.+Approved for Consideration',
    r'Approved for Consideration',
    r'Amendment.+Do Adopt',
    r'Amendment.+Concurs',
    r'Amendment.+Lost',
    r'Amendment.+Withdrawn',
    r'Amendment.+Motion.+Concur',
    r'Amendment.+Motion.+Table',
    r'Amendment.+Rules Refers',
    r'Amendment.+Motion to Concur Recommends be Adopted',
    r'Amendment.+Assignments Refers',
    r'Amendment.+Assignments Refers',
    r'Amendment.+Held',
    r'Motion.+Suspend Rule 25',
    r'Motion.+Reconsider Vote',
    r'Placed on Calendar',
    r'Amendment.+Postponed - (?P<committee>.+)',
    r'Postponed - (?P<committee>.+)',
    r"Secretary's Desk",
    r'Rule 2-10 Committee Deadline Established',
    r'^Held in (?P<committee>.+)'
]

VOTE_VALUES = ['NV', 'Y', 'N', 'E', 'A', 'P', '-']


def _categorize_action(action):
    for pattern, atype in _action_classifiers:
        if pattern.findall(action):
            kwargs = {"type": atype}
            if "committee:referred" in atype:
                kwargs['committees'] = [pattern.sub("", action).strip()]
            return kwargs
    return {"type": 'other'}

LEGISLATION_URL = ('http://ilga.gov/legislation/grplist.asp')


def build_url_for_legislation_list(metadata, chamber, session, doc_type):
    params = metadata['session_details'][session].get('params', {})
    params['num1'] = '1'
    params['num2'] = '10000'
    params['DocTypeID'] = doc_type
    return '?'.join([LEGISLATION_URL, urlencode(params)])


def chamber_slug(chamber):
    if chamber == 'lower':
        return 'H'
    return 'S'


class ILBillScraper(BillScraper):

    jurisdiction = 'il'

    def url_to_doc(self, url):
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        return doc

    def get_bill_urls(self, chamber, session, doc_type):
        url = build_url_for_legislation_list(self.metadata, chamber, session,
                                             doc_type)
        doc = self.url_to_doc(url)
        for bill_url in doc.xpath('//li/a/@href'):
            yield bill_url

    def scrape(self, chamber, session):
        for doc_type in DOC_TYPES:
            doc_type = chamber_slug(chamber)+doc_type
            for bill_url in self.get_bill_urls(chamber, session, doc_type):
                self.scrape_bill(chamber, session, doc_type, bill_url)
        if chamber == 'upper':
            # add appointments and JSRs as upper chamber, not perfectly
            # accurate but it'll do
            for bill_url in self.get_bill_urls(chamber, session, 'AM'):
                self.scrape_bill(chamber, session, 'AM', bill_url,
                                 'appointment')
            for bill_url in self.get_bill_urls(chamber, session, 'JSR'):
                self.scrape_bill(chamber, session, 'JSR', bill_url,
                                 'joint session resolution')
            # TODO: also add EO's - they aren't voted upon anyway & we don't
            # handle governor so they are omitted for now

    def scrape_bill(self, chamber, session, doc_type, url, bill_type=None):
        doc = self.url_to_doc(url)
        # bill id, title, summary
        bill_num = re.findall('DocNum=(\d+)', url)[0]
        bill_type = bill_type or DOC_TYPES[doc_type[1:]]
        bill_id = doc_type + bill_num

        title = doc.xpath('//span[text()="Short Description:"]/following-sibling::span[1]/text()')[0].strip()
        summary = doc.xpath('//span[text()="Synopsis As Introduced"]/following-sibling::span[1]/text()')[0].strip()

        bill = Bill(session, chamber, bill_id, title, type=bill_type,
                    summary=summary)

        bill.add_source(url)
        # sponsors
        sponsor_list = build_sponsor_list(doc.xpath('//a[@class="content"]'))
        # don't add just yet; we can make them better using action data

        # actions
        action_tds = doc.xpath('//a[@name="actions"]/following-sibling::table[1]/td')
        for date, actor, action in group(action_tds, 3):
            date = datetime.datetime.strptime(date.text_content().strip(),
                                              "%m/%d/%Y")
            actor = actor.text_content()
            if actor == 'House':
                actor = 'lower'
            elif actor == 'Senate':
                actor = 'upper'

            action = action.text_content()
            bill.add_action(actor, action, date,
                            **_categorize_action(action))
            if action.lower().find('sponsor') != -1:
                self.refine_sponsor_list(actor, action, sponsor_list, bill_id)

        # now add sponsors
        for spontype, sponsor, chamber, official_type in sponsor_list:
            if chamber:
                bill.add_sponsor(spontype, sponsor,
                                 official_type=official_type, chamber=chamber)
            else:
                bill.add_sponsor(spontype, sponsor,
                                 official_type=official_type)

        # versions
        version_url = doc.xpath('//a[text()="Full Text"]/@href')[0]
        self.scrape_documents(bill, version_url)

        # if there's more than 1 votehistory link, there are votes to grab
        if len(doc.xpath('//a[contains(@href, "votehistory")]')) > 1:
            votes_url = doc.xpath('//a[text()="Votes"]/@href')[0]
            self.scrape_votes(session, bill, votes_url)

        self.save_bill(bill)

    def scrape_documents(self, bill, version_url):
        doc = self.url_to_doc(version_url)

        for link in doc.xpath('//a[contains(@href, "fulltext")]'):
            name = link.text
            url = link.get('href')
            if name in VERSION_TYPES:
                bill.add_version(name, url + '&print=true',
                                 mimetype='text/html')
            elif 'Amendment' in name or name in FULLTEXT_DOCUMENT_TYPES:
                bill.add_document(name, url)
            elif 'Printer-Friendly' in name:
                pass
            else:
                self.warning('unknown document type %s - adding as document' % name)
                bill.add_document(name, url)

    def scrape_votes(self, session, bill, votes_url):
        doc = self.url_to_doc(votes_url)

        for link in doc.xpath('//a[contains(@href, "votehistory")]'):

            pieces = link.text.split(' - ')
            date = pieces[-1]
            if len(pieces) == 3:
                motion = pieces[1]
            else:
                motion = 'Third Reading'

            chamber = link.xpath('../following-sibling::td/text()')[0]
            if chamber == 'HOUSE':
                chamber = 'lower'
            elif chamber == 'SENATE':
                chamber = 'upper'
            else:
                self.warning('unknown chamber %s' % chamber)

            date = datetime.datetime.strptime(date, "%A, %B %d, %Y")

            vote = self.scrape_pdf_for_votes(session, chamber, date, motion.strip(), link.get('href'))

            bill.add_vote(vote)

        bill.add_source(votes_url)

    def fetch_pdf_lines(self, href):
        # download the file
        fname, resp = self.urlretrieve(href)
        pdflines = [line.decode('utf-8') for line in convert_pdf(fname, 'text').splitlines()]
        os.remove(fname)
        return pdflines

    def scrape_pdf_for_votes(self, session, chamber, date, motion, href):
        warned = False
        # vote indicator, a few spaces, a name, newline or multiple spaces
        VOTE_RE = re.compile('(Y|N|E|NV|A|P|-)\s{2,5}(\w.+?)(?:\n|\s{2})')
        COUNT_RE = re.compile('^(\d+) YEAS?\s+(\d+) NAYS?\s+(\d+) PRESENT$')
        PASS_FAIL_WORDS = {
            'PASSED': True,
            'PREVAILED': True,
            'ADOPTED': True,
            'CONCURRED': True,
            'FAILED': False,
            'LOST': False,
        }

        pdflines = self.fetch_pdf_lines(href)

        yes_count = no_count = present_count = other_count = 0
        yes_votes = []
        no_votes = []
        present_votes = []
        other_vote_detail = defaultdict(list)
        passed = None
        counts_found = False
        vote_lines = []
        for line in pdflines:
            # consider pass/fail as a document property instead of a result of the vote count
            # extract the vote count from the document instead of just using counts of names
            if line.strip() in PASS_FAIL_WORDS:
                if passed is not None:
                    raise Exception("Duplicate pass/fail matches in [%s]" % href)
                passed = PASS_FAIL_WORDS[line.strip()]
            elif COUNT_RE.match(line):
                yes_count, no_count, present_count = map(int, COUNT_RE.match(line).groups())
                counts_found = True
            elif counts_found:
                if line and not line[0].isspace():
                    vote_lines.append(line)

        votes = find_columns_and_parse(vote_lines)
        for name, vcode in votes.items():
            if name == 'Mr. Speaker':
                name = self.metadata['session_details'][session]['speaker']
            elif name == 'Mr. President':
                name = self.metadata['session_details'][session]['president']
            if vcode == 'Y':
                yes_votes.append(name)
            elif vcode == 'N':
                no_votes.append(name)
            else:
                other_vote_detail[vcode].append(name)
                other_count += 1
                if vcode == 'P':
                    present_votes.append(name)
        # fake the counts
        if yes_count == 0 and no_count == 0 and present_count == 0:
            yes_count = len(yes_votes)
            no_count = len(no_votes)
        else:  # audit
            if yes_count != len(yes_votes):
                self.warning("Mismatched yes count [expect: %i] [have: %i]" % (yes_count, len(yes_votes)))
                warned = True
            if no_count != len(no_votes):
                self.warning("Mismatched no count [expect: %i] [have: %i]" % (no_count, len(no_votes)))
                warned = True
            if present_count != len(present_votes):
                self.warning("Mismatched present count [expect: %i] [have: %i]" % (present_count, len(present_votes)))
                warned = True

        if passed is None:
            if chamber == 'lower':  # senate doesn't have these lines
                self.warning("No pass/fail word found; fall back to comparing yes and no vote.")
                warned = True
            passed = yes_count > no_count
        vote = Vote(chamber, date, motion, passed, yes_count, no_count,
                    other_count, other_vote_detail=other_vote_detail)
        for name in yes_votes:
            vote.yes(name)
        for name in no_votes:
            vote.no(name)
        for other_type, names in other_vote_detail.iteritems():
            for name in names:
                vote.other(name)
        vote.add_source(href)

        if warned:
            self.warning("Warnings were issued. Best to check %s" % href)
        return vote

    def refine_sponsor_list(self, chamber, action, sponsor_list, bill_id):
        if action.lower().find('removed') != -1:
            return
        if action.startswith('Chief'):
            self.debug("[%s] Assuming we already caught 'chief' for %s" % (bill_id, action))
            return
        match = SPONSOR_REFINE_PATTERN.match(action)
        if match:
            if match.groupdict()['title'] == 'Rep':
                chamber = 'lower'
            else:
                chamber = 'upper'
            for i, tup in enumerate(sponsor_list):
                spontype, sponsor, this_chamber, otype = tup
                if this_chamber == chamber and sponsor == match.groupdict()['name']:
                    try:
                        sponsor_list[i] = (SPONSOR_TYPE_REFINEMENTS[match.groupdict()['spontype']], sponsor, this_chamber, match.groupdict()['spontype'])
                    except KeyError:
                        self.warning('[%s] Unknown sponsor refinement type [%s]' % (bill_id, match.groupdict()['spontype']))
                    return
            self.warning("[%s] Couldn't find sponsor [%s,%s] to refine" % (bill_id, chamber, match.groupdict()['name']))
        else:
            self.debug("[%s] Don't know how to refine [%s]" % (bill_id, action))


def find_columns_and_parse(vote_lines):
    columns = find_columns(vote_lines)
    votes = {}
    for line in vote_lines:
        for idx in reversed(columns):
            bit = line[idx:]
            line = line[:idx]
            if bit:
                vote, name = bit.split(' ', 1)
                votes[name.strip()] = vote
    return votes


def _is_potential_column(line, i):
    for val in VOTE_VALUES:
        test_val = val + ' '
        if line[i:i+len(test_val)] == test_val:
            return True
    return False


def find_columns(vote_lines):
    potential_columns = []

    for line in vote_lines:
        pcols = set()
        for i, x in enumerate(line):
            if _is_potential_column(line, i):
                pcols.add(i)
        potential_columns.append(pcols)

    starter = potential_columns[0]
    for pc in potential_columns[1:-1]:
        starter.intersection_update(pc)
    last_row_cols = potential_columns[-1]
    if not last_row_cols.issubset(starter):
        raise Exception("Last row columns [%s] don't align with candidate final columns [%s]" % (last_row_cols, starter))
    # we should now only have values that appeared in every line
    return sorted(starter)


def build_sponsor_list(sponsor_atags):
    """return a list of (spontype,sponsor,chamber,official_spontype) tuples"""
    sponsors = []
    house_chief = senate_chief = None
    spontype = 'cosponsor'
    for atag in sponsor_atags:
        sponsor = atag.text
        if 'house' in atag.attrib['href'].split('/'):
            chamber = 'lower'
        elif 'senate' in atag.attrib['href'].split('/'):
            chamber = 'upper'
        else:
            chamber = None
        if chamber == 'lower' and house_chief is None:
            spontype = 'primary'
            official_spontype = 'chief'
            house_chief = sponsor
        elif chamber == 'upper' and senate_chief is None:
            spontype = 'primary'
            official_spontype = 'chief'
            senate_chief = sponsor
        else:
            spontype = 'cosponsor'
            official_spontype = 'cosponsor'  # until replaced
        sponsors.append((spontype, sponsor, chamber, official_spontype))
    return sponsors

########NEW FILE########
__FILENAME__ = committees
from billy.scrape.committees import CommitteeScraper, Committee

import lxml.html

class ILCommitteeScraper(CommitteeScraper):
    jurisdiction = 'il'

    def scrape_members(self, com, url):
        data = self.urlopen(url)
        if 'No members added' in data:
            return
        doc = lxml.html.fromstring(data)

        for row in doc.xpath('//table[@cellpadding="3"]/tr')[1:]:
            tds = row.xpath('td')

            # remove colon and lowercase role
            role = tds[0].text_content().replace(':','').strip().lower()

            name = tds[1].text_content().strip()
            com.add_member(name, role)


    def scrape(self, chamber, term):
        chamber_name = 'senate' if chamber == 'upper' else 'house'

        url = 'http://ilga.gov/{0}/committees/default.asp'.format(chamber_name)
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        top_level_com = None

        for a in doc.xpath('//a[contains(@href, "members.asp")]'):
            name = a.text.strip()
            code = a.getparent().getnext().text_content().strip()
            if 'Sub' in name:
                com = Committee(chamber, top_level_com, name, code=code)
            else:
                top_level_com = name
                com = Committee(chamber, name, code=code)

            com_url = a.get('href')
            self.scrape_members(com, com_url)
            com.add_source(com_url)
            if not com['members']:
                self.log('skipping empty committee on {0}'.format(com_url))
            else:
                self.save_committee(com)

########NEW FILE########
__FILENAME__ = events
import datetime as dt
import re

from billy.scrape.events import Event, EventScraper

import lxml.html
import pytz

urls = {
    "upper": "http://www.ilga.gov/senate/schedules/weeklyhearings.asp",
    "lower": "http://www.ilga.gov/house/schedules/weeklyhearings.asp"
}


class ILEventScraper(EventScraper):
    jurisdiction = 'il'
    _tz = pytz.timezone('US/Eastern')

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def scrape_page(self, url, session, chamber):
        page = self.lxmlize(url)

        ctty_name = page.xpath("//span[@class='heading']")[0].text_content()

        tables = page.xpath("//table[@cellpadding='3']")
        info = tables[0]
        rows = info.xpath(".//tr")
        metainf = {}
        for row in rows:
            tds = row.xpath(".//td")
            key = tds[0].text_content().strip()
            value = tds[1].text_content().strip()
            metainf[key] = value

        where = metainf['Location:']
        description = ctty_name

        datetime = metainf['Scheduled Date:']
        datetime = re.sub("\s+", " ", datetime)
        repl = {
            "AM": " AM",
            "PM": " PM"  # Space shim.
        }
        for r in repl:
            datetime = datetime.replace(r, repl[r])
        datetime = dt.datetime.strptime(datetime, "%b %d, %Y %I:%M %p")

        event = Event(session, datetime, 'committee:meeting',
                      description, location=where)
        event.add_source(url)

        if ctty_name.startswith('Hearing Notice For'):
            ctty_name.replace('Hearing Notice For', '')
        event.add_participant('host', ctty_name, 'committee', chamber=chamber)

        bills = tables[1]
        for bill in bills.xpath(".//tr")[1:]:
            tds = bill.xpath(".//td")
            if len(tds) < 4:
                continue
            # First, let's get the bill ID:
            bill_id = tds[0].text_content()
            event.add_related_bill(bill_id,
                                   description=description,
                                   type='consideration')

        self.save_event(event)

    def scrape(self, chamber, session):
        try:
            url = urls[chamber]
        except KeyError:
            return  # Not for us.
        page = self.lxmlize(url)
        tables = page.xpath("//table[@width='550']")
        for table in tables:
            meetings = table.xpath(".//a")
            for meeting in meetings:
                self.scrape_page(meeting.attrib['href'],
                                 session, chamber)

########NEW FILE########
__FILENAME__ = legislators
# -*- coding: utf-8 -*-
from urlparse import urlparse
from urllib import quote

import lxml.html
from billy.scrape.legislators import LegislatorScraper, Legislator

MEMBER_LIST_URL = {
    'upper': 'http://ilga.gov/senate/default.asp?GA=%s',
    'lower': 'http://ilga.gov/house/default.asp?GA=%s',
}


class ILLegislatorScraper(LegislatorScraper):
    jurisdiction = 'il'

    def scrape(self, chamber, term):
        term_slug = term[:-2]
        url = MEMBER_LIST_URL[chamber] % term_slug

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        for row in doc.xpath('//table')[4].xpath('tr')[2:]:
            name, _, _, district, party = row.xpath('td')
            district = district.text
            party = {'D':'Democratic', 'R': 'Republican',
                     'I': 'Independent'}[party.text]
            leg_url = name.xpath('a/@href')[0]
            name = name.text_content().strip()

            # inactive legislator, skip them for now
            if name.endswith('*'):
                name = name.strip('*')
                continue

            leg_html = self.urlopen(leg_url)
            leg_doc = lxml.html.fromstring(leg_html)
            leg_doc.make_links_absolute(leg_url)

            leg = Legislator(term, chamber, district, name, party=party,
                             url=leg_url)
            leg.add_source(url)

            hotgarbage = (
                'Senate Biography Information for the 98th General '
                'Assembly is not currently available.')
            if hotgarbage in leg_html:
                # The legislator's bio isn't available yet.
                self.logger.warning('No legislator bio available for ' + name)
                self.save_legislator(leg)
                continue

            photo_url = leg_doc.xpath('//img[contains(@src, "/members/")]/@src')[0]
            photo_url_parsed = urlparse(photo_url)
            encoded_path = quote(photo_url_parsed.path)
            photo_url = photo_url_parsed._replace(path=encoded_path).geturl()
            leg.update(photo_url=photo_url)
            leg.add_source(leg_url)

            # email
            email = leg_doc.xpath('//b[text()="Email: "]')
            if email:
                leg['email'] = email[0].tail

            # function for turning an IL contact info table to office details
            def _table_to_office(table, office_type, office_name):
                addr = ''
                phone = ''
                fax = None
                for row in table.xpath('tr'):
                    row = row.text_content().strip()
                    # skip rows that aren't part of address
                    if 'Office:' in row or row == 'Cook County':
                        continue
                    # fax number row ends with FAX
                    elif 'FAX' in row:
                        fax = row.replace(' FAX', '')
                    # phone number starts with ( [make it more specific?]
                    elif row.startswith('('):
                        phone = row
                    # everything else is an address
                    else:
                        addr += (row + '\n')
                if addr.strip() != ',':
                    leg.add_office(office_type, office_name,
                                   address=addr.strip(), phone=phone, fax=fax)

            # extract both offices from tables
            table = leg_doc.xpath('//table[contains(string(), "Springfield Office")]')
            if table:
                _table_to_office(table[3], 'capitol', 'Springfield Office')
            table = leg_doc.xpath('//table[contains(string(), "District Office")]')
            if table:
                _table_to_office(table[3], 'district', 'District Office')

            self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = test_bill_metadata
#!/usr/bin/env python

import unittest
from openstates.il import metadata
from openstates.il.bills import DOC_TYPES, ILBillScraper
import logging

log = logging.getLogger('openstates.il.tests.test_bill_metadata')

class TestBillMetadata(unittest.TestCase):
    """Run a basic sanity check to ensure that something would get scraped for each session in the metadata"""
    
    def setUp(self):
        self.scraper = ILBillScraper(metadata)

    def test_lists(self):
        chambers = ['H','S']
        sessions = []
        for term in metadata['terms']:
            sessions.extend(term['sessions'])
        self.assertTrue(len(sessions) > 0, "Expected non-zero list of sessions")

        for session in sessions:
            for chamber in chambers:
                session_chamber_count = 0
                for doc_type in DOC_TYPES:
                    count = len(list(self.scraper.get_bill_urls(chamber, session, doc_type)))
                    log.info("Session: %s Chamber: %s Doc Type: %s Count: %i" % (session, chamber, doc_type, count))
                    session_chamber_count += count
                self.assertTrue(session_chamber_count > 0, "Expected non-zero bill count for Session %s, Chamber %s" % (session, chamber))
if __name__ == '__main__':
    unittest.main()


########NEW FILE########
__FILENAME__ = test_vote_parsing
#!/usr/bin/env python
# -*- coding: utf-8 -*-
from nose.tools import *
import unittest
from openstates.il import metadata
from openstates.il.bills import find_columns, find_columns_and_parse
import logging

log = logging.getLogger('openstates.il.tests.test_bill_metadata')

TEST_LINES1 = [
    'E   Acevedo        Y   Davis,Monique   Y   Jefferson        Y   Reboletti',
    'Y   Arroyo         Y   Davis,William   Y   Joyce            N   Reis',
    'Y   Bassi          Y   DeLuca          N   Kosel            Y   Reitz',
    'N   Beaubien       Y   Dugan           Y   Lang             Y   Riley',
    'Y   Beiser         Y   Dunkin          N   Leitch           Y   Rita',
    'E   Bellock        N   Durkin          Y   Lyons            Y   Rose',
 ]
TEST_LINES2 =  [
    'Y    Althoff     Y    Dillard     N   Lauzen        NV   Righter',
    'NV   Bivins      Y    Forby       Y   Lightford     P    Risinger',
    'Y    Bomke       Y    Frerichs    Y   Link          Y    Rutherford',
    'Y    Bond        Y    Garrett     Y   Luechtefeld   Y    Sandoval',
    'N    Brady       NV   Haine       Y   Maloney       Y    Schoenberg',
    'N    Burzynski   Y    Halvorson   Y   Martinez      Y    Silverstein',
    'Y    Clayborne   Y    Harmon      Y   Meeks         Y    Steans',
    'Y    Collins     Y    Hendon      Y   Millner       Y    Sullivan',
    'NV   Cronin      Y    Holmes      Y   Munoz         P    Syverson',
    'Y    Crotty      Y    Hultgren    N   Murphy        Y    Trotter',
    'Y    Cullerton   Y    Hunter      Y   Noland        Y    Viverito',
    'Y    Dahl        Y    Jacobs      Y   Pankau        Y    Watson',
    'Y    DeLeo       Y    Jones, J.   P   Peterson      Y    Wilhelmi',
    'Y    Delgado     Y    Koehler     Y   Radogno       Y    Mr. President',
    'Y    Demuzio     Y    Kotowski    Y   Raoul',
]

TEST_LINES3 = [
    'Y    Althoff       Y    Haine         Y    Lightford     Y   Raoul',
    'Y    Bivins        Y    Harmon        NV   Link          Y   Rezin',
    'Y    Bomke         Y    Holmes        Y    Luechtefeld   Y   Righter',
    'Y    Brady         Y    Hunter        Y    Maloney       Y   Sandack',
    'Y    Clayborne     Y    Hutchinson    Y    Martinez      Y   Sandoval',
    'Y    Collins, A.   N    Jacobs        Y    McCann        Y   Schmidt',
    'Y    Collins, J.   Y    Johnson, C.   Y    McCarter      Y   Schoenberg',
    'Y    Crotty        Y    Johnson, T.   Y    Meeks         Y   Silverstein',
    'Y    Cultra        Y    Jones, E.     Y    Millner       Y   Steans',
    'Y    Delgado       Y    Jones, J.     Y    Mulroe        Y   Sullivan',
    'Y    Dillard       Y    Koehler       Y    Mu\xc3\xb1oz         Y   Syverson',
    'Y    Duffy         NV   Kotowski      Y    Murphy        Y   Trotter',
    'Y    Forby         Y    LaHood        Y    Noland        Y   Wilhelmi',
    'Y    Frerichs      Y    Landek        Y    Pankau        Y   Mr. President',
    'NV   Garrett       Y    Lauzen        Y    Radogno',
]

TEST_LINES1 = map(lambda x: x.decode('utf-8'), TEST_LINES1)
TEST_LINES2 = map(lambda x: x.decode('utf-8'), TEST_LINES2)
TEST_LINES3 = map(lambda x: x.decode('utf-8'), TEST_LINES3)

class TestVoteParsing(object):
    def test_find_and_parse1(self):
        d = find_columns_and_parse(TEST_LINES1)
        eq_('E', d['Acevedo'])
        eq_('Y', d['Davis,William'])
        eq_('Y', d['Dunkin'])
        eq_('N', d['Durkin'])
        eq_('Y', d['Lyons'])
        eq_('N', d['Reis'])

    def test_find_and_parse1(self):
        d = find_columns_and_parse(TEST_LINES2)
        eq_('NV', d['Cronin'])
        eq_('Y', d['Holmes'])
        eq_('N', d['Murphy'])
        eq_('Y', d['Mr. President'])
        eq_('P', d['Peterson'])

    def test_find_and_parse1(self):
        d = find_columns_and_parse(TEST_LINES3)
        eq_('Y', d['Collins, A.'])
        eq_('NV', d['Garrett'])
        eq_('Y', d[u'Muñoz'])
        eq_('Y', d['Syverson'])
        eq_('NV', d['Link'])



    def test_find_columns1(self):
        columns = find_columns(TEST_LINES1)
        eq_(4,len(columns))
        a,b,c,d = columns
        eq_(0,a)
        eq_(19,b)
        eq_(39,c)
        eq_(60,d)

    def test_find_columns2(self):
        columns = find_columns(TEST_LINES2)
        eq_(4,len(columns))
        a,b,c,d = columns
        eq_(0,a)
        eq_(17,b)
        eq_(34,c)
        eq_(52,d)

    def test_find_columns3(self):
        columns = find_columns(TEST_LINES3)
        eq_(4,len(columns))
        a,b,c,d = columns
        eq_(0,a)
        eq_(19,b)
        eq_(38,c)
        eq_(57,d)



if __name__ == '__main__':
    unittest.main()


########NEW FILE########
__FILENAME__ = actions
import re
from billy.scrape.actions import Rule, BaseCategorizer

# These are regex patterns that map to action categories.
_categorizer_rules = (

    # Parse vote counts--possibly useful in future.
    Rule('Roll\s+Call\s+\d+:\s+yeas\s+(?P<yes_votes>\d+),'
         '\s+nays\s+(?P<no_votes>\d+)'),
    Rule(r'(?i)voice vote', voice_vote=True),
    Rule(r'Effective (?P<effective_date>.+)'),

    # Same for member names.
    Rule(('(?i)(co)?authored by (representative|senator)s?\s+'
          '(?P<legislators>.+)')),
    Rule((r'(?P<version>Amendment \d+)\s+\(\s*(?P<legislators>.+?)\)'
          r'.+?failed'), 'amendment:failed'),
    Rule((r'(?P<version>Amendment \d+)\s+\(\s*(?P<legislators>.+?)\)'
          r'.+?withdrawn'), 'amendment:withdrawn'),
    Rule((r'(?P<version>Amendment \d+)\s+\(\s*(?P<legislators>.+?)\)'
          r'.+ruled out of order'), 'amendment:failed'),
    Rule(r'(?i)^(senator|representative)s?(?P<legislators>.+?)\s+added'),
    Rule(r'(?i)^Senate\s+(advisor|sponsor|conferee)s?.*?:\s+'
         r'(?P<legislators>.+)'),
    Rule(r'(House|Senate) sponsors?:\s+Senators\s+(?P<legislators>.+)'),
    Rule(r'Senators (?P<legislators>.+?) added as (?:co)?sponsors'),
    Rule('(?i)(co)?sponsors: (?P<legislators>.+)'),

    # Amendments.
    Rule((r'(?P<version>Amendment \d+)\s+\(\s*(?P<legislators>.+?)\)'
          r'.+?prevailed'), 'amendment:passed'),
    Rule(r'(?i)(house|senate) concurred in (house|senate) amendments',
         'amendment:passed'),
    Rule(r'Senate sponsors: Senators (?P<legislators>.+)'),

    # Readings.
    Rule(r'(?i)^first reading:', ('bill:introduced', 'bill:reading:1')),
    Rule(r'(?i)^second reading:', 'bill:reading:2'),
    Rule(r'(?i)^third reading:', 'bill:reading:3'),

    # Committees.
    Rule(r'Committee report:.+?do pass', 'committee:passed:favorable'),
    Rule([r'(?i)referred to (?P<committees>.+)',
          r'(?i)reassigned to (?P<committees>.+)'], 'committee:referred'),
    Rule(r'(?i)Committee report:.*?do pass', 'committee:passed:favorable'),
    Rule(r'(?i)motion to recommit to (?P<committees>.+?)'
         r'\((?P<legislators>.+?)\)'),

    # Passage/failure.
    Rule(r'passed', 'bill:passed'),
    Rule(r'reading:\s+adopted', 'bill:passed'),
    Rule(r'(?i)Third reading: passed', 'bill:passed'),

    # Weird.
    Rule(r'Conference committee.+adopted',
         ['committee:passed', 'amendment:passed']),

    # Governor.
    Rule(r'Signed by the Governor', 'governor:signed'),
    )


class Categorizer(BaseCategorizer):
    rules = _categorizer_rules

    def post_categorize(self, attrs):
        res = set()
        if 'legislators' in attrs:
            for text in attrs['legislators']:
                rgx = r'(,\s+(?![a-z]\.)|\s+and\s+)'
                legs = re.split(rgx, text)
                legs = filter(lambda x: x not in [', ', ' and '], legs)
                res |= set(legs)
        attrs['legislators'] = list(res)
        return attrs

########NEW FILE########
__FILENAME__ = apiclient
import os
import time
import requests
import urlparse
import functools


class BadApiResponse(Exception):
    '''Raised if the service returns a service code higher than 400,
    other than 429. Makes the response object avaible as exc.resp
    '''
    def __init__(self, resp, *args):
        super(BadApiResponse, self).__init__(self, *args)
        self.resp = resp


def check_response(method):
    '''Decorated functions will run, and if they come back
    with a 429 and retry-after header, will wait and try again.
    '''
    @functools.wraps(method)
    def wrapped(self, *args, **kwargs):
        resp = method(self, *args, **kwargs)
        status = resp.status_code
        if 400 < status:
            if resp.status_code == 429:
                self.handle_429(resp, *args, **kwargs)
                return method(self, *args, **kwargs).json()
            msg_args = (resp, resp.text, resp.headers)
            msg = 'Bad api response: %r %r %r' % msg_args
            raise BadApiResponse(resp, msg)
        return resp.json()
    return wrapped


class ApiClient(object):
    '''
    docs: http://docs.api.iga.in.gov/
    '''
    root = "https://api.iga.in.gov/"
    resources = dict(
        sessions='/sessions',
        subjects='/{session}/subjects',
        chambers='/{session}/chambers',
        bills='/{session}/bills/{bill_id}',
        chamber_bills='/{session}/chambers/{chamber}/bills',
        bill_rollcalls='/{session}/bills/{bill_id}/roll-calls',
        committees='/{session}/committees',
        committee='/{session}/committees/{committee_name}',
        legislators='/{session}/legislators',
        legislator='/{session}/legislators/{legislator_id}',
        chamber_legislators='/{session}/chambers/{chamber}/legislators',
        )

    def __init__(self, scraper):
        self.scraper = scraper
        self.apikey = os.environ['INDIANA_API_KEY']

    @check_response
    def geturl(self, url):
        headers = {}
        headers['Authorization'] = self.apikey
        headers['Accept'] = "application/json"
        self.scraper.info('Api GET next page: %r, %r' % (url, headers))
        return self.scraper.get(url, headers=headers, verify=False)

    @check_response
    def get_relurl(self, url):
        headers = {}
        headers['Authorization'] = self.apikey
        headers['Accept'] = "application/json"
        url = urlparse.urljoin(self.root, url)
        self.scraper.info('Api GET: %r, %r' % (url, headers))
        return self.scraper.get(url, headers=headers, verify=False)

    def make_url(self, resource_name, **url_format_args):
        # Build up the url.
        url = self.resources[resource_name]
        url = url.format(**url_format_args)
        url = urlparse.urljoin(self.root, url)
        return url

    @check_response
    def get(self, resource_name, requests_args=None,
            requests_kwargs=None, **url_format_args):
        '''Resource is a self.resources dict key.
        '''
        url = self.make_url(resource_name, **url_format_args)

        # Add in the api key.
        requests_args = requests_args or ()
        requests_kwargs = requests_kwargs or {}
        requests_kwargs.update(verify=False)
        headers = requests_kwargs.get('headers', {})
        headers['Authorization'] = self.apikey
        headers['Accept'] = "application/json"
        requests_kwargs['headers'] = headers

        args = (url, requests_args, requests_kwargs)
        self.scraper.info('Api GET: %r, %r, %r' % args)
        return self.scraper.get(url, *requests_args, **requests_kwargs)

    def unpaginate(self, result):
        for data in result['items']:
            yield data
        while True:
            if 'nextLink' in result:
                url = result['nextLink']
                self.scraper.info('Api GET next page: %r' % url)
                result = self.get_relurl(url)
                if not result['items']:
                    return
                for data in result['items']:
                    yield data
            else:
                return

    def handle_429(self, resp, *args, **kwargs):
        '''According to the docs:

        "If the rate limit is exceeded, we will respond with a HTTP 429 Too Many
        Requests response code and a body that details the reason for the rate
        limiter kicking in. Further, the response will have a Retry-After
        header that tells you for how many seconds to sleep before retrying.
        You should anticipate this in your API client for the smoothest user
        experience."
        '''
        seconds = int(resp.headers['retry-after'])
        self.scraper.info('Got a 429: Sleeping %s seconds per retry-after header.' % seconds)
        time.sleep(seconds)

########NEW FILE########
__FILENAME__ = bills
import os
import re
import datetime
import urlparse
import operator
import itertools
from collections import defaultdict
from contextlib import contextmanager
from StringIO import StringIO

import scrapelib
import requests
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote
from billy.scrape.utils import convert_pdf
from billy.importers.bills import fix_bill_id

import pytz
import lxml.html

from .actions import Categorizer
from .models import parse_vote, BillDocuments, VoteParseError


def parse_vote_count(s):
    if s == 'NONE':
        return 0
    return int(s)


def insert_specific_votes(vote, specific_votes):
    for name, vtype in specific_votes:
        if vtype == 'yes':
            vote.yes(name)
        elif vtype == 'no':
            vote.no(name)
        elif vtype == 'other':
            vote.other(name)


def check_vote_counts(vote):
    try:
        assert vote['yes_count'] == len(vote['yes_votes'])
        assert vote['no_count'] == len(vote['no_votes'])
        assert vote['other_count'] == len(vote['other_votes'])
    except AssertionError:
        pass


class INBillScraper(BillScraper):
    jurisdiction = 'in'

    categorizer = Categorizer()
    _tz = pytz.timezone('US/Eastern')

    # Can turn this on or off. There are thousands of subjects and it takes hours.
    SCRAPE_SUBJECTS = True

    def scrape(self, term, chambers):
        if 0 < self._requests_per_minute:
            self.requests_per_minute = 30
        seen_bill_ids = set()

        # Get resolutions.
        url = 'http://iga.in.gov/legislative/2014/resolutions'
        self.logger.info('GET ' + url)
        html = self.get(url).text
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        for a in doc.xpath('//strong/ancestor::a'):
            bill_id = a.text_content().strip()
            bill_chamber = ('upper' if bill_id[0] in 'SJ' else 'lower')
            bill_url = a.attrib['href']
            bill_title = a.xpath('string(./following-sibling::em)').strip()
            bill = self.scrape_bill(
                bill_chamber, term, bill_id, bill_url, bill_title)

        url = 'http://iga.in.gov/legislative/%s/bills/' % term
        self.logger.info('GET ' + url)
        html = self.get(url).text
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)


        # First scrape subjects and any bills found on those pages.
        for subject_url, subject in self.generate_subjects():
            subject_bills = self.generate_subject_bills(subject_url)
            for bill_id, bill_url, bill_title in subject_bills:
                chamber = 'upper' if bill_id[0] == 'S' else 'lower'
                self.scrape_bill(
                    chamber, term, bill_id, bill_url, bill_title, subject=subject)
                seen_bill_ids.add(bill_id)

        # Then hit bill index page to catch any uncategorized bills.
        uls = doc.xpath('//ul[contains(@class, "clean-list")]')

        for chamber, ul in zip(chambers, uls):
            for li in ul.xpath('li'):
                bill_id = li.xpath('string(a/strong)')
                if bill_id in seen_bill_ids:
                    continue
                bill_url = li.xpath('string(a/@href)')
                bill_title = li.xpath('a/strong')[0].tail.rstrip().lstrip(': ')
                bill = self.scrape_bill(
                    chamber, term, bill_id, bill_url, bill_title)

    def generate_subjects(self):
        url = 'http://iga.in.gov/legislative/2014/bysubject/'
        self.logger.info('GET ' + url)
        resp = self.get(url)
        html = resp.text
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        for li in doc.xpath('//li[contains(@class, "subject-list_item")]'):
            yield li.xpath('string(a/@href)'), li.text_content().strip()

    def generate_subject_bills(self, url):
        self.logger.info('GET ' + url)
        resp = self.get(url)
        html = resp.text
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        for row in doc.xpath('//table//tr')[1:]:
            try:
                bill_id = row[2].text_content()
            except IndexError:
                # We hit the last row.
                return
            bill_url = row[2].xpath('string(a/@href)')
            bill_title = row[3].text_content()
            yield bill_id, bill_url, bill_title

    def scrape_bill(self, chamber, term, bill_id, url, title, subject=None):
        self.logger.info('GET ' + url)
        resp = self.get(url)
        html = resp.text
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        bill = Bill(term, chamber, bill_id, title)
        bill.add_source(url)
        if subject is not None:
            bill['subjects'] = [subject]

        # Sponsors
        sponsor_map = {
            'author': 'primary',
            'co-author': 'cosponsor',
            'sponsor': 'cosponsor',
            'co-sponsor': 'cosponsor',
            }
        for div in doc.xpath('//div[contains(@class, "bill-author-info")]'):
            name = div.xpath('string(b)').strip()
            sp_type = sponsor_map[div.xpath('string(p)').strip().lower()]
            bill.add_sponsor(sp_type, name)

        # Actions
        for li in doc.xpath('//div[@id="bill-actions"]//li')[::-1]:
            if li.text_content() == 'None currently available.':
                continue
            chamber_str = li.xpath('string(strong)').strip()
            action_chamber = dict(H='lower', S='upper')[chamber_str]
            action_date = li.xpath('string(span[@class="document-date"])')
            # Some resolution actions have no dates.
            if not action_date.strip():
                continue
            action_date = datetime.datetime.strptime(action_date.strip(), '%m/%d/%Y')
            action_text = li.xpath('string(span[2])').strip()
            if not action_text.strip():
                continue
            kwargs = dict(date=action_date, actor=action_chamber, action=action_text)
            kwargs.update(**self.categorizer.categorize(action_text))
            bill.add_action(**kwargs)

        # Documents (including votes)
        for doc_type, doc_meta in BillDocuments(self, doc):
            if doc_type == 'version':
                bill.add_version(
                    doc_meta.title or doc_meta.text, url=doc_meta.url,
                    mimetype='application/pdf')
            elif doc_type == 'document':
                bill.add_document(doc_meta.title or doc_meta.text, url=doc_meta.url,
                    mimetype='application/pdf')
            elif doc_type == 'rollcall':
                self.add_rollcall(chamber, bill, doc_meta)

        self.save_bill(bill)

    def add_rollcall(self, chamber, bill, doc_meta):
        try:
            vote = parse_vote(self, chamber, doc_meta)
            bill.add_vote(vote)
        except VoteParseError:
            # It was a scanned, hand-written document, most likely.
            return

    # def scrape_senate_vote(self, bill, url):
    #     try:
    #         (path, resp) = self.urlretrieve(url)
    #     except:
    #         return
    #     text = convert_pdf(path, 'text')
    #     os.remove(path)

    #     lines = text.split('\n')

    #     date_match = re.search(r'Date:\s+(\d+/\d+/\d+)', text)
    #     if not date_match:
    #         self.log("Couldn't find date on %s" % url)
    #         return

    #     time_match = re.search(r'Time:\s+(\d+:\d+:\d+)\s+(AM|PM)', text)
    #     date = "%s %s %s" % (date_match.group(1), time_match.group(1),
    #                          time_match.group(2))
    #     date = datetime.datetime.strptime(date, "%m/%d/%Y %I:%M:%S %p")
    #     date = self._tz.localize(date)

    #     vote_type = None
    #     yes_count, no_count, other_count = None, None, 0
    #     votes = []
    #     for line in lines[21:]:
    #         line = line.strip()
    #         if not line:
    #             continue

    #         if line.startswith('YEAS'):
    #             yes_count = int(line.split(' - ')[1])
    #             vote_type = 'yes'
    #         elif line.startswith('NAYS'):
    #             no_count = int(line.split(' - ')[1])
    #             vote_type = 'no'
    #         elif line.startswith('EXCUSED') or line.startswith('NOT VOTING'):
    #             other_count += int(line.split(' - ')[1])
    #             vote_type = 'other'
    #         else:
    #             votes.extend([(n.strip(), vote_type)
    #                           for n in re.split(r'\s{2,}', line)])

    #     if yes_count is None or no_count is None:
    #         self.log("Couldne't find vote counts in %s" % url)
    #         return

    #     passed = yes_count > no_count + other_count

    #     clean_bill_id = fix_bill_id(bill['bill_id'])
    #     motion_line = None
    #     for i, line in enumerate(lines):
    #         if line.strip() == clean_bill_id:
    #             motion_line = i + 2
    #     motion = lines[motion_line]
    #     if not motion:
    #         self.log("Couldn't find motion for %s" % url)
    #         return

    #     vote = Vote('upper', date, motion, passed, yes_count, no_count,
    #                 other_count)
    #     vote.add_source(url)

    #     insert_specific_votes(vote, votes)
    #     check_vote_counts(vote)

    #     bill.add_vote(vote)


########NEW FILE########
__FILENAME__ = legislators
import re
import datetime
import urlparse
import collections

import lxml.html

from billy.scrape.legislators import LegislatorScraper, Legislator
import scrapelib


class INLegislatorScraper(LegislatorScraper):
    jurisdiction = 'in'
    _url = 'http://iga.in.gov/legislative/%d/legislators'

    @property
    def url(self):
        return self._url % self.year

    def get_termdata(self, term_id):
        for term in self.metadata['terms']:
            if term['name'] == term_id:
                return term

    def scrape(self, chamber, term):
        self.requests_per_minute = 15
        self.termdata = self.get_termdata(term)

        year = datetime.datetime.now().year
        if year not in self.termdata.values():
            year = self.termdata['start_year']
        self.year = year

        # Get the find-a-legislator page.
        html = self.urlopen(self.url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(self.url)
        optgroup = dict(upper='Senators', lower='Representatives')[chamber]
        for option in doc.xpath('//optgroup[@id="%s"]/option' % optgroup):
            self.scrape_legislator(chamber, term, option)

    def scrape_legislator(self, chamber, term, option):
        url = urlparse.urljoin(self.url, option.attrib['value'])
        name, party, district = re.split(r'\s*,\s*', option.text.strip())
        name = re.sub(r'^(Sen\.|Rep\.)\s+', '', name)
        district = re.sub(r'^District\s+', '', district)
        leg = Legislator(term, chamber, district, name, party=party)
        leg.add_source(self.url)

        # Scrape leg page.
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(self.url)
        leg.add_source(url)

        # Scrape committees.
        for tr in doc.xpath('//table//tr'):
            committee, role = tr
            committee = committee.text_content().strip()
            role = role.text_content().strip()
            if 'member' in role.lower():
                role = 'committee member'
            elif 'chair' in role.lower():
                role = 'chair'
            leg.add_role(role, term, chamber=chamber, committee=committee)

        # Scrape offices.
        dist_office, phone = doc.xpath('//address')
        dist_office = dist_office.text_content().strip()
        dist_office = re.sub(r' {2,}', '', dist_office)

        phone = phone.text_content().strip()
        email = doc.xpath('string(//a[starts-with(@href, "mailto:")]/@href)')
        photo_url = doc.xpath('string(//img[contains(@class, "member")]/@src)')

        leg.update(email=email, photo_url=photo_url)
        leg.add_office(
            address=dist_office, name='District Office',
            type='district', phone=phone)

        self.save_legislator(leg)


########NEW FILE########
__FILENAME__ = models
import re
import os
import datetime
import collections
from StringIO import StringIO

import scrapelib
from billy.scrape.utils import convert_pdf, PlaintextColumns
from billy.scrape.votes import Vote


class VoteParseError(Exception):
    pass


def parse_vote(scraper, chamber, doc_meta):
    # Get the pdf text.
    try:
        (path, resp) = scraper.urlretrieve(doc_meta.url)
    except scrapelib.HTTPError as exc:
        scraper.warning('Got error %r while fetching %s' % (exc, url))
        raise VoteParseError()
    text = convert_pdf(path, 'text')
    text = text.replace('\xc2\xa0', ' ')
    text = text.replace('\xc2\xad', ' ')
    os.remove(path)

    # Figure out what type of vote this is.
    if 'Roll Call' in text:
        return RollCallVote(text, scraper, chamber, doc_meta).vote()
    else:
        scraper.warning('Skipping a committee vote (See Jira issue DATA-80).')
        raise VoteParseError()


class RollCallVote(object):

    def __init__(self, text, scraper, chamber, doc_meta):
        self.text = text
        self.doc_meta = doc_meta
        self.url = doc_meta.url
        self.scraper = scraper
        self.chamber = chamber

    def date(self):
        try:
            date = re.search(r'[A-Z]+ \d{2}, \d{4}', self.text).group()
        except AttributeError:
            msg = "Couldn't find date on %s" % self.url
            self.scraper.logger.warning(msg)
            raise self.VoteParseError(msg)
        return datetime.datetime.strptime(date, "%b %d, %Y")

    def get_counts(self):
        return dict(re.findall(r'(Yeas?|Nays?|Excused|Not Voting)\s+(\d+)', self.text))

    def motion(self):
        return re.search('Roll Call \d+', self.text).group()

    def chamber(self):
        first_line = self.text.splitlines()[0]
        if 'House or Representatives' in first_line:
            return 'lower'
        else:
            return 'upper'

    def passed(self):

        result_types = {
            'FAILED': False,
            'DEFEATED': False,
            'PREVAILED': True,
            'PASSED': True,
            'SUSTAINED': True,
            'NOT SECONDED': False,
            'OVERRIDDEN': True,
            'ADOPTED': True,
        }
        text = self.text.upper()
        for key, value in result_types.items():
            if key in text:
                return value
        raise Exception("Couldn't determine vote passage status.")

    def vote_values(self):
        chunks = re.split(r'(YEA|NAY|EXCUSED|NOT VOTING)\s+\d+', self.text)[1:]
        data = dict(zip(chunks[::2], chunks[1::2]))
        votekeys = dict(YEA='yes', NAY='no')
        for key, data in data.items():
            garbage = re.split(r'(\s{4,}|\n)', data)
            for name in [name.strip() for name in garbage if name.strip()]:
                yield votekeys.get(key, 'other'), name

    def vote(self):
        '''Return a billy vote.
        '''
        actual_vote_dict = collections.defaultdict(list)
        date = self.date()
        motion = self.motion()
        passed = self.passed()
        counts = self.get_counts()
        yes_count = sum(int(counts.get(key, 0)) for key in ('Yea', 'Yeas'))
        no_count = sum(int(counts.get(key, 0)) for key in ('Nay', 'Nays'))
        vote = Vote(self.chamber, date, motion,
                    passed, yes_count, no_count,
                    sum(map(int, counts.values())) - (yes_count + no_count),
                    actual_vote=dict(actual_vote_dict))

        for vote_val, voter in self.vote_values():
            getattr(vote, vote_val)(voter)
        vote.add_source(self.url)
        return vote


# ----------------------------------------------------------------------------
# Handle documents hot mess.
# ----------------------------------------------------------------------------
class BogusDocument(Exception):
    pass


class DocumentMeta(object):
    DocMeta = collections.namedtuple('DocMeta', 'a,href,uid,title,text,url')

    def __init__(self, scraper, el):
        self.el = el
        self.scraper = scraper

    def get_doc_meta(self):
        text = self.el.text_content()
        text = re.sub(r'\s+', ' ', text).strip()
        api_meta = self.get_doc_api_meta(self.el.attrib)
        if api_meta is None:
            msg = 'No data recieved from the API for %r' % self.el.attrib
            raise BogusDocument(msg)
        return self.DocMeta(
            a=self.el,
            text=text,
            href=self.el.attrib['href'],
            uid=self.el.attrib['data-myiga-actiondata'],
            title=self.el.attrib.get('title'),
            url=self.get_document_url(api_meta))

    def get_doc_api_meta(self, attrib):
        '''The document link gives you json if you hit with the right
        Accept header.
        '''
        headers = dict(accept="application/json, text/javascript, */*")
        version_id = attrib['data-myiga-actiondata']
        if not version_id.strip():
            return
        if version_id in 'None':
            return
        png_url = 'http://iga.in.gov/documents/' + version_id
        self.scraper.logger.info('GET ' + png_url)
        resp = self.scraper.get(png_url, headers=headers)
        try:
            data = resp.json()
        except:
            return
        return data

    def get_document_url(self, data):
        '''If version_id is b5ff1c9c, the url will be:
        http://iga.in.gov/static-documents/b/5/f/f/b5ff1c9c/{data[name]}
        '''
        buf = StringIO()
        buf.write('http://iga.in.gov/static-documents/')
        for char in str(data['uid'])[:4]:
            buf.write(char)
            buf.write('/')
        buf.write(data['uid'])
        buf.write('/')
        buf.write(data['name'])
        return buf.getvalue()


class BillDocuments(object):
    '''The new IN site has lots of documents for each bill. Sorting them
    out from the kooky accordian view on the site is messy, so lives in
    this separate class.
    '''

    def __init__(self, scraper, bill_doc):
        self.doc = bill_doc
        self.scraper = scraper

    def guess_doc_type(self, meta):
        '''Guess whether this is a version, document, report,
        or roll call vote.
        '''
        title = (meta.title or meta.text).lower()
        if 'bill' in title:
            return 'version'
        if 'roll call' in title:
            return 'rollcall'

        textbits = meta.text.split('.')
        lastbit = textbits[-1]
        # Fiscal note, amendment, committee report
        if lastbit.startswith(('FN', 'AMS', 'CR')):
            return 'document'

    def iter_doc_meta(self):
        xpath = '//*[@data-myiga-actiondata]'
        meta = []
        for a in self.doc.xpath(xpath):
            try:
                data = DocumentMeta(self.scraper, a).get_doc_meta()
            except BogusDocument as exc:
                self.scraper.logger.warning(exc)
                self.scraper.logger.warning('Skipping document: %r.' % (a.attrib,))
                continue
            meta.append(data)
        return meta

    def get_deduped_meta(self):
        meta = list(self.iter_doc_meta())
        grouped = collections.defaultdict(set)
        for data in meta:
            grouped[data.uid].add(data)
        for k, v in grouped.items():
            if 1 < len(v):
                for data in list(v):
                    if not data.title:
                        v.remove(data)
                    if not v:
                        v.add(data)
        for k, v in grouped.items():
            yield v.pop()

    def __iter__(self):
        for data in self.get_deduped_meta():
            yield self.guess_doc_type(data), data

########NEW FILE########
__FILENAME__ = action_codes_scrape
#!/usr/bin/env python

import re

data = re.compile(r'^([a-z]+_[a-z]+_[0-9]{3}): (.*) *$')
comment = re.compile(r'^#')
variable = re.compile(r'\$([a-z_]+)\$')

voted = re.compile(r'.*\$vote_tally\$.*')
passed = re.compile(r'.*(?!not )(passed|adopted).*\$vote_tally\$.*', re.IGNORECASE)
failed = re.compile(r'.*(failed|not adopted|not passed).*\$vote_tally\$.*', re.IGNORECASE)

voted_codes = []
passed_codes = []
failed_codes = []
numbers = []

with open('action_codes') as action_codes:
	action_codes_str = action_codes.read()
	for line in action_codes_str.split('\n'):
		if comment.match(line):
			continue
		if data.match(line):
			match = data.match(line)
			number = match.group(1)
			numbers.append(number)
			if voted.match(match.group(2)):
				voted_codes.append(number)
			if passed.match(match.group(2)):
				passed_codes.append(number)
			if failed.match(match.group(2)):
				failed_codes.append(number)

print("voted = %s" % voted_codes)
print("passed = %s" % passed_codes)
print("failed = %s" % failed_codes)


########NEW FILE########
__FILENAME__ = bills
import re
import os
import datetime
import json
import subprocess

import lxml.html
import scrapelib

from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote
from billy.scrape import NoDataForPeriod


import ksapi

class KSBillScraper(BillScraper):
    jurisdiction = 'ks'
    latest_only = True

    def scrape(self, chamber, session):
        # check for abiword
        if os.system('which abiword') != 0:
            raise ScrapeError('abiword is required for KS scraping')

        chamber_name = 'Senate' if chamber == 'upper' else 'House'
        chamber_letter = chamber_name[0]
        # perhaps we should save this data so we can make one request for both?
        bill_request = self.urlopen(ksapi.url + 'bill_status/')
        bill_request_json = json.loads(bill_request)
        bills = bill_request_json['content']
        for bill_data in bills:

            bill_id = bill_data['BILLNO']

            # filter other chambers
            if not bill_id.startswith(chamber_letter):
                continue

            if 'CR' in bill_id:
                btype = 'concurrent resolution'
            elif 'R' in bill_id:
                btype = 'resolution'
            elif 'B' in bill_id:
                btype = 'bill'

            title = bill_data['SHORTTITLE'] or bill_data['LONGTITLE']

            # main
            bill = Bill(session, chamber, bill_id, title,
                        type=btype, status=bill_data['STATUS'])
            bill.add_source(ksapi.url + 'bill_status/' + bill_id.lower())

            if (bill_data['LONGTITLE'] and
                bill_data['LONGTITLE'] != bill['title']):
                bill.add_title(bill_data['LONGTITLE'])

            for sponsor in bill_data['SPONSOR_NAMES']:
                stype = ('primary' if len(bill_data['SPONSOR_NAMES']) == 1
                         else 'cosponsor')
                bill.add_sponsor(stype, sponsor)

            # history is backwards
            for event in reversed(bill_data['HISTORY']):

                actor = ('upper' if event['chamber'] == 'Senate'
                         else 'lower')

                date = datetime.datetime.strptime(event['occurred_datetime'], "%Y-%m-%dT%H:%M:%S")
                # append committee names if present
                if 'committee_names' in event:
                    action = (event['status'] + ' ' +
                              ' and '.join(event['committee_names']))
                else:
                    action = event['status']

                if event['action_code'] not in ksapi.action_codes:
                    self.warning('unknown action code on %s: %s %s' %
                                 (bill_id, event['action_code'],
                                  event['status']))
                    atype = 'other'
                else:
                    atype = ksapi.action_codes[event['action_code']]
                bill.add_action(actor, action, date, type=atype)

            try:
                self.scrape_html(bill)
            except scrapelib.HTTPError as e:
                self.warning('unable to fetch HTML for bill {0}'.format(
                    bill['bill_id']))
            self.save_bill(bill)

    def scrape_html(self, bill):
        slug = {'2013-2014': 'b2013_14'}[bill['session']]
        # we have to go to the HTML for the versions & votes
        base_url = 'http://www.kslegislature.org/li/%s/measures/' % slug
        if 'resolution' in bill['type']:
            base_url = 'http://www.kslegislature.org/li/%s/year1/measures/' % slug

        url = base_url + bill['bill_id'].lower() + '/'
        doc = lxml.html.fromstring(self.urlopen(url))
        doc.make_links_absolute(url)

        bill.add_source(url)

        # versions & notes
        version_rows = doc.xpath('//tbody[starts-with(@id, "version-tab")]/tr')
        for row in version_rows:
            # version, docs, sn, fn
            tds = row.getchildren()
            title = tds[0].text_content().strip()
            doc_url = get_doc_link(tds[1])
            if doc_url:
                bill.add_version(title, doc_url, mimetype='application/pdf')
            if len(tds) > 2:
                sn_url = get_doc_link(tds[2])
                if sn_url:
                    bill.add_document(title + ' - Supplementary Note', sn_url)
            if len(tds) > 3:
                fn_url = get_doc_link(tds[3])
                if sn_url:
                    bill.add_document(title + ' - Fiscal Note', sn_url)


        history_rows = doc.xpath('//tbody[starts-with(@id, "history-tab")]/tr')
        for row in history_rows:
            row_text = row.xpath('.//td[3]')[0].text_content()

            # votes
            vote_url = row.xpath('.//a[contains(text(), "Yea:")]/@href')
            if vote_url:
                vote_date = row.xpath('.//td[1]')[0].text_content()
                vote_chamber = row.xpath('.//td[2]')[0].text_content()
                self.parse_vote(bill, vote_date, vote_chamber, row_text,
                                vote_url[0])

            # amendments & reports
            amendment = get_doc_link(row.xpath('.//td[4]')[0])
            if amendment:
                if 'Motion to Amend' in row_text:
                    _, offered_by = row_text.split('Motion to Amend -')
                    amendment_name = 'Amendment ' + offered_by.strip()
                elif 'Conference committee report now available' in row_text:
                    amendment_name = 'Conference Committee Report'
                else:
                    amendment_name = row_text.strip()
                bill.add_document(amendment_name, amendment)


    def parse_vote(self, bill, vote_date, vote_chamber, vote_status, vote_url):
        vote_chamber = 'upper' if vote_chamber == 'Senate' else 'lower'
        formats = ['%a %d %b %Y', '%b. %d, %Y, %H:%M %p',
                   '%B %d, %Y, %H:%M %p', '%B %d, %Y, %H %p']
        vote_date = vote_date.replace('.m.', 'm')
        for format in formats:
            try:
                vote_date = datetime.datetime.strptime(vote_date, format)
                break
            except ValueError:
                pass
        else:
            raise ValueError("couldn't parse date: " + vote_date)


        vote_doc, resp = self.urlretrieve(vote_url)

        try:
            subprocess.check_call('timeout 10 abiword --to=ksvote.txt %s' % vote_doc,
                                  shell=True, cwd='/tmp/')
        except subprocess.CalledProcessError:
            # timeout failed, some documents hang abiword
            self.error('abiword hung for longer than 10s on conversion')
            return
        vote_lines = open('/tmp/ksvote.txt').readlines()

        os.remove(vote_doc)

        comma_or_and = re.compile(', |\sand\s')
        comma_or_and_jrsr = re.compile(', (?!Sr.|Jr.)|\sand\s')

        vote = None
        passed = True
        for line in vote_lines:
            totals = re.findall('Yeas (\d+)[;,] Nays (\d+)[;,] (?:Present but not voting|Present and Passing):? (\d+)[;,] (?:Absent or not voting|Absent or Not Voting):? (\d+)',
                                line)
            line = line.strip()
            if totals:
                totals = totals[0]
                yeas = int(totals[0])
                nays = int(totals[1])
                nv = int(totals[2])
                absent = int(totals[3])
                # default passed to true
                vote = Vote(vote_chamber, vote_date, vote_status.strip(),
                            True, yeas, nays, nv+absent)
            elif vote and line.startswith('Yeas:'):
                line = line.split(':', 1)[1].strip()
                for member in comma_or_and.split(line):
                    if member != 'None.':
                        vote.yes(member)
            elif vote and line.startswith('Nays:'):
                line = line.split(':', 1)[1].strip()
                # slightly different vote format if Jr stands alone on a line
                if ', Jr.,' in line:
                    regex = comma_or_and_jrsr
                else:
                    regex = comma_or_and
                for member in regex.split(line):
                    if member != 'None.':
                        vote.no(member)
            elif vote and line.startswith('Present '):
                line = line.split(':', 1)[1].strip()
                for member in comma_or_and.split(line):
                    if member != 'None.':
                        vote.other(member)
            elif vote and line.startswith('Absent or'):
                line = line.split(':', 1)[1].strip()
                for member in comma_or_and.split(line):
                    if member != 'None.':
                        vote.other(member)
            elif 'the motion did not prevail' in line:
                passed = False

        if vote:
            vote['passed'] = passed
            vote.add_source(vote_url)
            bill.add_vote(vote)


def get_doc_link(elem):
    # try ODT then PDF
    link = elem.xpath('.//a[contains(@href, ".odt")]/@href')
    if link:
        return link[0]
    link = elem.xpath('.//a[contains(@href, ".pdf")]/@href')
    if link:
        return link[0]

########NEW FILE########
__FILENAME__ = committees
import re
import os
import datetime
import json
import scrapelib

from billy.scrape.committees import Committee, CommitteeScraper

import ksapi

class KSCommitteeScraper(CommitteeScraper):
    jurisdiction = 'ks'
    latest_only = True

    def scrape(self, chamber, term):
        # some committees, 500, let them go
        self.retry_attempts = 0

        self.scrape_current(chamber, term)

    def scrape_current(self, chamber, term):
        if chamber == 'upper':
            chambers = ['special_committees', 'senate_committees']
        else:
            chambers = ['house_committees']

        committee_request = self.urlopen(ksapi.url + 'ctte/')
        committee_json = json.loads(committee_request)

        for com_type in chambers:
            committees = committee_json['content'][com_type]

            for committee_data in committees:

                # set to joint if we are using the special_committees
                com_chamber = ('joint' if com_type == 'special_committees'
                               else chamber)

                committee = Committee(com_chamber, committee_data['TITLE'])

                com_url = ksapi.url + 'ctte/%s/' % committee_data['KPID']
                try:
                    detail_json = self.urlopen(com_url)
                except scrapelib.HTTPError:
                    self.warning("error fetching committee %s" % com_url)
                    continue
                details = json.loads(detail_json)['content']
                for chair in details['CHAIR']:
                    committee.add_member(chair['FULLNAME'], 'chairman')
                for vicechair in details['VICECHAIR']:
                    committee.add_member(vicechair['FULLNAME'], 'vice-chairman')
                for rankedmember in details['RMMEM']:
                    committee.add_member(rankedmember['FULLNAME'], 'ranking member')
                for member in details['MEMBERS']:
                    committee.add_member(member['FULLNAME'])

                if not committee['members']:
                    self.warning('skipping blank committee %s' %
                                 committee_data['TITLE'])
                else:
                    committee.add_source(com_url)
                    self.save_committee(committee)

########NEW FILE########
__FILENAME__ = ksapi
ksleg = 'http://www.kslegislature.org/li'
url = '%s/api/v7/rev-1/' % ksleg

# in order from sec 10.1 of KLISS doc
action_codes = {
     # motion to acede; appointed
    'ccac_om_370': 'other',
    'efa_fabc_343': 'bill:passed',
    'efa_fabc_342': 'bill:passed',
    'cref_cref_500': 'committee:referred',
    'cref_cref_318': 'committee:referred',  # withdrawn and re-referred
    'gov_avm_336': 'bill:veto_override:passed',
    # change sequence
    'mot_cgo_200': 'other', 'mot_cgo_201': 'other', 'mot_cgo_202': 'other',
    'gov_mg_378': 'governor:vetoed:line-item',
    'fa_fabc_115': 'bill:failed',
    'cr_rsc_292': 'committee:passed:favorable',
    'cr_rsc_276': 'committee:passed',
    'cr_rsc_274': 'committee:passed:unfavorable',
    'cr_rsc_275': 'committee:passed:unfavorable',
    'cr_rsc_273': 'committee:passed:unfavorable',
    'cr_rsc_270': 'bill:substituted',
    # untabled/reconsiderations
    'mot_tab_402': 'other', 'mot_tab_403': 'other', 'mot_tab_401': 'other',
    'mot_tab_404': 'other', 'mot_rcon_303': 'other', 'mot_rcon_302': 'other',
    'ee_enrb_149': 'governor:received',
    'cow_jcow_197': ['bill:passed', 'bill:substituted'],
    'mot_pspn_405': 'other', # postpone - failed
     # other COW actions
    'cow_jcow_211': 'other', 'cow_jcow_210': 'other', 'cow_jcow_214': 'other',
    'cow_jcow_695': 'other', 'cow_jcow_694': 'other', 'cow_jcow_693': 'other',
    'cow_jcow_692': 'other', 'cow_jcow_690': 'other', 'cow_jcow_317': 'other',
    'cow_jcow_718': 'other', 'cow_jcow_719': 'other', 'cow_jcow_720': 'other',
    'cow_jcow_681': 'other', 'cow_jcow_682': 'other', 'cow_jcow_683': 'other',
    'cow_jcow_684': 'other', 'cow_jcow_685': 'other', 'cow_jcow_688': 'other',
    'cow_jcow_689': 'other',
     # withdrawn from consent cal.'
    'ccal_rcc_233': 'other',
    'ccal_faccl_900': 'other',   # consent calendar passed
    'efa_fabc_933': 'bill:passed', # these 3 are 2/3 emergency clause votes...
    'efa_fabc_936': 'bill:failed',
    'efa_fabc_934': 'bill:passed',
    'cref_cref_316': ['bill:withdrawn','committee:referred'],
    'cref_cref_315':  ['bill:withdrawn','committee:referred'],
    'cur_con_374': 'other', # non-concur, conf. com. requested
    'cr_rsc_801': 'committee:passed:unfavorable', # these 3 are appointments..
    'cr_rsc_800': 'committee:passed:favorable',
    'cr_rsc_802': 'committee:passed',
    'ccr_rel_100': 'other',  # conference committee report available
    'misc_bs_100': 'other',  # adopting conf committee report
    'gov_mg_150': 'governor:signed',
    'gov_mg_151': 'other', # law w/o signature
    'gov_mg_154': 'governor:vetoed',
    'cow_jcow_180': 'bill:passed', # COW
    'ar_adj_605': 'other',  # adjourn
    'ee_enrb_888': 'other',   # enrolled and presented to Sec. of State
    'cow_jcow_239': 'bill:passed', # adopted
    'cur_con_875': 'other', # nonconcurrences
    'cur_con_876': 'other',
    'cur_con_873': 'other',
    'fa_fabc_341': 'bill:passed',
    'fa_fabc_340': 'bill:passed',
    'ccac_ccr_860': 'other',
    'efa_fabc_115': 'bill:failed',
    'intro_iopbc_158': 'bill:introduced',
    'intro_iopbc_681': 'bill:introduced',
    'intro_iopbc_251': 'bill:passed',
    'cr_rsc_291': 'committee:passed',
    'fa_fabc_116': 'bill:failed',
    'cow_jcow_728': 'amendment:withdrawn',
    'cow_jcow_727': 'amendment:failed',
    'cow_jcow_726': 'amendment:passed',
    'cow_jcow_725': ['bill:substituted', 'bill:passed'],
    # motions to postpone
    'mot_pspn_404': 'other', 'mot_pspn_403': 'other', 'mot_pspn_402': 'other',
    'fa_fabc_910': 'bill:failed',
    # suspend rules
    'mot_susp_216': 'other', 'mot_susp_214': 'other', 'mot_susp_215': 'other',
    'mot_susp_208': 'other', 'mot_susp_209': 'other',
    'cr_rsc_289': 'committee:passed',
    # conference committee
    'ccac_ccr_375': 'other', 'cur_con_337': 'other',
    'cur_con_336': 'other', 'cur_con_335': 'other',
    'ref_rbc_308': 'committee:referred',
    'ref_rbc_307': 'committee:referred',
    'efa_fabc_352': 'bill:passed',
    'efa_fabc_351': 'bill:passed',
    'intro_ibc_251': 'bill:passed',
    # COW recommendations
    'cow_jcow_705': ['bill:substituted', 'bill:passed'],
    'cow_jcow_704': ['bill:substituted', 'bill:passed'],
    'cow_jcow_707': 'amendment:introduced',
    'cow_jcow_709': 'bill:passed',
    'cow_jcow_708': 'bill:passed',
    # adjourn/recess
    'ar_adj_625': 'other', 'ar_adj_626': 'other',
    'intro_ires_251': 'bill:passed',
    'intro_ires_255': 'bill:passed',
    # engrossed/rengrossed
    'ee_eng_225': 'other', 'ee_eng_227': 'other',
    # referred to COW
    'ref_rbc_235': 'other',
    'cur_iopbc_141': 'committee:referred',
    'mot_wd_126': 'other', #'committee:withdrawn',
    'mot_wd_127': 'other', # withdraw from com- failed
    'mot_wd_125': 'other', # withdraw from com- pending
    # strike from calendar
    'mot_strk_505': 'other', 'mot_strk_504': 'other', 'mot_strk_501': 'other',
    # conf. com report adopted
    'ccac_om_832': 'bill:passed',
    'ccac_ccr_862': 'other', # motion to not adopt conf.com report failed
    'ccac_ccr_863': 'bill:failed', # failed in conf.com, report not adopted
    'ccac_ccr_865': 'other', # motion to not adopt conf.com report failed
    'ccac_ccr_867': 'other', # agree to disagree on conf. com report
    # passed over
    'cow_jcow_201': 'other', 'cow_jcow_202': 'other', 'cow_jcow_203': 'other',
    'ccac_cc_377': 'other', # conf committee changed member
    'ee_enrb_226': 'other', # Enrolled
    # veto overrides
    'gov_avm_885': 'bill:veto_override:failed',
    'gov_avm_887': 'bill:veto_override:passed',
    'ref_rsc_312': 'committee:referred',
    # more COW actions
    'cow_jcow_903': 'other', 'cow_jcow_902': 'other', 'cow_jcow_901': 'other',
    'cow_jcow_905': 'other',
    # no motion to veto override (count as failure?)
    'gov_avm_128': 'bill:veto_override:failed',
    'gov_avm_129': 'bill:veto_override:failed',
    'cow_jcow_191': 'bill:passed',
    'cow_jcow_192': 'bill:passed',
    'cow_jcow_195': 'other', # com. report adopted
    'cow_jcow_196': ['bill:passed', 'bill:substituted'],
    'gov_avm_125': 'bill:veto_override:failed',
    'mot_ref_102': 'committee:referred',
    'mot_ref_105': 'other',  # not referred to committee
    'cref_cref_551': 'committee:referred',
    'cref_cref_552': 'committee:referred',
    'mot_apt_301': 'other',  # 20 days in committee, returned to senate
    'ccac_om_878': 'other',  # Motion to accede failed
    'efa_fabc_925': ['bill:passed', 'bill:substituted'],
    'efa_fabc_926': ['bill:passed', 'bill:substituted'],
    'efa_fabc_923': ['bill:passed', 'bill:substituted'],
    'efa_fabc_922': ['bill:passed', 'bill:substituted'],
    'fa_fabc_105': ['bill:failed', 'bill:substituted'],
    'fa_fabc_104': 'bill:failed',
    'intro_ibc_157': 'bill:introduced',
    'intro_ibc_156': 'bill:filed',
    'fa_fabc_905': 'bill:passed',
    'intro_ires_681': 'bill:introduced',
    'cref_cref_290': 'committee:referred',
    'fa_fabc_352': 'bill:passed',
    'ccac_ccr_145': 'bill:failed',
    'fa_fabc_351': 'bill:passed',
    # motion to move to general orders
    'mot_adv_303': 'other', 'mot_adv_302': 'other', 'mot_adv_301': 'other',
    'efa_fabc_106': ['bill:failed', 'bill:substituted'],
    'efa_fabc_105': ['bill:failed', 'bill:substituted'],
    'efa_fabc_104': 'bill:failed',
    'ccac_ccr_833': 'bill:failed',
    'ref_rbc_310': 'committee:referred',
    'cr_rsc_283': 'committee:passed:favorable',
    'cr_rsc_282': 'committee:passed:favorable',
    'cr_rsc_281': 'committee:passed:favorable',
    'cr_rsc_287': 'committee:passed:favorable',
    'cr_rsc_286': 'committee:passed:favorable',
    'cr_rsc_285': 'committee:passed:favorable',
    'ref_rbc_500': 'committee:referred',
    'cr_rsc_288': 'committee:passed',
    # Conf. Com. reports
    'ccac_ccr_883': 'other', 'ccac_ccr_880': 'other', 'ccac_ccr_881': 'other',
    'cow_jcow_712': ['bill:passed', 'bill:substituted'],
    'cow_jcow_710': ['bill:passed', 'bill:substituted'],
    'cow_jcow_711': ['bill:passed', 'bill:substituted'],
    'cow_jcow_716': 'other',
    'fa_fabc_925': 'bill:passed',
    'fa_fabc_924': 'bill:passed',
    'fa_fabc_926': 'bill:failed',
    'fa_fabc_921': ['bill:passed', 'bill:substituted'],
    'fa_fabc_920': ['bill:passed', 'bill:substituted'],
    'fa_fabc_923': ['bill:passed', 'bill:substituted'],
    'fa_fabc_922': ['bill:passed', 'bill:substituted'],
    'cr_rsc_821': 'committee:passed:unfavorable',
    'cow_jcow_305': 'committee:referred',
    'cow_jcow_304': 'committee:referred',
    'gov_avm_349': 'bill:veto_override:failed',
    'intro_ibc_681': 'bill:introduced',
    'dss_627': 'other',
    'mot_susp_203': 'other',
    'mot_susp_202': 'other',
    'mot_susp_206': 'other',
    'cur_con_101': 'other', # concur. failed
    'cur_om_141': 'committee:referred',
    'misc_he_200': 'other', # hearing
    # Died on Calendar
    'dead_conf_101': 'other',  # died in conference
    'dead_cal_201': 'other',
    'dead_cal_202': 'other',
    'dead_cal_203': 'other',
    'dead_com_301': 'committee:failed',
    'dead_com_302': 'committee:failed',
    'dead_com_303': 'committee:failed',
    'dead_go_302': 'other', # died on general order
    'dead_go_401': 'other', # died on general order
    'dead_go_402': 'other', # died on general order
    'kill_reso_100': 'bill:failed',
    'kill_reso_200': 'bill:failed',
    }

########NEW FILE########
__FILENAME__ = legislators
from billy.scrape.legislators import LegislatorScraper, Legislator

from . import ksapi
import json


class KSLegislatorScraper(LegislatorScraper):
    jurisdiction = 'ks'

    def scrape(self, term, chambers):
        content = json.loads(self.urlopen(ksapi.url + 'members/'))['content']
        if 'upper' in chambers:
            for member in content['senate_members']:
                self.get_member(term, 'upper', member['KPID'])
        if 'lower' in chambers:
            for member in content['house_members']:
                self.get_member(term, 'lower', member['KPID'])


    def get_member(self, term, chamber, kpid):
        url = '%smembers/%s' % (ksapi.url, kpid)
        content = json.loads(self.urlopen(url))['content']

        party = content['PARTY']
        if party == 'Democrat':
            party = 'Democratic'

        slug = {'2013-2014': 'b2013_14'}[term]
        leg_url = 'http://www.kslegislature.org/li/%s/members/%s/' % (slug, kpid)
        photo_url = 'http://www.kslegislature.org/li/m/images/pics/%s.jpg' % kpid
        #photo = legislator_page.xpath('//img[@class="profile-picture"]/@src')[0]

        legislator = Legislator(term, chamber, str(content['DISTRICT']),
                                content['FULLNAME'], email=content['EMAIL'],
                                party=party, url=leg_url, photo_url=photo_url,
                                occupation=content['OCCUPATION'],
                               )

        # just do office address for now, can get others from api
        if content['OFFICENUM']:
            address = ('Kansas House of Representatives\n'
                       'Docking State Office Building\n'
                       '901 SW Harrison Street\n'
                       'Topeka, KS 66612')
        else:
            address = ('Room %s\n'
                       'Kansas State Capitol Building\n'
                       '300 SW 10th St.\n'
                       'Topeka, KS 66612') % content['OFFICENUM']
        legislator.add_office('capitol', 'Capitol Office',
                              phone=content['OFFPH'] or None,
                              address=address)

        legislator.add_source(url)
        self.save_legislator(legislator)

########NEW FILE########
__FILENAME__ = bills
import re
import datetime
from collections import defaultdict

from billy.scrape.bills import BillScraper, Bill

import lxml.html


def chamber_abbr(chamber):
    if chamber == 'upper':
        return 'S'
    else:
        return 'H'


def session_url(session):
    return "http://www.lrc.ky.gov/record/%s/" % session[2:]


class KYBillScraper(BillScraper):
    jurisdiction = 'ky'

    _subjects = defaultdict(list)

    def scrape_subjects(self, session):
        if self._subjects:
            return

        url = session_url(session) + 'indexhd.htm'
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        for subj_link in doc.xpath('//a[contains(@href, ".htm")]/@href'):
            # subject links are 4 numbers
            if re.match('\d{4}', subj_link):
                subj_html = self.urlopen(session_url(session) + subj_link)
                sdoc = lxml.html.fromstring(subj_html)
                subject = sdoc.xpath('//a[@name="TopOfPage"]/text()')[0]
                for bill in sdoc.xpath('//table[@id="table2"]//a/text()'):
                    self._subjects[bill.replace(' ', '')].append(subject)


    def scrape(self, chamber, session):
        self.scrape_subjects(session)
        self.scrape_session(chamber, session)
        for sub in self.metadata['session_details'][session].get('sub_sessions', []):
            self.scrape_session(chamber, sub)

    def scrape_session(self, chamber, session):
        bill_url = session_url(session) + "bills_%s.htm" % chamber_abbr(chamber)
        self.scrape_bill_list(chamber, session, bill_url)

        resolution_url = session_url(session) + "res_%s.htm" % (
            chamber_abbr(chamber))
        self.scrape_bill_list(chamber, session, resolution_url)

    def scrape_bill_list(self, chamber, session, url):
        bill_abbr = None
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        for link in page.xpath("//a"):
            if re.search(r"\d{1,4}\.htm", link.attrib.get('href', '')):
                bill_id = link.text

                match = re.match(r'([A-Z]+)\s+\d+', link.text)
                if match:
                    bill_abbr = match.group(1)
                    bill_id = bill_id.replace(' ', '')
                else:
                    bill_id = bill_abbr + bill_id

                self.parse_bill(chamber, session, bill_id,
                                link.attrib['href'])

    def parse_bill(self, chamber, session, bill_id, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        try:
            short_bill_id = re.sub(r'(H|S)([JC])R', r'\1\2', bill_id)

            version_link = page.xpath("//a[contains(@href, '%s/bill.doc')]" % short_bill_id)[0]
        except IndexError:
            # Bill withdrawn
            return

        pars = version_link.xpath("following-sibling::p")
        if len(pars) == 2:
            title = pars[0].xpath("string()")
            action_p = pars[1]
        else:
            title = pars[0].getprevious().tail
            if not title:
                self.warning('walking backwards to get bill title, error prone!')
                title = pars[0].getprevious().getprevious()
                while not title.tail:
                    title = title.getprevious()
                title = title.tail
                self.warning('got title the dangerous way: %s' % title)
            action_p = pars[0]

        title = re.sub(ur'[\s\xa0]+', ' ', title).strip()

        if 'CR' in bill_id:
            bill_type = 'concurrent resolution'
        elif 'JR' in bill_id:
            bill_type = 'joint resolution'
        elif 'R' in bill_id:
            bill_type = 'resolution'
        else:
            bill_type = 'bill'

        bill = Bill(session, chamber, bill_id, title, type=bill_type)
        bill['subjects'] = self._subjects[bill_id]
        bill.add_source(url)

        bill.add_version("Most Recent Version",
                         version_link.attrib['href'],
                         mimetype='application/msword')

        for link in page.xpath("//a[contains(@href, 'legislator/')]"):
            bill.add_sponsor('primary', link.text.strip())

        for line in action_p.xpath("string()").split("\n"):
            action = line.strip()
            if (not action or action == 'last action' or
                'Prefiled' in action or 'vetoed' in action):
                continue

            # add year onto date
            action_date = "%s %s" % (action.split('-')[0],
                                     session[0:4])
            action_date = datetime.datetime.strptime(
                action_date, '%b %d %Y')

            action = '-'.join(action.split('-')[1:])

            if action.endswith('House') or action.endswith('(H)'):
                actor = 'lower'
            elif action.endswith('Senate') or action.endswith('(S)'):
                actor = 'upper'
            else:
                actor = chamber

            atype = []
            if action.startswith('introduced in'):
                atype.append('bill:introduced')
                if '; to ' in action:
                    atype.append('committee:referred')
            elif action.startswith('signed by Governor'):
                atype.append('governor:signed')
            elif re.match(r'^to [A-Z]', action):
                atype.append('committee:referred')
            elif action == 'adopted by voice vote':
                atype.append('bill:passed')

            if '1st reading' in action:
                atype.append('bill:reading:1')
            if '3rd reading' in action:
                atype.append('bill:reading:3')
            if '2nd reading' in action:
                atype.append('bill:reading:2')

            if 'R' in bill_id and 'adopted by voice vote' in action:
                atype.append('bill:passed')

            amendment_re = (r'floor amendments?( \([a-z\d\-]+\))*'
                            r'( and \([a-z\d\-]+\))? filed')
            if re.search(amendment_re, action):
                atype.append('amendment:introduced')

            if not atype:
                atype = ['other']

            bill.add_action(actor, action, action_date, type=atype)

        try:
            votes_link = page.xpath(
                "//a[contains(@href, 'vote_history.pdf')]")[0]
            bill.add_document("Vote History",
                              votes_link.attrib['href'])
        except IndexError:
            # No votes
            pass

        # Ugly Hack Alert!
        # find actions before introduction date and subtract 1 from the year
        # if the date is after introduction
        intro_date = None
        for i, action in enumerate(bill['actions']):
            if 'bill:introduced' in action['type']:
                intro_date = action['date']
                break
        for action in bill['actions'][:i]:
            if action['date'] > intro_date:
                action['date'] = action['date'].replace(year=action['date'].year-1)
                self.debug('corrected year for %s', action['action'])

        self.save_bill(bill)

########NEW FILE########
__FILENAME__ = committees
import re

from billy.scrape import NoDataForPeriod
from billy.scrape.committees import CommitteeScraper, Committee

import lxml.html


class KYCommitteeScraper(CommitteeScraper):
    jurisdiction = 'ky'
    latest_only = True

    def scrape(self, chamber, term):

        if chamber == 'upper':
            url = "http://www.lrc.ky.gov/org_adm/committe/standing_senate.htm"
            # also invoke joint scraper
            self.scrape('joint', term)
        elif chamber == 'lower':
            url = "http://www.lrc.ky.gov/org_adm/committe/standing_house.htm"
        else:
            url = "http://www.lrc.ky.gov/org_adm/committe/interim.htm"
            chamber = 'joint'

        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        links = []

        cttypages = [
            "//a[contains(@href, 'standing/')]",
            "//a[contains(@href, 'interim')]"
        ]

        for exp in cttypages:
            linkz = page.xpath(exp)
            links = links + linkz

        for link in links:
            name = re.sub(r'\s+\((H|S)\)$', '', link.text).strip().title()
            comm = Committee(chamber, name)
            comm_url = link.attrib['href'].replace(
                'home.htm', 'members.htm')
            self.scrape_members(comm, comm_url)
            if comm['members']:
                self.save_committee(comm)

    def scrape_members(self, comm, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        comm.add_source(url)

        for link in page.xpath("//a[contains(@href, 'Legislator')]"):
            name = re.sub(r'^(Rep\.|Sen\.) ', '', link.text).strip()
            if not link.tail or not link.tail.strip():
                role = 'member'
            elif link.tail.strip() == '[Chair]':
                role = 'chair'
            elif link.tail.strip() == '[Co-Chair]':
                role = 'co-chair'
            elif link.tail.strip() == '[Vice Chair]':
                role = 'vice chair'
            else:
                raise Exception("unexpected position: %s" % link.tail)
            comm.add_member(name, role=role)

########NEW FILE########
__FILENAME__ = events
import datetime

from billy.scrape import NoDataForPeriod
from billy.scrape.events import EventScraper, Event

import lxml.html
import pytz


class KYEventScraper(EventScraper):
    jurisdiction = 'ky'

    _tz = pytz.timezone('US/Eastern')

    def scrape(self, chamber, session):
        url = "http://www.lrc.ky.gov/legislative_calendar/index.aspx"
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)

        for div in page.xpath("//div[@style = 'MARGIN-LEFT: 20px']"):
            date = div.xpath("string(../../span[1])").strip()

            try:
                time, location = div.xpath("string(span[1])").split(',')
            except ValueError:
                # No meetings
                continue

            if ':' not in time:
                self.warning('skipping event with invalid time: %s', time)
                continue
            when = "%s %s" % (date, time)
            try:
                when = datetime.datetime.strptime(when, "%A, %B %d, %Y %I:%M%p")
            except ValueError:
                when = datetime.datetime.strptime(when, "%A, %B %d, %Y %I:%M %p")

            when = self._tz.localize(when)

            desc = div.xpath("string(span[2])").strip()
            agenda = div.xpath("string(span[3])").strip()
            # XXX: Process `agenda' for related bills.
            event = Event(session, when, 'committee:meeting',
                          desc, location=location)
            event.add_source(url)

            # desc is actually the ctty name.
            event.add_participant('host', desc, 'committee',
                                  chamber=chamber)

            self.save_event(event)

########NEW FILE########
__FILENAME__ = legislators
import re
from collections import defaultdict

from billy.scrape.legislators import Legislator, LegislatorScraper

import lxml.html


class KYLegislatorScraper(LegislatorScraper):
    jurisdiction = 'ky'
    latest_only = True

    def scrape(self, chamber, year):

        if chamber == 'upper':
            leg_list_url = 'http://www.lrc.ky.gov/senate/senmembers.htm'
        else:
            leg_list_url = 'http://www.lrc.ky.gov/house/hsemembers.htm'

        page = self.urlopen(leg_list_url)
        page = lxml.html.fromstring(page)

        for link in page.xpath('//a[@onmouseout="hidePicture();"]'):
            self.scrape_member(chamber, year, link.get('href'))

    def scrape_office_info(self, url):
        ret = {}
        legislator_page = self.urlopen(url)
        legislator_page = lxml.html.fromstring(legislator_page)
        legislator_page.make_links_absolute(url)
        info = legislator_page.xpath("//table//span")
        for span in info:
            elements = span.xpath("./*")
            if len(elements) < 1:
                continue
            if elements[0].tag != "b":
                continue
            txt = elements[0].text_content().strip()

            if txt == "Bio" or \
               "committees" in txt.lower() or \
               "service" in txt.lower() or \
               txt == "":
                continue

            def _handle_phone(obj):
                ret = defaultdict(list)
                for x in obj.xpath(".//*")[:-1]:
                    phone = x.tail.strip()
                    obj = phone.split(":", 1)
                    if len(obj) != 2:
                        continue
                    typ, number = obj
                    typ, number = typ.strip(), number.strip()
                    ret[typ].append(number)
                return ret

            def _handle_address(obj):
                addr = " ".join([x.tail or "" for x in obj.xpath(".//*")[1:]])
                return [addr.strip()]

            def _handle_emails(obj):
                ret = []
                emails = obj.xpath(".//a[contains(@href, 'mailto')]")
                if len(emails) < 1:
                    return []
                for email in emails:
                    _, efax = email.attrib['href'].split(":", 1)
                    ret.append(efax)
                return ret

            handlers = {
                "Mailing Address": _handle_address,
                "Frankfort Address(es)": _handle_address,
                "Phone Number(s)": _handle_phone,
                "Email Address(es)": _handle_emails
            }

            try:
                handler = handlers[txt]
                ret[txt] = handler(span)
            except KeyError:
                pass

        return ret

    def scrape_member(self, chamber, year, member_url):
        member_page = self.urlopen(member_url)
        doc = lxml.html.fromstring(member_page)

        photo_url = doc.xpath('//div[@id="bioImage"]/img/@src')[0]
        name_pieces = doc.xpath('//span[@id="name"]/text()')[0].split()
        full_name = ' '.join(name_pieces[1:-1]).strip()

        party = name_pieces[-1]
        if party == '(R)':
            party = 'Republican'
        elif party == '(D)':
            party = 'Democratic'
        elif party == '(I)':
            party = 'Independent'

        district = doc.xpath('//span[@id="districtHeader"]/text()')[0].split()[-1]

        leg = Legislator(year, chamber, district, full_name, party=party,
                         photo_url=photo_url, url=member_url)
        leg.add_source(member_url)

        address = '\n'.join(doc.xpath('//div[@id="FrankfortAddresses"]//span[@class="bioText"]/text()'))
        phone = None
        phone_numbers = doc.xpath('//div[@id="PhoneNumbers"]//span[@class="bioText"]/text()')
        for num in phone_numbers:
            if num.startswith('Annex: '):
                phone = num.replace('Annex: ', '')

        leg.add_office('capitol', 'Capitol Office', address=address,
                       phone=phone)

        self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = votes
import sys

from billy.scrape.votes import VoteScraper, Vote

#import Image
#import ImageChops


#def crop(image, threshold=0.99):
#    """
#    Crop the leftmost/topmost rows/cols with percentage of white pixel
#    less than threshold.
#    """
#    bbox = [0, 0, image.size[0], image.size[1]]

#    for x in xrange(0, image.size[0]):
#        row = image.crop((x, 0, x + 1, image.size[1]))
#        first = row.getcolors()[0]

#        if first[1] == (255, 255, 255):
#            if first[0] / float(image.size[1]) < threshold:
#                bbox[0] = x
#                break

#    for y in xrange(0, image.size[1]):
#        row = image.crop((0, y, image.size[0], y + 1))
#        first = row.getcolors()[0]

#        if first[1] == (255, 255, 255):
#            if first[0] / float(image.size[0]) < threshold:
#                bbox[1] = y
#                break

#    return image.crop(bbox)


#def get_rect_color(image, rect):
#    box = image.crop(rect)
#    colors = box.getcolors()

#    if len(colors) > 1:
#        raise ValueError("Not a solid color: %r" % colors)

#    return colors[0][1]


#def parse_votes(filename):
#    "Extract votes from roll call images from the KY Senate."
#    image = Image.open(filename)

#    # The vote pages have a variable amount of whitespace around the
#    # top and left that we want to strip
#    image = crop(image)

#    votes = []

#    cols = [365, 885, 1410]
#    for col_x in cols:
#        for row in xrange(0, 13):
#            if col_x == 1410 and row == 12:
#                # Thrid column only has 11 entries
#                continue

#            y = 395 + 50 * row

#            yes_rect = (col_x, y, col_x + 10, y + 15)
#            if get_rect_color(image, yes_rect) == (0, 0, 0):
#                yes = True
#            else:
#                yes = False

#            no_rect = (col_x + 35, y, col_x + 45, y + 15)
#            if get_rect_color(image, no_rect) == (0, 0, 0):
#                no = True
#            else:
#                no = False

#            if yes and no:
#                raise ValueError("Double vote")

#            if yes:
#                votes.append('yes')
#            elif no:
#                votes.append('no')
#            else:
#                votes.append('other')

#    return votes


class KYVoteScraper(VoteScraper):
    jurisdiction = 'ky'

    def scrape(self, chamber, session):
        pass

########NEW FILE########
__FILENAME__ = bills
from billy.scrape import ScrapeError
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote
from billy.scrape.utils import pdf_to_lxml

import datetime as dt
import lxml.html
import scrapelib
import tempfile
import os
import re


URL = "http://www.legis.la.gov/Legis/BillSearchListQ.aspx?r=%s1*"

bill_types = {
    "upper": ["SB", "SCR"],
    "lower": ["HB", "HCR"]
}


class LABillScraper(BillScraper):
    jurisdiction = 'la'

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def do_post_back(self, page, event_target, event_argument):
        form = page.xpath("//form[@id='aspnetForm']")[0]
        block = {name: value for name, value in [(obj.name, obj.value)
                    for obj in form.xpath(".//input")]}
        block['__EVENTTARGET'] = event_target
        block['__EVENTARGUMENT'] = event_argument
        ret = lxml.html.fromstring(self.urlopen(form.action,
                                   method=form.method,
                                   body=block))

        ret.make_links_absolute(form.action)
        return ret

    def bill_pages(self, bill_type):
        page = self.lxmlize(URL % (bill_type))
        yield page

        while True:
            hrefs = page.xpath("//a[text()=' > ']")
            if hrefs == [] or "disabled" in hrefs[0].attrib:
                return

            href = hrefs[0].attrib['href']
            tokens = re.match(".*\(\'(?P<token>.*)\',\'.*", href).groupdict()

            page = self.do_post_back(
                page,
                tokens['token'],
                ""
            )
            if page:
                yield page

    def scrape_bare_page(self, url):
        page = self.lxmlize(url)
        return page.xpath("//a")

    def scrape(self, chamber, session):
        for bill_type in bill_types[chamber]:
            for bill_page in self.bill_pages(bill_type):
                for bill in bill_page.xpath(
                        "//a[contains(@href, 'BillInfo.aspx')]"):
                    self.scrape_bill_page(chamber,
                                          session,
                                          bill.attrib['href'],
                                          bill_type)


    def get_one_xpath(self, page, xpath):
        ret = page.xpath(xpath)
        if len(ret) != 1:
            raise Exception
        return ret[0]

    def scrape_votes(self, bill, url):
        text = self.urlopen(url)
        page = lxml.html.fromstring(text)
        page.make_links_absolute(url)

        for a in page.xpath("//a[contains(@href, 'ViewDocument.aspx')]"):
            self.scrape_vote(bill, a.text, a.attrib['href'])

    def scrape_vote(self, bill, name, url):
        match = re.match('^(Senate|House) Vote on [^,]*,(.*)$', name)

        if not match:
            return

        chamber = {'Senate': 'upper', 'House': 'lower'}[match.group(1)]
        motion = match.group(2).strip()

        if motion.startswith('FINAL PASSAGE'):
            type = 'passage'
        elif motion.startswith('AMENDMENT'):
            type = 'amendment'
        elif 'ON 3RD READING' in motion:
            type = 'reading:3'
        else:
            type = 'other'

        vote = Vote(chamber, None, motion, None,
                    None, None, None)
        vote['type'] = type
        vote.add_source(url)

        (fd, temp_path) = tempfile.mkstemp()
        self.urlretrieve(url, temp_path)

        html = pdf_to_lxml(temp_path)
        os.close(fd)
        os.remove(temp_path)

        vote_type = None
        total_re = re.compile('^Total--(\d+)$')
        body = html.xpath('string(/html/body)')

        date_match = re.search('Date: (\d{1,2}/\d{1,2}/\d{4})', body)
        try:
            date = date_match.group(1)
        except AttributeError:
            self.warning("BAD VOTE: date error")
            return

        vote['date'] = dt.datetime.strptime(date, '%m/%d/%Y')

        for line in body.replace(u'\xa0', '\n').split('\n'):
            line = line.replace('&nbsp;', '').strip()
            if not line:
                continue

            if line in ('YEAS', 'NAYS', 'ABSENT'):
                vote_type = {'YEAS': 'yes', 'NAYS': 'no',
                             'ABSENT': 'other'}[line]
            elif line in ('Total', '--'):
                vote_type = None
            elif vote_type:
                match = total_re.match(line)
                if match:
                    vote['%s_count' % vote_type] = int(match.group(1))
                elif vote_type == 'yes':
                    vote.yes(line)
                elif vote_type == 'no':
                    vote.no(line)
                elif vote_type == 'other':
                    vote.other(line)

        # tally counts
        vote['yes_count'] = len(vote['yes_votes'])
        vote['no_count'] = len(vote['no_votes'])
        vote['other_count'] = len(vote['other_votes'])

        # The PDFs oddly don't say whether a vote passed or failed.
        # Hopefully passage just requires yes_votes > not_yes_votes
        if vote['yes_count'] > (vote['no_count'] + vote['other_count']):
            vote['passed'] = True
        else:
            vote['passed'] = False

        bill.add_vote(vote)

    def scrape_bill_page(self, chamber, session, bill_url, bill_type):
        page = self.lxmlize(bill_url)
        author = self.get_one_xpath(
            page,
            "//a[@id='ctl00_PageBody_LinkAuthor']/text()"
        )

        sbp = lambda x: self.scrape_bare_page(page.xpath(
            "//a[contains(text(), '%s')]" % (x))[0].attrib['href'])

        authors = [x.text for x in sbp("Authors")]

        try:
            digests = sbp("Digests")
        except IndexError:
            digests = []

        try:
            versions = sbp("Text")
        except IndexError:
            versions = []

        title = page.xpath(
            "//span[@id='ctl00_PageBody_LabelShortTitle']/text()")[0]
        actions = page.xpath(
            "//div[@id='ctl00_PageBody_PanelBillInfo']/"
            "/table[@style='font-size:small']/tr")

        bill_id = page.xpath(
            "//span[@id='ctl00_PageBody_LabelBillID']/text()")[0]

        bill_type = {"B": "bill", "CR": "concurrent resolution"}[bill_type[1:]]
        bill = Bill(session, chamber, bill_id, title, type=bill_type)
        bill.add_source(bill_url)

        authors.remove(author)
        bill.add_sponsor('primary', author)
        for author in authors:
            bill.add_sponsor('cosponsor', author)

        for digest in digests:
            bill.add_document(digest.text,
                              digest.attrib['href'],
                              mimetype="application/pdf")

        for version in versions:
            bill.add_version(version.text,
                             version.attrib['href'],
                             mimetype="application/pdf")

        flags = {
            "prefiled": ["bill:filed"],
            "referred to the committee": ["committee:referred"],
            "sent to the house": ['bill:passed'],
            "ordered to the senate": ['bill:passed'],
        }

        try:
            votes_link = page.xpath("//a[text() = 'Votes']")[0]
            self.scrape_votes(bill, votes_link.attrib['href'])
        except IndexError:
            # Some bills don't have any votes
            pass


        for action in actions:
            date, chamber, page, text = [x.text for x in action.xpath(".//td")]
            date += "/%s" % (session)  # Session is April --> June. Prefiles
            # look like they're in January at earliest.
            date = dt.datetime.strptime(date, "%m/%d/%Y")
            chamber = {"S": "upper", "H": "lower", "J": 'joint'}[chamber]

            cat = []
            for flag in flags:
                if flag in text.lower():
                    cat += flags[flag]

            if cat == []:
                cat = ["other"]
            bill.add_action(chamber, text, date, cat)

        self.save_bill(bill)

########NEW FILE########
__FILENAME__ = committees
from billy.scrape import NoDataForPeriod
from billy.scrape.committees import CommitteeScraper, Committee

import lxml.html


class LACommitteeScraper(CommitteeScraper):
    jurisdiction = 'la'

    def scrape(self, chamber, term):
        if term != self.metadata['terms'][-1]['name']:
            raise NoDataForPeriod(term)

        if chamber == 'upper':
            self.scrape_senate()
        else:
            self.scrape_house()

    def scrape_senate(self):
        committee_types = {
            'Standing': 'http://senate.legis.state.la.us/Committees/default.asp',
            'Select': 'http://senate.louisiana.gov/committees/default.asp?type=Select'
        }
        
        for name, url in committee_types.items():
            text = self.urlopen(url)
            page = lxml.html.fromstring(text)
            page.make_links_absolute(url)

            links = page.xpath('//b[contains(text(), "{0} Committees")]'
                               '/../following-sibling::font/ul/li/a'.format(name))

            for link in links:
                name = link.xpath('string()')
                url = link.attrib['href']
                self.scrape_senate_committee(name, url)
        

    def scrape_senate_committee(self, name, url):
        url = url.replace('Default.asp', 'Assignments.asp')

        committee = Committee('upper', name)
        committee.add_source(url)

        text = self.urlopen(url)
        page = lxml.html.fromstring(text)

        links = page.xpath('//table[@bordercolor="#EBEAEC"]/tr/td/font/a')

        for link in links:
            role = "member"
            if link.tail:
                role = link.tail.strip().strip("() ")

            name = link.xpath('string()')
            name = name.replace('Senator ', '').strip()

            committee.add_member(name, role)

        self.save_committee(committee)

    def scrape_house(self):
        url = "http://house.louisiana.gov/H_Reps/H_Reps_CmtesFull.asp"
        comm_cache = {}
        text = self.urlopen(url)
        page = lxml.html.fromstring(text)

        for row in page.xpath("//table[@bordercolorlight='#EAEAEA']/tr"):
            cells = row.xpath('td')

            name = cells[0].xpath('string()').strip()

            if name.startswith('Vacant'):
                continue

            font = cells[1]
            committees = []

            if font is not None and font.text:
                committees.append(font.text.strip())
            for br in font.xpath('br'):
                if br.text:
                    committees.append(br.text.strip())
                if br.tail:
                    committees.append(br.tail)

            for comm_name in committees:
                mtype = 'member'
                if comm_name.endswith(', Chairman'):
                    mtype = 'chairman'
                    comm_name = comm_name.replace(', Chairman', '')
                elif comm_name.endswith(', Co-Chairmain'):
                    mtype = 'co-chairmain'
                    comm_name = comm_name.replace(', Co-Chairmain', '')
                elif comm_name.endswith(', Vice Chair'):
                    mtype = 'vice chair'
                    comm_name = comm_name.replace(', Vice Chair', '')
                elif comm_name.endswith(', Ex Officio'):
                    mtype = 'ex officio'
                    comm_name = comm_name.replace(', Ex Officio', '')
                elif comm_name.endswith(", Interim Member"):
                    mtype = 'interim'
                    comm_name = comm_name.replace(", Interim Member", "")


                if comm_name.startswith('Joint'):
                    chamber = 'joint'
                else:
                    chamber = 'lower'

                try:
                    committee = comm_cache[comm_name]
                except KeyError:
                    if comm_name.strip() == "":
                        continue

                    committee = Committee(chamber, comm_name)
                    committee.add_source(url)
                    comm_cache[comm_name] = committee

                committee.add_member(name, mtype)

        special = self.scrape_house_special(comm_cache.keys())
        for name, comm in special.items():
            comm_cache[name] = comm

        for committee in comm_cache.values():
            self.save_committee(committee)
            
    def scrape_house_special(self, scraped_committees):
        url = 'http://house.louisiana.gov/H_Reps/H_Reps_SpecialCmtes.asp'
        text = self.urlopen(url)
        page = lxml.html.fromstring(text)
        page.make_links_absolute('http://house.louisiana.gov')
        
        committees = {}
        for el in page.xpath("//a[contains(@href,'../H_Cmtes/')]"):
            comm_name = el.xpath('normalize-space(string())')
            comm_name = self.normalize_committee_name(comm_name)
            
            # skip committees that have already been scraped from 
            # http://house.louisiana.gov/H_Reps/H_Reps_CmtesFull.asp
            if comm_name not in scraped_committees:    
                comm_url = el.get('href').replace('../','')
                committees[comm_name] = comm_url

        for name, url in committees.items():
            chamber = 'joint' if name.startswith('Joint') else 'lower'
            committee = Committee(chamber, name)
            committee.add_source(url)
            
            text = self.urlopen(url)
            page = lxml.html.fromstring(text)
            page.make_links_absolute('http://house.louisiana.gov')

            for row in page.xpath('//table[@id="table1"]//tbody/tr'):
                member_info = row.xpath('./td')
                mname = member_info[0].xpath('normalize-space(string())')
                mtype = member_info[1].xpath('normalize-space(string())')
                if mtype == 'Chairman':
                    mtype = 'chairman'
                elif mtype  == 'Co-Chairmain':
                    mtype = 'co-chairmain'
                elif mtype ==  'Vice Chair':
                    mtype = 'vice chair'
                elif mtype  == 'Ex Officio':
                    mtype = 'ex officio'
                elif mtype == 'Interim Member':
                    mtype = 'interim'
                else:
                    mtype = 'member'
                committee.add_member(mname, mtype)
            
            committees[name] = committee
        
        return committees
        
    def normalize_committee_name(self, name):
        committees = {
            'House Executive Cmte': 'House Executive Committee',
            'Atchafalaya Basin Oversight': 'Atchafalaya Basin Program Oversight Committee',
            'Homeland Security': 'House Select Committee on Homeland Security',
            'Hurricane Recovery': 'Select Committee on Hurricane Recovery',
            'Legislative Budgetary Control': 'Legislative Budgetary Control Council',
            'Military and Veterans Affairs': 'Special Committee on Military and Veterans Affairs'
        }
        return committees[name] if name in committees else name

########NEW FILE########
__FILENAME__ = events
import re
import pytz
import datetime

from billy.scrape import NoDataForPeriod
from billy.scrape.events import EventScraper, Event

import lxml.html


def parse_datetime(s, year):
    dt = None

    match = re.match(r"[A-Z][a-z]{2,2} \d+, \d\d:\d\d (AM|PM)", s)
    if match:
        dt = datetime.datetime.strptime(match.group(0), "%b %d, %I:%M %p")

    # Commented out; unlikely this is correct anymore.
    #match = re.match(r"[A-Z][a-z]{2,2} \d+", s)
    #if match:
    #    dt = datetime.datetime.strptime(match.group(0), "%b %d").date()

    if dt:
        return dt.replace(year=int(year))
    else:
        raise ValueError("Bad date string: %s" % s)


class LAEventScraper(EventScraper):
    jurisdiction = 'la'
    _tz = pytz.timezone('America/Chicago')

    def scrape(self, chamber, session):
        if chamber == 'lower':
            self.scrape_house_weekly_schedule(session)

        self.scrape_committee_schedule(session, chamber)

    def scrape_committee_schedule(self, session, chamber):
        url = "http://www.legis.la.gov/legis/ByCmte.aspx"

        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        for link in page.xpath("//a[contains(@href, 'Agenda.aspx')]"):
            self.scrape_meeting(session, chamber, link.attrib['href'])

    def scrape_bills(self, line):
        ret = []
        for blob in [x.strip() for x in line.split(",")]:
            if (blob[0] in ['H', 'S', 'J'] and
                    blob[1] in ['R', 'M', 'B', 'C']):
                blob = blob.replace("-", "")
                ret.append(blob)
        return ret

    def scrape_meeting(self, session, chamber, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        title ,= page.xpath("//a[@id='linkTitle']//text()")
        date ,= page.xpath("//span[@id='lDate']/text()")
        time ,= page.xpath("//span[@id='lTime']/text()")
        location ,= page.xpath("//span[@id='lLocation']/text()")

        if ("UPON ADJOURNMENT" in time.upper() or
                "UPON  ADJOURNMENT" in time.upper()):
            return

        substs = {
            "AM": ["A.M.", "a.m."],
            "PM": ["P.M.", "p.m."],
        }

        for key, values in substs.items():
            for value in values:
                time = time.replace(value, key)

        try:
            when = datetime.datetime.strptime("%s %s" % (
                date, time
            ), "%B %d, %Y %I:%M %p")
        except ValueError:
            when = datetime.datetime.strptime("%s %s" % (
                date, time
            ), "%B %d, %Y %I:%M")

        # when = self._tz.localize(when)

        description = "Meeting on %s of the %s" % (date, title)
        chambers = {"house": "lower",
                    "senate": "upper",
                    "joint": "joint",}

        for chamber_, normalized in chambers.items():
            if chamber_ in title.lower():
                chamber = normalized
                break
        else:
            return

        event = Event(
            session,
            when,
            'committee:meeting',
            description,
            location=location
        )
        event.add_source(url)

        event.add_participant('host', title, 'committee',
                              chamber=chamber)

        trs = iter(page.xpath("//tr[@valign='top']"))
        next(trs)

        for tr in trs:
            try:
                _, _, bill, whom, descr = tr.xpath("./td")
            except ValueError:
                continue

            bill_title = bill.text_content()

            if "S" in bill_title:
                bill_chamber = "upper"
            elif "H" in bill_title:
                bill_chamber = "lower"
            else:
                continue

            event.add_related_bill(bill_id=bill_title,
                                   description=descr.text_content(),
                                   chamber=bill_chamber,
                                   type='consideration')
        self.save_event(event)

    def scrape_house_weekly_schedule(self, session):
        url = "http://house.louisiana.gov/H_Sched/Hse_Sched_Weekly.htm"

        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        for link in page.xpath("//img[@alt = 'See Agenda in pdf']/.."):
            try:
                guid = link.attrib['href']
            except KeyError:
                continue  # Sometimes we have a dead link. This is only on
                # dead entries.

            committee = link.xpath("string(../../td[1])").strip()

            when_and_where = link.xpath("string(../../td[2])").strip()
            when_and_where = re.sub("\s+", " ", when_and_where).strip()
            if "@" in when_and_where:
                continue  # Contains no time data.

            if when_and_where.strip() == "":
                continue

            info = re.match(
                r"(?P<when>.*) (?P<where>H|C.*-.*?)",
                when_and_where
            ).groupdict()

            when_and_where = info['when']
            location = info['where']

            year = datetime.datetime.now().year
            when = parse_datetime(when_and_where, year)  # We can only scrape
            # when = self._tz.localize(when)

            bills = self.scrape_bills(when_and_where)

            description = 'Committee Meeting: %s' % committee

            event = Event(session, when, 'committee:meeting',
                          description, location=location)
            event.add_source(url)
            event.add_participant('host', committee, 'committee',
                                  chamber='lower')
            event.add_document("Agenda", guid, type='agenda',
                               mimetype="application/pdf")
            for bill in bills:
                event.add_related_bill(bill, description=when_and_where,
                                       type='consideration')
            event['link'] = guid

            self.save_event(event)

########NEW FILE########
__FILENAME__ = legislators
import re

from billy.scrape import NoDataForPeriod
from billy.scrape.legislators import LegislatorScraper, Legislator

import scrapelib
import lxml.html


def xpath_one(el, expr):
    ret = el.xpath(expr)
    if len(ret) != 1:
        print ret
        raise Exception
    return ret[0]


class LALegislatorScraper(LegislatorScraper):
    jurisdiction = 'la'
    latest_only = True

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def scrape_upper_leg_page(self, term, url, who):
        page = self.lxmlize(url)
        info = page.xpath("//td[@bgcolor='#EBEAEC']")
        who = page.xpath("//font[@size='4']")
        who = who[0].text_content()
        who = re.sub("\s+", " ", who)
        who, district = (x.strip() for x in who.rsplit("-", 1))
        who = who.replace("Senator", "").strip()
        district = district.replace("District", "").strip()

        infopane = page.xpath("//table[@cellpadding='3']")
        infos = [x.tail.strip() if x.tail else ""
                 for x in infopane[1].xpath(".//br")]

        keys = ["party", "email", "capitol-office",
                "district-office", "phone", "fax", "staffer"]
        nodes = [[]]
        for node in infos:
            if node == "":
                if nodes[-1] != []:
                    nodes.append([])
                continue
            nodes[-1].append(node)

        data = dict(zip(keys, nodes))

        district_office = "\n".join(data['district-office'])
        capitol_office = "\n".join(data['capitol-office'])

        rundown = infopane[1].xpath("./*")[-1]
        rundown_txt = rundown.text_content()
        parties = {
            "Republican": "Republican",
            "Democrat": "Democratic",
        }

        party = 'other'
        for slug in parties:
            if slug in rundown_txt:
                party = parties[slug]

        if party == 'other':
            raise Exception

        kwargs = {
            "party": party
        }

        leg = Legislator(term,
                         'upper',
                         district,
                         who,
                         **kwargs)


        leg.add_office('district',
                       'District Office',
                       address=district_office)

        leg.add_office('capitol',
                       'Capitol Office',
                       address=capitol_office)

        leg.add_source(url)

        self.save_legislator(leg)

    def scrape_upper(self, chamber, term):
        url = "http://senate.la.gov/Senators/"
        page = self.lxmlize(url)
        table = page.xpath("//table[@width='94%']")[0]
        legs = table.xpath(".//tr//a[contains(@href, 'senate.la.gov')]")
        for leg in legs:
            who = leg.text_content().strip()
            if who == "":
                continue
            self.scrape_upper_leg_page(term, leg.attrib['href'], who)

    def scrape_lower_legislator(self, url, leg_info, term):
        page = self.lxmlize(url)
        photo = xpath_one(page, '//img[@rel="lightbox"]').attrib['src']
        infoblk = xpath_one(page,
                         '//font/b[contains(text(), "CAUCUS/DELEGATION MEMBERSHIP")]')
        infoblk = infoblk.getparent()
        info = infoblk.text_content()
        cty = xpath_one(infoblk, "./b[contains(text(), 'ASSIGNMENTS')]")
        cty = cty.getnext()

        partyblk = filter(lambda x: "District" in x,
                          page.xpath('//p[@align="center"]//text()'))[0]

        party_flags = {
            "Democrat": "Democratic",
            "Republican": "Republican",
            "Independent": "Independent"
        }

        if leg_info['name'].startswith("Vacant"):
            return

        party = 'other'
        for p in party_flags:
            if p in partyblk:
                party = party_flags[p]

        if party == 'other':
            raise Exception

        kwargs = {"url": url,
                  "party": party,
                  "photo_url": photo}

        leg = Legislator(term,
                         'lower',
                         leg_info['dist'],
                         leg_info['name'],
                         **kwargs)

        kwargs = {
            "address": leg_info['office'],
            "phone": leg_info['phone'],
        }

        if leg_info['email'] != "":
            kwargs['email'] = leg_info['email']

        leg.add_office('district',
                       'District Office',
                       **kwargs)

        leg.add_source(url)
        self.save_legislator(leg)

    def scrape_lower(self, chamber, term):
        url = "http://house.louisiana.gov/H_Reps/H_Reps_FullInfo.asp"
        page = self.lxmlize(url)
        meta = ["name", "dist", "office", "phone", "email"]
        for tr in page.xpath("//table[@id='table61']//tr"):
            tds = tr.xpath("./td")
            if tds == []:
                continue

            info = {}
            for i in range(0, len(meta)):
                info[meta[i]] = tds[i].text_content().strip()

            hrp = tr.xpath(
                ".//a[contains(@href, 'H_Reps')]")[0].attrib['href']

            self.scrape_lower_legislator(hrp, info, term)

    def scrape(self, chamber, term):
        if chamber == "upper":
            return self.scrape_upper(chamber, term)
        elif chamber == "lower":
            return self.scrape_lower(chamber, term)

########NEW FILE########
__FILENAME__ = actions
import re
from billy.scrape.actions import Rule, BaseCategorizer

# These are regex patterns that map to action categories.
_categorizer_rules = (
    Rule([u'Amendment #\\S+ \\((?P<legislator>.+?)\\) bundle YES adopted'],
         [u'amendment:passed']),
    Rule([u'(?i)Signed by (the )Governor'], [u'committee:referred']),
    Rule([u'Accompanied (by )?(?P<bill_id>[SH]\\S+)'], []),
    Rule([u'Discharged to the committee on (?P<committees>.+)'],
         [u'committee:referred']),
    Rule([u'(?i)Amendment #\\d+ adopted'], [u'amendment:passed']),
    Rule([u'Amendment #\\d+ \\((?P<legislator>.+?)\\) rejected',
    u'(?i)amendment.+?rejected'],
         [u'amendment:failed']),
    Rule([u'(?is)Amendment \\S+ withdrawn'], [u'amendment:withdrawn']),
    Rule([u'Amendment #\\S+ \\((?P<legislator>.+?)\\) Pending'],
         [u'amendment:introduced']),
    Rule([u'(?P<bill>[HS]\\d+)'], []),
    Rule([u'(?i)Amendment \\(#\\d+\\) adopted'], [u'amendment:passed']),
    Rule([u'(?i)with veto'], [u'governor:vetoed']),
    Rule([u'reported favorably by committee'], [u'committee:passed:favorable']),
    Rule([u'Accompan\\S+ .+?(?P<bill_id>[SH]\\S+)'], []),
    Rule([u'(?i)Amendment \\d+ pending'], [u'amendment:tabled']),
    Rule([u'Read,'], [u'bill:reading:1']),
    Rule([u'(?i)Amendment #\\S+ \\((?P<legislator>.+?)\\)\\s+-\\s+rejected',
    u'(?i)Amendment \\d+ rejected',
    u'Amendment #?\\S+ \\((?P<legislator>.+?)\\) rejected'],
         [u'amendment:failed']),
    Rule([u'Amended \\((?P<legislator>.+?)\\) ',
    u'Amendment #?\\S+ \\((?P<legislator>.+?)\\) adopted'],
         [u'amendment:passed']),
    Rule([u'(?i)read.{,10}second'], [u'bill:reading:2']),
    Rule([u'Amendment #\\d+ \\((?P<legislator>.+?)\\) pending'],
         [u'amendment:introduced']),
    Rule([u'Enacted'], [u'bill:passed']),
    Rule([u'Amendment #\\S+ \\((?P<legislator>.+?)\\) Adopted',
    u'Accompanied a study order, (see )?(?P<bill_id>[SH]\\S+)'],
         []),
    Rule([u'passed over veto'], [u'bill:veto_override:passed']),
    Rule([u'(?i)Read third'], [u'bill:reading:3']),
    Rule([u'Bill Filed'], [u'bill:introduced']),
    Rule([u'(?i)Amendment #\\S+ rejected'], [u'amendment:failed']),
    Rule([u'laid aside'], [u'amendment:tabled']),
    Rule([u'(?i)Amendment \\(#\\d+\\) rejected'], [u'amendment:failed']),
    Rule([u'(?i)amendment.+?adopted'], [u'amendment:passed']),
    Rule([u'Adopted, (see )?(?P<bill_id>[SH]\\S+)'], []),
    Rule([u'(?is)Amendment \\(\\d+\\) rejected'], [u'amendment:failed']),
    Rule([u'(?P<yes_votes>\\d+) YEAS.+?(?P<no_votes>\\d+) NAYS'], []),
    Rule([u'Passed to be engrossed'], [u'bill:passed']),
    Rule([u'Amendment #\\d+ \\((?P<legislator>.+?)\\) adopted'],
         [u'amendment:passed']),
    Rule([u'Amendment #\\S+ \\((?P<legislator>.+?)\\) Rejected'],
         [u'amendment:failed']),
    Rule([u'referred to (?P<committees>.+)'], []),
    Rule([u'Amended by'], [u'amendment:passed']),
    Rule(['Committee recommended ought to pass'], ['committee:passed:favorable']),
    Rule([u'Amendment #\\S+ \\((?P<legislator>.+?)\\) bundle NO rejected'],
         [u'amendment:failed']),
    Rule([u'(?is)Amendment \\(\\d+\\) adopted'], [u'amendment:passed']),
    Rule([u'(?i)(Referred|Recommittedra) to (?P<committees>committee on.+)'],
         [u'committee:referred']),
    Rule([u'Accompanied a new draft, (see )?(?P<bill_id>[SH]\\S+)'], []),
    Rule([u'(?i)Amendment #\\S+ \\((?P<legislator>.+?)\\) bundle NO rejected'],
         [u'amendment:failed']),
    Rule([u'(?i)(Referred|Recommittedra) to (?P<chamber>\\S+) (?P<committees>committee on.+)'],
         [u'committee:referred']),
    Rule(['Committee recommended ought NOT'], ['committee:passed:unfavorable']),
    Rule([u'(?i)(Referred|Recommittedra) (to|from)( the)? (?P<chamber>\\S+) (?P<committees>committee on.+)'],
         [u'committee:referred']),
    Rule([u'(?i)Amendment #\\d+ rejected'], [u'amendment:failed']),
    Rule([u'(?i)Amendment \\d+ adopted'], [u'amendment:passed']),
    Rule([u'Committee of Conference appointed \\((?P<legislator>.+?)\\)'], [])
    )


class Categorizer(BaseCategorizer):
    rules = _categorizer_rules


########NEW FILE########
__FILENAME__ = bills
import re
import time
import itertools
from datetime import datetime

import lxml.html

from billy.scrape.bills import BillScraper, Bill

from .actions import Categorizer


class MABillScraper(BillScraper):
    jurisdiction = 'ma'
    categorizer = Categorizer()

    def __init__(self, *args, **kwargs):
        super(MABillScraper, self).__init__(*args, **kwargs)
        # forcing these values so that 500s come back as skipped bills
        # self.retry_attempts = 0
        self.raise_errors = False

    def scrape(self, chamber, session):
        # for the chamber of the action
        chamber_map = {'House': 'lower', 'Senate': 'upper', 'Joint': 'joint',
                       'Governor': 'executive'}

        session_slug = session[:-2]
        chamber_slug = 'House' if chamber == 'lower' else 'Senate'

        # keep track of how many we've had to skip
        skipped = 0

        for n in itertools.count(1):
            bill_id = '%s%d' % (chamber_slug[0], n)
            bill_url = 'http://www.malegislature.gov/Bills/%s/%s/%s' % (
                session_slug, chamber_slug, bill_id)

            # lets assume if 10 bills are missing we're done
            if skipped == 10:
                break

            html = self.urlopen(bill_url)
            if 'Unable to find the Bill' in html:
                self.warning('skipping %s' % bill_url)
                skipped += 1
                continue

            # sometimes the site breaks, missing vital data
            if 'billShortDesc' not in html:
                self.warning('truncated page on %s' % bill_url)
                time.sleep(1)
                html = self.urlopen(bill_url)
                if 'billShortDesc' not in html:
                    self.warning('skipping %s' % bill_url)
                    skipped += 1
                    continue
                else:
                    skipped = 0
            else:
                skipped = 0

            doc = lxml.html.fromstring(html)
            doc.make_links_absolute('http://www.malegislature.gov/')

            title = doc.xpath('//h2/span/text()')[0].strip()
            desc = doc.xpath('//p[@class="billShortDesc"]/text()')[0]

            # create bill
            bill = Bill(session, chamber, bill_id, title, summary=desc)
            bill.add_source(bill_url)

            # actions
            for act_row in doc.xpath('//tbody[@class="bgwht"]/tr'):
                date = act_row.xpath('./td[@headers="bDate"]/text()')[0]
                date = datetime.strptime(date, "%m/%d/%Y")
                actor_txt = act_row.xpath('./td[@headers="bBranch"]')[0].text_content().strip()
                if actor_txt:
                    actor = chamber_map[actor_txt]
                action = act_row.xpath('./td[@headers="bAction"]')[0].text_content().strip()
                attrs = self.categorizer.categorize(action)
                bill.add_action(actor, action, date, **attrs)

            # I tried to, as I was finding the sponsors, detect whether a
            # sponsor was already known. One has to do this because an author
            # is listed in the "Sponsors:" section and then the same person
            # will be listed with others in the "Petitioners:" section. We are
            # guessing that "Sponsors" are authors and "Petitioners" are
            # co-authors. Does this make sense?

            sponsors = dict((a.get('href'), a.text) for a in
                            doc.xpath('//p[@class="billReferral"]/a'))
            petitioners = dict((a.get('href'), a.text) for a in
                               doc.xpath('//div[@id="billSummary"]/p[1]/a'))

            if len(sponsors) == 0:
                spons = doc.xpath('//p[@class="billReferral"]')[0].text_content()
                spons = spons.strip()
                spons = spons.split("\n")
                cspons = []
                for s in spons:
                    if s and s.strip() != "":
                        cspons.append(s)

                sponsors = dict((s, s) for s in cspons)

            # remove sponsors from petitioners
            for k in sponsors:
                petitioners.pop(k, None)

            for sponsor in sponsors.values():
                if sponsor == 'NONE':
                    continue
                if sponsor is None:
                    continue
                bill.add_sponsor('primary', sponsor)

            for petitioner in petitioners.values():
                if sponsor == 'NONE':
                    continue
                bill.add_sponsor('cosponsor', petitioner)

            # sometimes html link is just missing
            bill_text_url = (
                doc.xpath('//a[contains(@href, "BillHtml")]/@href') or
                doc.xpath('//a[contains(@href, "Bills/PDF")]/@href')
            )
            if bill_text_url:
                if 'PDF' in bill_text_url[0]:
                    mimetype = 'application/pdf'
                else:
                    mimetype = 'text/html'
                bill.add_version('Current Text', bill_text_url[0],
                                 mimetype=mimetype)

            self.save_bill(bill)

########NEW FILE########
__FILENAME__ = committees
from billy.scrape.committees import CommitteeScraper, Committee

import lxml.html


class MACommitteeScraper(CommitteeScraper):
    jurisdiction = 'ma'

    def scrape(self, term, chambers):
        page_types = []
        if 'upper' in chambers:
            page_types += ['Senate', 'Joint']
        if 'lower' in chambers:
            page_types += ['House']
        chamber_mapping = {'Senate': 'upper',
                           'House': 'lower',
                           'Joint': 'joint'}

        foundComms = []

        for page_type in page_types:
            url = 'http://www.malegislature.gov/Committees/' + page_type

            html = self.urlopen(url)
            doc = lxml.html.fromstring(html)
            doc.make_links_absolute('http://www.malegislature.gov')

            for com_url in doc.xpath('//ul[@class="committeeList"]/li/a/@href'):
                chamber = chamber_mapping[page_type]
                self.scrape_committee(chamber, com_url)

    def scrape_committee(self, chamber, url):
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        name = doc.xpath('//span[@class="committeeShortName"]/text()')
        if len(name) == 0:
            self.warning("Had to skip this malformed page.")
            return
        # Because of http://www.malegislature.gov/Committees/Senate/S29 this
        # XXX: hack had to be pushed in. Remove me ASAP. This just skips
        #      malformed pages.

        name = name[0]
        com = Committee(chamber, name)
        com.add_source(url)

        # get both titles and names, order is consistent
        titles = doc.xpath('//p[@class="rankingMemberTitle"]/text()')
        names = doc.xpath('//p[@class="rankingMemberName"]/a/text()')

        for title, name in zip(titles, names):
            com.add_member(name, title)

        for member in doc.xpath('//div[@class="committeeRegularMembers"]//a/text()'):
            com.add_member(member)

        if com['members']:
            self.save_committee(com)

########NEW FILE########
__FILENAME__ = events
import re
import datetime as dt

from billy.scrape import NoDataForPeriod
from billy.scrape.events import Event, EventScraper

import pytz
import lxml.html

urls = "http://www.malegislature.gov/Events/%s"
pages = {
    "upper" : [ urls % "SenateSessions" ],
    "lower" : [ urls % "HouseSessions" ],
    "other" : [ urls % "JointSessions",
                urls % "Hearings", urls % "SpecialEvents" ]
}

class MAEventScraper(EventScraper):
    jurisdiction = 'ma'

    _tz = pytz.timezone('US/Eastern')

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def add_agenda(self, event, url):
        event.add_source(url)
        page = self.lxmlize(url)
        description = page.xpath("//div[@id='eventData']/p")
        if len(description) > 0:
            description = description[0].text_content().strip()
            if description != "":
                event['description'] = description

        # Let's (hillariously) add "Chair"(s) to the event.
        people = page.xpath("//a[contains(@href, 'People')]")
        for person in people:
            if "Facilitator" in person.text_content():
                kruft, chair = person.text_content().split(":", 1)
                kruft = kruft.strip()
                chair = chair.strip()
                chamber = 'other'
                if "senate" in kruft:
                    chamber = 'upper'
                elif "house" in kruft:
                    chamber = 'lower'

                event.add_participant('chair', chair, 'legislator',
                                      position_name=kruft,
                                      chamber=chamber)

        trs = page.xpath("//tbody/tr")
        for tr in trs:
            # Alright. Let's snag some stuff.
            cells = {
                "num" : "agendaItemNum",
                "bill_id": "agendaBillNum",
                "title": "agendaBillTitle",
                "spons": "agendaBillSponsor"
            }
            metainf = {}
            for cell in cells:
                metainf[cell] = tr.xpath(".//td[@class='" + cells[cell] + "']")
            if metainf['bill_id'] == []:
                return
            kwargs = { "type" : "consideration" }
            # Alright. We can assume we have at least the bill ID.
            bill_id = metainf['bill_id'][0].text_content().strip()
            if cells['title'] != []:
                kwargs['description'] = metainf['title'][0].text_content(
                    ).strip()
            # XXX: Add sponsors.
            event.add_related_bill(bill_id, **kwargs)


    def parse_row(self, row, session, chamber):
        dates = row.xpath("./td[@class='dateCell']")
        for date in dates:
            # alright, so we *may* not get a date, in which case the date
            # is the same as the last event.
            cal_date = date.xpath("./span[@class='calendarMonth']")[0]
            cal_day = date.xpath("./span[@class='calendarDay']")[0]
            self.last_month = cal_date.text_content()
            self.last_day = cal_day.text_content()
        time = row.xpath("./td[@class='timeCell']")
        if not time:
            return  # Nada.
        time = time[0]
        time = time.text.strip()
        dt_string = "%s %s %s %s" % (
            self.last_month,
            self.last_day,
            self.year,
            time
        )
        fmt = "%b %d %Y %I:%M %p"
        when = dt.datetime.strptime(dt_string, fmt)
        cells = {
            "event": "eventCell",
            "status": "statusCell",
            "location": "locationCell",
            "transcript": "transcriptCell",
            "video": "videoCell"
        }
        metainf = {}
        for thing in cells:
            mi = row.xpath("./td[@class='" + cells[thing] + "']")
            if mi == []:
                continue
            metainf[thing] = mi[0]

        if metainf['location'].xpath("./*") == []:
            metainf['location'] = self.last_location
        else:
            self.last_location = metainf['location']

        if "Session" in metainf['event'].text_content().strip():
            return  # Nada.

        loc_url = metainf['location'].xpath(".//a")
        loc_url = loc_url[0].attrib['href']
        event = Event(session,
                      when,
                      'committee:meeting',
                      metainf['event'].text_content().strip(),
                      chamber=chamber,
                      location=metainf['location'].text_content().strip(),
                      location_url=loc_url)
        event.add_participant("host", metainf['event'].text_content().strip(),
                              'committee', chamber=chamber)
        self.add_agenda(event, metainf['event'].xpath(".//a")[0].attrib['href'])
        return event

    def scrape(self, chamber, session):
        scrape_list = pages[chamber]
        self.year = dt.datetime.now().year
        for site in scrape_list:
            page = self.lxmlize(site)
            rows = page.xpath("//tbody[not(contains(@id, 'noDates'))]/tr[contains(@class, 'dataRow')]")
            for row in rows:
                event = self.parse_row(row, session, chamber)
                if event:
                    event.add_source(site)
                    self.save_event(event)

########NEW FILE########
__FILENAME__ = legislators
import re

import lxml.html
from billy.scrape.legislators import LegislatorScraper, Legislator

def clean_district(district):

    # Ugh.
    if district.startswith('Consisting of'):
        return district.split().pop().strip('., ')

    mappings = (
        ('\s+', ' '),
        ('Consisting.*', ''),
        ('Consistng.*', ''),
        ('Consiting.*', ''),
        (u'\xe2.*', ''),
        ('\..*', ''),
        ('Frist', 'First'),
        ('Norfok', 'Norfolk'),
        # these have a capitol T because we call title() first
        ('8Th', 'Eighth'),
        ('9Th', 'Ninth'),
        ('12Th', 'Twelfth'),
        ('16Th', 'Sixteenth'),
        (' District', ''),
        ('yfirst', 'y-First'),
        ('ysecond', 'y-Second'),
        ('ythird', 'y-Third'),
        ('yfourth', 'y-Fourth'),
        ('yfifth', 'y-Fifth'),
        ('ysixth', 'y-Sixth'),
        ('yseventh', 'y-Seventh'),
        ('yeighth', 'y-Eighth'),
        ('yninth', 'y-Ninth'),
        (' And ', ' and '),
        ('\s*-+\s*$', ''),
    )
    district = district.title()
    for pattern, repl in mappings:
        district = re.sub(pattern, repl, district)
    district = district.strip(', -')
    district = district.strip(' -')
    return district


class MALegislatorScraper(LegislatorScraper):
    jurisdiction = 'ma'

    def scrape(self, chamber, term):
        self.validate_term(term, latest_only=True)

        if chamber == 'upper':
            chamber_type = 'Senate'
        else:
            chamber_type = 'House'

        url = "http://www.malegislature.gov/People/%s" % chamber_type
        page = self.urlopen(url)
        doc = lxml.html.fromstring(page)
        doc.make_links_absolute("http://www.malegislature.gov")

        for member_url in doc.xpath('//td[@class="nameCol firstCol"]/a/@href'):
            self.scrape_member(chamber, term, member_url)

    def scrape_member(self, chamber, term, member_url):
        page = self.urlopen(member_url)
        root = lxml.html.fromstring(page)
        root.make_links_absolute(member_url)

        photo_url = root.xpath('//div[starts-with(@class,"bioPicContainer")]/img/@src')[0]
        photo_url = root.xpath('//div[starts-with(@class,"bioPicContainer")]/img/@src')[0]
        full_name = root.xpath('//div[starts-with(@class,"bioPicContainer")]/img/@alt')[0]


        email = root.xpath('//a[contains(@href, "mailto")]/@href')[0]
        email = email.replace('mailto:','')
        # if full_name == 'Frank A. Moran':

        district = root.xpath('//div[@id="District"]//div[starts-with(@class,"widgetContent")]')
        if len(district):
            district_dirty = district[0].text_content().strip()
            district = clean_district(district_dirty)
        else:
            self.logger.warning('No district tab found for this hot garbage. Skipping.')
            return
        if district_dirty and not district:
            self.logger.critical('clean_district wiped out all district text.')
            assert False
            return

        party = root.xpath('//span[@class="legislatorAffiliation"]/text()')[0]

        if party == 'D':
            party = 'Democratic'
        elif party == 'R':
            party = 'Republican'
        else:
            party = 'Other'

        leg = Legislator(term, chamber, district, full_name, party=party,
                         photo_url=photo_url, url=member_url, email=email)
        leg.add_source(member_url)

        # offices
        for dl in root.xpath('//dl[@class="address"]'):
            office_name = phone = fax = email = None
            address = []
            for child in dl.getchildren():
                text = child.text_content()
                if child.tag == 'dt':
                    office_name = text
                else:
                    if text.startswith('Phone:'):
                        phone = text.strip('Phone: ') or None
                    elif text.startswith('Fax:'):
                        fax = text.strip('Fax: ') or None
                    elif text.startswith('Email:'):
                        pass
                    else:
                        address.append(text)
            # all pieces collected
            if 'District' in office_name:
                otype = 'district'
            else:
                otype = 'capitol'

            address = filter(
                None, [re.sub(r'\s+', ' ', s).strip() for s in address])

            if address:
                leg.add_office(otype, office_name, phone=phone, fax=fax,
                               address='\n'.join(address), email=None)

        self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = without_image
import logging
import datetime

from tater import Node, matches
from tater import Lexer, bygroups, include, Rule as r
from tater import Visitor


class HeaderLexer(Lexer):
    '''Lexer for the House PDF vote files that are normal (nice)
    PDFs with machine-readable text and no embedded images.
    '''
    # DEBUG = logging.DEBUG
    re_skip = r'[\s\n]+'
    dont_emit = ['Skip']

    tokendefs = {

        'root': [
            ('Skip', 'MASSACHUSETTS HOUSE OF REPRESENTATIVES'),
            include('meta'),
        ],

        'meta': [
            r('BillId', '[A-Z]\.\s{,2}\d+', push='motion'),
            (bygroups('Motion'), '(Shall.+?)\n\n'),
            ('Skip', 'Yea and Nay'),
            ('Date', r'[\d/]+ [\d:]+ [AP]M'),
            r(bygroups('CalendarNumber'), r'No. (\d+)',
              push=['votes', 'counts']),
        ],

        'motion': [
            (bygroups('Motion'), '(.+?)\n\n'),
        ],

        'counts': [
            (bygroups('YesCount'), r'(\d+)\s{,5}YEAS'),
            (bygroups('NoCount'), r'(\d+)\s{,5}NAYS'),
            (bygroups('OtherCount'), r'(\d+)\s{,5}N/V'),
        ],

        'votes': [
            (bygroups('Votes'), '(?s)(.+)\*=AFTER VOTE.+'),
        ]
    }


# ---------------------------------------------------------------
# Node definitions.
class Rollcall(Node):

    @matches('BillId')
    def handle_bill_id(self, *items):
        return self.descend('BillId', items)

    @matches('Motion')
    def handle_motion(self, *items):
        return self.descend('Motion', items)

    @matches('Date')
    def handle_date(self, *items):
        return self.descend('Date', items)

    @matches('CalendarNumber')
    def handle_calendar_number(self, *items):
        return self.descend('CalendarNumber', items)

    @matches('YesCount')
    def handle_yes_count(self, *items):
        return self.descend('YesCount', items)

    @matches('NoCount')
    def handle_no_count(self, *items):
        return self.descend('NoCount', items)

    @matches('OtherCount')
    def handle_other_count(self, *items):
        return self.descend('OtherCount', items)

    @matches('Votes')
    def handle_votes(self, *items):
        return self.descend('Votes', items)


# -----------------------------------------------------------------
# Visitor

class VoteVisitor(Visitor):

    def __init__(self):
        self.data = {}

    def visit_BillId(self, node):
        bill_id = node.first_text().replace('.', '')
        self.data['bill_id'] = bill_id

    def visit_Motion(self, node):
        self.data['motion'] = node.first_text()

    def visit_Date(self, node):
        fmt_string = '%m/%d/%y %S:%M %p'
        date = datetime.datetime.strptime(node.first_text(), fmt_string)
        self.data['date'] = date

    def visit_YesCount(self, node):
        self.data['yes_count'] = int(node.first_text())

    def visit_NoCount(self, node):
        self.data['no_count'] = int(node.first_text())

    def visit_OtherCount(self, node):
        self.data['other_count'] = int(node.first_text())

    def visit_Votes(self, node):
        self.data['votes'] = node.first_text()

    def finalize(self):
        return self.data

########NEW FILE########
__FILENAME__ = with_image
'''Lexer/parser classes for PDF vote files in which the roll call
votes are shown as embedded images.
'''
import logging
from operator import attrgetter

from tater import Lexer, bygroups, include
from tater import Node, matches
from tater import Visitor


class Lexer(Lexer):

    re_skip = r'[\s\n]+'
    dont_emit = ['Skip']

    re_name = r'[A-Z][a-z]+[A-Za-z]+'

    # DEBUG = logging.DEBUG
    tokendefs = {

        'root': [
            include('vote_value'),
        ],

        'vote_value': [
            # Tokenize vote values.
            (bygroups('VoteVal'), r'^([NYXP])', 'name'),
            (bygroups('VoteVal'), r'([NYXP])', 'name'),
        ],

        'name': [
            ('Skip', r'\*'),

            # Hypephenated last name.
            ('Name', r'%s\s*-\s*%s' % (re_name, re_name)),

            # Tokenize names, Smith
            ('Name', re_name),

            # Tokenize deMacedo
            ('Name', r'(de|di|da)%s' % re_name),

            # Special case of Mr. Speaker
            ('Speaker', 'Mrs?\s*\.\s+Speaker'),

            # O'Flanery, D'Emilia
            ('Name', "[OD]\s*'\s*[A-Z][a-z]+"),

            # The comma after Smith ,
            ('Comma', r','),

            # The trailing initial of Smith , J .
            (bygroups('Initial'), '([A-Z])\s*\.'),

            # Lower case name fragments.
            ('Fragment', '[a-z]+'),
            ],
        }


# ---------------------------------------------------------------------------
# Node definitions for assembling the tokens into a tree.
class Rollcall(Node):

    @matches('VoteVal')
    def handle_vote(self, *items):
        return self.descend(Vote).descend('VoteValue', items)


class Vote(Node):

    @matches('Name')
    def handle_name(self, *items):
        return self.descend('Name', items)

    @matches('Speaker')
    def handle_speaker(self, *items):
        return self.descend('Speaker', items)


class Name(Node):

    @matches('Comma', 'Initial')
    def handle_initial(self, *items):
        comma, initial = items
        return self.descend('Initial', initial)

    @matches('Fragment')
    def handle_fragment(self, *items):
        '''Append any lowercase name fragments to the main name.
        '''
        return self.extend(*items)


# ---------------------------------------------------------------------------
# Visit the parse tree and add votes from it.
class VoteVisitor(Visitor):

    def __init__(self, vote_object):
        self.vote = vote_object

    def visit_Vote(self, node):
        # Get vote value.
        vote_value = node.find_one('VoteValue').first_text()

        # Get voter name.
        if node.find_one('Speaker'):
            voter_name = node.find_one('Speaker').first_text()
            voter_name = voter_name.replace(' . ', '. ')
            initial = ''
        else:
            voter_name = map(attrgetter('text'), node.find_one('Name').items)
            voter_name = ''.join(voter_name)
            voter_name = voter_name.replace(' ', '')
            initial = node.find_one('Initial')
            if initial is not None:
                initial = initial.first_text()
                voter_name += ', %s.' % initial

        # Add to the vote object.
        if vote_value == 'N':
            self.vote.no(voter_name)
        elif vote_value == 'Y':
            self.vote.yes(voter_name)
        if vote_value in 'XP':
            self.vote.other(voter_name)


########NEW FILE########
__FILENAME__ = votes
# -*- coding: utf-8 -*-
import re
import os
import pdb
import datetime
from operator import itemgetter
import contextlib

import sh
import tesseract

import scrapelib
from billy.scrape.utils import convert_pdf
from billy.scrape.votes import VoteScraper, Vote as BillyVote

from .lexers import with_image
from .lexers import without_image


@contextlib.contextmanager
def cd(path):
    '''Creates the path if it doesn't exist'''
    old_dir = os.getcwd()
    try:
        os.makedirs(path)
    except OSError:
        pass
    os.chdir(path)
    try:
        yield
    finally:
        os.chdir(old_dir)


class MAVoteScraper(VoteScraper):
    jurisdiction = 'ma'

    class EndOfHouseVotes(Exception):
        '''Raise when there are no more house votes to scrape.
        '''
        pass

    class MiscellaneousVote(Exception):
        '''Sometimes the chamber will vote on something that isn't
        related to a bill, like whether to suspend the rules in order
        to continue to meet late in the night.
        See http://www.mass.gov/legis/journal/RollCallPdfs/188/00060.pdf?Session=188&RollCall=00060
        '''

    def scrape(self, chamber, session):
        self.filenames = []
        if chamber == 'upper':
            self.scrape_senate(session)
        elif chamber == 'lower':
            self.scrape_house(session)

    def scrape_senate(self, session):
        pass

    def scrape_house(self, session):
        n = 1
        while True:
            try:
                self.scrape_vote(session, n)
            except self.EndOfHouseVotes:
                break
            except self.MiscellaneousVote:
                pass
            n += 1

    def scrape_vote(self, session, rollcall_number):

        # Fetch this piece of garbage.
        url = (
            'http://www.mass.gov/legis/journal/RollCallPdfs/'
            '{session}/{rollcall}.pdf?Session={session}&RollCall={rollcall}')
        url_args = dict(
            session=re.findall(r'\d+', session).pop(),
            rollcall=str(rollcall_number).zfill(5))
        url = url.format(**url_args)

        try:
            vote_file, resp = self.urlretrieve(url)
        except scrapelib.HTTPError:
            # We'll hit a 404 at the end of the votes.
            self.warning('Stopping; encountered a 404 at %s' % url)
            raise self.EndOfHouseVotes

        text = convert_pdf(vote_file, type='text')
        text = text.decode('utf8')

        # A hack to guess whether this PDF has embedded images or contains
        # machine readable text.
        if len(re.findall(r'[YNPX]', text)) > 157:
            vote = self.house_get_vote(text, vote_file, session)
        else:
            vote = self.house_get_vote_with_images(text, vote_file, session)
            self.house_add_votes_from_image(vote_file, vote)

        vote.add_source(url)
        if not self.house_check_vote(vote):
            self.logger.warning('Bad vote counts for %s' % vote)
            return
        self.save_vote(vote)
        os.remove(vote_file)

    def house_get_vote(self, text, vote_file, session):

        # Skip quorum votes.*
        if 'QUORUM' in text:
            raise self.MiscellaneousVote

        # Parse the text into a tree.
        tree = without_image.Rollcall.parse(without_image.HeaderLexer(text))

        # Visit the tree and add rollcall votes to the vote object.
        vote_data = without_image.VoteVisitor().visit(tree)

        if 'bill_id' not in vote_data:
            msg = 'Skipping vote not associated with any bill_id'
            self.logger.warning(msg)
            raise self.MiscellaneousVote(msg)

        vote_data['passed'] = vote_data['yes_count'] > vote_data['no_count']
        vote_data['session'] = session
        vote_data['bill_chamber'] = {
            'S': 'upper',
            'H': 'lower'}[vote_data['bill_id'][0]]

        voters = vote_data.pop('votes')
        vote = BillyVote('lower', **vote_data)

        # Parse the text into a tree.
        tree = with_image.Rollcall.parse(with_image.Lexer(voters))

        # Visit the tree and add rollcall votes to the vote object.
        visitor = with_image.VoteVisitor(vote).visit(tree)

        return vote

    def house_get_vote_with_images(self, text, vote_file, session):
        _, motion_start = re.search('Yea and Nay No.+', text).span()
        motion_end, _ = re.search('YEAS', text).span()
        motion = text[motion_start:motion_end]
        motion = ' '.join(motion.strip().split())

        counts_re = r'([A-Z\-]+):\s+(\d+)'
        counts = dict(re.findall(counts_re, text))

        date = re.search(r'\S+ \d+, \d{4}', text).group()
        date = datetime.datetime.strptime(date, '%B %d, %Y')

        chamber_re = r'(Senate|House),\s+No\. (\d+)'
        bill_chamber = re.search(chamber_re, text)
        if bill_chamber is None:
            raise self.MiscellaneousVote('Vote not realted to a bill.')
        chamber, bill_id = bill_chamber.groups()
        bill_chamber = {
            'Senate': 'upper',
            'House': 'lower'}[chamber]

        if bill_chamber == 'lower':
            bill_id = 'H ' + bill_id
        else:
            bill_id = 'S ' + bill_id

        yes = int(counts['YEAS'])
        no = int(counts['NAYS'])
        other = int(counts.get('N-V', 0))

        vote = BillyVote('lower', date, motion, (yes > no),
                    yes, no, other, session=session, bill_id=bill_id,
                    bill_chamber=bill_chamber)

        return vote

    def house_add_votes_from_image(self, vote_file, vote):

        # Extract the image.
        with cd('/tmp'):
            sh.pdfimages(vote_file, vote_file)

        # Convert it to .png
        image_file = vote_file + '-000.pbm'

        with open(image_file, 'rb') as f:
            data = f.read()
            api = tesseract.TessBaseAPI()
            api.Init(".", "eng", tesseract.OEM_DEFAULT)
            api.SetPageSegMode(tesseract.PSM_SINGLE_BLOCK)
            whitelist = (
                "abcdefghijklmnopqrstuvwxyz',-.*"
                "ABCDEFGHIJKLMNOPQRSTUVWXYZ ")
            api.SetVariable("tessedit_char_whitelist", whitelist)
            text = tesseract.ProcessPagesBuffer(data, len(data), api)

        # Parse the text into a tree.
        tree = with_image.Rollcall.parse(with_image.Lexer(text))

        # Visit the tree and add rollcall votes to the vote object.
        visitor = with_image.VoteVisitor(vote).visit(tree)

        os.remove(image_file)

    def house_check_vote(self, vote):
        return all([
            len(vote['yes_votes']) == vote['yes_count'],
            len(vote['no_votes']) == vote['no_count'],
            len(vote['other_votes']) == vote['other_count']])

########NEW FILE########
__FILENAME__ = bills
#!/usr/bin/env python
import datetime
import re

import lxml.html

from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

CHAMBERS = {
    'upper': ('SB','SJ'),
    'lower': ('HB','HJ'),
}

classifiers = {
    r'Committee Amendment .+? Adopted': 'amendment:passed',
    r'Favorable': 'committee:passed:favorable',
    r'First Reading': 'committee:referred',
    r'Floor (Committee )?Amendment\s?\(.+?\)$': 'amendment:introduced',
    r'Floor Amendment .+? Rejected': 'amendment:failed',
    r'Floor (Committee )?Amendment.+?Adopted': 'amendment:passed',
    r'Floor Amendment.+? Withdrawn': 'amendment:withdrawn',
    r'Pre\-filed': 'bill:introduced',
    r'Re\-(referred|assigned)': 'committee:referred',
    r'Recommit to Committee': 'committee:referred',
    r'Referred': 'committee:referred',
    r'Third Reading Passed': 'bill:passed',
    r'Third Reading Failed': 'bill:failed',
    r'Unfavorable': 'committee:passed:unfavorable',
    r'Vetoed': 'governor:vetoed',
    r'Approved by the Governor': 'governor:signed',
    r'Conference Committee|Passed Enrolled|Special Order|Senate Concur|Motion|Laid Over|Hearing|Committee Amendment|Assigned a chapter|Second Reading|Returned Passed|House Concur|Chair ruled|Senate Refuses to Concur|Senate Requests': 'other',
}

vote_classifiers = {
    r'third': 'passage',
    r'fla|amend|amd': 'amendment',
}

def _classify_action(action):
    if not action:
        return None

    ctty = None

    for regex, type in classifiers.iteritems():
        if re.match(regex, action):
            if 'committee:referred' in type:
                ctty = re.sub(regex, "", action).strip()
            return ( type, ctty )
    return ( None, ctty )

def _clean_sponsor(name):
    if name.startswith('Delegate') or name.startswith('Senator'):
        name = name.split(' ', 1)[1]
    if ', District' in name:
        name = name.rsplit(',', 1)[0]
    return name.strip().strip('*')


def _get_td(doc, th_text):
    td = doc.xpath('//th[text()="%s"]/following-sibling::td' % th_text)
    if td:
        return td[0]
    td = doc.xpath('//th/span[text()="%s"]/../following-sibling::td' % th_text)
    if td:
        return td[0]


class MDBillScraper(BillScraper):
    jurisdiction = 'md'

    def parse_bill_sponsors(self, doc, bill):
        sponsor_list = doc.xpath('//a[@name="Sponlst"]')
        if sponsor_list:
            # more than one bill sponsor exists
            elems = sponsor_list[0].xpath('../../..//dd/a')
            for elem in elems:
                bill.add_sponsor('cosponsor',
                                 _clean_sponsor(elem.text.strip()))
        else:
            # single bill sponsor
            sponsor = doc.xpath('//a[@name="Sponsors"]/../../dd')[0].text_content()
            bill.add_sponsor('primary', _clean_sponsor(sponsor))

    def parse_bill_actions(self, doc, bill):
        for h5 in doc.xpath('//h5'):
            if h5.text == 'House Action':
                chamber = 'lower'
            elif h5.text == 'Senate Action':
                chamber = 'upper'
            elif h5.text.startswith('Action after passage'):
                chamber = 'governor'
            else:
                break
            dts = h5.getnext().xpath('dl/dt')
            for dt in dts:
                action_date = dt.text.strip()
                if action_date and action_date != 'No Action':
                    year = int(bill['session'][:4])
                    action_date += ('/%s' % year)
                    action_date = datetime.datetime.strptime(action_date,
                                                             '%m/%d/%Y')

                    # no actions after June?, decrement the year on these
                    if action_date.month > 6:
                        year -= 1
                        action_date = action_date.replace(year)

                    # iterate over all dds following the dt
                    dcursor = dt
                    while (dcursor.getnext() is not None and
                           dcursor.getnext().tag == 'dd'):
                        dcursor = dcursor.getnext()
                        actions = dcursor.text_content().split('\r\n')
                        for act in actions:
                            act = act.strip()
                            if not act:
                                continue
                            atype, committee = _classify_action(act)
                            kwargs = {
                                "type": atype
                            }
                            if committee is not None:
                                kwargs['committees'] = committee

                            if atype:
                                bill.add_action(chamber, act, action_date,
                                                **kwargs)
                            else:
                                self.log('unknown action: %s' % act)



    def parse_bill_documents(self, doc, bill):
        bill_text_b = doc.xpath('//b[contains(text(), "Bill Text")]')[0]
        for sib in bill_text_b.itersiblings():
            if sib.tag == 'a':
                bill.add_version(sib.text.strip(','), sib.get('href'),
                                 mimetype='application/pdf')

        note_b = doc.xpath('//b[contains(text(), "Fiscal and Policy")]')[0]
        for sib in note_b.itersiblings():
            if sib.tag == 'a' and sib.text == 'Available':
                bill.add_document('Fiscal and Policy Note', sib.get('href'))

    def parse_bill_votes(self, doc, bill):
        params = {
            'chamber': None,
            'date': None,
            'motion': None,
            'passed': None,
            'yes_count': None,
            'no_count': None,
            'other_count': None,
        }
        elems = doc.xpath('//a')

        # MD has a habit of listing votes twice
        seen_votes = set()

        for elem in elems:
            href = elem.get('href')
            if (href and "votes" in href and href.endswith('htm') and
                href not in seen_votes):
                seen_votes.add(href)
                vote = self.parse_vote_page(href)
                vote.add_source(href)
                bill.add_vote(vote)


    def parse_vote_page(self, vote_url):
        vote_html = self.urlopen(vote_url)
        doc = lxml.html.fromstring(vote_html)

        # chamber
        if 'senate' in vote_url:
            chamber = 'upper'
        else:
            chamber = 'lower'

        # date in the following format: Mar 23, 2009
        date = doc.xpath('//td[starts-with(text(), "Legislative")]')[0].text
        date = date.replace(u'\xa0', ' ')
        date = datetime.datetime.strptime(date[18:], '%b %d, %Y')

        # motion
        motion = ''.join(x.text_content() for x in \
                         doc.xpath('//td[@colspan="23"]'))
        if motion == '':
            motion = "No motion given"  # XXX: Double check this. See SJ 3.
        motion = motion.replace(u'\xa0', ' ')

        # totals
        tot_class = doc.xpath('//td[contains(text(), "Yeas")]')[0].get('class')
        totals = doc.xpath('//td[@class="%s"]/text()' % tot_class)[1:]
        yes_count = int(totals[0].split()[-1])
        no_count = int(totals[1].split()[-1])
        other_count = int(totals[2].split()[-1])
        other_count += int(totals[3].split()[-1])
        other_count += int(totals[4].split()[-1])
        passed = yes_count > no_count

        vote = Vote(chamber=chamber, date=date, motion=motion,
                    yes_count=yes_count, no_count=no_count,
                    other_count=other_count, passed=passed)

        # go through, find Voting Yea/Voting Nay/etc. and next tds are voters
        func = None
        for td in doc.xpath('//td/text()'):
            td = td.replace(u'\xa0', ' ')
            if td.startswith('Voting Yea'):
                func = vote.yes
            elif td.startswith('Voting Nay'):
                func = vote.no
            elif td.startswith('Not Voting'):
                func = vote.other
            elif td.startswith('Excused'):
                func = vote.other
            elif func:
                func(td)

        return vote

    def scrape_bill_2012(self, chamber, session, bill_id, url):
        """ Creates a bill object """
        if len(session) == 4:
            session_url = session+'rs'
        else:
            session_url = session

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        # find <a name="Title">, get parent dt, get parent dl, then dd n dl
        title = doc.xpath('//a[@name="Title"][1]/../../dd[1]/text()')[0].strip()

        summary = doc.xpath('//font[@size="3"]/p/text()')[0].strip()

        if 'B' in bill_id:
            _type = ['bill']
        elif 'J' in bill_id:
            _type = ['joint resolution']

        bill = Bill(session, chamber, bill_id, title, type=_type,
                    summary=summary)
        bill.add_source(url)

        self.parse_bill_sponsors(doc, bill)     # sponsors
        self.parse_bill_actions(doc, bill)      # actions
        self.parse_bill_documents(doc, bill)    # documents and versions
        self.parse_bill_votes(doc, bill)        # votes

        # subjects
        subjects = []
        for subj in doc.xpath('//a[contains(@href, "/subjects/")]'):
            subjects.append(subj.text.split('-see also-')[0])
        bill['subjects'] = subjects

        # add bill to collection
        self.save_bill(bill)


    def scrape_vote(self, bill, action_text, url):
        doc = lxml.html.fromstring(self.urlopen(url))

        date = None
        yes_count = no_count = other_count = None

        # process action_text - might look like "Vote - Senate Floor - Third Reading Passed (46-0) - 01/16/12"
        if action_text.startswith('Vote - Senate Floor - '):
            action_text = action_text[22:]
            chamber = 'upper'
        elif action_text.startswith('Vote - House Floor - '):
            action_text = action_text[21:]
            chamber = 'lower'

        motion, unused_date = action_text.rsplit(' - ', 1)
        yes_count, no_count = re.findall('\((\d+)-(\d+)\)', motion)[0]
        if 'Passed' in motion:
            motion = motion.split(' Passed')[0]
            passed = True
        elif 'Adopted' in motion:
            motion = motion.split(' Adopted')[0]
            passed = True
        elif 'Rejected' in motion:
            motion = motion.split(' Rejected')[0]
            passed = False
        elif 'Failed' in motion:
            motion = motion.split(' Failed')[0]
            passed = False
        elif 'Concur' in motion:
            passed = True
        elif 'Floor Amendment' in motion:
            passed = int(yes_count) > int(no_count)
        else:
            raise Exception('unknown motion: %s' % motion)

        vote = Vote(chamber=chamber, date=None, motion=motion,
                    yes_count=int(yes_count), no_count=int(no_count),
                    other_count=0, passed=passed)
        vfunc = None

        nobrs = doc.xpath('//nobr/text()')
        for text in nobrs:
            text = text.replace(u'\xa0', ' ')
            if text.startswith('Calendar Date: '):
                if vote['date']:
                    self.warning('two dates!, skipping rest of bill')
                    break
                vote['date'] = datetime.datetime.strptime(text.split(': ', 1)[1], '%b %d, %Y %H:%M %p')
            elif 'Yeas' in text and 'Nays' in text and 'Not Voting' in text:
                yeas, nays, nv, exc, absent = re.match('(\d+) Yeas\s+(\d+) Nays\s+(\d+) Not Voting\s+(\d+) Excused \(Absent\)\s+(\d+) Absent', text).groups()
                vote['yes_count'] = int(yeas)
                vote['no_count'] = int(nays)
                vote['other_count'] = int(nv) + int(exc) + int(absent)
            elif 'Voting Yea' in text:
                vfunc = vote.yes
            elif 'Voting Nay' in text:
                vfunc = vote.no
            elif 'Not Voting' in text or 'Excused' in text:
                vfunc = vote.other
            elif vfunc:
                if ' and ' in text:
                    a, b = text.split(' and ')
                    vfunc(a)
                    vfunc(b)
                else:
                    vfunc(text)

        vote.validate()
        vote.add_source(url)
        bill.add_vote(vote)


    def scrape_bill(self, chamber, session, bill_id, url):
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        title = doc.xpath('//h3[@class="h3billright"]')[0].text_content()
        # TODO: grab summary (none present at time of writing)

        if 'B' in bill_id:
            _type = ['bill']
        elif 'J' in bill_id:
            _type = ['joint resolution']
        else:
            raise ValueError('unknown bill type ' + bill_id)

        bill = Bill(session, chamber, bill_id, title, type=_type)
        bill.add_source(url)

        # process sponsors
        sponsors = _get_td(doc, 'All Sponsors:').text_content()
        sponsors = sponsors.replace('Delegates ', '')
        sponsors = sponsors.replace('Delegate ', '')
        sponsors = sponsors.replace('Senator ', '')
        sponsors = sponsors.replace('Senators ', '')
        sponsor_type = 'primary'
        for sponsor in re.split(', (?:and )?', sponsors):
            #self.debug('sponsor: %s', sponsor)
            bill.add_sponsor(sponsor_type, sponsor)
            sponsor_type = 'cosponsor'

        # subjects
        subject_list = []
        for heading in ('Broad Subject(s):', 'Narrow Subject(s):'):
            subjects =  _get_td(doc, heading).xpath('a/text()')
            subject_list += [s.split(' -see also-')[0] for s in subjects if s]
        bill['subjects'] = subject_list

        # documents
        self.scrape_documents(bill, url.replace('stab=01', 'stab=02'))
        # actions
        self.scrape_actions(bill, url.replace('stab=01', 'stab=03'))

        self.save_bill(bill)


    def scrape_documents(self, bill, url):
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        for td in doc.xpath('//table[@class="billdocs"]//td'):
            a = td.xpath('a')[0]
            if a.text == 'Text':
                bill.add_version('Bill Text', a.get('href'),
                                 mimetype='application/pdf')
            elif a.text == 'Analysis':
                bill.add_document(a.tail.replace(' - ', ' ').strip(),
                                  a.get('href'), mimetype='application/pdf')
            elif a.text in ('Bond Bill Fact Sheet',
                            "Attorney General's Review Letter",
                            "Governor's Veto Letter",
                           ):
                bill.add_document(a.text, a.get('href'),
                                  mimetype='application/pdf')
            elif a.text in ('Amendments', 'Conference Committee Amendment',
                            'Conference Committee Report'):
                bill.add_document(a.text + ' - ' + a.tail.strip(),
                                  a.get('href'), mimetype='application/pdf')
            elif a.text == 'Vote - Senate - Committee':
                bill.add_document('Senate %s Committee Vote' %
                                  a.tail.replace(' - ', ' ').strip(),
                                  a.get('href'), mimetype='application/pdf')
            elif a.text == 'Vote - House - Committee':
                bill.add_document('House %s Committee Vote' %
                                  a.tail.replace(' - ', ' ').strip(),
                                  a.get('href'), mimetype='application/pdf')
            elif a.text == 'Vote - Senate Floor':
                self.scrape_vote(bill, td.text_content(), a.get('href'))
            elif a.text == 'Vote - House Floor':
                self.scrape_vote(bill, td.text_content(), a.get('href'))
            else:
                raise ValueError('unknown document type: %s', a.text)


    def scrape_actions(self, bill, url):
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        for row in doc.xpath('//table[@class="billgrid"]/tr')[1:]:
            new_chamber, cal_date, leg_date, action, proceedings = row.xpath('td')

            if new_chamber.text == 'Senate':
                chamber = 'upper'
            elif new_chamber.text == 'House':
                chamber = 'lower'
            elif new_chamber.text == 'Post Passage':
                chamber = 'executive'
            elif new_chamber.text is not None:
                raise ValueError('unexpected chamber: ' + new_chamber.text)

            action = action.text
            if cal_date.text:
                action_date = datetime.datetime.strptime(cal_date.text, '%m/%d/%Y')

            atype, committee = _classify_action(action)
            kwargs = { "type": atype }
            if committee is not None:
                kwargs['committees'] = committee

            bill.add_action(chamber, action, action_date, **kwargs)


    def scrape(self, chamber, session):
        session_slug = session if 's' in session else session + 'rs'

        main_page = 'http://mgaleg.maryland.gov/webmga/frmLegislation.aspx?pid=legisnpage&tab=subject3&ys=' + session_slug
        chamber_prefix = 'S' if chamber == 'upper' else 'H'
        html = self.urlopen(main_page)
        doc = lxml.html.fromstring(html)

        ranges = doc.xpath('//table[@class="box1leg"]//td/text()')
        for range_text in ranges:
            match = re.match('(\w{2})0*(\d+) - \wB0*(\d+)', range_text.strip())
            if match:
                prefix, begin, end = match.groups()
                if prefix[0] == chamber_prefix:
                    self.debug('scraping %ss %s-%s', prefix, begin, end)
                    for number in range(int(begin), int(end)+1):
                        bill_id = prefix + str(number)
                        url = 'http://mgaleg.maryland.gov/webmga/frmMain.aspx?id=%s&stab=01&pid=billpage&tab=subject3&ys=%s' % (bill_id, session_slug)
                        if session < '2013':
                            self.scrape_bill_2012(chamber, session, bill_id, url)
                        else:
                            self.scrape_bill(chamber, session, bill_id, url)

########NEW FILE########
__FILENAME__ = committees
import lxml.html

from billy.scrape.committees import CommitteeScraper, Committee

class MDCommitteeScraper(CommitteeScraper):

    jurisdiction = 'md'

    def scrape(self, term, chambers):
        # committee list
        url = 'http://mgaleg.maryland.gov/webmga/frmcommittees.aspx?pid=commpage&tab=subject7'
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        for a in doc.xpath('//a[contains(@href, "cmtepage")]'):
            url = a.get('href').replace('stab=01', 'stab=04')
            chamber = a.xpath('../../..//th/text()')[0]
            if 'Senate' in chamber and 'upper' in chambers:
                chamber = 'upper'
                self.scrape_committee(chamber, a.text, url)
            elif 'House' in chamber and 'Delegation' not in chamber and 'lower' in chambers:
                chamber = 'lower'
                self.scrape_committee(chamber, a.text, url)
            elif chamber in ('Joint', 'Statutory', 'Special Joint'):
                chamber = 'joint'
                self.scrape_committee(chamber, a.text, url)


    def scrape_committee(self, chamber, com_name, url):
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        com = Committee(chamber, com_name)
        com.add_source(url)

        for table in doc.xpath('//table[@class="grid"]'):
            rows = table.xpath('tr')
            sub_name = rows[0].getchildren()[0].text

            # new table - subcommittee
            if sub_name != 'Full Committee':
                com = Committee(chamber, com_name, subcommittee=sub_name)
                com.add_source(url)

            for row in rows[1:]:
                name = row.getchildren()[0].text_content()
                if name.endswith(' (Chair)'):
                    name = name.strip(' (Chair)')
                    role = 'chair'
                elif name.endswith(' (Vice Chair)'):
                    name = name.strip(' (Vice Chair)')
                    role = 'vice chair'
                else:
                    role = 'member'
                com.add_member(name, role)

            self.save_committee(com)

########NEW FILE########
__FILENAME__ = events
import datetime as dt

from billy.scrape.events import Event, EventScraper

import re
import pytz
import lxml.html

def last_space(string):
    # this is a big hack.
    for x in range(0, len(string)):
        if string[x] != " ":
            return x
    return None

class MDEventScraper(EventScraper):
    jurisdiction = 'md'
    _tz = pytz.timezone('US/Eastern')
    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def scrape(self, chamber, session):
        if chamber != 'other':
            return None  # We're going to do it all on one shot.

        if session[-2:] == "s1":
            return None  # Special sessions 404

        url = "http://mlis.state.md.us/%s/hearsch/alladd.htm" % ( session )
        page = self.lxmlize(url)
        events = page.xpath("//pre")
        for event in events:
            ctty_name = [
                x.strip() for x in
                event.getparent().getprevious().text_content().split("-", 1)
            ]
            ctty_name = ctty_name[0]
            event_text = event.text_content()
            if "This meeting has been cancelled." in event_text:
                continue
            # OK. In order to process this text-only notice, we have to resort
            # to some major hackage. Just roll with it.
            lines = event_text.split("\n")
            # In order to get the key stuff, we need to figure out where the
            # address "block" starts.
            address_block = last_space(lines[4])
            assert address_block is not None
            # OK. Given the offset, we can "split" the time off the date block.
            time_room = lines[3]
            time = time_room[:address_block].strip()

            if "TBD" in time:
                continue  # Nothing's set yet.
            time = "%s %s" % (
                lines[1],
                time
            )
            time = re.sub("\s+", " ", time).strip()
            trans = {
                "P.M." : "PM",
                "A.M." : "AM"
            }
            for transition in trans:
                time = time.replace(transition, trans[transition])

            when = dt.datetime.strptime(time, "%A %B %d, %Y %I:%M %p")

            room = time_room[address_block:].strip()
            place_block = lines[4:]
            where = room + "\n"
            done = False
            offset = 4
            for place in place_block:
                if place.strip() == "":
                    done = True
                if done:
                    continue
                offset += 1
                where += place.strip() + "\n"
            where = where.strip()
            # Now that the date's processed, we can move on.
            moreinfo = lines[offset + 1:]
            info = {}
            key = "unattached_header"
            for inf in moreinfo:
                if ":" in inf:
                    key, value = inf.split(":", 1)
                    key = key.strip()
                    info[key] = value.strip()
                else:
                    info[key] += " " + inf.strip()
            # Alright. We should have enough now.
            subject = info['Subject']

            event = Event(session, when, 'committee:meeting',
                          subject, location=where)
            event.add_source(url)

            flags = {
                "joint": "joint",
                "house": "lower",
                "senate": "upper"
            }
            chamber = "other"
            for flag in flags:
                if flag in ctty_name.lower():
                    chamber = flags[flag]

            # Let's try and hack out some bill names.
            trans = {
                "SENATE": "S",
                "HOUSE": "H",
                "JOINT": "J",
                "BILL": "B",
                "RESOLUTION": "R",
            }
            _t_subject = subject.upper()
            for t in trans:
                regex = "%s(\s+)?" % t
                _t_subject = re.sub(regex, trans[t], _t_subject)
            print _t_subject
            bills = re.findall("(S|H)(J)?(B|R|M)\s*(\d{4})", _t_subject)
            for bill in bills:
                name = bill[:3]
                bid = bill[3]
                bill_id = "%s %s" % ( ''.join(name), bid )
                event.add_related_bill(bill_id,
                                       description=subject,
                                       type='consideration')


            event.add_participant("host", ctty_name, 'committee',
                                  chamber=chamber)

            self.save_event(event)

########NEW FILE########
__FILENAME__ = legislators
import re
from collections import defaultdict
import lxml.html

from billy.scrape.legislators import LegislatorScraper, Legislator

def _get_table_item(doc, name):
    """ fetch items out of table that has a left column of th """
    return doc.xpath('//th[contains(text(), "%s")]/following-sibling::td' % name)[0]


class MDLegislatorScraper(LegislatorScraper):
    jurisdiction = 'md'
    latest_term = True

    def scrape(self, term, chambers):
        url = 'http://mgaleg.maryland.gov/webmga/frmmain.aspx?pid=legisrpage&tab=subject6'

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        sen_tbl, house_tbl = doc.xpath('//div[@class="legislrlist"]//table[@class="grid"]')

        if 'upper' in chambers:
            self.scrape_table(term, 'upper', sen_tbl)
        if 'lower' in chambers:
            self.scrape_table(term, 'lower', house_tbl)

    def scrape_table(self, term, chamber, tbl):
        # skip first
        for row in tbl.xpath('tr')[1:]:
            leg_a, district, _, _ = row.xpath('td')
            district = district.text
            name = leg_a.text_content().strip()
            leg_url = leg_a.xpath('a/@href')[0]

            # get details
            html = self.urlopen(leg_url)
            ldoc = lxml.html.fromstring(html)
            ldoc.make_links_absolute(leg_url)

            party = _get_table_item(ldoc, 'Party Affiliation:').text
            if party == 'Democrat':
                party = 'Democratic'
            addr_lines = _get_table_item(ldoc, 'Annapolis Address:').xpath('text()')
            address = []
            for line in addr_lines:
                if 'Phone:' not in line:
                    address.append(line)
                else:
                    phone = line
            address = '\n'.join(address)
            phone = re.findall('Phone: (\d{3}-\d{3}-\d{4})', phone)[0]

            email = ldoc.xpath('//a[contains(@href, "mailto:")]/@href')
            if email:
                email = email[0].strip('mailto:')
                email = email.split('?')[0]
            else:
                email = ''

            leg = Legislator(term, chamber, district, name, party=party,
                             url=leg_url, email=email)
            leg.add_source(url=leg_url)

            # photo
            img_src = ldoc.xpath('//img[@class="sponimg"]/@src')
            if img_src:
                leg['photo_url'] = img_src[0]

            leg.add_office('capitol', 'Capitol Office', address=address or None,
                           phone=phone)
            self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = actions
import re
from billy.scrape.actions import Rule, BaseCategorizer


rules = (
    Rule([(u'(?P<yes_votes>\d+) Yeas - (?P<no_votes>\d+) '
           u'Nays- (?P<excused>\d+) Excused - (?P<absent>\d+) Absent'),
          (u'(?P<yes_votes>\d+) -Yeas, (?P<no_votes>\d+) -Nays, '
           u'(?P<excused>\d+) -Excused, (?P<absent>\d+) -Absent'),
           u'(?P<committees>Committee on .+?) suggested and ordered printed',
          (u'\(Yeas (?P<yes_votes>\d+) - Nays (?P<no_votes>\d+) - Absent '
           u'(?P<absent>\d+) - Excused (?P<excused>\d+)\)( \(Vacancy '
           u'(?P<vacant>\d+)\))?')]),

    Rule([u'Representative (?P<legislators>.+?) of \S+',
          u'Senator (?P<legislators>.+?of \S+)',
          'Representative (?P<legislators>[A-Z]+?( of [A-Za-z]+))',
          u'Senator (?P<legislators>\S+ of \S+)',
          u'Representative [A-Z ]+? of \S+']),

    Rule(u'REFERRED to the (?P<committees>Committee on [A-Z ]+(?![a-z]))',
         'committee:referred'),
    Rule(['READ A SECOND TIME'], ['bill:reading:2']),
    Rule(['(?i)read once'], ['bill:reading:1']),
    Rule('(?i)finally passed', 'bill:passed'),
    Rule('(?i)passed to be enacted', 'bill:passed'),
    Rule('COMMITTED to the (?P<committees>Committee on .+?)\.',
         'committee:referred'),
    Rule(r'VETO was NOT SUSTAINED', 'bill:veto_override:passed'),
    Rule(r'VETO was SUSTAINED', 'bill:veto_override:failed'),
    )


class Categorizer(BaseCategorizer):
    rules = rules

    def categorize(self, text):
        '''Wrap categorize and add boilerplate committees.
        '''
        attrs = BaseCategorizer.categorize(self, text)
        committees = attrs['committees']
        for committee in re.findall(committees_rgx, text):
            if committee not in committees:
                committees.append(committee)
        return attrs

    def post_categorize(self, attrs):
        res = set()
        if 'legislators' in attrs:
            for text in attrs['legislators']:
                rgx = r'(,\s+(?![a-z]\.)|\s+and\s+)'
                legs = re.split(rgx, text)
                legs = filter(lambda x: x not in [', ', ' and '], legs)
                res |= set(legs)
        attrs['legislators'] = list(res)

        res = set()
        if 'committees' in attrs:
            for text in attrs['committees']:
                text = text.strip()
                res.add(text)
        attrs['committees'] = list(res)
        return attrs


def get_actor(action_text, chamber, rgxs=(
        (re.compile(r'(in|by) senate', re.I), 'upper'),
        (re.compile(r'(in|by) house', re.I), 'lower'),
        (re.compile(r'by governor', re.I), 'governor'),
        )):
    '''Guess the actor for a particular action.
    '''
    for r, actor in rgxs:
        m = r.search(action_text)
        if m:
            return actor
    return chamber

committees = [
    u'AGRICULTURE, CONSERVATION AND FORESTRY',
    u'APPROPRIATIONS AND FINANCIAL AFFAIRS',
    u'CRIMINAL JUSTICE AND PUBLIC SAFETY',
    u'EDUCATION AND CULTURAL AFFAIRS',
    u'ENERGY, UTILITIES AND TECHNOLOGY',
    u'ENVIRONMENT AND NATURAL RESOURCES',
    u'HEALTH AND HUMAN SERVICES',
    u'INLAND FISHERIES AND WILDLIFE',
    u'INSURANCE AND FINANCIAL SERVICES',
    u'JOINT RULES',
    u'JUDICIARY',
    u'LABOR, COMMERCE, RESEARCH AND ECONOMIC DEVELOPMENT',
    u'MARINE RESOURCES',
    u'REGULATORY FAIRNESS AND REFORM',
    u'STATE AND LOCAL GOVERNMENT',
    u'TAXATION',
    u'TRANSPORTATION',
    u'VETERANS AND LEGAL AFFAIRS',
    ]
committees_rgx = '(%s)' % '|'.join(sorted(committees, key=len, reverse=True))

########NEW FILE########
__FILENAME__ = bills
import re
import socket
import datetime
from operator import methodcaller
import htmlentitydefs

import lxml.html

from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

from .actions import Categorizer


strip = methodcaller('strip')


def unescape(text):
    '''Removes HTML or XML character references and entities
    from a text string.

    @param text The HTML (or XML) source text.
    @return The plain text, as a Unicode string, if necessary.

    Source: http://effbot.org/zone/re-sub.htm#unescape-html'''

    def fixup(m):
        text = m.group(0)
        if text[:2] == "&#":
            # character reference
            try:
                if text[:3] == "&#x":
                    return unichr(int(text[3:-1], 16))
                else:
                    return unichr(int(text[2:-1]))
            except ValueError:
                pass
        else:
            # named entity
            try:
                text = unichr(htmlentitydefs.name2codepoint[text[1:-1]])
            except KeyError:
                pass
        return text  # leave as is
    return re.sub("&#?\w+;", fixup, text)


class BillNotFound(Exception):
    'Raised if bill is not found on their site.'


class MEBillScraper(BillScraper):
    jurisdiction = 'me'
    categorizer = Categorizer()

    def scrape(self, chamber, session):
        if session[-1] == "1":
            session_abbr = session + "st"
        elif session[-1] == "2":
            session_abbr = session + "nd"
        elif session[-1] == "3":
            session_abbr = session + "rd"
        else:
            session_abbr = session + "th"

        self.scrape_session(session, session_abbr, chamber)

    def scrape_session(self, session, session_abbr, chamber):
        url = ('http://www.mainelegislature.org/legis/bills/bills_%s'
               '/billtexts/' % session_abbr)

        page = self.urlopen(url, retry_on_404=True)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        for link in page.xpath('//a[contains(@href, "contents")]/@href'):
            self.scrape_session_directory(session, chamber, link)

    def scrape_session_directory(self, session, chamber, url):
        # decide xpath based on upper/lower
        link_xpath = {'lower': '//big/a[starts-with(text(), "HP")]',
                      'upper': '//big/a[starts-with(text(), "SP")]'}[chamber]

        page = self.urlopen(url, retry_on_404=True)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        for link in page.xpath(link_xpath):
            bill_id = link.text
            title = link.xpath("string(../../following-sibling::dd[1])")

            # A temporary hack to add one particular title that's missing
            # on the directory page.
            if len(title) == 0:
                if session == '125' and bill_id == 'SP0681':
                    msg = 'Adding hard-coded title for bill_id %r'
                    self.warning(msg % bill_id)
                    title = ('An Act To Simplify the Certificate of Need '
                             'Process and Lessen the Regulatory Burden on'
                             ' Providers')

            if not title:
                title = '[Title not available]'

            if (title.lower().startswith('joint order') or
                    title.lower().startswith('joint resolution')):
                bill_type = 'joint resolution'
            else:
                bill_type = 'bill'

            bill = Bill(session, chamber, bill_id, title, type=bill_type)
            try:
                self.scrape_bill(bill, link.attrib['href'])
            except BillNotFound:
                continue
            else:
                self.save_bill(bill)

    def scrape_bill(self, bill, url):
        session_id = (int(bill['session']) - 124) + 8
        url = ("http://www.mainelegislature.org/LawMakerWeb/summary.asp"
               "?paper=%s&SessionID=%d" % (bill['bill_id'], session_id))
        html = self.urlopen(url, retry_on_404=True)
        page = lxml.html.fromstring(html)
        page.make_links_absolute(url)

        # Add the LD number in.
        for ld_num in page.xpath("//b[contains(text(), 'LD ')]/text()"):
            if re.search(r'LD \d+', ld_num):
                bill['ld_number'] = ld_num

        if 'Bill not found.' in html:
            self.warning('%s returned "Bill not found." page' % url)
            raise BillNotFound

        bill.add_source(url)

        # Add bill sponsors.
        try:
            xpath = '//a[contains(@href, "sponsors")]/@href'
            sponsors_url = page.xpath(xpath)[0]
        except IndexError:
            msg = ('Page didn\'t contain sponsors url with expected '
                   'format. Page url was %s' % url)
            raise ValueError(msg)
        sponsors_html = self.urlopen(sponsors_url, retry_on_404=True)
        sponsors_page = lxml.html.fromstring(sponsors_html)
        sponsors_page.make_links_absolute(sponsors_url)

        tr_text = sponsors_page.xpath('//tr')
        tr_text = [tr.text_content() for tr in tr_text]
        rgx = '(Speaker|President|Senator|Representative) ([A-Z ]+)'
        for text in tr_text:

            if 'the Majority' in text:
                # At least one bill was sponsored by 'the Majority'.
                bill.add_sponsor('primary', 'the Majority',
                                 chamber=bill['chamber'])
                continue

            if text.lower().startswith('sponsored by:'):
                type_ = 'primary'
            elif 'introduc' in text.lower():
                type_ = 'primary'
            elif text.lower().startswith('cosponsored by:'):
                type_ = 'cosponsor'
            else:
                continue

            for match in re.finditer(rgx, text):
                chamber_title, name = map(strip, match.groups())
                if chamber_title in ['President', 'Speaker']:
                    chamber = bill['chamber']
                else:
                    chamber = {'Senator': 'upper',
                               'Representative': 'lower'}
                    chamber = chamber[chamber_title]
                bill.add_sponsor(type_.lower(), name.strip(), chamber=chamber)

        bill.add_source(sponsors_url)

        docket_link = page.xpath("//a[contains(@href, 'dockets.asp')]")[0]
        self.scrape_actions(bill, docket_link.attrib['href'])

        # Add signed by guv action.
        if page.xpath('//b[contains(text(), "Signed by the Governor")]'):
            date = page.xpath(
                ('string(//td[contains(text(), "Date")]/'
                 'following-sibling::td/b/text())'))
            dt = datetime.datetime.strptime(date, "%m/%d/%Y")
            bill.add_action(
                action="Signed by Governor", date=dt,
                actor="governor", type=["governor:signed"])

        xpath = "//a[contains(@href, 'rollcalls.asp')]"
        votes_link = page.xpath(xpath)[0]
        self.scrape_votes(bill, votes_link.attrib['href'])

        spon_link = page.xpath("//a[contains(@href, 'subjects.asp')]")[0]
        spon_url = spon_link.get('href')
        bill.add_source(spon_url)
        spon_html = self.urlopen(spon_url, retry_on_404=True)
        sdoc = lxml.html.fromstring(spon_html)
        xpath = '//table[@class="sectionbody"]/tr[2]/td/text()'
        srow = sdoc.xpath(xpath)[1:]
        if srow:
            bill['subjects'] = [s.strip() for s in srow if s.strip()]

        ver_link = page.xpath("//a[contains(@href, 'display_ps.asp')]")[0]
        ver_url = ver_link.get('href')
        try:
            ver_html = self.urlopen(ver_url, retry_on_404=True)
        except socket.timeout:
            pass
        else:
            if ver_html:
                vdoc = lxml.html.fromstring(ver_html)
                vdoc.make_links_absolute(ver_url)
                # various versions: billtexts, billdocs, billpdfs
                vurl = vdoc.xpath('//a[contains(@href, "billtexts/")]/@href')
                if vurl:
                    bill.add_version('Initial Version', vurl[0],
                                     mimetype='text/html')

    def scrape_votes(self, bill, url):
        page = self.urlopen(url, retry_on_404=True)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        path = "//div/a[contains(@href, 'rollcall.asp')]"
        for link in page.xpath(path):
            # skip blank motions, nothing we can do with these
            # seen on /LawMakerWeb/rollcalls.asp?ID=280039835
            if link.text:
                motion = link.text.strip()
                url = link.attrib['href']

                self.scrape_vote(bill, motion, url)

    def scrape_vote(self, bill, motion, url):
        page = self.urlopen(url, retry_on_404=True)
        page = lxml.html.fromstring(page)

        yeas_cell = page.xpath("//td[text() = 'Yeas (Y):']")[0]
        yes_count = int(yeas_cell.xpath("string(following-sibling::td)"))

        nays_cell = page.xpath("//td[text() = 'Nays (N):']")[0]
        no_count = int(nays_cell.xpath("string(following-sibling::td)"))

        abs_cell = page.xpath("//td[text() = 'Absent (X):']")[0]
        abs_count = int(abs_cell.xpath("string(following-sibling::td)"))

        ex_cell = page.xpath("//td[text() = 'Excused (E):']")[0]
        ex_count = int(ex_cell.xpath("string(following-sibling::td)"))

        other_count = abs_count + ex_count

        if 'chamber=House' in url:
            chamber = 'lower'
        elif 'chamber=Senate' in url:
            chamber = 'upper'

        date_cell = page.xpath("//td[text() = 'Date:']")[0]
        date = date_cell.xpath("string(following-sibling::td)")
        try:
            date = datetime.datetime.strptime(date, "%B %d, %Y")
        except ValueError:
            date = datetime.datetime.strptime(date, "%b. %d, %Y")

        outcome_cell = page.xpath("//td[text()='Outcome:']")[0]
        outcome = outcome_cell.xpath("string(following-sibling::td)")

        vote = Vote(chamber, date, motion,
                    outcome == 'PREVAILS',
                    yes_count, no_count, other_count)
        vote.add_source(url)

        member_cell = page.xpath("//td[text() = 'Member']")[0]
        for row in member_cell.xpath("../../tr")[1:]:
            name = row.xpath("string(td[2])")
            # name = name.split(" of ")[0]

            vtype = row.xpath("string(td[4])")
            if vtype == 'Y':
                vote.yes(name)
            elif vtype == 'N':
                vote.no(name)
            elif vtype == 'X' or vtype == 'E':
                vote.other(name)

        bill.add_vote(vote)

    def scrape_actions(self, bill, url):
        page = self.urlopen(url, retry_on_404=True)
        page = lxml.html.fromstring(page)
        bill.add_source(url)

        path = "//b[. = 'Date']/../../../following-sibling::tr"
        for row in page.xpath(path):
            date = row.xpath("string(td[1])")
            date = datetime.datetime.strptime(date, "%m/%d/%Y").date()

            chamber = row.xpath("string(td[2])").strip()
            if chamber == 'Senate':
                chamber = 'upper'
            elif chamber == 'House':
                chamber = 'lower'

            action = gettext(row[2])
            action = unescape(action).strip()

            actions = []
            for action in action.splitlines():
                action = re.sub(r'\s+', ' ', action)
                if not action or 'Unfinished Business' in action:
                    continue

                actions.append(action)

            for action in actions:
                attrs = dict(actor=chamber, action=action, date=date)
                attrs.update(self.categorizer.categorize(action))
                bill.add_action(**attrs)


def _get_chunks(el, buff=None, until=None):
    tagmap = {'br': '\n'}
    buff = buff or []

    # Tag, text, tail, recur...
    yield tagmap.get(el.tag.lower(), '')
    yield el.text or ''
    # if el.text == until:
    #     return
    for kid in el:
        for text in _get_chunks(kid):
            yield text
            # if text == until:
            #     return
    if el.tail:
        yield el.tail
        # if el.tail == until:
        #     return
    if el.tag == 'text':
        yield '\n'


def gettext(el):
    '''Join the chunks, then split and rejoin to normalize the whitespace.
    '''
    return ''.join(_get_chunks(el))

########NEW FILE########
__FILENAME__ = committees
import re
import urlparse
import datetime
from collections import defaultdict

from billy.scrape import NoDataForPeriod
from billy.scrape.committees import CommitteeScraper, Committee
from .utils import clean_committee_name

import lxml.html
import xlrd


class MECommitteeScraper(CommitteeScraper):
    jurisdiction = 'me'

    def scrape(self, chamber, term_name):
        self.validate_term(term_name, latest_only=True)

        if chamber == 'upper':
            self.scrape_senate_comm()
            # scrape joint committees under senate
            self.scrape_joint_comm()
        elif chamber == 'lower':
            self.scrape_reps_comm()

    def scrape_reps_comm(self):

        url = 'http://www.maine.gov/legis/house/hsecoms.htm'

        page = self.urlopen(url)
        root = lxml.html.fromstring(page)

        count = 0

        for n in range(1, 12, 2):
            path = 'string(//body/center[%s]/h1/a)' % (n)
            comm_name = root.xpath(path)
            committee = Committee('lower', comm_name)
            count = count + 1

            path2 = '/html/body/ul[%s]/li/a' % (count)

            for el in root.xpath(path2):
                rep = el.text
                if rep.find('(') != -1:
                    mark = rep.find('(')
                    rep = rep[15: mark].strip()
                if 'chair' in rep.lower():
                    role = 'chair'
                    rep = re.sub(r'(?i)[\s,]*chair\s*$', '', rep).strip()
                else:
                    role = 'member'
                committee.add_member(rep, role)
            committee.add_source(url)

            self.save_committee(committee)

    def scrape_senate_comm(self):
        url = 'http://www.maine.gov/legis/senate/Senate-Standing-Committees.html'

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        # committee titles
        for item in doc.xpath('//span[@style="FONT-SIZE: 11pt"]'):
            text = item.text_content().strip()
            # some contain COMMITTEE ON & some are blank, drop those
            if not text or text.startswith('COMMITTEE'):
                continue

            # titlecase committee name
            com = Committee('upper', text.title())
            com.add_source(url)

            # up two and get ul sibling
            for leg in item.xpath('../../following-sibling::ul[1]/li'):
                lname = leg.text_content().strip()
                if 'Chair' in lname:
                    role = 'chair'
                else:
                    role = 'member'
                lname = leg.text_content().strip().split(' of ')[0].strip()
                com.add_member(lname, role)

            self.save_committee(com)

    def scrape_joint_comm(self):
        fileurl = 'http://www.maine.gov/legis/house/commlist.xls'
        fname, resp = self.urlretrieve(fileurl)

        wb = xlrd.open_workbook(fname)
        sh = wb.sheet_by_index(0)

        chamber = 'joint'

        # Special default dict.
        class Committees(dict):
            def __missing__(self, key):
                val = Committee('joint', key)
                self[key] = val
                return val
        committees = Committees()

        for rownum in range(1, sh.nrows):

            comm_name = sh.cell(rownum, 0).value
            committee = committees[comm_name]

            ischair = sh.cell(rownum, 1).value
            role = 'chair' if ischair else 'member'
            chamber = sh.cell(rownum, 2).value
            first = sh.cell(rownum, 3).value
            middle = sh.cell(rownum, 4).value
            last = sh.cell(rownum, 5).value
            suffix = sh.cell(rownum, 6).value

            name = filter(None, [first, middle, last])
            name = ' '.join(name)
            if suffix:
                name += ', ' + suffix

            name = name.strip()
            committee.add_member(name, role)

        for _, committee in committees.items():
            committee.add_source(fileurl)
            self.save_committee(committee)

########NEW FILE########
__FILENAME__ = legislators
import re
from billy.scrape.legislators import LegislatorScraper, Legislator

import scrapelib
import lxml.html
import xlrd

_party_map = {'D': 'Democratic', 'R': 'Republican', 'U': 'Independent',
              'I': 'Independent'}


class MELegislatorScraper(LegislatorScraper):
    jurisdiction = 'me'

    def scrape(self, chamber, term):
        self.validate_term(term, latest_only=True)

        if chamber == 'upper':
            self.scrape_senators(chamber, term)
        elif chamber == 'lower':
            self.scrape_reps(chamber, term)

    def scrape_reps(self, chamber, term_name):
        url = 'http://www.maine.gov/legis/house/dist_mem.htm'
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        # There are 151 districts
        for district in xrange(1, 152):
            if (district % 10) == 0:
                path = '/html/body/p[%s]/a[3]' % (district + 4)
            else:
                path = '/html/body/p[%s]/a[2]' % (district + 4)

            try:
                link = page.xpath(path)[0]
            except IndexError:
                # If the the district % 10 == 0 query doesn't
                # produce a link, retry the second link. Horrible.
                path = '/html/body/p[%s]/a[2]' % (district + 4)
                link = page.xpath(path)[0]

            leg_url = link.get('href')
            name = link.text_content()

            if len(name) == 0:
                return
            if name.split()[0] == 'District':
                return

            mark = name.find('(')
            party = name[mark + 1]
            district_name = name[mark + 3:-1]
            name = name[15:mark]

            # vacant
            if party == "V":
                continue
            else:
                party = _party_map[party]

            leg = Legislator(term_name, chamber, str(district),
                             name, party=party, url=leg_url,
                             district_name=district_name)
            leg.add_source(url)
            leg.add_source(leg_url)

            # Get the photo url.
            html = self.urlopen(leg_url)
            doc = lxml.html.fromstring(html)
            doc.make_links_absolute(leg_url)

            # Get the default (B&W) photo url.
            photo_url = doc.xpath('//img')[0]
            if 'src' in photo_url.attrib:
                photo_url = photo_url.attrib.pop('src')
                leg['photo_url'] = photo_url
            else:
                photo_url = None

            # Try to get color photo from the GPO website.
            if party == 'Republican':
                xpath = '//a[contains(@href, "house_gop")]/@href'
                party_website_url = doc.xpath(xpath)[0]
                party_website_html = self.urlopen(party_website_url)
                if party_website_html.response.status_code == 200:
                    party_website = lxml.html.fromstring(party_website_html)
                    photo_url = party_website.xpath('//img/@src')[1]

            # Try to get color photo from the dems' website.
            elif party == 'Democratic':
                xpath = '//a[contains(@href, "housedems")]/@href'

                els = doc.xpath(xpath)
                if els:
                    party_website_url = els[0]

                try:
                    party_website_html = self.urlopen(party_website_url)
                except scrapelib.HTTPError:
                    # Sometimes the page doesn't exist.
                    pass
                else:
                    if party_website_html.response.status_code == 200:
                        party_website = lxml.html.fromstring(party_website_html)
                        photo_url = party_website.xpath('//img/@src')[1]

            self.scrape_lower_offices(leg, page, leg_url)
            self.save_legislator(leg)

    def scrape_lower_offices(self, legislator, list_page, url):
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        xpath = '//b[contains(., "Legislative Web Site:")]/../a/@href'
        url = doc.xpath(xpath)
        if url:
            url = url.pop()

        if 'housedems' in url:
            self.scrape_lower_offices_dem(legislator, doc)

        elif 'house_gop' in url:
            self.scrape_lower_offices_gop(legislator, url)

    def scrape_lower_offices_dem(self, legislator, doc):
        address = doc.xpath('//b[contains(., "Address:")]')[0].tail
        address = address.split(',', 1)
        address = '\n'.join(s.strip() for s in address)

        home_xpath = '//b[contains(., "Home Telephone:")]'
        home_phone = doc.xpath(home_xpath)
        if not home_phone:
            home_xpath = '//b[contains(., "Cell Phone:")]'
            home_phone = doc.xpath(home_xpath)
        if not home_phone:
            home_phone = None
        else:
            home_phone = home_phone.pop().tail

        xpath = '//a[contains(@href, "mailto")]'
        try:
            email = doc.xpath(xpath)[0].attrib['href'][7:]
            legislator['email'] = email
        except IndexError:
            # This beast has no email address.
            email = None

        address = ''.join(address)
        office = dict(
            name='District Office', type='district',
            phone=home_phone,
            fax=None, email=None,
            address=''.join(address))
        legislator.add_office(**office)

        business_xpath = '//b[contains(., "Business Telephone:")]'
        business_phone = doc.xpath(business_xpath)
        if business_phone:
            business_phone = business_phone[0].tail
            office = dict(
                name='District Office', type='district',
                phone=business_phone,
                fax=None, email=None,
                address=None)
            legislator.add_office(**office)

        # Add the dem main office.
        office = dict(
            name='House Democratic Office',
            type='capitol',
            address='\n'.join(['Room 333, State House',
                     '2 State House Station',
                     'Augusta, Maine 04333-0002']),
            fax=None, email=None,
            phone='(207) 287-1430')
        legislator.add_office(**office)

    def scrape_lower_offices_gop(self, legislator, url):
        # Get the www.maine.gov url.
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        # Handle any meta refresh.
        meta = doc.xpath('//meta')[0]
        attrib = meta.attrib
        if 'http-equiv' in attrib and attrib['http-equiv'] == 'REFRESH':
            _, url = attrib['content'].split('=', 1)
            html = self.urlopen(url)
            doc = lxml.html.fromstring(html)
            legislator.add_source(url)

        xpath = '//a[contains(@href, "mailto")]'
        try:
            email = doc.xpath(xpath)[0].attrib['href'][7:]
            legislator['email'] = email
        except IndexError:
            # This beast has no email address.
            email = None

        xpath = '//*[@id="innersidebarlargefont"]'
        text = doc.xpath(xpath)[0].text_content()

        lines = filter(None, text.strip().splitlines())
        lines = lines[1:-1]
        _ = lines.pop()
        _, phone = lines.pop().split(':', 1)
        phone = phone.strip()
        if not phone.strip():
            phone = None

        address = '\n'.join(lines)

        # Add the district office main office.
        office = dict(
            name='District Office',
            type='district',
            address=address,
            fax=None, email=None,
            phone=phone)
        legislator.add_office(**office)

        # Add the GOP main office.
        office = dict(
            name='House GOP Office',
            type='capitol',
            address='\n'.join(['Room 332, State House',
                     '2 State House Station',
                     'Augusta, Maine 04333-0002']),
            fax=None, email=None,
            phone='(207) 287-1440')
        legislator.add_office(**office)

    def scrape_senators(self, chamber, term):
        session = ((int(term[0:4]) - 2009) / 2) + 124

        mapping = {
            'district': 1,
            'first_name': 2,
            'middle_name': 3,
            'last_name': 4,
            # 'suffix': 6,
            'party': 6,
            'resident_county': 5,
            'street_addr': 7,
            'city': 8,
            'state': 9,
            'zip_code': 10,
            'phone1': 12,
            'phone2': 13,
            'email': 11,
        }

        url = (
            'http://legisweb1.mainelegislature.org/wp/senate/'
            'wp-content/uploads/sites/2/2013/09/%sthSenatorsList.xlsx' % session)

        try:
            fn, result = self.urlretrieve(url)
        except scrapelib.HTTPError:
            url = 'http://www.maine.gov/legis/senate/%dthSenatorsList.xls'
            url = url % session
            fn, result = self.urlretrieve(url)

        wb = xlrd.open_workbook(fn)
        sh = wb.sheet_by_index(0)

        for rownum in xrange(1, sh.nrows):
            # get fields out of mapping
            d = {}
            for field, col_num in mapping.iteritems():
                try:
                    d[field] = str(sh.cell(rownum, col_num).value)
                except IndexError:
                    # This col_num doesn't exist in the sheet.
                    pass

            full_name = " ".join((d['first_name'], d['middle_name'],
                                  d['last_name']))
            full_name = re.sub(r'\s+', ' ', full_name).strip()

            address = "{street_addr}\n{city}, ME {zip_code}".format(**d)

            # For matching up legs with votes
            district_name = d['city']

            phone = d['phone1']

            district = d['district'].split('.')[0]

            leg_url = 'http://www.maine.gov/legis/senate/bio%02ds.htm' % int(district)

            leg = Legislator(term, chamber, district, full_name,
                             d['first_name'], d['middle_name'], d['last_name'],
                             _party_map[d['party']],
                             resident_county=d['resident_county'],
                             office_address=address,
                             office_phone=phone,
                             email=None,
                             district_name=district_name,
                             url=leg_url)
            leg.add_source(url)
            leg.add_source(leg_url)

            html = self.urlopen(leg_url)
            doc = lxml.html.fromstring(html)
            doc.make_links_absolute(leg_url)
            xpath = '//td[@class="XSP_MAIN_PANEL"]/descendant::img/@src'
            photo_url = doc.xpath(xpath)
            if photo_url:
                photo_url = photo_url.pop()
                leg['photo_url'] = photo_url
            else:
                photo_url = None

            office = dict(
                name='District Office', type='district',
                fax=None, email=None,
                address=''.join(address))

            leg['email'] = d['email']
            leg.add_office(**office)
            self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = utils
import re


def clean_committee_name(comm_name):
    comm_name = comm_name.strip()
    comm_name = re.sub(' ?[-,] (Co|Vice)?[- ]?Chair$', '', comm_name)
    comm_name = re.sub('Appropriations - S/C:', 'Appropriations-S/C on',
                       comm_name)
    if comm_name == 'Appropriations-S/C Stimulus':
        comm_name = 'Appropriations-S/C on Stimulus'

    return comm_name


def parse_ftp_listing(text):
    lines = text.strip().split('\r\n')
    return (' '.join(line.split()[3:]) for line in lines)


def chamber_name(chamber):
    if chamber == 'upper':
        return 'senate'
    else:
        return 'house'


########NEW FILE########
__FILENAME__ = bills
import datetime
import re

from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

import lxml.html

BASE_URL = 'http://www.legislature.mi.gov'

def jres_id(n):
    """ joint res ids go from A-Z, AA-ZZ, etc. """
    return chr(ord('A')+(n-1)%25)*((n/26)+1)

bill_types = {'B':'bill',
              'R':'resolution',
              'CR':'concurrent resolution',
              'JR':'joint resolution'}

_categorizers = {
    'read a first time': 'bill:reading:1',
    'read a second time': 'bill:reading:2',
    'read a third time': 'bill:reading:3',
    'introduced by': 'bill:introduced',
    'passed': 'bill:passed',
    'referred to committee': 'committee:referred',
    'reported': 'committee:passed',
    'received': 'bill:introduced',
    'presented to governor': 'governor:received',
    'approved by governor': 'governor:signed',
    'adopted': 'bill:passed',
    'amendment(s) adopted': 'amendment:passed',
    'amendment(s) defeated': 'amendment:failed',
}

def categorize_action(action):
    for prefix, atype in _categorizers.iteritems():
        if action.lower().startswith(prefix):
            return atype

class MIBillScraper(BillScraper):
    jurisdiction = 'mi'

    def scrape_bill(self, chamber, session, bill_id):
        # try and get bill for current year
        url = 'http://legislature.mi.gov/doc.aspx?%s-%s' % (
            session[:4], bill_id.replace(' ', '-'))
        html = self.urlopen(url)
        # if first page isn't found, try second year
        if 'Page Not Found' in html:
            html = self.urlopen('http://legislature.mi.gov/doc.aspx?%s-%s'
                                % (session[-4:], bill_id.replace(' ','-')))
            if 'Page Not Found' in html:
                return None

        doc = lxml.html.fromstring(html)

        title = doc.xpath('//span[@id="frg_billstatus_ObjectSubject"]')[0].text_content()

        # get B/R/JR/CR part and look up bill type
        bill_type = bill_types[bill_id.split(' ')[0][1:]]

        bill = Bill(session=session, chamber=chamber, bill_id=bill_id,
                    title=title, type=bill_type)
        bill.add_source(url)

        # sponsors
        sp_type = 'primary'
        for sponsor in doc.xpath('//span[@id="frg_billstatus_SponsorList"]/a/text()'):
            sponsor = sponsor.replace(u'\xa0', ' ')
            bill.add_sponsor(sp_type, sponsor)
            sp_type = 'cosponsor'

        bill['subjects'] = doc.xpath('//span[@id="frg_billstatus_CategoryList"]/a/text()')

        # actions (skip header)
        for row in doc.xpath('//table[@id="frg_billstatus_HistoriesGridView"]/tr')[1:]:
            tds = row.xpath('td')  # date, journal link, action
            date = tds[0].text_content()
            journal = tds[1].text_content()
            action = tds[2].text_content()
            date = datetime.datetime.strptime(date, "%m/%d/%Y")
            # instead of trusting upper/lower case, use journal for actor
            actor = 'upper' if 'SJ' in journal else 'lower'
            type = categorize_action(action)
            bill.add_action(actor, action, date, type=type)

            # check if action mentions a vote
            rcmatch = re.search('Roll Call # (\d+)', action, re.IGNORECASE)
            if rcmatch:
                rc_num = rcmatch.groups()[0]
                # in format mileg.aspx?page=getobject&objectname=2011-SJ-02-10-011
                journal_link = tds[1].xpath('a/@href')
                if journal_link:
                    objectname = journal_link[0].rsplit('=', 1)[-1]
                    chamber_name = {'upper': 'Senate', 'lower': 'House'}[actor]
                    vote_url = BASE_URL + '/documents/%s/Journal/%s/htm/%s.htm' % (
                        session, chamber_name, objectname)
                    vote = Vote(actor, date, action, False, 0, 0, 0)
                    self.parse_roll_call(vote, vote_url, rc_num)

                    # check the expected counts vs actual
                    count = re.search('YEAS (\d+)', action, re.IGNORECASE)
                    count = int(count.groups()[0]) if count else 0
                    if count != len(vote['yes_votes']):
                        self.warning('vote count mismatch for %s %s, %d != %d' % 
                                     (bill_id, action, count, len(vote['yes_votes'])))
                    count = re.search('NAYS (\d+)', action, re.IGNORECASE)
                    count = int(count.groups()[0]) if count else 0
                    if count != len(vote['no_votes']):
                        self.warning('vote count mismatch for %s %s, %d != %d' % 
                                     (bill_id, action, count, len(vote['no_votes'])))

                    vote['yes_count'] = len(vote['yes_votes'])
                    vote['no_count'] = len(vote['no_votes'])
                    vote['other_count'] = len(vote['other_votes'])
                    vote['passed'] = vote['yes_count'] > vote['no_count']
                    vote.add_source(vote_url)
                    bill.add_vote(vote)
                else:
                    self.warning("missing journal link for %s %s" % 
                                 (bill_id, journal))

        # versions
        for row in doc.xpath('//table[@id="frg_billstatus_DocumentGridTable"]/tr'):
            version = self.parse_doc_row(row)
            if version:
                if version[1].endswith('.pdf'):
                    mimetype = 'application/pdf'
                elif version[1].endswith('.htm'):
                    mimetype = 'text/html'
                bill.add_version(*version, mimetype=mimetype)

        # documents
        for row in doc.xpath('//table[@id="frg_billstatus_HlaTable"]/tr'):
            document = self.parse_doc_row(row)
            if document:
                bill.add_document(*document)
        for row in doc.xpath('//table[@id="frg_billstatus_SfaTable"]/tr'):
            document = self.parse_doc_row(row)
            if document:
                bill.add_document(*document)

        self.save_bill(bill)
        return True

    def scrape(self, chamber, session):
        bill_types = {
            'upper': [('SB', 1), ('SR', 1), ('SCR', 1), ('SJR', 1)],
            'lower': [('HB', 4001), ('HR', 1), ('HCR', 1), ('HJR', 1)]
        }

        for abbr, start_num in bill_types[chamber]:
            n = start_num
            # keep trying bills until scrape_bill returns None
            while True:
                if 'JR' in abbr:
                    bill_id = '%s %s' % (abbr, jres_id(n))
                else:
                    bill_id = '%s %04d' % (abbr, n)
                if not self.scrape_bill(chamber, session, bill_id):
                    break
                n += 1

    def parse_doc_row(self, row):
        # first anchor in the row is HTML if present, otherwise PDF
        a = row.xpath('.//a')
        if a:
            name = row.xpath('.//b/text()')
            if name:
                name = name[0]
            else:
                name = row.text_content().strip()
            url = BASE_URL + a[0].get('href').replace('../', '/')
            return name, url

    def parse_roll_call(self, vote, url, rc_num):
        html = self.urlopen(url)
        if 'In The Chair' not in html:
            self.warning('"In The Chair" indicator not found, unable to extract vote')
            return
        vote_doc = lxml.html.fromstring(html)

        # split the file into lines using the <p> tags
        pieces = [p.text_content().replace(u'\xa0', ' ')
                  for p in vote_doc.xpath('//p')]

        # go until we find the roll call
        for i, p in enumerate(pieces):
            if p.startswith(u'Roll Call No. %s' % rc_num):
                break

        vtype = None

        # once we find the roll call, go through voters
        for p in pieces[i:]:
            # mdash: \xe2\x80\x94 splits Yeas/Nays/Excused/NotVoting
            if 'Yeas' in p:
                vtype = vote.yes
            elif 'Nays' in p:
                vtype = vote.no
            elif 'Excused' in p or 'Not Voting' in p:
                vtype = vote.other
            elif 'Roll Call No' in p:
                continue
            elif p.startswith('In The Chair:'):
                break
            elif vtype:
                # split on spaces not preceeded by commas
                for l in re.split('(?<!,)\s+', p):
                    if l:
                        vtype(l)
            else:
                self.warning('piece without vtype set: %s', p)

########NEW FILE########
__FILENAME__ = committees
import re
import urllib

from billy.scrape.committees import CommitteeScraper, Committee
import lxml.html

class MICommitteeScraper(CommitteeScraper):
    jurisdiction = 'mi'

    def scrape(self, chamber, term):
        self.validate_term(term, latest_only=True)

        if chamber == 'lower':
            self.scrape_house_committees()
        else:
            self.scrape_senate_committees()

    def scrape_house_committees(self):
        base_url = 'http://house.mi.gov/MHRPublic/CommitteeInfo.aspx?comkey='
        html = self.urlopen('http://house.mi.gov/mhrpublic/committee.aspx')
        doc = lxml.html.fromstring(html)

        # get values out of drop down
        for opt in doc.xpath('//option'):
            name = opt.text
            # skip invalid choice
            if opt.text in ('Statutory Committees', 'Select One'):
                continue
            if 'have not been created' in opt.text:
                self.warning('no committees yet for the house')
                return
            com_url = base_url + opt.get('value')
            com_html =  self.urlopen(com_url)
            cdoc = lxml.html.fromstring(com_html)
            com = Committee(chamber='lower', committee=name)
            com.add_source(com_url)

            for a in doc.xpath('//a[starts-with(@id, "memberLink")]'):
                name = a.text.strip()

            # all links to http:// pages in servicecolumn2 are legislators
            for a in cdoc.xpath('//div[@class="servicecolumn2"]//a[starts-with(@href, "http")]'):
                name = a.text.strip()
                text = a.xpath('following-sibling::span/text()')[0]
                if 'Committee Chair' in text:
                    role = 'chair'
                elif 'Vice-Chair' in text:
                    role = 'vice chair'
                else:
                    role = 'member'
                com.add_member(name, role=role)

            self.save_committee(com)

    def scrape_senate_committees(self):
        url = 'http://www.senate.michigan.gov/committee.html'
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        for link in doc.xpath('//li/a[contains(@href, "/committee/")]/@href'):
            if link.endswith('appropssubcommittee.html'):
                self.scrape_approp_subcommittees(link)
            elif not link.endswith(('statutory.htm','pdf','taskforce.html')):
                self.scrape_senate_committee(link)


    def scrape_senate_committee(self, url):
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        name = doc.xpath('//h3/text()')[0]

        com = Committee(chamber='upper', committee=name)

        for member in doc.xpath('//div[@id="committeeright"]//a'):
            member_name = member.text.strip()

            # don't add clerks
            if member_name == 'Committee Clerk':
                continue

            if 'Committee Chair' in member.tail:
                role = 'chair'
            elif 'Majority Vice' in member.tail:
                role = 'majority vice chair'
            elif 'Minority Vice' in member.tail:
                role = 'minority vice chair'
            else:
                role = 'member'

            com.add_member(member_name, role=role)

        com.add_source(url)
        self.save_committee(com)


    def scrape_approp_subcommittees(self, url):
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        for strong in doc.xpath('//strong'):
            com = Committee(chamber='upper', committee='Appropriations',
                            subcommittee=strong.text.strip())
            com.add_source(url)

            legislators = strong.getnext().tail.replace('Senators', '').strip()
            for leg in re.split(', | and ', legislators):
                if leg.endswith('(C)'):
                    role = 'chairman'
                    leg = leg[:-4]
                elif leg.endswith('(VC)'):
                    role = 'vice chairman'
                    leg = leg[:-5]
                elif leg.endswith('(MVC)'):
                    role = 'minority vice chairman'
                    leg = leg[:-6]
                else:
                    role = 'member'
                com.add_member(leg, role=role)

            self.save_committee(com)

########NEW FILE########
__FILENAME__ = events
import datetime as dt
import re

from billy.scrape.events import Event, EventScraper

import lxml.html
import pytz

mi_events = "http://legislature.mi.gov/doc.aspx?CommitteeMeetings"

class MIEventScraper(EventScraper):
    jurisdiction = 'mi'

    _tz = pytz.timezone('US/Eastern')

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def scrape_event_page(self, url, chamber, session):
        page = self.lxmlize(url)
        trs = page.xpath("//table[@id='frg_committeemeeting_MeetingTable']/tr")
        metainf = {}
        for tr in trs:
            tds = tr.xpath(".//td")
            if len(tds) <= 1:
                continue
            key = tds[0].text_content().strip()
            val = tds[1]
            metainf[key] = {
                "txt": val.text_content().strip(),
                "obj": val
            }

        if metainf == {}:
            return

        # Wednesday, 5/16/2012 3:00 pm
        datetime = "%s %s" % (
            metainf['Date']['txt'],
            metainf['Time']['txt']
        )
        if "Cancelled" in datetime:
            return

        translate = {
            "noon": " PM",
            "a.m.": " AM",
            "am": " AM"  # This is due to a nasty line they had.
        }

        for t in translate:
            if t in datetime:
                datetime = datetime.replace(t, translate[t])

        datetime = re.sub("\s+", " ", datetime)

        flag = "or after committees are given leave"

        if flag in datetime:
            datetime = datetime[:datetime.find(flag)].strip()

        datetime = datetime.replace('p.m.', 'pm')
        datetime = dt.datetime.strptime(datetime, "%A, %m/%d/%Y %I:%M %p")
        where = metainf['Location']['txt']
        title = metainf['Committee']['txt']  # XXX: Find a better title

        if chamber == 'other':
            chamber = 'joint'

        event = Event(session, datetime, 'committee:meeting',
                      title, location=where)
        event.add_source(url)
        event.add_source(mi_events)

        event.add_participant('chair', metainf['Chair']['txt'],
                              'legislator',
                              chamber=chamber)

        event.add_participant('host', metainf['Committee']['txt'],
                              'committee',
                              chamber=chamber)

        agenda = metainf['Agenda']['obj']
        agendas = agenda.text_content().split("\r")

        related_bills = agenda.xpath("//a[contains(@href, 'getObject')]")
        for bill in related_bills:
            description = agenda
            for a in agendas:
                if bill.text_content() in a:
                    description = a

            event.add_related_bill(
                bill.text_content(),
                description=description,
                type='consideration'
            )

        self.save_event(event)

    def scrape(self, chamber, session):
        page = self.lxmlize(mi_events)
        xpaths = {
            "lower": "//span[@id='frg_committeemeetings_HouseMeetingsList']",
            "upper": "//span[@id='frg_committeemeetings_SenateMeetingsList']",
            "other": "//span[@is='frg_committeemeetings_JointMeetingsList']"
        }
        span = page.xpath(xpaths[chamber])
        if len(span) > 0:
            span = span[0]
        else:
            return
        events = span.xpath("//a[contains(@href, 'committeemeeting')]")
        for event in events:
            self.scrape_event_page(event.attrib['href'], chamber, session)

########NEW FILE########
__FILENAME__ = legislators
import re

from billy.scrape.legislators import LegislatorScraper, Legislator
import lxml.html

abbr = {'D': 'Democratic', 'R': 'Republican'}

class MILegislatorScraper(LegislatorScraper):
    jurisdiction = 'mi'

    def scrape(self, chamber, term):
        self.validate_term(term, latest_only=True)
        if chamber == 'lower':
            return self.scrape_lower(chamber, term)
        return self.scrape_upper(chamber, term)

    def scrape_lower(self, chamber, term):
        url = 'http://www.house.mi.gov/mhrpublic/frmRepList.aspx'
        table = [
            "website",
            "district",
            "name",
            "party",
            "location",
            "phone",
            "email"
        ]
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        # skip two rows at top
        for row in doc.xpath('//table[@id="grvRepInfo"]/*'):
            tds = row.xpath('.//td')
            if len(tds) == 0:
                continue
            metainf = {}
            for i in range(0, len(table)):
                metainf[table[i]] = tds[i]
            district = str(int(metainf['district'].text_content().strip()))
            party = metainf['party'].text_content().strip()
            office = metainf['location'].text_content().strip()
            phone = metainf['phone'].text_content().strip()
            email = metainf['email'].text_content().strip()
            leg_url = metainf['website'].xpath("./a")[0].attrib['href']
            name = metainf['name'].text_content().strip()
            if name == 'Vacant':
                self.info('district %s is vacant', district)
                continue
            leg = Legislator(term=term,
                             chamber=chamber,
                             full_name=name,
                             district=district,
                             party=abbr[party],
                             url=leg_url)

            leg.add_office('capitol', 'Capitol Office',
                           address=office,
                           phone=phone,
                           email=email)

            leg.add_source(url)
            self.save_legislator(leg)

    def scrape_upper(self, chamber, term):
        url = 'http://www.senate.michigan.gov/members/memberlist.htm'
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        for row in doc.xpath('//table[@width=550]/tr')[1:39]:
            # party, dist, member, office_phone, office_fax, office_loc
            party, dist, member, phone, fax, loc = row.getchildren()
            party = abbr[party.text]
            district = dist.text_content().strip()
            name = member.text_content().strip()
            if name == 'Vacant':
                self.info('district %s is vacant', district)
                continue
            leg_url = member.xpath('a/@href')[0]
            office_phone = phone.text
            office_fax = fax.text
            office_loc = loc.text
            leg = Legislator(term=term, chamber=chamber,
                             district=district,
                             full_name=name,
                             party=party,
                             url=leg_url)

            leg.add_office('capitol', 'Capitol Office',
                           address=office_loc,
                           fax=office_fax,
                           phone=office_phone)


            leg.add_source(url)
            self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = bills
import re
import datetime
import urlparse
from collections import defaultdict
import lxml.html

from billy.scrape import NoDataForPeriod
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

# Base URL for the details of a given bill.
BILL_DETAIL_URL_BASE = 'https://www.revisor.mn.gov/revisor/pages/search_status/'
BILL_DETAIL_URL = ('https://www.revisor.mn.gov/bills/bill.php'
    '?b=%s&f=%s&ssn=0&y=%s')

# The versions of a bill use a different base URL.
VERSION_URL_BASE = 'https://www.revisor.mn.gov/bills/'
VERSION_URL = ('https://www.revisor.mn.gov/bin/getbill.php'
    '?session_year=%s&session_number=%s&number=%s&version=list')

# Search URL
BILL_SEARCH_URL = ('https://www.revisor.mn.gov/revisor/pages/search_status/'
    'status_results.php?body=%s&session=%s&bill=%s-%s'
    '&bill_type=%s&submit_bill=GO')


class MNBillScraper(BillScraper):
    jurisdiction = 'mn'

    # For testing purposes, this will do a lite version of things.  If
    # testing_bills is set, only these bills will be scraped.  Use SF0077
    testing = False
    testing_bills = [ 'SF1952' ]

    # Regular expressions to match category of actions
    _categorizers = (
        ('Introduced', 'bill:introduced'),
        ('Introduction and first reading, referred to',
         ['bill:introduced', 'committee:referred']),
        ('Committee report, to pass as amended and re-refer to', ['committee:referred']),
        ('Introduction and first reading', 'bill:introduced'),
        ('Referred (by Chair )?to', 'committee:referred'),
        ('Second reading', 'bill:reading:2'),
        ('Comm(ittee)? report: (T|t)o pass( as amended)? and re-refer(red)? to',
         ['committee:passed', 'committee:referred']),
        ('Comm(ittee)? report: (T|t)o pass( as amended)?', 'committee:passed'),
        ('Third reading Passed', 'bill:passed'),
        ('Bill was passed', 'bill:passed'),
        ('Third reading', 'bill:reading:3'),
        ("Governor('s action)? (A|a)pproval", 'governor:signed'),
        (".+? (V|v)eto", 'governor:vetoed'),
        ("Presented to Governor", 'governor:received'),
        ("Amended", 'amendment:passed'),
        ("Amendments offered", 'amendment:introduced'),
        (" repassed ", 'bill:passed'),
        (" re-referred ", 'committee:referred'),
        ("Received from", "bill:introduced"),
    )


    def scrape(self, chamber, session):
        """
        Scrape all bills for a given chamber and a given session.

        This method uses the legislature's search page to collect all the bills
        for a given chamber and session.
        """
        # If testing, print a message
        if self.is_testing():
            self.debug('TESTING...')

        # Get bill topics for matching later
        self.get_bill_topics(chamber, session)

        # If testing and certain bills to test, only test those
        if self.is_testing() and len(self.testing_bills) > 0:
            for b in self.testing_bills:
                bill_url = BILL_DETAIL_URL % (self.search_chamber(chamber), b,
                    session.split('-')[0])
                version_url = VERSION_URL % (self.search_session(session)[-4:],
                    self.search_session(session)[0], b)
                self.get_bill_info(chamber, session, bill_url, version_url)

            return

        # Find list of all bills
        bills = self.get_full_bill_list(chamber, session)

        # Get each bill
        for b in bills:
            self.get_bill_info(chamber, session, b['bill_url'], b['version_url'])


    def get_full_bill_list(self, chamber, session):
        """
        Uses the legislator search to get a full list of bills.  Search page
        returns a maximum of 500 results.
        """
        search_chamber = self.search_chamber(chamber)
        search_session = self.search_session(session)
        total_rows = list()
        bills = []
        stride = 500
        start = 0

        # If testing, only do a few
        total = 300 if self.is_testing() else 10000

        # Get total list of rows
        for bill_type in ('bill', 'concurrent', 'resolution'):
            for start in xrange(0, total, stride):
                # body: "House" or "Senate"
                # session: legislative session id
                # bill: Range start-end (e.g. 1-10)
                url = BILL_SEARCH_URL % (search_chamber, search_session, start,
                    start + stride, bill_type)

                # Parse HTML
                html = self.urlopen(url)
                doc = lxml.html.fromstring(html)

                # get table containing bills
                rows = doc.xpath('//table/tr')[1:]
                total_rows.extend(rows)

                # Out of rows
                if len(rows) == 0:
                    self.debug("Total Bills Found: %d" % len(total_rows))
                    break

        # Go through each row found
        for row in total_rows:
            bill = {}

            # Second column: status link
            bill_details_link = row.xpath('td[2]/a')[0]
            bill['bill_url'] = urlparse.urljoin(BILL_DETAIL_URL_BASE,
                bill_details_link.get('href'))

            # Version link sometimes goes to wrong place, forge it
            bill['version_url'] =  VERSION_URL % (search_session[-4:],
                search_session[0], bill_details_link.text_content())

            bills.append(bill)

        return bills


    def get_bill_info(self, chamber, session, bill_detail_url, version_list_url):
        """
        Extracts all the requested info for a given bill.

        Calls the parent's methods to enter the results into JSON files.
        """
        chamber = 'lower' if chamber.lower() == 'house' else chamber
        chamber = 'upper' if chamber.lower() == 'senate' else chamber

        # Get html and parse
        bill_html = self.urlopen(bill_detail_url)
        doc = lxml.html.fromstring(bill_html)

        # Get the basic parts of the bill
        bill_id = doc.xpath('//h1/text()')[0]
        bill_title = doc.xpath('//h2/following-sibling::p/text()')[0].strip()
        bill_type = {'F': 'bill', 'R':'resolution',
                     'C': 'concurrent resolution'}[bill_id[1]]
        bill = Bill(session, chamber, bill_id, bill_title, type=bill_type)

        # Add source
        bill.add_source(bill_detail_url)

        # Add subjects.  Currently we are not mapping to Open States
        # standardized subjects, so use 'scraped_subjects'
        bill['scraped_subjects'] = self._subject_mapping[bill_id]

        # Get companion bill.
        companion = doc.xpath('//table[@class="status_info"]//tr[1]/td[2]/a[starts-with(@href, "?")]/text()')
        companion = self.make_bill_id(companion[0]) if len(companion) > 0 else None
        companion_chamber = self.chamber_from_bill(companion)
        if companion is not None:
          bill.add_companion(companion, chamber=companion_chamber)

        # Grab sponsors
        bill = self.extract_sponsors(bill, doc, chamber)

        # Add Actions performed on the bill.
        bill = self.extract_actions(bill, doc, chamber)

        # Get all versions of the bill.
        bill = self.extract_versions(bill, doc, chamber, version_list_url)

        self.save_bill(bill)


    def get_bill_topics(self, chamber, session):
        """
        Uses the leg search to map topics to bills.
        """
        search_chamber = {'lower':'House', 'upper':'Senate'}[chamber]
        search_session = self.metadata['session_details'][session]['site_id']
        self._subject_mapping = defaultdict(list)

        url = '%sstatus_search.php?body=%s&search=topic&session=%s' % (
            BILL_DETAIL_URL_BASE, search_chamber, search_session)
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        # For testing purposes, we don't really care about getting
        # all the topics, just a few
        if self.is_testing():
            option_set = doc.xpath('//select[@name="topic[]"]/option')[0:5]
        else:
            option_set = doc.xpath('//select[@name="topic[]"]/option')[0:]

        for option in option_set:
            # Subjects look like "Name of Subject (##)" -- split off the #
            subject = option.text.rsplit(' (')[0]
            value = option.get('value')
            opt_url = '%sstatus_results.php?body=%s&search=topic&session=%s&topic[]=%s' % (
                BILL_DETAIL_URL_BASE, search_chamber, search_session, value)
            opt_html = self.urlopen(opt_url)
            opt_doc = lxml.html.fromstring(opt_html)
            for bill in opt_doc.xpath('//table/tr/td[2]/a/text()'):
                bill = self.make_bill_id(bill)
                self._subject_mapping[bill].append(subject)


    def extract_actions(self, bill, doc, current_chamber):
        """
        Extract the actions taken on a bill.
        A bill can have actions taken from either chamber.  The current
        chamber's actions will be the first table of actions. The other
        chamber's actions will be in the second table.
        """

        bill_actions = list()
        action_tables = doc.xpath('//table[@class="actions"]')

        for cur_table in action_tables:
            for row in cur_table.xpath('.//tr'):
                bill_action = dict()

                # Split up columns
                date_col, the_rest = row.xpath('td')

                # The second column can hold a link to full text
                # and pages (what should be in another column),
                # but also links to committee elements or other spanned
                # content.
                action_date = date_col.text_content().strip()
                action_text = the_rest.text.strip()
                committee = the_rest.xpath("a[contains(@href,'committee')]/text()")
                extra = ''.join(the_rest.xpath('span[not(@style)]/text() | a/text()'))

                # skip non-actions (don't have date)
                if action_text in ('Chapter number', 'See also', 'See',
                                   'Effective date', 'Secretary of State'):
                    continue

                # dates are really inconsistent here, sometimes in action_text
                try:
                    action_date = datetime.datetime.strptime(action_date,
                                                             '%m/%d/%Y')
                except ValueError:
                    try:
                        action_date = datetime.datetime.strptime(extra,
                                                                 '%m/%d/%y')
                    except ValueError:
                        try:
                            action_date = datetime.datetime.strptime(
                                extra, '%m/%d/%Y')
                        except ValueError:
                            self.warning('ACTION without date: %s' %
                                         action_text)
                            continue

                # categorize actions
                action_type = 'other'
                for pattern, atype in self._categorizers:
                    if re.match(pattern, action_text):
                        action_type = atype
                        if 'committee:referred' in action_type and len(committee) > 0:
                            bill_action['committees'] = committee[0]
                        break

                if extra:
                    action_text += ' ' + extra
                bill_action['action_text'] = action_text
                if isinstance(action_type, list):
                    for atype in action_type:
                        if atype.startswith('governor'):
                            bill_action['action_chamber'] = 'executive'
                            break
                    else:
                        bill_action['action_chamber'] = current_chamber
                else:
                    if action_type.startswith('governor'):
                        bill_action['action_chamber'] = 'executive'
                    else:
                        bill_action['action_chamber'] = current_chamber
                bill_action['action_date'] = action_date
                bill_action['action_type'] = action_type
                bill_actions.append(bill_action)

                # Try to extract vote
                bill = self.extract_vote_from_action(bill, bill_action, current_chamber, row)

            # if there's a second table, toggle the current chamber
            if current_chamber == 'upper':
                current_chamber = 'lower'
            else:
                current_chamber = 'upper'


        # Add acctions to bill
        for action in bill_actions:
            kwargs = {}
            if 'committees' in action:
                kwargs['committees'] = action['committees']

            bill.add_action(action['action_chamber'],
                            action['action_text'],
                            action['action_date'],
                            type=action['action_type'],
                            **kwargs)

        return bill


    def extract_sponsors(self, bill, doc, chamber):
        """
        Extracts sponsors from bill page.
        """
        sponsors = doc.xpath('//h2[text()="Authors"]/following-sibling::ul[1]/li/a/text()')
        if sponsors:
            primary_sponsor = sponsors[0].strip()
            bill.add_sponsor('primary', primary_sponsor, chamber=chamber)
            cosponsors = sponsors[1:]
            for leg in cosponsors:
                bill.add_sponsor('cosponsor', leg.strip(), chamber=chamber)

        other_sponsors = doc.xpath('//h3[contains(text(), "Authors")]/following-sibling::ul[1]/li/a/text()')
        for leg in other_sponsors:
            bill.add_sponsor('cosponsor', leg.strip(), chamber=self.other_chamber(chamber))

        return bill


    def extract_versions(self, bill, doc, chamber, version_list_url):
      """
      Versions of a bill are on a separate page, linked to from the column
      labeled, "Bill Text", on the search results page.
      """
      version_html = self.urlopen(version_list_url)
      if 'resolution' in version_html.response.url:
          bill.add_version('resolution text', version_html.response.url,
              mimetype='text/html')
      else:
          version_doc = lxml.html.fromstring(version_html)
          for v in version_doc.xpath('//a[starts-with(@href, "text.php")]'):
              version_url = urlparse.urljoin(VERSION_URL_BASE, v.get('href'))
              if 'pdf' not in version_url:
                  bill.add_version(v.text.strip(), version_url,
                                   mimetype='text/html',
                                   on_duplicate='use_new')

      return bill


    def extract_vote_from_action(self, bill, action, chamber, action_row):
        """
        Gets vote data.  For the Senate, we can only get yes and no
        counts, but for the House, we can get details on who voted
        what.

        TODO: Follow links for Houses and get votes for individuals.

        About votes:
        https://billy.readthedocs.org/en/latest/scrapers.html#billy.scrape.votes.Vote
        """

        # Check if there is vote at all
        has_vote = action_row.xpath('td/span[contains(text(), "vote:")]')
        if len(has_vote) > 0:
            vote_element = has_vote[0]
            parts = re.match(r'vote:\s+([0-9]*)-([0-9]*)', vote_element.text_content())
            if parts is not None:
                yeas = int(parts.group(1))
                nays = int(parts.group(2))

                # Check for URL
                vote_url = None
                if len(vote_element.xpath('a[@href]')) > 0:
                    vote_url = vote_element.xpath('a[@href]')[0].get('href')

                # Vote found
                vote = Vote(chamber, action['action_date'],
                    action['action_text'], yeas > nays, yeas, nays, 0)
                # Add source
                if vote_url is not None:
                    vote.add_source(vote_url)
                # Attach to bill
                bill.add_vote(vote)

        return bill


    def make_bill_id(self, bill):
        """
        Given a string, ensure that it is in a consistent format.  Bills
        can be written as HF 123, HF123, or HF0123.

        Historically, HF 123 has been used for top level bill id.
        (HF0123 is a better id and should be considered in the future)
        """
        if bill is None:
            return bill

        return re.sub(r'(\w+?)0*(\d+)', r'\1 \2', bill)


    def chamber_from_bill(self, bill):
        """
        Given a bill id, determine chamber.
        """
        if bill is None:
            return bill

        return 'lower' if bill.lower().startswith('hf') else 'upper'


    def other_chamber(self, chamber):
        """
        Given a chamber, get the other.
        """
        return 'lower' if chamber == 'upper' else 'upper'


    def search_chamber(self, chamber):
        """
        Given chamber, like lower, make into MN site friendly search chamber.
        """
        return { 'lower':'House', 'upper':'Senate' }[chamber]


    def search_session(self, session):
        """
        Given session ID, make into MN site friendly search.
        """
        return self.metadata['session_details'][session]['site_id']


    def is_testing(self):
        """
        Determine if this is test mode.
        """
        return False if self.testing is False or self.testing is None else True

########NEW FILE########
__FILENAME__ = committees
import re
import datetime

from billy.scrape import NoDataForPeriod
from billy.scrape.committees import Committee, CommitteeScraper
import lxml.html

def fix_whitespace(s):
    return re.sub(r'\s+', ' ', s)

class MNCommitteeScraper(CommitteeScraper):
    jurisdiction = 'mn'
    latest_only = True

    def scrape(self, term, chambers):
        if 'upper' in chambers:
            self.scrape_senate_committees(term)
        if 'lower' in chambers:
            self.scrape_house_committees(term)

    def scrape_senate_committees(self, term):
        for t in self.metadata['terms']:
            if term == t['name']:
                biennium = t['biennium']
                break

        url = 'http://www.senate.mn/committees/index.php?ls=%s' % biennium

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        for link in doc.xpath('//a[contains(@href, "committee_bio")]/@href'):
            self.scrape_senate_committee(term, link)

    def scrape_senate_committee(self, term, url):
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        com_name = doc.xpath('//a[contains(@href, "committee_bio")]/text()')[0]
        parent = doc.xpath('//h4//a[contains(@href, "committee_bio")]/text()')
        if parent:
            self.log('%s is subcommittee of %s', com_name, parent[0])
            com = Committee('upper', parent[0], subcommittee=com_name)
        else:
            com = Committee('upper', com_name)

        for link in doc.xpath('//div[@id="members"]//a[contains(@href, "member_bio")]'):
            name = link.text_content().strip()
            if name:
                position = link.xpath('.//preceding-sibling::b/text()')
                if not position:
                    position = 'member'
                elif position[0] == 'Chair:':
                    position = 'chair'
                elif position[0] == 'Vice Chair:':
                    position = 'vice chair'
                elif position[0] == 'Ranking Minority Member:':
                    position = 'ranking minority member'
                else:
                    raise ValueError('unknown position: %s' % position[0])

                name = name.split(' (')[0]
                com.add_member(name, position)

        com.add_source(url)
        self.save_committee(com)

    def scrape_house_committees(self, term):
        url = 'http://www.house.leg.state.mn.us/comm/commemlist.asp'

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        for com in doc.xpath('//h2[@class="commhighlight"]'):
            members_url = com.xpath('following-sibling::p[1]/a[text()="Members"]/@href')[0]

            com = Committee('lower', com.text)
            com.add_source(members_url)

            member_html = self.urlopen(members_url)
            mdoc = lxml.html.fromstring(member_html)

            # each legislator in their own table
            # first row, second column contains all the info
            for ltable in mdoc.xpath('//table/tr[1]/td[2]/p/b[1]'):

                # name is tail string of last element
                name = ltable.text_content()
                text = ltable.text
                if text and name != text:
                    name = name.replace(text, '')

                # role is inside a nested b tag
                role = ltable.xpath('b/*/text()')
                if role:
                    # if there was a role, remove it from name
                    role = role[0]
                    name = name.replace(role, '')
                else:
                    role = 'member'
                name = name.split(' (')[0]
                com.add_member(name, role)

            # save
            self.save_committee(com)

########NEW FILE########
__FILENAME__ = events
from datetime import datetime as datetime
import re

from billy.scrape import NoDataForPeriod
from billy.scrape.events import Event, EventScraper

import lxml.html
import pytz

url = "http://www.leg.state.mn.us/calendarday.aspx?jday=all"

class MNEventScraper(EventScraper):
    jurisdiction = 'mn'
    date_formats = (
        '%A, %B %d, %Y %I:%M %p',
        '%A, %B %d'
    )

    def scrape(self, chamber, session):
        self.session = session

        page = self.lxmlize(url)

        commission_meetings = page.xpath("//div[@class='Comm_item']")
        self.scrape_meetings(commission_meetings, 'commission')

        house_meetings = page.xpath("//div[@class='house_item']")
        self.scrape_meetings(house_meetings, 'house')

        senate_meetings = page.xpath("//div[@class='senate_item']")
        self.scrape_meetings(senate_meetings, 'senate')

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def scrape_meetings(self, meetings, group):
        """
        Scrape and save event data from a list of meetings.

        Arguments:
        meetings -- A list of lxml elements containing event information
        group -- The type of meeting. The legislature site applies
                 different formatting to events based on which group
                 they correspond to.  `group` should be one of the
                 following strings: 'house', 'senate', or 'commission'.

        """
        for meeting in meetings:
            when = self.get_date(meeting)
            description = self.get_description(meeting)
            location = self.get_location(meeting)

            if when and description and location:
                kwargs = {}
                if group in self.metadata['chambers'].keys():
                    kwargs['chamber'] = group
                agenda = self.get_agenda(meeting)
                if agenda:
                    kwargs['agenda'] = agenda

                # Event prototype is as follows:
                # class Event(SourcedObject):
                #    def __init__(self, session, when, type,
                #                 description, location, end=None, **kwargs)
                event = Event(self.session, when, 'committee:meeting',
                        description, location, **kwargs)
                event.add_source(url)
                self.save_event(event)

    def get_date(self, meeting):
        """
        Get the date from a meeting lxml element.

        Arguments:
        meeting -- A lxml element containing event information

        """
        date_raw = meeting.xpath(".//b")
        if len(date_raw) < 1:
            return

        date_string = date_raw[0].text_content().strip()

        for date_format in self.date_formats:
            try:
                date = datetime.strptime(date_string, date_format)
                return date
            except ValueError:
                pass

    def get_description(self, meeting, i=0):
        """
        Get the description from a meeting lxml element.

        Because some events include a "House" or "Senate" `span`
        before the `span` containing the description, a repetitive
        search is necessary.

        TODO Other events include a date span before the span
        containing the description.

        Arguments:
        meeting -- A lxml element containing event information
        i -- The index of `a`/`span` tags to look for.

        """
        description_raw = meeting.xpath(".//a")
        if (len(description_raw) < 1 or
                description_raw[0].text_content() == ''):
            description_raw = meeting.xpath(".//span")
        if len(description_raw) < (i + 1):
            return

        description = description_raw[i].text_content().strip()

        if description == 'House' or description == 'Senate':
            return self.get_description(meeting, i + 1)

        return description

    def get_location(self, meeting):
        """
        Get the location from a meeting lxml element.

        Location information follows a `b` element containing the text
        "Room:".

        Arguments:
        meeting -- A lxml element containing event information

        """
        return self.get_tail_of(meeting, '^Room:')

    def get_agenda(self, meeting):
        """
        Get the agenda from a meeting lxml element.

        Agenda information follows a `b` element containing the text
        "Agenda:".

        Arguments:
        meeting -- A lxml element containing event information

        """
        return self.get_tail_of(meeting, '^Agenda:')

    def get_tail_of(self, meeting, pattern_string):
        """
        Get the tail of a `b` element matching `pattern_string`, all
        inside a `p` tag.

        Surprisingly useful for the markup on the Minnesota
        legislative events calendar page.

        Arguments:
        pattern_string -- A regular expression string to match
                          against

        """
        pattern = re.compile(pattern_string)

        p_tags = meeting.xpath(".//p")
        if len(p_tags) < 1:
            return

        p_tag = p_tags[0]

        for element in p_tag.iter():
            if element.tag == 'b':
                raw = element.text_content().strip()
                r = pattern.search(raw)
                if r and element.tail:
                    tail = element.tail.strip()
                    if tail != '':
                        return tail
                    break
        return


########NEW FILE########
__FILENAME__ = legislators
import csv
from collections import defaultdict
from cStringIO import StringIO

from billy.scrape.legislators import Legislator, LegislatorScraper
from billy.scrape import NoDataForPeriod

import lxml.html

class MNLegislatorScraper(LegislatorScraper):
    jurisdiction = 'mn'
    latest_only = True

    _parties = {'DFL': 'Democratic-Farmer-Labor',
                'R': 'Republican'}

    def scrape(self, chamber, term):
        if chamber == 'lower':
            self.scrape_house(term)
        else:
            self.scrape_senate(term)

    def scrape_house(self, term):
        url = 'http://www.house.leg.state.mn.us/members/housemembers.asp'

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        # skip first header row
        for row in doc.xpath('//tr')[1:]:
            tds = [td.text_content().strip() for td in row.xpath('td')]
            if len(tds) == 5:
                district = tds[0].lstrip('0')
                name, party = tds[1].rsplit(' ', 1)
                if party == '(R)':
                    party = 'Republican'
                elif party == '(DFL)':
                    party = 'Democratic-Farmer-Labor'
                leg_url = row.xpath('td[2]/p/a/@href')[0]
                addr = tds[2]
                phone = tds[3]
                email = tds[4]

            leg = Legislator(term, 'lower', district, name,
                             party=party, email=email, url=leg_url)

            addr = ('{0} State Office Building\n'
                    '100 Rev. Dr. Martin Luther King Jr. Blvd.\n'
                    'St. Paul, MN 55155').format(addr)
            leg.add_office('capitol', 'Capitol Office', address=addr,
                           phone=phone)

            # add photo_url
            leg_html = self.urlopen(leg_url)
            leg_doc = lxml.html.fromstring(leg_html)
            img_src = leg_doc.xpath('//img[contains(@src, "memberimg")]/@src')
            if img_src:
                leg['photo_url'] = img_src[0]

            leg.add_source(url)
            leg.add_source(leg_url)
            self.save_legislator(leg)

    def scrape_senate(self, term):

        index_url = 'http://www.senate.mn/members/index.php'
        doc = lxml.html.fromstring(self.urlopen(index_url))
        doc.make_links_absolute(index_url)

        leg_data = defaultdict(dict)

        # get all the tds in a certain div
        tds = doc.xpath('//div[@id="hide_show_alpha_all"]//td[@style="vertical-align:top;"]')
        for td in tds:
            # each td has 2 <a>s- site & email
            main_link, email = td.xpath('.//a')
            # get name
            name = main_link.text_content().split(' (')[0]
            leg = leg_data[name]
            leg['leg_url'] = main_link.get('href')
            leg['photo_url'] = td.xpath('./preceding-sibling::td/a/img/@src')[0]
            if 'mailto:' in email.get('href'):
                leg['email'] = email.get('href').replace('mailto:', '')

        self.info('collected preliminary data on %s legislators', len(leg_data))
        assert leg_data

        # use CSV for most of data
        csv_url = 'http://www.senate.mn/members/member_list_ascii.php?ls='
        csvfile = self.urlopen(csv_url)

        for row in csv.DictReader(StringIO(csvfile)):
            if not row['First Name']:
                continue
            name = '%s %s' % (row['First Name'], row['Last Name'])
            party = self._parties[row['Party']]
            leg = Legislator(term, 'upper', row['District'].lstrip('0'), name,
                             party=party,
                             first_name=row['First Name'],
                             last_name=row['Last Name'],
                             **leg_data[name]
                            )
            row['rmnum'] = row['Rm. Number']
            leg.add_office('capitol', 'Capitol Office',
                           address='{rmnum} {Office Building}\n{Office Address}\n{City}, {State} {Zipcode}'.format(**row),
                           phone='%s-%s' % (row['Area Code'], row['Office Phone']))


            leg.add_source(csv_url)
            leg.add_source(index_url)

            self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = votes
import re
import datetime
import itertools

from billy.scrape import NoDataForPeriod
from billy.scrape.votes import VoteScraper, Vote
import lxml.html

class MNVoteScraper(VoteScraper):
    jurisdiction = 'mn'

    yeanay_re = re.compile(r'(\d+) YEA and (\d+) Nay')
    date_re = re.compile(r'Date: (\d+/\d+/\d+)')

    def scrape(self, chamber, session):
        self.validate_session(session)
        votes_url = self.metadata['session_details'][session].get('votes_url')
        if not votes_url:
            self.warning('no house votes URL for %s', session)
            return
        html = self.urlopen(votes_url)
        doc = lxml.html.fromstring(html)
        prefix = {'lower': 'H', 'upper': 'S'}[chamber]
        xpath = '//a[contains(@href, "votesbynumber.asp?billnum=%s")]' % prefix
        links = doc.xpath(xpath)
        for link in links:
            bill_id = link.text
            link_url = link.get('href')
            self.scrape_votes(chamber, session, bill_id, link_url)

    def scrape_votes(self, chamber, session, bill_id, link_url):
        html = self.urlopen(link_url)
        doc = lxml.html.fromstring(html)
        for vote_url in doc.xpath('//a[starts-with(text(), "View Vote")]/@href'):
            self.scrape_vote(chamber, session, bill_id, vote_url)

    def scrape_vote(self, chamber, session, bill_id, vote_url):
        NO_VOTE_URL = 'http://www.house.leg.state.mn.us/votes/novotefound.asp'
        html = self.urlopen(vote_url)

        # sometimes the link is broken, will redirect to NO_VOTE_URL
        if html.response.url == NO_VOTE_URL:
            return

        doc = lxml.html.fromstring(html)
        paragraphs = doc.xpath('//h1/following-sibling::p')

        # first paragraph has motion and vote total
        top_par = paragraphs[0].text_content()
        lines = top_par.splitlines()
        # 3rd line is the motion except in cases where first line is gone
        motion = lines[2] or lines[1]
        # last line is "__ YEA and __ Nay"
        yeas, nays = self.yeanay_re.match(lines[-1]).groups()
        yeas = int(yeas)
        nays = int(nays)

        # second paragraph has date
        date = self.date_re.match(paragraphs[1].text_content()).groups()[0]
        date = datetime.datetime.strptime(date, '%m/%d/%Y')

        vote = Vote('lower', date, motion, yeas>nays, yeas, nays, 0,
                    session=session, bill_id=bill_id, bill_chamber=chamber)
        vote.add_source(vote_url)

        # first table has YEAs
        for name in doc.xpath('//table[1]/tr/td/font/text()'):
            vote.yes(name.strip())

        # second table is nays
        for name in doc.xpath('//table[2]/tr/td/font/text()'):
            vote.no(name.strip())

        self.save_vote(vote)

########NEW FILE########
__FILENAME__ = bills
import re
import datetime as dt
import scrapelib

from collections import defaultdict

import lxml.html

from billy.scrape import NoDataForPeriod
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

from utils import (clean_text, house_get_actor_from_action,
                   senate_get_actor_from_action,find_nodes_with_matching_text)

bill_types = {
    "HB " : "bill",
    "HJR" : "joint resolution",
    "HCR" : "concurrent resolution",
    "SB " : "bill",
    "SJR" : "joint resolution",
    "SCR" : "concurrent resolution"
}

class MOBillScraper(BillScraper):

    jurisdiction = 'mo'
    senate_base_url = 'http://www.house.mo.gov'
    # a list of URLS that aren't working when we try to visit them (but probably should work):
    bad_urls = []
    subjects = defaultdict(list)

    def __init__(self, *args, **kwargs):
        super(BillScraper, self).__init__(*args, **kwargs)
        lk_session = self.metadata['terms'][0]['sessions'][-1]
        self.scrape_subjects(lk_session)

    def url_xpath(self, url, path):
        doc = lxml.html.fromstring(self.urlopen(url))
        return doc.xpath(path)

    def scrape_subjects(self, session):
        self.scrape_house_subjects(session)
        self.scrape_senate_subjects(session)

    def scrape_senate_subjects(self, session):
        short = session[2:4]
        subject_url = "http://www.senate.mo.gov/%sinfo/BTS_Web/Keywords.aspx?SessionType=R" % (
            short
        )
        subject_page = self.urlopen(subject_url)
        subject_page = lxml.html.fromstring(subject_page)
        subjects = subject_page.xpath("//h3")
        for subject in subjects:
            subject_text = subject.text_content()
            if ")" in subject_text:
                subject_text = subject_text[:subject_text.find("(")].strip()

            bills = subject.getnext().xpath("./b/a")
            for bill in bills:
                bill_id = bill.text.replace(" ", "")
                self.subjects[bill_id].append(subject_text)

    def scrape_house_subjects(self, session):
        subject_url = "http://house.mo.gov/subjectindexlist.aspx"
        subject_page = self.urlopen(subject_url)
        subject_page = lxml.html.fromstring(subject_page)
        # OK. Let's load all the subjects up.
        subjects = subject_page.xpath("//ul[@id='subjectindexitems']/div/li")
        for subject in subjects:
            ahref = subject.xpath("./a")[0]
            if ahref.text_content().strip() == "":
                continue
            link = ahref.attrib['href']
            link = self.senate_base_url + "/" + link
            rows = self.url_xpath(link, "//table[@id='reportgrid']/tbody/tr")
            for row in rows:
                bill_id = row.xpath("./td")[0].xpath("./a")
                if len(bill_id) == 0:
                    continue
                bill_id = bill_id[0].text
                self.subjects[bill_id].append(subject.text_content())

    def scrape(self, chamber, year):
        # wrapper to call senate or house scraper. No year check
        # here, since house and senate have different backdates
        if chamber == 'upper':
            self.scrape_senate(year)
        elif chamber == 'lower':
            self.scrape_house(year)
        if len(self.bad_urls) > 0:
            self.log("WARNINGS:")
            for url in self.bad_urls:
                self.log( "%s" % url )

    def scrape_senate(self, year):
        # We only have data from 2005-present
        if int(year) < 2005 or int(year) > dt.date.today().year:
            raise NoDataForPeriod(year)

        year2 = "%02d" % (int(year) % 100)

        # year is mixed in to the directory. set a root_url, since
        # we'll use it later
        bill_root = 'http://www.senate.mo.gov/%sinfo/BTS_Web/' % year2
        index_url = bill_root + 'BillList.aspx?SessionType=R'
        #print "index = %s" % index_url

        index_page = self.urlopen(index_url)
        index_page = lxml.html.fromstring(index_page)
        # each bill is in it's own table (nested in a larger table)
        bill_tables = index_page.xpath('//a[@id]')

        if not bill_tables:
            return

        for bill_table in bill_tables:
            # here we just search the whole table string to get
            # the BillID that the MO senate site uses
            if re.search(r'dgBillList.*hlBillNum',bill_table.attrib['id']):
                #print "keys = %s" % bill_table.attrib['id']
                #print "table = %s " % bill_table.attrib.get('href')
                self.parse_senate_billpage(bill_root + bill_table.attrib.get('href'), year)
                #print "one down!"

    def parse_senate_billpage(self, bill_url, year):
        bill_page = self.urlopen(bill_url)
        bill_page = lxml.html.fromstring(bill_page)
        # get all the info needed to record the bill
        # TODO probably still needs to be fixed
        bill_id = bill_page.xpath('//*[@id="lblBillNum"]')[0].text_content()
        bill_title = bill_page.xpath('//*[@id="lblBillTitle"]')[0].text_content()
        bill_desc = bill_page.xpath('//*[@id="lblBriefDesc"]')[0].text_content()
        bill_lr = bill_page.xpath('//*[@id="lblLRNum"]')[0].text_content()
        #print "bill id = "+ bill_id

        bill_type = "bill"
        triplet = bill_id[:3]
        if triplet in bill_types:
            bill_type = bill_types[triplet]

        subs = []
        bid = bill_id.replace(" ", "")

        if bid in self.subjects:
            subs = self.subjects[bid]
            self.log("With subjects for this bill")

        self.log(bid)

        bill = Bill(year, 'upper', bill_id, bill_desc,
                    bill_lr=bill_lr, type=bill_type, subjects=subs)
        bill.add_source(bill_url)

        # Get the primary sponsor
        sponsor = bill_page.xpath('//*[@id="hlSponsor"]')[0]
        bill_sponsor = sponsor.text_content()
        bill_sponsor_link = sponsor.attrib.get('href')
        bill.add_sponsor('primary', bill_sponsor, sponsor_link=bill_sponsor_link)

        # cosponsors show up on their own page, if they exist
        cosponsor_tag = bill_page.xpath('//*[@id="hlCoSponsors"]')
        if len(cosponsor_tag) > 0 and cosponsor_tag[0].attrib.has_key('href'):
            self.parse_senate_cosponsors(bill, cosponsor_tag[0].attrib['href'])

        # get the actions
        action_url = bill_page.xpath('//*[@id="hlAllActions"]')
        if len(action_url) > 0:
            action_url =  action_url[0].attrib['href']
            #print "actions = %s" % action_url
            self.parse_senate_actions(bill, action_url)

        # stored on a separate page
        versions_url = bill_page.xpath('//*[@id="hlFullBillText"]')
        if len(versions_url) > 0 and versions_url[0].attrib.has_key('href'):
            self.parse_senate_bill_versions(bill, versions_url[0].attrib['href'])

        self.save_bill(bill)

    def parse_senate_bill_versions(self, bill, url):
        bill.add_source(url)
        versions_page = self.urlopen(url)
        versions_page = lxml.html.fromstring(versions_page)
        version_tags = versions_page.xpath('//li/font/a')
        for version_tag in version_tags:
            description = version_tag.text_content()
            pdf_url = version_tag.attrib['href']
            if pdf_url.endswith('pdf'):
                mimetype = 'application/pdf'
            else:
                mimetype = None
            bill.add_version(description, pdf_url, mimetype=mimetype,
                             on_duplicate='use_new')

    def get_action(self, actor, action):
        # Alright. This covers both chambers and everyting else.
        flags = {
            "Introduced"       : "bill:introduced",
            "Offered"          : "bill:introduced",
            "First Read"       : "bill:reading:1",
            "Read Second Time" : "bill:reading:2",
            "Second Read"      : "bill:reading:2",
            "Third Read"       : "bill:reading:3",
            "Referred"         : "committee:referred",
            "Withdrawn"        : "bill:withdrawn",
            "S adopted"        : "bill:passed",

            "Truly Agreed To and Finally Passed": "bill:passed",
            "Third Read and Passed": "bill:passed",

            "Approved by Governor" : "governor:signed",
        }
        ret = []
        for flag in flags:
            if flag in action:
                ret.append(flags[flag])
        if len(ret) == 0:
            ret.append("other")
        return ret

    def get_votes(self, date, actor, action):
        ret = []
        vre = r"(?P<leader>.*)(AYES|YEAS):\s+(?P<yeas>\d+)\s+(NOES|NAYS):\s+(?P<nays>\d+).*"
        if "YEAS" in action.upper() or "AYES" in action.upper():
            match = re.match(vre, action)
            if match:
                v = match.groupdict()
                yes, no = int(v['yeas']), int(v['nays'])
                vote = Vote(actor, date, v['leader'],
                            (yes > no), yes, no, 0)
                ret.append(vote)
        return ret


    def parse_senate_actions(self, bill, url):
        bill.add_source(url)
        actions_page = self.urlopen(url)
        actions_page = lxml.html.fromstring(actions_page)
        bigtable = actions_page.xpath('/html/body/font/form/table/tr[3]/td/div/table/tr')

        for row in bigtable:
            date = row[0].text_content()
            date = dt.datetime.strptime(date, '%m/%d/%Y')
            action = row[1].text_content()
            actor = senate_get_actor_from_action(action)
            type_class = self.get_action(actor, action)
            bill.add_action(actor, action, date, type=type_class)

    def parse_senate_cosponsors(self, bill, url):
        bill.add_source(url)
        cosponsors_page = self.urlopen(url)
        cosponsors_page = lxml.html.fromstring(cosponsors_page)
        # cosponsors are all in a table
        cosponsors = cosponsors_page.xpath('//table[@id="dgCoSponsors"]/tr/td/a')
        #print "looking for cosponsors = %s" % cosponsors

        for cosponsor_row in cosponsors:
            # cosponsors include district, so parse that out
            cosponsor_string = cosponsor_row.text_content()
            cosponsor = clean_text(cosponsor_string)
            cosponsor = cosponsor.split(',')[0]

            # they give us a link to the congressperson, so we might
            # as well keep it.
            cosponsor_url = cosponsor_row.attrib['href']

            bill.add_sponsor('cosponsor', cosponsor, sponsor_link=cosponsor_url)

    def scrape_house(self, year):
        if int(year) < 2000 or int(year) > dt.date.today().year:
            raise NoDataForPeriod(year)

        bill_page_url = ('%s/BillList.aspx?year=%s' % (self.senate_base_url,year))
        self.parse_house_billpage(bill_page_url, year)

    def parse_house_billpage(self, url, year):
        url_root = re.match("(.*//.*?/)", url).group(1)

        bill_list_page = self.urlopen(url)
        bill_list_page = lxml.html.fromstring(bill_list_page)
        # find the first center tag, take the text after
        # 'House of Representatives' and before 'Bills' as
        # the session name
        header_tag = bill_list_page.xpath('//*[@id="ContentPlaceHolder1_lblAssemblyInfo"]')[0].text_content()
        if header_tag.find('1st Extraordinary Session') != -1:
            session = year + ' 1st Extraordinary Session'
        elif header_tag.find('2nd Extraordinary Session') != -1:
            session = year + ' 2nd Extraordinary Session'
        else:
            session = year

        bills = bill_list_page.xpath('//table[@id="billAssignGroup"]/tr')

        isEven = False
        count = 0
        for bill in bills:
            if not isEven:
                # the non even rows contain bill links, the other rows contain brief
                # descriptions of the bill.
                #print "bill = %s" % bill[0][0].attrib['href']
                count = count + 1
                #if (count > 1140):
                self.parse_house_bill(bill[0][0].attrib['href'], session)
            isEven = not isEven


    def parse_house_bill(self, url, session):
        # using the print page makes the page simpler, and also *drastically* smaller (8k rather than 100k)
        url = re.sub("billsummary", "billsummaryprn", url)
        url = '%s/%s' % (self.senate_base_url,url)

        bill_page = self.urlopen(url)
        bill_page = lxml.html.fromstring(bill_page)

        bill_id = bill_page.xpath('//*[@class="entry-title"]')
        if len(bill_id) == 0:
            self.log("WARNING: bill summary page is blank! (%s)" % url)
            self.bad_urls.append(url)
            return
        bill_id = bill_id[0].text_content()
        bill_id = clean_text(bill_id)

        bill_desc = bill_page.xpath('//*[@class="BillDescription"]')[0].text_content()
        bill_desc = clean_text(bill_desc)

        table_rows = bill_page.xpath('//table/tr')
        # if there is a cosponsor all the rows are pushed down one for the extra row for the cosponsor:
        cosponsorOffset = 0
        if table_rows[2][0].text_content().strip() == 'Co-Sponsor:':
            cosponsorOffset = 1

        lr_label_tag = table_rows[3+cosponsorOffset]
        assert lr_label_tag[0].text_content().strip() == 'LR Number:'
        bill_lr = lr_label_tag[1].text_content()

        lastActionOffset = 0
        if table_rows[4+cosponsorOffset][0].text_content().strip() == 'Governor Action:':
            lastActionOffset = 1
        official_title_tag = table_rows[5+cosponsorOffset+lastActionOffset]
        assert official_title_tag[0].text_content().strip() == 'Bill String:'
        official_title = official_title_tag[1].text_content()

        # could substitute the description for the name,
        # but keeping it separate for now.

        bill_type = "bill"
        triplet = bill_id[:3]
        if triplet in bill_types:
            bill_type = bill_types[triplet]

        subs = []
        bid = bill_id.replace(" ", "")

        if bid in self.subjects:
            subs = self.subjects[bid]
            self.log("With subjects for this bill")

        self.log(bid)

        bill = Bill(session, 'lower', bill_id, bill_desc, bill_url=url,
                    bill_lr=bill_lr, official_title=official_title,
                    type=bill_type, subjects=subs)
        bill.add_source(url)

        bill_sponsor = clean_text(table_rows[0][1].text_content())
        try:
            bill_sponsor_link = table_rows[0][1][0].attrib['href']
        except IndexError:
            return

        if bill_sponsor_link:
            bill_sponsor_link = '%s%s' % (self.senate_base_url,bill_sponsor_link)

        bill.add_sponsor('primary', bill_sponsor, sponsor_link=bill_sponsor_link)

        # check for cosponsors
        if cosponsorOffset == 1:
            if len(table_rows[2][1]) == 1: # just a name
                cosponsor = table_rows[2][1][0]
                bill.add_sponsor('cosponsor', cosponsor.text_content(),
                                 sponsor_link='%s/%s' % (
                                     self.senate_base_url,
                                     cosponsor.attrib['href']
                                ))
            else: # name ... etal
                try:
                    cosponsor = table_rows[2][1][0]
                    bill.add_sponsor('cosponsor',
                                     clean_text(cosponsor.text_content()),
                                     sponsor_link='%s/%s' % (
                                         self.senate_base_url,
                                         cosponsor.attrib['href']
                                     ))
                    self.parse_cosponsors_from_bill(bill,'%s/%s' % (
                        self.senate_base_url,
                        table_rows[2][1][1].attrib['href']))
                except scrapelib.HTTPError as e:
                    self.log("WARNING: " + str(e))
                    self.bad_urls.append(url)
                    self.log( "WARNING: no bill summary page (%s)" % url )

        actions_link_tag = bill_page.xpath('//div[@class="Sections"]/a')[0]
        actions_link = '%s/%s' % (self.senate_base_url,actions_link_tag.attrib['href'])
        actions_link = re.sub("content", "print", actions_link)
        self.parse_house_actions(bill, actions_link)

        # get bill versions
        doc_tags = bill_page.xpath('//div[@class="BillDocsSection"][1]/span')
        for doc_tag in reversed(doc_tags):
            doc = clean_text(doc_tag.text_content())
            text_url = '%s%s' % (
                self.senate_base_url,
                doc_tag[0].attrib['href']
            )
            bill.add_document(doc, text_url,
                              mimetype="text/html")

        # get bill versions
        version_tags = bill_page.xpath('//div[@class="BillDocsSection"][2]/span')
        for version_tag in reversed(version_tags):
            version = clean_text(version_tag.text_content())
            text_url = '%s%s' % (self.senate_base_url,version_tag[0].attrib['href'])
            pdf_url = '%s%s' % (self.senate_base_url,version_tag[1].attrib['href'])
            if text_url.endswith('htm'):
                mimetype = 'text/html'
            elif text_url.endswith('pdf'):
                mimetype = 'application/pdf'
            bill.add_version(version, text_url, pdf_url=pdf_url,
                             on_duplicate='use_new', mimetype=mimetype)
        self.save_bill(bill)

    def parse_cosponsors_from_bill(self, bill, url):
        bill_page = self.urlopen(url)
        bill_page = lxml.html.fromstring(bill_page)
        sponsors_text = find_nodes_with_matching_text(bill_page,'//p/span',r'\s*INTRODUCED.*')
        if len(sponsors_text) == 0:
            # probably its withdrawn
            return
        sponsors_text = sponsors_text[0].text_content()
        sponsors = clean_text(sponsors_text).split(',')
        if len(sponsors) > 1: # if there are several comma separated entries, list them.
            # the sponsor and the cosponsor were already got from the previous page, so ignore those:
            sponsors = sponsors[2::]
            for part in sponsors:
                parts = re.split(r' (?i)and ',part)
                for sponsor in parts:
                    cosponsor_name = clean_text(sponsor)
                    if cosponsor_name != "":
                        cosponsor_name = cosponsor_name.replace(
                            u'\u00a0', " ") # epic hax
                        for name in re.split(r'\s+AND\s+', cosponsor_name):
                        # for name in cosponsor_name.split("AND"):
                            name = name.strip()
                            if name:
                                bill.add_sponsor('cosponsor', name)

    def parse_house_actions(self, bill, url):
        url = re.sub("BillActions", "BillActionsPrn", url)
        bill.add_source(url)
        actions_page = self.urlopen(url)
        actions_page = lxml.html.fromstring(actions_page)
        rows = actions_page.xpath('//table/tr')

        for row in rows[1:]:
            # new actions are represented by having dates in the first td
            # otherwise, it's a continuation of the description from the
            # previous action
            if len(row) > 0 and row[0].tag == 'td':
                if len(row[0].text_content().strip()) > 0:
                    date = row[0].text_content().strip()
                    date = dt.datetime.strptime(date, '%m/%d/%Y')
                    action = row[2].text_content().strip()
                else:
                    action += ('\n' + row[2].text_content())
                    action = action.rstrip()
                actor = house_get_actor_from_action(action)
                type_class = self.get_action(actor, action)

                votes = self.get_votes(date, actor, action)
                for vote in votes:
                    bill.add_vote(vote)

                bill.add_action(actor, action, date, type=type_class)

########NEW FILE########
__FILENAME__ = committees
import datetime as dt
import lxml.html
import xlrd
import os

from billy.scrape.committees import CommitteeScraper, Committee


class MOCommitteeScraper(CommitteeScraper):
    jurisdiction = 'mo'
    reps_url_base = 'http://www.house.mo.gov/'
    senate_url_base = 'http://www.senate.mo.gov/'
    no_members_text = 'This Committee does not have any members'

    def scrape(self, chamber, term_name):
        session = None
        if chamber == 'upper':
            self.scrape_senate_committees(term_name, chamber)
        elif chamber == 'lower':
            self.validate_term(term_name, latest_only=True)
            self.scrape_reps_committees(term_name, chamber)

    def scrape_senate_committees(self, term_name, chamber):
        years = [ t[2:] for t in term_name.split('-') ]

        for year in years:
            if int(year) > int(str(dt.datetime.now().year)[2:]):
                self.log("Not running session %s, it's in the future." % (
                    term_name
                ))
                continue
            url = '{base}{year}info/com-standing.htm'.format(
                                            base=self.senate_url_base, year=year)
            page_string = self.urlopen(url)
            page = lxml.html.fromstring(page_string)
            ps = page.xpath('id("mainContent")/table/*[3]/p')
            for p in ps:
                links = p.xpath('a[1]')
                if not links:
                    continue
                a = links[0]
                committee_name = a.text_content().strip()
                committee_url = a.attrib.get('href')

                if 'joint' in committee_name.lower():
                    c = "joint"
                else:
                    c = chamber

                committee = Committee(c, committee_name)
                committee_page_string = self.urlopen(committee_url)
                committee_page = lxml.html.fromstring(
                                                    committee_page_string)
                lis = committee_page.xpath(
                    "//div[@id='mainContent']/ul/ul[1]/li")
                if len(lis) == 0:
                    lis = committee_page.xpath(
                        "//div[@id='mainContent']//li")
                    # This MIGHT cause issues.
                for li in lis:
                    mem_parts = li.text_content().strip().split(',')
                    mem_name = mem_parts[0]
                    mem_role = 'member'
                    if len(mem_parts) > 2:
                        mem_role = mem_parts[2].lower()

                    if mem_name == "":
                        continue

                    committee.add_member(mem_name, role=mem_role)
                committee.add_source(url)
                committee.add_source(committee_url)
                self.save_committee(committee)


    def scrape_reps_committees(self, term_name, chamber):
        url = '{base}ActiveCommittees.aspx'.format(base=self.reps_url_base)
        page_string = self.urlopen(url)
        page = lxml.html.fromstring(page_string)
        table = page.xpath('//div[@class="lightened"]/table[1]')[0]
        # Last tr has the date
        trs = table.xpath('tr')[:-1]
        for tr in trs:
            committee_parts = [part.strip()
                                for part in tr.text_content().split(',')]
            committee_name = committee_parts[0].title().strip()
            if len(committee_parts) > 0:
                status = committee_parts[1].strip()
            committee_url = tr.xpath('td/a')[0].attrib.get('href')
            committee_url = '{base}{url}'.format(base=self.reps_url_base,
                                                 url=committee_url)
            actual_chamber = chamber
            if 'joint' in committee_name.lower():
                actual_chamber = 'joint'

            committee = Committee(actual_chamber, committee_name, status=status)
            committee_page_string = self.urlopen(committee_url)
            committee_page = lxml.html.fromstring(
                                committee_page_string)
            # First tr has the title (sigh)
            mem_trs = committee_page.xpath('id("memGroup")/tr')[1:]
            for mem_tr in mem_trs:
                mem_code = None
                mem_links = mem_tr.xpath('td/a[1]')
                if len(mem_links):
                    mem_code = mem_links[0].attrib.get('href')
                # Output is "Rubble, Barney, Neighbor"
                mem_parts = mem_tr.text_content().strip().split(',')
                if self.no_members_text in mem_parts:
                    continue
                mem_name = (mem_parts[1].strip() + ' ' +
                            mem_parts[0].strip())
                # Sometimes Senator abbreviation is in the name
                mem_name = mem_name.replace('Sen. ', '')
                mem_role = 'member'
                if len(mem_parts) > 2:
                    # Handle the case where there is a comma in the
                    # role name
                    mem_role = ', '.join(
                        [p.strip() for p in mem_parts[2:]]).lower()
                committee.add_member(mem_name, role=mem_role,
                                    _code=mem_code)
            committee.add_source(url)
            committee.add_source(committee_url)
            self.save_committee(committee)

########NEW FILE########
__FILENAME__ = legislators
import lxml.html
import datetime as dt

from billy.scrape.legislators import LegislatorScraper, Legislator

class MOLegislatorScraper(LegislatorScraper):
    jurisdiction = 'mo'
    assumed_telephone_prefix = '573'
    assumed_address_fmt = ('201 West Capitol Avenue %s, ' 'Jefferson City, MO 65101')
    # senator_url = 'http://www.senate.mo.gov/%sinfo/senalpha.htm'
    # ^^^^^^^^^^^ pre-2013 URL. Keep if we need to scrape old pages.

    senator_url = 'http://www.senate.mo.gov/%sinfo/SenateRoster.htm'
    senator_details_url = 'http://www.senate.mo.gov/%sinfo/members/mem%02d.htm'
    senator_address_url = 'http://www.senate.mo.gov/%sinfo/members/d%02d/OfficeInfo.htm'
    reps_url = 'http://www.house.mo.gov/member.aspx?year=%s'
    rep_details_url = 'http://www.house.mo.gov/member.aspx?year=%s&district=%s'
    vacant_legislators = []

    def scrape(self, chamber, term):
        sessions = term.split('-')
        for session in sessions:
            if int(session) > int(dt.datetime.now().year):
                self.log("Not running session %s, it's in the future." % (
                    session
                ))
                continue

            if chamber == 'upper':
                self.scrape_senators(chamber, session, term)
            elif chamber == 'lower':
                self.scrape_reps(chamber, session, term)

    def scrape_senators(self, chamber, session, term):
        url = self.senator_url % (session[2:])
        root_url = url
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        table = page.xpath('//*[@id="mainContent"]/table//table/tr')
        rowcount = 0
        for tr in table:
            rowcount += 1
            # the first two rows are headers, skip:
            if rowcount < 2:
                continue
            tds = tr.xpath('td')
            full_name = tds[0].xpath('div/a')[0].text_content().strip()

            if full_name == 'Vacant':
                continue

            party_and_district = tds[1].xpath('div')[0].text_content().strip().split('-')
            if party_and_district[0] == 'D':
                party = 'Democratic'
            elif party_and_district[0] == 'R':
                party = 'Republican'

            senator_key = "%s%s" % (party_and_district[0].lower(),party_and_district[1])
            district = party_and_district[1]
            phone = tds[3].xpath('div')[0].text_content().strip()
            url = self.senator_details_url % (session[2:],int(district))
            leg = Legislator(term, chamber, district, full_name,
                             party=party, url=url)
            leg.add_source(root_url)
            details_page = self.urlopen(url)
            leg.add_source(url)
            homepage = url
            page = lxml.html.fromstring(details_page)
            photo_url = page.xpath("//div[@id='container']/div[1]/img")
            photo_url = photo_url[0].attrib['src']

            url = self.senator_address_url % (
                session[2:],int(senator_key[1:]))

            details_page = self.urlopen(url)
            leg.add_source(url)
            page = lxml.html.fromstring(details_page)
            address = page.xpath('/html/body//span[2]')[0].text_content().split('\n')
            emails = page.xpath('/html/body/p/span[2]/a/@href')
            # TODO This is only true if the href doesn't contain 'mail_form'. If it does,
            # then there is only a webform. So...no email?
            # TODO a lot of these have fax numbers. Include?

            for email in emails:
                if 'Contact.aspx' in email:
                    email = None
                if email:
                    break

            kwargs = {
                "address": "%s%s" % (address[0],address[1]),
                "email": email,
            }

            if email:
                leg['email'] = email

            if phone.strip() != "":
                kwargs['phone'] = phone

            leg.add_office("capitol", "Capitol Office",
                           **kwargs)

            leg['photo_url'] = photo_url
            self.save_legislator(leg)

    def scrape_reps(self, chamber, session, term):
        url = (self.reps_url % (session))
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        # This is the ASP.net table container
        table_xpath = ('id("ContentPlaceHolder1_'
                        'gridMembers_DXMainTable")')
        table = page.xpath(table_xpath)[0]
        for tr in table.xpath('tr')[1:]:
            tds = tr.xpath('td')
            leg_code = tds[0].xpath('a[1]')[0].attrib.get('href')
            last_name = tds[0].text_content().strip()
            first_name = tds[1].text_content().strip()
            full_name = '%s %s' % (first_name, last_name)
            district = str(int(tds[2].text_content().strip()))
            party = tds[3].text_content().strip()
            if party == 'Democrat':
                party = 'Democratic'
            phone = tds[4].text_content().strip()
            room = tds[5].text_content().strip()
            address = self.assumed_address_fmt % (room if room else '')

            kwargs = {
                "address": address
            }
            if phone.strip() != "":
                kwargs['phone'] = phone

            if last_name == 'Vacant':
                leg = Legislator(term, chamber, district, full_name=full_name,
                            first_name=first_name, last_name=last_name,
                            party=party, url=url)

                leg.add_office('capitol', "Capitol Office",
                               **kwargs)

                leg.add_source(url)
                self.save_vacant_legislator(leg)
            else:
                leg = Legislator(term, chamber, district, full_name=full_name,
                          first_name=first_name, last_name=last_name,
                          party=party, url=url)

                leg.add_office('capitol', 'Capitol Office',
                               **kwargs)

                url = (self.rep_details_url % (session,district))
                leg.add_source(url)
                details_page = self.urlopen(url)
                page = lxml.html.fromstring(details_page)
                picture = page.xpath('//*[@id="ContentPlaceHolder1_imgPhoto"]/@src')
                email = page.xpath('//*[@id="ContentPlaceHolder1_lblAddresses"]/table/tr[4]/td/a/@href')
                terms = page.xpath('//*[@id="ContentPlaceHolder1_lblElected"]')
                committees = page.xpath('//*[@id="ContentPlaceHolder1_lblCommittees"]/li/a')
                # TODO home address?
                if len(email) > 0 and email[0] != 'mailto:':
                    #print "Found email : %s" % email[0]
                    leg['email'] = email[0].split(':')[1]
                if len(picture) > 0:
                    #print "Found picture : %s" % picture[0]
                    leg['photo_url'] = picture[0]
                #leg.add_source(url)
                self.save_legislator(leg)

    def save_vacant_legislator(self,leg):
        # Here is a stub to save the vacant records - but its not really being used
        # since the current infrastructure pays attention to the legislators and not
        # the seats. See: http://bit.ly/jOtrhd
        self.vacant_legislators.append(leg)

########NEW FILE########
__FILENAME__ = utils
import re


# remove whitespace, linebreaks, and end parentheses
def clean_text(text):
    newtext = re.sub(r"[\r\n]+", " ", text)
    newtext = re.sub(r"\s{2,}", " ", newtext)
    m = re.match(r"(.*)\(.*?\)", newtext)
    if not m:
        return newtext.strip()
    else:
        return m.group(1).strip()


def house_get_actor_from_action(text):
    m = re.search(r"\((\bH\b|\bS\b)\)", text)
    if not m:
        if text.endswith('Governor'):
            return 'Governor'
        else:
            return 'lower'

    abbrev = m.group(1)
    if abbrev == 'S':
        return 'upper'
    return 'lower'


def find_nodes_with_matching_text(page,xpath,regex):
    """
    Using an lxml node, find matching xpath results that have text content
    matching the regex that is passed in here.
    """
    xpath_matches = page.xpath(xpath)
    results = []
    for xp in xpath_matches:
        if re.search(regex,xp.text_content()):
            results.append(xp)
    return results

def senate_get_actor_from_action(text):
    if re.search("Prefiled", text):
        return 'upper'

    m = re.search(r"(\bH\b|\bS\b|House)", text)
    if not m:
        if text.endswith('Governor'):
            return 'Governor'
        else:
            return 'upper'

    if m.group(1) == 'S':
        return 'upper'
    else:
        return 'lower'

########NEW FILE########
__FILENAME__ = votes
from billy.scrape.votes import VoteScraper, Vote
from billy.scrape.utils import convert_pdf

import datetime as dt
import lxml
import re
import os


SENATE_URL = 'http://www.senate.mo.gov/%sinfo/jrnlist/journals.aspx'
HOUSE_URL = 'http://www.house.mo.gov/journallist.aspx'

motion_re = r"(?i)On motion of .*, .*"
bill_re = r"(H|S)(C|J)?(R|M|B) (\d+)"
date_re = r"(MONDAY|TUESDAY|WEDNESDAY|THURSDAY|FRIDAY|SATURDAY|SUNDAY)" \
           ", (\w+) (\d+), (\d+)"


def _clean_line(obj):
    patterns = {
        "\xe2\x80\x94": "-"
    }
    for pattern in patterns:
        obj = obj.replace(pattern, patterns[pattern])

    return obj


class MOVoteScraper(VoteScraper):
    jurisdiction = 'mo'

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def get_pdf(self, url):
        (path, response) = self.urlretrieve(url)
        data = convert_pdf(path, type='text')
        os.remove(path)
        return data

    def scrape_senate(self, session):
        url = SENATE_URL % (session[-2:])
        classes = [
            "YEAS",
            "NAYS",
            "Absent with leave",
            "Absent",
            "Vacancies"
        ]
        klasses = {
            "YEAS": 'yes',
            "NAYS": 'no',
            "Absent with leave": 'other',
            "Absent": 'other',
            "Vacancies": 'other'
        }
        page = self.lxmlize(url)
        journs = page.xpath("//table")[0].xpath(".//a")
        for a in journs:
            pdf_url = a.attrib['href']
            data = self.get_pdf(pdf_url)
            lines = data.split("\n")

            in_vote = False
            cur_date = None
            vote_type = 'other'
            cur_bill = ''
            cur_motion = ''
            bc = None
            vote = {}
            counts = {
                "yes": 0,
                "no": 0,
                "other": 0
            }

            for line in lines:
                line = line.strip()

                if cur_date is None:
                    matches = re.findall(date_re, line)
                    if matches != []:
                        date = matches[0]
                        date = "%s, %s %s, %s" % date
                        date = dt.datetime.strptime(date, "%A, %B %d, %Y")
                        cur_date = date

                matches = re.findall(motion_re, line)
                if matches != []:
                    cont = False
                    for x in matches:
                        if "vote" in x.lower():
                            cur_motion = x
                            bill = re.findall(bill_re, x)
                            if bill != []:
                                bc = {'H': 'lower',
                                      'S': 'upper',
                                      'J': 'joint'}[bill[0][0]]

                                cur_bill = "%s%s%s %s" % bill[0]
                            in_vote = True
                            cont = True
                    if cont:
                        continue
                if in_vote:
                    if (line == line.upper() and line.strip() != "") or \
                       "The President" in line or (
                           "senator" in line.lower() and
                           (
                               "moved" in line.lower() or
                               "requested" in line.lower()
                           )
                       ) or \
                       "assumed the chair" in line.lower():
                        in_vote = False
                        # print vote
                        # print cur_motion
                        yes, no, other = counts['yes'], counts['no'], \
                                            counts['other']
                        if bc is None:
                            continue

                        v = Vote('upper',
                                  date,
                                  cur_motion,
                                  (yes > no),
                                  yes,
                                  no,
                                  other,
                                  session=session,
                                  bill_id=cur_bill,
                                  bill_chamber=bc)
                        v.add_source(url)
                        v.add_source(pdf_url)
                        for key in vote:
                            for person in vote[key]:
                                getattr(v, key)(person)

                        self.save_vote(v)
                        vote = {}
                        counts = {  # XXX: Fix this. Dupe'd.
                            "yes": 0,
                            "no": 0,
                            "other": 0
                        }
                        continue
                    if "Journal of the Senate" in line:
                        continue
                    if re.match(
                        r".*(Monday|Tuesday|Wednesday|Thursday|Friday|" \
                         "Saturday|Sunday), .* \d+, \d+.*",
                        line):
                        continue

                    found = False
                    rl = None
                    for klass in classes:
                        if line.lower().startswith(klass.lower()):
                            if "none" in line.lower():
                                continue

                            if "Senator" in line and not "Senators" in line:
                                line = _clean_line(line)
                                line = line[len(klass):]
                                line = line.replace("-Senator ", "")
                                rl = line
                            vote_type = klasses[klass]
                            found = True
                            if vote_type not in vote:
                                vote[vote_type] = []
                    if found and rl is None:
                        continue
                    elif rl:
                        line = rl

                    # print line
                    names = [_clean_line(x) for x in line.strip().split()]
                    if names == []:
                        continue

                    # print names
                    lname = names[-1]
                    lname = lname.rsplit("-", 1)
                    if len(lname) > 1:
                        person, count = lname
                        if count.lower() == 'none':
                            continue

                        names.pop(-1)
                        names.append(person)
                        counts[vote_type] += int(count)
                        # print counts

                    for name in names:
                        vote[vote_type].append(name)
                # else:
                #     print line

    def scrape_house(self, session):
        pass
#  Ugh, so, the PDFs are in nasty shape. Scraping them is a mess, with
#  crazy spacing to break up the names. Most votes aren't on bills, but rather
#  the agenda of the day.

    def scrape(self, chamber, session):
        if chamber == 'upper':
            self.scrape_senate(session)
        elif chamber == 'lower':
            self.scrape_house(session)

########NEW FILE########
__FILENAME__ = bills
from .utils import chamber_name, parse_ftp_listing
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import VoteScraper, Vote
from billy.scrape.utils import convert_pdf
from datetime import datetime
import lxml.etree
import os
import re

def _combine_lines(lines):
    newlines = []
    lastline = '.'
    for line in lines:
        if lastline and lastline[-1] in '.,:' and not line.startswith('('):
            newlines.append(line)
            lastline = line
        else:
            lastline = newlines[-1] = newlines[-1] + ' ' + line
    return newlines

class MSBillScraper(BillScraper):
    jurisdiction = 'ms'

    _action_types = (
        ('Died in Committee', 'committee:failed'),
        ('Enrolled Bill Signed', 'other'),
        ('Immediate Release', 'other'),
        ('Passed', 'bill:passed'),
        ('Adopted', 'bill:passed'),
        ('Amended', 'amendment:passed'),
        ('Failed', 'bill:failed'),
        ('Committee Substitute Adopted', 'bill:substituted'),
        ('Amendment Failed', 'amendment:failed'),
        ('Amendment Withdrawn', 'amendment:withdrawn'),
        ('Referred To', 'committee:referred'),
        ('Rereferred To', 'committee:referred'),
        ('Transmitted To', 'bill:introduced'),
        ('Approved by Governor', 'governor:signed'),
        ('Vetoed', 'governor:vetoed'),
        ('Partially Vetoed', 'governor:vetoed:line-item'),
        ('Title Suff Do', 'committee:passed'),
        ('Read the Third Time', 'bill:reading:3'),
    )

    def scrape(self, chamber, session):
        self.save_errors=False
        if int(session[0:4]) < 2008:
            raise NoDataForPeriod(session)
        self.scrape_bills(chamber, session)

    def scrape_bills(self, chamber_to_scrape, session):
        url = 'http://billstatus.ls.state.ms.us/%s/pdf/all_measures/allmsrs.xml' % session

        bill_dir_page = self.urlopen(url)
        root = lxml.etree.fromstring(bill_dir_page.bytes)
        for mr in root.xpath('//LASTACTION/MSRGROUP'):
            bill_id = mr.xpath('string(MEASURE)').replace(" ", "")
            if bill_id[0] == "S":
                chamber = "upper"
            else:
                chamber = "lower"

            bill_type = {'B':'bill', 'C': 'concurrent resolution',
                         'R': 'resolution', 'N': 'nomination'}[bill_id[1]]

            # just skip past bills that are of the wrong chamber
            if chamber != chamber_to_scrape:
                continue

            link = mr.xpath('string(ACTIONLINK)').replace("..", "")
            main_doc = mr.xpath('string(MEASURELINK)').replace("../../../", "")
            main_doc_url = 'http://billstatus.ls.state.ms.us/%s' % main_doc
            bill_details_url = 'http://billstatus.ls.state.ms.us/%s/pdf/%s' % (session, link)
            details_page = self.urlopen(bill_details_url)

            page = details_page.bytes.replace(chr(11), "")
            # Some pages have the (invalid) byte 11 sitting around. Just drop
            # them out. Might as well.

            details_root = lxml.etree.fromstring(page)
            title = details_root.xpath('string(//SHORTTITLE)')
            longtitle = details_root.xpath('string(//LONGTITLE)')

            bill = Bill(session, chamber, bill_id, title,
                        type=bill_type, summary=longtitle)

            #sponsors
            main_sponsor = details_root.xpath('string(//P_NAME)').split()
            if main_sponsor:
                main_sponsor = main_sponsor[0]
                main_sponsor_link = details_root.xpath('string(//P_LINK)').replace(" ", "_")
                main_sponsor_url =  'http://billstatus.ls.state.ms.us/%s/pdf/House_authors/%s.xml' % (session, main_sponsor_link)
                type = "primary"
                bill.add_sponsor(type, main_sponsor, main_sponsor_url = main_sponsor_url)
            for author in details_root.xpath('//AUTHORS/ADDITIONAL'):
                leg = author.xpath('string(CO_NAME)').replace(" ", "_")
                if leg:
                    leg_url = 'http://billstatus.ls.state.ms.us/%s/pdf/House_authors/%s.xml' % (session, leg)
                    type = "cosponsor"
                    bill.add_sponsor(type, leg, leg_url=leg_url)

            #Versions 
            curr_version = details_root.xpath('string(//CURRENT_OTHER)').replace("../../../../", "")
            if curr_version != "":
                curr_version_url = "http://billstatus.ls.state.ms.us/" \
                        + curr_version
                bill.add_version("Current version", curr_version_url,
                                 on_duplicate='use_new',
                                 mimetype='text/html')

            intro_version = details_root.xpath('string(//INTRO_OTHER)').replace("../../../../", "")
            if intro_version != "":
                intro_version_url = "http://billstatus.ls.state.ms.us/"\
                        + intro_version
                bill.add_version("As Introduced", intro_version_url,
                                 on_duplicate='use_new',
                                 mimetype='text/html')

            comm_version = details_root.xpath('string(//CMTESUB_OTHER)').replace("../../../../", "")
            if comm_version.find("documents") != -1:
                comm_version_url = "http://billstatus.ls.state.ms.us/" + comm_version
                bill.add_version("Committee Substitute", comm_version_url,
                                 on_duplicate='use_new',
                                 mimetype='text/html')
            passed_version = details_root.xpath('string(//PASSED_OTHER)').replace("../../../../", "")
            if passed_version.find("documents") != -1:
                passed_version_url = "http://billstatus.ls.state.ms.us/" + passed_version
                title = "As Passed the " + chamber
                bill.add_version(title, passed_version_url,
                                 on_duplicate='use_new',
                                 mimetype='text/html')

            asg_version = details_root.xpath('string(//ASG_OTHER)').replace("../../../../", "")
            if asg_version.find("documents") != -1:
                asg_version_url = "http://billstatus.ls.state.ms.us/" + asg_version
                bill.add_version("Approved by the Governor", asg_version_url,
                                 on_duplicate='use_new',
                                 mimetype='text/html')


            # avoid duplicate votes
            seen_votes = set()

            #Actions
            for action in details_root.xpath('//HISTORY/ACTION'):
                action_num  = action.xpath('string(ACT_NUMBER)').strip()
                action_num = int(action_num)
                act_vote = action.xpath('string(ACT_VOTE)').replace("../../../..", "")
                action_desc = action.xpath('string(ACT_DESC)')
                date, action_desc = action_desc.split(" ", 1)
                date = date + "/" + session[0:4]
                date = datetime.strptime(date, "%m/%d/%Y")

                if action_desc.startswith("(H)"):
                    actor = "lower"
                    action = action_desc[4:]
                elif action_desc.startswith("(S)"):
                    actor = "upper"
                    action = action_desc[4:]
                else:
                    actor = "executive"
                    action = action_desc

                if action.find("Veto") != -1:
                    version_path = details_root.xpath("string(//VETO_OTHER)")
                    version_path = version_path.replace("../../../../", "")
                    version_url = "http://billstatus.ls.state.ms.us/" + version_path
                    bill.add_document("Veto", version_url) 

                atype = 'other'
                for prefix, prefix_type in self._action_types:
                    if action.startswith(prefix):
                        atype = prefix_type
                        break

                bill.add_action(actor, action, date, type=atype,
                                action_num=action_num)

                # use committee names as scraped subjects
                subjects = details_root.xpath('//H_NAME/text()')
                subjects += details_root.xpath('//S_NAME/text()')
                bill['subjects'] = subjects

                if act_vote:
                    vote_url = 'http://billstatus.ls.state.ms.us%s' % act_vote
                    if vote_url not in seen_votes:
                        seen_votes.add(vote_url)
                        vote = self.scrape_votes(vote_url, action,
                                                 date, actor)
                        vote.add_source(vote_url)
                        bill.add_vote(vote)

            bill.add_source(bill_details_url)
            self.save_bill(bill)

    _vote_mapping = {
        'Passed': ('Passage', True),
        'Adopted': ('Passage', True),
        'Failed': ('Passage', False),
        'Passed As Amended': ('Passage as Amended', True),
        'Adopted As Amended': ('Passage as Amended', True),
        'Appointment Confirmed': ('Appointment Confirmation', True),
        'Committee Substitute Adopted': ('Adopt Committee Substitute', True),
        'Committee Substitute Failed': ('Adopt Committee Substitute', False),
        'Conference Report Adopted': ('Adopt Conference Report', True),
        'Conference Report Failed': ('Adopt Conference Report', False),
        'Motion to Reconsider Tabled': ('Table Motion to Reconsider', True),
        'Motion to Recnsdr Tabled Lost': ('Table Motion to Reconsider', False),
        'Veto Overridden': ('Override Veto', True),
        'Veto Sustained': ('Override Veto', False),
        'Concurred in Amend From House': ('Concurrence in Amendment From House', True),
        'Concurred in Amend From Senate': ('Concurrence in Amendment From Senate', True),
        'Decline to Concur/Invite Conf': ('Decline to Concur', True),
        'Decline Concur/Inv Conf Lost': ('Decline to Concur', False),
        'Failed to Suspend Rules': ('Motion to Suspend Rules', False),
        'Motion to Recommit Lost': ('Motion to Recommit', True),
        'Reconsidered': ('Reconsideration', True),
        'Motion to Concur Failed': ('Motion to Concur', False),
        'Recommitted to Committee': ('Recommit to Committee', True),
    }

    def scrape_votes(self, url, motion, date, chamber):
        vote_pdf, resp = self.urlretrieve(url)
        text = convert_pdf(vote_pdf, 'text')
        os.remove(vote_pdf)

        # this way we get a key error on a missing vote type
        motion, passed = self._vote_mapping[motion]

        yes_votes = []
        no_votes = []
        other_votes = []

        # point at array to add names to
        cur_array = None

        precursors = (
            ('Yeas--', yes_votes),
            ('Nays--', no_votes),
            ('Absent or those not voting--', other_votes),
            ('Absent and those not voting--', other_votes),
            ('Voting Present--', other_votes),
            ('Present--', other_votes),
            ('DISCLAIMER', None),
        )

        # split lines on newline, recombine lines that don't end in punctuation
        lines = _combine_lines(text.split('\n'))

        for line in lines:

            # check if the line starts with a precursor, switch to that array
            for pc, arr in precursors:
                if pc in line:
                    cur_array = arr
                    line = line.replace(pc, '')

            # split names
            for name in line.split(','):
                name = name.strip()

                # move on if that's all there was
                if not name:
                    continue

                # None or a Total indicate the end of a section
                if 'None.' in name:
                    cur_array = None
                match = re.match(r'(.+?)\. Total--.*', name)
                if match:
                    cur_array.append(match.groups()[0])
                    cur_array = None

                # append name if it looks ok
                junk_in_name = False
                for junk in ('on final passage', 'Necessary', 'who would have',
                             'being a tie', 'therefore', 'Vacancies', 'a pair',
                             'Total-', 'ATTORNEY', 'on final passage',
                             'SPEAKER', 'BOARD', 'TREASURER', 'GOVERNOR',
                             'ARCHIVES', 'SECRETARY'):
                    if junk in name:
                        junk_in_name = True
                        break
                if cur_array is not None and not junk_in_name:
                    # strip trailing .
                    if name[-1] == '.':
                        name = name[:-1]
                    cur_array.append(name)

        # return vote object
        yes_count = len(yes_votes)
        no_count = len(no_votes)
        other_count = len(other_votes)
        vote = Vote(chamber, date, motion, passed, yes_count, no_count,
                    other_count)
        vote['yes_votes'] = yes_votes
        vote['no_votes'] = no_votes
        vote['other_votes'] = other_votes
        return vote

########NEW FILE########
__FILENAME__ = committees
from billy.scrape import NoDataForPeriod
from billy.scrape.committees import CommitteeScraper, Committee
from .utils import clean_committee_name

import lxml.etree


class MSCommitteeScraper(CommitteeScraper):
    jurisdiction = 'ms'

    def scrape(self, chamber, term_name):
        self.save_errors=False
        if int(term_name[0:4]) < 2008:
            raise NoDataForPeriod(term_name)

        if chamber == 'lower':
            chamber = 'h'
        else:
            chamber = 's'

        self.scrape_comm(chamber, term_name)

    def scrape_comm(self, chamber, term_name):
        url = 'http://billstatus.ls.state.ms.us/htms/%s_cmtememb.xml' % chamber
        comm_page =  self.urlopen(url)
        root = lxml.etree.fromstring(comm_page.bytes)
        if chamber == 'h':
            chamber = "lower"
        else:
            chamber = "upper"
        for mr in root.xpath('//COMMITTEE'):
            name = mr.xpath('string(NAME)')
            comm = Committee(chamber, name)

            chair = mr.xpath('string(CHAIR)')
            chair = chair.replace(", Chairman", "")
            role = "Chairman"
            if len(chair) > 0:
                comm.add_member(chair, role=role)
            vice_chair = mr.xpath('string(VICE_CHAIR)')
            vice_chair = vice_chair.replace(", Vice-Chairman", "")
            role = "Vice-Chairman"
            if len(vice_chair) > 0:
                comm.add_member(vice_chair, role=role)
            members = mr.xpath('string(MEMBERS)').split(";")
            if "" in members:
                members.remove("")

            for leg in members:
                leg = leg.strip()
                comm.add_member(leg)

            comm.add_source(url)
            self.save_committee(comm)

########NEW FILE########
__FILENAME__ = legislators
import urlparse
import lxml.etree

from billy.scrape import NoDataForPeriod
from billy.scrape.legislators import LegislatorScraper, Legislator
from .utils import clean_committee_name

import scrapelib
import os.path


CAP_ADDRESS = """P. O. Box 1018
Jackson, MS 39215"""

class MSLegislatorScraper(LegislatorScraper):
    jurisdiction = 'ms'

    def scrape(self, chamber, term_name):
        self.validate_term(term_name, latest_only=True)
        self.scrape_legs(chamber, term_name)

    def scrape_legs(self, chamber, term_name):
        if chamber == 'upper':
            url = 'http://billstatus.ls.state.ms.us/members/ss_membs.xml'
            range_num = 5
        else:
            url = 'http://billstatus.ls.state.ms.us/members/hr_membs.xml'
            range_num = 6

        leg_dir_page = self.urlopen(url)
        root = lxml.etree.fromstring(leg_dir_page.bytes)
        for mr in root.xpath('//LEGISLATURE/MEMBER'):
            for num in range(1, range_num):
                leg_path = "string(M%s_NAME)" % num
                leg_link_path = "string(M%s_LINK)" % num
                leg = mr.xpath(leg_path)
                leg_link = mr.xpath(leg_link_path)
                role = "member"
                self.scrape_details(chamber, term_name, leg, leg_link, role)
        if chamber == 'lower':
            chair_name = root.xpath('string(//CHAIR_NAME)')
            chair_link = root.xpath('string(//CHAIR_LINK)')
            role = root.xpath('string(//CHAIR_TITLE)')
            self.scrape_details(chamber, term_name, chair_name, chair_link, role)
        else:
            #Senate Chair is the Governor. Info has to be hard coded
            chair_name = root.xpath('string(//CHAIR_NAME)')
            role = root.xpath('string(//CHAIR_TITLE)')
            # TODO: if we're going to hardcode the governor, do it better
            #district = "Governor"
            #leg = Legislator(term_name, chamber, district, chair_name,
            #                 first_name="", last_name="", middle_name="",
            #                 party="Republican", role=role)

        protemp_name = root.xpath('string(//PROTEMP_NAME)')
        protemp_link = root.xpath('string(//PROTEMP_LINK)')
        role = root.xpath('string(//PROTEMP_TITLE)')
        self.scrape_details(chamber, term_name, protemp_name, protemp_link, role)

    def scrape_details(self, chamber, term, leg_name, leg_link, role):
        if not leg_link:
            # Vacant post, likely:
            if "Vacancy" in leg_name:
                return
            raise Exception("leg_link is null. something went wrong")
        try:
            url = 'http://billstatus.ls.state.ms.us/members/%s' % leg_link
            url_root = os.path.dirname(url)
            details_page = self.urlopen(url)
            root = lxml.etree.fromstring(details_page.bytes)
            party = root.xpath('string(//PARTY)')
            district = root.xpath('string(//DISTRICT)')
            photo = "%s/%s" % (url_root, root.xpath('string(//IMG_NAME)'))

            home_phone = root.xpath('string(//H_PHONE)')
            bis_phone = root.xpath('string(//B_PHONE)')
            capital_phone = root.xpath('string(//CAP_PHONE)')
            other_phone = root.xpath('string(//OTH_PHONE)')
            org_info = root.xpath('string(//ORG_INFO)')
            email_name = root.xpath('string(//EMAIL_ADDRESS)')
            cap_room = root.xpath('string(//CAP_ROOM)')

            if party == 'D':
                party = 'Democratic'
            elif party == 'R':
                party = 'Republican'
            elif leg_name in ('Oscar Denton', 'Lataisha Jackson', 'John G. Faulkner'):
                party = 'Democratic'

            leg = Legislator(term, chamber, district, leg_name, party=party, role=role,
                             org_info=org_info, url=url, photo_url=photo)
            leg.add_source(url)

            kwargs = {}

            if email_name.strip() != "":
                email = '%s@%s.ms.gov' % (email_name,
                                          {"upper": "senate", "lower": "house"}[chamber])
                kwargs['email'] = email

            if capital_phone != "":
                kwargs['phone'] = capital_phone

            if cap_room != "":
                kwargs["address"] = "Room %s\n%s" % (cap_room, CAP_ADDRESS)
            else:
                kwargs['address'] = CAP_ADDRESS

            leg.add_office('capitol', 'Capitol Office', **kwargs)

            self.save_legislator(leg)
        except scrapelib.HTTPError, e:
            self.warning(str(e))


########NEW FILE########
__FILENAME__ = utils
import re


def clean_committee_name(comm_name):
    comm_name = comm_name.strip()
    comm_name = re.sub(' ?[-,] (Co|Vice)?[- ]?Chair$', '', comm_name)
    comm_name = re.sub('Appropriations - S/C:', 'Appropriations-S/C on',
                       comm_name)
    if comm_name == 'Appropriations-S/C Stimulus':
        comm_name = 'Appropriations-S/C on Stimulus'

    return comm_name


def parse_ftp_listing(text):
    lines = text.strip().split('\r\n')
    return (' '.join(line.split()[3:]) for line in lines)


def chamber_name(chamber):
    if chamber == 'upper':
        return 'senate'
    else:
        return 'house'




########NEW FILE########
__FILENAME__ = actions
import re


# ----------------------------------------------------------------------------
# Data for action categorization.

_categories = {

    # Bill is introduced or prefiled
    "bill:introduced": {
        'rgxs': ['^Introduced$'],
        'funcs': {},
        },

    # Bill has passed a chamber
    "bill:passed": {
        'rgxs': [u'3rd Reading Passed',
                 u'^Resolution Adopted',
                 u'3rd Reading Concurred',
                 u'3rd Reading Passed as Amended by Senate',
                 u'3rd Reading Passed as Amended by House']
        },

    # Bill has failed to pass a chamber
    "bill:failed": {
        'rgxs': [
            u'3rd Reading Failed',
            u'Died in Process'
            ],
        'funcs': {},
        },

    # Bill has been withdrawn from consideration
    "bill:withdrawn": {
        'rgxs': [],
        'funcs': {},
        },

    # ???
    # The chamber attempted a veto override and succeeded
    "bill:veto_override:passed": {
        'rgxs': [
            u'Veto Overridden in House',
            ]
        },

    # ???
    # The chamber attempted a veto override and failed
    "bill:veto_override:failed": {
        'rgxs': [
            u'Veto Override Motion Failed',
            u'Veto Override Failed',
            ],
        },

    # ???
    # A bill has undergone its first reading
    "bill:reading:1": {
        'rgxs': ['First Reading'],
        'funcs': {},
        },

    # A bill has undergone its second reading
    "bill:reading:2": {
        'rgxs': [
            u'Taken from Committee; Placed on 2nd Reading',
            u'2nd Reading Passed',
            u'2nd Reading Conference Committee Report Adopted',
            u'2nd Reading Senate Amendments Concurred',
            u'2nd Reading Pass Motion Failed; 3rd Reading Vote Required',
            u'2nd Reading Not Passed as Amended',
            u'2nd Reading House Amendments Concurred',
            u'2nd Reading Concurred',
            u'Reconsidered Previous Action; Placed on 2nd Reading',
            u'2nd Reading Indefinitely Postponed',
            u'Taken from 3rd Reading; Placed on 2nd Reading',
            u'2nd Reading Concur Motion Failed',
            u'2nd Reading Not Concurred; 3rd Reading Vote Required',
            u'Reconsidered Previous Act; Remains in 2nd Reading FCC Process',
            u'2nd Reading Indefinitely Postpone Motion Failed',
            u'2nd Reading Pass Motion Failed',
            u'2nd Reading Not Concurred',
            u'2nd Reading Not Passed',
            ],
        },

    # A bill has undergone its third (or final) reading
    "bill:reading:3": {
        'rgxs': [
            u'3rd Reading Passed as Amended by Senate',
            u'3rd Reading Passed as Amended by House',
            u'3rd Reading Pass Consideration',
            u'3rd Reading Concurred',
            u'3rd Reading Passed',
            u'3rd Reading Not Passed as Amended by Senate',
            u'Reconsidered Previous Action; Remains in 3rd Reading Process',
            u'3rd Reading Conference Committee Report Adopted',
            u'3rd Reading Failed',
            ],
        },

    # A bill has been filed (for states where this is a separate event from
    # bill:introduced)
    "bill:filed": {
        'rgxs': [],
        'funcs': {},
        },

    # A bill has been replaced with a substituted wholesale (called hoghousing
    # in some states)
    "bill:substituted": {
        'rgxs': [],
        'funcs': {},
        },

    # The bill has been transmitted to the governor for consideration
    "governor:received": {
        'rgxs': ['Transmitted to Governor'],
        'funcs': {},
        },

    # The bill has signed into law by the governor
    "governor:signed": {
        'rgxs': ['Signed by Governor'],
        'funcs': {},
        },

    # The bill has been vetoed by the governor
    "governor:vetoed": {
        'rgxs': ['Vetoed by Governor'],
        'funcs': {},
        },

    # The governor has issued a line-item (partial) veto
    "governor:vetoed:line-item": {
        'rgxs': [
            u"Returned with Governor's Line-item Veto",
            ],
        },

    # An amendment has been offered on the bill
    "amendment:introduced": {
        'rgxs': ['^(?i)amendment.{,200}introduced'],
        },

    # The bill has been amended
    "amendment:passed": {
        'rgxs': [
            u"3rd Reading Governor's Proposed Amendments Adopted",
            u"2nd Reading Governor's Proposed Amendments Adopted",
            u'2nd Reading House Amendments Concurred',
            u'2nd Reading Senate Amendments Concurred',
            ],
        },

    # An offered amendment has failed
    "amendment:failed": {
        'rgxs': [
            u'2nd Reading House Amendments Not Concur Motion Failed',
            u'2nd Reading Senate Amendments Concur Motion Failed',
            u'2nd Reading House Amendments Concur Motion Failed',
            u"2nd Reading Governor's Proposed Amendments Not Adopted",
            u"3rd Reading Governor's Proposed Amendments Not Adopted",
            u"2nd Reading Governor's Proposed Amendments Adopt Motion Failed",
            u'2nd Reading Motion to Amend Failed',
            u'2nd Reading House Amendments Not Concurred',
            ],
        },

    # An offered amendment has been amended (seen in Texas)
    "amendment:amended": {
        'rgxs': [
            u"3rd Reading Governor's Proposed Amendments Adopted",
            u"2nd Reading Governor's Proposed Amendments Adopted",
            u'2nd Reading House Amendments Concurred',
            u'2nd Reading Senate Amendments Concurred',
            ],
        },

    # ???
    # An offered amendment has been withdrawn
    "amendment:withdrawn": {
        'rgxs': [],
        },

    # An amendment has been 'laid on the table' (generally
    # preventing further consideration)
    "amendment:tabled": {
        'rgxs': ['Tabled in Committee'],
        },

    # The bill has been referred to a committee
    "committee:referred": {
        'rgxs': ["Referred to Committee",
                 "Rereferred to Committee"],
        },

    # The bill has been passed out of a committee
    "committee:passed": {
        'rgxs': [r'Committee Executive Action--Bill Passed',
                 r'Committee Report--Bill Passed',
                 r'Committee Executive Action--Resolution Adopted',]
        },

    # ??? Looks like this'd require parsing
    # The bill has been passed out of a committee with a favorable report
    "committee:passed:favorable": {
        'rgxs': [],
        },

    # ??? Looks like this'd require parsing
    # The bill has been passed out of a committee with an unfavorable report
    "committee:passed:unfavorable": {
        'rgxs': [],
        },

    # The bill has failed to make it out of committee
    "committee:failed": {
        'rgxs': [r'Committee Executive Action--Resolution Not Adopted',
                 r'Committee Executive Action--Bill Not Passed',
                 r'Died in Standing Committee'],
        },

    # All other actions will have a type of "other"
    }

_funcs = []
append = _funcs.append
for category, data in _categories.items():

    for rgx in data['rgxs']:
        append((category, re.compile(rgx).search))

ac = set([u'2nd Reading Concur Motion Failed',
     u'2nd Reading Concur as Amended Motion Failed',
     u'2nd Reading Concurred',
     u'2nd Reading Concurred as Amended',
     u'2nd Reading Conference Committee Report Adopt Motion Failed',
     u'2nd Reading Conference Committee Report Adopted',
     u'2nd Reading Free Conference Committee Report Adopt Motion Failed',
     u'2nd Reading Free Conference Committee Report Adopted',
     u'2nd Reading Free Conference Committee Report Rejected',
     u"2nd Reading Governor's Proposed Amendments Adopt Motion Failed",
     u"2nd Reading Governor's Proposed Amendments Adopted",
     u"2nd Reading Governor's Proposed Amendments Not Adopted",
     u'2nd Reading House Amendments Concur Motion Failed',
     u'2nd Reading House Amendments Concurred',
     u'2nd Reading House Amendments Not Concur Motion Failed',
     u'2nd Reading House Amendments Not Concurred',
     u'2nd Reading Indefinitely Postpone Motion Failed',
     u'2nd Reading Indefinitely Postponed',
     u'2nd Reading Motion to Amend Carried',
     u'2nd Reading Motion to Amend Failed',
     u'2nd Reading Not Concurred',
     u'2nd Reading Not Concurred; 3rd Reading Vote Required',
     u'2nd Reading Not Passed',
     u'2nd Reading Not Passed as Amended',
     u'2nd Reading Pass Consideration',
     u'2nd Reading Pass Motion Failed',
     u'2nd Reading Pass Motion Failed; 3rd Reading Vote Required',
     u'2nd Reading Pass as Amended Motion Failed',
     u'2nd Reading Passed',
     u'2nd Reading Passed Consideration',
     u'2nd Reading Passed as Amended',
     u'2nd Reading Senate Amendments Concur Motion Failed',
     u'2nd Reading Senate Amendments Concurred',
     u'2nd Reading Senate Amendments Not Concurred',
     u'3rd Reading Concurred',
     u'3rd Reading Conference Committee Report Adopted',
     u'3rd Reading Failed',
     u'3rd Reading Free Conference Committee Report Adopted',
     u"3rd Reading Governor's Proposed Amendments Adopted",
     u"3rd Reading Governor's Proposed Amendments Not Adopted",
     u'3rd Reading Not Passed as Amended by Senate',
     u'3rd Reading Pass Consideration',
     u'3rd Reading Passed',
     u'3rd Reading Passed as Amended by House',
     u'3rd Reading Passed as Amended by Senate',
     u'Adverse Committee Report Rejected',
     u'Bill Draft Text Available Electronically',
     u"Bill Not Heard at Sponsor's Request",
     u'Chapter Number Assigned',
     u'Clerical Corrections Made - New Version Available',
     u'Committee Executive Action--Bill Concurred',
     u'Committee Executive Action--Bill Concurred as Amended',
     u'Committee Executive Action--Bill Not Passed as Amended',
     u'Committee Executive Action--Bill Passed',
     u'Committee Executive Action--Bill Passed as Amended',
     u'Committee Executive Action--Resolution Adopted',
     u'Committee Executive Action--Resolution Adopted as Amended',
     u'Committee Executive Action--Resolution Not Adopted',
     u'Committee Report--Bill Concurred',
     u'Committee Report--Bill Concurred as Amended',
     u'Committee Report--Bill Passed',
     u'Committee Report--Bill Passed as Amended',
     u'Committee Report--Resolution Adopted',
     u'Committee Report--Resolution Adopted as Amended',
     u'Committee Report--Resolution Not Adopted',
     u'Committee Vote Failed; Remains in Committee',
     u'Conference Committee Appointed',
     u'Conference Committee Dissolved',
     u'Conference Committee Report Received',
     u'Died in Process',
     u'Died in Standing Committee',
     u'Draft Back for Redo',
     u'Draft Back for Requester Changes',
     u'Draft Back for Technical Correction',
     u'Draft Canceled',
     u'Draft Delivered to Requester',
     u'Draft On Hold',
     u'Draft Ready for Delivery',
     u'Draft Request Received',
     u'Draft Taken Off Hold',
     u'Draft Taken by Drafter',
     u'Draft in Assembly/Executive Director Review',
     u'Draft in Edit',
     u'Draft in Final Drafter Review',
     u'Draft in Input/Proofing',
     u'Draft in Legal Review',
     u'Draft to Drafter - Edit Review [CMD]',
     u'Draft to Drafter - Edit Review [JLN]',
     u'Draft to Drafter - Edit Review [SAB]',
     u'Draft to Requester for Review',
     u'Filed with Secretary of State',
     u'First Reading',
     u'Fiscal Note Printed',
     u'Fiscal Note Probable',
     u'Fiscal Note Received',
     u'Fiscal Note Requested',
     u'Fiscal Note Requested (Local Government Fiscal Impact)',
     u'Fiscal Note Signed',
     u'Free Conference Committee Appointed',
     u'Free Conference Committee Dissolved',
     u'Free Conference Committee Report Received',
     u'Hearing',
     u'Hearing Canceled',
     u'Introduced',
     u'Introduced Bill Text Available Electronically',
     u'Line-item Veto Override Failed',
     u'Missed Deadline for Appropriation Bill Transmittal',
     u'Missed Deadline for General Bill Transmittal',
     u'Missed Deadline for Interim Study Resolution Transmittal',
     u'Missed Deadline for Referendum Proposal Transmittal',
     u'Missed Deadline for Revenue Bill Transmittal',
     u'Motion Carried',
     u'Motion Failed',
     u'On Motion Rules Suspended',
     u'Placed on Consent Calendar',
     u'Pre-Introduction Letter Sent',
     u'Printed - Enrolled Version Available',
     u'Printed - New Version Available',
     u'Reconsidered Previous Act; Remains in 2nd Read Process to Consider (H) Amend',
     u'Reconsidered Previous Act; Remains in 2nd Read Process to Consider (S) Amend',
     u'Reconsidered Previous Act; Remains in 2nd Reading FCC Process',
     u'Reconsidered Previous Act; Remains in 3rd Read Gov Amend Process',
     u'Reconsidered Previous Action; Placed on 2nd Reading',
     u'Reconsidered Previous Action; Remains in 2nd Reading Process',
     u'Reconsidered Previous Action; Remains in 3rd Reading Process',
     u'Referred to Committee',
     u'Rereferred to Committee',
     u'Resolution Adopted',
     u'Resolution Failed',
     u'Returned from Enrolling',
     u'Returned to House',
     u"Returned to House Concurred in Governor's Proposed Amendments",
     u"Returned to House Not Concurred in Governor's Proposed Amendments",
     u'Returned to House with Amendments',
     u'Returned to Senate',
     u"Returned to Senate Concurred in Governor's Proposed Amendments",
     u"Returned to Senate Not Concurred in Governor's Proposed Amendments",
     u'Returned to Senate with Amendments',
     u"Returned with Governor's Line-item Veto",
     u"Returned with Governor's Proposed Amendments",
     u'Revised Fiscal Note Printed',
     u'Revised Fiscal Note Received',
     u'Revised Fiscal Note Requested',
     u'Revised Fiscal Note Signed',
     u'Rules Suspended to Accept Late Return of Amended Bill',
     u'Rules Suspended to Accept Late Transmittal of Bill',
     u'Scheduled for 2nd Reading',
     u'Scheduled for 3rd Reading',
     u'Scheduled for Consideration under Special Orders',
     u'Scheduled for Executive Action',
     u'Segregated from Committee of the Whole Report',
     u'Sent to Enrolling',
     u'Signed by Governor',
     u'Signed by President',
     u'Signed by Speaker',
     u'Special Note',
     u'Sponsor List Modified',
     u'Sponsor Rebuttal to Fiscal Note Printed',
     u'Sponsor Rebuttal to Fiscal Note Received',
     u'Sponsor Rebuttal to Fiscal Note Requested',
     u'Sponsor Rebuttal to Fiscal Note Signed',
     u'Sponsors Engrossed',
     u'Tabled in Committee',
     u'Taken from 2nd Reading; Rereferred to Committee',
     u'Taken from 3rd Reading; Placed on 2nd Reading',
     u'Taken from Committee; Placed on 2nd Reading',
     u'Taken from Table in Committee',
     u'Transmitted to Governor',
     u'Transmitted to House',
     u"Transmitted to House for Consideration of Governor's Proposed Amendments",
     u'Transmitted to Senate',
     u"Transmitted to Senate for Consideration of Governor's Proposed Amendments",
     u'Veto Overridden in House',
     u'Veto Override Failed in Legislature',
     u'Veto Override Motion Failed in House',
     u'Veto Override Motion Failed in Senate',
     u'Veto Override Vote Mail Poll Letter Being Prepared',
     u'Veto Override Vote Mail Poll in Progress',
     u'Vetoed by Governor'])

def categorize(action, funcs=_funcs):
    '''
    '''
    action = action.strip('" ')
    res = set()
    for category, f in funcs:
        if f(action):
            res.add(category)

    if not res:
        return ('other',)

    return tuple(res)

########NEW FILE########
__FILENAME__ = bills
import os
import re
import itertools
import copy
import tempfile
from datetime import datetime
from urlparse import urljoin
from collections import defaultdict

from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote
from billy.scrape.utils import convert_pdf
from scrapelib import urlopen, HTTPError

import lxml.html
from lxml.etree import ElementTree, XMLSyntaxError

from . import actions


def url2lxml(url):
    html = urlopen(url)
    return lxml.html.fromstring(html)


actor_map = {
    '(S)': 'upper',
    '(H)': 'lower',
    '(C)': 'clerk',
    }

sponsor_map = {
    'Primary Sponsor': 'primary'
    }

vote_passage_indicators = ['Adopted',
                           'Appointed',
                           'Carried',
                           'Concurred',
                           'Dissolved',
                           'Passed',
                           'Rereferred to Committee',
                           'Transmitted to',
                           'Veto Overidden',
                           'Veto Overridden']
vote_failure_indicators = ['Failed',
                           'Rejected',
                           ]
vote_ambiguous_indicators = [
    'Indefinitely Postponed',
    'On Motion Rules Suspended',
    'Pass Consideration',
    'Reconsidered Previous',
    'Rules Suspended',
    'Segregated from Committee',
    'Special Action',
    'Sponsor List Modified',
    'Tabled',
    'Taken from']


class MTBillScraper(BillScraper):
    #must set state attribute as the state's abbreviated name
    jurisdiction = 'mt'

    def __init__(self, *args, **kwargs):
        super(MTBillScraper, self).__init__(*args, **kwargs)

        self.search_url_template = (
            'http://laws.leg.mt.gov/laws%s/LAW0203W$BSRV.ActionQuery?'
            'P_BLTP_BILL_TYP_CD=%s&P_BILL_NO=%s&P_BILL_DFT_NO=&'
            'Z_ACTION=Find&P_SBJ_DESCR=&P_SBJT_SBJ_CD=&P_LST_NM1=&'
            'P_ENTY_ID_SEQ=')

    def scrape(self, chamber, session):
        for term in self.metadata['terms']:
            if session in term['sessions']:
                year = term['start_year']
                break

        self.versions_dict = self._versions_dict(year)

        base_bill_url = 'http://data.opi.mt.gov/bills/%d/BillHtml/' % year
        index_page = ElementTree(lxml.html.fromstring(self.urlopen(base_bill_url)))

        bill_urls = []
        for bill_anchor in index_page.findall('//a'):
            # See 2009 HB 645
            if bill_anchor.text.find("govlineveto") == -1:
                # House bills start with H, Senate bills start with S
                if chamber == 'lower' and bill_anchor.text.startswith('H'):
                    bill_urls.append("%s%s" % (base_bill_url, bill_anchor.text))
                elif chamber == 'upper' and bill_anchor.text.startswith('S'):
                    bill_urls.append("%s%s" % (base_bill_url, bill_anchor.text))

        for bill_url in bill_urls:
            bill = self.parse_bill(bill_url, session, chamber)
            if bill:
                self.save_bill(bill)

    def parse_bill(self, bill_url, session, chamber):

        # Temporarily skip the differently-formatted house budget bill.
        if 'billhtml/hb0002.htm' in bill_url.lower():
            return

        bill = None
        try:
            doc = lxml.html.fromstring(self.urlopen(bill_url))
        except XMLSyntaxError as e:
            self.logger.warning("Got %r while parsing %r" % (e, bill_url))
            return
        bill_page = ElementTree(doc)

        for anchor in bill_page.findall('//a'):
            if (anchor.text_content().startswith('status of') or
                anchor.text_content().startswith('Detailed Information (status)')):
                status_url = anchor.attrib['href'].replace("\r", "").replace("\n", "")
                bill = self.parse_bill_status_page(status_url, bill_url, session, chamber)

        if bill is None:
            # No bill was found.  Maybe something like HB0790 in the 2005 session?
            # We can search for the bill metadata.
            page_name = bill_url.split("/")[-1].split(".")[0]
            bill_type = page_name[0:2]
            bill_number = page_name[2:]
            laws_year = self.metadata['session_details'][session]['years'][0] % 100

            status_url = self.search_url_template % (laws_year, bill_type, bill_number)
            bill = self.parse_bill_status_page(status_url, bill_url, session, chamber)

        # Get versions on the detail page.
        versions = [a['action'] for a in bill['actions']]
        versions = [a for a in versions if 'Version Available' in a]
        if not versions:
            version_name = 'Introduced'
        else:
            version = versions.pop()
            if 'New Version' in version:
                version_name = 'Amended'
            elif 'Enrolled' in version:
                version_name = 'Enrolled'

        self.add_other_versions(bill)

        # Add html.
        bill.add_version(version_name, bill_url, mimetype='text/html')

        # Add pdf.
        url = set(bill_page.xpath('//a/@href[contains(., "BillPdf")]')).pop()
        bill.add_version(version_name, url, mimetype='application/pdf')

        # Add status url as a source.
        bill.add_source(status_url)

        return bill

    def _get_tabledata(self, status_page):
        '''Montana doesn't currently list co/multisponsors on any of the
        legislation I've seen. So this function only adds the primary
        sponsor.'''
        tabledata = defaultdict(list)
        join = ' '.join

        # Get the top data table.
        for tr in status_page.xpath('//tr'):
            tds = tr.xpath("td")
            try:
                key = tds[0].text_content().lower()
                val = join(tds[1].text_content().strip().split())
            except IndexError:
                continue
            if not key.startswith('('):
                tabledata[key].append(val)

        return dict(tabledata)

    def parse_bill_status_page(self, status_url, bill_url, session, chamber):
        status_page = lxml.html.fromstring(self.urlopen(status_url))
        # see 2007 HB 2... weird.
        try:
            bill_id = status_page.xpath("//tr[2]/td[2]")[0].text_content()
        except IndexError:
            bill_id = status_page.xpath('//tr[1]/td[2]')[0].text_content()

        try:
            xp = '//b[text()="Short Title:"]/../following-sibling::td/text()'
            title = status_page.xpath(xp).pop()
        except IndexError:
            title = status_page.xpath('//tr[1]/td[2]')[0].text_content()

        # Add bill type.
        _bill_id = bill_id.lower()
        if 'b' in _bill_id:
            type_ = 'bill'

        elif 'j' in _bill_id or 'jr' in _bill_id:
            type_ = 'joint resolution'

        elif 'cr' in _bill_id:
            type_ = 'concurrent resolution'

        elif 'r' in _bill_id:
            type_ = 'resolution'

        bill = Bill(session, chamber, bill_id, title, type=type_)
        self.add_actions(bill, status_page)
        self.add_votes(bill, status_page, status_url)

        tabledata = self._get_tabledata(status_page)

        # Add sponsor info.
        bill.add_sponsor('primary', tabledata['primary sponsor:'][0])

        # A various plus fields MT provides.
        plus_fields = [
            'requester',
            ('chapter number:', 'chapter'),
            'transmittal date:',
            'drafter',
            'fiscal note probable:',
            'bill draft number:',
            'preintroduction required:',
            'by request of',
            'category:']

        for x in plus_fields:
            if isinstance(x, tuple):
                _key, key = x
            else:
                _key = key = x
                key = key.replace(' ', '_')

            try:
                val = tabledata[_key]
            except KeyError:
                continue

            if len(val) == 1:
                val = val[0]

            bill[key] = val

        # Add bill subjects.
        xp = '//th[contains(., "Revenue/Approp.")]/ancestor::table/tr'
        subjects = []
        for tr in status_page.xpath(xp):
            try:
                subj = tr.xpath('td')[0].text_content()
            except:
                continue
            subjects.append(subj)

        bill['subjects'] = subjects

        self.add_fiscal_notes(status_page, bill)

        return bill

    def add_actions(self, bill, status_page):

        for action in reversed(status_page.xpath('//div/form[3]/table[1]/tr')[1:]):
            try:
                actor = actor_map[action.xpath("td[1]")[0].text_content().split(" ")[0]]
                action_name = action.xpath("td[1]")[0].text_content().replace(actor, "")[4:].strip()
            except KeyError:
                action_name = action.xpath("td[1]")[0].text_content().strip()
                actor = 'clerk' if action_name == 'Chapter Number Assigned' else ''

            action_name = action_name.replace("&nbsp", "")
            action_date = datetime.strptime(action.xpath("td[2]")[0].text, '%m/%d/%Y')
            action_type = actions.categorize(action_name)

            if 'by senate' in action_name.lower():
                actor = 'upper`'
            bill.add_action(actor, action_name, action_date, action_type)

    def _versions_dict(self, year):
        '''Get a mapping of ('HB', '2') tuples to version urls.'''

        res = defaultdict(dict)

        url = 'http://data.opi.mt.gov/bills/%d/' % year

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        for url in doc.xpath('//a[contains(@href, "/bills/")]/@href')[1:]:
            doc = url2lxml(url)
            for fn in doc.xpath('//a/@href')[1:]:
                _url = urljoin(url, fn)
                _, _, fn = fn.rpartition('/')
                m = re.search(r'([A-Z]+)0*(\d+)_?([^.]+)', fn)
                if m:
                    type_, id_, version = m.groups()
                    res[(type_, id_)][version] = _url

        return res

    def add_other_versions(self, bill):

        count = itertools.count(1)
        xcount = itertools.chain([1], itertools.count(1))
        type_, id_ = bill['bill_id'].split()
        version_urls = copy.copy(self.versions_dict[(type_, id_)])
        mimetype = 'application/pdf'
        version_strings = [
            'Introduced Bill Text Available Electronically',
            'Printed - New Version Available',
            'Clerical Corrections Made - New Version Available']

        if bill['bill_id'] == 'HB 2':
            # Need to special-case this one.
            return

        for i, a in enumerate(bill['actions']):

            text = a['action']
            actions = bill['actions']
            if text in version_strings:

                name = actions[i - 1]['action']

                if 'Clerical Corrections' in text:
                    name += ' (clerical corrections made)'
                try:
                    url = version_urls.pop(str(count.next()))
                except KeyError:
                    msg = "No url found for version: %r" % name
                    self.warning(msg)
                else:
                    if 'Introduced Bill' in text:
                        name = 'Introduced'
                    bill.add_version(name, url, mimetype)
                    continue

                try:
                    url = version_urls['x' + str(xcount.next())]
                except KeyError:
                    continue

                name = actions[i - 1]['action']
                bill.add_version(name, url, mimetype)

    def add_votes(self, bill, status_page, status_url):
        '''For each row in the actions table that links to a vote,
        retrieve the vote object created by the scraper in add_actions
        and update the vote object with the voter data.
        '''
        base_url, _, _ = status_url.rpartition('/')
        base_url += '/'
        status_page.make_links_absolute(base_url)

        for tr in status_page.xpath('//table')[3].xpath('tr')[2:]:
            tds = list(tr)

            if tds:
                vote_url = tds[2].xpath('a/@href')

                if vote_url:

                    # Get the matching vote object.
                    text = tr.itertext()
                    action = text.next().strip()
                    chamber, action = action.split(' ', 1)
                    date = datetime.strptime(text.next(), '%m/%d/%Y')
                    vote_url = vote_url[0]

                    chamber = actor_map[chamber]
                    vote = dict(chamber=chamber, date=date,
                                action=action,
                                sources=[{'url': vote_url}])

                    # Update the vote object with voters..
                    vote = self._parse_votes(vote_url, vote)
                    if vote:
                        bill.add_vote(vote)

    def _parse_votes(self, url, vote):
        '''Given a vote url and a vote object, extract the voters and
        the vote counts from the vote page and update the vote object.
        '''
        if url.lower().endswith('.pdf'):

            try:
                resp = self.urlopen(url)
            except HTTPError:
                # This vote document wasn't found.
                msg = 'No document found at url %r' % url
                self.logger.warning(msg)
                return

            try:
                v = PDFCommitteeVote(url, resp.bytes)
                return v.asvote()
            except PDFCommitteeVoteParseError as e:
                # Warn and skip.
                self.warning("Could't parse committee vote at %r" % url)
                return

        keymap = {'Y': 'yes', 'N': 'no'}
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        # Yes, no, excused, absent.
        try:
            vals = doc.xpath('//table')[1].xpath('tr/td/text()')
        except IndexError:
            # Most likely was a bogus link lacking vote data.
            return

        y, n, e, a = map(int, vals)
        vote.update(yes_count=y, no_count=n, other_count=e + a)

        # Get the motion.
        try:
            motion = doc.xpath('//br')[-1].tail.strip()
        except:
            # Some of them mysteriously have no motion listed.
            motion = vote['action']

        vote['motion'] = motion

        # Add placeholder for passed (see below)
        vote['passed'] = False

        vote = Vote(**vote)

        for text in doc.xpath('//table')[2].xpath('tr/td/text()'):
            if not text.strip(u'\xa0'):
                continue
            v, name = filter(None, text.split(u'\xa0'))
            getattr(vote, keymap.get(v, 'other'))(name)

        action = vote['action']

        # Existing code to deterimine value of `passed`
        yes_votes = vote['yes_votes']
        no_votes = vote['no_votes']
        passed = None

        # some actions take a super majority, so we aren't just
        # comparing the yeas and nays here.
        for i in vote_passage_indicators:
            if action.count(i):
                passed = True
        for i in vote_failure_indicators:
            if action.count(i) and passed == True:
                # a quick explanation:  originally an exception was
                # thrown if both passage and failure indicators were
                # present because I thought that would be a bug in my
                # lists.  Then I found 2007 HB 160.
                # Now passed = False if the nays outnumber the yays..
                # I won't automatically mark it as passed if the yays
                # ounumber the nays because I don't know what requires
                # a supermajority in MT.
                if no_votes >= yes_votes:
                    passed = False
                else:
                    raise Exception("passage and failure indicator"
                                    "both present at: %s" % url)
            if action.count(i) and passed == None:
                passed = False
        for i in vote_ambiguous_indicators:
            if action.count(i):
                passed = yes_votes > no_votes
        if passed is None:
            raise Exception("Unknown passage at: %s" % url)

        vote['passed'] = passed

        return vote

    def add_fiscal_notes(self, doc, bill):

        for link in doc.xpath('//a[contains(text(), "Fiscal Note")]'):
            bill.add_document(name=link.text_content().strip(),
                              url=link.attrib['href'],
                              mimetype='application/pdf')


class PDFCommitteeVoteParseError(Exception):
    pass


class PDFCommitteeVote404Error(PDFCommitteeVoteParseError):
    pass


class PDFCommitteeVote(object):

    def __init__(self, url, resp):

        self.url = url

        # Fetch the document and put it into tempfile.
        fd, filename = tempfile.mkstemp()

        with open(filename, 'wb') as f:
            f.write(resp)

        # Convert it to text.
        try:
            text = convert_pdf(filename, type='text')
        except:
            msg = "couldn't convert pdf."
            raise PDFCommitteeVoteParseError(msg)

        # Get rid of the temp file.
        os.close(fd)
        os.remove(filename)

        if not text.strip():
            msg = 'PDF file was empty.'
            raise PDFCommitteeVoteParseError(msg)

        self.text = '\n'.join(filter(None, text.splitlines()))

    def committee(self):
        """
        XXX: OK. So, the error here:


            When we have a `joint' chamber vote, we also need the committee
            attached with the bill, or the OCD conversion won't know which
            committee on the whole to associate with.

            In addition, matching to the COW is wrong; since this was a
            committee vote. I'm stubbing this out since the site is currently
            offline
        """
        raise NotImplemented

    def chamber(self):
        chamber_dict = {'HOUSE': 'lower', 'SENATE': 'upper', 'JOINT': 'joint'}
        chamber = re.search(r'(HOUSE|SENATE|JOINT)', self.text)
        return chamber_dict[chamber.group(1)]

    def date(self):

        months = '''january february march april may june july
            august september october november december'''.split()

        text = iter(self.text.splitlines())

        line = text.next().strip()
        while True:

            _line = line.lower()
            break_outer = False
            for m in months:
                if m in _line:
                    break_outer = True
                    break

            if break_outer:
                break

            try:
                line = text.next().strip()
            except StopIteration:
                msg = 'Couldn\'t parse the vote date.'
                raise PDFCommitteeVoteParseError(msg)

        try:
            return datetime.strptime(line, '%B %d, %Y')
        except ValueError:
            raise PDFCommitteeVoteParseError("Could't parse the vote date.")

    def motion(self):

        text = iter(self.text.splitlines())

        while True:
            line = text.next()
            if 'VOTE TABULATION' in line:
                break

        line = text.next()
        _, motion = line.split(' - ')
        motion = motion.strip()
        return motion

    def _getcounts(self):
        m = re.search(r'YEAS \- .+$', self.text, re.MULTILINE)
        if m:
            x = m.group()
        else:
            msg = "Couldn't find vote counts."
            raise PDFCommitteeVoteParseError(msg)
        self._counts_data = dict(re.findall(r'(\w+) - (\d+)', x))

    def yes_count(self):
        if not hasattr(self, '_counts_data'):
            self._getcounts()
        return int(self._counts_data['YEAS'])

    def no_count(self):
        if not hasattr(self, '_counts_data'):
            self._getcounts()
        return int(self._counts_data['NAYS'])

    def other_count(self):
        return len(self.other_votes())

    def _getvotes(self):
        junk = ['; by Proxy']
        res = defaultdict(list)
        data = re.findall(r'([A-Z]) {6,7}(.+)', self.text, re.MULTILINE)
        for val, name in data:
            for j in junk:
                name = name.replace(j, '')
            res[val].append(name)
        self._votes_data = res

    def yes_votes(self):
        if not hasattr(self, '_votes_data'):
            self._getvotes()
        return self._votes_data['Y']

    def other_votes(self):
        if not hasattr(self, '_votes_data'):
            self._getvotes()
        return self._votes_data['--']

    def no_votes(self):
        if not hasattr(self, '_votes_data'):
            self._getvotes()
        return self._votes_data['N']

    def passed(self):
        return self.no_count() < self.yes_count()

    def asdict(self):
        res = {}
        methods = '''yes_count no_count motion
                  chamber committee other_count passed date'''.split()
        for m in methods:
            res[m] = getattr(self, m)()
        return res

    def asvote(self):
        v = Vote(**self.asdict())
        for key in 'yes_votes no_votes other_votes'.split():
            v[key] = getattr(self, key)()
        v.add_source(self.url)
        return v

########NEW FILE########
__FILENAME__ = committees
'''
This file has a slightly unusual structure. The urs and the main
scrape function are defined at the top level because the legislator
scrape requires data from the committee pages in order to get
properly capitalized names. So that part needs to be importable and
hence the need to dcouple it from the scraper instance. If that makes
sense.

This file currently scrapes only standing committees and doesn't
bother with the arguably important joint appropriations subcomittees,
Which contain members of the appropriations committees from each
and deal with budgetary matters.
'''
import re
from itertools import dropwhile, takewhile
from collections import defaultdict

import lxml.html

from billy.scrape.committees import CommitteeScraper, Committee
from billy.scrape.utils import convert_pdf
import scrapelib


committee_urls = {
    'lower': {
        2011: 'http://leg.mt.gov/css/House/house-committees-2011.asp',
        2013: 'http://leg.mt.gov/content/Committees/Session/2013%20house%20committees%20-%20columns.pdf',
        },

    'upper': {
        2011: 'http://leg.mt.gov/css/Senate/senate%20committees-2011.asp',
        2013: 'http://leg.mt.gov/content/Committees/Session/2013%20senate%20committees%20-%20columns.pdf',
        },

    'joint': {
        2011: 'http://leg.mt.gov/css/Sessions/62nd/joint%20subcommittees.asp',
        # 2013: 'http://leg.mt.gov/css/Sessions/62nd/joint%20subcommittees.asp',
        }
    }


class MTCommitteeScraper(CommitteeScraper):

    jurisdiction = 'mt'

    def scrape(self, chamber, term):
        '''Since the legislator names aren't properly capitalized in the
        csv file, scrape the committee page and use the names listed there
        instead.
        '''
        for tdata in self.metadata['terms']:
            if term == tdata['name']:
                year = tdata['start_year']
                break

        url = committee_urls[chamber][year]
        fn, response = self.urlretrieve(url)

        if response.headers['content-type'] == 'application/pdf':
            # The committee list is a pdf.
            self.scrape_committees_pdf(year, chamber, fn, url)

        else:
            # Here it's html.
            with open(fn) as f:
                doc = lxml.html.fromstring(response.text)
                for name_dict, c in scrape_committees_html(year, chamber, doc):
                    if c['members']:
                        self.save_committee(c)

    def scrape_committees_pdf(self, year, chamber, filename, url):
        text = convert_pdf(filename, type='text-nolayout')

        # Hot garbage.
        for hotgarbage, replacement in (
            ('Judicial Branch, Law Enforcement,\s+and\s+Justice',
             'Judicial Branch, Law Enforcement, and Justice'),

            ('Natural Resources and\s+Transportation',
             'Natural Resources and Transportation'),

            ('Federal Relations, Energy,\sand\sTelecommunications',
             'Federal Relations, Energy, and Telecommunications')
            ):
            text = re.sub(hotgarbage, replacement, text)

        lines = iter(text.splitlines())

        # Drop any lines before the ag committee.
        lines = dropwhile(lambda s: 'Agriculture' not in s, lines)

        def is_committee_name(line):
            if '(cont.)' in line.lower():
                return False
            for s in (
                'committee', ' and ', 'business', 'resources',
                'legislative', 'administration', 'government',
                'local', 'planning', 'judicial', 'natural',
                'resources', 'general', 'health', 'human'):
                if s in line.lower():
                    return True
            if line.istitle() and len(line.split()) == 1:
                return True
            return False

        def is_legislator_name(line):
            return re.search(r'\([RD]', line)

        comm = None
        in_senate_subcommittees = False
        while 1:
            try:
                line = lines.next()
            except StopIteration:
                break

            if 'Joint Appropriations/Finance &' in line:
                # Toss the line continuation.
                lines.next()

                # Move on.
                in_senate_subcommittees = True
                chamber = 'joint'
                continue

            if is_committee_name(line):
                subcommittee = None

                if in_senate_subcommittees:
                    committee = ('Joint Appropriations/Finance & Claims')
                    subcommittee = line
                else:
                    committee = line

                if comm and comm['members']:
                    self.save_committee(comm)

                comm = Committee(chamber, committee=committee,
                                 subcommittee=subcommittee)
                comm.add_source(url)

            elif is_legislator_name(line):
                name, party = line.rsplit('(', 1)
                name = name.strip()
                if re.search('[^V] Ch', party):
                    role = 'chair'
                elif 'V Ch' in party:
                    role = 'vice chair'
                else:
                    role = 'member'
                comm.add_member(name, role)

        if comm['members']:
            self.save_committee(comm)


def scrape_committees_html(year, chamber, doc):
    name_dict = defaultdict(set)
    tds = doc.xpath('//td[@valign="top"]')[3:]

    cache = []
    for td in tds:
        for name_dict, c in _committees_td(td, chamber, url, name_dict):
            if c not in cache:
                cache.append(c)
                yield name_dict, c

    # Get the joint approps subcommittees during the upper scrape.
    if chamber == 'upper':
        url = committee_urls['joint'][year]
        html = scrapelib.urlopen(url)

        name_dict = defaultdict(set)
        doc = lxml.html.fromstring(html)
        tds = doc.xpath('//td[@valign="top"]')[3:]

        cache = []
        for td in tds:
            for name_dict, c in _committees_td(td, 'joint', url, name_dict):
                if c not in cache:
                    cache.append(c)

                    # These are subcommittees, so a quick switcheroo of the names:
                    c['subcommittee'] = c['committee']
                    c['committee'] = 'Appropriations'
                    yield name_dict, c


def _committees_td(el, chamber, url, name_dict):
    '''Get all committees data from a particular td in the
    comittees page.
    '''
    edge = '      '

    # The unreliable HTML (dreamweaver...) is different on upper/lower pages.
    if chamber == 'lower':
        not_edge = lambda s: (s != edge and s != 'PDF Version')
        is_edge = lambda s: (s == edge or s == 'PDF Version')
        predicate = lambda s: ('Secretary:' not in s)

    if chamber == 'upper':
        not_edge = lambda s: s != edge
        is_edge = lambda s: s == edge
        predicate = not_edge

    if chamber == 'joint':
        not_edge = lambda s: not s.strip().startswith('Education')
        is_edge = lambda s: s == edge
        predicate = lambda s: ('Secretary:' not in s)

    itertext = el.itertext()

    # Toss preliminary junk.
    itertext = dropwhile(not_edge, itertext)

    committees_data = []
    failures = 0
    while True:

        # Drop any leading "edge"
        itertext = dropwhile(is_edge, itertext)

        # Get the rest of committee data.
        data = list(takewhile(predicate, itertext))

        if not data:
            if failures > 5:
                break
            else:
                failures += 1
                continue

        committees_data.append(data)

    for data in committees_data:
        c = _committee_data(data, chamber, url, name_dict)
        if c:
            yield c


def _committee_data(lines, chamber, url, name_dict):
    '''Given a list of lines of committee data from a td element
    on the committees page, extract the commitee name, the members,
    and yeild a committee object. Also yield the name dict incase
    the calling function needs it for something.
    '''
    name_pattern = r'\s{,20}(?:(.+)\:)?\s{,20}(.+?) \((?:\w\-([^)]+))'

    # Functions to identify unused data.
    junk = [lambda s: s != 'On Call',
            lambda s: 'Staff:' not in s,
            lambda s: 'Secretary:' not in s,
            lambda s: s.strip(),
            lambda s: not s.isupper()]

    # Toss unused data.
    for j in junk:
        lines = filter(j, lines)

    if (len(lines) < 2) or (u'\xa0' in lines):
        return

    lines = lines[::-1]
    kw = {'chamber': chamber}
    kw['committee'] = lines.pop().strip()

    if lines[-1].startswith('Meets'):
        kw['meetings_info'] = lines.pop().strip()

    c = Committee(**kw)

    for name in reversed(lines):
        kwargs = {}
        m = re.search(name_pattern, name)
        if m:
            title, name, city = m.groups()
            if title:
                title = title.lower()
            house = re.search(r'(Sen\.|Rep\.)\s+', name)
            if house:
                house = house.group()
                if 'Sen.' in house:
                    kwargs['chamber'] = 'upper'
                elif 'Rep.' in house:
                    kwargs['chamber'] = 'lower'
                name = name.replace(house, '').strip()
            name_dict[city.lower()].add(name)
            c.add_member(name, role=(title or 'member'), **kwargs)

    c.add_source(url)

    return name_dict, c

########NEW FILE########
__FILENAME__ = legislators
import re
import csv
import difflib
import urlparse
from itertools import dropwhile, takewhile

import lxml.html

from billy.scrape.legislators import LegislatorScraper, Legislator
import scrapelib


class NoDetails(Exception):
    pass


class MTLegislatorScraper(LegislatorScraper):

    jurisdiction = 'mt'

    def url_xpath(self, url):
        # Montana's legislator page was returning valid content with 500
        # code as of 1/9/2013. Previous discussions with them after similar
        # incidents in the past suggest some external part of their stack
        # is having some issue and the error is bubbling up to the ret code.
        self.raise_errors = False
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        self.raise_errors = True
        return doc

    def scrape(self, chamber, term):

        for tdata in self.metadata['terms']:
            if term == tdata['name']:
                year = tdata['start_year']
                session_number = tdata['session_number']
                break

        # Fetch the csv.
        url = 'http://leg.mt.gov/content/sessions/%s/%d%sMembers.txt' % \
            (session_number, year, chamber == 'upper' and 'Senate' or 'House')

        # Parse it.
        data = self.urlopen(url)
        data = data.replace('"""', '"')  # weird triple quotes
        data = data.splitlines()

        fieldnames = ['last_name', 'first_name', 'party', 'district',
                      'address', 'city', 'state', 'zip']
        csv_parser = csv.DictReader(data, fieldnames)

        district_leg_urls = self._district_legislator_dict()

        # Toss the row headers.
        next(csv_parser)

        for entry in csv_parser:
            if not entry:
                continue

            # City.
            entry['city'] = entry['city']

            # Address.
            entry['address'] = entry['address']

            # District.
            district = entry['district']
            hd_or_sd, district = district.split()
            del entry['district']

            # Party.
            party_letter = entry['party']
            party = {'D': 'Democratic', 'R': 'Republican'}[party_letter]
            entry['party'] = party
            del entry['party']

            # Get full name properly capped.
            fullname = _fullname = '%s %s' % (entry['first_name'].capitalize(),
                                   entry['last_name'].capitalize())

            city_lower = entry['city'].lower()

            # Get any info at the legislator's detail_url.
            detail_url = district_leg_urls[hd_or_sd][district]

            # Get the office.
            address = '\n'.join([
                entry['address'],
                '%s, %s %s' % (entry['city'], entry['state'], entry['zip'])
                ])

            office = dict(
                name='District Office', type='district', phone=None,
                fax=None, email=None,
                address=address)

            try:
                deets = self._scrape_details(detail_url)
            except NoDetails:
                self.logger.warning("No details found at %r" % detail_url)
                continue

            # Add the details and delete junk.
            entry.update(deets)
            del entry['first_name'], entry['last_name']

            legislator = Legislator(term, chamber, district, fullname,
                                    party=party)
            legislator.update(entry)
            legislator.add_source(detail_url)
            legislator.add_source(url)
            legislator['url'] = detail_url

            office['phone'] = deets.get('phone')
            office['fax'] = deets.get('fax')
            legislator.add_office(**office)

            self.save_legislator(legislator)

    def _district_legislator_dict(self):
        '''Create a mapping of districts to the legislator who represents
        each district in each house.

        Used to get properly capitalized names in the legislator scraper.
        '''
        res = {'HD': {}, 'SD': {}}

        url = 'http://leg.mt.gov/css/find%20a%20legislator.asp'

        # Get base url.
        parts = urlparse.urlparse(url)
        parts._replace(path='')
        baseurl = parts.geturl()

        # Go the find-a-legislator page.
        doc = self.url_xpath(url)
        doc.make_links_absolute(baseurl)

        # Get the link to the current member roster.
        url = doc.xpath('//a[contains(@href, "roster.asp")]/@href')[0]

        # Fetch it.
        # self.raise_errors = False
        # html = self.urlopen(url)
        # doc = lxml.html.fromstring(html)
        # self.raise_errors = True
        try:
            with open('mt_hotgarbage.txt') as f:
                html = f.read()
                doc = lxml.html.fromstring(html)
        except IOError:
            self.raise_errors = False
            html = self.urlopen(url)
            doc = lxml.html.fromstring(html)
            self.raise_errors = True
            with open('mt_hotgarbage.txt', 'w') as f:
                f.write(html)

        # Get the new baseurl, like 'http://leg.mt.gov/css/Sessions/62nd/'
        parts = urlparse.urlparse(url)
        path, _, _ = parts.path.rpartition('/')
        parts._replace(path=path)
        baseurl = parts.geturl()
        doc.make_links_absolute(baseurl)
        table = doc.xpath('//table[@name="Legislators"]')[0]

        for tr in table.xpath('tr'):

            td1, td2 = tr.xpath('td')

            # Get link to the member's page.
            detail_url = td1.xpath('h4/a/@href')[0]

            # Get the members district so we can match the
            # profile page with its csv record.
            house, district = td2.text_content().split()

            res[house][district] = detail_url

        return res

    def _scrape_details(self, url):
        '''Scrape the member's bio page.

        Things available but not currently scraped are office address,
        and waaay too much contanct info, including personal email, phone.
        '''
        doc = self.url_xpath(url)
        # Get base url.
        parts = urlparse.urlparse(url)
        parts._replace(path='')
        baseurl = parts.geturl()

        doc.make_links_absolute(baseurl)

        xpath = '//img[contains(@src, "legislator")]/@src'

        try:
            photo_url = doc.xpath(xpath).pop()
        except IndexError:
            raise NoDetails('No details found at %r' % url)

        details = { 'photo_url': photo_url }

        # # Parse address.
        elements = list(doc.xpath('//b[contains(., "Address")]/..')[0])
        # # dropper = lambda element: element.tag != 'b'
        # # elements = dropwhile(dropper, elements)
        # # assert next(elements).text == 'Address'
        # # elements = list(takewhile(taker, elements))

        # # MT's website currently has a typo that places the "address"
        # # heading inline with the "Information Office" phone number.
        # # This hack tempprarily makes things work.
        elements = elements[3:]
        chunks = []
        for br in elements:
            chunks.extend(filter(None, [br.text, br.tail]))

        # As far as I can tell, MT legislators don't have capital offices.
        # office = dict(name='District Office', type='district', phone=None,
        #               fax=None, email=None,
        #               address='\n'.join(chunks[:2]))
        for line in chunks[2:]:
            if not line.strip():
                continue
            for key in ('ph', 'fax'):
                if key in line.lower():
                    key = {'ph': 'phone'}.get(key)
                    break
            number = re.search('\(\d{3}\) \d{3}\-\d{4}', line)
            if number:
                number = number.group()
                if key:
                    # Used to set this on the office.
                    details[key] = number

        # details['offices'] = [office]

        try:
            email = doc.xpath('//b[contains(., "Email")]/..')[0]
        except IndexError:
            pass
        else:
            if email:
                html = lxml.html.tostring(email.getparent())
                match = re.search('\w+@\w+\.[a-z]+', html)
                if match:
                    details['email'] = match.group()

        return details


########NEW FILE########
__FILENAME__ = bills
import datetime as dt
import re
from collections import defaultdict

import lxml.html

from billy.scrape.bills import BillScraper, Bill

class NCBillScraper(BillScraper):

    jurisdiction = 'nc'

    _action_classifiers = {
        'Vetoed': 'governor:vetoed',
        'Signed By Gov': 'governor:signed',
        'Signed by Gov': 'governor:signed',
        'Pres. To Gov.': 'governor:received',
        'Withdrawn from ': 'bill:withdrawn',
        'Ref ': 'committee:referred',
        'Re-ref ': 'committee:referred',
        'Reptd Fav': 'committee:passed:favorable',
        'Reptd Unfav': 'committee:passed:unfavorable',
        'Passed 1st Reading': 'bill:reading:1',
        'Passed 2nd Reading': 'bill:reading:2',
        'Passed 3rd Reading': ['bill:passed', 'bill:reading:3'],
        'Passed 2nd & 3rd Reading': ['bill:passed', 'bill:reading:2',
                                     'bill:reading:3'],
        'Failed 3rd Reading': ['bill:failed', 'bill:reading:3'],
        'Filed': 'bill:introduced',
        'Adopted': 'bill:passed',       # resolutions
        'Concurred In': 'amendment:passed',
        'Com Amend Adopted': 'amendment:passed',
        'Became Law w/o Signature': 'other',
        'Assigned To': 'committee:referred',
        'Amendment Withdrawn': 'amendment:withdrawn',
        'Amendment Offered': 'amendment:introduced',
        'Amend Failed': 'amendment:failed',
        'Amend Adopted': 'amendment:passed',
    }

    def is_latest_session(self, session):
        return self.metadata['terms'][-1]['sessions'][-1] == session

    def build_subject_map(self):
        # don't scan subject list twice in one run
        if hasattr(self, 'subject_map'):
            return

        self.subject_map = defaultdict(list)
        cur_subject = None

        letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'

        for letter in letters:
            url = 'http://www.ncga.state.nc.us/gascripts/Reports/keywords.pl?Letter=' + letter
            html = self.urlopen(url)
            doc = lxml.html.fromstring(html)
            for td in doc.xpath('//td[@class="tableText"]'):
                if td.get('style') == 'font-weight: bold;':
                    cur_subject = td.text_content()
                else:
                    bill_link = td.xpath('a/text()')
                    if bill_link:
                        self.subject_map[bill_link[0]].append(cur_subject)

    def scrape_bill(self, chamber, session, bill_id):
        # there will be a space in bill_id if we're doing a one-off bill scrape
        # convert HB 102 into H102
        if ' ' in bill_id:
            bill_id = bill_id[0] + bill_id.split(' ')[-1]

        # if chamber comes in as House/Senate convert to lower/upper
        if chamber == 'Senate':
            chamber = 'upper'
        elif chamber == 'House':
            chamber = 'lower'

        bill_detail_url = 'http://www.ncga.state.nc.us/gascripts/'\
            'BillLookUp/BillLookUp.pl?Session=%s&BillID=%s' % (
            session, bill_id)

        # parse the bill data page, finding the latest html text
        data = self.urlopen(bill_detail_url)
        doc = lxml.html.fromstring(data)

        title_div_txt = doc.xpath('//td[@style="text-align: center; white-space: nowrap; width: 60%; font-weight: bold; font-size: x-large;"]/text()')[0]
        if 'Joint Resolution' in title_div_txt:
            bill_type = 'joint resolution'
            bill_id = bill_id[0] + 'JR ' + bill_id[1:]
        elif 'Resolution' in title_div_txt:
            bill_type = 'resolution'
            bill_id = bill_id[0] + 'R ' + bill_id[1:]
        elif 'Bill' in title_div_txt:
            bill_type = 'bill'
            bill_id = bill_id[0] + 'B ' + bill_id[1:]

        title_style_xpath = '//div[@style="text-align: center; font: bold 20px Arial; margin-top: 15px; margin-bottom: 8px;"]/text()'
        bill_title = doc.xpath('//div[@id="title"]')[0].text_content()

        bill = Bill(session, chamber, bill_id, bill_title, type=bill_type)
        bill.add_source(bill_detail_url)

        # skip first PDF link (duplicate link to cur version)
        if chamber == 'lower':
            link_xpath = '//a[contains(@href, "/Bills/House/PDF/")]'
        else:
            link_xpath = '//a[contains(@href, "/Bills/Senate/PDF/")]'
        for vlink in doc.xpath(link_xpath)[1:]:
            # get the name from the PDF link...
            version_name = vlink.text.replace(u'\xa0', ' ')
            # but neighboring span with anchor inside has the HTML version
            version_url = vlink.xpath('./following-sibling::span/a/@href')
            version_url = 'http://www.ncga.state.nc.us' + version_url[0]
            bill.add_version(version_name, version_url,
                             mimetype='text/html', on_duplicate='use_new')

        # sponsors
        spon_td = doc.xpath('//th[text()="Sponsors:"]/following-sibling::td')[0]
        for leg in spon_td.xpath('a'):
            type = 'primary' if 'Primary' in leg.tail else 'cosponsor'
            name = leg.text_content().replace(u'\xa0', ' ')
            bill.add_sponsor('primary', name, chamber=chamber)

        # actions
        action_tr_xpath = '//td[starts-with(text(),"History")]/../../tr'
        # skip two header rows
        for row in doc.xpath(action_tr_xpath)[2:]:
            tds = row.xpath('td')
            act_date = tds[0].text
            actor = tds[1].text or ''
            # if text is blank, try diving in
            action = tds[2].text.strip() or tds[2].text_content().strip()

            act_date = dt.datetime.strptime(act_date, '%m/%d/%Y')

            if actor == 'Senate':
                actor = 'upper'
            elif actor == 'House':
                actor = 'lower'
            else:
                actor = 'executive'

            for pattern, atype in self._action_classifiers.iteritems():
                if action.startswith(pattern):
                    break
            else:
                atype = 'other'

            bill.add_action(actor, action, act_date, type=atype)

        if hasattr(self, 'subject_map'):
            subj_key = bill_id[0] + ' ' + bill_id.split(' ')[-1]
            bill['subjects'] = self.subject_map[subj_key]

        self.save_bill(bill)

    def scrape(self, session, chambers):
        if self.is_latest_session(session):
            self.build_subject_map()
        for chamber in chambers:
            self.scrape_chamber(chamber, session)

    def scrape_chamber(self, chamber, session):
        chamber = {'lower': 'House', 'upper': 'Senate'}[chamber]
        url = 'http://www.ncga.state.nc.us/gascripts/SimpleBillInquiry/'\
            'displaybills.pl?Session=%s&tab=Chamber&Chamber=%s' % (
            session, chamber)

        data = self.urlopen(url)
        doc = lxml.html.fromstring(data)
        for row in doc.xpath('//table[@cellpadding=3]/tr')[1:]:
            bill_id = row.xpath('td[1]/a/text()')[0]
            self.scrape_bill(chamber, session, bill_id)

########NEW FILE########
__FILENAME__ = committees
from billy.scrape.committees import CommitteeScraper, Committee

import lxml.html

class NCCommitteeScraper(CommitteeScraper):
    jurisdiction = 'nc'

    def scrape_committee(self, committee, url):
        url = url.replace(' ', '%20') + '&bPrintable=true'
        data = self.urlopen(url)
        doc = lxml.html.fromstring(data)
        for row in doc.xpath('//table/tr'):
            children = row.getchildren()
            if len(children) != 2:
                self.log('skipping members for ' + committee['committee'])
                continue
            mtype, members = row.getchildren()
            if mtype.text == 'Members':
                for m in members.getchildren():
                    committee.add_member(m.text)
            else:
                committee.add_member(members.text_content(), mtype.text)


    def scrape(self, term, chambers):
        base_url = 'http://www.ncga.state.nc.us/gascripts/Committees/Committees.asp?bPrintable=true&sAction=ViewCommitteeType&sActionDetails='

        chamber_slugs = {'upper': ['Senate%20Standing', 'Senate%20Select'],
                         'lower': ['House%20Standing', 'House%20Select']}

        for chamber in chambers:
            for ctype in chamber_slugs[chamber]:
                data = self.urlopen(base_url + ctype)
                doc = lxml.html.fromstring(data)
                doc.make_links_absolute(base_url+ctype)
                for comm in doc.xpath('//ul/li/a'):
                    name = comm.text
                    # skip committee of whole Senate
                    if 'Whole Senate' in name:
                        continue
                    url = comm.get('href')
                    committee = Committee(chamber, name)
                    self.scrape_committee(committee, url)
                    committee.add_source(url)
                    if not committee['members']:
                        self.warning('empty committee: %s', name)
                    else:
                        self.save_committee(committee)


########NEW FILE########
__FILENAME__ = legislators
from billy.scrape.legislators import LegislatorScraper, Legislator
import lxml.html

party_map = {'Dem': 'Democratic',
             'Rep': 'Republican',
             'Una': 'Unaffiliated'}

def get_table_item(doc, name):
    # get span w/ item
    span = doc.xpath('//span[text()="{0}"]'.format(name))[0]
    # get neighboring td's span
    dataspan = span.getparent().getnext().getchildren()[0]
    if dataspan.text:
        return (dataspan.text + '\n' +
                '\n'.join([x.tail for x in dataspan.getchildren()])).strip()
    else:
        return None

class NCLegislatorScraper(LegislatorScraper):
    jurisdiction = 'nc'

    def scrape(self, term, chambers):
        for chamber in chambers:
            self.scrape_chamber(chamber, term)

    def scrape_chamber(self, chamber, term):
        url = "http://www.ncga.state.nc.us/gascripts/members/"\
            "memberList.pl?sChamber="

        if chamber == 'lower':
            url += 'House'
        else:
            url += 'Senate'

        data = self.urlopen(url)
        doc = lxml.html.fromstring(data)
        doc.make_links_absolute('http://www.ncga.state.nc.us')
        rows = doc.xpath('//div[@id="mainBody"]/table/tr')

        for row in rows[1:]:
            party, district, full_name, counties = row.getchildren()

            party = party.text_content()
            party = party_map[party]

            district = district.text_content()

            notice = full_name.xpath('span')
            if notice:
                notice = notice[0].text_content()
                # skip resigned legislators
                if 'Resigned' in notice or 'Deceased' in notice:
                    continue
            else:
                notice = None
            link = full_name.xpath('a/@href')[0]
            full_name = full_name.xpath('a')[0].text_content()
            full_name = full_name.replace(u'\u00a0', ' ')

            # scrape legislator page details
            lhtml = self.urlopen(link)
            ldoc = lxml.html.fromstring(lhtml)
            ldoc.make_links_absolute('http://www.ncga.state.nc.us')
            photo_url = ldoc.xpath('//a[contains(@href, "pictures")]/@href')[0]
            phone = get_table_item(ldoc, 'Phone:')
            address = get_table_item(ldoc, 'Legislative Mailing Address:') or None
            email = ldoc.xpath('//a[starts-with(@href, "mailto:")]')[0].text or ''

            # save legislator
            legislator = Legislator(term, chamber, district, full_name,
                                    photo_url=photo_url, party=party,
                                    url=link, notice=notice, email=email)
            legislator.add_source(link)
            legislator.add_office('capitol', 'Capitol Office',
                                  address=address, phone=phone)
            self.save_legislator(legislator)

########NEW FILE########
__FILENAME__ = votes
import os
import datetime
from zipfile import ZipFile

from billy.scrape.votes import VoteScraper, Vote

class NCVoteScraper(VoteScraper):
    jurisdiction = 'nc'

    def scrape(self, chamber, session):
        if session == '2009':
            # 2009 files have a different delimiter and naming scheme.
            vote_data_url = 'ftp://www.ncga.state.nc.us/Bill_Status/Vote Data 2009.zip'
            naming_scheme = '{session}{file_label}.txt'
            delimiter = ";"
        else:
            vote_data_url = 'ftp://www.ncga.state.nc.us/Bill_Status/Votes%s.zip' % session
            naming_scheme = '{file_label}_{session}.txt'
            delimiter = "\t"
        fname, resp = self.urlretrieve(vote_data_url)
        # fname = "/Users/brian/Downloads/Vote Data 2009.zip"
        zf = ZipFile(fname)

        chamber_code = 'H' if chamber == 'lower' else 'S'

        # Members_YYYY.txt: tab separated
        # 0: id (unique only in chamber)
        # 1: H or S
        # 2: member name
        # 3-5: county, district, party
        # 6: mmUserId
        member_file = zf.open(naming_scheme.format(file_label='Members', session=session))
        members = {}
        for line in member_file.readlines():
            data = line.split(delimiter)
            if data[1] == chamber_code:
                members[data[0]] = data[2]

        # Votes_YYYY.txt
        # 0: sequence number
        # 1: chamber (S/H)
        # 2: date
        # 3: prefix
        # 4: bill_id
        # 5: yes votes
        # 6: no votes
        # 7: excused absences
        # 8: excused votes
        # 9: didn't votes
        # 10: total yes+no
        # 11: sponsor
        # 12: reading info
        # 13: info
        # 20: PASSED/FAILED
        # 21: legislative day
        vote_file = zf.open(naming_scheme.format(file_label='Votes', session=session))
        bill_chambers = {'H':'lower', 'S':'upper'}
        votes = {}
        for line in vote_file.readlines():
            data = line.split(delimiter)
            if len(data) < 24:
                self.warning('line too short %s', data)
                continue
            if data[1] == chamber_code:
                date = datetime.datetime.strptime(data[2][:16],
                                                  '%Y-%m-%d %H:%M')
                if data[3][0] not in bill_chambers:
                    # skip votes that aren't on bills
                    self.log('skipping vote %s' % data[0])
                    continue

                votes[data[0]] = Vote(chamber, date, data[13],
                                      'PASS' in data[20],
                                      int(data[5]),
                                      int(data[6]),
                                      int(data[7])+int(data[8])+int(data[9]),
                                      bill_chamber=bill_chambers[data[3][0]],
                                      bill_id=data[3]+data[4], session=session)

        member_vote_file = zf.open(naming_scheme.format(file_label='MemberVotes', session=session))
        # 0: member id
        # 1: chamber (S/H)
        # 2: vote id
        # 3: vote chamber (always same as 1)
        # 4: vote (Y,N,E,X)
        # 5: pair ID (member)
        # 6: pair order
        # If a vote is paired then it should be counted as an 'other'
        for line in member_vote_file.readlines():
            data = line.split(delimiter)
            if data[1] == chamber_code:
                try:
                    member_voting = members[data[0]]
                except KeyError:
                    self.debug('Member %s not found.' % data[0])
                    continue
                try:
                    vote = votes[data[2]]
                except KeyError:
                    self.debug('Vote %s not found.' % data[2])
                    continue

                # -1 votes are Lt. Gov, not included in count, so we add them
                if data[4] == 'Y' and not data[5]:
                    if data[0] == '-1':
                        vote['yes_count'] += 1
                    vote.yes(member_voting)
                elif data[4] == 'N' and not data[5]:
                    if data[0] == '-1':
                        vote['no_count'] += 1
                    vote.no(member_voting)
                else:
                    # for some reason other_count is high for paired votes
                    if data[5]:
                        vote['other_count'] -= 1
                    # is either E: excused, X: no vote, or paired (doesn't count)
                    vote.other(member_voting)

        for vote in votes.itervalues():
            vote.validate()
            vote.add_source(vote_data_url)
            self.save_vote(vote)

        # remove file
        zf.close()
        os.remove(fname)

########NEW FILE########
__FILENAME__ = actions
from billy.scrape.actions import Rule, BaseCategorizer

# These are regex patterns that map to action categories.
_categorizer_rules = (
    Rule(r'Amendment proposed on floor', 'amendment:introduced'),
    Rule(r'Amendment failed', 'amendment:failed'),
    Rule(r'Amendment adopted, placed on calendar', ''),
    Rule(r'^Filed with ', 'bill:introduced'),
    Rule(r'^Introduced', 'bill:introduced'),
    Rule(r'^Second reading', 'bill:reading:2'),
    Rule(r'passed as amended', 'bill:passed'),
    Rule(r'passed', 'bill:passed'),
    Rule(r'Sent to Governor', 'governor:received'),
    Rule(r'Reported back', 'committee:passed'),
    Rule(r'Reported back.*do pass', 'committee:passed:favorable'),
    Rule(r'Reported back.*do not pass', 'committee:passed:unfavorable'),
    Rule(r'^Signed by Governor', 'governor:signed'),
)


class NDCategorizer(BaseCategorizer):
    rules = _categorizer_rules

########NEW FILE########
__FILENAME__ = bills
from collections import defaultdict
from urlparse import urljoin
from datetime import datetime
import lxml.html
from billy.scrape import NoDataForPeriod, ScrapeError
from billy.scrape.bills import Bill, BillScraper
from billy.scrape.votes import Vote
from .actions import NDCategorizer
import re

base_url = "http://www.legis.nd.gov/assembly/%s-%s/subject-index/major-topic.html"


class NDBillScraper(BillScraper):
    """
    Scrapes available legislative information from the website of the North
    Dakota legislature and stores it in the openstates  backend.
    """
    jurisdiction = 'nd'
    categorizer = NDCategorizer()

    def scrape_actions(self, session, subject, href, bid):
        page = self.urlopen(href)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(href)
        table = page.xpath(
            "//table[@summary='Bill Number Breakdown']"
        )

        if len(table) > 1:  # Pre-2013 pages.
            ttable, table = table
            ttrows = ttable.xpath(".//tr")
            descr = ttrows[-1]
        else:
            table = table[0]
            curnode = page.xpath("//div[@id='fastpath']")[0].getnext()
            ret = []
            while curnode.tag != "table":
                curnode = curnode.getnext()
                ret.append(curnode)
            ttrows = ret
            descr = page.xpath("//div[@class='section']//p")[-2]

        title = re.sub("\s+", " ", descr.text_content()).strip()
        ttrows = ttrows[:-1]

        chamber = {
            "H": "lower",
            "S": "upper"
        }[bid[0]]

        type_ = bid[1:3]
        bill_type = "bill"
        if type_.startswith("B"):
            bill_type = "bill"

        if type_.startswith("R"):
            bill_type = "resolution"

        if type_ == "CR":
            bill_type = "concurrent resolution"

        bill = Bill(session,
                    chamber,
                    bid,
                    title,
                    subject=subject,
                    type=bill_type)

        bill.add_source(href)

        for row in ttrows:
            sponsors = row.text_content().strip()
            sinf = re.match(
                "(?i)introduced by( (rep\.|sen\.))? (?P<sponsors>.*)",
                sponsors
            )
            if sinf:
                sponsors = sinf.groupdict()
                for sponsor in [
                    x.strip() for x in sponsors['sponsors'].split(",")
                ]:
                    bill.add_sponsor('primary',
                                     sponsor)


        dt = None
        oldchamber = 'other'
        for row in table.xpath(".//tr"):
            if row.text_content().strip() == '':
                continue

            if "Meeting Description" in [
                x.strip() for x in row.xpath(".//th/text()")
            ]:
                continue

            row = row.xpath("./*")
            row = [x.text_content().strip() for x in row]

            if len(row) > 3:
                row = row[:3]

            date, chamber, action = row

            try:
                chamber = {
                    "House": "lower",
                    "Senate": "upper"
                }[chamber]
                oldchamber = chamber
            except KeyError:
                chamber = oldchamber

            if date != '':
                dt = datetime.strptime("%s %s" % (date, self.year), "%m/%d %Y")

            kwargs = self.categorizer.categorize(action)

            bill.add_action(chamber, action, dt, **kwargs)

        version_url = page.xpath("//a[contains(text(), 'Versions')]")
        if len(version_url) == 1:
            href = version_url[0].attrib['href']
            bill = self.scrape_versions(bill, href)

        self.save_bill(bill)

    def scrape_versions(self, bill, href):
        page = self.urlopen(href)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(href)
        versions = page.xpath("//a[contains(@href, '/documents/')]")
        for version in versions:
            name, href = version.text, version.attrib['href']
            bill.add_version(name, href, mimetype='application/pdf')

        return bill

    def scrape_subject(self, session, href, subject):
        page = self.urlopen(href)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(href)
        bills = page.xpath("//a[contains(@href, 'bill-actions')]")
        for bill in bills:
            bt = bill.text_content()
            typ, idd, _, = bt.split()
            bid = "%s %s" % (typ, idd)
            self.scrape_actions(session, subject, bill.attrib['href'], bid)

    def scrape(self, term, chambers):
        # figuring out starting year from metadata
        for t in self.metadata['terms']:
            if t['name'] == term:
                start_year = t['start_year']
                self.year = start_year
                break

        url = base_url % (term, start_year)
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        subjects = page.xpath(
            "//div[@id='application']"
            "//a[not(contains(@href, 'major-topic'))]"
        )
        for subject in subjects:
            subject_name = subject.xpath("text()")
            if subject_name == [] \
               or subject_name[0].strip() == '' \
               or 'href' not in subject.attrib:
                continue

            href = subject.attrib['href']
            self.scrape_subject(term, href, subject.text.strip())

########NEW FILE########
__FILENAME__ = committees
from billy.scrape import NoDataForPeriod
from billy.scrape.committees import CommitteeScraper, Committee
import lxml.html
import re

class NDCommitteeScraper(CommitteeScraper):
    jurisdiction = 'nd'

    def scrape_committee(self, term, chambers, href, name):
        page = self.urlopen(href)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(href)
        members =page.xpath("//div[@class='view-content']"
                            "//a[contains(@href, 'members')]")

        if '/joint/' in href:
            chamber = 'joint'
        elif '/senate/' in href:
            chamber = 'upper'
        elif '/house/' in href:
            chamber = 'lower'
        else:
            print "XXX: Fail! %s" % (href)
            return

        cttie = Committee(chamber, name)

        for a in members:
            member = a.text
            role = a.xpath("ancestor::div/h2[@class='pane-title']/text()")[0]
            role = {"Legislative Members": "member",
                    "Chairman": "chair",
                    "Vice Chairman": "member"}[role]

            if member is None or member.startswith("District"):
                continue

            cttie.add_member(member, role=role)

        cttie.add_source(href)
        self.save_committee(cttie)

    def scrape(self, term, chambers):
        self.validate_term(term, latest_only=True)

        # figuring out starting year from metadata
        for t in self.metadata['terms']:
            if t['name'] == term:
                start_year = t['start_year']
                break

        root = "http://www.legis.nd.gov/assembly"
        main_url = "%s/%s-%s/committees" % (
            root,
            term,
            start_year
        )

        page = self.urlopen(main_url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(main_url)

        ctties = page.xpath("//div[@class='inside']")[0]
        for a in ctties.xpath(".//a[contains(@href, 'committees')]"):
            self.scrape_committee(term, chambers, a.attrib['href'], a.text)

########NEW FILE########
__FILENAME__ = legislators
from billy.scrape.legislators import Legislator, LegislatorScraper
from billy.scrape import NoDataForPeriod
import lxml.html
import logging
import re

logger = logging.getLogger('openstates')

class NDLegislatorScraper(LegislatorScraper):
    jurisdiction = 'nd'

    def scrape(self, term, chambers):
        self.validate_term(term, latest_only=True)

        # figuring out starting year from metadata
        for t in self.metadata['terms']:
            if t['name'] == term:
                start_year = t['start_year']
                break

        root = "http://www.legis.nd.gov/assembly"
        main_url = "%s/%s-%s/members/members-by-district" % (
            root,
            term,
            start_year
        )

        page = self.urlopen(main_url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(main_url)
        for person in page.xpath("//div[contains(@class, 'all-members')]//a"):
            self.scrape_legislator_page(term, person.attrib['href'])


    def scrape_legislator_page(self, term, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        name = page.xpath("//h1[@id='page-title']/text()")[0]
        district = page.xpath("//a[contains(@href, 'district')]/text()")[0]
        district = district.replace("District", "").strip()

        committees = page.xpath("//a[contains(@href, 'committees')]/text()")

        party = page.xpath(
            "//div[contains(text(), 'Political Party')]"
        )[0].getnext().text_content().strip()

        photo = page.xpath(
            "//div[@class='field-person-photo']/img/@src"
        )
        photo = photo[0] if len(photo) else None

        address = page.xpath("//div[@class='adr']")[0]
        address = re.sub("\s+", " ", address.text_content()).strip()

        item_mapping = {
            "email": "email",
            "home telephone": "home-telephone",
            "cellphone": "cellphone",
            "office telephone": "office-telephone",
            "political party": "party",
            "chamber": "chamber",
            "fax": "fax"
        }
        metainf = {}

        for block in page.xpath("//div[contains(@class, 'field-label-inline')]"):
            label, items = block.xpath("./*")
            key = label.text_content().strip().lower()
            if key.endswith(":"):
                key = key[:-1]

            metainf[item_mapping[key]] = items.text_content().strip()

        chamber = {
            "Senate": "upper",
            "House": "lower"
        }[metainf['chamber']]

        kwargs = {
            "party": {"Democrat": "Democratic",
                      "Republican": "Republican"}[metainf['party']]
        }
        if photo:
            kwargs['photo_url'] = photo

        leg = Legislator(term,
                         chamber,
                         district,
                         name,
                         **kwargs)

        kwargs = {
            "address": address,
            "url": url
        }

        for key, leg_key in [
            ('email', 'email'),
            ('home-telephone', 'home_phone'),
            ('cellphone', 'cellphone'),
            ('fax', 'fax'),
            ('office-telephone', 'office_phone'),
        ]:
            if key in metainf:
                kwargs[leg_key] = metainf[key]


        leg.add_office('district',
                       'District Office',
                       **kwargs)

        #for committee in committees:
        #    leg.add_role('committee member',
        #                 term=term,
        #                 chamber=chamber,
        #                 committee=committee)

        leg.add_source(url)
        self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = votes
import requests.exceptions
from billy.scrape.votes import VoteScraper, Vote
from billy.scrape.utils import convert_pdf
import datetime
import subprocess
import lxml
import os
import re

fin_re = r"(?i).*(?P<bill_id>(S|H|J)(B|R|M) \d+).*(?P<passfail>(passed|lost)).*"
date_re = r".*(?P<date>(MONDAY|TUESDAY|WEDNESDAY|THURSDAY|FRIDAY|SATURDAY|SUNDAY), .*\d{1,2},\s\d{4}).*"

class NDVoteScraper(VoteScraper):
    jurisdiction = 'nd'

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page


    def scrape(self, chamber, session):
        chamber_name = 'senate' if chamber == 'lower' else 'house'
        session_slug = {'62': '62-2011', '63': '63-2013'}[session]

        url = "http://www.legis.nd.gov/assembly/%s/journals/%s-journal.html" % (
            session_slug, chamber_name)
        page = self.lxmlize(url)
        pdfs = page.xpath("//a[contains(@href, '.pdf')]")
        for pdf in pdfs:

            bill_id = None
            results = {}
            in_vote = False
            cur_date = None
            in_motion = False
            cur_vote = None
            in_vote = False
            cur_motion = ""

            pdf_url = pdf.attrib['href']

            try:
                (path, response) = self.urlretrieve(pdf_url)
            except requests.exceptions.ConnectionError:
                continue

            data = convert_pdf(path, type='text')
            os.unlink(path)
            lines = data.splitlines()
            for line in lines:
                date = re.findall(date_re, line)
                if date != [] and not cur_date:
                    date = date[0][0]
                    cur_date = datetime.datetime.strptime(date, "%A, %B %d, %Y")

                if line.strip() == "":
                    in_motion = False
                    continue

                if True in [x in line.lower() for x in ['passed', 'lost']] and in_vote:
                    in_vote = False
                    bills = re.findall(r"(?i)(H|S|J)(B|R|M) (\d+)", line)
                    if bills == [] or cur_motion.strip() == "":
                        bill_id = None
                        results = {}
                        in_vote = False
                        in_motion = False
                        cur_vote = None
                        in_vote = False
                        continue

                    print "CM: ", cur_motion

                    cur_bill_id = "%s%s %s" % (bills[-1])
                    keys = {
                        "YEAS": "yes",
                        "NAYS": "no",
                        "ABSENT AND NOT VOTING": "other"
                    }
                    res = {}
                    for key in keys:
                        if key in results:
                            res[keys[key]] = filter(lambda a: a != "",
                                                    results[key])
                        else:
                            res[keys[key]] = []

                    # results
                    results = {}
                    yes, no, other = len(res['yes']), len(res['no']), \
                                        len(res['other'])
                    chambers = {
                        "H": "lower",
                        "S": "upper",
                        "J": "joint"
                    }
                    try:
                        bc = chambers[cur_bill_id[0]]
                    except KeyError:
                        bc = 'other'

                    if cur_date is None:
                        self.warning("Cur-date is None. Passing.")
                        continue

                    vote = Vote(chamber,
                                cur_date,
                                cur_motion,
                                (yes > no),
                                yes,
                                no,
                                other,
                                session=session,
                                bill_id=cur_bill_id,
                                bill_chamber=bc)

                    vote.add_source(pdf_url)
                    vote.add_source(url)

                    for key in res:
                        obj = getattr(vote, key)
                        for person in res[key]:
                            obj(person)

                    self.save_vote(vote)


                    bill_id = None
                    results = {}
                    in_vote = False
                    in_motion = False
                    cur_vote = None
                    in_vote = False
                    cur_motion = ""

                    # print bills
                    # print "VOTE TAKEN"

                if 'VOTES FOR' in line:
                    in_motion = False
                    in_vote = False
                    continue

                if 'ABSET' in line:
                    if in_motion:
                        in_vote = True
                    in_motion = False

                if ":" in line and in_vote:
                    cur_vote, who = line.split(":", 1)
                    who = [x.strip() for x in who.split(';')]
                    results[cur_vote] = who
                    continue

                if in_vote:
                    if cur_vote is None:
                        continue

                    who = [x.strip() for x in line.split(";")]
                    for person in who:
                        # print cur_vote
                        results[cur_vote].append(person)
                    continue

                if "question being" in line:
                    cur_motion = line.strip()
                    in_motion = True
                    continue

                if in_motion:
                    cur_motion += line.strip()
                    continue

                if line.strip() == 'ROLL CALL':
                    in_vote = True

########NEW FILE########
__FILENAME__ = bills
from billy.scrape.bills import BillScraper, Bill
from datetime import datetime
import lxml.html
import urllib

class NEBillScraper(BillScraper):
    jurisdiction = 'ne'

    def scrape(self, session, chambers):
        start_year = self.metadata['session_details'][session]['start_date'].year
        end_year = self.metadata['session_details'][session]['end_date'].year
        self.scrape_year(session, start_year)
        if start_year != end_year:
            self.scrape_year(session, end_year)

    def scrape_year(self, session, year):
        main_url = 'http://nebraskalegislature.gov/bills/search_by_date.php?SessionDay=%s' % year
        page = self.urlopen(main_url)
        page = lxml.html.fromstring(page)

        for docs in page.xpath('//div[@class="cal_content_full"]/table[@id="bill_results"]/tr/td[1]/a'):
            bill_abbr = docs.text

            #POST request for search form
            post_dict = {'DocumentNumber': bill_abbr, 'Legislature': session}
            #headers = urllib.urlencode(post_dict)
            bill_page = self.urlopen( 'http://nebraskalegislature.gov/bills/search_by_number.php',
                                     method="POST", body=post_dict)
            bill_link = bill_page.response.url

            #scrapes info from bill page
            self.bill_info(bill_link, session, main_url, bill_page)

    #Scrapes info from the bill page
    def bill_info(self, bill_link, session, main_url, bill_page):

        bill_page = lxml.html.fromstring(bill_page)

        #basic info
        try:
            long_title = bill_page.xpath('//div[@id="content_text"]/h2')[0].text.split()
        except IndexError:
            return None
        bill_id = long_title[0]
        title = ''
        for x in range(2, len(long_title)):
            title += long_title[x] + ' '
        title = title[0:-1]

        if not title:
            self.error('no title, skipping %s', bill_id)
            return

        #bill_type
        bill_type = 'resolution' if 'LR' in bill_id else 'bill'

        bill = Bill(session, 'upper', bill_id, title, type = bill_type)

        #sources
        bill.add_source(main_url)
        bill.add_source(bill_link)

        #Sponsor
        introduced_by = bill_page.xpath('//div[@id="content_text"]/div[2]/table/tr[2]/td[1]/a[1]')[0].text
        bill.add_sponsor('primary', introduced_by)

        #actions
        for actions in bill_page.xpath('//div[@id="content_text"]/div[3]/table/tr[1]/td[1]/table/tr'):
            date = actions[0].text
            if 'Date' not in date:
                date = datetime.strptime(date, '%b %d, %Y')
                action = actions[1].text

                if 'Governor' in action:
                    actor = 'Governor'
                elif 'Speaker' in action:
                    actor = 'Speaker'
                else:
                    actor = 'upper'

                action_type = self.action_types(action)
                bill.add_action(actor, action, date, action_type)

        # were in reverse chronological order
        bill['actions'].reverse()

        #versions
        for versions in bill_page.xpath('//div[@id="content_text"]/div[2]/table/tr[2]/td[2]/a'):
            version_url = versions.attrib['href']
            version_url = 'http://nebraskalegislature.gov/' + version_url[3:len(version_url)]
            version_name = versions.text
            # replace Current w/ session number
            version_url = version_url.replace('Current', session)
            bill.add_version(version_name, version_url,
                             mimetype='application/pdf')


        #documents
        # this appear to be same as versions, dropped for now
        #for additional_info in bill_page.xpath('//div[@id="content_text"]/div[2]/table/tr[2]/td/a'):
        #    document_name = additional_info.text
        #    document_url = additional_info.attrib['href']
        #    document_url = 'http://nebraskalegislature.gov/' + document_url[3:len(document_url)]
        #    if '.pdf' in document_url:
        #        bill.add_document(document_name, document_url)

        #amendments
        for admendments in bill_page.xpath('//div[@id="content_text"]/div[3]/table/tr[1]/td[2]/table/tr/td/a'):
            admendment_name = admendments.text
            admendment_url = admendments.attrib['href']
            admendment_url = 'http://nebraskalegislature.gov/' + admendment_url[3:len(admendment_url)]
            bill.add_document(admendment_name, admendment_url)

        #related transcripts
        for transcripts in bill_page.xpath('//div[@id="content_text"]/div[3]/table/tr[2]/td[2]/a'):
            transcript_name = transcripts.text
            transcript_url = transcripts.attrib['href']
            bill.add_document(transcript_name, transcript_url)

        self.save_bill(bill)


    #Setting action types
    def action_types(self, action):

        if 'Date of introduction' in action:
            action_type = 'bill:introduced'
        elif 'Referred to' in action:
            action_type = 'committee:referred'
        elif 'Indefinitely postponed' in action:
            action_type = 'committee:failed'
        elif ('File' in action) or ('filed' in action):
            action_type = 'bill:filed'
        elif 'Placed on Final Reading' in action:
            action_type = 'bill:reading:3'
        elif 'Passed' in action or 'President/Speaker signed' in action:
            action_type = 'bill:passed'
        elif 'Presented to Governor' in action:
            action_type = 'governor:received'
        elif 'Approved by Governor' in action:
            action_type = 'governor:signed'
        elif 'Failed to pass notwithstanding the objections of the Governor' in action:
            action_type = 'governor:vetoed'
        elif 'Failed' in action:
            action_type = 'bill:failed'
        else:
            action_type = ''
        return action_type

########NEW FILE########
__FILENAME__ = committees
from billy.scrape import NoDataForPeriod
from billy.scrape.committees import CommitteeScraper, Committee

import lxml.html


class NECommitteeScraper(CommitteeScraper):
    jurisdiction = 'ne'
    latest_only = True

    def scrape(self, term, chambers):
        self.standing_comm()
        self.select_special_comm()

    def select_special_comm(self):
        main_url = 'http://www.nebraskalegislature.gov/committees/select-committees.php'
        page = self.urlopen(main_url)
        page = lxml.html.fromstring(page)

        for comm_names in page.xpath('//div[@class="content_box"]'):
           name = comm_names.xpath('h2')[0].text
           if name != None:
               committee = Committee('upper', name)
               committee.add_source(main_url)
               for senators in comm_names.xpath('ul[@class="nobullet"]/li'):
                   senator = senators[0].text
                   if 'Chairperson' in senator:
                       role = 'Chairperson'
                       senator = senator[5:-13].strip()
                   else:
                       role = 'member'
                       senator = senator[5:].strip()
                   committee.add_member(senator, role)
           else:
               name = comm_names.xpath('h2/a')[0].text
               committee = Committee('upper', name)
               committee.add_source(main_url)
               for senators in comm_names.xpath('ul[@class="nobullet"]/li'):
                   senator = senators[0].text
                   if 'Chairperson' in senator:
                       role = 'chairperson'
                       senator = senator[5:-13].strip()
                   else:
                       role = 'member'
                       senator = senator[5:].strip()
                   committee.add_member(senator, role)

           if not committee['members']:
               self.warning('no members in %s', committee['committee'])
           else:
               self.save_committee(committee)



    def standing_comm(self):
       main_url = 'http://www.nebraskalegislature.gov/committees/standing-committees.php'
       page = self.urlopen(main_url)
       page = lxml.html.fromstring(page)
       
       for comm_links in page.xpath('//div[@id="content_text"]/div[@class="content_box_container"]/div[@class="content_box"][1]/ul[@class="nobullet"]/li/a'):
           detail_link = comm_links.attrib['href']

           detail_page =  self.urlopen(detail_link)
           detail_page = lxml.html.fromstring(detail_page)
           name = detail_page.xpath('//div[@id="content"]/div[@class="content_header"]/div[@class="content_header_right"]/a')[0].text
           name = name.split()
           name = name[0:-1]
           comm_name = ''
           for x in range(len(name)):
               comm_name += name[x] + ' '
           comm_name = comm_name[0: -1]
           committee = Committee('upper', comm_name)

           for senators in detail_page.xpath('//div[@id="sidebar"]/ul[1]/li[1]/ul/li/a'):
               senator = senators.text
               if 'Chairperson' in senator:
                   role = 'Chairperson'
                   senator = senator[6:-13].strip()
               else:
                    role = 'member'
                    senator = senator[6:].strip()
               committee.add_member(senator, role)
           committee.add_source(main_url)
           committee.add_source(detail_link)
           self.save_committee(committee)

########NEW FILE########
__FILENAME__ = legislators
from billy.scrape.legislators import Legislator, LegislatorScraper
import lxml.html
import scrapelib

class NELegislatorScraper(LegislatorScraper):
    jurisdiction = 'ne'
    latest_only = True

    def scrape(self, term, chambers):
        base_url = 'http://news.legislature.ne.gov/dist'

        #there are 49 districts
        for district in range(1, 50):
            if district < 10:
                rep_url = base_url + '0' + str(district) + '/biography/'
            else:
                rep_url = base_url + str(district) + '/biography/'

            try:
                html = self.urlopen(rep_url)
                page = lxml.html.fromstring(html)

                full_name = page.xpath('//div[@class="content_header_right"]/a')[0].text.split(' ',1)[1].strip()
                # This is hacky, are lis always the same?
                address = page.xpath('//div[@id="sidebar"]/ul[1]/li[3]')[0].text.strip() + '\n'
                address += page.xpath('//div[@id="sidebar"]/ul[1]/li[4]')[0].text.strip() + '\n'
                address += page.xpath('//div[@id="sidebar"]/ul[1]/li[5]')[0].text.strip()
                phone = page.xpath('//div[@id="sidebar"]/ul[1]/li[6]')[0].text.split()
                if len(phone) > 2:
                    phone = phone[1] + ' ' + phone[2]
                else:
                    phone = None
                email = page.xpath('//div[@id="sidebar"]/ul[1]/li[7]/a')[0].text or ''

                #Nebraska is offically nonpartisan
                party = 'Nonpartisan'
                leg = Legislator(term, 'upper', str(district), full_name,
                                 party=party, email=email, url=rep_url)
                leg.add_source(rep_url)
                leg.add_office('capitol', 'Capitol Office', address=address,
                               phone=phone)
                self.save_legislator(leg)
            except scrapelib.HTTPError:
                self.warning('could not retrieve %s' % rep_url)


########NEW FILE########
__FILENAME__ = votes
import os
import re
import datetime

from billy.scrape.votes import VoteScraper, Vote
from billy.scrape.utils import convert_pdf

BILL_RE = re.compile('^LEGISLATIVE (BILL|RESOLUTION) (\d+C?A?).')
VETO_BILL_RE = re.compile('MOTION - Override (?:Line-Item )?Veto on (\w+)')
DATE_RE = re.compile('(JANUARY|FEBRUARY|MARCH|APRIL|MAY|JUNE|JULY|AUGUST|SEPTEMBER|OCTOBER|NOVEMBER|DECEMBER) (\d+), (\d{4})')
QUESTION_RE = re.compile("(?:the question is, '|The question shall be, ')(.+)")
QUESTION_MATCH_END = "' \""
YES_RE = re.compile('Voting in the affirmative, (\d+)')
NO_RE = re.compile('Voting in the negative, (\d+)')
NOT_VOTING_RE = re.compile('(?:Present|Absent|Excused)?(?: and )?[Nn]ot voting, (\d+)')


class NEVoteScraper(VoteScraper):
    jurisdiction = 'ne'

    def scrape(self, session, chambers):
        urls = {'103': ['http://www.nebraskalegislature.gov/FloorDocs/Current/PDF/Journal/r1journal.pdf',]
               }
        for url in urls[session]:
            self.scrape_journal(session, url)

    def scrape_journal(self, session, url):
        journal, resp = self.urlretrieve(url)
        text = convert_pdf(journal, type='text')
        lines = text.splitlines()

        #  state machine:
        #      None - undefined state
        #      question_quote - in question, looking for end quote
        #      pre-yes - vote is active, haven't hit yes votes yet
        #      yes     - yes votes
        #      no      - no votes
        #      other   - other votes
        state = None
        vote = None

        for line_num, line in enumerate(lines):
            date_match = DATE_RE.findall(line)

            # skip headers
            if 'LEGISLATIVE JOURNAL' in line:
                continue

            elif date_match:
                date = datetime.datetime.strptime(' '.join(date_match[0]),
                                                  '%B %d %Y')
                continue

            # keep adding lines to question while quotes are open
            elif state == 'question_quote':
                question += ' %s' % line

            elif state in ('pre-yes', 'yes', 'no', 'other'):
                yes_match = YES_RE.match(line)
                no_match = NO_RE.match(line)
                other_match = NOT_VOTING_RE.match(line)
                if yes_match:
                    vote['yes_count'] = int(yes_match.group(1))
                    state = 'yes'
                elif no_match:
                    vote['no_count'] = int(no_match.group(1))
                    state = 'no'
                elif other_match:
                    vote['other_count'] += int(other_match.group(1))
                    state = 'other'
                elif 'having voted in the affirmative' in line:
                    vote['passed'] = True
                    state = None
                    vote.validate()
                    self.save_vote(vote)
                    vote = None
                elif 'Having failed' in line:
                    vote['passed'] = False
                    state = None
                    vote.validate()
                    self.save_vote(vote)
                    vote = None
                elif line:
                    people = re.split('\s{3,}', line)
                    #try:
                    func = {'yes': vote.yes, 'no': vote.no,
                            'other': vote.other}[state]
                    #except KeyError:
                        #self.warning('line showed up in pre-yes state: %s',
                        #             line)
                    for p in people:
                        if p:
                            # special case for long name w/ 1 space
                            if p.startswith(('Lautenbaugh ', 'Langemeier ')):
                                p1, p2 = p.split(' ', 1)
                                func(p1)
                                func(p2)
                            else:
                                func(p)

            # check the text against our regexes
            bill_match = BILL_RE.match(line)
            veto_match = VETO_BILL_RE.findall(line)
            question_match = QUESTION_RE.findall(line)
            if bill_match:
                bill_type, bill_id = bill_match.groups()
                if bill_type == 'BILL':
                    bill_id = 'LB ' + bill_id
                elif bill_type == 'RESOLUTION':
                    bill_id = 'LR ' + bill_id
            elif question_match:
                question = question_match[0]
                state = 'question_quote'
            elif veto_match:
                bill_id = veto_match[0]

            # line just finished a question
            if state == 'question_quote' and QUESTION_MATCH_END in question:
                question = re.sub('\s+', ' ',
                              question.replace(QUESTION_MATCH_END, '').strip())
                # save prior vote
                vote = Vote(bill_id=bill_id, session=session,
                            bill_chamber='upper', chamber='upper',
                            motion=question, type='passage', passed=False,
                            date=date, yes_count=0, no_count=0, other_count=0)
                vote.add_source(url)
                state = 'pre-yes'
                # reset bill_id and question
                bill_id = question = None

########NEW FILE########
__FILENAME__ = bills
import os
import re
import zipfile
import datetime as dt

from billy.scrape.bills import Bill, BillScraper
from billy.scrape.votes import Vote


body_code = {'lower': 'H', 'upper': 'S'}
bill_type_map = {'B': 'bill',
                 'R': 'resolution',
                 'CR': 'concurrent resolution',
                 'JR': 'joint resolution',
                 'CO': 'concurrent order'
                }
action_classifiers = [
    ('Ought to Pass', ['bill:passed']),
    ('Passed by Third Reading', ['bill:reading:3', 'bill:passed']),
    ('.*Ought to Pass', ['committee:passed:favorable']),
    ('Introduced(.*) and (R|r)eferred', ['bill:introduced', 'committee:referred']),
    ('.*Inexpedient to Legislate', ['committee:passed:unfavorable']),
    ('Proposed(.*) Amendment', 'amendment:introduced'),
    ('Amendment .* Adopted', 'amendment:passed'),
    ('Amendment .* Failed', 'amendment:failed'),
    ('Signed', 'governor:signed'),
    ('Vetoed', 'governor:vetoed'),
]
VERSION_URL = 'http://www.gencourt.state.nh.us/legislation/%s/%s.html'
AMENDMENT_URL = 'http://www.gencourt.state.nh.us/legislation/amendments/%s.html'


def classify_action(action):
    for regex, classification in action_classifiers:
        if re.match(regex, action):
            return classification
    return 'other'


def extract_amendment_id(action):
    piece = re.findall('Amendment #(\d{4}-\d+[hs])', action)
    if piece:
        return piece[0]


class NHBillScraper(BillScraper):
    jurisdiction = 'nh'

    def scrape(self, chamber, session):
        zip_url = self.metadata['session_details'][session]['zip_url']

        fname, resp = self.urlretrieve(zip_url)
        self.zf = zipfile.ZipFile(open(fname))
        os.remove(fname)

        # bill basics
        self.bills = {}         # LSR->Bill
        self.bills_by_id = {}   # need a second table to attach votes
        last_line = []
        for line in self.zf.open('tbllsrs.txt').readlines():
            line = line.split('|')
            if len(line) < 36:
                if len(last_line + line[1:]) == 36:
                    # combine two lines for processing
                    # (skip an empty entry at beginning of second line)
                    line = last_line + line
                    self.warning('used bad line')
                else:
                    # skip this line, maybe we'll use it later
                    self.warning('bad line: %s' % '|'.join(line))
                    last_line = line
                    continue
            session_yr = line[0]
            lsr = line[1]
            title = line[2]
            body = line[3]
            type_num = line[4]
            expanded_bill_id = line[9]
            bill_id = line[10]

            if body == body_code[chamber] and session_yr == session:
                if expanded_bill_id.startswith('CACR'):
                    bill_type = 'constitutional amendment'
                elif expanded_bill_id.startswith('PET'):
                    bill_type = 'petition'
                else:
                    bill_type = bill_type_map[expanded_bill_id.split(' ')[0][1:]]

                if title.startswith('('):
                    title = title.split(')', 1)[1].strip()

                self.bills[lsr] = Bill(session, chamber, bill_id, title,
                                       type=bill_type)
                version_url = VERSION_URL % (session,
                                             expanded_bill_id.replace(' ', ''))
                self.bills[lsr].add_version('latest version', version_url,
                                            mimetype='text/html')
                self.bills_by_id[bill_id] = self.bills[lsr]

        # load legislators
        self.legislators = {}
        for line in self.zf.open('tbllegislators.txt').readlines():
            line = line.split('|')
            employee_num = line[0]

            # first, last, middle
            if line[3]:
                name = '%s %s %s' % (line[2], line[3], line[1])
            else:
                name = '%s %s' % (line[2], line[1])

            self.legislators[employee_num] = {'name': name,
                                              'seat': line[5]}
            #body = line[4]

        # sponsors
        for line in self.zf.open('tbllsrsponsors.txt').readlines():
            session_yr, lsr, seq, employee, primary = line.strip().split('|')

            if session_yr == session and lsr in self.bills:
                sp_type = 'primary' if primary == '1' else 'cosponsor'
                try:
                    self.bills[lsr].add_sponsor(sp_type,
                                        self.legislators[employee]['name'],
                                        _code=self.legislators[employee]['seat'])
                except KeyError:
                    self.warning("Error, can't find person %s" % employee)


        # actions
        for line in self.zf.open('tbldocket.txt').readlines():
            # a few blank/irregular lines, irritating
            if '|' not in line:
                continue

            (session_yr, lsr, _, timestamp, bill_id, body,
             action, _) = line.split('|')

            if session_yr == session and lsr in self.bills:
                actor = 'lower' if body == 'H' else 'upper'
                time = dt.datetime.strptime(timestamp,
                                                  '%m/%d/%Y %H:%M:%S %p')
                action = action.strip()
                atype = classify_action(action)
                self.bills[lsr].add_action(actor, action, time, type=atype)
                amendment_id = extract_amendment_id(action)
                if amendment_id:
                    self.bills[lsr].add_document('amendment %s' % amendment_id,
                                                 AMENDMENT_URL % amendment_id)

        self.scrape_votes(session)

        # save all bills
        for bill in self.bills.values():
            bill.add_source(zip_url)
            self.save_bill(bill)


    def scrape_votes(self, session):
        votes = {}
        last_line = []

        for line in self.zf.open('tblrollcallsummary.txt'):
            if line.strip() == "":
                continue

            line = line.split('|')
            if len(line) < 14:
                if len(last_line + line[1:]) == 14:
                    line = last_line
                    self.warning('used bad vote line')
                else:
                    last_line = line
                    self.warning('bad vote line %s' % '|'.join(line))
            session_yr = line[0]
            body = line[1]
            vote_num = line[2]
            timestamp = line[3]
            bill_id = line[4].strip()
            yeas = int(line[5])
            nays = int(line[6])
            present = int(line[7])
            absent = int(line[8])
            motion = line[11].strip() or '[not available]'

            if session_yr == session and bill_id in self.bills_by_id:
                actor = 'lower' if body == 'H' else 'upper'
                time = dt.datetime.strptime(timestamp,
                                                  '%m/%d/%Y %I:%M:%S %p')
                # TODO: stop faking passed somehow
                passed = yeas > nays
                vote = Vote(actor, time, motion, passed, yeas, nays,
                            other_count=0)
                votes[body+vote_num] = vote
                self.bills_by_id[bill_id].add_vote(vote)

        for line in self.zf.open('tblrollcallhistory.txt'):
            # 2012    | H   | 2    | 330795  | HB309  | Yea |1/4/2012 8:27:03 PM
            session_yr, body, v_num, employee, bill_id, vote, date \
                    = line.split('|')

            if not bill_id:
                continue

            if session_yr == session and bill_id.strip() in self.bills_by_id:
                try:
                    leg = self.legislators[employee]['name']
                except KeyError:
                    self.warning("Error, can't find person %s" % employee)
                    continue

                vote = vote.strip()
                if not body+v_num in votes:
                    self.warning("Skipping processing this vote:")
                    self.warning("Bad ID: %s" % ( body+v_num ) )
                    continue

                #code = self.legislators[employee]['seat']
                if vote == 'Yea':
                    votes[body+v_num].yes(leg)
                elif vote == 'Nay':
                    votes[body+v_num].no(leg)
                else:
                    votes[body+v_num].other(leg)
                    votes[body+v_num]['other_count'] += 1

########NEW FILE########
__FILENAME__ = legislators
import re

from billy.scrape.legislators import LegislatorScraper, Legislator
import lxml.html

chamber_map = {'House': 'lower', 'Senate': 'upper'}
party_map = {'d': 'Democratic', 'r': 'Republican', 'i': 'Independent',
             # see wikipedia http://en.wikipedia.org/wiki/New_Hampshire_House_of_Representatives
             # Coulombe & Wall are listed as D+R
             'd+r': 'Democratic'}


class NHLegislatorScraper(LegislatorScraper):
    jurisdiction = 'nh'
    latest_only = True

    def get_photo(self, url, chamber):
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        if chamber == 'lower':
            src = doc.xpath('//img[contains(@src, "images/memberpics")]/@src')
        else:
            src = doc.xpath('//img[contains(@src, "images/senators")]/@src')
        if src and 'nophoto' not in src[0]:
            return src[0]
        return ''

    def scrape(self, term, chambers):
        url = 'http://gencourt.state.nh.us/downloads/Members.txt'

        option_map = {}
        html = self.urlopen('http://www.gencourt.state.nh.us/house/members/memberlookup.aspx')
        doc = lxml.html.fromstring(html)
        for opt in doc.xpath('//option'):
            option_map[opt.text] = opt.get('value')

        data = self.urlopen(url)
        for line in data.splitlines():
            if line.strip() == "":
                continue

            (chamber, fullname, last, first, middle, county, district_num,
             seat, party, street, street2, city, astate, zipcode,
             home_phone, office_phone, fax, email, com1, com2, com3,
             com4, com5) = line.split('\t')

            chamber = chamber_map[chamber]

            # skip legislators from a chamber we aren't scraping
            if chamber not in chambers:
                continue

            if middle:
                full = '%s %s %s' % (first, middle, last)
            else:
                full = '%s %s' % (first, last)

            address = street
            if street2:
                address += (' ' + street2)
            address += '\n%s, %s %s' % (city, astate, zipcode)

            district = str(int(district_num))
            if county:
                district = '%s %s' % (county, district)

            leg = Legislator(term, chamber, district, full, first, last,
                             middle, party_map[party], email=email)
            leg.add_office('district', 'Home Address',
                           address=address, phone=home_phone or None)
            leg.add_office('district', 'Office Address',
                           phone=office_phone or None, fax=fax or None)

            if chamber == 'upper':
                leg['url'] = 'http://www.gencourt.state.nh.us/Senate/members/webpages/district%02d.aspx' % int(district_num)
            elif chamber == 'lower':
                code = option_map.get('{0}, {1}'.format(last, first))
                if code:
                    leg['url'] = 'http://www.gencourt.state.nh.us/house/members/member.aspx?member=' + code

            romans = r'(?i)\s([IXV]+)(?:\s|$)'
            for com in (com1, com2, com3, com4, com5):
                com = com.strip('"')
                if com:
                    com_name = com.title()
                    com_name = re.sub(romans, lambda m: m.group().upper(),
                                      com_name)
                    leg.add_role('committee member', term=term,
                                  chamber=chamber, committee=com_name)

            if 'url' in leg:
                leg['photo_url'] = self.get_photo(leg['url'], chamber)

            leg.add_source(url)
            self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = bills
from datetime import datetime
from .utils import chamber_name, MDBMixin
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

import scrapelib
import zipfile
import csv
import os

class NJBillScraper(BillScraper, MDBMixin):
    jurisdiction = 'nj'

    _bill_types = {
        '': 'bill',
        'R': 'resolution',
        'JR': 'joint resolution',
        'CR': 'concurrent resolution',
    }

    _actions = {
        'INT 1RA AWR 2RA': ('Introduced, 1st Reading without Reference, 2nd Reading', 'bill:introduced'),
        'INT 1RS SWR 2RS': ('Introduced, 1st Reading without Reference, 2nd Reading', 'bill:introduced'),
        'REP 2RA': ('Reported out of Assembly Committee, 2nd Reading', 'committee:passed'),
        'REP 2RS': ('Reported out of Senate Committee, 2nd Reading', 'committee:passed'),
        'REP/ACA 2RA': ('Reported out of Assembly Committee with Amendments, 2nd Reading', 'committee:passed'),
        'REP/SCA 2RS': ('Reported out of Senate Committee with Amendments, 2nd Reading', 'committee:passed'),
        'R/S SWR 2RS': ('Received in the Senate without Reference, 2nd Reading', 'other'),
        'R/A AWR 2RA': ('Received in the Assembly without Reference, 2nd Reading', 'other'),
        'R/A 2RAC': ('Received in the Assembly, 2nd Reading on Concurrence', 'other'),
        'R/S 2RSC': ('Received in the Senate, 2nd Reading on Concurrence', 'other'),
        'REP/ACS 2RA': ('Reported from Assembly Committee as a Substitute, 2nd Reading', 'other'),
        'REP/SCS 2RS': ('Reported from Senate Committee as a Substitute, 2nd Reading', 'other'),
        'AA 2RA': ('Assembly Floor Amendment Passed', 'amendment:passed'),
        'SA 2RS': ('Senate Amendment', 'amendment:passed'),
        'SUTC REVIEWED': ('Reviewed by the Sales Tax Review Commission', 'other'),
        'PHBC REVIEWED': ('Reviewed by the Pension and Health Benefits Commission', 'other'),
        'SUB FOR': ('Substituted for', 'other'),
        'SUB BY': ('Substituted by', 'other'),
        'PA': ('Passed Assembly', 'bill:passed'),
        'PS': ('Passed Senate', 'bill:passed'),
        'PA PBH': ('Passed Assembly (Passed Both Houses)', 'bill:passed'),
        'PS PBH': ('Passed Senate (Passed Both Houses)', 'bill:passed'),
        'APP': ('Approved', 'governor:signed'),
        'APP W/LIV': ('Approved with Line Item Veto', ['governor:signed', 'governor:vetoed:line-item']),
        'AV R/A': ('Absolute Veto, Received in the Assembly', 'governor:vetoed'),
        'AV R/S': ('Absolute Veto, Received in the Senate', 'governor:vetoed'),
        'CV R/A': ('Conditional Veto, Received in the Assembly', 'governor:vetoed'),
        'CV R/A 1RAG': ('Conditional Veto, Received in the Assembly, 1st Reading/Governor Recommendation', 'governor:vetoed'),
        'CV R/S': ('Conditional Veto, Received in the Senate', 'governor:vetoed'),
        'PV': ('Pocket Veto - Bill not acted on by Governor-end of Session', 'governor:vetoed'),
        '2RSG': ("2nd Reading on Concur with Governor's Recommendations", 'other'),
        'CV R/S 2RSG': ("Conditional Veto, Received, 2nd Reading on Concur with Governor's Recommendations", 'other'),
        '1RAG': ('First Reading/Governor Recommendations Only', 'other'),
        '2RAG': ("2nd Reading in the Assembly on Concur. w/Gov's Recommendations", 'other'),
        'R/S 2RSG': ("Received in the Senate, 2nd Reading - Concur. w/Gov's Recommendations", 'other'),
        'R/A 2RAG': ("Received in the Assembly, 2nd Reading - Concur. w/Gov's Recommendations", 'other'),
        'R/A': ("Received in the Assembly", 'other'),
        'REF SBA': ('Referred to Senate Budget and Appropriations Committee', 'committee:referred'),
        'RSND/V': ('Rescind Vote', 'other'),
        'RCON/V': ('Reconsidered Vote', 'other'),
        'CONCUR AA': ("Concurred by Assembly Amendments", 'other'),
        'CONCUR SA': ('Concurred by Senate Amendments', 'other'),
        'SS 2RS': ('Senate Substitution', 'other'),
        'AS 2RA': ('Assembly Substitution', 'other'),
        'ER': ('Emergency Resolution', 'other'),
        'FSS': ('Filed with Secretary of State', 'other'),
        'LSTA': ('Lost in the Assembly', 'other'),
        'LSTS': ('Lost in the Senate', 'other'),
        'SEN COPY ON DESK': ('Placed on Desk in Senate', 'other'),
        'ASM COPY ON DESK': ('Placed on Desk in Assembly', 'other'),
        'COMB/W': ('Combined with', 'other'),
        'MOTION': ('Motion', 'other'),
        'PUBLIC HEARING': ('Public Hearing Held', 'other'),
        'PH ON DESK SEN': ('Public Hearing Placed on Desk Senate Transcript Placed on Desk', 'other'),
        'PH ON DESK ASM': ('Public Hearing Placed on Desk Assembly Transcript Placed on Desk', 'other'),
        'W': ('Withdrawn from Consideration', 'bill:withdrawn'),
    }

    _com_actions = {
        'INT 1RA REF': ('Introduced in the Assembly, Referred to', ['bill:introduced', 'committee:referred']),
        'INT 1RS REF': ('Introduced in the Senate, Referred to', ['bill:introduced', 'committee:referred']),
        'R/S REF': ('Received in the Senate, Referred to', 'committee:referred'),
        'R/A REF': ('Received in the Assembly, Referred to', 'committee:referred'),
        'TRANS': ('Transferred to', 'committee:referred'),
        'RCM': ('Recommitted to', 'committee:referred'),
        'REP/ACA REF': ('Reported out of Assembly Committee with Amendments and Referred to', 'committee:referred'),
        'REP/ACS REF': ('Reported out of Senate Committee with Amendments and Referred to', 'committee:referred'),
        'REP REF': ('Reported and Referred to', 'committee:referred'),
    }

    _com_vote_motions = {
        'r w/o rec.': 'Reported without recommendation',
        'r w/o rec. ACS': 'Reported without recommendation out of Assembly committee as a substitute',
        'r w/o rec. SCS': 'Reported without recommendation out of Senate committee as a substitute',
        'r w/o rec. Sca': 'Reported without recommendation out of Senate committee with amendments',
        'r w/o rec. Aca': 'Reported without recommendation out of Assembly committee with amendments',
        'r/ACS': 'Reported out of Assembly committee as a substitute',
        'r/Aca': 'Reported out of Assembly committee with amendments',
        'r/SCS': 'Reported out of Senate committee as a substitute',
        'r/Sca': 'Reported out of Senate committee with amendments',
        'r/favorably': 'Reported favorably out of committee',
    }

    _doctypes = {
        'FE':  'Legislative Fiscal Estimate',
        'I':   'Introduced Version',
        'S':   'Statement',
        'V':   'Veto',
        'FN':  'Fiscal Note',
        'F':   'Fiscal Note',
        'R':   'Reprint',
        'FS':  'Floor Statement',
        'TR':  'Technical Report',
        'AL':  'Advance Law',
        'PL':  'Pamphlet Law',
        'RS':  'Reprint of Substitute',
        'ACS': 'Assembly Committee Substitute',
        'AS':  'Assembly Substitute',
        'SCS': 'Senate Committee Substitute',
        'SS':  'Senate Substitute',
        'GS':  "Governor's Statement",
    }

    _version_types = ('I', 'R', 'RS', 'ACS', 'AS', 'SCS', 'SS')

    def initialize_committees(self, year_abr):
        chamber = {'A':'Assembly', 'S': 'Senate', '':''}

        com_csv = self.access_to_csv('Committee')

        self._committees = {}

        for com in com_csv:
            # map XYZ -> "Assembly/Senate _________ Committee"
            self._committees[com['Code']] = ' '.join((chamber[com['House']],
                                                      com['Description'],
                                                      'Committee'))

    def categorize_action(self, act_str, bill_id):
        if act_str in self._actions:
            return self._actions[act_str]

        for prefix, act_pair in self._com_actions.iteritems():
            if act_str.startswith(prefix):
                last3 = act_str.rsplit(' ', 1)[-1]
                com_name = self._committees[last3]
                action, acttype = act_pair
                return (action + ' ' + com_name, acttype)

        # warn about missing action
        self.warning('unknown action: {0} on {1}'.format(act_str, bill_id))

        return (act_str, 'other')

    def scrape(self, session, chambers):
        year_abr = ((int(session) - 209) * 2) + 2000
        self._init_mdb(year_abr)
        self.initialize_committees(year_abr)
        self.scrape_bills(session, year_abr)

    def scrape_bills(self, session, year_abr):
        #Main Bill information
        main_bill_csv = self.access_to_csv('MainBill')

        # keep a dictionary of bills (mapping bill_id to Bill obj)
        bill_dict = {}

        for rec in main_bill_csv:
            bill_type = rec["BillType"].strip()
            bill_number = int(rec["BillNumber"])
            bill_id = bill_type + str(bill_number)
            title = rec["Synopsis"]
            if bill_type[0] == 'A':
                chamber = "lower"
            else:
                chamber = "upper"

            # some bills have a blank title.. just skip it
            if not title:
                continue

            bill = Bill(str(session), chamber, bill_id, title,
                        type=self._bill_types[bill_type[1:]])
            if rec['IdenticalBillNumber']:
                bill.add_companion(rec['IdenticalBillNumber'].split()[0])
            # TODO: last session info is in there too
            bill_dict[bill_id] = bill

        #Sponsors
        bill_sponsors_csv = self.access_to_csv('BillSpon')

        for rec in bill_sponsors_csv:
            bill_type = rec["BillType"].strip()
            bill_number = int(rec["BillNumber"])
            bill_id = bill_type + str(bill_number)
            bill = bill_dict[bill_id]
            name = rec["Sponsor"]
            sponsor_type = rec["Type"]
            if sponsor_type == 'P':
                sponsor_type = "primary"
            else:
                sponsor_type = "cosponsor"
            bill.add_sponsor(sponsor_type, name)


        #Documents
        bill_document_csv = self.access_to_csv('BillWP')

        for rec in bill_document_csv:
            bill_type = rec["BillType"].strip()
            bill_number = int(rec["BillNumber"])
            bill_id = bill_type + str(bill_number)
            if bill_id not in bill_dict:
                self.warning('unknown bill %s in document database' % bill_id)
                continue
            bill = bill_dict[bill_id]
            document = rec["Document"]
            document = document.split('\\')
            document = document[-2] + "/" + document[-1]
            year = str(year_abr) + str((year_abr + 1))

            #doc_url = "ftp://www.njleg.state.nj.us/%s/%s" % (year, document)
            htm_url = 'http://www.njleg.state.nj.us/%s/Bills/%s' % (year_abr,
                document.replace('.DOC', '.HTM'))

            # name document based _doctype
            try:
                doc_name = self._doctypes[rec['DocType']]
            except KeyError:
                raise Exception('unknown doctype %s on %s' %
                                (rec['DocType'], bill_id))
            if rec['Comment']:
                doc_name += ' ' + rec['Comment']

            if rec['DocType'] in self._version_types:
                if htm_url.endswith('HTM'):
                    mimetype = 'text/html'
                elif htm_url.endswith('wpd'):
                    mimetype = 'application/vnd.wordperfect'
                bill.add_version(doc_name, htm_url, mimetype=mimetype)
            else:
                bill.add_document(doc_name, htm_url)

        # Votes
        next_year = int(year_abr)+1
        vote_info_list = ['A%s' % year_abr,
                          'A%s' % next_year,
                          'S%s' % year_abr,
                          'S%s' % next_year,
                          'CA%s-%s' % (year_abr, next_year),
                          'CS%s-%s' % (year_abr, next_year),
                         ]

        for filename in vote_info_list:
            s_vote_url = 'ftp://www.njleg.state.nj.us/votes/%s.zip' % filename
            try:
                s_vote_zip, resp = self.urlretrieve(s_vote_url)
            except scrapelib.FTPError:
                self.warning('could not find %s' % s_vote_url)
                continue
            zipedfile = zipfile.ZipFile(s_vote_zip)
            for vfile in ["%s.txt" % (filename), "%sEnd.txt" % (filename)]:
                try:
                    vote_file = zipedfile.open(vfile, 'U')
                except KeyError:
                    #
                    # Right, so, 2011 we have an "End" file with more
                    # vote data than was in the original dump.
                    #
                    self.warning("No such file: %s" % (vfile))
                    continue

                vdict_file = csv.DictReader(vote_file)

                votes = {}
                if filename.startswith('A') or filename.startswith('CA'):
                    chamber = "lower"
                else:
                    chamber = "upper"

                if filename.startswith('C'):
                    vote_file_type = 'committee'
                else:
                    vote_file_type = 'chamber'

                for rec in vdict_file:

                    if vote_file_type == 'chamber':
                        bill_id = rec["Bill"].strip()
                        leg = rec["Full_Name"]

                        date = rec["Session_Date"]
                        action = rec["Action"]
                        leg_vote = rec["Legislator_Vote"]
                    else:
                        bill_id = '%s%s' % (rec['Bill_Type'], rec['Bill_Number'])
                        leg = rec['Name']
                        # drop time portion
                        date = rec['Agenda_Date'].split()[0]
                        # make motion readable
                        action = self._com_vote_motions[rec['BillAction']]
                        # first char (Y/N) use [0:1] to ignore ''
                        leg_vote = rec['LegislatorVote'][0:1]

                    date = datetime.strptime(date, "%m/%d/%Y")
                    vote_id = '_'.join((bill_id, chamber, action))
                    vote_id = vote_id.replace(" ", "_")

                    if vote_id not in votes:
                        votes[vote_id] = Vote(chamber, date, action, None, None,
                                              None, None, bill_id=bill_id)
                    if vote_file_type == 'committee':
                        votes[vote_id]['committee'] = self._committees[
                            rec['Committee_House']]

                    if leg_vote == "Y":
                        votes[vote_id].yes(leg)
                    elif leg_vote == "N":
                        votes[vote_id].no(leg)
                    else:
                        votes[vote_id].other(leg)

            # remove temp file
            os.remove(s_vote_zip)

            #Counts yes/no/other votes and saves overall vote
            for vote in votes.itervalues():
                vote_yes_count = len(vote["yes_votes"])
                vote_no_count = len(vote["no_votes"])
                vote_other_count = len(vote["other_votes"])
                vote["yes_count"] = vote_yes_count
                vote["no_count"] = vote_no_count
                vote["other_count"] = vote_other_count

                # Veto override.
                if vote['motion'] == 'OVERRIDE':
                    # Per the NJ leg's glossary, a veto override requires
                    # 2/3ds of each chamber. 27 in the senate, 54 in the house.
                    # http://www.njleg.state.nj.us/legislativepub/glossary.asp
                    vote['passed'] = False
                    if vote['chamber'] == 'lower':
                        if vote_yes_count >= 54:
                            vote['passed'] = True
                    elif vote['chamber'] == 'upper':
                        if vote_yes_count >= 27:
                            vote['passed'] = True

                # Regular vote.
                elif vote_yes_count > vote_no_count:
                    vote["passed"] = True
                else:
                    vote["passed"] = False
                vote_bill_id = vote["bill_id"]
                bill = bill_dict[vote_bill_id]
                bill.add_vote(vote)

        #Actions
        bill_action_csv = self.access_to_csv('BillHist')
        actor_map = {'A': 'lower', 'G': 'executive', 'S': 'upper'}

        for rec in bill_action_csv:
            bill_type = rec["BillType"].strip()
            bill_number = int(rec["BillNumber"])
            bill_id = bill_type + str(bill_number)
            bill = bill_dict[bill_id]
            action = rec["Action"]
            date = rec["DateAction"]
            date = datetime.strptime(date, "%m/%d/%y %H:%M:%S")
            actor = actor_map[rec["House"]]
            comment = rec["Comment"]
            action, atype = self.categorize_action(action, bill_id)
            if comment:
                action += (' ' + comment)
            bill.add_action(actor, action, date, type=atype)

        # Subjects
        subject_csv = self.access_to_csv('BillSubj')
        for rec in subject_csv:
            bill_id = rec['BillType'].strip() + str(int(rec['BillNumber']))
            bill = bill_dict.get(bill_id)
            if bill:
                bill.setdefault('subjects', []).append(rec['SubjectKey'])
            else:
                self.warning('invalid bill id in BillSubj: %s' % bill_id)

        phony_bill_count = 0
        # save all bills at the end
        for bill in bill_dict.itervalues():
            # add sources
            if not bill['actions'] and not bill['versions']:
                self.warning('probable phony bill detected %s',
                             bill['bill_id'])
                phony_bill_count += 1
            else:
                bill.add_source('http://www.njleg.state.nj.us/downloads.asp')
                self.save_bill(bill)

        if phony_bill_count:
            self.warning('%s total phony bills detected', phony_bill_count)

########NEW FILE########
__FILENAME__ = committees
import datetime

from billy.scrape import NoDataForPeriod
from billy.scrape.committees import CommitteeScraper, Committee
from .utils import clean_committee_name, MDBMixin


class NJCommitteeScraper(CommitteeScraper, MDBMixin):
    jurisdiction = 'nj'

    def scrape(self, term, chambers):
        year_abr = term[0:4]

        self._init_mdb(year_abr)
        members_csv = self.access_to_csv('COMember')
        info_csv = self.access_to_csv('Committee')

        comm_dictionary = {}

        #Committe Info Database
        for rec in info_csv:
            abrv = rec["Code"]
            comm_name = rec["Description"]
            comm_type = rec["Type"]
            aide = rec["Aide"]
            contact_info = rec["Phone"]

            if abrv[0] == "A":
                chamber = "lower"
            elif abrv[0] == "S":
                chamber = "upper"

            comm = Committee(chamber, comm_name, comm_type = comm_type,
                             aide = aide, contact_info = contact_info)
            comm.add_source('http://www.njleg.state.nj.us/downloads.asp')
            comm_dictionary[abrv] = comm

        #Committee Member Database
        POSITIONS = {
            'C': 'chair',
            'V': 'vice-chair',
            '': 'member'
        }
        for member_rec in members_csv:
            # assignment=P means they are active, assignment=R means removed
            if member_rec['Assignment_to_Committee'] == 'P':
                abr = member_rec["Code"]
                comm_name = comm_dictionary[abr]

                leg = member_rec["Member"]
                role = POSITIONS[member_rec["Position_on_Committee"]]
                comm_name.add_member(leg, role=role)

                self.save_committee(comm_name)

########NEW FILE########
__FILENAME__ = events
import datetime as dt
import pytz
import re
from .utils import MDBMixin

from billy.scrape.events import EventScraper, Event


class NJEventScraper(EventScraper, MDBMixin):
    jurisdiction = 'nj'
    _tz = pytz.timezone('US/Eastern')

    def initialize_committees(self, year_abr):
        chamber = {'A':'Assembly', 'S': 'Senate', '':''}

        com_csv = self.access_to_csv('Committee')

        self._committees = {}
        # There are some IDs that are missing. I'm going to add them
        # before we load the DBF, in case they include them, we'll just
        # override with their data.
        #
        # This data is from:
        # http://www.njleg.state.nj.us/media/archive_audio2.asp?KEY=<KEY>&SESSION=2012
        overlay = {
            'A': 'Assembly on the Whole',
            'S': 'Senate on the Whole',
            'J': 'Joint Legislature on the Whole',
            'ABUB': 'Assembly Budget Committee',
            'JBOC': 'Joint Budget Oversight',
            'JPS': 'Joint Committee on the Public Schools',
            'LRC': 'New Jersey Law Revision Commission',
            'LSI': 'Select Committee on Investigation',
            'PHBC': 'Pension and Health Benefits Review Commission',
            'SBAB': 'Senate Budget and Appropriations Committee',
            'JLSU': 'Space Leasing and Space Utilization Committee',
            'SUTC': 'Sales and Use Tax Review Commission',
            'SPLS': 'Special Session',
            'JCES': 'Joint Committee on Ethical Standards',
        }
        self._committees = overlay

        for com in com_csv:
            # map XYZ -> "Assembly/Senate _________ Committee"
            self._committees[com['Code']] = ' '.join((chamber[com['House']],
                                                      com['Description'],
                                                      'Committee'))

    def scrape(self, chamber, session):
        year_abr = ((int(session) - 209) * 2) + 2000
        self._init_mdb(year_abr)
        self.initialize_committees(year_abr)
        records = self.access_to_csv("Agendas")
        for record in records:
            if record['Status'] != "Scheduled":
                continue
            description = record['Comments']
            related_bills = []

            for bill in re.findall("(A|S)(-)?(\d{4})", description):
                related_bills.append({
                    "bill_id" : "%s %s" % ( bill[0], bill[2] ),
                    "descr": description
                })

            date_time = "%s %s" % (record['Date'], record['Time'])
            date_time = dt.datetime.strptime(date_time, "%m/%d/%Y %I:%M %p")
            hr_name = self._committees[record['CommHouse']]

            event = Event(
                session,
                date_time,
                'committee:meeting',
                "Meeting of the %s" % ( hr_name ),
                location=record['Location'] or "Statehouse",
            )
            for bill in related_bills:
                event.add_related_bill(bill['bill_id'],
                                      description=bill['descr'],
                                      type='consideration')
            try:
                chamber = {
                    "a" : "lower",
                    "s" : "upper",
                    "j" : "joint"
                }[record['CommHouse'][0].lower()]
            except KeyError:
                chamber = "joint"

            event.add_participant("host",
                                  hr_name,
                                  'committee',
                                  committee_code=record['CommHouse'],
                                  chamber=chamber)
            event.add_source('http://www.njleg.state.nj.us/downloads.asp')
            self.save_event(event)

########NEW FILE########
__FILENAME__ = legislators
import re
import urlparse
import htmlentitydefs

from billy.scrape import NoDataForPeriod
from billy.scrape.legislators import LegislatorScraper, Legislator
from .utils import clean_committee_name, MDBMixin

import scrapelib

class NJLegislatorScraper(LegislatorScraper, MDBMixin):
    jurisdiction = 'nj'

    def scrape(self, term, chambers):
        year_abr = term[0:4]

        self._init_mdb(year_abr)

        roster_csv = self.access_to_csv('Roster')
        bio_csv = self.access_to_csv('LegBio')

        photos = {}
        for rec in bio_csv:
            photos[rec['Roster Key']] = rec['URLPicture']

        for rec in roster_csv:
            first_name = rec["Firstname"]
            middle_name = rec["MidName"]
            last_name = rec["LastName"]
            suffix = rec["Suffix"]
            full_name = first_name + " " + middle_name + " " + last_name + " " + suffix
            full_name = full_name.replace('  ', ' ')
            full_name = full_name[0: len(full_name) - 1]

            district = int(rec["District"])
            party = rec["Party"]
            if party == 'R':
                party = "Republican"
            elif party == 'D':
                party = "Democratic"
            else:
                party = party
            chamber = rec["House"]
            if chamber == 'A':
                chamber = "lower"
            elif chamber == 'S':
                chamber = "upper"

            leg_status = rec["LegStatus"]
            # skip Deceased/Retired members
            if leg_status != 'Active':
                continue
            title = rec["Title"]
            legal_position = rec["LegPos"]
            phone = rec["Phone"] or None
            if 'Email' in rec:
                email = rec["Email"]
            else:
                email = ''
            try:
                photo_url = photos[rec['Roster Key']]
            except KeyError:
                photo_url = ''
                self.warning('no photo url for %s', rec['Roster Key'])
            url = ('http://www.njleg.state.nj.us/members/bio.asp?Leg=' +
                   str(int(rec['Roster Key'])))
            address = '{0}\n{1}, {2} {3}'.format(rec['Address'], rec['City'],
                                                 rec['State'], rec['Zipcode'])
            gender = {'M': 'Male', 'F': 'Female'}[rec['Sex']]

            leg = Legislator(term, chamber, str(district), full_name,
                             first_name, last_name, middle_name, party,
                             suffixes=suffix, title=title,
                             legal_position=legal_position,
                             email=email, url=url, photo_url=photo_url,
                             gender=gender)
            leg.add_office('district', 'District Office', address=address,
                           phone=phone)
            leg.add_source(url)
            leg.add_source('http://www.njleg.state.nj.us/downloads.asp')
            self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = utils
import os
import re
import csv
import zipfile
import subprocess

def clean_committee_name(comm_name):
    comm_name = comm_name.strip()
    comm_name = re.sub(' ?[-,] (Co|Vice)?[- ]?Chair$', '', comm_name)
    comm_name = re.sub('Appropriations - S/C:', 'Appropriations-S/C on',
                       comm_name)
    if comm_name == 'Appropriations-S/C Stimulus':
        comm_name = 'Appropriations-S/C on Stimulus'

    return comm_name


def parse_ftp_listing(text):
    lines = text.strip().split('\r\n')
    return (' '.join(line.split()[3:]) for line in lines)


def chamber_name(chamber):
    if chamber == 'upper':
        return 'senate'
    else:
        return 'assembly'

class MDBMixin(object):

    def _init_mdb(self, year):
        self.mdbfile = 'DB%s.mdb' % year
        url = 'ftp://www.njleg.state.nj.us/ag/%sdata/DB%s.zip' % (year, year)
        fname, resp = self.urlretrieve(url)
        zf = zipfile.ZipFile(fname)
        zf.extract(self.mdbfile)
        os.remove(fname)

    # stolen from nm/bills.py
    def access_to_csv(self, table):
        """ using mdbtools, read access tables as CSV """
        commands = ['mdb-export', self.mdbfile, table]
        pipe = subprocess.Popen(commands, stdout=subprocess.PIPE,
                                                        close_fds=True).stdout
        csvfile = csv.DictReader(pipe)
        return csvfile

########NEW FILE########
__FILENAME__ = actions
'''

'''
import re
from billy.scrape.actions import Rule, BaseCategorizer


committees = [
    u'LEGISLATIVE FINANCE COMMITTEE COMMITTEE',
    u'NEW MEXICO FINANCE AUTHORITY OVERSIGHT COMMITTEE COMMITTEE',
    u'APPROPRIATIONS and FINANCE COMMITTEE',
    u'TRANSPORTATION and PUBLIC WORKS COMMITTEE',
    u'PRINTING and SUPPLIES COMMITTEE',
    u'LAND GRANT COMMITTEE COMMITTEE',
    u'LEGISLATIVE HEALTH and HUMAN SERVICES COMMITTEE COMMITTEE',
    u'LEGISLATIVE EDUCATION STUDY COMMITTEE COMMITTEE',
    u'LABOR and HUMAN RESOURCES COMMITTEE',
    u'HEALTH and GOVERNMENT AFFAIRS COMMITTEE',
    u'WATER and NATURAL RESOURCES COMMITTEE COMMITTEE',
    u'ENERGY and NATURAL RESOURCES COMMITTEE',
    u'JUDICIARY COMMITTEE',
    u'MORTGAGE FINANCE AUTHORITY ACT OVERSIGHT COMMITTEE COMMITTEE',
    u'ENROLLING and ENGROSSING - A COMMITTEE',
    u'COURTS, CORRECTIONS and JUSTICE COMMITTEE COMMITTEE',
    u'RULES and ORDER OF BUSINESS COMMITTEE',
    u'AGRICULTURE and WATER RESOURCES COMMITTEE',
    u'ECONOMIC and RURAL DEVELOPMENT COMMITTEE COMMITTEE',
    u'BUSINESS and INDUSTRY COMMITTEE',
    u'ENROLLING and ENGROSSING - B COMMITTEE',
    u'INVESTMENTS and PENSIONS OVERSIGHT COMMITTEE COMMITTEE',
    u'REVENUE STABILIZATION and TAX POLICY COMMITTEE COMMITTEE',
    u'VOTERS and ELECTIONS COMMITTEE',
    u'SCIENCE, TECHNOLOGY and TELECOMMUNICATIONS COMMITTEE COMMITTEE',
    u'TOBACCO SETTLEMENT REVENUE OVERSIGHT COMMITTEE COMMITTEE',
    u'RADIOACTIVE and HAZARDOUS MATERIALS COMMITTEE COMMITTEE',
    u'EDUCATION COMMITTEE',
    u'TAXATION and REVENUE COMMITTEE',
    u"MILITARY and VETERANS' AFFAIRS COMMITTEE",
    u'CAPITOL BUILDINGS PLANNING COMMISSION COMMITTEE',
    u'LEGISLATIVE COUNCIL COMMITTEE',
    u'PUBLIC SCHOOL CAPITAL OUTLAY OVERSIGHT TASK FORCE COMMITTEE',
    u'INDIAN AFFAIRS COMMITTEE COMMITTEE',
    u'CONSUMER and PUBLIC AFFAIRS COMMITTEE',
    u'INTERIM LEGISLATIVE ETHICS COMMITTEE COMMITTEE',
    u'DISABILITIES CONCERNS SUBCOMMITTEE COMMITTEE',
    u'BEHAVIORAL HEALTH SUBCOMMITTEE COMMITTEE',
    u'INVESTIGATORY SUBCOMMITTEE OF THE RULES and ORDER OF BUSINESS SUBCOMMITTEE COMMITTEE',
    u'CORPORATIONS and TRANSPORTATION COMMITTEE',
    u'FINANCE COMMITTEE',
    u'PUBLIC AFFAIRS COMMITTEE',
    u"COMMITTEES' COMMITTEE",
    u'CONSERVATION COMMITTEE',
    u'INDIAN and CULTURAL AFFAIRS COMMITTEE',
    u'SENATE RULES COMMITTEE',
    ]

committees_rgx = '(%s)' % '|'.join(sorted(committees, key=len, reverse=True))


rules = (
    )


class Categorizer(BaseCategorizer):
    rules = rules

    def categorize(self, text):
        '''Wrap categorize and add boilerplate committees.
        '''
        attrs = BaseCategorizer.categorize(self, text)
        committees = attrs['committees']
        for committee in re.findall(committees_rgx, text, re.I):
            if committee not in committees:
                committees.append(committee)
        return attrs

    def post_categorize(self, attrs):
        res = set()
        if 'legislators' in attrs:
            for text in attrs['legislators']:
                rgx = r'(,\s+(?![a-z]\.)|\s+and\s+)'
                legs = re.split(rgx, text)
                legs = filter(lambda x: x not in [', ', ' and '], legs)
                res |= set(legs)
        attrs['legislators'] = list(res)

        res = set()
        if 'committees' in attrs:
            for text in attrs['committees']:

                # Strip stuff like "Rules on 1st reading"
                for text in text.split('then'):
                    text = re.sub(r' on .+', '', text)
                    text = text.strip()
                    res.add(text)
        attrs['committees'] = list(res)
        return attrs

########NEW FILE########
__FILENAME__ = bills
import os
import re
import csv
import zipfile
import subprocess
from datetime import datetime

import lxml.html
import scrapelib

from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote
from billy.scrape.utils import convert_pdf

from .actions import Categorizer

# {spaces}{vote indicator (Y/N/E/ )}{name}{lookahead:2 spaces, space-indicator}
HOUSE_VOTE_RE = re.compile('([YNE ])\s+([A-Z][a-z\'].+?)(?=\s[\sNYE])')


def convert_sv_text(text):
    """
    normalize Senate vote text from pdftotext

    senate votes come out of pdftotext with characters shifted in weird way
    convert_sv_text converts that text to a readable format with junk stripped

    example after decoding:
     OFFICIALROLLCALL
    NEWMEXICOSTATESENATE
    FIFTIETHLEGISLATURE,FIRSTSESSION,2011

    LEGISLATIVEDAY35DATE:03-09-11
    RCS#330
    SENATEBILL233,ASAMENDED
      YES NO ABS EXC  YES NO ABS EXC
     ADAIR X    LOVEJOY X
     ASBILL X    MARTINEZ X
     WILSONBEFFORT X    MCSORLEY X
     BOITANO X    MORALES    X
     BURT X    MUNOZ    X
     CAMPOS X    NAVA X
     CISNEROS X    NEVILLE X
     CRAVENS X    ORTIZYPINO X
     EICHENBERG X    PAPEN X
     FELDMAN X    PAYNE X
     FISCHMANN X    PINTO X
     GARCIA X    RODRIGUEZ   X
     GRIEGO,E. X    RUE X
     GRIEGO,P. X    RYAN X
     HARDEN X    SANCHEZ,B. X
     INGLE X    SANCHEZ,M. X
     JENNINGS X    SAPIEN X
     KELLER   X  SHARER X
     KERNAN X    SMITH   X
     LEAVELL X    ULIBARRI X
     LOPEZ    X WIRTH X
          TOTALS=> 36 0 3 3

    PASSED:36-0
    """
    ret_lines = []
    for line in text.splitlines():
        line = convert_sv_line(line)
        if 'DCDCDC' not in line:
            ret_lines.append(line)
    return ret_lines


def convert_sv_line(line):
    """ convert a single line of the garbled vote text """
    line = line.strip()
    # strip out junk filler char
    line = line.replace('\xef\x80\xa0', '')
    # shift characters
    line = ''.join(convert_sv_char(c) for c in line)
    # clean up the remaining cruft
    line = line.replace('B3', ' ').replace('oA', '').replace('o', '').replace('\x00', '')
    return line


def convert_sv_char(c):
    """ logic for shifting senate vote characters to real ASCII """
    # capital letters shift 64
    if 65 <= ord(c) - 64 <= 90:
        return chr(ord(c) - 64)
    # punctuation shift 128
    else:
        try:
            return chr(ord(c) - 128)
        except ValueError:
            return c


class NMBillScraper(BillScraper):
    jurisdiction = 'nm'
    categorizer = Categorizer()

    def _init_mdb(self, session):
        ftp_base = 'ftp://www.nmlegis.gov/other/'
        if session == '2014':
            fname = 'LegInfo14'
            fname_re = '(\d{2}-\d{2}-\d{2}  \d{2}:\d{2}(?:A|P)M) .* (LegInfo14.*zip)'
        else:
            raise ValueError('no zip file present for %s' % session)

        # use listing to get latest modified LegInfo zip
        listing = self.urlopen(ftp_base)
        matches = re.findall(fname_re, listing)
        matches = sorted([
            (datetime.strptime(date, '%m-%d-%y  %H:%M%p'), filename)
            for date, filename in matches])
        if matches == []:
            raise ValueError("%s contains no matching files." % (ftp_base))

        remote_file = ftp_base + matches[-1][1]

        # all of the data is in this Access DB, download & retrieve it
        mdbfile = '%s.mdb' % fname

        # if a new mdbfile or it has changed
        if getattr(self, 'mdbfile', None) != mdbfile:
            self.mdbfile = mdbfile
            fname, resp = self.urlretrieve(remote_file)
            zf = zipfile.ZipFile(fname)
            zf.extract(self.mdbfile)
            os.remove(fname)

    def access_to_csv(self, table):
        """ using mdbtools, read access tables as CSV """
        commands = ['mdb-export', self.mdbfile, table]
        pipe = subprocess.Popen(commands, stdout=subprocess.PIPE,
                                close_fds=True).stdout
        csvfile = csv.DictReader(pipe)
        return csvfile

    def scrape(self, chamber, session):
        chamber_letter = 'S' if chamber == 'upper' else 'H'
        bill_type_map = {'B': 'bill',
                         'CR': 'concurrent resolution',
                         'JM': 'joint memorial',
                         'JR': 'joint resolution',
                         'M': 'memorial',
                         'R': 'resolution',
                        }

        # used for faking sources
        session_year = session[2:]

        self._init_mdb(session)

        # read in sponsor & subject mappings
        sponsor_map = {}
        for sponsor in self.access_to_csv('tblSponsors'):
            sponsor_map[sponsor['SponsorCode']] = sponsor['FullName']

        subject_map = {}
        for subject in self.access_to_csv('TblSubjects'):
            subject_map[subject['SubjectCode']] = subject['Subject']

        # get all bills into this dict, fill in action/docs before saving
        self.bills = {}
        for data in self.access_to_csv('Legislation'):
            # use their BillID for the key but build our own for storage
            bill_key = data['BillID'].replace(' ', '')

            # if this is from the wrong chamber, skip it
            if not bill_key.startswith(chamber_letter):
                continue

            bill_id = '%s%s%s' % (data['Chamber'], data['LegType'],
                                  data['LegNo'])
            bill_type = bill_type_map[data['LegType']]
            bill_id = bill_id.replace(' ', '')  # remove spaces for consistency
            self.bills[bill_key] = bill = Bill(session, chamber, bill_id,
                                               data['Title'], type=bill_type)

            # fake a source
            data['SessionYear'] = session_year
            data.update({x: data[x].strip() for x in ["Chamber", "LegType",
                                                      "LegNo", "SessionYear"]})

            bill.add_source(
                'http://www.nmlegis.gov/lcs/legislation.aspx?Chamber='
                "{Chamber}&LegType={LegType}&LegNo={LegNo}"
                "&year={SessionYear}".format(**data))

            bill.add_sponsor('primary', sponsor_map[data['SponsorCode']])
            if data['SponsorCode2'] not in ('NONE', 'X', ''):
                bill.add_sponsor('primary', sponsor_map[data['SponsorCode2']])

            # maybe use data['emergency'] data['passed'] data['signed'] as well

            bill['subjects'] = []
            if data['SubjectCode1']:
                bill['subjects'].append(subject_map[data['SubjectCode1']])
            if data['SubjectCode2']:
                bill['subjects'].append(subject_map[data['SubjectCode2']])
            if data['SubjectCode3']:
                bill['subjects'].append(subject_map[data['SubjectCode3']])

        # bills and actions come from other tables
        self.scrape_actions(chamber_letter)
        self.scrape_documents(session, 'bills', chamber)
        self.scrape_documents(session, 'resolutions', chamber)
        self.scrape_documents(session, 'memorials', chamber)
        self.scrape_documents(session, 'votes', chamber, chamber_name='')
        self.check_other_documents(session, chamber)
        self.dedupe_docs()

        # ..and save it all
        for bill in self.bills.itervalues():
            self.save_bill(bill)

    def check_other_documents(self, session, chamber):
        """ check for documents that reside in their own directory """

        s_slug = self.metadata['session_details'][session]['slug']
        firs_url = 'http://www.nmlegis.gov/Sessions/%s/firs/' % s_slug
        lesc_url = 'http://www.nmlegis.gov/Sessions/%s/LESCAnalysis/' % s_slug
        final_url = 'http://www.nmlegis.gov/Sessions/%s/final/' % s_slug

        # go through all of the links on these pages and add them to the
        # appropriate bills
        def check_docs(url, doc_type):
            html = self.urlopen(url)
            doc = lxml.html.fromstring(html)

            for fname in doc.xpath('//a/text()'):
                # split filename into bill_id format
                match = re.match('([A-Z]+)0*(\d{1,4})', fname)
                if match:
                    bill_type, bill_num = match.groups()
                    bill_id = bill_type.replace('B', '') + bill_num
                    try:
                        bill = self.bills[bill_id]
                    except KeyError:
                        self.warning('document for unknown bill %s' % fname)
                    else:
                        if doc_type == 'Final Version':
                            bill.add_version(
                                'Final Version', url + fname, mimetype='text/html')
                        else:
                            bill.add_document(doc_type, url + fname)

        check_docs(firs_url, 'Fiscal Impact Report')
        check_docs(lesc_url, 'LESC Analysis')
        check_docs(final_url, 'Final Version')

    def scrape_actions(self, chamber_letter):
        """ append actions to bills """

        # we could use the TblLocation to get the real location, but we can
        # fake it with the first letter
        location_map = {'H': 'lower', 'S': 'upper', 'P': 'executive'}

        com_location_map = {}
        for loc in self.access_to_csv('TblLocations'):
            com_location_map[loc['LocationCode']] = loc['LocationDesc']

        # combination of tblActions and
        # http://www.nmlegis.gov/lcs/action_abbreviations.aspx
        # table will break when new actions are encountered
        action_map = {
            # committee results
            '7601': ('DO PASS committee report adopted', 'committee:passed:favorable'),
            '7602': ('DO PASS, as amended, committee report adopted', 'committee:passed:favorable'),
            '7603': ('WITHOUT RECOMMENDATION committee report adopted', 'committee:passed'),
            '7604': ('WITHOUT RECOMMENDATION, as amended, committee report adopted', 'committee:passed'),
            # 7605 - 7609 are Committee Substitutes in various amend states
            '7605': ('DO NOT PASS, replaced with committee substitute', 'committee:passed'),
            '7606': ('DO NOT PASS, replaced with committee substitute', 'committee:passed'),
            '7608': ('DO NOT PASS, replaced with committee substitute', 'committee:passed'),
            # withdrawals
            '7611': ('withdrawn from committee', 'bill:withdrawn'),
            '7612': ('withdrawn from all committees', 'bill:withdrawn'),
            '7613': ('withdrawn and tabled', 'bill:withdrawn'),
            '7615': ('germane', 'other'),
            '7616': ('germane & printed', 'other'),
            # 7621-7629 are same as 760*s but add the speakers table (-T)
            '7621': ("DO PASS committee report adopted, placed on Speaker's table", 'committee:passed:favorable'),
            '7622': ("DO PASS, as amended, committee report adopted, placed on Speaker's table", 'committee:passed:favorable'),
            '7623': ("WITHOUT RECOMMENDATION committee report adopted, placed on Speaker's table", 'committee:passed'),
            '7624': ("WITHOUT RECOMMENDATION, as amended, committee report adopted, placed on Speaker's table", 'committee:passed'),
            '7625': ("DO NOT PASS, replaced with committee substitute, placed on Speaker's table", 'committee:passed'),
            '7628': ("DO NOT PASS, replaced with committee substitute, placed on Speaker's table", 'committee:passed'),
            # floor actions
            '7631': ('Withdrawn on the Speakers table by rule from the daily calendar', 'other'),
            '7638': ('Germane as amended', 'other'),
            '7639': ('tabled in House', 'other'),
            '7640': ('tabled in Senate', 'other'),
            '7641': ('floor substitute adopted', 'other'),
            '7642': ('floor substitute adopted (1 amendment)', 'other'),
            '7643': ('floor substitute adopted (2 amendment)', 'other'),
            '7644': ('floor substitute adopted (3 amendment)', 'other'),
            '7655': ('Referred to the House Appropriations & Finance',
                     'committee:referred'),
            '7645': ('motion to reconsider adopted', 'other'),
            '7649': ('printed', 'other'),
            '7650': ('not printed %s', 'other'),
            '7652': ('not printed, not referred to committee, tabled', 'other'),
            '7654': ('referred to %s', 'committee:referred'),
            '7656': ('referred to Finance committee', 'committee:referred'),
            '7660': ('passed House', 'bill:passed'),
            '7661': ('passed Senate', 'bill:passed'),
            '7663': ('House report adopted', 'other'),
            '7664': ('Senate report adopted', 'other'),
            '7665': ('House concurred in Senate amendments', 'other'),
            '7666': ('Senate concurred in House amendments', 'other'),
            '7667': ('House failed to concur in Senate amendments', 'other'),
            '7668': ('Senate failed to concur in House amendments', 'other'),
            '7669': ('this procedure could follow if the Senate refuses to recede from its amendments', 'other'),
            '7670': ('this procedure could follow if the House refuses to recede from its amendments', 'other'),
            '7671': ('this procedure could follow if the House refuses to recede from its amendments', 'other'),
            '7678': ('tabled in Senate', 'other'),
            '7681': ('failed passage in House', 'bill:failed'),
            '7682': ('failed passage in Senate', 'bill:failed'),
            '7685': ('Signed', 'other'),
            '7699': ('special', 'other'),
            '7701': ('failed passage in House', 'bill:failed'),
            '7702': ('failed passage in Senate', 'bill:failed'),
            '7704': ('tabled indefinitely', 'other'),
            '7708': ('action postponed indefinitely', 'other'),
            '7709': ('bill not germane', 'other'),
            '7711': ('DO NOT PASS committee report adopted', 'committee:passed:unfavorable'),
            '7712': ('DO NOT PASS committee report adopted', 'committee:passed:unfavorable'),
            '7798': ('Succeeding entries', 'other'),
            '7804': ('Signed', 'governor:signed'),
            '7805': ('Signed', 'governor:signed'),
            '7806': ('Vetoed', 'governor:vetoed'),
            '7807': ('Pocket Veto', 'governor:vetoed'),
            '7811': ('Veto Override Passed House', 'bill:veto_override:passed'),
            '7812': ('Veto Override Passed Senate', 'bill:veto_override:passed'),
            '7813': ('Veto Override Failed House', 'bill:veto_override:failed'),
            '7814': ('Veto Override Failed Senate', 'bill:veto_override:failed'),
            '7799': ('Dead', 'other'),
            'SENT': ('Sent to %s', ['bill:introduced', 'committee:referred']),
        }

        # these actions need a committee name spliced in
        actions_with_committee = ('SENT', '7650', '7654')

        for action in self.access_to_csv('Actions'):
            bill_key = action['BillID'].replace(' ', '')

            # if this is from the wrong chamber or an unknown bill skip it
            if not bill_key.startswith(chamber_letter):
                continue
            if bill_key not in self.bills:
                self.warning('action for unknown bill %s' % bill_key)
                continue

            # ok the whole Day situation is madness, N:M mapping to real days
            # see http://www.nmlegis.gov/lcs/lcsdocs/legis_day_chart_11.pdf
            # first idea was to look at all Days and use the first occurance's
            # timestamp, but this is sometimes off by quite a bit
            # instead lets just use EntryDate and take radical the position
            # something hasn't happened until it is observed
            action_date = datetime.strptime(action['EntryDate'].split()[0],
                                            "%m/%d/%y")
            if action['LocationCode']:
                actor = location_map.get(action['LocationCode'][0], 'other')
            else:
                actor = 'other'
            action_code = action['ActionCode']

            try:
                action_name, action_type = action_map[action_code]
            except KeyError:
                self.warning('unknown action code %s on %s' % (action_code,
                                                               bill_key))
                raise

            # if there's room in this action for a location name, map locations
            # to their names from the Location table
            if action_code in actions_with_committee:
                # turn A/B/C into Full Name & Full Name 2 & Full Name 3
                locs = [com_location_map[l]
                        for l in action['Referral'].split('/') if l
                        and l in com_location_map]
                action_name = action_name % (' & '.join(locs))

            # Fix known quirks related to actor
            if action_name == 'passed Senate':
                actor = 'upper'
            if action_name == 'passed House':
                actor = 'lower'

            attrs = dict(actor=actor, action=action_name, date=action_date)
            attrs.update(self.categorizer.categorize(action_name))
            if action_type not in attrs['type']:
                if isinstance(action_type, basestring):
                    attrs['type'].append(action_type)
                else:
                    attrs['type'].extend(action_type)
            self.bills[bill_key].add_action(**attrs)

    def scrape_documents(self, session, doctype, chamber, chamber_name=None):
        """ most document types (+ Votes)are in this common directory go
        through it and attach them to their related bills """

        session_path = self.metadata['session_details'][session]['slug']

        if chamber_name != '':
            chamber_name = 'house' if chamber == 'lower' else 'senate'

        doc_path = 'http://www.nmlegis.gov/Sessions/%s/%s/%s/'
        doc_path = doc_path % (session_path, doctype, chamber_name)

        html = self.urlopen(doc_path)

        doc = lxml.html.fromstring(html)

        # all links but first one
        for fname in doc.xpath('//a/text()')[1:]:

            # skip PDFs for now -- everything but votes have HTML versions
            if fname.endswith('pdf') and 'VOTE' not in fname:
                continue

            match = re.match('([A-Z]+)0*(\d{1,4})([^.]*)', fname.upper())
            if match is None:
                self.warning("No match, skipping")
                continue

            bill_type, bill_num, suffix = match.groups()

            # adapt to bill_id format
            bill_id = bill_type.replace('B', '') + bill_num
            try:
                bill = self.bills[bill_id]
            except KeyError:
                self.warning('document for unknown bill %s' % fname)
                continue

            # no suffix = just the bill
            if suffix == '':
                bill.add_version('introduced version', doc_path + fname,
                                 mimetype='text/html')

            # floor amendments
            elif re.match('F(S|H)\d', suffix):
                a_chamber, num = re.match('F(S|H)(\d)', suffix).groups()
                a_chamber = 'House' if a_chamber == 'H' else 'Senate'
                bill.add_document('%s Floor Amendment %s' %
                                  (a_chamber, num),
                                  doc_path + fname)

            # committee substitutes
            elif suffix.endswith('S'):
                committee_name = suffix[:-1]
                bill.add_version('%s substitute' % committee_name,
                                 doc_path + fname, mimetype='text/html')

            # votes
            elif 'SVOTE' in suffix:
                vote = self.parse_senate_vote(doc_path + fname)
                if vote:
                    bill.add_vote(vote)
            elif 'HVOTE' in suffix:
                vote = self.parse_house_vote(doc_path + fname)
                if vote:
                    bill.add_vote(vote)

            # committee reports
            elif re.match('\w{2,3,4}\d', suffix):
                committee_name = re.match('[A-Z]+', suffix).group()
                bill.add_document('%s committee report' % committee_name,
                                  doc_path + fname)

            # ignore list, mostly typos reuploaded w/ proper name
            elif suffix in ('HEC', 'HOVTE', 'GUI'):
                pass
            else:
                # warn about unknown suffix
                self.warning('unknown document suffix %s' % (fname))

    def parse_senate_vote(self, url):
        """ senate PDFs -> garbled text -> good text -> Vote """
        vote = Vote('upper', '?', 'senate passage', False, 0, 0, 0)
        vote.add_source(url)

        fname, resp = self.urlretrieve(url)
        # this gives us the cleaned up text
        sv_text = convert_sv_text(convert_pdf(fname, 'text'))
        os.remove(fname)
        in_votes = False

        # use in_votes as a sort of state machine
        for line in sv_text:

            # not 'in_votes', get date or passage
            if not in_votes:
                dmatch = re.search('DATE:(\d{2}-\d{2}-\d{2})', line)
                if dmatch:
                    date = dmatch.groups()[0]
                    vote['date'] = datetime.strptime(date, '%m-%d-%y')

                if 'YES NO ABS EXC' in line:
                    in_votes = True
                elif 'PASSED' in line:
                    vote['passed'] = True

            # in_votes: totals & votes
            else:
                # totals
                if 'TOTALS' in line:

                    # Lt. Governor voted
                    if 'GOVERNOR' in line:
                        rgx = ' ((?:LT\. )?[A-Z,.]+)(\s+)X(.*)'
                        name, spaces, line = re.match(rgx, line).groups()
                        if len(spaces) == 1:
                            vote.yes(name)
                        else:
                            vote.no(name)

                    _, yes, no, abs, exc = line.split()
                    vote['yes_count'] = int(yes)
                    vote['no_count'] = int(no)
                    vote['other_count'] = int(abs) + int(exc)
                    # no longer in votes
                    in_votes = False
                    continue

                # pull votes out
                matches = re.match(
                    ' ([A-Z,\'\-.]+)(\s+)X\s+([A-Z,\'\-.]+)(\s+)X', line)

                if matches is not None:
                    matches = matches.groups()
                    name1, spaces1, name2, spaces2 = matches

                    if "District" in name1 or "District" in name2:
                        continue

                    # vote can be determined by # of spaces
                    if len(spaces1) == 1:
                        vote.yes(name1)
                    elif len(spaces1) == 2:
                        vote.no(name1)
                    else:
                        vote.other(name1)

                    if len(spaces2) == 1:
                        vote.yes(name2)
                    elif len(spaces2) == 2:
                        vote.no(name2)
                    else:
                        vote.other(name2)

        if not isinstance(vote['date'], datetime):
            return None

        return vote

    # house totals
    HOUSE_TOTAL_RE = re.compile('\s+Absent:\s+(\d+)\s+Yeas:\s+(\d+)\s+Nays:\s+(\d+)\s+Excused:\s+(\d+)')

    def parse_house_vote(self, url):
        """ house votes are pdfs that can be converted to text, require some
        nasty regex to get votes out reliably """

        fname, resp = self.urlretrieve(url)
        text = convert_pdf(fname, 'text')
        if not text.strip():
            self.warning('image PDF %s' % url)
            return
        os.remove(fname)

        # get date
        if text.strip() == 'NEW MEXICO HOUSE OF REPRESENTATIVES':
            self.warning("What the heck: %s" % (url))
            return None

        date = re.findall('(\d+/\d+/\d+)', text)[0]
        date = datetime.strptime(date, '%m/%d/%y')

        # get totals
        absent, yea, nay, exc = self.HOUSE_TOTAL_RE.findall(text)[0]

        # make vote (faked passage indicator)
        vote = Vote('lower', date, 'house passage', int(yea) > int(nay),
                    int(yea), int(nay), int(absent) + int(exc))
        vote.add_source(url)

        # votes
        real_votes = False
        for v, name in HOUSE_VOTE_RE.findall(text):
            # our regex is a bit broad, wait until we see 'Nays' to start
            # and end when we see CERTIFIED or ____ signature line
            if 'Nays' in name or 'Excused' in name:
                real_votes = True
                continue
            elif 'CERTIFIED' in name or '___' in name:
                break
            elif real_votes and name.strip():
                if v == 'Y':
                    vote.yes(name)
                elif v == 'N':
                    vote.no(name)
                else:   # excused/absent
                    vote.other(name)
        return vote

    def dedupe_docs(self):
        for bill_id, bill in self.bills.items():
            documents = bill['documents']
            if 1 < len(documents):
                resp_set = set()
                for doc in documents:
                    try:
                        resp = self.urlopen(doc['url'])
                    except scrapelib.HTTPError:
                        documents.remove(doc)
                        continue
                    if resp in resp_set:
                        documents.remove(doc)
                    else:
                        resp_set.add(resp)

########NEW FILE########
__FILENAME__ = legislators
from billy.scrape.legislators import LegislatorScraper, Legislator

import lxml.html


class NMLegislatorScraper(LegislatorScraper):
    jurisdiction = 'nm'

    def scrape(self, chamber, term):
        self.validate_term(term, latest_only=True)

        if chamber == 'lower':
            xpath = '//table[@id="ctl00_mainCopy_gridViewHouseDistricts"]//a[contains(@href, "SPONCODE")]/@href'
        else:
            xpath = '//table[@id="ctl00_mainCopy_gridViewSenateDistricts"]//a[contains(@href, "SPONCODE")]/@href'

        html = self.urlopen('http://www.nmlegis.gov/lcs/districts.aspx')
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute('http://www.nmlegis.gov/lcs/')
        for link in doc.xpath(xpath):
            # dummy id used for empty seat
            if 'SNULL' in link:
                continue
            self.scrape_legislator(chamber, term, link)

    def scrape_legislator(self, chamber, term, url):
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        # most properties are easy to pull
        optional = ["home_phone"]
        properties = {
            'start_year': 'lblStartYear',
            'district': "linkDistrict",
            'occupation': "lblOccupation",
            'header': "lblHeader",
            'addr_street': "lblAddress",
            'office_phone': ["lblCapitolPhone", "lblOfficePhone"],
            'home_phone': "lblHomePhone",
#            '': "",
#            '': "",
#            '': "",
#            '': "",
            }

        for key, value in properties.iteritems():
            if isinstance(value, list):
                values = value
            else:
                values = [value]

            found = False
            for value in values:
                id_ = 'ctl00_mainCopy_formViewLegislator_%s' % value
                val = None
                try:
                    val = "\n".join(doc.get_element_by_id(id_).itertext())
                    found = True
                except KeyError:
                    pass
                if val:
                    properties[key] = val.strip()
                else:
                    properties[key] = None

            if found is False and key not in optional:
                self.warning('bad legislator page %s missing %s' %
                             (url, id_))
                return

        # image & email are a bit different
        properties['photo_url'] = doc.xpath('//img[@id="ctl00_mainCopy_formViewLegislator_imgLegislator"]/@src')[0]
        email = doc.get_element_by_id('ctl00_mainCopy_formViewLegislator_linkEmail').text
        if email:
            properties['email'] = email.strip()

        properties['url'] = url

        properties['chamber'] = chamber
        properties['term'] = term

        full_name, party = properties['header'].rsplit("-", 1)

        properties['full_name'] = full_name
        properties['party'] = party

        if '(D)' in properties['party']:
            properties['party'] = 'Democratic'
        elif '(R)' in properties['party']:
            properties['party'] = 'Republican'
        elif '(DTS)' in properties['party']:
            # decline to state = independent
            properties['party'] = 'Independent'
        else:
            raise Exception("unknown party encountered")

        address = properties.pop('addr_street')

        phone = (properties.pop('office_phone') or
                 properties.pop('home_phone'))

        leg = Legislator(**properties)
        leg.add_source(url)

        leg.add_office('district', 'District Address', address=address,
                       phone=phone)

        # committees
        # skip first header row
        for row in doc.xpath('//table[@id="ctl00_mainCopy_gridViewCommittees"]/tr')[1:]:
            role, committee, note = [x.text_content()
                                     for x in row.xpath('td')]
            committee = committee.title()
            if 'Interim' in note:
                role = 'interim ' + role.lower()
            else:
                role = role.lower()
            leg.add_role('committee member', term, committee=committee,
                          position=role, chamber=chamber)

        # Already have the photo url.
        try:
            del leg['image_url']
        except KeyError:
            pass

        self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = test_hvote_regex
# hb1 is relatively normal
hb1 = """
                       NEW MEXICO HOUSE OF REPRESENTATIVES
RCS# 2668                    Forty-Ninth Legislature                         1/19/11
                               FIRST SESSION, 2011                          11:45 AM


                                     HB 1/EC
                                 REP Martinez, W.
                                  FINAL PASSAGE


                  Absent: 4    Yeas: 66    Nays: 0      Excused: 0


  Y   Alcon, E.        Y Ezzell, C. S.    Y   Little, R.       Y   Saavedra, H.
  Y   Anderson, T. A   Y Garcia, M.H.     Y   Lujan, A.        Y   Salazar, N.
  Y   Baldonado, A.    Y Garcia, M.P.     Y   Lujan, Ben       Y   Sandoval, E.
  Y   Bandy, P. C.     Y Garcia, T.A.     Y   Lundstrom, P.    Y   Smith, J. S.
  Y   Begaye, Ray      Y Gentry, N.           Madalena, J. R   Y   Stapleton, S.
  Y   Bratton, D.      Y Gonzales, R.     Y   Maestas, A.      Y   Stewart, M.
  Y   Brown, C. B.     Y Gray, W. J.      Y   Martinez, R.     Y   Strickler, J.
  Y   Cervantes, J.    Y Gutierrez, J.    Y   Martinez, W. K   Y   Taylor T. C.
  Y   Chasey, Gail     Y Hall, J. C.      Y   McMillan, T.     Y   Tripp, D.
  Y   Chavez, D.       Y Hamilton, D.     Y   Miera, R.        Y   Trujillo, J.R.
  Y   Chavez, E.       Y Herrell, Y.      Y   Nunez, A.        Y   Tyler, S. A.
  Y   Chavez, E. H.    Y Irwin, D. G.     Y   O'Neill, B.      Y   Varela, L.
  Y   Cook, Z.J.       Y James,C. D.      Y   Park, A.         Y   Vigil, R. D.
  Y   Crook, A. M.     Y Jeff, S.         Y   Picraux, D.      Y   Wallace, J.
  Y   Dodge, G.        Y King, R.             Powdrell-C, J.   Y   White, J. P.
  Y   Doyle, D.        Y Kintigh, D.      Y   Rehm, W.         Y   Wooley, B.
  Y   Egolf, B.          Larranaga, L.        Roch, D.
  Y   Espinoza, N.     Y Lewis, T.        Y   Rodella, D.




                              CERTIFIED CORRECT TO THE BEST OF OUR KNOWLEDGE



                              ___________________________________ (Speaker)



                              ___________________________________ (Chief Clerk)"""


# has some single-spaced votes, ugh
hb131 = """                     NEW MEXICO HOUSE OF REPRESENTATIVES
RCS# 2811                    Fiftieth Legislature                       2/24/11
                             FIRST SESSION, 2011                       11:41 AM


                                    HB 131
                                  Rep Varela
                                FINAL PASSAGE


               Absent: 0    Yeas: 34     Nays: 34    Excused: 2


  Y Alcon, E.        N Ezzell, C. S.    N  Little, R.        Y Saavedra, H.
 N  Anderson, T. A    Y Garcia, M.H.     Y Lujan, A.      E    Salazar, N.
 N  Baldonado, A.     Y Garcia, M.P.     Y Lujan, Ben        Y Sandoval, E.
 N  Bandy, P. C.      Y Garcia, T.A.     Y Lundstrom, P.    N Smith, J. S.
  Y Begaye, Ray      N Gentry, N.        Y Madalena, J. R    Y Stapleton, S.
 N Bratton, D.        Y Gonzales, R.     Y Maestas, A.       Y Stewart, M.
 N Brown, C. B.      N Gray, W. J.       Y Martinez, R.     N Strickler, J.
  Y Cervantes, J.     Y Gutierrez, J.    Y Martinez, W. K N Taylor T. C.
  Y Chasey, Gail     N Hall, J. C.       Y McMillan, T.     N Tripp, D.
 N Chavez, D.        N Hamilton, D.      Y Miera, R.         Y Trujillo, J.R.
  Y Chavez, E.       N Herrell, Y.      N Nunez, A.         N Tyler, S. A.
  Y Chavez, E.H.   E    Irwin, D. G.     Y O'Neill, B.       Y Varela, L.
 N Cook, Z.J.        N James,C. D.       Y Park, A.          Y Vigil, R. D.
  Y Crook, A. M.     N Jeff, S.         N Picraux, D.       N Wallace, J.
  Y Dodge, G.         Y King, R.        N Powdrell-C, J. N White, J. P.
 N Doyle, D.         N Kintigh, D.       Y Rehm, W.         N Wooley, B.
  Y Egolf, B.        N Larranaga, L.    N Roch, D.
 N Espinoza, N.      N Lewis, T.        N Rodella, D.




                            CERTIFIED CORRECT TO THE BEST OF OUR KNOWLEDGE



                            ___________________________________ (Speaker)



                            ___________________________________ (Chief Clerk)"""


from collections import defaultdict
import re
HOUSE_VOTE_RE = re.compile('([YNE ])\s+([A-Z][a-z\'].+?)(?=\s[\sNYE])')

def check_regex_against_vote(vote, y, n, a, e):
    counts = defaultdict(int)
    for v in HOUSE_VOTE_RE.findall(vote):
        if 'Excused' in v[1]:
            counts = defaultdict(int)
            continue
        counts[v[0]] += 1
    if counts['Y'] != y or counts['N'] != n or counts[' '] != a or counts['E'] != e:
        print counts
        for x in HOUSE_VOTE_RE.findall(vote):
            print ' ',x
    else:
        print '  good'

print 'HB1'
check_regex_against_vote(hb1, 66, 0, 4, 0)
print 'HB131'
check_regex_against_vote(hb131, 34, 34, 0, 2)

########NEW FILE########
__FILENAME__ = bills
import re
from datetime import datetime
from collections import defaultdict

from .utils import chamber_name, parse_ftp_listing
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import VoteScraper, Vote

import lxml.html

class NVBillScraper(BillScraper):
    jurisdiction = 'nv'

    _classifiers = (
        ('Approved by the Governor', 'governor:signed'),
        ('Bill read. Veto not sustained', 'bill:veto_override:passed'),
        ('Bill read. Veto sustained', 'bill:veto_override:failed'),
        ('Enrolled and delivered to Governor', 'governor:received'),
        ('From committee: .+? adopted', 'committee:passed'),
        ('From committee: .+? pass', 'committee:passed'),
        ('Prefiled. Referred', ['bill:introduced', 'committee:referred']),
        ('Read first time. Referred', ['bill:reading:1', 'committee:referred']),
        ('Read first time.', 'bill:reading:1'),
        ('Read second time.', 'bill:reading:2'),
        ('Read third time. Lost', ['bill:failed', 'bill:reading:3']),
        ('Read third time. Passed', ['bill:passed', 'bill:reading:3']),
        ('Read third time.', 'bill:reading:3'),
        ('Rereferred', 'committee:referred'),
        ('Resolution read and adopted', 'bill:passed'),
        ('Vetoed by the Governor', 'governor:vetoed')
    )

    def scrape(self, chamber, session):
        if 'Special' in session:
            year = session[0:4]
        elif int(session) >= 71:
            year = ((int(session) - 71) * 2) + 2001
        else:
            raise NoDataForPeriod(session)

        sessionsuffix = 'th'
        if str(session)[-1] == '1':
            sessionsuffix = 'st'
        elif str(session)[-1] == '2':
            sessionsuffix = 'nd'
        elif str(session)[-1] == '3':
            sessionsuffix = 'rd'

        self.subject_mapping = defaultdict(list)

        if 'Special' in session:
            insert = session[-2:] + sessionsuffix + str(year) + "Special"
        else:
            insert = str(session) + sessionsuffix + str(year)
            self.scrape_subjects(insert, session, year)

        if chamber == 'upper':
            self.scrape_senate_bills(chamber, insert, session, year)
        elif chamber == 'lower':
            self.scrape_assem_bills(chamber, insert, session, year)

    def scrape_subjects(self, insert, session, year):
        url = 'http://www.leg.state.nv.us/Session/%s/Reports/TablesAndIndex/%s_%s-index.html' % (insert, year, session)

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        # first, a bit about this page:
        # Level0 are the bolded titles
        # Level1,2,3,4 are detailed titles, contain links to bills
        # all links under a Level0 we can consider categorized by it
        # there are random newlines *everywhere* that should get replaced

        subject = None

        for p in doc.xpath('//p'):
            if p.get('class') == 'Level0':
                subject = p.text_content().replace('\r\n', ' ')
            else:
                if subject:
                    for a in p.xpath('.//a'):
                        bill_id = (a.text.replace('\r\n', '') if a.text
                                   else None)
                        self.subject_mapping[bill_id].append(subject)

    def scrape_senate_bills(self, chamber, insert, session, year):
        doc_type = {2: 'bill', 4: 'resolution', 7: 'concurrent resolution',
                    8: 'joint resolution'}

        for docnum, bill_type in doc_type.iteritems():
            parentpage_url = 'http://www.leg.state.nv.us/Session/%s/Reports/HistListBills.cfm?DoctypeID=%s' % (insert, docnum)
            links = self.scrape_links(parentpage_url)
            count = 0
            for link in links:
                count = count + 1
                page_path = 'http://www.leg.state.nv.us/Session/%s/Reports/%s' % (insert, link)

                page = self.urlopen(page_path)
                page = page.replace(u"\xa0", " ")
                root = lxml.html.fromstring(page)

                bill_id = root.xpath('string(/html/body/div[@id="content"]/table[1]/tr[1]/td[1]/font)')
                title = root.xpath('string(/html/body/div[@id="content"]/table[1]/tr[5]/td)')

                bill = Bill(session, chamber, bill_id, title,
                            type=bill_type)
                bill['subjects'] = self.subject_mapping[bill_id]

                bill_text = root.xpath("string(/html/body/div[@id='content']/table[6]/tr/td[2]/a/@href)")
                text_url = "http://www.leg.state.nv.us" + bill_text
                bill.add_version("Bill Text", text_url,
                                 mimetype='application/pdf')

                primary, secondary = self.scrape_sponsors(page)

                for leg in primary:
                    bill.add_sponsor('primary', leg)
                for leg in secondary:
                    bill.add_sponsor('cosponsor', leg)


                minutes_count = 2
                for mr in root.xpath('//table[4]/tr/td[3]/a'):
                    minutes =  mr.xpath("string(@href)")
                    minutes_url = "http://www.leg.state.nv.us" + minutes
                    minutes_date_path = "string(//table[4]/tr[%s]/td[2])" % minutes_count
                    minutes_date = mr.xpath(minutes_date_path).split()
                    minutes_date = minutes_date[0] + minutes_date[1] + minutes_date[2] + " Agenda"
                    bill.add_document(minutes_date, minutes_url)
                    minutes_count = minutes_count + 1

                self.scrape_actions(root, bill, "upper")
                self.scrape_votes(page, bill, insert, year)
                bill.add_source(page_path)
                self.save_bill(bill)



    def scrape_assem_bills(self, chamber, insert, session, year):

        doc_type = {1: 'bill', 3: 'resolution', 5: 'concurrent resolution',
                    6: 'joint resolution'}
        for docnum, bill_type in doc_type.iteritems():
            parentpage_url = 'http://www.leg.state.nv.us/Session/%s/Reports/HistListBills.cfm?DoctypeID=%s' % (insert, docnum)
            links = self.scrape_links(parentpage_url)
            count = 0
            for link in links:
                count = count + 1
                page_path = 'http://www.leg.state.nv.us/Session/%s/Reports/%s' % (insert, link)
                page = self.urlopen(page_path)
                page = page.replace(u"\xa0", " ")
                root = lxml.html.fromstring(page)

                bill_id = root.xpath('string(/html/body/div[@id="content"]/table[1]/tr[1]/td[1]/font)')
                title = root.xpath('string(/html/body/div[@id="content"]/table[1]/tr[5]/td)')

                bill = Bill(session, chamber, bill_id, title,
                            type=bill_type)
                bill['subjects'] = self.subject_mapping[bill_id]
                bill_text = root.xpath("string(/html/body/div[@id='content']/table[6]/tr/td[2]/a/@href)")
                text_url = "http://www.leg.state.nv.us" + bill_text
                bill.add_version("Bill Text", text_url,
                                 mimetype='application/pdf')

                primary, secondary = self.scrape_sponsors(page)

                for leg in primary:
                    bill.add_sponsor('primary', leg)
                for leg in secondary:
                    bill.add_sponsor('cosponsor', leg)

                minutes_count = 2
                for mr in root.xpath('//table[4]/tr/td[3]/a'):
                    minutes =  mr.xpath("string(@href)")
                    minutes_url = "http://www.leg.state.nv.us" + minutes
                    minutes_date_path = "string(//table[4]/tr[%s]/td[2])" % minutes_count
                    minutes_date = mr.xpath(minutes_date_path).split()
                    minutes_date = minutes_date[0] + minutes_date[1] + minutes_date[2] + " Minutes"
                    bill.add_document(minutes_date, minutes_url)
                    minutes_count = minutes_count + 1


                self.scrape_actions(root, bill, "lower")
                self.scrape_votes(page, bill, insert, year)
                bill.add_source(page_path)
                self.save_bill(bill)

    def scrape_links(self, url):
        links = []

        page = self.urlopen(url)
        root = lxml.html.fromstring(page)
        path = '/html/body/div[@id="ScrollMe"]/table/tr[1]/td[1]/a'
        for mr in root.xpath(path):
            if '*' not in mr.text:
                web_end = mr.xpath('string(@href)')
                links.append(web_end)
        return links


    def scrape_sponsors(self, page):
        primary = []
        sponsors = []
        doc = lxml.html.fromstring(page)
        for b in doc.xpath('//div[@id="content"]/table[1]/tr[4]/td/b'):
            name = b.text.strip()
            # add these as sponsors (excluding junk text)
            if name not in ('By:', 'Bolded'):
                primary.append(name)

        # tail of last b has remaining sponsors
        for name in b.tail.split(', '):
            if name.strip():
                sponsors.append(name.strip())

        return primary, sponsors

    def scrape_actions(self, root, bill, actor):
        path = '/html/body/div[@id="content"]/table/tr/td/p[1]'
        for mr in root.xpath(path):
            date = mr.text_content().strip()
            date = date.split()[0] + " " + date.split()[1] + " " + date.split()[2]
            date = datetime.strptime(date, "%b %d, %Y")
            for el in mr.xpath('../../following-sibling::tr[1]/td/ul/li'):
                action = el.text_content().strip()

                # skip blank actions
                if not action:
                    continue

                # catch chamber changes
                if action.startswith('In Assembly'):
                    actor = 'lower'
                elif action.startswith('In Senate'):
                    actor = 'upper'
                elif 'Governor' in action:
                    actor = 'executive'

                action_type = 'other'
                for pattern, atype in self._classifiers:
                    if re.match(pattern, action):
                        action_type = atype
                        break

                bill.add_action(actor, action, date, type=action_type)

    def scrape_votes(self, bill_page, bill, insert, year):
        root = lxml.html.fromstring(bill_page)
        for link in root.xpath('//a[contains(text(), "Passage")]'):
            motion = link.text
            if 'Assembly' in motion:
                chamber = 'lower'
            else:
                chamber = 'upper'
            vote_url = 'http://www.leg.state.nv.us/Session/%s/Reports/%s' % (
                insert, link.get('href'))
            bill.add_source(vote_url)
            page = self.urlopen(vote_url)
            page = page.replace(u"\xa0", " ")
            root = lxml.html.fromstring(page)

            date = root.xpath('//h1/text()')[-1].strip()
            if not date:
                date = root.xpath('//h1/text()')[-2].strip()
            date = datetime.strptime(date, "%B %d, %Y at %H:%M %p")
            top_block_text = root.xpath('//div[@align="center"]')[0].text_content()
            yes_count = int(re.findall("(\d+) Yea", top_block_text)[0])
            no_count = int(re.findall("(\d+) Nay", top_block_text)[0])
            excused = int(re.findall("(\d+) Excused", top_block_text)[0])
            not_voting = int(re.findall("(\d+) Not Voting", top_block_text)[0])
            absent = int(re.findall("(\d+) Absent", top_block_text)[0])
            other_count = excused + not_voting + absent
            passed = yes_count > no_count

            vote = Vote(chamber, date, motion, passed, yes_count, no_count,
                        other_count, not_voting=not_voting, absent=absent)

            for el in root.xpath('//table[2]/tr'):
                tds = el.xpath('td')
                name = tds[1].text_content().strip()
                vote_result = tds[2].text_content().strip()

                if vote_result == 'Yea':
                    vote.yes(name)
                elif vote_result == 'Nay':
                    vote.no(name)
                else:
                    vote.other(name)
            bill.add_vote(vote)

########NEW FILE########
__FILENAME__ = committees
import re
import datetime

from billy.scrape.committees import CommitteeScraper, Committee

import lxml.html


class NVCommitteeScraper(CommitteeScraper):
    jurisdiction = 'nv'

    def scrape(self, chamber, term):

        for t in self.metadata['terms']:
            if t['name'] == term:
                session = t['sessions'][-1]

        sessionsuffix = 'th'
        if str(session)[-1] == '1':
            sessionsuffix = 'st'
        elif str(session)[-1] == '2':
            sessionsuffix = 'nd'
        elif str(session)[-1] == '3':
            sessionsuffix = 'rd'
        insert = str(session) + sessionsuffix + str(term[0:4])

        chamber_letter = {'lower':'A', 'upper':'S'}[chamber]

        insert = self.metadata['session_details'][session].get(
            '_committee_session', insert
        )


        url = 'http://www.leg.state.nv.us/Session/%s/Committees/%s_Committees/' % (
            insert, chamber_letter)

        page = self.urlopen(url)
        root = lxml.html.fromstring(page)
        for com_a in root.xpath('//strong/a'):
            com_url = url + com_a.get('href')
            if com_a.text == 'Committee of the Whole':
                continue
            com = Committee(chamber, com_a.text)
            com.add_source(com_url)
            self.scrape_comm_members(chamber, com, com_url)
            self.save_committee(com)

    def scrape_comm_members(self, chamber, committee, url):
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        for li in doc.xpath('//li'):
            pieces = li.text_content().split(' - ')
            name = pieces[0].strip()
            role = pieces[1] if len(pieces) == 2 else 'member'
            committee.add_member(name, role)

########NEW FILE########
__FILENAME__ = legislators
import json
import datetime

from billy.scrape.legislators import LegislatorScraper, Legislator
from .utils import clean_committee_name

import lxml.html
import scrapelib

class NVLegislatorScraper(LegislatorScraper):
    jurisdiction = 'nv'

    def scrape(self, chamber, term_name):

        for t in self.metadata['terms']:
            if t['name'] == term_name:
                session = t['sessions'][-1]
                slug = self.metadata['session_details'][session]['slug']

        if chamber == 'upper':
            chamber_slug = 'Senate'
        elif chamber == 'lower':
            chamber_slug = 'Assembly'

        leg_base_url = 'http://www.leg.state.nv.us/App/Legislator/A/%s/%s/' % (chamber_slug, slug)
        leg_json_url = 'http://www.leg.state.nv.us/App/Legislator/A/api/%s/Legislator?house=%s' % (slug, chamber_slug)

        resp = json.loads(self.urlopen(leg_json_url))

        for item in resp:
            # empty district
            if 'District No' in item['FullName']:
                continue
            leg = Legislator(term_name, chamber, item['DistrictNbr'],
                             item['FullName'], party=item['Party'],
                             photo_url=item['PhotoURL'])
            leg_url = leg_base_url + item['DistrictNbr']

            # fetch office from legislator page
            try:
                doc = lxml.html.fromstring(self.urlopen(leg_url))
                if not doc.xpath('//div'):
                    self.warning('invalid page, maybe a weird PDF?')
                else:
                    address = doc.xpath('//div[@class="contactAddress"]')[0].text_content()
                    address2 = doc.xpath('//div[@class="contactAddress2"]')
                    if address2:
                        address += ' ' + address2[0].text_content()
                    address += '\n' + doc.xpath('//div[@class="contactCityStateZip"]')[0].text_content()
                    phone = doc.xpath('//div[@class="contactPhone"]')[0].text_content()

                    leg.add_office('district', 'District Address', address=address,
                                   phone=phone)
            except scrapelib.HTTPError:
                self.warning('could not fetch %s' % leg_url)
                pass

            leg.add_source(leg_url)
            self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = utils
import re


def clean_committee_name(comm_name):
    comm_name = comm_name.strip()
    comm_name = re.sub(' ?[-,] (Co|Vice)?[- ]?Chair$', '', comm_name)
    comm_name = re.sub('Appropriations - S/C:', 'Appropriations-S/C on',
                       comm_name)
    if comm_name == 'Appropriations-S/C Stimulus':
        comm_name = 'Appropriations-S/C on Stimulus'

    return comm_name


def parse_ftp_listing(text):
    lines = text.strip().split('\r\n')
    return (' '.join(line.split()[3:]) for line in lines)


def chamber_name(chamber):
    if chamber == 'upper':
        return 'senate'
    else:
        return 'assembly'



########NEW FILE########
__FILENAME__ = actions
'''
NY needs an @after_categorize function to expand committee names
and help the importer figure out which committees are being mentioned.
'''
import re
from functools import partial
from collections import namedtuple, defaultdict
from types import MethodType


class Rule(namedtuple('Rule', 'regexes types stop attrs')):
    '''If anyh of ``regexes`` matches the action text, the resulting
    action's types should include ``types``.

    If stop is true, no other rules should be tested after this one;
    in other words, this rule conclusively determines the action's
    types and attrs.

    The resulting action should contain ``attrs``, which basically
    enables overwriting certain attributes, like the chamber if
    the action was listed in the wrong column.
    '''
    def __new__(_cls, regexes, types=None, stop=False, **kwargs):
        'Create new instance of Rule(regex, types, attrs, stop)'

        # Regexes can be a string or a sequence.
        if isinstance(regexes, basestring):
            regexes = set([regexes])
        regexes = set(regexes or [])

        # Types can be a string or a sequence.
        if isinstance(types, basestring):
            types = set([types])
        types = set(types or [])

        return tuple.__new__(_cls, (regexes, types, stop, kwargs))


class BaseCategorizer(object):
    '''A class that exposes a main categorizer function
    and before and after hooks, in case a state requires specific
    steps that make use of action or category info. The return
    value is a 2-tuple of category types and a dictionary of
    attributes to overwrite on the target action object.
    '''
    rules = []

    def __init__(self):
        before_funcs = []
        after_funcs = []
        for name in dir(self):
            attr = getattr(self, name)
            if isinstance(attr, MethodType):
                # func = partial(attr, self)
                func = attr
                if getattr(attr, 'before', None):
                    before_funcs.append(func)
                if getattr(attr, 'after', None):
                    after_funcs.append(func)
        self._before_funcs = before_funcs
        self._after_funcs = after_funcs

    def categorize(self, text):

        whitespace = partial(re.sub, '\s{1,4}', '\s{,4}')

        # Run the before hook.
        text = self.before_categorize(text)
        for func in self._before_funcs:
            text = func(text)

        types = set()
        attrs = defaultdict(set)
        for rule in self.rules:

            for regex in rule.regexes:

                # Try to match the regex.
                m = re.search(whitespace(regex), text)
                if m or (regex in text):
                    # If so, apply its associated types to this action.
                    types |= rule.types

                    # Also add its specified attrs.
                    for k, v in m.groupdict().items():
                        attrs[k].add(v)

                    for k, v in rule.attrs.items():
                        attrs[k].add(v)

                    # Break if the rule says so, otherwise
                    # continue testing against other rules.
                    if rule.stop is True:
                        break

        # Returns types, attrs
        return_val = (list(types), attrs)
        return_val = self.after_categorize(return_val)
        for func in self._after_funcs:
            return_val = func(*return_val)
        return self.finalize(return_val)

    def before_categorize(self, text):
        '''A precategorization hook. Takes/returns text.
        '''
        return text

    def after_categorize(self, return_val):
        '''A post-categorization hook. Takes, returns
        a tuple like (types, attrs), where types is a sequence
        of categories (e.g., bill:passed), and attrs is a
        dictionary of addition attributes that can be used to
        augment the action (or whatever).
        '''
        return return_val

    def finalize(self, return_val):
        '''Before the types and attrs get passed to the
        importer they need to be altered by converting lists to
        sets, etc.
        '''
        types, attrs = return_val
        _attrs = {}

        # Get rid of defaultdict.
        for k, v in attrs.items():

            # Skip empties.
            if not v:
                continue
            else:
                v = filter(None, v)

            # Get rid of sets.
            if isinstance(v, set):
                v = list(v)

            # Some vals should be strings, not seqs.
            if k == 'actor' and len(v) == 1:
                v = v.pop()

            _attrs[k] = v

        return types, _attrs


def after_categorize(f):
    '''A decorator to mark a function to be run
    before categorization has happened.
    '''
    f.after = True
    return f


def before_categorize(f):
    '''A decorator to mark a function to be run
    before categorization has happened.
    '''
    f.before = True
    return f


# These are regex patterns that map to action categories.
_categorizer_rules = (

    # Senate passage.
    Rule(r'(?i)^(RE)?PASSED', 'bill:passed'),
    Rule(r'(?i)^ADOPTED', 'bill:passed'),

    # Amended
    Rule(r'(?i)AMENDED (?P<bill_id>\d+)', 'amendment:passed'),
    Rule(r'(?i)AMEND AND RECOMMIT TO (?P<committees>.+)',
         ['amendment:passed', 'committee:referred']),
    Rule(r'(?i)amend .+? and recommit to (?P<committees>.+)',
         ['amendment:passed', 'committee:referred']),
    Rule(r'(?i)AMENDED ON THIRD READING (\(T\) )?(?P<bill_id>.+)',
         'amendment:passed'),
    Rule(r'(?i)print number (?P<bill_id>\d+)', 'amendment:passed'),
    Rule(r'(?i)tabled', 'amendment:tabled'),

    # Committees
    Rule(r'(?i)held .+? in (?P<committees>.+)', 'bill:failed'),
    Rule(r'(?i)REFERRED TO (?P<committees>.+)', 'committee:referred'),
    Rule(r'(?i)reference changed to (?P<committees>.+)',
          'committee:referred'),
    Rule(r'(?i) committed to (?P<committees>.+)', 'committee:referred'),
    Rule(r'(?i)^reported$'),

    # Governor
    Rule(r'(?i)signed chap.(?P<session_laws>\d+)', 'governor:signed'),
    Rule(r'(?i)vetoed memo.(?P<veto_memo>.+)', 'governor:vetoed'),
    Rule(r'(?i)DELIVERED TO GOVERNOR', 'governor:received'),

    # Random.
    Rule(r'(?i)substituted by (?P<bill_id>\w\d+)')
    )


class Categorizer(BaseCategorizer):
    rules = _categorizer_rules

########NEW FILE########
__FILENAME__ = bills
'''Note, this needs to scrape both assembly and senate sites. Neither
house has the other's votes, so you have to scrape both and merge them.
'''
import re
import datetime
from collections import defaultdict

from billy.utils import term_for_session
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

import scrapelib
import lxml.html
import lxml.etree

from .models import AssemblyBillPage, SenateBillPage
from .actions import Categorizer


class NYBillScraper(BillScraper):

    jurisdiction = 'ny'
    categorizer = Categorizer()

    def scrape(self, session, chambers):
        term_id = term_for_session('ny', session)
        for term in self.metadata['terms']:
            if term['name'] == term_id:
                break
        self.term = term
        for billset in self.yield_grouped_versions():
            self.scrape_bill(session, billset)

    def scrape_bill(self, session, bills):

        billdata, details = bills[0]

        (senate_url, assembly_url, bill_chamber, bill_type, bill_id,
         title, (letter, number, is_amd)) = details

        data = billdata['data']['bill']

        assembly = AssemblyBillPage(self, session, bill_chamber, details)
        assembly.build()
        bill = assembly.bill
        bill.add_source(billdata['url'])

        # Add companion.
        if data['sameAs']:
            bill.add_companion(data['sameAs'])

        if data['summary']:
            bill['summary'] = data['summary']

        if data['votes']:
            for vote_data in data['votes']:
                vote = Vote(
                    chamber='upper',
                    date=self.date_from_timestamp(vote_data['voteDate']),
                    motion=vote_data['description'] or '[No motion available.]',
                    passed=False,
                    yes_votes=[],
                    no_votes=[],
                    other_votes=[],
                    yes_count=0,
                    no_count=0,
                    other_count=0)

                for name in vote_data['ayes']:
                    vote.yes(name)
                    vote['yes_count'] += 1
                for names in map(vote_data.get, ['absent', 'excused', 'abstains']):
                    for name in names:
                        vote.other(name)
                        vote['other_count'] += 1
                for name in vote_data['nays']:
                    vote.no(name)
                    vote['no_count'] += 1

                bill.add_vote(vote)

        # if data['previousVersions']:
        #   These are instances of the same bill from prior sessions.
        #     import pdb; pdb.set_trace()

        if not data['title']:
            bill['title'] = bill['summary']

        self.save_bill(bill)

    def date_from_timestamp(self, timestamp):
        return datetime.datetime.fromtimestamp(int(timestamp) / 1000)

    def bill_id_details(self, billdata):
        data = billdata['data']['bill']
        api_id = billdata['oid']
        source_url = billdata['url']

        title = data['title'].strip()
        if not title:
            return

        # Parse the bill_id into beginning letter, number
        # and any trailing letters indicating its an amendment.
        bill_id, year = api_id.split('-')
        bill_id_rgx = r'(^[A-Z])(\d{,6})([A-Z]{,3})'
        bill_id_base = re.search(bill_id_rgx, bill_id)
        letter, number, is_amd = bill_id_base.groups()

        bill_chamber, bill_type = {
            'S': ('upper', 'bill'),
            'R': ('upper', 'resolution'),
            'J': ('upper', 'legislative resolution'),
            'B': ('upper', 'concurrent resolution'),
            'A': ('lower', 'bill'),
            'E': ('lower', 'resolution'),
            'K': ('lower', 'legislative resolution'),
            'L': ('lower', 'joint resolution')}[letter]

        senate_url = billdata['url']

        assembly_url = (
            'http://assembly.state.ny.us/leg/?'
            'default_fld=&bn=%s&Summary=Y&Actions=Y') % bill_id

        return (senate_url, assembly_url, bill_chamber, bill_type, bill_id,
                title, (letter, number, is_amd))


    def yield_api_bills(self):
        '''Yield individual versions. The caller can get all versions
        for a particular ID, process the group, then throw everything
        away and move onto the next ID.
        '''
        # The bill api object keys we'll actually use. Throw rest away.
        keys = set([
            'coSponsors', 'multiSponsors', 'sponsor', 'actions',
            'versions', 'votes', 'title', 'sameAs', 'summary'])

        index = 0
        bills = defaultdict(list)

        billdata = defaultdict(lambda: defaultdict(list))
        for year in (self.term['start_year'], self.term['end_year']):
            while True:
                index += 1
                url = (
                    'http://open.nysenate.gov/legislation/2.0/search.json'
                    '?term=otype:bill AND year:2013&pageSize=20&pageIdx=%d'
                    )
                url = url % index
                self.logger.info('GET ' + url)
                resp = self.get(url)

                data = resp.json()
                if not data['response']['results']:
                    break

                for bill in data['response']['results']:
                    billdata = bill['data']['bill']
                    for junk in set(billdata) - keys:
                        del billdata[junk]

                    details = self.bill_id_details(bill)
                    if details is None:
                        continue
                    (senate_url, assembly_url, bill_chamber, bill_type, bill_id,
                     title, (letter, number, is_amd)) = details

                    key = (letter, number)
                    yield key, bill, details

    def yield_grouped_versions(self):
        '''Generates a lists of versions grouped by bill id.
        '''
        prev_key = None
        versions = []
        for key, bill, details in self.yield_api_bills():
            if key is not prev_key and versions:
                yield versions
                versions = []
            versions.append((bill, details))
            prev_key = key



########NEW FILE########
__FILENAME__ = committees
import re

from billy.scrape import NoDataForPeriod
from billy.scrape.committees import CommitteeScraper, Committee

import lxml.html


def parse_name(name):
    """
    Split a committee membership string into name and role.

    >>> parse_name('Felix Ortiz')
    ('Felix Ortiz', 'member')
    >>> parse_name('Felix Ortiz (Chair)')
    ('Felix Ortiz', 'chair')
    >>> parse_name('Hon. Felix Ortiz, Co-Chair')
    ('Felix Ortiz', 'co-chair')
    >>> parse_name('Owen H.\\r\\nJohnson (Vice Chairperson)')
    ('Owen H. Johnson', 'vice chairperson')
    """
    name = re.sub(r'^(Hon\.|Assemblyman|Assemblywoman)\s+', '', name)
    name = re.sub(r'\s+', ' ', name)

    roles = ["Chairwoman", "Chairperson", "Chair", "Secretary", "Treasurer",
             "Parliamentarian", "Chaplain"]
    match = re.match(
        r'([^(]+),? \(?((Co|Vice)?-?\s*(%s))\)?' % '|'.join(roles),
        name)

    if match:
        name = match.group(1).strip(' ,')
        role = match.group(2).lower()
        return (name, role)
    return (name, 'member')


class NYCommitteeScraper(CommitteeScraper):
    jurisdiction = "ny"
    latest_only = True

    def scrape(self, chamber, term):
        getattr(self, 'scrape_' + chamber)()

    def scrape_lower(self, only_names=None):
        committees = []
        url = "http://assembly.state.ny.us/comm/"
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        for link in page.xpath("//a[contains(@href, 'sec=mem')]"):
            name = link.xpath("string(../strong)").strip()
            if 'Caucus' in name:
                continue

            url = link.attrib['href']

            committees.append(name)

            self.scrape_lower_committee(name, url)
        return committees

    def scrape_lower_committee(self, name, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)

        comm = Committee('lower', name)
        comm.add_source(url)
        seen = set()
        for link in page.xpath("//div[@class='commlinks']//a[contains(@href, 'mem')]"):

            member = link.text.strip()
            member = re.sub(r'\s+', ' ', member)

            name, role = parse_name(member)

            # Figure out if this person is the chair.
            if 'Chair' in link.xpath('../../preceding-sibling::div[1]/text()'):
                role = 'chair'
            else:
                role = 'member'

            if name not in seen:
                comm.add_member(name)
                seen.add(name)

        if comm['members']:
            self.save_committee(comm)

    def scrape_upper(self):
        committees = []
        url = "http://www.nysenate.gov/committees"
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        for h2 in page.xpath("//h2"):
            committee_types = [
                'Standing Committees',
                'Temporary Committees',
                'Task Forces & Other Entities'
            ]

            if h2.text not in committee_types:
                continue

            for link in h2.getparent().xpath(".//a[contains(@href, '/committee/')]"):
                name = link.text.strip()

                committees.append(name)
                self.scrape_upper_committee(name, link.attrib['href'])

        return committees

    def scrape_upper_committee(self, name, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)

        comm = Committee('upper', name)
        comm.add_source(url)

        member_div = page.xpath("//div[@class = 'committee-members']")[0]

        xpath = '//label[contains(., "Chair:")]/following-sibling::a/text()'
        chair = page.xpath(xpath)
        if chair:
            comm.add_member(chair.pop().strip(), 'chair')

        seen = set([member['name'] for member in comm['members']])
        for link in member_div.xpath(".//a"):
            if not link.text:
                try:
                    # On one vice chair, the text was nested differently.
                    member = link[0].tail.strip()
                except (IndexError, AttributeError):
                    continue
            else:
                member = link.text.strip()

            next_elem = link.getnext()
            if (next_elem is not None and
                next_elem.tag == 'a' and
                next_elem.attrib['href'] == link.attrib['href']):
                # Sometimes NY is cool and splits names across a
                # couple links
                member = "%s %s" % (member, next_elem.text.strip())

            member = re.sub(r'\s+', ' ', member)

            if member in seen or not member:
                continue
            seen.add(member)

            name, role = parse_name(member)
            comm.add_member(name, role)

        if comm['members']:
            self.save_committee(comm)

########NEW FILE########
__FILENAME__ = events
import re
import datetime as dt

from billy.scrape.events import EventScraper, Event

import pytz
import lxml.html

url = "http://assembly.state.ny.us/leg/?sh=hear"


class NYEventScraper(EventScraper):
    _tz = pytz.timezone('US/Eastern')
    jurisdiction = 'ny'

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def lower_parse_page(self, url, session):
        page = self.lxmlize(url)
        tables = page.xpath("//table[@class='pubhrgtbl']")
        date = None
        ctty = None
        chamber = 'other'
        for table in tables:
            metainf = {}
            rows = table.xpath(".//tr")
            for row in rows:
                tds = row.xpath("./*")
                if len(tds) < 2:
                    continue
                key, value = tds
                if key.tag == 'th':
                    date = key.text_content()
                    date = re.sub("\s+", " ", date)
                    date = re.sub(".*POSTPONED NEW DATE", "", date).strip()
                    ctty = value.xpath(".//strong")[0]
                    ctty = ctty.text_content()

                    chamber = 'other'
                    if "senate" in ctty.lower():
                        chamber = 'upper'
                    if "house" in ctty.lower():
                        chamber = 'lower'
                    if "joint" in ctty.lower():
                        chamber = 'joint'
                elif key.tag == 'td':
                    key = key.text_content().strip()
                    value = value.text_content().strip()
                    value = value.replace(u'\x96', '-')
                    value = re.sub("\s+", " ", value)
                    metainf[key] = value

            time = metainf['Time:']
            repl = {
                "A.M.": "AM",
                "P.M.": "PM",
            }
            drepl = {
                "Sept": "Sep"
            }
            for r in repl:
                time = time.replace(r, repl[r])

            for r in drepl:
                date = date.replace(r, drepl[r])

            time = re.sub("-.*", "", time)
            time = time.strip()

            year = dt.datetime.now().year

            date = "%s %s %s" % (
                date,
                year,
                time
            )

            if "tbd" in date.lower():
                continue

            date = date.replace(' PLEASE NOTE NEW TIME', '')

            # Check if the event has been postponed.
            postponed = 'POSTPONED' in date
            if postponed:
                date = date.replace(' POSTPONED', '')

            date_formats = ["%B %d %Y %I:%M %p", "%b. %d %Y %I:%M %p"]
            datetime = None
            for fmt in date_formats:
                try:
                    datetime = dt.datetime.strptime(date, fmt)
                except ValueError:
                    pass

            # If the datetime can't be parsed, bail.
            if datetime is None:
                return

            title_key = set(metainf) & set([
                'Public Hearing:', 'Summitt:', 'Roundtable:',
                'Public Roundtable:'])
            assert len(title_key) == 1, "Couldn't determine event title."
            title_key = list(title_key).pop()
            title = metainf[title_key]

            # If event was postponed, add a warning to the title.
            if postponed:
                title = 'POSTPONED: %s' % title

            event = Event(session, datetime, 'committee:meeting',
                          title,
                          location=metainf['Place:'],
                          contact=metainf['Contact:'])
            if 'Media Contact:' in metainf:
                event.update(media_contact=metainf['Media Contact:'])
            event.add_source(url)
            event.add_participant('host',
                                  ctty,
                                  'committee',
                                  chamber=chamber)

            self.save_event(event)

    def scrape(self, chamber, session):
        self.scrape_lower(chamber, session)
        self.scrape_upper(chamber, session)

    def scrape_lower(self, chamber, session):
        if chamber == 'other':
            self.lower_parse_page(url, session)

    def scrape_upper(self, chamber, session):
        if chamber != 'upper':
            return

        url = (r'http://open.nysenate.gov/legislation/2.0/search.json?'
               r'term=otype:meeting&pageSize=1000&pageIdx=%d')
        page_index = 1
        while True:
            resp = self.urlopen(url % page_index)
            if not resp.response.json():
                break
            if not resp.response.json()['response']['results']:
                break
            for obj in resp.response.json()['response']['results']:
                event = self.upper_scrape_event(chamber, session, obj)
                if event:
                    self.save_event(event)
            page_index += 1

    def upper_scrape_event(self, chamber, session, obj):
        meeting = obj['data']['meeting']
        date = int(meeting['meetingDateTime'])
        date = dt.datetime.fromtimestamp(date / 1000)
        if str(date.year) not in session:
            return
        description = 'Committee Meeting: ' + meeting['committeeName']
        event = Event(session, date, 'committee:meeting',
                      description=description,
                      location=meeting['location'] or 'No location given.')
        event.add_source(obj['url'])
        event.add_participant('chair', meeting['committeeChair'],
                              'legislator', chamber='upper')
        event.add_participant('host', meeting['committeeName'],
                              'committee', chamber='upper')

        rgx = r'([a-z]+)(\d+)'
        for bill in meeting['bills']:
            raw_id = bill['senateBillNo']
            bill_id = ' '.join(re.search(rgx, raw_id, re.I).groups())
            event.add_related_bill(
                bill_id, type='bill',
                description=bill['summary'] or 'No description given.')
        return event

########NEW FILE########
__FILENAME__ = legislators
# -*- coding: utf-8 -*-
import re
import itertools

from billy.scrape.legislators import LegislatorScraper, Legislator

import lxml.html



class NYLegislatorScraper(LegislatorScraper):
    jurisdiction = 'ny'

    def scrape(self, chamber, term):
        if chamber == 'upper':
            self.scrape_upper(term)
        else:
            self.scrape_lower(term)

    def scrape_upper(self, term):
        url = "http://www.nysenate.gov/senators"
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        xpath = (
            '//div[contains(@class, "views-row")]/'
            'div[contains(@class, "last-name")]/'
            'span[contains(@class, "field-content")]/a')
        for link in page.xpath(xpath):
            if link.text in (None, 'Contact', 'RSS'):
                continue
            name = link.text.strip()
            if name.lower().startswith('senate district'):
                continue

            district = link.xpath("string(../../../div[3]/span[1])")
            district = re.match(r"District (\d+)", district).group(1)

            photo_link = link.xpath("../../../div[1]/span/a/img")[0]
            photo_url = photo_link.attrib['src']

            legislator = Legislator(term, 'upper', district,
                                    name, party="Unknown",
                                    photo_url=photo_url)
            legislator.add_source(url)

            contact_link = link.xpath("../span[@class = 'contact']/a")[0]
            contact_url = contact_link.attrib['href']
            self.scrape_upper_offices(legislator, contact_url)

            legislator['url'] = contact_url.replace('/contact', '')

            self.save_legislator(legislator)

    def scrape_upper_offices(self, legislator, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        legislator.add_source(url)

        xpath = '//a[contains(@href, "profile-pictures")]/@href'
        legislator['photo_url'] = page.xpath(xpath).pop()

        email = page.xpath('//span[@class="spamspan"]')
        if email:
            email = email[0].text_content()
            email = email.replace(' [at] ', '@').replace(' [dot] ', '.')
            legislator['email'] = email

        dist_str = page.xpath("string(//div[@class = 'district'])")
        match = re.findall(r'\(([A-Za-z,\s]+)\)', dist_str)
        if match:
            match = match[0].split(', ')
            party_map = {'D': 'Democratic', 'R': 'Republican',
                         'WF': 'Working Families',
                         'C': 'Conservative',
                         'IP': 'Independence',
                        }
            parties = [party_map.get(p.strip(), p.strip()) for p in match
                       if p.strip()]
            if 'Republican' in parties:
                party = 'Republican'
                parties.remove('Republican')
            elif 'Democratic' in parties:
                party = 'Democratic'
                parties.remove('Democratic')
            legislator['roles'][0]['party'] = party
            legislator['roles'][0]['other_parties'] = parties

        try:
            span = page.xpath("//span[. = 'Albany Office']/..")[0]
            address = span.xpath("string(div[1])").strip()
            address += "\nAlbany, NY 12247"

            phone = span.xpath("div[@class='tel']/span[@class='value']")[0]
            phone = phone.text.strip()

            office = dict(
                    name='Capitol Office',
                    type='capitol', phone=phone,
                    fax=None, email=None,
                    address=address)
            legislator.add_office(**office)

        except IndexError:
            # Sometimes contact pages are just plain broken
            pass

        try:
            span = page.xpath("//span[. = 'District Office']/..")[0]
            address = span.xpath("string(div[1])").strip() + "\n"
            address += span.xpath(
                "string(span[@class='locality'])").strip() + ", "
            address += span.xpath(
                "string(span[@class='region'])").strip() + " "
            address += span.xpath(
                "string(span[@class='postal-code'])").strip()

            phone = span.xpath("div[@class='tel']/span[@class='value']")[0]
            phone = phone.text.strip()

            office = dict(
                    name='District Office',
                    type='district', phone=phone,
                    fax=None, email=None,
                    address=address)

            legislator.add_office(**office)
        except IndexError:
            # No district office yet?
            pass

    def scrape_lower(self, term):
        url = "http://assembly.state.ny.us/mem/?sh=email"
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        # full_names = []

        def _split_list_on_tag(lis, tag):
            data = []
            for entry in lis:
                if entry.attrib['class'] == tag:
                    yield data
                    data = []
                else:
                    data.append(entry)

        for row in _split_list_on_tag(page.xpath("//div[@id='mememailwrap']/*"),
                                      "emailclear"):

            try:
                name, district, email = row
            except ValueError:
                name, district = row
                email = None

            link = name.xpath(".//a[contains(@href, '/mem/')]")
            if link != []:
                link = link[0]
            else:
                link = None

            if email is not None:
            # XXX: Missing email from a record on the page
            # as of 12/11/12. -- PRT
                email = email.xpath(".//a[contains(@href, 'mailto')]")
                if email != []:
                    email = email[0]
                else:
                    email = None

            name = link.text.strip()
            if name == 'Assembly Members':
                continue

            # empty seats
            if 'Assembly District' in name:
                continue

            district = link.xpath("string(../following-sibling::"
                                  "div[@class = 'email2'][1])")
            district = district.rstrip('rthnds')

            # unicodedata.normalize didn't help here.
            if name == u'Sep\xfalveda, Luis':
                party_name = party_dict['Sepulveda, Luis']
            else:
                party_name = party_dict[name]

            leg_url = link.get('href')
            legislator = Legislator(term, 'lower', district, name,
                                    party=party_name,
                                    url=leg_url)
            legislator.add_source(url)

            # Legislator
            self.scrape_lower_offices(leg_url, legislator)

            if email is not None:
                email = email.text_content().strip()
                if email:
                    legislator['email'] = email

            self.save_legislator(legislator)

    def scrape_lower_offices(self, url, legislator):
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        try:
            contact = doc.xpath('//div[@id="addrinfo"]')[0]
        except:
            contact = doc
        email = None

        # Sometimes class is "addrcol1", others "addrcola"
        col_generators = [

            # Try alpha second.
            iter('abcedef'),

            # Try '' first, then digits.
            itertools.chain(iter(['']), iter(xrange(1, 5)))
            ]

        cols = col_generators.pop()
        while True:

            # Get the column value.
            try:
                col = cols.next()
            except StopIteration:
                try:
                    cols = col_generators.pop()
                except IndexError:
                    break
                else:
                    continue

            xpath = 'div[@class="addrcol%s"]' % str(col)
            address_data = contact.xpath(xpath)
            if not address_data:
                continue

            for data in address_data:
                data = (data.xpath('div[@class="officehdg"]/text()'),
                        data.xpath('div[@class="officeaddr"]/text()'))
                ((office_name,), address) = data

                if 'district' in office_name:
                    office_type = 'district'
                else:
                    office_type = 'capitol'

                # Phone can't be blank.
                phone = address.pop().strip()
                if not phone:
                    phone = None

                office = dict(
                    name=office_name, type=office_type, phone=phone,
                    fax=None, email=email,
                    address=''.join(address).strip())

                if not office['address']:
                    # Congrat's Maritza Davila, you have your own special
                    # exception in the code for people who have magical
                    # district offices with no mailing address.
                    # http://assembly.state.ny.us/mem/Maritza-Davila
                    continue

                legislator.add_office(**office)

# Map ID's to party affiliation. Has to be an id-to-party mapping, because
# full_name gets normalized on import and may be different at scrape time
# than at the time get_parties_dict.py is run (which uses post-import data).
party_dict = {

    'Abinanti, Thomas': 'Democratic',        'Skoufis, James': 'Democratic',

    'Magnarelli, William': 'Democratic',     'McDonough, David': 'Republican',

    'Hevesi, Andrew': 'Democratic',          'Hooper, Earlene': 'Democratic',

    'Blankenbush, Ken': 'Republican',        'Kellner, Micah': 'Democratic',

    'Camara, Karim': 'Democratic',           'Gottfried, Richard': 'Democratic',

    u'Rivera, Jos\xe9': 'Democratic',        'Otis, Steven': 'Democratic',

    'Graf, Al': 'Republican',                'Stirpe, Al': 'Democratic',

    'Crespo, Marcos': 'Democratic',          'Rodriguez, Robert': 'Democratic',

    'Raia, Andrew': 'Republican',            'Thiele, Jr., Fred': 'Democratic',

    'Moya, Francisco': 'Democratic',         'Titone, Matthew': 'Democratic',

    'McDonald, III, John': 'Democratic',     'Saladino, Joseph': 'Republican',

    'Crouch, Clifford': 'Republican',        'Rabbitt, Annie': 'Republican',

    'Steck, Phil': 'Democratic',             'Stevenson, Eric': 'Democratic',

    'Cusick, Michael': 'Democratic',         'Rosa, Gabriela': 'Democratic',

    'Roberts, Samuel': 'Democratic',         'Aubry, Jeffrion': 'Democratic',

    'Brindisi, Anthony': 'Democratic',       'Galef, Sandy': 'Democratic',

    'Lentol, Joseph': 'Democratic',          'Curran, Brian': 'Republican',

    'Perry, N. Nick': 'Democratic',          'Tedisco, James': 'Republican',

    'Lifton, Barbara': 'Democratic',         'Ramos, Phil': 'Democratic',

    'Oaks, Bob': 'Republican',               'Lupinacci, Chad': 'Republican',

    'Pretlow, J. Gary': 'Democratic',        'Miller, Michael': 'Democratic',

    'Rozic, Nily': 'Democratic',             'Walter, Raymond': 'Republican',

    'Brennan, James': 'Democratic',          'Skartados, Frank': 'Democratic',

    'Espinal, Jr., Rafael': 'Democratic',    'Gibson, Vanessa': 'Democratic',

    'Butler, Marc': 'Republican',            'Farrell, Jr., Herman': 'Democratic',

    'Mayer, Shelley': 'Democratic',          'Lupardo, Donna': 'Democratic',

    'Sepulveda, Luis': 'Democratic',         'Titus, Michele': 'Democratic',

    'Garbarino, Andrew': 'Republican',       'Finch, Gary': 'Republican',

    'Borelli, Joseph': 'Republican',         'Millman, Joan': 'Democratic',

    'Barron, Inez': 'Democratic',            'Malliotakis, Nicole': 'Republican',

    'Kolb, Brian M.': 'Republican',          'Wright, Keith L.T.': 'Democratic',

    'Weinstein, Helene': 'Democratic',       'Tenney, Claudia': 'Republican',

    'Englebright, Steve': 'Democratic',      'Fahy, Patricia': 'Democratic',

    'Maisel, Alan': 'Democratic',            'Kavanagh, Brian': 'Democratic',

    'Peoples-Stokes, Crystal': 'Democratic', 'Goldfeder, Phillip': 'Democratic',

    'Solages, Michaelle': 'Democratic',      'Braunstein, Edward': 'Democratic',

    'Simanowitz, Michael': 'Democratic',     'Rosenthal, Linda': 'Democratic',

    'Glick, Deborah': 'Democratic',          'Lavine, Charles': 'Democratic',

    'Giglio, Joseph': 'Republican',          'Buchwald, David': 'Democratic',

    'Magee, William': 'Democratic',          'Jordan, Tony': 'Republican',

    'Duprey, Janet': 'Republican',           'Schimminger, Robin': 'Democratic',

    'Friend, Christopher': 'Republican',     'Reilich, Bill': 'Republican',

    'Stec, Dan': 'Republican',               'Barrett, Didi': 'Democratic',

    'Gjonaj, Mark': 'Democratic',            'Ceretto, John': 'Republican',

    u'Ortiz, F\xe9lix': 'Democratic',        'Morelle, Joseph': 'Democratic',

    'Nojay, Bill': 'Republican',             'Heastie, Carl': 'Democratic',

    'Arroyo, Carmen': 'Democratic',          'Cook, Vivian': 'Democratic',

    'Cahill, Kevin': 'Democratic',           'Zebrowski, Kenneth': 'Democratic',

    'DiPietro, David': 'Republican',         'Quart, Dan': 'Democratic',

    'Hikind, Dov': 'Democratic',             'Hennessey, Edward': 'Democratic',

    'Johns, Mark': 'Republican',             'Kim, Ron': 'Democratic',

    'McLaughlin, Steven': 'Republican',      'Montesano, Michael': 'Republican',

    'Losquadro, Dan': 'Republican',          'Sweeney, Robert': 'Democratic',

    'Robinson, Annette': 'Democratic',       'Bronson, Harry': 'Democratic',

    'Cymbrowitz, Steven': 'Democratic',      'Palmesano, Philip': 'Republican',

    'Corwin, Jane': 'Republican',            'Markey, Margaret': 'Democratic',

    'Dinowitz, Jeffrey': 'Democratic',       'Gunther, Aileen': 'Democratic',

    'Castro, Nelson': 'Democratic',          'Scarborough, William': 'Democratic',

    'Lopez, Vito': 'Democratic',             'Goodell, Andy': 'Republican',

    'Russell, Addie': 'Democratic',          'Mosley, Walter': 'Democratic',

    'Ra, Edward': 'Republican',              'Weisenberg, Harvey': 'Democratic',

    'Gantt, David': 'Democratic',            'Jaffee, Ellen': 'Democratic',

    'Santabarbara, Angelo': 'Democratic',    'Brook-Krasny, Alec': 'Democratic',

    'Katz, Steve': 'Republican',             'Barclay, William': 'Republican',

    'Weprin, David': 'Democratic',           'Gabryszak, Dennis': 'Democratic',

    'Silver, Sheldon': 'Democratic',         'Lalor, Kieran Michael': 'Republican',

    "O'Donnell, Daniel": 'Democratic',       'Colton, William': 'Democratic',

    'Abbate, Jr., Peter': 'Democratic',      'Simotas, Aravella': 'Democratic',

    'Boyland, Jr., William': 'Democratic',   'Jacobs, Rhoda': 'Democratic',

    'Fitzpatrick, Michael': 'Republican',    'DenDekker, Michael': 'Democratic',

    'Paulin, Amy': 'Democratic',             'Schimel, Michelle': 'Democratic',

    'Benedetto, Michael': 'Democratic',      'Ryan, Sean': 'Democratic',

    'Kearns, Michael': 'Democratic',         'Hawley, Stephen': 'Republican',

    'McKevitt, Tom': 'Republican',           'Lopez, Peter': 'Republican',

    'Clark, Barbara': 'Democratic',          'Nolan, Catherine': 'Democratic',

    'Davila, Maritza': 'Democratic',         'Pichardo, Victor': 'Democratic',

    'Palumbo, Anthony H.': 'Republican',
    }


########NEW FILE########
__FILENAME__ = models
'''OVERENGINEERING FTW!
'''
import re
import inspect
import datetime
import collections
from itertools import islice

import lxml.html

from billy.scrape.bills import Bill
from billy.scrape.votes import Vote
from billy.utils import term_for_session, metadata

from .utils import Urls, CachedAttr


class BasePageyThing(object):
    metadata = metadata('ny')
    chamber_name = None
    def __init__(self, scraper, session, chamber, details):
        (senate_url, assembly_url, bill_chamber, bill_type, bill_id,
          title, bill_id_parts) = details

        self.scraper = scraper
        self.session = session
        self.chamber = chamber
        self.data = {}
        self.bill = Bill(session, bill_chamber, bill_id, title, type=bill_type)

        self.term = term_for_session('ny', session)
        for data in self.metadata['terms']:
            if session in data['sessions']:
                self.termdata = data
            self.term_start_year = data['start_year']

        self.assembly_url = assembly_url
        self.senate_url = senate_url
        self.bill_chamber = bill_chamber
        self.bill_type = bill_type
        self.bill_id = bill_id
        self.title = title
        self.letter, self.number, self.version = bill_id_parts

        self.urls = Urls(scraper=self.scraper, urls={
            'assembly': assembly_url,
            'senate': senate_url})

    def build(self):
        '''Run all the build_* functions.
        '''
        for name, member in inspect.getmembers(self):
            if inspect.ismethod(member):
                if name.startswith('build_'):
                    key = re.sub(r'^build_', '', name)
                    member()


class AssemblyBillPage(BasePageyThing):
    '''Get the actions, sponsors, sponsors memo and summary
    and assembly floor votes from the assembly page.
    '''
    @CachedAttr
    def chunks(self):
        url = ('http://assembly.state.ny.us/leg/?default_fld=&'
               'bn=%s&Summary=Y&Actions=Y&term=%s')
        url = url % (self.bill_id, self.term_start_year)
        self.urls.add(summary=url)
        self.bill.add_source(url)
        summary, actions = self.urls.summary.xpath('//pre')[:2]
        summary = summary.text_content()
        actions = actions.text_content()
        self.data['summary'] = summary
        self.data['actions'] = actions
        return summary, actions

    def build_version(self):
        url = 'http://assembly.state.ny.us/leg/?sh=printbill&bn=%s&term=%s'
        url = url % (self.bill_id, self.term_start_year)
        version = self.bill_id
        self.bill.add_version(version, url, mimetype='text/html')

    def build_companions(self):
        summary, _ = self.chunks
        chunks = summary.split('\n\n')
        for chunk in chunks:
            if chunk.startswith('SAME AS'):
                companions = chunk.replace('SAME AS    ', '')
                if companions != 'No same as':
                    for companion in re.split(r'\s*[\,\\]\s*', companions):
                        companion = re.sub(r'^(?i)Same as ', '', companion)
                        companion = re.sub(r'^Uni', '', companion)
                        companion = re.sub(r'\-\w+$', '', companion)
                        self.bill.add_companion(companion)

    def build_sponsors_memo(self):
        if self.chamber == 'lower':
            url = ('http://assembly.state.ny.us/leg/?'
                   'default_fld=&bn=%s&term=%s&Memo=Y')
            url = url % (self.bill_id, self.term_start_year)
            self.bill.add_document("Sponsor's Memorandum", url)

    def build_summary(self):
        summary, _ = self.chunks
        chunks = summary.split('\n\n')
        self.bill['summary'] = ' '.join(chunks[-1].split())

    def _scrub_name(self, name):
        junk = [
            r'^Rules\s+',
            '\(2nd Vice Chairperson\)',
            '\(MS\)',
            'Assemblyman',
            'Assemblywoman',
            'Senator']
        for rgx in junk:
            name = re.sub(rgx, '', name, re.I)

        # Collabpse whitespace.
        name = re.sub('\s+', ' ', name)
        return name.strip('(), ')

    def build_sponsors(self):
        summary, _ = self.chunks
        chunks = summary.split('\n\n')
        for chunk in chunks:
            for sponsor_type in ('SPONSOR', 'COSPNSR', 'MLTSPNSR'):
                if chunk.startswith(sponsor_type):
                    _, data = chunk.split(' ', 1)
                    for sponsor in re.split(r',\s+', data.strip()):

                        if not sponsor:
                            continue

                        # If it's a "Rules" bill, add the Rules committee
                        # as the primary.
                        if sponsor.startswith('Rules'):
                            self.bill.add_sponsor('primary', 'Rules Committee',
                                                  chamber='lower')

                        sponsor = self._scrub_name(sponsor)

                        # Figure out sponsor type.
                        spons_swap = {'SPONSOR': 'primary'}
                        _sponsor_type = spons_swap.get(
                            sponsor_type, 'cosponsor')

                        self.bill.add_sponsor(_sponsor_type, sponsor.strip(),
                                         official_type=sponsor_type)

    def build_actions(self):
        _, actions = self.chunks
        categorizer = self.scraper.categorizer
        actions_rgx = r'(\d{2}/\d{2}/\d{4})\s+(.+)'
        actions_data = re.findall(actions_rgx, actions)
        for date, action in actions_data:
            date = datetime.datetime.strptime(date, r'%m/%d/%Y')
            act_chamber = ('upper' if action.isupper() else 'lower')
            types, attrs = categorizer.categorize(action)
            self.bill.add_action(act_chamber, action, date, type=types, **attrs)
            # Bail if the bill has been substituted by another.
            if 'substituted by' in action:
                return

    def build_lower_votes(self):

        url = ('http://assembly.state.ny.us/leg/?'
               'default_fld=&bn=%s&term=%s&Votes=Y')
        url = url % (self.bill_id, self.term_start_year)
        self.urls.add(votes=url)
        self.bill.add_source(url)
        doc = self.urls.votes.doc
        if doc is None:
            return

        pre = doc.xpath('//pre')[0].text_content()
        no_votes = ('There are no votes for this bill in this '
                    'legislative session.')
        if pre == no_votes:
            return

        actual_vote = collections.defaultdict(list)
        for table in doc.xpath('//table'):

            date = table.xpath('caption/label[contains(., "DATE:")]')
            date = date[0].itersiblings().next().text
            date = datetime.datetime.strptime(date, '%m/%d/%Y')

            votes = table.xpath('caption/span/label[contains(., "YEA/NAY:")]')
            votes = votes[0].itersiblings().next().text
            yes_count, no_count = map(int, votes.split('/'))

            passed = yes_count > no_count
            vote = Vote('lower', date, 'Floor Vote', passed, yes_count,
                        no_count, other_count=0)

            tds = table.xpath('tr/td/text()')
            votes = iter(tds)
            while True:
                try:
                    data = list(islice(votes, 2))
                    name, vote_val = data
                except (StopIteration, ValueError):
                    # End of data. Stop.
                    break
                name = self._scrub_name(name)

                if vote_val.strip() == 'Y':
                    vote.yes(name)
                elif vote_val.strip() in ('N', 'NO'):
                    vote.no(name)
                else:
                    vote.other(name)
                    actual_vote[vote_val].append(name)

            # The page doesn't provide an other_count.
            vote['other_count'] = len(vote['other_votes'])
            vote['actual_vote'] = actual_vote
            self.bill.add_vote(vote)


class SenateBillPage(object):
    '''Used for categories, senate votes, events.'''

    def __init__(self, scraper, session, chamber, bill, details):
        (senate_url, assembly_url, bill_chamber, bill_type, bill_id,
          title, (letter, number, is_amd)) = details

    def build_subjects(self):
        subjects = []
        for link in self.doc.xpath("//a[contains(@href, 'lawsection')]"):
            subjects.append(link.text.strip())

        self.bill['subjects'] = subjects

    def build_sponsors_memo(self):
        if self.chamber == 'upper':
            self.bill.add_document("Sponsor's Memorandum", self.url)

    def build_senate_votes(self):
        xpath = "//div/b[starts-with(., 'VOTE: FLOOR VOTE:')]"
        for b in self.urls.senate.xpath(xpath):
            date = b.text.split('-')[1].strip()
            date = datetime.datetime.strptime(date, "%b %d, %Y").date()

            yes_votes, no_votes, other_votes = [], [], []
            yes_count, no_count, other_count = 0, 0, 0
            actual_vote = collections.defaultdict(list)

            vtype = None
            for tag in b.xpath("following-sibling::blockquote/*"):
                if tag.tag == 'b':
                    text = tag.text
                    if text.startswith('Ayes'):
                        vtype = 'yes'
                        yes_count = int(re.search(
                            r'\((\d+)\):', text).group(1))
                    elif text.startswith('Nays'):
                        vtype = 'no'
                        no_count = int(re.search(
                            r'\((\d+)\):', text).group(1))
                    elif (text.startswith('Excused') or
                          text.startswith('Abstain') or
                          text.startswith('Absent')
                         ):
                        vtype = 'other'
                        other_count += int(re.search(
                            r'\((\d+)\):', text).group(1))
                    else:
                        raise ValueError('bad vote type: %s' % tag.text)
                elif tag.tag == 'a':
                    name = tag.text.strip()
                    if vtype == 'yes':
                        yes_votes.append(name)
                    elif vtype == 'no':
                        no_votes.append(name)
                    elif vtype == 'other':
                        other_votes.append((name, tag.text))

            passed = yes_count > (no_count + other_count)

            vote = Vote('upper', date, 'Floor Vote', passed, yes_count,
                        no_count, other_count)

            for name in yes_votes:
                vote.yes(name)
            for name in no_votes:
                vote.no(name)
            for name, vote_val in other_votes:
                vote.other(name)
                actual_vote[vote_val].append(name)

            vote['actual_vote'] = actual_vote
            vote.add_source(self.url)
            self.bill.add_vote(vote)

        xpath = "//div/b[starts-with(., 'VOTE: COMMITTEE VOTE:')]"
        for b in self.urls.senate.xpath(xpath):
            _, committee, date = re.split(r'\s*\t+\s*-\s*', b.text)
            date = date.strip()
            date = datetime.datetime.strptime(date, "%b %d, %Y").date()

            yes_votes, no_votes, other_votes = [], [], []
            yes_count, no_count, other_count = 0, 0, 0

            vtype = None
            for tag in b.xpath("following-sibling::blockquote/*"):
                if tag.tag == 'b':
                    text = tag.text
                    if text.startswith('Ayes'):
                        vtype = 'yes'
                        yes_count += int(re.search(
                            r'\((\d+)\):', text).group(1))
                    elif text.startswith('Nays'):
                        vtype = 'no'
                        no_count += int(re.search(
                            r'\((\d+)\):', text).group(1))
                    elif (text.startswith('Excused') or
                          text.startswith('Abstain') or
                          text.startswith('Absent')
                         ):
                        vtype = 'other'
                        other_count += int(re.search(
                            r'\((\d+)\):', text).group(1))
                    else:
                        raise ValueError('bad vote type: %s' % tag.text)
                elif tag.tag == 'a':
                    name = tag.text.strip()
                    if vtype == 'yes':
                        yes_votes.append(name)
                    elif vtype == 'no':
                        no_votes.append(name)
                    elif vtype == 'other':
                        other_votes.append(name)

            passed = yes_count > (no_count + other_count)

            vote = Vote('upper', date, '%s Committee Vote' % committee,
                        passed, yes_count, no_count, other_count)

            for name in yes_votes:
                vote.yes(name)
            for name in no_votes:
                vote.no(name)
            for name in other_votes:
                vote.other(name)

            vote.add_source(self.url)
            self.bill.add_vote(vote)

    def build_versions(self):
        xpath = '//*[contains(., "Versions")]'
        text = self.urls.senate.xpath(xpath)[-1].text_content()
        version_text = re.sub('Versions:?\s*', '', text)

        url_tmpl = 'http://open.nysenate.gov/legislation/bill/'
        for version_bill_id in re.findall('\S+', version_text):
            version_bill_id_noyear, _ = version_bill_id.rsplit('-')
            version_url = url_tmpl + version_bill_id
            self.bill.add_version(version_bill_id_noyear, version_url,
                                  mimetype='text/html')

########NEW FILE########
__FILENAME__ = get_parties_dict
'''
This module parses the pdb of assembly contact info to get each member's
party affiliation. It's prinst out a matching dictionary that can be
(shudder) pasted into the NY legislator scraper. It's output needs to be
double checked though, because it uses fuzzy matching to match names on the
sheet to each legislator's full_name.
'''
import re
import lxml.html
import difflib

from billy.scrape.utils import convert_pdf


def getname(string):
    _, string = re.split(r'\s*\d+\s+', string, 1)
    name, _ = re.split(r'\.{4,}', string, 1)
    return name.strip()


def main():
    html = convert_pdf('openstates/ny/scripts/assembly_parties.pdf')
    doc = lxml.html.fromstring(html)
    dems = doc.xpath('//text[@font="4"]/b/text()')
    dems = map(getname, dems)
    repubs = doc.xpath('//text[@font="5"]/i/b/text()')
    repubs = map(getname, repubs)

    name_to_party = {}
    for list_, party in ((dems, 'Democratic'), (repubs, 'Republican')):
        for name in list_:
            print name, 'matched',
            try:
                full_name = difflib.get_close_matches(name, legs).pop(0)
            except IndexError:
                print 'NO MATCH FOUND'
                continue
            print full_name, ':: party = ', party
            name_to_party[full_name] = party
            print party

    # import pprint
    # pprint.pprint(name_to_party)

    print 'party_dict = {'
    it = iter(name_to_party.items())
    while True:

        try:
            # Col 1
            full_name1, party1 = next(it)
            full_name2, party2 = next(it)

            print ('\n    %r: %r,' % (full_name1, party1)).ljust(45),
            print ('%r: %r,' % (full_name2, party2))
        except StopIteration:
            break

    print '    }'

legs = ['Abbate, Jr., Peter',
 'Abinanti, Thomas',
 'Arroyo, Carmen',
 'Aubry, Jeffrion',
 'Barclay, William',
 'Barrett, Didi',
 'Barron, Inez',
 'Benedetto, Michael',
 'Blankenbush, Ken',
 'Borelli, Joseph',
 'Boyland, Jr., William',
 'Braunstein, Edward',
 'Brennan, James',
 'Brindisi, Anthony',
 'Bronson, Harry',
 'Brook-Krasny, Alec',
 'Buchwald, David',
 'Butler, Marc',
 'Cahill, Kevin',
 'Camara, Karim',
 'Castro, Nelson',
 'Ceretto, John',
 'Clark, Barbara',
 'Colton, William',
 'Cook, Vivian',
 'Corwin, Jane',
 'Crespo, Marcos',
 'Crouch, Clifford',
 'Curran, Brian',
 'Cusick, Michael',
 'Cymbrowitz, Steven',
 'DenDekker, Michael',
 'Dinowitz, Jeffrey',
 'DiPietro, David',
 'Duprey, Janet',
 'Englebright, Steve',
 'Espinal, Jr., Rafael',
 'Fahy, Patricia',
 'Farrell, Jr., Herman',
 'Finch, Gary',
 'Fitzpatrick, Michael',
 'Friend, Christopher',
 'Gabryszak, Dennis',
 'Galef, Sandy',
 'Gantt, David',
 'Garbarino, Andrew',
 'Gibson, Vanessa',
 'Giglio, Joseph',
 'Gjonaj, Mark',
 'Glick, Deborah',
 'Goldfeder, Phillip',
 'Goodell, Andy',
 'Gottfried, Richard',
 'Graf, Al',
 'Gunther, Aileen',
 'Hawley, Stephen',
 'Heastie, Carl',
 'Hennessey, Edward',
 'Hevesi, Andrew',
 'Hikind, Dov',
 'Hooper, Earlene',
 'Jacobs, Rhoda',
 'Jaffee, Ellen',
 'Johns, Mark',
 'Jordan, Tony',
 'Katz, Steve',
 'Kavanagh, Brian',
 'Kearns, Michael',
 'Kellner, Micah',
 'Kim, Ron',
 'Kolb, Brian M.',
 'Lalor, Kieran Michael',
 'Lavine, Charles',
 'Lentol, Joseph',
 'Lifton, Barbara',
 'Lopez, Peter',
 'Lopez, Vito',
 'Losquadro, Dan',
 'Lupardo, Donna',
 'Lupinacci, Chad',
 'Magee, William',
 'Magnarelli, William',
 'Maisel, Alan',
 'Malliotakis, Nicole',
 'Markey, Margaret',
 'Mayer, Shelley',
 'McDonald, III, John',
 'McDonough, David',
 'McKevitt, Tom',
 'McLaughlin, Steven',
 'Miller, Michael',
 'Millman, Joan',
 'Montesano, Michael',
 'Morelle, Joseph',
 'Mosley, Walter',
 'Moya, Francisco',
 'Nojay, Bill',
 'Nolan, Catherine',
 "O'Donnell, Daniel",
 'Oaks, Bob',
 u'Ortiz, F\xe9lix',
 'Otis, Steven',
 'Palmesano, Philip',
 'Paulin, Amy',
 'Peoples-Stokes, Crystal',
 'Perry, N. Nick',
 'Pretlow, J. Gary',
 'Quart, Dan',
 'Ra, Edward',
 'Rabbitt, Annie',
 'Raia, Andrew',
 'Ramos, Phil',
 'Reilich, Bill',
 u'Rivera, Jos\xe9',
 'Roberts, Samuel',
 'Robinson, Annette',
 'Rodriguez, Robert',
 'Rosa, Gabriela',
 'Rosenthal, Linda',
 'Rozic, Nily',
 'Russell, Addie',
 'Ryan, Sean',
 'Saladino, Joseph',
 'Santabarbara, Angelo',
 'Scarborough, William',
 'Schimel, Michelle',
 'Schimminger, Robin',
 'Sepulveda, Luis',
 'Silver, Sheldon',
 'Simanowitz, Michael',
 'Simotas, Aravella',
 'Skartados, Frank',
 'Skoufis, James',
 'Solages, Michaelle',
 'Stec, Dan',
 'Steck, Phil',
 'Stevenson, Eric',
 'Stirpe, Al',
 'Sweeney, Robert',
 'Tedisco, James',
 'Tenney, Claudia',
 'Thiele, Jr., Fred',
 'Titone, Matthew',
 'Titus, Michele',
 'Walter, Raymond',
 'Weinstein, Helene',
 'Weisenberg, Harvey',
 'Weprin, David',
 'Wright, Keith L.T.',
 'Zebrowski, Kenneth']


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = utils
import collections

import lxml.html


class CachedAttr(object):
    '''Computes attribute value and caches it in instance.

    Example:
        class MyClass(object):
            def myMethod(self):
                # ...
            myMethod = CachedAttribute(myMethod)
    Use "del inst.myMethod" to clear cache.'''

    def __init__(self, method, name=None):
        self.method = method
        self.name = name or method.__name__

    def __get__(self, inst, cls):
        if inst is None:
            return self
        result = self.method(inst)
        setattr(inst, self.name, result)
        return result


class UrlData(object):
    '''Given a url, its nickname, and a scraper instance,
    provide the parsed lxml doc, the raw html, and the url
    '''
    def __init__(self, name, url, scraper, urls_object):
        '''urls_object is a reference back to the Urls container.
        '''
        self.url = url
        self.name = name
        self.scraper = scraper
        self.urls_object = urls_object

    def __repr__(self):
        return 'UrlData(url=%r)' % self.url

    @CachedAttr
    def text(self):
        text = self.scraper.urlopen(self.url)
        self.urls_object.validate(self.name, self.url, text)
        return text

    @CachedAttr
    def resp(self):
        '''Return the decoded html or xml or whatever. sometimes
        necessary for a quick "if 'page not found' in html:..."
        '''
        return self.text.response

    @CachedAttr
    def doc(self):
        '''Return the page's lxml doc.
        '''
        doc = lxml.html.fromstring(self.text)
        doc.make_links_absolute(self.url)
        return doc

    @CachedAttr
    def xpath(self):
        return self.doc.xpath

    @CachedAttr
    def pdf_to_lxml(self):
        filename, resp = self.scraper.urlretrieve(self.url)
        text = convert_pdf(filename, 'html')
        return lxml.html.fromstring(text)

    @CachedAttr
    def etree(self):
        '''Return the documents element tree.
        '''
        return lxml.etree.fromstring(self.text)


class UrlsMeta(type):
    '''This metaclass aggregates the validator functions marked
    using the Urls.validate decorator.
    '''
    def __new__(meta, name, bases, attrs):
        '''Just aggregates the validator methods into a defaultdict
        and stores them on cls._validators.
        '''
        validators = collections.defaultdict(set)
        for attr in attrs.values():
            if hasattr(attr, 'validates'):
                validators[attr.validates].add(attr)
        attrs['_validators'] = validators
        cls = type.__new__(meta, name, bases, attrs)
        return cls


class Urls(object):
    '''Contains urls we need to fetch during this scrape.
    '''
    __metaclass__ = UrlsMeta

    def __init__(self, scraper, urls):
        '''Sets a UrlData object on the instance for each named url given.
        '''
        self.urls = urls
        self.scraper = scraper
        for name, url in urls.items():
            url = UrlData(name, url, scraper, urls_object=self)
            setattr(self, name, url)

    def __repr__(self):
        return '%s(%r)' % (self.__class__.__name__, self.urls)

    def __iter__(self):
        '''A generator of this object's UrlData members.
        '''
        for name in self.urls:
            yield getattr(self, name)

    def add(self, **name_to_url_map):
        for name, url in name_to_url_map.items():
            url_data = UrlData(name, url, self.scraper, urls_object=self)
        setattr(self, name, url_data)

    @staticmethod
    def validates(name, retry=False):
        '''A decorator to mark validator functions for use on a particular
        named url. Use like so:

        @Urls.validates('history')
        def must_have_actions(self, url, text):
            'Skip bill that hasn't been introduced yet.'
            if 'no actions yet' in text:
                raise Skip('Bill had no actions yet.')
        '''
        def decorator(method):
            method.validates = name
            method.retry = retry
            return method
        return decorator

    def validate(self, name, url, text):
        '''Run each validator function for the named url and its text.
        '''
        for validator in self._validators[name]:
            try:
                validator(self, url, text)
            except Exception as e:
                if validator.retry:
                    validator(self, url, text)
                else:
                    raise e

########NEW FILE########
__FILENAME__ = bills
import os
import urlparse
import datetime

from billy.scrape import ScrapeError
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

import xlrd
import scrapelib
import lxml.html


class OHBillScraper(BillScraper):
    jurisdiction = 'oh'

    def scrape(self, chamber, session):
        if int(session) < 128:
            raise NoDataForPeriod(session)

        base_url = 'http://www.lsc.state.oh.us/status%s/' % session

        bill_types = {'lower': [('hb','bill'),
                                ('hjr','joint resolution'),
                                ('hcr','concurrent resolution')],
                      'upper': [('sb','bill'),
                                ('sjr','joint resolution'),
                                ('scr','concurrent resolution')]}

        for bill_prefix, bill_type in bill_types[chamber]:
            url = base_url + '%s.xlsx' % bill_prefix

            try:
                fname, resp = self.urlretrieve(url)
            except scrapelib.HTTPError:
                # if there haven't yet been any bills of a given type
                # then the excel url for that type will 404
                continue

            sh = xlrd.open_workbook(fname).sheet_by_index(0)

            # once workbook is open, we can remove tempfile
            os.remove(fname)

            for rownum in range(1, sh.nrows):
                bill_no = sh.cell(rownum, 0).value
                if isinstance(bill_no, basestring):
                    if "*" in bill_no:
                        continue

                bill_no = int(bill_no)
                bill_id = '%s %s' % (bill_prefix.upper(), bill_no)
                bill_title = str(sh.cell(rownum, 3).value)
                bill = Bill(session, chamber, bill_id, bill_title,
                            type=bill_type)
                bill.add_source(url)
                bill.add_sponsor('primary', str(sh.cell(rownum, 1).value))

                # add cosponsor
                if sh.cell(rownum, 2).value:
                    bill.add_sponsor('cosponsor',
                                     str(sh.cell(rownum, 2).value))

                actor = ""

                # Actions start column after bill title
                for colnum in range(4, sh.ncols - 1):
                    action = str(sh.cell(0, colnum).value)
                    cell = sh.cell(rownum, colnum)
                    date = cell.value

                    if len(action) != 0:
                        if action.split()[0] == 'House':
                            actor = "lower"
                        elif action.split()[0] == 'Senate':
                            actor = "upper"
                        elif action.split()[-1] == 'Governor':
                            actor = "executive"
                        elif action.split()[0] == 'Gov.':
                            actor = "executive"
                        elif action.split()[-1] == 'Gov.':
                            actor = "executive"

                    if action in ('House Intro. Date', 'Senate Intro. Date'):
                        atype = ['bill:introduced']
                        action = action.replace('Intro. Date', 'Introduced')
                    elif action == '3rd Consideration':
                        atype = ['bill:reading:3', 'bill:passed']
                    elif action == 'Sent to Gov.':
                        atype = ['governor:received']
                    elif action == 'Signed By Governor':
                        atype = ['governor:signed']
                    else:
                        atype = ['other']

                    if type(date) == float:
                        date = str(xlrd.xldate_as_tuple(date, 0))
                        date = datetime.datetime.strptime(
                            date, "(%Y, %m, %d, %H, %M, %S)")
                        bill.add_action(actor, action, date, type=atype)

                self.scrape_votes(bill, bill_prefix, rownum, session)
                self.scrape_versions(bill, bill_prefix, rownum, session)
                self.save_bill(bill)

    def scrape_versions(self, bill, prefix, number, session):
        base_url = 'http://www.legislature.state.oh.us'

        if 'r' in prefix:
            piece = '/res.cfm?ID=%s_%s_%s' % (session, prefix.upper(),
                                                number)
        else:
            piece = '/bills.cfm?ID=%s_%s_%s' % (session, prefix.upper(),
                                                number)

        def _get_html_or_pdf_version(url):
            doc = lxml.html.fromstring(url)
            name = doc.xpath('//font[@size="2"]/a/text()')[0]
            html_links = doc.xpath('//a[text()="(.html format)"]')
            pdf_links = doc.xpath('//a[text()="(.pdf format)"]')
            if html_links:
                link = html_links[0].get('href')
                bill.add_version(name, base_url + link, on_duplicate='use_old',
                                 mimetype='text/html')
            elif pdf_links:
                link = pdf_links[0].get('href')
                bill.add_version(name, base_url + link,
                                 mimetype='application/pdf')

        html = self.urlopen(base_url + piece)
        # pass over missing bills - (unclear why this happens)
        if 'could not be found.' in html:
            self.warning('missing page: %s' % base_url + piece)
            return

        _get_html_or_pdf_version(html)
        doc = lxml.html.fromstring(html)
        for a in doc.xpath('//a[starts-with(@href, "/bills.cfm")]/@href'):
            if a != piece:
                _get_html_or_pdf_version(self.urlopen(base_url + a))
        for a in doc.xpath('//a[starts-with(@href, "/res.cfm")]/@href'):
            if a != piece:
                _get_html_or_pdf_version(self.urlopen(base_url + a))

    def scrape_votes(self, bill, bill_prefix, number, session):
        vote_url = ('http://www.legislature.state.oh.us/votes.cfm?ID=' +
                    session + '_' + bill_prefix + '_' + str(number))

        page = self.urlopen(vote_url)
        page = lxml.html.fromstring(page)

        for jlink in page.xpath("//a[contains(@href, 'JournalText')]"):
            date = datetime.datetime.strptime(jlink.text,
                                              "%m/%d/%Y").date()

            details = jlink.xpath("string(../../../td[2])")

            chamber = details.split(" - ")[0]
            if chamber == 'House':
                chamber = 'lower'
            elif chamber == 'Senate':
                chamber = 'upper'
            else:
                raise ScrapeError("Bad chamber: %s" % chamber)

            motion = details.split(" - ")[1].split("\n")[0].strip()

            vote_row = jlink.xpath("../../..")[0].getnext()

            yea_div = vote_row.xpath(
                "td/font/div[contains(@id, 'Yea')]")[0]
            yeas = []
            for td in yea_div.xpath("table/tr/td"):
                name = td.xpath("string()")
                if name:
                    yeas.append(name)

            no_div = vote_row.xpath(
                "td/font/div[contains(@id, 'Nay')]")[0]
            nays = []
            for td in no_div.xpath("table/tr/td"):
                name = td.xpath("string()")
                if name:
                    nays.append(name)

            yes_count = len(yeas)
            no_count = len(nays)

            vote = Vote(chamber, date, motion, yes_count > no_count,
                        yes_count, no_count, 0)

            for yes in yeas:
                vote.yes(yes)
            for no in nays:
                vote.no(no)

            vote.add_source(vote_url)

            bill.add_vote(vote)

########NEW FILE########
__FILENAME__ = events
import re
import datetime as dt

from billy.scrape.events import EventScraper, Event

import pytz
import lxml.html

pages = {
    "lower" : "http://www.legislature.state.oh.us/house_committee_schedule.cfm"
}

class OHEventScraper(EventScraper):
    jurisdiction = 'oh'

    _tz = pytz.timezone('US/Eastern')

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def scrape_page(self, chamber, session):
        url = pages[chamber]
        page = self.lxmlize(url)

        rows = page.xpath("//table[@class='MsoNormalTable']/tr")
        header = rows[0]
        rows = rows[1:]

        week_of = page.xpath("//h3[@align='center']/b/text()")[0]
        match = re.match(
            "(?i)Week of (?P<month>.*) (?P<day>\d+), (?P<year>\d{4})",
            week_of)
        day_info = match.groupdict()
        monday_dom = int(day_info['day'])
        days = ["monday", "tuesday", "wednesday", "thursday",
                "friday", "saturday", "sunday"]

        dates = {}
        dIdex = 0
        for row in header.xpath(".//td")[1:]:
            date = row.text_content()
            date = re.sub("\s+", " ", date).strip()
            dates[dIdex] = date
            dIdex += 1

        def _parse_time_block(block):
            if block.strip() == "No Meeting":
                return None, None, []
            bills = []
            room = None

            blocks = [ x.strip() for x in block.split("\n") ]

            hour = re.sub("\(.*\)", "", blocks[1])
            bills = blocks[2]
            bills = bills.encode('ascii', errors='ignore')

            if "after" in hour or "after" in bills:
                return None, None, []

            # Extra time cleanup
            # "Rm"
            if "Rm" in hour:
                inf = hour.split("Rm")
                assert len(inf) == 2
                room = inf[1]
                hour = inf[0]

            # "and"
            hour = [ x.strip() for x in hour.split('and') ]

            # We'll pass over this twice.
            single_bill = re.search("(H|S)(C?)(B|R) \d+", bills)
            if single_bill is not None:
                start, end = single_bill.regs[0]
                description = bills
                bills = bills[start:end]
                bills = [{
                    "bill_id": bills,
                    "description": description
                }]
            else:
                multi_bills = re.search("(H|S)(B|R|M)s (\d+((;,) )?)+",
                                        bills)
                if multi_bills is not None:
                    # parse away.
                    bill_array = bills.split()
                    type = bill_array[0]
                    bill_array = bill_array[1:]

                    def _c(f):
                        for thing in [";", ",", "&", "*"]:
                            f = f.replace(thing, "")
                        return re.sub("\s+", " ", f).strip()

                    bill_array = [_c(x) for x in bill_array]
                    type = type.replace("s", "")
                    bill_array = [
                        { "bill_id": "%s %s" % ( type, x ),
                          "description": bills } for x in bill_array ]
                    bills = bill_array

                else:
                    self.warning("Unknwon bill thing: %s" % (bills))
                    bills = []

            return hour, room, bills

        for row in rows:
            tds = row.xpath(".//td")
            ctty = re.sub("\s+", " ", tds[0].text_content().strip())
            times = tds[1:]
            for i in range(0, len(times)):
                hours, room, bills = _parse_time_block(times[i].text_content())
                if hours is None or bills == []:
                    continue

                for hour in hours:
                    datetime = "%s %s" % ( dates[i], hour )
                    datetime = datetime.encode("ascii", "ignore")

                    # DAY_OF_WEEK MONTH/DAY/YY %I:%M %p"
                    dow, time = datetime.split()
                    month = day_info['month']
                    year = day_info['year']
                    day = monday_dom + days.index(dow.lower())

                    datetime = "%s %s %s, %s %s" % (
                        dow, month, day, year, time
                    )

                    formats = [
                        "%A %B %d, %Y %I:%M %p",
                        "%A %B %d, %Y %I:%M%p",
                        "%A %B %d, %Y %I %p",
                        "%A %B %d, %Y %I%p",
                    ]

                    dtobj = None
                    for fmt in formats:
                        try:
                            dtobj = dt.datetime.strptime(datetime, fmt)
                        except ValueError as e:
                            continue

                    if dtobj is None:
                        self.warning("Unknown guy: %s" % (datetime))
                        raise Exception

                    datetime = dtobj

                    event = Event(session,
                                  datetime,
                                  'committee:meeting',
                                  'Meeting Notice',
                                  "Room %s" % (room))
                    event.add_source(url)

                    for bill in bills:
                        event.add_related_bill(
                            bill['bill_id'],
                            description=bill['description'],
                            type='consideration'
                        )

                    event.add_participant("host",
                                          ctty, 'committee', chamber=chamber)
                    self.save_event(event)

    def scrape(self, chamber, session):
        if chamber != "lower":
            self.warning("Sorry, we can't scrape !(lower) ATM. Skipping.")
            return

        self.scrape_page(chamber, session)

########NEW FILE########
__FILENAME__ = legislators
import re

from billy.scrape import NoDataForPeriod
from billy.scrape.legislators import LegislatorScraper, Legislator

import lxml.html


JOINT_COMMITTEE_OVERRIDE = [  # without Joint" in the name.
    "State Controlling Board",
    "Legislative Service Commission",
    "Correctional Institution Inspection Committee"
]

SUBCOMMITTEES = {
    "Education Finance Subcommittee": "Education",
    "Medicaid Finance Subcommittee": "Medicaid, Health And Human Services",
    "General Government Finance Subcommittee":
            "State Government Oversight And Reform",
    "Shared Services and Government Efficiency Subcommittee":
            "Public Safety, Local Government And Veterans Affairs",
    "Higher Education Subcommittee": "Education",
    "Health and Human Services Subcommittee":
            "Medicaid, Health And Human Services",
    "Transportation Subcommittee": "Transportation",
    "Agriculture and Development Subcommittee":
            "Agriculture And Natural Resources",
    "Primary and Secondary Education Subcommittee": "Education",
    "Tax Reform Ways and Means Subcommittee":
            "Ways and Means",
}

committee_cache = {}


class OHLegislatorScraper(LegislatorScraper):
    jurisdiction = 'oh'
    latest_only = True

    def scrape(self, chamber, term):
        url = (
            "http://www.ohiosenate.gov/senate/members/senate-directory"
            if chamber == "upper" else
            "http://www.ohiohouse.gov/members/member-directory")
        self.scrape_page(chamber, term, url)

    def fetch_committee_positions(self, a):
        page = self.urlopen(a.attrib['href'])
        page = lxml.html.fromstring(page)
        page.make_links_absolute(a.attrib['href'])
        ret = {}
        for entry in page.xpath("//div[@class='committeeMembers']//td//a"):
            person = re.sub(
                "\s+", " ", re.sub("\(.*\)", "", entry.text or "")).strip()

            if person == "":
                continue

            title = entry.xpath(".//div[@class='title']/text()") or None

            if title:
                title = title[0]
                ret[person] = title

        return ret

    def scrape_homepage(self, leg, chamber, homepage, term):
        page = self.urlopen(homepage)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(homepage)
        bio = page.xpath(
            "//div[@class='biography']//div[@class='right']//p/text()")
        if bio != []:
            bio = bio[0]
            leg['biography'] = bio

        ctties = page.xpath("//div[@class='committeeList']//a")
        for a in ctties:
            entry = a.text_content()

            if entry in committee_cache:
                committee_positions = committee_cache[entry]
            else:
                committee_positions = self.fetch_committee_positions(a)
                committee_cache[entry] = committee_positions

            position = "member"
            name = leg['full_name']
            if name in committee_positions:
                position = committee_positions[name]

            chmbr = "joint" if "joint" in entry.lower() else chamber
            if entry in JOINT_COMMITTEE_OVERRIDE:
                chmbr = "joint"

            kwargs = {}

            if "subcommittee" in entry.lower():
                if entry in SUBCOMMITTEES:
                    kwargs['subcommittee'] = entry
                    entry = SUBCOMMITTEES[entry]
                else:
                    self.warning("No subcommittee known - %s" % (entry))
                    raise Exception

            leg.add_role('committee member',
                         position=position,
                         term=term,
                         chamber=chmbr,
                         committee=entry,
                         **kwargs)

    def scrape_page(self, chamber, term, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        for legislator in page.xpath("//div[contains(concat(' ', "
                "normalize-space(@class), ' '), ' memberModule ')]"):

            img = legislator.xpath(
                ".//div[@class='thumbnail']//img")[0].attrib['src']
            data = legislator.xpath(".//div[@class='data']")[0]
            homepage = data.xpath(".//a[@class='black']")[0]
            full_name = homepage.text_content()
            homepage = homepage.attrib['href']
            party = data.xpath(
                ".//span[@class='partyLetter']")[0].text_content()
            party = {"R": "Republican", "D": "Democratic"}[party]
            office_lines = data.xpath("child::text()")
            phone = office_lines.pop(-1)
            office = "\n".join(office_lines)
            h3 = data.xpath("./h3")
            if len(h3):
                h3 = h3[0]
                district = h3.xpath("./br")[0].tail.replace("District", ""
                                                           ).strip()
            else:
                district = re.findall(
                    "\d+\.png",
                    legislator.attrib['style']
                )[-1].split(".", 1)[0]

            full_name = re.sub("\s+", " ", full_name).strip()
            leg = Legislator(term, chamber, district, full_name,
                             party=party, url=homepage, photo_url=img)

            leg.add_office('capitol', 'Capitol Office',
                           address=office,
                           phone=phone)

            self.scrape_homepage(leg, chamber, homepage, term)

            leg.add_source(url)
            self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = actions
import re
from billy.scrape.actions import Rule, BaseCategorizer

# These are regex patterns that map to action categories.
_categorizer_rules = (

    Rule([
        # Add a bajilion links to entities.
        r'SCs (changed|removed) (?P<legislators>.+)',
        r'Conference Committee on (?P<committees>.+)',
        r'Conference granted, naming:?\s+Conference Committee on (?P<committees>.+)',
        r'vote by Representative(?P<legislators>.+)',
        r'amended (?P<committees>.+?) committee',
        r'coauthored by (?P<legislators>.+)',
        (r'Remove Senator .+? as principal Senate author '
         r'and substitute with Senator (?P<legislators>.+?)'),
        r'(?i)committee substitute (?P<committees>.+)',
        r'(?i)remove\s{,10}as\s{,10}author\s{,10}(?P<legislators>.+);',
        r'(?i)SCs\s{,10}named\s{,10}(?P<legislators>.+)',
        (r'Pending removal author Senator (?P<legislators>.+?) '
         r'and replace with Senator'),
        r'(?i)Representative\(s\)\s{,10}(?P<legislators>.+)',
        r'Withdrawn from Calendar; (?P<committees>.+)',
        (r'Pending removal author Senator .+? and replace '
         r'with Senator (?P<legislators>.+)'),
        r'Ayes:\s+(?P<yes_votes>\d+)\s+Nays:\s+(?P<no_votes>\d+)',
        (r'remove as principal author Representative .+? and substitute '
         r'with Representative (?P<legislators>.+?)'),
        r'Pending coauthorship Senator\(s\) (?P<legislators>.+)',
        (r'Remove Representative (?P<legislators>.+?) as principal '
         r'House author and substitute with Representative'),
        r'Pending removal principal author Representative .+? and '
        r'replace with Representative (?P<legislators>.+)',
        r'(?i)(co)?authored\s{,10}by\s{,10}(?P<legislators>.+)',
        r'Second Reading referred to (?P<committees>.+? Committee)',
        r'Notice served to reconsider vote on measure (?P<legislators>.+)',
        (r'Pending removal principal author Representative (?P<legislators>.+) '
         r'and replace with Representative .+'),
        (r'remove as principal author Representative (?P<legislators>.+?) '
         r'and substitute with Representative'),
        r'CR; Do Pass(, as amended,|, amended by)? (?P<committees>.+)',
        r'coauthor (Senator|Representative) (?P<legislators>.+)',
        r'Ayes:\s+(?P<yes_votes>\d+)\s+Nays:\s+(?P<no_votes>\d+)']),

    Rule(u'Introduced', 'bill:introduced'),
    Rule([u'Adopted'], [u'bill:passed']),
    Rule([u'HAs rejected'], [u'amendment:failed']),
    Rule(u'(?i)Measure.+?passed', 'bill:passed'),
    Rule(u'(?i)Measure.+?passed', 'bill:passed'),
    Rule(u'Third Reading, Measure passed',
         ['bill:passed', 'bill:reading:3']),

    Rule([u'^Amendment withdrawn'], [u'amendment:withdrawn']),
    Rule([u'^Amended$'], [u'amendment:passed']),
    Rule([u'^Amendment failed'], [u'amendment:failed']),
    Rule([u'^Amendment restore'], [u'amendment:passed']),

    Rule('First Reading', ('bill:introduced', 'bill:reading:1')),
    Rule([u'Second Reading referred to (?P<committees>.+)'],
         [u'committee:referred', u'bill:reading:2']),
    Rule([u'Second Reading referred to (?P<committees>.+? Committee)'],
         [u'committee:referred', u'bill:reading:2']),
    Rule([u'Second Reading referred to .+? then to (?P<committees>.+)'],
         [u'committee:referred', u'bill:reading:2']),
    Rule([u'Second Reading referred to (?P<committees>.+?) then to '],
         [u'committee:referred', u'bill:reading:2']),
    Rule([u'Second Reading referred to .+? then to (?P<committees>.+)'],
         [u'committee:referred', u'bill:reading:2']),
    Rule([u'(?i)Placed on Third Reading'], [u'bill:reading:3']),
    Rule([u'^(?i)Third Reading'], [u'bill:reading:3']),

    Rule(r'committee substitute (?P<committees>.+?);'),
    Rule([u'Do Pass (as amended )?(?P<committees>.+)'], [u'committee:passed']),
    Rule([u'Failed in Committee - (?P<committees>.+)'], [u'committee:failed']),
    Rule([u'CR; Do not pass (?P<committees>.+)'], [u'committee:failed']),
    Rule([u'rereferred to (?P<committees>.+)'], [u'committee:referred']),
    Rule([u'Referred to (?P<committees>.+?)'], [u'committee:referred']),
    Rule([u'Reported Do Pass, amended by committee substitute (?P<committees>.+?);'],
         [u'committee:passed']),
    Rule([u'^(?i)Reported Do Pass'], [u'committee:passed']),
    Rule([u'Do pass, amended by committee substitute (?P<committees>)'],
         [u'committee:passed']),
    Rule([u'Sent to Governor'], [u'governor:received'], actor='governor'),
    Rule([u'^(Signed|Approved) by Governor'], [u'governor:signed'], actor='governor'),
    Rule([u'^Vetoed'], [u'governor:vetoed'], actor='governor'),
    )


class Categorizer(BaseCategorizer):
    rules = _categorizer_rules

    def post_categorize(self, attrs):
        res = set()
        if 'legislators' in attrs:
            for text in attrs['legislators']:
                text = text.replace('Representative(s)', '')
                text = text.replace('(principal House author) ', '')
                rgx = r'(,\s+(?![a-z]\.)|\s+and\s+)'
                legs = re.split(rgx, text)
                legs = filter(lambda x: x not in [', ', ' and '], legs)
                res |= set(legs)
        attrs['legislators'] = list(res)

        res = set()
        if 'committees' in attrs:
            for text in attrs['committees']:
                text = text.replace('by committee substitute', '')
                text = text.replace('CR filed;', '')
                text = text.replace('; pending CR', '')
                text = text.strip()
                res.add(text)
        attrs['committees'] = list(res)
        return attrs

########NEW FILE########
__FILENAME__ = bills
import re
import urllib
import datetime
import collections

from billy.utils import urlescape
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

import lxml.html
import scrapelib

from .actions import Categorizer


class OKBillScraper(BillScraper):

    jurisdiction = 'ok'
    bill_types = ['B', 'JR', 'CR', 'R']
    subject_map = collections.defaultdict(list)

    categorizer = Categorizer()

    def scrape(self, chamber, session, only_bills=None):
        # start by building subject map
        self.scrape_subjects(chamber, session)

        url = "http://webserver1.lsb.state.ok.us/WebApplication3/WebForm1.aspx"
        form_page = lxml.html.fromstring(self.urlopen(url))

        if chamber == 'upper':
            chamber_letter = 'S'
        else:
            chamber_letter = 'H'

        session_id = self.metadata['session_details'][session]['session_id']

        values = {'cbxSessionId': session_id,
                  'cbxActiveStatus': 'All',
                  'RadioButtonList1': 'On Any day',
                  'Button1': 'Retrieve'}

        lbxTypes = []
        for bill_type in self.bill_types:
            lbxTypes.append(chamber_letter + bill_type)
        values['lbxTypes'] = lbxTypes

        for hidden in form_page.xpath("//input[@type='hidden']"):
            values[hidden.attrib['name']] =  hidden.attrib['value']

        page = self.urlopen(url, "POST", values)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        bill_nums = []
        for link in page.xpath("//a[contains(@href, 'BillInfo')]"):
            bill_id = link.text.strip()
            bill_num = int(re.findall('\d+', bill_id)[0])
            if bill_num >= 9900:
                self.log('skipping likely bad bill %s' % bill_id)
                continue
            if only_bills is not None and bill_id not in only_bills:
                self.log('skipping bill we are not interested in %s' % bill_id)
                continue
            bill_nums.append(bill_num)
            self.scrape_bill(chamber, session, bill_id, link.attrib['href'])
        return bill_nums

    def scrape_bill(self, chamber, session, bill_id, url):
        try:
            page = lxml.html.fromstring(self.urlopen(url))
        except scrapelib.HTTPError as e:
            self.warning('error (%s) fetching %s, skipping' % (e, url))
            return

        title = page.xpath(
            "string(//span[contains(@id, 'PlaceHolder1_txtST')])").strip()

        if 'JR' in bill_id:
            bill_type = ['joint resolution']
        elif 'CR' in bill_id:
            bill_type = ['concurrent resolution']
        elif 'R' in bill_id:
            bill_type = ['resolution']
        else:
            bill_type = ['bill']

        bill = Bill(session, chamber, bill_id, title, type=bill_type)
        bill.add_source(url)
        bill['subjects'] = self.subject_map[bill_id]

        for link in page.xpath("//a[contains(@id, 'Auth')]"):
            name = link.xpath("string()").strip()

            if ':' in name:
                raise Exception(name)
            if 'otherAuth' in link.attrib['id']:
                bill.add_sponsor('cosponsor', name)
            else:
                bill.add_sponsor('primary', name)

        act_table = page.xpath("//table[contains(@id, 'Actions')]")[0]
        for tr in act_table.xpath("tr")[2:]:
            action = tr.xpath("string(td[1])").strip()
            if not action or action == 'None':
                continue

            date = tr.xpath("string(td[3])").strip()
            date = datetime.datetime.strptime(date, "%m/%d/%Y").date()

            actor = tr.xpath("string(td[4])").strip()
            if actor == 'H':
                actor = 'lower'
            elif actor == 'S':
                actor = 'upper'

            attrs = dict(actor=actor, action=action, date=date)
            attrs.update(**self.categorizer.categorize(action))
            bill.add_action(**attrs)

        version_table = page.xpath("//table[contains(@id, 'Versions')]")[0]
        for link in version_table.xpath(".//a[contains(@href, '.PDF')]"):
            version_url = link.attrib['href']
            name = link.text.strip()

            if 'COMMITTEE REPORTS' in version_url:
                bill.add_document(name, version_url, mimetype='application/pdf')
                continue

            bill.add_version(name, version_url, mimetype='application/pdf')

        for link in page.xpath(".//a[contains(@href, '_VOTES')]"):
            if 'HT_' not in link.attrib['href']:
                self.scrape_votes(bill, urlescape(link.attrib['href']))

        # # If the bill has no actions and no versions, it's a bogus bill on
        # # their website, which appears to happen occasionally. Skip.
        has_no_title = (bill['title'] == "Short Title Not Found.")
        if has_no_title:
            # If there's no title, this is an empty page. Skip!
            return

        else:
            # Otherwise, save the bills.
            self.save_bill(bill)

    def scrape_votes(self, bill, url):
        page = lxml.html.fromstring(self.urlopen(url).replace(u'\xa0', ' '))

        re_ns = "http://exslt.org/regular-expressions"
        path = "//p[re:test(text(), 'OKLAHOMA\s+(HOUSE|STATE\s+SENATE)')]"
        for header in page.xpath(path, namespaces={'re': re_ns}):
            if 'HOUSE' in header.xpath("string()"):
                chamber = 'lower'
                motion_index = 8
            else:
                chamber = 'upper'
                motion_index = 9

            motion = header.xpath(
                "string(following-sibling::p[%d])" % motion_index).strip()
            motion = re.sub(r'\s+', ' ', motion)
            match = re.match(r'^(.*) (PASSED|FAILED)$', motion)
            if match:
                motion = match.group(1)
                passed = match.group(2) == 'PASSED'
            else:
                passed = None

            rcs_p = header.xpath(
                "following-sibling::p[contains(., 'RCS#')]")[0]
            rcs_line = rcs_p.xpath("string()").replace(u'\xa0', ' ')
            rcs = re.search(r'RCS#\s+(\d+)', rcs_line).group(1)

            date_line = rcs_p.getnext().xpath("string()")
            date = re.search(r'\d+/\d+/\d+', date_line).group(0)
            date = datetime.datetime.strptime(date, "%m/%d/%Y").date()

            vtype = None
            counts = collections.defaultdict(int)
            votes = collections.defaultdict(list)

            seen_yes = False

            for sib in header.xpath("following-sibling::p")[13:]:
                line = sib.xpath("string()").replace('\r\n', ' ').strip()
                if "*****" in line:
                    break

                match = re.match(
                    r'(YEAS|NAYS|EXCUSED|VACANT|CONSTITUTIONAL PRIVILEGE|NOT VOTING|N/V)\s*:\s*(\d+)',
                    line)
                if match:
                    if match.group(1) == 'YEAS' and 'RCS#' not in line:
                        vtype = 'yes'
                        seen_yes = True
                    elif match.group(1) == 'NAYS' and seen_yes:
                        vtype = 'no'
                    elif match.group(1) == 'VACANT':
                        continue  # skip these
                    elif seen_yes:
                        vtype = 'other'
                    counts[vtype] += int(match.group(2))
                elif seen_yes:
                    for name in line.split('   '):
                        if not name:
                            continue
                        if 'HOUSE BILL' in name or 'SENATE BILL' in name:
                            continue
                        votes[vtype].append(name.strip())

            if passed is None:
                passed = counts['yes'] > (counts['no'] + counts['other'])

            if not motion:
                motion = 'Senate Vote' if chamber == 'upper' else 'House Vote'

            vote = Vote(chamber, date, motion, passed,
                        counts['yes'], counts['no'], counts['other'],
                        rcs_num=rcs)
            vote.validate()

            vote.add_source(url)

            for name in votes['yes']:
                vote.yes(name)
            for name in votes['no']:
                if ':' in name:
                    raise Exception(name)
                vote.no(name)
            for name in votes['other']:
                vote.other(name)

            vote.validate()
            bill.add_vote(vote)

    def scrape_subjects(self, chamber, session):
        form_url = 'http://webserver1.lsb.state.ok.us/WebApplication19/WebForm1.aspx'
        form_html = self.urlopen(form_url)
        fdoc = lxml.html.fromstring(form_html)

        # bill types
        letter = 'H' if chamber == 'lower' else 'S'
        types = [letter + t for t in self.bill_types]

        session_id = self.metadata['session_details'][session]['session_id']

        # do a request per subject
        for subj in fdoc.xpath('//select[@name="lbxSubjects"]/option/@value'):
            # these forms require us to get hidden session keys
            values = {'cbxInclude': 'All', 'Button1': 'Retrieve',
                      'RadioButtonList1': 'On Any Day',
                      'cbxSessionID': session_id,
                      'lbxSubjects': subj, 'lbxTypes': types}
            for hidden in fdoc.xpath("//input[@type='hidden']"):
                values[hidden.attrib['name']] = hidden.attrib['value']
            #values = urllib.urlencode(values, doseq=True)
            page_data = self.urlopen(form_url, 'POST', values)
            page_doc = lxml.html.fromstring(page_data)

            # all links after first are bill_ids
            for bill_id in page_doc.xpath('//a/text()')[1:]:
                self.subject_map[bill_id].append(subj)

########NEW FILE########
__FILENAME__ = committees
import re

from billy.scrape import NoDataForPeriod
from billy.scrape.committees import CommitteeScraper, Committee

import lxml.html


class OKCommitteeScraper(CommitteeScraper):
    jurisdiction = "ok"
    latest_only = True

    def scrape(self, chamber, term):
        if chamber == "upper":
            self.scrape_upper()
        elif chamber == "lower":
            self.scrape_lower()

    def scrape_lower(self):
        url = "http://www.okhouse.gov/Committees/Default.aspx"
        page = lxml.html.fromstring(self.urlopen(url))
        page.make_links_absolute(url)

        parents = {}

        for link in page.xpath("//table[@id='ctl00_ContentPlaceHolder1_dgrdCommittee_ctl00']//a[contains(@href, 'Members')]"):
            name = link.xpath("string()").strip()

            if 'Members' in name or 'Conference' in name:
                continue

            match = re.search(r'CommID=(\d+)&SubCommID=(\d+)',
                              link.attrib['href'])
            comm_id, sub_comm_id = int(match.group(1)), int(match.group(2))

            if sub_comm_id == 0:
                parents[comm_id] = name
                parent = None
            else:
                parent = parents[comm_id]

            self.scrape_lower_committee(name, parent, link.attrib['href'])

    def scrape_lower_committee(self, name, parent, url):
        page = lxml.html.fromstring(self.urlopen(url))
        page.make_links_absolute(url)

        if 'Joint' in name or (parent and 'Joint' in parent):
            chamber = 'joint'
        else:
            chamber = 'lower'

        if parent:
            comm = Committee(chamber, parent, subcommittee=name)
        else:
            comm = Committee(chamber, name)
        comm.add_source(url)

        xpath = "//a[contains(@href, 'District')]"
        for link in page.xpath(xpath):
            member = link.xpath('string()').strip()
            member = re.sub(r'\s+', ' ', member)

            if not member or member == 'House District Maps':
                continue

            match = re.match(r'((Co-)?(Vice )?Chair)?Rep\. ([^\(]+)', member)
            member = match.group(4).strip()
            role = match.group(1) or 'member'

            comm.add_member(member, role.lower())

        if not comm['members']:
            if comm['subcommittee'] == 'test':
                # Whoopsie, prod data.
                return

            raise Exception('no members for %s (%s)' % (
                comm['committee'], comm['subcommittee'])
            )

        self.save_committee(comm)

    def scrape_upper(self):
        url = "http://www.oksenate.gov/Committees/standingcommittees.htm"
        page = lxml.html.fromstring(self.urlopen(url))
        page.make_links_absolute(url)

        for link in page.xpath("//a[contains(@href, 'standing/')]"):
            name = link.text.strip()
            name = re.sub(r'\s+', ' ', name)
            if 'Committee List' in name:
                continue

            self.scrape_upper_committee(name, link.attrib['href'])

    def scrape_upper_committee(self, name, url):
        page = lxml.html.fromstring(self.urlopen(url))

        comm = Committee('upper', name)
        comm.add_source(url)

        for link in page.xpath("//a[contains(@href, 'biographies')]"):
            member = link.xpath("string()").strip()
            member = re.sub(r'\s+', ' ', member)
            if not member:
                continue
            role = link.tail
            if not role:
                role = 'member'
            elif 'Vice Chair' in role:
                role = 'vice chair'
            elif 'Chair' in role:
                role = 'chair'
            comm.add_member(member, role=role)

        if not comm['members']:
            raise Exception('no members for %s', comm['name'])
        self.save_committee(comm)

########NEW FILE########
__FILENAME__ = events
import re
import datetime
import time

from billy.scrape.events import EventScraper, Event

import lxml.html


class OKEventScraper(EventScraper):
    jurisdiction = 'ok'

    def scrape(self, chamber, session):
        if chamber == 'upper':
            self.scrape_upper(session)

    def scrape_upper(self, session):
        url = "http://www.oksenate.gov/Committees/meetingnotices.htm"
        page = lxml.html.fromstring(self.urlopen(url))
        page.make_links_absolute(url)

        text = page.text_content()
        _, text = text.split('MEETING NOTICES')
        re_date = r'[A-Z][a-z]+,\s+[A-Z][a-z]+ \d+, \d{4}'
        chunks = zip(re.finditer(re_date, text), re.split(re_date, text)[1:])

        for match, data in chunks:
            when = match.group()
            when = datetime.datetime.strptime(when, "%A, %B %d, %Y")

            lines = filter(None, [x.strip() for x in data.splitlines()])

            time_ = re.search(r'^\s*TIME:\s+(.+?)\s+\x96', data, re.M).group(1)
            time_ = time_.replace('a.m.', 'AM').replace('p.m.', 'PM')
            time_ = time.strptime(time_, '%I:%M %p')
            when += datetime.timedelta(hours=time_.tm_hour, minutes=time_.tm_min)

            title = lines[0]

            where = re.search(r'^\s*PLACE:\s+(.+)', data, re.M).group(1)
            where = where.strip()

            event = Event(session, when, 'committee:meeting', title,
                          location=where)
            event.add_source(url)

            self.save_event(event)

########NEW FILE########
__FILENAME__ = legislators
import re

import xlrd
import lxml.html
import name_tools

from billy.scrape.legislators import LegislatorScraper, Legislator
import scrapelib


def scrub(text):
    '''Squish whitespace and kill \xa0.
    '''
    return re.sub(r'[\s\xa0]+', ' ', text)


class OKLegislatorScraper(LegislatorScraper):
    jurisdiction = 'ok'
    latest_only = True

    def scrape(self, chamber, term):
        if chamber == 'lower':
            self.scrape_lower(term)
        else:
            self.scrape_upper(term)

    def scrape_lower(self, term):
        url = "http://www.okhouse.gov/Members/Default.aspx"
        page = lxml.html.fromstring(self.urlopen(url))
        page.make_links_absolute(url)

        for tr in page.xpath("//table[@class='rgMasterTable']/tbody/tr")[1:]:
            name = tr.xpath('.//td[1]/a')[0].text.strip()
            district = tr.xpath('.//td[3]')[0].text_content().strip()
            party = tr.xpath('.//td[4]')[0].text_content().strip()
            party = {'R': 'Republican', 'D': 'Democratic'}[party]

            leg_url = 'http://www.okhouse.gov/District.aspx?District=' + district
            leg_doc = lxml.html.fromstring(self.urlopen(leg_url))
            leg_doc.make_links_absolute(leg_url)
            photo_url = leg_doc.xpath('//a[contains(@href, "HiRes")]/@href')[0]

            if name.startswith('House District'):
                self.warning("skipping %s %s" % (name, leg_url))
                continue

            leg = Legislator(term, 'lower', district, name, party=party,
                             photo_url=photo_url, url=leg_url)
            leg.add_source(url)
            leg.add_source(leg_url)

            # Scrape offices.
            self.scrape_lower_offices(leg_doc, leg)

            self.save_legislator(leg)

    def scrape_lower_offices(self, doc, legislator):

        # Capitol offices:
        xpath = '//*[contains(text(), "Capitol Address")]'
        for bold in doc.xpath(xpath):

            # Get the address.
            address_div = bold.getparent().itersiblings().next()

            # Get the room number.
            xpath = '//*[contains(@id, "CapitolRoom")]/text()'
            room = address_div.xpath(xpath)
            if room:
                parts = map(scrub, list(address_div.itertext()))
                phone = parts.pop()
                parts = [parts[0], 'Room ' + room[0], parts[-1]]
                address = '\n'.join(parts)
            else:
                address = None
                phone = None

            if not phone:
                phone = None

            # Set the email on the legislator object.
            try:
                xpath = '//a[contains(@href, "mailto")]/@href'
                email = doc.xpath(xpath)[0][7:]
            except IndexError:
                email = ''

            legislator['email'] = email

            office = dict(
                name='Capitol Office', type='capitol', phone=phone,
                fax=None, email=None, address=address)

            legislator.add_office(**office)

        # District offices:
        xpath = '//*[contains(text(), "District Address")]'
        for bold in doc.xpath(xpath):

            # Get the address.
            parts = []
            for node in bold.getparent().itersiblings():
                if node.tag != 'div':
                    parts.append(node.text)
                else:
                    break

            parts = filter(None, parts)
            parts = map(scrub, parts)
            phone = parts.pop()
            address = '\n'.join(parts)
            office = dict(
                name='District Office', type='district', phone=phone,
                fax=None, email=None, address=address)

            legislator.add_office(**office)

    def scrape_upper(self, term):
        url = "http://oksenate.gov/Senators/Default.aspx"
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        for a in doc.xpath('//table[@summary]')[1].xpath('.//td//a[contains(@href, "biographies")]'):
            name, party = a.text.rsplit(None, 1)

            if party == '(D)':
                party = 'Democratic'
            elif party == '(R)':
                party = 'Republican'

            tail = a.xpath('..')[0].tail
            if tail:
                district = tail.split()[1]
            else:
                district = a.xpath('../../span')[1].text.split()[1]
            url = a.get('href')

            leg = Legislator(term, 'upper', district, name, party=party, url=url)
            leg.add_source(url)
            self.scrape_upper_offices(leg, url)
            self.save_legislator(leg)

    def scrape_upper_offices(self, legislator, url):
        url = url.replace('aspx', 'html')
        html = self.urlopen(url)
        legislator.add_source(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        xpath = '//h3[contains(., "Office")]'
        for table in doc.xpath(xpath)[0].itersiblings():
            if table.tag == 'table':
                break
        col1, col2 = table.xpath('tr[2]/td')

        # Add the capitol office.
        col1 = map(scrub, col1.itertext())
        while True:
            # Throw away anything after the email address.
            last = col1[-1]
            if '@' not in last and not re.search(r'[\d\-\(\) ]{7,}', last):
                print col1.pop()
            else:
                break

        # Set email on the leg object.
        email = col1.pop()
        legislator['email'] = email

        # Next line is the phone number.
        phone = col1.pop()
        office = dict(
            name='Capitol Office',
            type='capitol',
            address='\n'.join(col1),
            fax=None, email=None, phone=phone)
        legislator.add_office(**office)

        col2 = map(scrub, col2.itertext())
        if len(col2) < 2:
            return

        office = dict(
            name='District Office',
            type='district',
            address='\n'.join(col2),
            fax=None, email=None, phone=phone)
        legislator.add_office(**office)



########NEW FILE########
__FILENAME__ = bills
from billy.scrape.utils import convert_pdf
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote
from .utils import year_from_session

from collections import defaultdict
import datetime as dt
import os
import re
import lxml.html
import scrapelib



class ORBillScraper(BillScraper):
    jurisdiction = 'or'

    baseFtpUrl    = 'ftp://landru.leg.state.or.us'

    bill_types = {'B': 'bill',
                  'M': 'memorial',
                  'R': 'resolution',
                  'JM': 'joint memorial',
                  'JR': 'joint resolution',
                  'CR': 'concurrent resolution'}

    action_classifiers = (
        ('.*Introduction and first reading.*',
             ['bill:introduced', 'bill:reading:1']),

        ('.*First reading.*', ['bill:introduced', 'bill:reading:1']),
        ('.*Second reading.*', ['bill:reading:2']),
        ('.*Referred to .*', ['committee:referred']),
        ('.*Assigned to Subcommittee.*', ['committee:referred']),
        ('.*Recommendation: Do pass.*', ['committee:passed:favorable']),
        ('.*Governor signed.*', ['governor:signed']),
        ('.*Third reading.* Passed', ['bill:passed', 'bill:reading:3']),
        ('.*Third reading.* Failed', ['bill:failed', 'bill:reading:3']),
        ('.*President signed.*', ['bill:passed']),
        ('.*Speaker signed.*', ['bill:passed']),
        ('.*Final reading.* Adopted', ['bill:passed']),
        ('.*Read third time .* Passed', ['bill:passed', 'bill:reading:3']),
        ('.*Read\. .* Adopted.*', ['bill:passed']),
    )

    all_bills = {}

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def create_url(self, url, bill_id):
        return "https://olis.leg.state.or.us/liz/{session}/{url}".format(
            session=self.slug,
            url=url
        ).format(bill=bill_id)

    def scrape(self, chamber, session):
        sessionYear = year_from_session(session)
        measure_url = self._resolve_ftp_path(sessionYear, 'measures.txt')
        action_url = self._resolve_ftp_path(sessionYear, 'meashistory.txt')
        self.slug = self.metadata['session_details'][session]['slug']

        self.all_bills = {}
        slug = self.metadata['session_details'][session]['slug']

        # get the actual bills
        bill_data = self.urlopen(measure_url)
        # skip header row
        for line in bill_data.split("\n")[1:]:
            if line:
                self.parse_bill(session, chamber, line.strip())

        for bill_id, bill in self.all_bills.items():
            if bill is None:
                continue  # XXX: ...

            bid = bill_id.replace(" ", "")
            overview = self.create_url("Measures/Overview/{bill}", bid)
            # Right, let's do some versions.

            page = self.lxmlize(overview)
            versions = page.xpath(
                "//ul[@class='dropdown-menu']/li/a[contains(@href, 'Text')]")

            measure_info = {}
            info = page.xpath("//table[@id='measureOverviewTable']/tr")
            for row in info:
                key, value = row.xpath("./*")
                key = key.text.strip(": ")
                measure_info[key] = value

            for sponsor in measure_info['Chief Sponsors'].xpath("./a"):
                bill.add_sponsor(type='primary', name=sponsor.text_content())

            for sponsor in measure_info['Regular Sponsors'].xpath("./a"):
                bill.add_sponsor(type='cosponsor', name=sponsor.text_content())

            c = lambda x: re.sub("\s+", " ", x).strip()

            title = c(measure_info['Bill Title'].text_content())
            summary = c(measure_info['Catchline/Summary'].text_content())
            bill['summary'] = summary

            for version in versions:
                name = version.text

                link = self.create_url(
                    'Downloads/MeasureDocument/{bill}/%s' % (name), bid)

                bill.add_version(name=name, url=link,
                                 mimetype='application/pdf')


            history = self.create_url('Measures/Overview/GetHistory/{bill}', bid)
            history = self.lxmlize(history).xpath("//table/tr")
            for entry in history:
                wwhere, action = [c(x.text_content()) for x in entry.xpath("*")]
                wwhere = re.match(
                    "(?P<when>.*) \((?P<where>.*)\)", wwhere).groupdict()

                chamber = {"S": "upper", "H": "lower"}[wwhere['where']]
                when = "%s-%s" % (slug[:4], wwhere['when'])
                when = dt.datetime.strptime(when, "%Y-%m-%d")

                types = []
                for expr, types_ in self.action_classifiers:
                    m = re.match(expr, action)
                    if m:
                        types += types_

                if types == []:
                    types = ['other']

                #if types == ['other']:
                #    print(action)

                # actor, action, date, type, committees, legislators
                bill.add_action(chamber, action, when, type=types)

            amendments = self.create_url(
                'Measures/ProposedAmendments/{bill}', bid)

            amendments = self.lxmlize(amendments).xpath(
                "//div[@id='amendments']/table//tr")

            for amendment in amendments:
                nodes = amendment.xpath("./td")

                if nodes == []:
                    continue

                pdf_href, date, committee, adopted = nodes
                pdf_href, = pdf_href.xpath("./a")
                pdf_link = pdf_href.attrib['href']

                name = "Ammendment %s" % (pdf_href.text_content())

                adopted = adopted.text
                bill.add_document(name=name, url=pdf_link,
                                  adopted=adopted,
                                  mimetype='application/pdf')

            bill.add_source(overview)
            self.save_bill(bill)


    def parse_bill(self, session, chamber, line):
        found = False
        found_thing = None
        splits = [u"\xe4", u"\ufffd", u"\u05d4"]
        for s in splits:
            info = line.split(s)
            if len(info) != 5:
                info = filter(lambda x: x != "", info)

            if len(info) == 5:
                found = True
                found_thing = info
                break

        if not found:
            raise Exception(info)

        info = found_thing

        (type, combined_id, number, title, relating_to) = info
        if ((type[0] == 'H' and chamber == 'lower') or
            (type[0] == 'S' and chamber == 'upper')):

            # basic bill info
            bill_id = "%s %s" % (type, number)
            # lookup type without chamber prefix
            bill_type = self.bill_types[type[1:]]

            # may encounter an ellipsis in the source data
            title = title.replace(u'\x85', '...')

            if title.strip() == "":
                self.all_bills[bill_id] = None
                return

            self.all_bills[bill_id] = Bill(session, chamber, bill_id, title,
                                            type=bill_type)

    def _resolve_ftp_path(self, sessionYear, filename):
        currentYear = dt.datetime.today().year
        currentTwoDigitYear = currentYear % 100
        sessionTwoDigitYear = sessionYear % 100
        if currentTwoDigitYear != sessionTwoDigitYear:
            filename = 'archive/%02d%s' % (sessionTwoDigitYear, filename)

        return "%s/pub/%s" % (self.baseFtpUrl, filename)

########NEW FILE########
__FILENAME__ = committees
from billy.scrape.committees import CommitteeScraper, Committee
import lxml.html
import scrapelib


class ORCommitteeScraper(CommitteeScraper):
    jurisdiction = 'or'

    def lxmlize(self, url, ignore=None):
        if ignore is None:
            ignore = []

        try:
            page = self.urlopen(url)
        except scrapelib.HTTPError as e:
            if e.response.status_code in ignore:
                self.warning("Page is giving me a 500: %s" % (url))
                return None
            raise

        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def scrape(self, term, chambers):
        cdict = {"upper": "SenateCommittees_search",
                 "lower": "HouseCommittees_search",
                 "joint": "JointCommittees_search",}

        page = self.lxmlize("https://olis.leg.state.or.us/liz/Committees/list/")
        for chamber, id_ in cdict.items():
            for committee in page.xpath("//ul[@id='%s']//li/a" % (id_)):
                self.scrape_committee(committee.attrib['href'],
                                      committee.text, chamber)

    def scrape_committee(self, committee_url, committee_name, chamber):
        page = self.lxmlize(committee_url, ignore=[500])
        if page is None:
            return
        people = page.xpath("//div[@id='membership']//tbody/tr")
        c = Committee(chamber=chamber, committee=committee_name)
        for row in people:
            role, who = [x.text_content().strip() for x in row.xpath("./td")]
            c.add_member(who, role=role)
        c.add_source(committee_url)
        self.save_committee(c)

########NEW FILE########
__FILENAME__ = legislators
from billy.scrape.legislators import LegislatorScraper, Legislator
import lxml.html
import re


def itergraphs(elements, break_):
    buf = []
    for element in elements:
        if element.tag == break_:
            yield buf
            buf = []
            continue
        buf.append(element)
    if buf:
        yield buf


class ORLegislatorScraper(LegislatorScraper):
    jurisdiction = 'or'

    URLs = {
        "lower": "http://www.oregonlegislature.gov/house/Pages/RepresentativesAll.aspx",
        "upper": "http://www.oregonlegislature.gov/senate/Pages/SenatorsAll.aspx",
    }

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def scrape(self, chamber, term):
        url = self.URLs[chamber]
        page = self.lxmlize(url)

        for block in page.xpath("//div[@class='ms-rtestate-field']")[1:-1]:
            # Each legislator block.

            photo_block = block.xpath("ancestor::td/preceding-sibling::td")
            if len(photo_block) == 0:
                continue

            photo_block, = photo_block
            # (The <td> before ours was the photo)
            img, = photo_block.xpath("*")
            img = img.attrib['src']

            h2, = block.xpath(".//h2/a")
            name = h2.text

            info = {}
            # Right, now let's get info out of their little profile box.
            for entry in block.xpath(".//p"):
                for kvpair in itergraphs(entry.xpath("./*"), 'br'):
                    # OK. We either get the tail or the next element
                    # (usually an <a> tag)
                    if len(kvpair) == 1:
                        key, = kvpair
                        value = key.tail.strip() if key.tail else None
                        if value:
                            value = re.sub("\s+", " ", value).strip()
                    elif len(kvpair) == 2:
                        key, value = kvpair
                    else:
                        # Never seen text + an <a> tag, perhaps this can happen.
                        raise ValueError("Too many elements. Something changed")

                    key = key.text_content().strip(" :")
                    if value is None:
                        # A page has the value in a <strong> tag. D'oh.
                        key, value = (x.strip() for x in key.rsplit(":", 1))

                    key = re.sub("\s+", " ", key).strip()

                    info[key] = value

            info['District'] = info['District'].encode(
                'ascii', 'ignore').strip()

            info['Party'] = info['Party'].strip(": ")

            leg = Legislator(term=term,
                             url=h2.attrib['href'],
                             chamber=chamber,
                             full_name=name,
                             party=info['Party'],
                             district=info['District'],
                             photo_url=img)
            leg.add_source(url)

            phone = info.get('Capitol Phone', info.get('apitol Phone'))
            if hasattr(phone, 'text_content'):
                phone = phone.text_content()

            leg.add_office(type='capitol',
                           name='Capitol Office',
                           address=info['Capitol Address'],
                           phone=phone,
                           email=info['Email'].attrib['href'].replace("mailto:",""))

            self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = utils
def clean_space(str):
    new_str = ' '.join(str.split())
    return new_str

def base_url():
    return 'http://www.leg.state.or.us/'

def bills_url():
    return 'http://www.leg.state.or.us/bills_laws/billsinfo.htm'


def year_from_session(session):
    return int(session.split()[0])

########NEW FILE########
__FILENAME__ = actions
import re
from billy.scrape.actions import Rule, BaseCategorizer

# These are regex patterns that map to action categories.
_categorizer_rules = (

    # Capture some groups.
    Rule(r'Senators (?P<legislators>.+) a committee of conference'),
    Rule(r"(?P<version>Printer's No. \d+)"),

    Rule(r'(?i)introduced', 'bill:introduced'),

    # Committee referred, reported.
    Rule(r"Referred to (?P<committees>.+)", 'committee:referred'),
    Rule(r"Re-(referred|committed) to (?P<committees>.+)",
         'committee:referred'),
    Rule(r'(?i)(re-)?reported', 'committee:passed'),
    Rule(r'Reported with request to re-refer to (?P<committees>.+)',
         ['committee:referred', 'committee:passed']),

    Rule([r'^Amended on', r'as amended'], 'amendment:passed'),

    # Governor.
    Rule(r'^Approved by the Governor', 'governor:signed'),
    Rule(r'^Presented to the Governor', 'governor:received'),

    # Passage.
    Rule([r'^Final passage', '^Third consideration and final passage'],
         'bill:passed'),
    Rule(r'(?i)adopted', 'bill:passed'),
    Rule(r'^First consideration', 'bill:reading:1'),
    Rule(r'Second consideration', 'bill:reading:2'),
    Rule(r'Third consideration', 'bill:reading:3'),
    )


class Categorizer(BaseCategorizer):
    rules = _categorizer_rules

    def post_categorize(self, attrs):
        res = set()
        if 'legislators' in attrs:
            for text in attrs['legislators']:
                rgx = r'(,\s+(?![a-z]\.)|\s+and\s+)'
                legs = re.split(rgx, text)
                legs = filter(lambda x: x not in [', ', ' and '], legs)
                res |= set(legs)
        attrs['legislators'] = list(res)
        return attrs

########NEW FILE########
__FILENAME__ = bills
import re
import datetime
import collections

from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote
from .utils import (bill_abbr, parse_action_date, bill_list_url, history_url,
                    info_url, vote_url)

import lxml.html
import urlparse

from .actions import Categorizer


class PABillScraper(BillScraper):
    jurisdiction = 'pa'
    categorizer = Categorizer()

    def scrape(self, chamber, session):
        self.validate_session(session)

        match = re.search("#(\d+)", session)
        if match:
            self.scrape_session(chamber, session, int(match.group(1)))
        else:
            self.scrape_session(chamber, session)

    def scrape_session(self, chamber, session, special=0):
        url = bill_list_url(chamber, session, special)

        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        for link in page.xpath('//a[contains(@href, "billinfo")]'):
            self.parse_bill(chamber, session, special, link)

    def parse_bill(self, chamber, session, special, link):
        bill_num = link.text.strip()
        type_abbr = re.search('type=(B|R|)', link.attrib['href']).group(1)

        if type_abbr == 'B':
            btype = ['bill']
        elif type_abbr == 'R':
            btype = ['resolution']

        bill_id = "%s%s %s" % (bill_abbr(chamber), type_abbr, bill_num)

        url = info_url(chamber, session, special, type_abbr, bill_num)
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        xpath = '//div[contains(@class, "BillInfo-ShortTitle")]/div[@class="BillInfo-Section-Data"]'
        title = page.xpath(xpath).pop().text_content().strip()
        if not title:
            return
        bill = Bill(session, chamber, bill_id, title, type=btype)
        bill.add_source(url)

        self.parse_bill_versions(bill, page)


        self.parse_history(bill, history_url(chamber, session, special,
            type_abbr, bill_num))

        # only fetch votes if votes were seen in history
        # if vote_count:
        self.parse_votes(bill, vote_url(chamber, session, special,
                                        type_abbr, bill_num))

        # Dedupe sources.
        sources = bill['sources']
        for source in sources:
            if 1 < sources.count(source):
                sources.remove(source)

        self.save_bill(bill)

    def parse_bill_versions(self, bill, page):
        mimetypes = {
            'icon-IE': 'text/html',
            'icon-file-pdf': 'application/pdf',
            'icon-file-word': 'application/msword',
            }
        for a in page.xpath('//*[contains(@class, "BillInfo-PNTable")]//td/a'):
            try:
                span = a[0]
            except IndexError:
                continue
            for cls in span.attrib['class'].split():
                if cls in mimetypes:
                    mimetype = mimetypes[cls]
                    break

            href = a.attrib['href']
            params = urlparse.parse_qs(href[href.find("?") + 1:])

            for key in ('pn', 'PrintersNumber'):
                try:
                    printers_number = params[key][0]
                    break
                except KeyError:
                    continue

            bill.add_version("Printer's No. %s" % printers_number,
                             href, mimetype=mimetype, on_duplicate='use_old')

    def parse_history(self, bill, url):
        bill.add_source(url)
        html = self.urlopen(url)
        tries = 0
        while 'There is a problem generating the page you requested.' in html:
            html = self.urlopen(url)
            if tries < 2:
                self.logger.warning('Internal error')
                return
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        self.parse_sponsors(bill, doc)
        self.parse_actions(bill, doc)
        # vote count
        return len(doc.xpath('//a[contains(@href, "rc_view_action1")]/text()'))

    def parse_sponsors(self, bill, page):
        first = True

        xpath = ("//div[contains(@class, 'BillInfo-PrimeSponsor')]"
                 "/div[@class='BillInfo-Section-Data']/a")
        sponsors = page.xpath(xpath)

        first = True
        for sponsor in sponsors:
            sponsor = sponsor.text_content()
            if first:
                sponsor_type = 'primary'
                first = False
            else:
                sponsor_type = 'cosponsor'

            if sponsor.find(' and ') != -1:
                dual_sponsors = sponsor.split(' and ')
                bill.add_sponsor(sponsor_type, dual_sponsors[0].strip().title())
                bill.add_sponsor('cosponsor', dual_sponsors[1].strip().title())
            else:
                name = sponsor.strip().title()
                bill.add_sponsor(sponsor_type, name)

    def parse_actions(self, bill, page):
        chamber = bill['chamber']

        for tr in page.xpath("//table[@class='DataTable']//tr"):
            action = tr.xpath("string()").replace(u'\xa0', ' ').strip()

            if action == 'In the House':
                chamber = 'lower'
                continue
            elif action == 'In the Senate':
                chamber = 'upper'
                continue
            elif action.startswith("(Remarks see"):
                continue

            match = re.match(
                r"(.*),\s+(\w+\.?\s+\d{1,2},\s+\d{4})( \(\d+-\d+\))?", action)

            if not match:
                continue

            action = match.group(1)
            attrs = self.categorizer.categorize(action)
            date = parse_action_date(match.group(2))
            bill.add_action(chamber, action, date, **attrs)

    def parse_votes(self, bill, url):
        bill.add_source(url)
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        for url in page.xpath("//a[contains(., 'Vote')]/@href"):
            bill.add_source(url)
            page = self.urlopen(url)
            page = lxml.html.fromstring(page)
            page.make_links_absolute(url)
            if '/RC/' in url:
                self.parse_chamber_votes(bill, url)
            elif '/RCC/' in url:
                self.parse_committee_votes(bill, url)
            else:
                msg = 'Unexpected vote url: %r' % url
                raise Exception(msg)

    def parse_chamber_votes(self, bill, url):
        bill.add_source(url)
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        xpath = "//a[contains(@href, 'rc_view_action2')]"
        chamber = ('upper'if 'Senate' in page.xpath('string(//h1)') else 'lower')
        for link in page.xpath(xpath)[::-1]:
            date_str = link.xpath('string(../preceding-sibling::td)').strip()
            date = datetime.datetime.strptime(date_str, "%m/%d/%Y")
            vote = self.parse_roll_call(link, chamber, date)
            bill.add_vote(vote)

    def parse_roll_call(self, link, chamber, date):
        url = link.attrib['href']
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)

        xpath = 'string(//div[@class="Column-OneFourth"]/div[3])'
        motion = page.xpath(xpath).strip()
        motion = re.sub(r'\s+', ' ', motion)

        if motion == 'FP':
            motion = 'FINAL PASSAGE'

        if motion == 'FINAL PASSAGE':
            type = 'passage'
        elif re.match(r'CONCUR(RENCE)? IN \w+ AMENDMENTS', motion):
            type = 'amendment'
        else:
            type = 'other'
            motion = link.text_content()

        yeas = int(page.xpath("//div[text() = 'YEAS']")[0].getnext().text)
        nays = int(page.xpath("//div[text() = 'NAYS']")[0].getnext().text)
        lve = int(page.xpath("//div[text() = 'LVE']")[0].getnext().text)
        nv = int(page.xpath("//div[text() = 'N/V']")[0].getnext().text)
        other = lve + nv

        passed = yeas > (nays + other)

        vote = Vote(chamber, date, motion, passed, yeas, nays, other,
                    type=type)

        for div in page.xpath('//*[contains(@class, "RollCalls-Vote")]'):
            name = div.text_content().strip()
            name = re.sub(r'^[\s,]+', '', name)
            name = re.sub(r'[\s,]+$', '', name)
            class_attr = div.attrib['class'].lower()
            if 'yea' in class_attr:
                voteval = 'yes'
            elif 'nay' in class_attr:
                voteval = 'no'
            elif 'nvote' in class_attr:
                voteval = 'other'
            elif 'lve' in class_attr:
                voteval = 'other'
            else:
                msg = 'Unrecognized vote val: %s' % class_attr
                raise Exception(msg)
            vote[voteval + '_votes'].append(name)

        return vote

    def parse_committee_votes(self, bill, url):
        bill.add_source(url)
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        chamber = ('upper'if 'Senate' in doc.xpath('string(//h1)') else 'lower')
        committee = tuple(doc.xpath('//h2')[0].itertext())[-2].strip()
        for link in doc.xpath("//a[contains(@href, 'listVoteSummary.cfm')]"):

            # Date
            for fmt in ("%m/%d/%Y", "%m-%d-%Y"):
                date = link.xpath('../../td')[0].text_content()
                try:
                    date = datetime.datetime.strptime(date, fmt)
                except ValueError:
                    continue
                break

            # Motion

            motion = link.text_content().split(' - ')[-1].strip()
            motion = 'Committee vote (%s): %s' % (committee, motion)

            # Roll call.
            vote_url = link.attrib['href']
            rollcall = self.parse_upper_committee_vote_rollcall(bill, vote_url)

            vote = Vote(chamber, date, motion, type='other',
                        committee=committee, **rollcall)

            for voteval in ('yes', 'no', 'other'):
                for name in rollcall.get(voteval + '_votes', []):
                    getattr(vote, voteval)(name)

            vote.add_source(url)
            vote.add_source(vote_url)
            bill.add_vote(vote)

    def parse_upper_committee_vote_rollcall(self, bill, url):
        bill.add_source(url)
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        rollcall = collections.defaultdict(list)
        for div in doc.xpath('//*[contains(@class, "RollCalls-Vote")]'):
            name = div.xpath('../preceding-sibling::td/text()')[0]
            name = re.sub(r'^[\s,]+', '', name)
            name = re.sub(r'[\s,]+$', '', name)
            class_attr = div.attrib['class'].lower()
            if 'yea' in class_attr:
                voteval = 'yes'
            elif 'nay' in class_attr:
                voteval = 'no'
            elif 'nvote' in class_attr:
                voteval = 'other'
            elif 'lve' in class_attr:
                voteval = 'other'
            else:
                msg = 'Unrecognized vote val: %s' % class_attr
                raise Exception(msg)
            rollcall[voteval + '_votes'].append(name)

        for voteval, xpath in (('yes', '//*[contains(@class, "RollCalls-Vote-Yeas")]'),
                               ('no', '//*[contains(@class, "RollCalls-Vote-Nays")]'),
                               ('other', '//*[contains(@class, "RollCalls-Vote-NV")]')):

            count = len(doc.xpath(xpath))
            rollcall[voteval + '_count'] = int(count)

        rollcall['passed'] = rollcall['yes_count'] > rollcall['no_count']

        return dict(rollcall)


########NEW FILE########
__FILENAME__ = committees
import re

from billy.scrape.committees import CommitteeScraper, Committee

import lxml.html


class CommitteeDict(dict):

    def __missing__(self, key):
        (chamber, committee_name, subcommittee_name) = key
        committee = Committee(chamber, committee_name)
        if subcommittee_name:
            committee['subcommittee'] = subcommittee_name
        self[key] = committee
        return committee


class PACommitteeScraper(CommitteeScraper):
    jurisdiction = 'pa'
    latest_only = True

    def scrape(self, chamber, term):

        if chamber == 'upper':
            url = ('http://www.legis.state.pa.us/cfdocs/legis/'
                   'home/member_information/senators_ca.cfm')
        else:
            url = ('http://www.legis.state.pa.us/cfdocs/legis/'
                   'home/member_information/representatives_ca.cfm')

        page = self.urlopen(url)
        page = lxml.html.fromstring(page)

        committees = CommitteeDict()

        for div in page.xpath("//div[@class='MemberInfoCteeList-Member']"):
            thumbnail, bio, committee_list, _ = list(div)
            name = bio.xpath(".//a")[-1].text_content().strip()
            namey_bits = name.split()
            party = namey_bits.pop().strip('()')
            name = ' '.join(namey_bits).replace(' ,', ',')

            for li in committee_list.xpath('div/ul/li'):

                # Add the ex-officio members to all committees, apparently.
                msg = 'Member ex-officio of all Standing Committees'
                if li.text_content() == msg:
                    for (_chamber, _, _), committee in committees.items():
                        if chamber != _chamber:
                            continue
                        committee.add_member(name, 'member')
                    continue

                # Everybody else normal.
                subcommittee_name = None
                committee_name = li.xpath('a/text()').pop()
                role = 'member'
                for _role in li.xpath('i/text()') or []:
                    if 'subcommittee' in _role.lower():
                        subcommittee_name, _, _role = _role.rpartition('-')
                        subcommittee_name = re.sub(r'[\s,]+', ' ',
                                                   subcommittee_name).strip()
                    role = re.sub(r'[\s,]+', ' ', _role).lower()

                # Add the committee member.
                key = (chamber, committee_name, subcommittee_name)
                committees[key].add_member(name, role)

        # Save the non-empty committees.
        for committee in committees.values():
            if not committee['members']:
                continue
            committee.add_source(url)
            self.save_committee(committee)

########NEW FILE########
__FILENAME__ = events
import re
import datetime
import urlparse

from billy.scrape.events import EventScraper, Event

import pytz
import lxml.html


class PAEventScraper(EventScraper):
    jurisdiction = 'pa'

    _tz = pytz.timezone('US/Eastern')

    def scrape(self, chamber, session):
        if chamber == 'upper':
            url = "http://www.legis.state.pa.us/WU01/LI/CO/SM/COSM.HTM"
        elif chamber == 'lower':
            url = "http://www.legis.state.pa.us/WU01/LI/CO/HM/COHM.HTM"
        else:
            return

        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        for date_td in page.xpath("//td[@valign='middle']"):
            date = date_td.text_content().strip()

            datetime.datetime.strptime(
                date, "%A, %B %d, %Y").date()

            next_tr = date_td.getparent().getnext()
            while next_tr is not None:
                if next_tr.xpath("td[@valign='middle']"):
                    break

                time = next_tr.xpath("string(td[1])").strip()
                dt = "%s %s" % (date, time)

                try:
                    dt = datetime.datetime.strptime(
                        dt, "%A, %B %d, %Y %I:%M %p")
                    dt = self._tz.localize(dt)
                except ValueError:
                    break

                desc = next_tr.xpath("string(td[2])").strip()
                desc_el = next_tr.xpath("td[2]")[0]
                desc = re.sub(r'\s+', ' ', desc)

                ctty = None
                cttyraw = desc.split("COMMITTEE", 1)
                if len(cttyraw) > 1:
                    ctty = cttyraw[0]

                related_bills = desc_el.xpath(
                    ".//a[contains(@href, 'billinfo')]")
                bills = []
                urls = [x.attrib['href'] for x in related_bills]

                for u in urls:
                    o = urlparse.urlparse(u)
                    qs = urlparse.parse_qs(o.query)
                    bills.append({
                        "bill_id": "%sB %s" % ( qs['body'][0], qs['bn'][0] ),
                        "bill_num": qs['bn'][0],
                        "bill_chamber": qs['body'][0],
                        "session": qs['syear'][0],
                        "descr": desc
                    })

                location = next_tr.xpath("string(td[3])").strip()
                location = re.sub(r'\s+', ' ', location)

                event = Event(session, dt, 'committee:meeting',
                              desc, location)
                event.add_source(url)

                if not ctty is None:
                    event.add_participant('host', ctty, 'committee',
                                          chamber=chamber)

                for bill in bills:
                    event.add_related_bill(
                        bill['bill_id'],
                        description=bill['descr'],
                        type='consideration'
                    )
                self.save_event(event)
                next_tr = next_tr.getnext()

########NEW FILE########
__FILENAME__ = legislators
import re
import itertools

from billy.scrape.legislators import LegislatorScraper, Legislator
from .utils import legislators_url

import lxml.html


class PALegislatorScraper(LegislatorScraper):
    jurisdiction = 'pa'

    def scrape(self, chamber, term):
        # Pennsylvania doesn't make member lists easily available
        # for previous sessions, unfortunately
        self.validate_term(term, latest_only=True)

        leg_list_url = legislators_url(chamber)

        page = self.urlopen(leg_list_url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(leg_list_url)

        for link in page.xpath("//a[contains(@href, '_bio.cfm')]"):
            full_name = link.text
            district = link.getparent().getnext().tail.strip()
            district = re.search("District (\d+)", district).group(1)

            party = link.getparent().tail.strip()[-2]
            if party == 'R':
                party = 'Republican'
            elif party == 'D':
                party = 'Democratic'

            url = link.get('href')

            legislator = Legislator(term, chamber, district,
                                    full_name, party=party, url=url)
            legislator.add_source(leg_list_url)

            # Scrape email, offices, photo.
            page = self.urlopen(url)
            doc = lxml.html.fromstring(page)
            doc.make_links_absolute(url)

            self.scrape_email_address(url, page, legislator)
            self.scrape_offices(url, doc, legislator)
            self.save_legislator(legislator)

    def scrape_email_address(self, url, page, legislator):
        if re.search(r'var \S+\s+= "(\S+)";', page):
            vals = re.findall(r'var \S+\s+= "(\S+)";', page)
            legislator['email'] = '%s@%s%s' % tuple(vals)

    def scrape_offices(self, url, doc, legislator):
        el = doc.xpath('//h4[contains(., "Contact")]/..')[0]
        for office in Offices(el, self):
            legislator.add_office(**office)
        legislator.add_source(url)


class Offices(object):
    '''Terrible. That's what PA's offices are.
    '''

    class ParseError(Exception):
        pass

    def __init__(self, el, scraper):
        self.el = el
        self.scraper = scraper
        lines = list(el.itertext())[5:]
        lines = [x.strip() for x in lines]
        lines = filter(None, lines)
        self.lines = lines

    def __iter__(self):
        try:
            for lines in self.offices_lines():
                yield Office(lines).parsed()
        except self.ParseError:
            self.scraper.logger.warning("Couldn't parse offices.")
            return

    def break_at(self):
        '''The first line of the address, usually his/her name.'''
        lines = self.lines[::-1]

        # The legr's full name is the first line in each address.
        junk = set('contact district capitol information'.split())
        while True:
            try:
                break_at = lines.pop()
            except IndexError:
                raise self.ParseError

            # Skip lines that are like "Contact" instead of
            # the legislator's full name.
            if junk & set(break_at.lower().split()):
                continue
            else:
                break
        return break_at

    def offices_lines(self):
        office = []
        lines = self.lines
        break_at = self.break_at()
        while lines and True:
            line = lines.pop()
            if line == break_at:
                yield office
                office = []
            else:
                office.append(re.sub(r'\s+', ' ', line))


class Office(object):
    '''They're really quite bad.'''

    re_phone = re.compile(r' \d{3}\-\d{4}')
    re_fax = re.compile(r'^\s*fax:\s*', re.I)

    def __init__(self, lines):
        junk = ['Capitol', 'District']
        self.lines = [x for x in lines if x not in junk]

    def phone(self):
        '''Return the first thing that looks like a phone number.'''
        lines = filter(self.re_phone.search, self.lines)
        for line in lines:
            if not line.strip().lower().startswith('fax:'):
                return line.strip()

    def fax(self):
        lines = filter(self.re_fax.search, self.lines)
        if lines:
            return self.re_fax.sub('', lines.pop()) or None

    def type_(self):
        for line in self.lines:
            if 'capitol' in line.lower():
                return 'capitol'
            elif 'east wing' in line.lower():
                return 'capitol'
        return 'district'

    def name(self):
        return self.type_().title() + ' Office'

    def address(self):
        lines = itertools.ifilterfalse(self.re_phone.search, self.lines)
        lines = itertools.ifilterfalse(self.re_fax.search, lines)
        lines = list(lines)
        for i, line in enumerate(lines):
            if re.search('PA \d{5}', line):
                break

        # If address lines are backwards, fix.
        if i <= 2:
            lines = lines[::-1]

        # Make extra sure "PA 12345" line is last.
        count = 0
        while not re.search(', PA( \d{5})?', lines[-1]):
            lines = lines[-1:] + lines[:-1]
            count += 1
            if count > 1000:
                # If we're here, this address is badly hosed; skip.
                return
        address = '\n'.join(lines)
        return address

    def parsed(self):
        return dict(
            phone=self.phone(),
            fax=self.fax(),
            address=self.address(),
            type=self.type_(),
            name=self.name())

########NEW FILE########
__FILENAME__ = utils
import datetime


def bill_abbr(chamber):
    if chamber == 'upper':
        return 'S'
    else:
        return 'H'


def start_year(session):
    return session[0:4]


def parse_action_date(date_str):
    date_str = date_str.replace('Sept.', 'September')
    try:
        date = datetime.datetime.strptime(date_str, '%b. %d, %Y')
    except ValueError:
        date = datetime.datetime.strptime(date_str, '%B %d, %Y')
    return date


def bill_list_url(chamber, session, special):
    return 'http://www.legis.state.pa.us/cfdocs/legis/bi/'\
        'BillIndx.cfm?sYear=%s&sIndex=%i&bod=%s' % (
        start_year(session), special, bill_abbr(chamber))


def history_url(chamber, session, special, type, bill_number):
    return 'http://www.legis.state.pa.us/cfdocs/billinfo/'\
        'bill_history.cfm?syear=%s&sind=%i&body=%s&type=%s&BN=%s' % (
        start_year(session), special, bill_abbr(chamber), type, bill_number)


def info_url(chamber, session, special, type, bill_number):
    return 'http://www.legis.state.pa.us/cfdocs/billinfo/'\
        'billinfo.cfm?syear=%s&sind=%i&body=%s&type=%s&BN=%s' % (
        start_year(session), special, bill_abbr(chamber), type, bill_number)


def vote_url(chamber, session, special, type, bill_number):
    return 'http://www.legis.state.pa.us/cfdocs/billinfo/'\
        'bill_votes.cfm?syear=%s&sind=%d&body=%s&type=%s&bn=%s' % (
        start_year(session), special, bill_abbr(chamber), type, bill_number)


def legislators_url(chamber):
    if chamber == 'upper':
        return "http://www.legis.state.pa.us/cfdocs/legis/home/"\
            "member_information/senators_alpha.cfm"
    else:
        return "http://www.legis.state.pa.us/cfdocs/legis/home/"\
            "member_information/representatives_alpha.cfm"

########NEW FILE########
__FILENAME__ = bills
# -*- coding: utf-8 -*-
from billy.scrape import ScrapeError, NoDataForPeriod
from billy.scrape.votes import Vote
from billy.scrape.bills import BillScraper, Bill
from .utils import grouper, doc_link_url, year_from_session

import lxml.html
import datetime
import itertools
import subprocess
import shutil
import os
import re

class NoSuchBill(Exception):
    pass

_voteChambers = (
    (u'Aprobado por el Senado en Votac','upper'),
    (u'Aprobado por C','lower'),
)
_docVersion = (
    ('Entirillado del Informe'),
    ('Texto de Aprobaci'),
#    ('Ley N'),
    ('rendido con enmiendas'),
    ('Radicado'),
)
_classifiers = (
    ('Radicado','', 'bill:introduced'),
    (u'Aprobado por Cámara en Votación Final','lower', 'bill:passed'),
    (u'Aprobado por el Senado en Votación','upper', 'bill:passed'),
    ('Aparece en Primera Lectura del', 'upper','bill:reading:1'),
    ('Aparece en Primera Lectura de la','lower','bill:reading:1'),
    ('Enviado al Gobernador', 'governor','governor:received'),
    ('Veto', 'governor','governor:vetoed'),
    ('Veto de Bolsillo','governor','governor:vetoed'),
    # comissions give a report but sometimes they dont do any amendments and
    # leave them as they are.
    # i am not checking if they did or not. but it be easy just read the end and
    # if it dosnt have amendments it should say 'sin enmiendas'
    ('1er Informe','committee','amendment:amended'),
    ('2do Informe','committee','amendment:amended'),
    ('Aprobado con enmiendas','','amendment:passed'),
    (u'Remitido a Comisión','', 'committee:referred'),
    (u'Referido a Comisión','', 'committee:referred'),
    ('En el Calendario de Ordenes Especiales de la C','lower','other'),
    ('Texto de Aprobación Final enviado al Senado','upper','other'),
    ('Retirada por su Autor','sponsor','bill:withdrawn'),
    ('Comisión : * no recomienda aprobación de la medida','','committee:passed:unfavorable'),
    ('Ley N','governor','governor:signed')
)


class PRBillScraper(BillScraper):
    jurisdiction = 'pr'

    bill_types = {
        'P': 'bill',
        'R': 'resolution',
        'RK': 'concurrent resolution',
        'RC': 'joint resolution',
        #'PR': 'plan de reorganizacion',
    }

    def clean_name(self, name):
        for ch in ['Sr,','Sr.','Sra.','Rep.','Sen.']:
            if ch in name:
                name = name.replace(ch,'')
        return name

    def scrape(self, chamber, session):
        # check for abiword
        if os.system('which abiword') != 0:
            raise ScrapeError('abiword is required for PR scraping')


        year = session[0:4]
        self.base_url = 'http://www.oslpr.org/legislatura/tl%s/tl_medida_print2.asp' % year
        chamber_letter = {'lower':'C','upper':'S'}[chamber]
        for code, type in self.bill_types.iteritems():
            counter = itertools.count(1)
            for n in counter:
                bill_id = '%s%s%s' % (code, chamber_letter, n)
                try:
                    self.scrape_bill(chamber, session, bill_id, type)
                except NoSuchBill:
                    break

    def parse_action(self,chamber,bill,action,action_url,date):
        #if action.startswith('Referido'):
                #committees = action.split(',',1)
                #multiple committees
        if action.startswith('Ley N'):
                action = action[0:42]
        elif action.startswith('Res. Conj.'):
                action = action[0:42]
        action_actor = ''
        atype = 'other'
        #check it has a url and is not just text
        if action_url:
            action_url = action_url[0]
            isVersion = False;
            for text_regex in _docVersion:
                if re.match(text_regex, action):
                   isVersion = True;
            if isVersion:
                # versions are mentioned several times, lets use original name
                if action_url.lower().endswith('.doc'):
                    mimetype = 'application/msword'
                elif action_url.lower().endswith('.rtf'):
                    mimetype = 'application/rtf'
                elif action_url.lower().endswith('.pdf'):
                    mimetype = 'application/pdf'
                elif action_url.lower().endswith('docx'):
                    mimetype = 'application/octet-stream'
                else:
                    raise Exception('unknown version type: %s' % action_url)
                bill.add_version(action, action_url, on_duplicate='use_old',
                                 mimetype=mimetype)
            else:
                bill.add_document(action, action_url)
            for pattern, action_actor,atype in _classifiers:
                if re.match(pattern, action):
                    break
                else:
                    action_actor = ''
                    atype = 'other'
        if action_actor == '':
            if action.find('SENADO') != -1:
                action_actor = 'upper'
            elif action.find('CAMARA') != -1:
                action_actor = 'lower'
            else:
                action_actor = chamber
        #if action.startswith('Referido'):
            #for comme in committees:
            #print comme
        bill.add_action(action_actor, action.replace('.',''),date,type=atype)
        return atype,action

    def scrape_bill(self, chamber, session, bill_id, bill_type):
        url = '%s?r=%s' % (self.base_url, bill_id)
        html = self.urlopen(url)
        if "error '80020009'" in html:
            self.warning('asp error on page, skipping %s', bill_id)
            return
        doc = lxml.html.fromstring(html)
        # search for Titulo, accent over i messes up lxml, so use 'tulo'
        title = doc.xpath(u'//td/b[contains(text(),"tulo")]/../following-sibling::td/text()')
        if not title:
            raise NoSuchBill()
        bill = Bill(session, chamber, bill_id, title[0], type=bill_type)
        author = doc.xpath(u'//td/b[contains(text(),"Autor")]/../text()')[0]
        for aname in author.split(','):
            aname = self.clean_name(aname).strip()
            if aname:
                bill.add_sponsor('primary', aname)
        co_authors = doc.xpath(u'//td/b[contains(text(),"Co-autor")]/../text()')
        if len(co_authors) != 0:
            for co_author in co_authors[1].split(','):
                bill.add_sponsor('cosponsor', self.clean_name(co_author).strip());
        action_table = doc.xpath('//table')[-1]
        for row in action_table[1:]:
            tds = row.xpath('td')
            # ignore row missing date
            if len(tds) != 2:
                continue
            if tds[0].text_content():
                date = datetime.datetime.strptime(tds[0].text_content(), "%m/%d/%Y")
            action = tds[1].text_content().strip()
            #parse the text to see if it's a new version or a unrelated document
            #if has - let's *shrug* assume it's a vote document

            #get url of action
            action_url = tds[1].xpath('a/@href')
            atype,action = self.parse_action(chamber,bill,action,action_url,date)
            if atype == 'bill:passed' and action_url:
                vote_chamber  = None
                for pattern, vote_chamber in _voteChambers:
                   if re.match(pattern,action):
                       break

                else:
                   self.warning('coudnt find voteChamber pattern')

                if vote_chamber == 'lower' and len(action_url) > 0:
                    vote = self.scrape_votes(action_url[0], action,date,
                                             vote_chamber)
                    if not vote[0] == None:
                        vote[0].add_source(action_url[0])
                        bill.add_vote(vote[0])
                    else:
                        self.warning('Problem Reading vote: %s,%s' %
                                     (vote[1], bill_id))

        bill.add_source(url)
        self.save_bill(bill)

    def get_filename_parts_from_url(self,url):
        fullname = url.split('/')[-1].split('#')[0].split('?')[0]
        t = list(os.path.splitext(fullname))
        if t[1]:
            t[1] = t[1][1:]
            return t

    def scrape_votes(self, url, motion, date, bill_chamber):
        if isinstance(url,basestring):
            filename1, extension = self.get_filename_parts_from_url(url)
        else:
            return None, 'No url'
        if extension == 'pdf':
            return None,'Vote on PDF'

        vote_doc, resp = self.urlretrieve(url)

        # use abiword to convert document
        html_name = vote_doc + '.html'
        subprocess.check_call('abiword --to=%s %s' % (html_name, vote_doc),
                              shell=True, cwd='/tmp/')
        text = open(html_name).read()
        os.remove(html_name)
        try:
            # try and remove files too
            shutil.rmtree(html_name + '_files')
        except OSError:
            pass
        os.remove(vote_doc)

        yes_votes = []
        no_votes = []
        other_votes = []

        doc = lxml.html.fromstring(text)

#           header = doc.xpath('/html/body/div/table[1]/tbody/tr[1]/td[2]')
#           header_txt = header[0][0][0].text_content();
#           assembly_number = header[0][0][1].text_content();
#           bill_id = header[0][0][3].text_content().lstrip().rstrip();
        #show legislator,party,yes,no,abstention,observations

        table = doc.xpath('/html/body/div/table[2]/tbody')
        if len(table) == 0:
            return None,'Table body Problem'

        # they have documents(PC0600') that have the action name as vote but in
        # reality the actual content is the bill text which breaks the parser
        try:
            table[0].xpath('tr')[::-1][0].xpath('td')[1]
        except IndexError:
            table = doc.xpath('/html/body/div/table[3]/tbody')

        #loop thru table and skip first one
        vote = None
        for row in table[0].xpath('tr')[::-1]:
            tds = row.xpath('td')
            party = tds[1].text_content().replace('\n', ' ').replace(' ','').replace('&nbsp;','');
            yes_td =  tds[2].text_content().replace('\n', ' ').replace(' ','').replace('&nbsp;','')
            nays_td =  tds[3].text_content().replace('\n', ' ').replace(' ','').replace('&nbsp;','')
            abstent_td =  tds[4].text_content().replace('\n', ' ').replace(' ','').replace('&nbsp;','')
            if party != 'Total':
                name_td = self.clean_name(tds[0].text_content().replace('\n', ' ').replace('','',1)).strip();
                split_name = name_td.split(',')
                if len(split_name) > 1:
                   name_td = split_name[1].strip() + ' ' + split_name[0].strip()

                if yes_td == 'r':
                    yes_votes.append(name_td)
                if nays_td == 'r':
                    no_votes.append(name_td)
                if abstent_td == 'r':
                    other_votes.append(name_td)
                #observations
#                   observations_td =  tds[5].text_content().replace('\n', ' ').replace(' ','').replace('&nbsp;','')
#                   print observations_td


        # return vote object
        yes_count = len(yes_votes)
        no_count = len(no_votes)
        other_count = len(other_votes)
        #FIXME: Since i am searching for the word passed it means that passed
        # will always be true.
        vote = Vote(bill_chamber, date, motion, True, yes_count, no_count,
                    other_count)
        vote['yes_votes'] = yes_votes
        vote['no_votes'] = no_votes
        vote['other_votes'] = other_votes
        return vote,'Good'

########NEW FILE########
__FILENAME__ = committees
# -*- coding: utf-8 -*-
import lxml.html
import lxml.etree
import os
from billy.scrape import NoDataForPeriod
from billy.scrape.committees import CommitteeScraper, Committee
from billy.scrape.utils import convert_pdf

import re

def clean_spaces(s):
    """ remove \xa0, collapse spaces, strip ends """
    if s is not None:
        return re.sub('\s+', ' ', s.replace(u'\xa0', ' ')).strip()


class PRCommitteeScraper(CommitteeScraper):
    jurisdiction = 'pr'
    latest_only = True

    def scrape(self, term, chambers):
        if 'upper' in chambers:
            self.scrape_upper()
        elif 'lower' in chambers:
            self.scrape_lower()

    def scrape_upper(self):
        raise Exception('needs to be rewritten for 2013')

    def scrape_upper_committee(self, url):
        filename, resp = self.urlretrieve(url)
        root = lxml.etree.fromstring( convert_pdf(filename,'xml'))
        for link in root.xpath('/pdf2xml/page'):
            comm = None
            for line in link.findall('text'):
                text = line.findtext('b')
                if text is not None and text.startswith('Comisi'):
                    comm = Committee('upper',text);
                    comm.add_source(url)
                else:
                    if line.text and line.text.startswith('Hon.'):
                        line_text = line.text.replace(u'–','-')
                        name_split = line_text.split(u'-',1)
                        title = 'member'
#           print name_split
                        if len(name_split) >= 2:
                            name_split[1] = name_split[1].strip()
                            if name_split[1] == 'Presidenta' or name_split[1] == 'Presidente':
                                title = 'chairman'
                            elif name_split[1] == 'Vicepresidente' or name_split[1] == 'Vicepresidenta':
                                title = 'vicechairman'
                            elif name_split[1] == 'Secretaria' or name_split[1] == 'Secretario':
                                title = 'secretary'
#           if title != 'member':
#               print name_split[0]
                        if name_split[0] != 'VACANTE':
                            comm.add_member(name_split[0].replace('Hon.',''),title)
            self.save_committee(comm)
        os.remove(filename);

    def scrape_lower(self):
        url = 'http://www.camaraderepresentantes.org/comisiones.asp'
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        for link in doc.xpath('//a[contains(@href, "comisiones2")]'):
            self.scrape_lower_committee(link.text, link.get('href'))

    def scrape_lower_committee(self, name, url):
        com = Committee('lower', name)
        com.add_source(url)

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        contact, directiva, reps = doc.xpath('//div[@class="sbox"]/div[2]')
        # all members are tails of images (they use img tags for bullets)
        # first three members are in the directiva div
        chair = directiva.xpath('b[text()="Presidente:"]/following-sibling::img[1]')
        vchair = directiva.xpath('b[text()="Vice Presidente:"]/following-sibling::img[1]')
        sec = directiva.xpath('b[text()="Secretario(a):"]/following-sibling::img[1]')
        member = 0;
        if chair and chair[0].tail is not None:
            chair = chair[0].tail
            com.add_member(clean_spaces(chair), 'chairman')
            member += 1
        if vchair and vchair[0].tail is not None:
            vchair = vchair[0].tail
            com.add_member(clean_spaces(vchair), 'vice chairman')
            member += 1
        if sec and sec is not None:
            sec = sec[0].tail
            com.add_member(clean_spaces(sec), 'secretary')
            member += 1

        for img in reps.xpath('.//img'):
            member_name = clean_spaces(img.tail)
            if member_name is not None:
                com.add_member(member_name)
                member += 1
        if member > 0:
            self.save_committee(com)

########NEW FILE########
__FILENAME__ = events

import re
import datetime
import itertools
from billy.scrape import NoDataForPeriod,ScrapeError
from billy.scrape.events import Event, EventScraper

import pytz
import lxml.html
import feedparser


class PREventScraper(EventScraper):
    jurisdiction = 'pr'

    _tz = pytz.timezone('US/Alaska')

    def scrape(self, chamber, session):
        if chamber == 'lower':
            return

        self.upper_url = 'http://www.senadopr.us/Lists/Calendario%20Legislativo/DispForm_original.aspx?ID='#29 is the start number for the counter <_<
        self.lower_url = 'http://www.camaraderepresentantes.org/cr_calendario.asp?d=3/28/2007'# %
        counter = itertools.count(29)
        for event_id in counter:
            try:
                self.scrape_events(chamber, session,event_id)
            except ScrapeError:
                break

        #year, year2 = None, None
        #for term in self.metadata['terms']:
            #if term['sessions'][0] == session:
                #year = str(term['start_year'])
                #year2 = str(term['end_year'])
                #break

    def scrape_events(self, chamber,session,event_id):
        url = '%s%s' % (self.upper_url, event_id)
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        rows = doc.xpath("//div[@id='WebPartWPQ2']")
        #some ids are empty
        if len( rows):
            table_data = rows[0].find('table')[1]

            for link in table_data.iterchildren('td'):
                td = link.xpath('//td[@class="ms-formbody"]')

                description =  td[18].text
                when =  td[19].text
                where = td[25].text
                type = td[27].text
                meeting_lead =  td[28].text

                when = datetime.datetime.strptime(when, "%m/%d/%Y  %H:%M %p")
                when = self._tz.localize(when)
                event_type = 'committee:meeting';
                kwargs = { "location" : "State House" }
                if where is not None and where != "":
                    kwargs['location'] = where
                event = Event(session, when, event_type, description, **kwargs)

                if td[20].text is None:
                    participants = meeting_lead
                else:
                    participants = td[20].text.split(';')
                if participants:
                    for participant in participants:
                        name = participant.strip().replace('HON.','',1)
                        if name != "":
                            event.add_participant('committee',
                                                  name,
                                                  'committee',
                                                  chamber=chamber)

                event.add_source(url)
                self.save_event(event)
        else :
            #hack so we dont fail on the first id numbers where there are some gaps between the numbers that work and not.
            if event_id > 1700:
                raise ScrapeError("Parsing is done we are on future ids that are not used yet.")

########NEW FILE########
__FILENAME__ = legislators
from billy.scrape import NoDataForPeriod
from billy.scrape.legislators import LegislatorScraper, Legislator

import lxml.html
import re
import unicodedata
import scrapelib

PHONE_RE = re.compile('\(?\d{3}\)?\s?-?\d{3}-?\d{4}')

class PRLegislatorScraper(LegislatorScraper):
    jurisdiction = 'pr'

    def scrape(self, chamber, term):
        self.validate_term(term, latest_only=True)

        if chamber == 'upper':
            self.scrape_senate(term)
        elif chamber == 'lower':
            self.scrape_house(term)

    def scrape_senate(self, term):
        urls = { 
            'At-Large': 'http://www.senadopr.us/Pages/SenadoresporAcumulacion.aspx',
            'I': 'http://www.senadopr.us/Pages/Senadores%20Distrito%20I.aspx',
            'II': 'http://www.senadopr.us/Pages/Senadores%20Distrito%20II.aspx',
            'III': 'http://www.senadopr.us/Pages/Senadores%20Distrito%20III.aspx',
            'IV': 'http://www.senadopr.us/Pages/Senadores%20Distrito%20IV.aspx',
            'V': 'http://www.senadopr.us/Pages/Senadores%20Distrito%20V.aspx',
            'VI': 'http://www.senadopr.us/Pages/Senadores%20Distrito%20VI.aspx',
            'VII': 'http://www.senadopr.us/Pages/Senadores%20Distrito%20VII.aspx',
            'VIII': 'http://www.senadopr.us/Pages/Senadores%20Distrito%20VIII.aspx'
        }

        for district, url in urls.iteritems():
            leg_page_html = self.urlopen(url)
            doc = lxml.html.fromstring(leg_page_html)
            doc.make_links_absolute(url)
            table = doc.xpath('//table[@summary="Senadores 2013-2016"]')[0]

            # skip first row
            for row in table.xpath('tr')[1:]:
                tds = row.xpath('td')

                name = tds[0].text_content().title().replace('Hon.','',1).strip()
                party = tds[1].text_content()
                phone = tds[2].text_content()
                email = tds[3].text_content()

                #Code to guess the picture
                namefixed = unicode(name.replace(".",". "))  #Those middle names abbreviations are sometimes weird.
                namefixed = unicodedata.normalize('NFKD', namefixed).encode('ascii', 'ignore') #Remove the accents
                nameparts = namefixed.split()
                if nameparts[1].endswith('.'):
                    lastname = nameparts[2]
                else:
                    lastname = nameparts[1]

                # Construct the photo url
                picture_filename = 'http://www.senadopr.us/Fotos%20Senadores/sen_' + (nameparts[0][0] + lastname).lower() + '.jpg'

                try:
                    picture_data = self.urlopen(picture_filename)  # Checking to see if the file is there
                    leg = Legislator(term, 'upper', district, name,
                                     party=party,
                                     email=email, url=url,
                                     photo_url=picture_filename)

                except scrapelib.HTTPError:         # If not, leave out the photo_url
                    leg = Legislator(term, 'upper', district, name,
                                     party=party, phone=phone, email=email,
                                     url=url)

                leg.add_office('capitol', 'Oficina del Capitolio',
                               phone=phone)
                leg.add_source(url)
                self.save_legislator(leg)

    def scrape_house(self, term):
        url = 'http://www.camaraderepresentantes.org/cr_legs.asp'

        party_map = {'PNP': 'Partido Nuevo Progresista',
                     'PPD': u'Partido Popular Democr\xe1tico'}

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        tables = doc.xpath('//table[@width="90%"]')

        # first table is district-based, second is at-large
        for table, at_large in zip(tables, [False, True]):

            for tr in table.xpath('.//tr')[1:]:
                tds = tr.getchildren()
                if not at_large:
                    # tds: name, district, addr, phone, office, email
                    name = tds[0]
                    district = tds[1].text_content().lstrip('0')
                    capitol_office = tds[2]
                    phone = tds[3]
                    email = tds[5]
                    # district offices
                    district_office = tds[4]
                    district_addr = []
                    district_phone  = None
                    district_fax = None
                    pieces = district_office.xpath('.//text()')
                    for piece in pieces:
                        if piece.startswith('Tel'):
                            district_phone = PHONE_RE.findall(piece)[0]
                        elif piece.startswith('Fax'):
                            district_fax = PHONE_RE.findall(piece)[0]
                        else:
                            district_addr.append(piece)
                    if district_addr:
                        district_addr = ' '.join(district_addr)
                else:
                    # name, addr, phone, email
                    name = tds[0]
                    district = 'At-Large'
                    capitol_office = tds[1]
                    phone = tds[2]
                    email = tds[3]
                    district_addr = None

                # cleanup is same for both tables
                name = re.sub('\s+', ' ',
                              name.text_content().strip().replace(u'\xa0', ' '))
                email = email.xpath('.//a/@href')[0].strip('mailto:')

                numbers = {}
                for b in phone.xpath('b'):
                    numbers[b.text] = b.tail.strip()

                # capitol_office as provided is junk
                # things like 'Basement', and '2nd Floor'

                # urls @ http://www.camaraderepresentantes.org/legs2.asp?r=BOKCADHRTZ
                # where random chars are tr's id
                leg_url = 'http://www.camaraderepresentantes.org/legs2.asp?r=' + tr.get('id')

                leg = Legislator(term, 'lower', district, name,
                                 party='unknown', email=email, url=url)
                leg.add_office('capitol', 'Oficina del Capitolio',
                               phone=numbers.get('Tel:') or None,
                               # could also add TTY
                               #tty=numbers.get('TTY:') or None,
                               fax=numbers.get('Fax:') or None)
                if district_addr:
                    leg.add_office('district', 'Oficina de Distrito',
                                   address=district_addr,
                                   phone=district_phone,
                                   fax=district_fax)

                leg.add_source(url)
                self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = utils
import itertools

# From the itertools docs's recipe section 
def grouper(n, iterable, fillvalue=None):
    "grouper(3, 'ABCDEFG', 'x') --> ABC DEF Gxx"
    args = [iter(iterable)] * n
    return itertools.izip_longest(fillvalue=fillvalue, *args) 

def clean_newline(str):
        new_str = ' '.join(str.split('\n'))
        return new_str

def between_keywords(key1, key2, str):
    right_part = str.split(key1)[0]
    return right_part.split(key2)[1]

def doc_link_url(doc_link_part):
    return 'http://www.camaraderepresentantes.org' + doc_link_part  


def year_from_session(session):
    return int(session.split()[0])

########NEW FILE########
__FILENAME__ = bills
import datetime as dt
import lxml.html
import urllib
import re

from billy.scrape.bills import BillScraper, Bill
from billy.scrape.utils import url_xpath

subjects      = None
bill_subjects = None

HB_START_BILLNO=7000
SB_START_BILLNO=2000
# This was 2012 and 2014's list.
#   (The second half of a term.)
# XXX: Change this for the 2015 session.

#HB_START_BILLNO=5000
#SB_START_BILLNO=1
# This was 2013's list.
#   (The first half of a term.)
# XXX: Change this for the 2015 session.

START_IDEX = {
    "lower" : HB_START_BILLNO,
    "upper" : SB_START_BILLNO
}

MAXQUERY=250 # What a silly low number. This is just putting more load on the
# server, not even helping with that. Sheesh.

def get_postable_subjects():
    global subjects
    if subjects == None:
        subs = url_xpath( "http://status.rilin.state.ri.us/",
            "//select[@id='rilinContent_cbCategory']" )[0].xpath("./*")
        subjects = { o.text : o.attrib['value'] for o in subs }
        subjects.pop(None)
    return subjects

def get_default_headers( page ):
    headers = {}
    for el in url_xpath( page, "//*[@name]" ):
        name = el.attrib['name']
        value = ""
        try:
            value = el.attrib['value']
        except KeyError:
            value = el.text

        if value:
            value = value.strip()

        headers[name] = value or ""
    headers['__EVENTTARGET'] = ""
    headers['__EVENTARGUMENT'] = ""
    headers['__LASTFOCUS'] = ""
    return headers

SEARCH_URL = "http://status.rilin.state.ri.us/"

BILL_NAME_TRANSLATIONS = {
    "House Bill No."  : "HB",
    "Senate Bill No." : "SB",
    "Senate Resolution No." : "SR",
    "House Resolution No."  : "HR"
}

BILL_STRING_FLAGS = {
    "bill_id"    : r"^[House|Senate].*",
    "sponsors"   : r"^BY.*",
    "title"      : r"ENTITLED,.*",
    "version"    : r"\{.*\}",
    "resolution" : r"Resolution.*",
    "chapter"    : r"^Chapter.*",
    "by_request" : r"^\(.*\)$",
    "act"        : r"^Act\ \d*$"
}

class RIBillScraper(BillScraper):
    jurisdiction = 'ri'

    def parse_results_page( self, page ):
        blocks  = []
        current = []

        p = lxml.html.fromstring(page)
        if "We're Sorry! You seem to be lost." in p.text_content():
            raise ValueError('POSTing has gone wrong')

        nodes = p.xpath("//span[@id='lblBills']/*")
        for node in nodes:
            if node.tag == "br":
                if len(current) > 0:
                    blocks.append(current)
                    current = []
            else:
                current.append(node)
        if current:
            blocks.append(current)
        return blocks

    def digest_results_page( self, nodes ):
        blocks = {}
        for node in nodes:
            nblock = { 'actions' : [] }
            lines = [(n.text_content().strip(), n) for n in node]
            if 'No Bills Met this Criteria' in [x[0] for x in lines]:
                self.info("No results. Skipping block")
                return []

            for line in lines:
                line, node = line
                if ('Total Bills:' in line and
                        'State House, Providence, Rhode Island' in line):
                    continue

                found = False
                for regexp in BILL_STRING_FLAGS:
                    if re.match(BILL_STRING_FLAGS[regexp], line):
                        hrefs = node.xpath("./a")
                        if len(hrefs) > 0:
                             nblock[regexp + "_hrefs"] = hrefs
                        nblock[regexp] = line
                        found = True
                if not found:
                    nblock['actions'].append(line)

            self.info("Working on %s" % (nblock.get("bill_id")))
            if "bill_id" in nblock:
                blocks[nblock['bill_id']] = nblock
            else:
                self.warning(lines)
                self.warning("ERROR! Can not find bill_id for current entry!")
                self.warning("This should never happen!!! Oh noes!!!")
        return blocks

    def get_subject_bill_dict(self, session):
        global bill_subjects
        if bill_subjects != None:
            return bill_subjects
        ret = {}
        subjects = get_postable_subjects()
        self.info('getting subjects (total=%s)', len(subjects))
        for subject in subjects:

            default_headers = get_default_headers( SEARCH_URL )

            default_headers['ctl00$rilinContent$cbCategory'] = \
                subjects[subject]
            default_headers['ctl00$rilinContent$cbYear'] = session

            #headers = urllib.urlencode( default_headers )

            #print "\n".join([
            #    "%s: %s" % (x, default_headers[x][:20]) for x in default_headers
            #])

            blocks = self.parse_results_page(self.urlopen( SEARCH_URL,
                method="POST", body=default_headers))
            blocks = blocks[1:-1]
            blocks = self.digest_results_page(blocks)
            for block in blocks:
                try:
                    ret[block].append(subject)
                except KeyError:
                    ret[block] = [ subject ]
        bill_subjects = ret
        return bill_subjects

    def process_actions( self, actions, bill ):
        for action in actions:
            actor = "joint"

            if "house"  in action.lower():
                actor = "lower"

            if "senate" in action.lower():
                if actor == "joint":
                    actor = "upper"
                else:
                    actor = "joint"
            if "governor" in action.lower():
                actor = "governor"
            date = action.split(" ")[0]
            date = dt.datetime.strptime(date, "%m/%d/%Y")
            bill.add_action( actor, action, date,
                type=self.get_type_by_action(action))

    def get_type_by_name(self, name):
        name = name.lower()
        self.log(name)

        things = [
            "resolution",
            "joint resolution"
            "memorial",
            "memorandum",
            "bill"
        ]

        for t in things:
            if t in name:
                self.log( "Returning %s" % t )
                return t

        self.warning("XXX: Bill type fallthrough. This ain't great.")
        return "bill"

    def get_type_by_action(self, name):
        types = {
            "introduced" : "bill:introduced",
            "referred"   : "committee:referred",
            "passed"     : "bill:passed",
            "recommends passage" : "committee:passed:favorable",
            # XXX: need to find the unfavorable string
            # XXX: What's "recommended measure be held for further study"?
            "withdrawn"               : "bill:withdrawn",
            "signed by governor"      : "governor:signed",
            "transmitted to governor" : "governor:received"
        }
        ret = []
        name = name.lower()
        for flag in types:
            if flag in name:
                ret.append(types[flag])

        if len(ret) > 0:
            return ret
        return "other"

    def scrape_bills(self, chamber, session, subjects):
        idex = START_IDEX[chamber]
        FROM="ctl00$rilinContent$txtBillFrom"
        TO="ctl00$rilinContent$txtBillTo"
        YEAR="ctl00$rilinContent$cbYear"
        blocks = "FOO" # Ugh.
        while len(blocks) > 0:
            default_headers = get_default_headers( SEARCH_URL )
            default_headers[FROM] = idex
            default_headers[TO]   = idex + MAXQUERY
            default_headers[YEAR] = session
            idex += MAXQUERY
            #headers = urllib.urlencode( default_headers )
            blocks = self.parse_results_page(self.urlopen( SEARCH_URL,
                method="POST", body=default_headers))
            blocks = blocks[1:-1]
            blocks = self.digest_results_page(blocks)

            for block in blocks:
                bill = blocks[block]
                subs = []
                try:
                    subs = subjects[bill['bill_id']]
                except KeyError:
                    pass

                title = bill['title'][len("ENTITLED, "):]
                billid = bill['bill_id']
                try:
                    subs   = subjects[bill['bill_id']]
                except KeyError:
                    subs   = []

                for b in BILL_NAME_TRANSLATIONS:
                    if billid[:len(b)] == b:
                        billid = BILL_NAME_TRANSLATIONS[b] + \
                            billid[len(b)+1:].split()[0]

                b = Bill(session, chamber, billid, title,
                    type=self.get_type_by_name(bill['bill_id']),
                    subjects=subs
                )

                self.process_actions( bill['actions'], b )
                sponsors = bill['sponsors'][len("BY"):].strip()
                sponsors = sponsors.split(",")
                sponsors = [ s.strip() for s in sponsors ]

                for href in bill['bill_id_hrefs']:
                    b.add_version( href.text, href.attrib['href'],
                        mimetype="application/pdf" )

                for sponsor in sponsors:
                    b.add_sponsor("primary", sponsor)

                b.add_source( SEARCH_URL )
                self.save_bill(b)
                # print bill['bill_id'], subs

    def scrape(self, chamber, session):
        subjects = self.get_subject_bill_dict(session)
        self.scrape_bills( chamber, session, subjects )

########NEW FILE########
__FILENAME__ = committees
import re
import urlparse
import datetime

from billy.scrape import NoDataForPeriod
from billy.scrape.committees import CommitteeScraper, Committee

import lxml.etree, lxml.html

COMM_BLACKLIST = [
    "Constitutional and Regulatory Issues"
    # This page is timing out. This is most likely an issue with Upstream,
    # it seems to happen all over the place.
]

class RICommitteeScraper(CommitteeScraper):
    jurisdiction = 'ri'

    def scrape(self, chamber, term_name):
        self.validate_term(term_name, latest_only=True)

        if chamber == 'upper':
            self.scrape_senate_comm()
            # scrape joint committees under senate
            self.scrape_joint_comm()
        elif chamber == 'lower':
            self.scrape_reps_comm()

    def scrape_comm_list(self, ctype):
        url = 'http://webserver.rilin.state.ri.us/CommitteeMembers/'
        self.log("looking for "+ctype)
        page = self.urlopen(url)
        root = lxml.html.fromstring(page)
        return root.xpath("//a[contains(@href,'"+ctype+"')]")

    def add_members(self,comm,url):
        page = self.urlopen(url)
        self.log(comm)
        root = lxml.html.fromstring(page)
        # The first <tr> in the table of members
        membertable=root.xpath('//p[@class="style28"]/ancestor::table[1]')[0]
        members = membertable.xpath("*")[1:]

        order = {
            "name" : 0,
            "appt" : 1,
            "email" : 2
        }

        prefix = "Senator"

        for member in members:
            name = member[order['name']].text_content().strip()
            if name[:len(prefix)] == prefix:
                name = name[len(prefix):].strip()
            appt = member[order['appt']].text_content().strip()
            self.log("name "+ name +" role " + appt)
            comm.add_member(name, appt)

    def scrape_reps_comm(self):
        base = 'http://webserver.rilin.state.ri.us'

        linklist = self.scrape_comm_list('ComMemR')
        if linklist is not None:
            for a in linklist:
                link=a.attrib['href']
                commName=a.text
                url=base+link
                self.log("url "+url)
                c=Committee('lower',commName)
                self.add_members(c,url)
                c.add_source(url)
                self.save_committee(c)

    def scrape_senate_comm(self):
        base = 'http://webserver.rilin.state.ri.us'

        linklist = self.scrape_comm_list('ComMemS')
        if linklist is not None:
            for a in linklist:
                link=a.attrib['href']
                commName=a.text
                self.log( commName )
                if commName in COMM_BLACKLIST:
                    self.log( "XXX: Blacklisted" )
                    continue
                url=base+link
                self.log("url "+url)
                c=Committee('upper',commName)
                self.add_members(c,url)
                c.add_source(url)
                self.save_committee(c)

    def scrape_joint_comm(self):
        base = 'http://webserver.rilin.state.ri.us'

        linklist = self.scrape_comm_list('ComMemJ')
        if linklist is not None:
            for a in linklist:
                link=a.attrib['href']
                commName=a.text
                url=base+link
                self.log("url "+url)
                c=Committee('joint',commName)
                self.add_members(c,url)
                c.add_source(url)
                self.save_committee(c)


########NEW FILE########
__FILENAME__ = events
import datetime as dt

from billy.scrape import NoDataForPeriod
from billy.scrape.events import Event, EventScraper

import lxml.html
import pytz

agenda_url = "http://status.rilin.state.ri.us/agendas.aspx"
column_order = {
    "upper" : 1,
    "other" : 2,
    "lower" : 0
}

all_day = [  # ugh, hack
    "Rise of the House",
    "Rise of the Senate",
    "Rise of the House & Senate"
]
replace = {
    "House Joint Resolution No." : "HJR",
    "House Resolution No." : "HR",
    "House Bill No." : "HB",

    "Senate Joint Resolution No." : "SJR",
    "Senate Resolution No." : "SR",
    "Senate Bill No." : "SB",
    u"\xa0" : " ",
    "SUB A" : "",
    "SUB A as amended": ""
}

class RIEventScraper(EventScraper):
    jurisdiction = 'ri'

    _tz = pytz.timezone('US/Eastern')

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def scrape_agenda(self, url, session):
        page = self.lxmlize(url)
        # Get the date/time info:
        date_time = page.xpath("//table[@class='time_place']")
        if date_time == []:
            return

        date_time = date_time[0]
        lines = date_time.xpath("./tr")
        metainf = {}
        for line in lines:
            tds = line.xpath("./td")
            metainf[tds[0].text_content()] = tds[1].text_content()
        date = metainf['DATE:']
        time = metainf['TIME:']
        where = metainf['PLACE:']
        fmts = [
            "%A, %B %d, %Y",
            "%A, %B %d, %Y %I:%M %p",
            "%A, %B %d, %Y %I:%M",
        ]

        if time in all_day:
            datetime = date
        else:
            datetime = "%s %s" % ( date, time )
        if "CANCELLED" in datetime or "Rise of the House" in datetime:
            # XXX: Do something more advanced.
            return

        transtable = {
            "P.M" : "PM",
            "PM." : "PM",
            "P.M." : "PM",
            "A.M." : "AM",
            "POSTPONED" : "",
            "RESCHEDULED": "",
            "and Rise of the Senate": "",
        }
        for trans in transtable:
            datetime = datetime.replace(trans, transtable[trans])

        datetime = datetime.strip()

        for fmt in fmts:
            try:
                datetime = dt.datetime.strptime(datetime, fmt)
                break
            except ValueError:
                continue

        event = Event(session, datetime, 'committee:meeting',
                      'Meeting Notice', location=where)
        event.add_source(url)
        # aight. Let's get us some bills!
        bills = page.xpath("//b/a")
        for bill in bills:
            bill_ft = bill.attrib['href']
            event.add_document(bill.text_content(), bill_ft, type="full-text",
                               mimetype="application/pdf")
            root = bill.xpath('../../*')
            root = [ x.text_content() for x in root ]
            bill_id = "".join(root)

            if "SCHEDULED FOR" in bill_id:
                continue

            descr = bill.getparent().getparent().getparent().getnext().getnext(
                ).text_content()

            for thing in replace:
                bill_id = bill_id.replace(thing, replace[thing])

            event.add_related_bill(bill_id,
                                   description=descr,
                                   type='consideration')
        committee = page.xpath("//span[@id='lblSession']")[0].text_content()
        chambers = {
            "house" : "lower",
            "joint" : "joint",
            "senate" : "upper"
        }
        chamber = "other"
        for key in chambers:
            if key in committee.lower():
                chamber = chambers[key]

        event.add_participant("host", committee, 'committee', chamber=chamber)

        self.save_event(event)

    def scrape_agenda_dir(self, url, session):
        page = self.lxmlize(url)
        rows = page.xpath("//table[@class='agenda_table']/tr")[2:]
        for row in rows:
            url = row.xpath("./td")[-1].xpath(".//a")[0]
            self.scrape_agenda(url.attrib['href'], session)

    def scrape(self, chamber, session):
        offset = column_order[chamber]
        page = self.lxmlize(agenda_url)
        rows = page.xpath("//table[@class='agenda_table']/tr")[1:]
        for row in rows:
            ctty = row.xpath("./td")[offset]
            to_scrape = ctty.xpath("./a")
            for page in to_scrape:
                self.scrape_agenda_dir(page.attrib['href'], session)

########NEW FILE########
__FILENAME__ = legislators
import re
import datetime

from billy.scrape import NoDataForPeriod
from billy.scrape.legislators import LegislatorScraper, Legislator

import lxml.html
import xlrd

excel_mapping = {
    'district': 0,
    'town_represented': 2,
    'full_name': 3,
    'party': 4,
    'address': 5,
    'email': 6,
}

class RILegislatorScraper(LegislatorScraper):
    jurisdiction = 'ri'
    latest_only = True

    def scrape(self, chamber, term):
        if chamber == 'upper':
            url = ('http://webserver.rilin.state.ri.us/Documents/Senators.xls')
            rep_type = 'Senator '
            source_url = 'http://www.rilin.state.ri.us/senators/default.aspx'
            source_url_title_replacement = rep_type
        elif chamber == 'lower':
            url = ('http://webserver.rilin.state.ri.us/Documents/Representatives.xls')
            rep_type = 'Representative '
            source_url = 'http://www.rilin.state.ri.us/representatives/default.aspx'
            source_url_title_replacement = 'Rep. '

        self.urlretrieve(url, 'ri_leg.xls')

        wb = xlrd.open_workbook('ri_leg.xls')
        sh = wb.sheet_by_index(0)

        # This isn't perfect but it's cheap and better than using the
        # XLS doc as the source URL for all legislators.
        # 374: RI: legislator url
        leg_source_url_map = {}
        leg_page = lxml.html.fromstring(self.urlopen(source_url))
        leg_page.make_links_absolute(source_url)

        for link in leg_page.xpath('//td[@class="ms-vb2"]'):
            leg_name = link.text_content().replace(source_url_title_replacement,'')
            leg_url = link.xpath("..//a")[0].attrib['href']
            leg_source_url_map[leg_name] = leg_url

        for rownum in xrange(1, sh.nrows):
            d = {}
            for field, col_num in excel_mapping.iteritems():
                d[field] = sh.cell(rownum, col_num).value
            dist = str(int(d['district']))
            district_name = dist
            full_name = re.sub(rep_type, '', d['full_name']).strip()
            translate = {
                "Democrat"    : "Democratic",
                "Republican"  : "Republican",
                "Independent" : "Independent"
            }

            homepage_url = None
            if full_name in leg_source_url_map.keys():
                homepage_url = leg_source_url_map[full_name]

            kwargs = {
                "town_represented": d['town_represented'],
                "email": d['email']
            }

            if homepage_url is not None:
                kwargs['url'] = homepage_url

            leg = Legislator(term, chamber, district_name, full_name,
                             '', '', '',
                             translate[d['party']],
                             **kwargs)

            leg.add_office('district', 'Address', address=d['address'])
            leg.add_source(source_url)
            if homepage_url:
                leg.add_source(homepage_url)
            self.save_legislator(leg)


########NEW FILE########
__FILENAME__ = votes
from billy.scrape.votes import VoteScraper, Vote
from billy.scrape.utils import url_xpath

import datetime as dt

import urllib
import lxml
import re

RI_URL_BASE = "http://webserver.rilin.state.ri.us"

class RIVoteScraper(VoteScraper):
    jurisdiction = 'ri'

    def get_dates(self, page):
        dates = url_xpath( page, "//select[@name='votedate']" )[0].\
                xpath("./*")
        ret = [ a.text for a in dates ]
        return ret

    def get_votes(self, url, session):
        ret = {}
        html = self.urlopen(url)
        p = lxml.html.fromstring(html)
        tables = \
            p.xpath("//td[@background='/images/capBG.jpg']/div/table")

        metainf = tables[0]
        table   = tables[1]

        inf = metainf.xpath("./tr/td/pre")[0]
        headers = [ br.tail for br in inf.xpath("./*") ]

        dateinf = metainf.xpath("./tr/td")[3]
        date = dateinf.text
        time = dateinf.xpath("./*")[0].tail

        vote_digest = metainf.xpath("./tr/td[@colspan='3']")
        digest = vote_digest[2].text_content()
        dig = []
        for d in digest.split("\n"):
            lis = d.strip().split("-")
            for l in lis:
                if l != None and l != "":
                    dig.append(l.strip())
        digest = dig

        il = iter( digest )
        d = dict(zip(il, il))
        vote_count = d
        vote_count['passage'] = int(vote_count['YEAS']) > \
                int(vote_count['NAYS'])
        # XXX: This here has a greater then normal chance of failing.
        # However, it's an upstream issue.

        time_string = "%s %s" % ( time, date )

        fmt_string = "%I:%M:%S %p %A, %B %d, %Y"
        # 4:31:14 PM TUESDAY, JANUARY 17, 2012
        date_time = dt.datetime.strptime( time_string, fmt_string )

        bill_s_n_no = r"(?P<year>[0-9]{2,4})(-?)(?P<chamber>[SH])\s*(?P<bill>[0-9]+)"
        # This is technically wrong, but it's close enough to be fine.
        # something like "123S  3023" is technically valid, even though it's
        # silly

        bill_metainf = None
        remaining    = None

        for hid in range(0,len(headers)):
            h = headers[hid]
            inf = re.search( bill_s_n_no, h )
            if inf != None:
                bill_metainf = inf.groupdict()
                if bill_metainf['year'][-2:] != session[-2:]:
                    self.log(
"Skipping vote - it's in the %s session, we're in the %s session." % (
bill_metainf['year'][-2:],
session[-2:]
)
                    )
                    return ret
                remaining = headers[hid+1:]

        if bill_metainf == None:
            self.warning("No metainf for this bill. Aborting snag")
            return ret

        try:
            motion = remaining[-2]
        except IndexError:
            self.warning("Mission motion on this vote")
            motion = "Unknown" # XXX: Because the motion is not on some
            #                         pages.

        bill_metainf['extra'] = {
            "motion" : motion
        }

        votes = []

        for t in table.xpath("./tr/td"):
            nodes = t.xpath("./*")
            for node in nodes:
                if node.tag == "span":
                    vote = node.text.strip().upper()
                    name = node.tail.strip()
                    votes.append({
                        "name" : name,
                        "vote" : vote
                    })
            if len(votes) > 0:
                bid = bill_metainf['bill']
                ret[bid] = {
                    "votes" : votes,
                    "meta"  : bill_metainf,
                    "time"  : date_time,
                    "count" : vote_count,
                    "source": url
                }
        return ret

    def parse_vote_page(self, page, context_url, session):
        ret = []
        p = lxml.html.fromstring(page)
        votes = p.xpath( "//center/div[@class='vote']" )
        for vote in votes:
            votes = self.get_votes( context_url + "/" +
                            vote.xpath("./a")[0].attrib["href"], session )
            ret.append(votes)
        return ret

    def post_to(self, url, vote):
        headers = {
            "votedate" : vote
        }
        #headers = urllib.urlencode( headers )
        return self.urlopen( url, method="POST", body=headers)

    def scrape(self, chamber, session):
        url = {
            "upper" : "%s/%s" % ( RI_URL_BASE, "SVotes" ),
            "lower" : "%s/%s" % ( RI_URL_BASE, "HVotes" )
        }
        url = url[chamber]
        action = "%s/%s" % ( url, "votes.asp" )
        dates = self.get_dates( url )
        for date in dates:
            votes = self.parse_vote_page( self.post_to( action, date ), url,
                                         session )
            for vote_dict in votes:
                for vote in vote_dict:
                    vote = vote_dict[vote]
                    count = vote['count']
                    chamber = {
                        "H" : "lower",
                        "S" : "upper"
                    }[vote['meta']['chamber']]
                    v = Vote( chamber, vote['time'] ,
                             vote['meta']['extra']['motion'],
                             count['passage'], int(count['YEAS']),
                             int(count['NAYS']),
                             int(count['NOT VOTING']),
                             session=session,
                             bill_id=vote['meta']['bill'],
                             bill_chamber=chamber,
                             bill_session=vote['meta']['year'],
                    )
                    v.add_source( vote['source'] )
                    for vt in vote['votes']:
                        if vt['vote'] == "Y":
                            v.yes( vt['name'] )
                        elif vt['vote'] == "N":
                            v.no(  vt['name'] )
                        else:
                            v.other( vt['name'] )
                    self.save_vote(v)


########NEW FILE########
__FILENAME__ = bills
import datetime
import os
import re
from collections import defaultdict

from billy.scrape import ScrapeError
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote
from billy.scrape.utils import convert_pdf

import lxml.html


def action_type(action):
    # http://www.scstatehouse.gov/actionsearch.php is very useful for this
    classifiers = (('Adopted', 'bill:passed'),
                   ('Amended and adopted',
                    ['bill:passed', 'amendment:passed']),
                   ('Amended', 'amendment:passed'),
                   ('Certain items vetoed', 'governor:vetoed:line-item'),
                   ('Committed to', 'committee:referred'),
                   ('Committee Amendment Adopted', 'amendment:passed'),
                   ('Committee Amendment Amended and Adopted',
                    ['amendment:passed', 'amendment:amended']),
                   ('Committee Amendment Amended', 'amendment:amended'),
                   ('Committee Amendment Tabled', 'amendment:tabled'),
                   ('Committee report: Favorable',
                    'committee:passed:favorable'),
                   ('Committee report: Majority favorable',
                    'committee:passed'),
                   ('House amendment amended', 'amendment:amended'),
                   ('Introduced and adopted',
                    ['bill:introduced', 'bill:passed']),
                   ('Introduced, adopted',
                    ['bill:introduced', 'bill:passed']),
                   ('Introduced and read first time', ['bill:introduced', 'bill:reading:1']),
                   ('Introduced, read first time', ['bill:introduced', 'bill:reading:1']),
                   ('Introduced', 'bill:introduced'),
                   ('Prefiled', 'bill:filed'),
                   ('Read second time', 'bill:reading:2'),
                   ('Read third time', ['bill:passed', 'bill:reading:3']),
                   ('Recommitted to Committee', 'committee:referred'),
                   ('Referred to Committee', 'committee:referred'),
                   ('Rejected', 'bill:failed'),
                   ('Senate amendment amended', 'amendment:amended'),
                   ('Signed by governor', 'governor:signed'),
                   ('Tabled', 'bill:failed'),
                   ('Veto overridden', 'bill:veto_override:passed'),
                   ('Veto sustained', 'bill:veto_override:failed'),
                   ('Vetoed by Governor', 'governor:vetoed'),
                  )
    for prefix, atype in classifiers:
        if action.startswith(prefix):
            return atype
    # otherwise
    return 'other'


class SCBillScraper(BillScraper):
    jurisdiction = 'sc'
    urls = {
        'lower' : {
          'daily-bill-index': "http://www.scstatehouse.gov/hintro/hintros.php",
        },
        'upper' : {
          'daily-bill-index': "http://www.scstatehouse.gov/sintro/sintros.php",
        }
    }

    _subjects = defaultdict(set)

    def scrape_subjects(self, session_code):

        # only need to do it once
        if self._subjects:
            return

        subject_search_url = 'http://www.scstatehouse.gov/subjectsearch.php'
        data = self.urlopen(subject_search_url, 'POST',
                            dict((('GETINDEX','Y'), ('SESSION', session_code),
                                  ('INDEXCODE','0'), ('INDEXTEXT', ''),
                                  ('AORB', 'B'), ('PAGETYPE', '0'))))
        doc = lxml.html.fromstring(data)
        # skip first two subjects, filler options
        for option in doc.xpath('//option')[2:]:
            subject = option.text
            code = option.get('value')

            url = '%s?AORB=B&session=%s&indexcode=%s' % (subject_search_url,
                                                         session_code, code)
            data = self.urlopen(url)
            doc = lxml.html.fromstring(data)
            for bill in doc.xpath('//span[@style="font-weight:bold;"]'):
                match = re.match('(?:H|S) \d{4}', bill.text)
                if match:
                    # remove * and leading zeroes
                    bill_id = match.group().replace('*', ' ')
                    bill_id = re.sub(' 0*', ' ', bill_id)
                    self._subjects[bill_id].add(subject)


    def scrape_vote_history(self, bill, vurl):
        html = self.urlopen(vurl)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(vurl)

        # skip first two rows
        for row in doc.xpath('//table/tr')[2:]:
            tds = row.getchildren()
            if len(tds) != 11:
                self.warning('irregular vote row: %s' % vurl)
                continue
            timestamp, motion, vote, yeas, nays, nv, exc, pres, abst, total, result = tds

            timestamp = timestamp.text.replace(u'\xa0', ' ')
            timestamp = datetime.datetime.strptime(timestamp,
                                                   '%m/%d/%Y %H:%M %p')
            yeas = int(yeas.text)
            nays = int(nays.text)
            others = int(nv.text) + int(exc.text) + int(abst.text) + int(pres.text)
            assert yeas + nays + others == int(total.text)

            passed = (result.text == 'Passed')

            vote_link = vote.xpath('a')[0]
            if '[H]' in vote_link.text:
                chamber = 'lower'
            else:
                chamber = 'upper'

            vote = Vote(chamber, timestamp, motion.text, passed, yeas, nays,
                        others)
            vote.add_source(vurl)

            rollcall_pdf = vote_link.get('href')
            self.scrape_rollcall(vote, rollcall_pdf)
            vote.add_source(rollcall_pdf)

            bill.add_vote(vote)

    def scrape_rollcall(self, vote, vurl):
        (path, resp) = self.urlretrieve(vurl)
        pdflines = convert_pdf(path, 'text')
        os.remove(path)

        current_vfunc = None

        for line in pdflines.split('\n'):
            line = line.strip()

            # change what is being recorded
            if line.startswith('YEAS') or line.startswith('AYES'):
                current_vfunc = vote.yes
            elif line.startswith('NAYS'):
                current_vfunc = vote.no
            elif (line.startswith('EXCUSED') or
                  line.startswith('NOT VOTING') or
                  line.startswith('ABSTAIN')):
                current_vfunc = vote.other
            # skip these
            elif not line or line.startswith('Page '):
                continue

            # if a vfunc is active
            elif current_vfunc:
                # split names apart by 3 or more spaces
                names = re.split('\s{3,}', line)
                for name in names:
                    if name:
                        current_vfunc(name.strip())


    def scrape_details(self, bill_detail_url, session, chamber, bill_id):
        page = self.urlopen(bill_detail_url)

        if 'INVALID BILL NUMBER' in page:
            self.warning('INVALID BILL %s' % bill_detail_url)
            return

        doc = lxml.html.fromstring(page)
        doc.make_links_absolute(bill_detail_url)

        bill_div = doc.xpath('//div[@style="margin:0 0 40px 0;"]')[0]

        bill_type = bill_div.xpath('span/text()')[0]

        if 'General Bill' in bill_type:
            bill_type = 'bill'
        elif 'Concurrent Resolution' in bill_type:
            bill_type = 'concurrent resolution'
        elif 'Joint Resolution' in bill_type:
            bill_type = 'joint resolution'
        elif 'Resolution' in bill_type:
            bill_type = 'resolution'
        else:
            raise ValueError('unknown bill type: %s' % bill_type)

        # this is fragile, but less fragile than it was
        b = bill_div.xpath('./b[text()="Summary:"]')[0]
        bill_summary = b.getnext().tail.strip()

        bill = Bill(session, chamber, bill_id, bill_summary, type=bill_type)
        bill['subjects'] = list(self._subjects[bill_id])

        # sponsors
        for sponsor in doc.xpath('//a[contains(@href, "member.php")]/text()'):
            bill.add_sponsor('primary', sponsor)
        for sponsor in doc.xpath('//a[contains(@href, "committee.php")]/text()'):
            sponsor = sponsor.replace(u'\xa0', ' ').strip()
            bill.add_sponsor('primary', sponsor)

        # find versions
        version_url = doc.xpath('//a[text()="View full text"]/@href')[0]
        version_html = self.urlopen(version_url)
        version_doc = lxml.html.fromstring(version_html)
        version_doc.make_links_absolute(version_url)
        for version in version_doc.xpath('//a[contains(@href, "/prever/")]'):
            # duplicate versions with same date, use first appearance
            bill.add_version(version.text, version.get('href'),
                             on_duplicate='use_old',
                             mimetype='text/html')

        # actions
        for row in bill_div.xpath('table/tr'):
            date_td, chamber_td, action_td = row.xpath('td')

            date = datetime.datetime.strptime(date_td.text, "%m/%d/%y")
            action_chamber = {'Senate':'upper',
                              'House':'lower',
                              None: 'other'}[chamber_td.text]

            action = action_td.text_content()
            action = action.split('(House Journal')[0]
            action = action.split('(Senate Journal')[0].strip()

            atype = action_type(action)
            bill.add_action(action_chamber, action, date, atype)


        # votes
        vurl = doc.xpath('//a[text()="View Vote History"]/@href')
        if vurl:
            vurl = vurl[0]
            self.scrape_vote_history(bill, vurl)

        bill.add_source(bill_detail_url)
        self.save_bill(bill)


    def scrape(self, chamber, session):
        # start with subjects
        session_code = self.metadata['session_details'][session]['_code']
        self.scrape_subjects(session_code)

        # get bill index
        index_url = self.urls[chamber]['daily-bill-index']
        chamber_letter = 'S' if chamber == 'upper' else 'H'

        page = self.urlopen(index_url)
        doc = lxml.html.fromstring(page)
        doc.make_links_absolute(index_url)

        # visit each day and extract bill ids
        days = doc.xpath('//div/b/a/@href')
        for day_url in days:
            data = self.urlopen(day_url)
            doc = lxml.html.fromstring(data)
            doc.make_links_absolute(day_url)

            for bill_a in doc.xpath('//p/a[1]'):
                bill_id = bill_a.text.replace('.', '')
                if bill_id.startswith(chamber_letter):
                    self.scrape_details(bill_a.get('href'), session, chamber,
                                        bill_id)

########NEW FILE########
__FILENAME__ = events
import re
import pytz
import datetime
import lxml.html

from billy.scrape.events import EventScraper, Event

class SCEventScraper(EventScraper):
    jurisdiction = 'sc'
    _tz = pytz.timezone('US/Eastern')

    def get_page_from_url(self,url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def normalize_time(self, time_string):
        time_string = time_string.lower().strip()
        if re.search(r'adjourn',time_string):
            time_string = '12:00 am'
        if re.search(r' noon', time_string):
            time_string = time_string.replace(' noon', ' pm')
        # remove extra spaces
        if re.search('[^ ]+ ?- ?[0-9]', time_string):
            start, end = re.search(r'([^ ]+) ?- ?([0-9])',
                                   time_string).groups()
            time_string = re.sub(start + ' ?- ?' + end,
                                 start + '-' + end, time_string)
        # if it's a block of time, use the start time
        block_reg = re.compile(
            r'^([0-9]{1,2}:[0-9]{2}( [ap]m)?)-[0-9]{1,2}:[0-9]{2} ([ap]m)')

        if re.search(block_reg,time_string):
            start_time, start_meridiem, end_meridiem = re.search(
                block_reg,time_string).groups()

            start_hour = int(start_time.split(':')[0])
            if start_meridiem:
                time_string = re.search(
                    '^([0-9]{1,2}:[0-9]{2} [ap]m)', time_string).group(1)
            else:
                if end_meridiem == 'pm' and start_hour < 12:
                    time_string = start_time + ' am'
                else:
                    time_string = start_time + ' ' + end_meridiem
        return time_string

    def get_bill_description(self, url):
        bill_page = self.get_page_from_url(url)
        bill_text = bill_page.xpath(
            './/div[@id="resultsbox"]/div[2]')[0]
        bill_description = bill_text.text_content().encode(
            'utf-8').split('\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0')[0]

        bill_description = re.search(
            r'Summary: (.*)', bill_description).group(1).strip()
        return bill_description

    def scrape(self, chamber, session):
        if chamber == 'other':
            return

        events_url = 'http://www.scstatehouse.gov/meetings.php?chamber=%s' % (
            self.metadata['chambers'][chamber]['name'].upper()[0]
        )
        page = self.get_page_from_url(events_url)

        meeting_year = page.xpath(
            '//h2[@class="barheader"]/span')[0].text_content()
        meeting_year = re.search(
            r'Week of [A-Z][a-z]+\s+[0-9]{1,2}, ([0-9]{4})',
            meeting_year).group(1)

        dates = page.xpath("//div[@id='contentsection']/ul")

        for date in dates:
            date_string = date.xpath('span')

            if len(date_string) == 1:
                date_string = date_string[0].text_content()
            else:
                continue

            for meeting in date.xpath('li'):
                time_string = meeting.xpath('span')[0].text_content()

                if time_string == 'CANCELED' or len(
                        meeting.xpath(
                            './/span[contains(text(), "CANCELED")]')) > 0:
                    continue

                time_string = self.normalize_time(time_string)
                date_time = datetime.datetime.strptime(
                    meeting_year + ' ' + date_string
                    + ' ' + time_string, "%Y %A, %B %d %I:%M %p")

                date_time = self._tz.localize(date_time)
                meeting_info = meeting.xpath(
                    'br[1]/preceding-sibling::node()')[1]
                location, description = re.search(
                    r'-- (.*?) -- (.*)', meeting_info).groups()

                if re.search(r'committee', description, re.I):
                    meeting_type = 'committee:meeting'
                else:
                    meeting_type = 'other:meeting'

                event = Event(
                    session,
                    date_time,
                    meeting_type,
                    description,
                    location
                )
                event.add_source(events_url)

                agenda_url = meeting.xpath(".//a[contains(@href,'agendas')]")

                if agenda_url:
                    agenda_url = agenda_url[0].attrib['href']
                    event.add_source(agenda_url)
                    agenda_page = self.get_page_from_url(agenda_url)

                    for bill in agenda_page.xpath(
                            ".//a[contains(@href,'billsearch.php')]"):
                        bill_url = bill.attrib['href']
                        bill_id = bill.text_content().replace(
                            '.','').replace(' ','')
                        bill_description = self.get_bill_description(bill_url)

                        event.add_related_bill(
                            bill_id=bill_id,
                            type='consideration',
                            description=bill_description
                        )
                self.save_event(event)

########NEW FILE########
__FILENAME__ = legislators
import lxml.html

from billy.scrape.legislators import LegislatorScraper, Legislator


class SCLegislatorScraper(LegislatorScraper):
    jurisdiction = 'sc'

    def scrape(self, chamber, term):
        # CSS isn't there without this, it serves up a mobile version
        self.user_agent = 'Mozilla/5.0'

        if chamber == 'lower':
            url = 'http://www.scstatehouse.gov/member.php?chamber=H'
        else:
            url = 'http://www.scstatehouse.gov/member.php?chamber=S'

        data = self.urlopen(url)
        doc = lxml.html.fromstring(data)
        doc.make_links_absolute(url)

        for a in doc.xpath('//a[contains(@href, "code=")]'):
            full_name = a.text
            leg_url = a.get('href')

            leg_html = self.urlopen(leg_url)
            leg_doc = lxml.html.fromstring(leg_html)
            leg_doc.make_links_absolute(leg_url)

            if 'Resigned effective' in leg_html:
                self.info('Resigned')
                continue

            party, district, _ = leg_doc.xpath('//p[@style="font-size: 17px; margin: 0 0 0 0; padding: 0;"]/text()')
            if 'Republican' in party:
                party = 'Republican'
            elif 'Democrat' in party:
                party = 'Democratic'

            # District # - County - Map
            district = district.split()[1]

            photo_url = leg_doc.xpath('//img[contains(@src,"/members/")]/@src')[0]


            legislator = Legislator(term, chamber, district, full_name,
                                    party=party, photo_url=photo_url,
                                    url=leg_url)
            # office address / phone
            try:
                addr_div = leg_doc.xpath('//div[@style="float: left; width: 225px; margin: 10px 5px 0 20px; padding: 0;"]')[0]
                addr = addr_div.xpath('p[@style="font-size: 13px; margin: 0 0 10px 0; padding: 0;"]')[0].text_content()

                phone = addr_div.xpath('p[@style="font-size: 13px; margin: 0 0 0 0; padding: 0;"]/text()')[0]
                phone = phone.strip()
                legislator.add_office('capitol', 'Columbia Address',
                                      address=addr, phone=phone)
            except IndexError:
                self.warning('no address for {0}'.format(full_name))

            legislator.add_source(leg_url)
            legislator.add_source(url)


            # committees (skip first link)
            for com in leg_doc.xpath('//a[contains(@href, "committee.php")]')[1:]:
                if com.text.endswith(', '):
                    committee, role = com.text_content().rsplit(', ',1)
                    # known roles
                    role = {'Treas.': 'treasurer',
                            'Secy.': 'secretary',
                            'Secy./Treas.': 'secretary/treasurer',
                            'V.C.': 'vice-chair',
                            '1st V.C.': 'first vice-chair',
                            '2nd V.C.': 'second vice-chair',
                            '3rd V.C.': 'third vice-chair',
                            'Ex.Officio Member': 'ex-officio member',
                            'Chairman': 'chairman'}[role]
                else:
                    committee = com.text
                    role = 'member'
                legislator.add_role('committee member', term=term,
                                    chamber=chamber, committee=committee,
                                    position=role)

            self.save_legislator(legislator)

########NEW FILE########
__FILENAME__ = bills
import re
import datetime

from billy.scrape import ScrapeError
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

import lxml.html


class SDBillScraper(BillScraper):
    jurisdiction = 'sd'

    def scrape(self, chamber, session):
        url = 'http://legis.sd.gov/Legislative_Session/Bills/default.aspx?Session=%s' % session

        if chamber == 'upper':
            bill_abbr = 'S'
        else:
            bill_abbr = 'H'

        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        for link in page.xpath("//a[contains(@href, 'Bill.aspx') and"
                               " starts-with(., '%s')]" % bill_abbr):
            bill_id = link.text.strip().replace(u'\xa0', ' ')

            title = link.xpath("string(../../td[2])").strip()

            self.scrape_bill(chamber, session, bill_id, title,
                             link.attrib['href'])

    def scrape_bill(self, chamber, session, bill_id, title, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        if re.match(r'^(S|H)B ', bill_id):
            btype = ['bill']
        elif re.match(r'(S|H)C ', bill_id):
            btype = ['commemoration']
        elif re.match(r'(S|H)JR ', bill_id):
            btype = ['joint resolution']
        elif re.match(r'(S|H)CR ', bill_id):
            btype = ['concurrent resolution']
        else:
            btype = ['bill']

        bill = Bill(session, chamber, bill_id, title, type=btype)
        bill.add_source(url)

        regex_ns = "http://exslt.org/regular-expressions"
        version_links = page.xpath(
            "//a[re:test(@href, 'Bill.aspx\?File=.*\.htm', 'i')]",
            namespaces={'re': regex_ns})
        for link in version_links:
            bill.add_version(link.xpath('string()').strip(),
                             link.attrib['href'],
                             mimetype='text/html')

        sponsor_links = page.xpath(
            "//td[contains(@id, 'tdSponsors')]/a")
        for link in sponsor_links:
            bill.add_sponsor("primary", link.text)

        actor = chamber
        use_row = False
        self.debug(bill_id)
        for row in page.xpath(
            "//table[contains(@id, 'BillActions')]/tr"):

            if 'Date' in row.text_content() and 'Action' in row.text_content():
                use_row = True
                continue
            elif not use_row:
                continue

            action = row.xpath("string(td[2])").strip()

            atypes = []
            if action.startswith('First read'):
                atypes.append('bill:introduced')
                atypes.append('bill:reading:1')
            elif action.startswith('Signed by Governor'):
                atypes.append('governor:signed')
                actor = 'executive'

            match = re.match(r'(.*) Do Pass( Amended)?, (Passed|Failed)',
                             action)
            if match:
                if match.group(1) in ['Senate',
                                      'House of Representatives']:
                    first = 'bill'
                else:
                    first = 'committee'
                atypes.append("%s:%s" % (first, match.group(3).lower()))

            if 'referred to' in action.lower():
                atypes.append('committee:referred')

            if 'Motion to amend, Passed Amendment' in action:
                atypes.append('amendment:introduced')
                atypes.append('amendment:passed')

            if 'Veto override, Passed' in action:
                atypes.append('bill:veto_override:passed')
            elif 'Veto override, Failed' in action:
                atypes.append('bill:veto_override:failed')

            if 'Delivered to the Governor' in action:
                atypes.append('governor:received')

            match = re.match("First read in (Senate|House)", action)
            if match:
                if match.group(1) == 'Senate':
                    actor = 'upper'
                else:
                    actor = 'lower'

            date = row.xpath("string(td[1])").strip()
            match = re.match('\d{2}/\d{2}/\d{4}', date)
            if not match:
                self.warning("Bad date: %s" % date)
                continue
            date = datetime.datetime.strptime(date, "%m/%d/%Y").date()

            for link in row.xpath("td[2]/a[contains(@href, 'RollCall')]"):
                self.scrape_vote(bill, date, link.attrib['href'])

            bill.add_action(actor, action, date, type=atypes)

        subjects = []
        for link in page.xpath("//a[contains(@href, 'Keyword')]"):
            subjects.append(link.text.strip())
        bill['subjects'] = subjects

        self.save_bill(bill)

    def scrape_vote(self, bill, date, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)

        header = page.xpath("string(//h4[contains(@id, 'hdVote')])")

        if 'No Bill Action' in header:
            self.warning("bad vote header -- skipping")
            return
        location = header.split(', ')[1]

        if location.startswith('House'):
            chamber = 'lower'
        elif location.startswith('Senate'):
            chamber = 'upper'
        else:
            raise ScrapeError("Bad chamber: %s" % chamber)

        committee = ' '.join(location.split(' ')[1:]).strip()
        if not committee or committee.startswith('of Representatives'):
            committee = None

        motion = ', '.join(header.split(', ')[2:]).strip()
        if not motion:
            # If we can't detect a motion, skip this vote
            return

        yes_count = int(
            page.xpath("string(//td[contains(@id, 'tdAyes')])"))
        no_count = int(
            page.xpath("string(//td[contains(@id, 'tdNays')])"))
        excused_count = int(
            page.xpath("string(//td[contains(@id, 'tdExcused')])"))
        absent_count = int(
            page.xpath("string(//td[contains(@id, 'tdAbsent')])"))
        other_count = excused_count + absent_count

        passed = yes_count > no_count

        if motion.startswith('Do Pass'):
            type = 'passage'
        elif motion == 'Concurred in amendments':
            type = 'amendment'
        elif motion == 'Veto override':
            type = 'veto_override'
        else:
            type = 'other'

        vote = Vote(chamber, date, motion, passed, yes_count, no_count,
                    other_count)
        vote['type'] = type

        if committee:
            vote['committee'] = committee

        vote.add_source(url)

        for td in page.xpath("//table[contains(@id, 'tblVotes')]/tr/td"):
            if td.text in ('Aye', 'Yea'):
                vote.yes(td.getprevious().text.strip())
            elif td.text == 'Nay':
                vote.no(td.getprevious().text.strip())
            elif td.text in ('Excused', 'Absent'):
                vote.other(td.getprevious().text.strip())

        bill.add_vote(vote)

########NEW FILE########
__FILENAME__ = legislators
from billy.scrape import NoDataForPeriod
from billy.scrape.legislators import LegislatorScraper, Legislator

import lxml.html


class SDLegislatorScraper(LegislatorScraper):
    jurisdiction = 'sd'
    latest_only = True

    def scrape(self, chamber, term):
        year = term[0:4]
        url = 'http://legis.sd.gov/Legislators/default.aspx?CurrentSession=True'

        if chamber == 'upper':
            search = 'Senate Members'
        else:
            search = 'House Members'

        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        for link in page.xpath("//h4[text()='%s']/../div/a" % search):
            name = link.text.strip()

            self.scrape_legislator(name, chamber, term,
                                   link.attrib['href'])

    def scrape_legislator(self, name, chamber, term, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        party = page.xpath("string(//span[contains(@id, 'Party')])")
        party = party.strip()

        if party == 'Democrat':
            party = 'Democratic'

        district = page.xpath("string(//span[contains(@id, 'District')])")
        district = district.strip().lstrip('0')

        occupation = page.xpath(
            "string(//span[contains(@id, 'Occupation')])")
        occupation = occupation.strip()

        photo_url = page.xpath(
            "//img[contains(@id, 'imgMember')]")[0].get('src') or ''

        office_phone = page.xpath(
            "string(//span[contains(@id, 'CapitolPhone')])").strip()

        legislator = Legislator(term, chamber, district, name,
                                party=party,
                                occupation=occupation,
                                photo_url=photo_url,
                                url=url)
        kwargs = {}
        if office_phone.strip() != "":
            kwargs['phone'] = office_phone

        legislator.add_office('capitol', 'Capitol Office',
                              **kwargs)
        legislator.add_source(url)

        comm_url = page.xpath("//a[. = 'Committees']")[0].attrib['href']
        self.scrape_committees(legislator, comm_url)

        self.save_legislator(legislator)

    def scrape_committees(self, leg, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        leg.add_source(url)

        term = leg['roles'][0]['term']

        for link in page.xpath("//a[contains(@href, 'CommitteeMem')]"):
            comm = link.text.strip()

            role = link.xpath('../following-sibling::td')[0].text_content().lower()

            if comm.startswith('Joint'):
                chamber = 'joint'
            else:
                chamber = leg['roles'][0]['chamber']

            leg.add_role('committee member', term=term, chamber=chamber,
                         committee=comm, position=role)

########NEW FILE########
__FILENAME__ = bills
import datetime
import lxml.html
import re
from collections import namedtuple

from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote


class Rule(namedtuple('Rule', 'regex types stop attrs')):
    '''If ``regex`` matches the action text, the resulting action's
    types should include ``types``.

    If stop is true, no other rules should be tested after this one;
    in other words, this rule conclusively determines the action's
    types and attrs.

    The resulting action should contain ``attrs``, which basically
    enables overwriting certain attributes, like the chamber if
    the action was listed in the wrong column.
    '''
    def __new__(_cls, regex, types=None, stop=True, **kwargs):
        'Create new instance of Rule(regex, types, attrs, stop)'

        # Types can be a string or a sequence.
        if isinstance(types, basestring):
            types = set([types])
        types = set(types or [])

        # If no types are associated, assume that the categorizer
        # should continue looking at other rules.
        if not types:
            stop = False
        return tuple.__new__(_cls, (regex, types, stop, kwargs))


# These are regex patterns that map to action categories.
_categorizer_rules = (

    # Some actions are listed in the wrong chamber column.
    # Fix the chamber before moving on to the other rules.
    Rule(r'^H\.\s', stop=False, actor='lower'),
    Rule(r'^S\.\s', stop=False, actor='upper'),
    Rule(r'Signed by S\. Speaker', actor='upper'),
    Rule(r'Signed by H\. Speaker', actor='lower'),

    # Extract the vote counts to help disambiguate chambers later.
    Rule(r'Ayes\s*(?P<yes_votes>\d+),\s*Nays\s*(?P<no_votes>\d+)', stop=False),

    # Committees
    Rule(r'(?i)ref\. to (?P<committees>.+?Comm\.)', 'committee:referred'),
    Rule(r'^Failed In S\.(?P<committees>.+?Comm\.)', 'committee:failed'),
    Rule(r'^Failed In s/c (?P<committees>.+)', 'committee:failed'),
    Rule(r'Rcvd\. from H., ref\. to S\. (?P<committees>.+)',
         'committee:referred', actor='upper'),
    Rule(r'Placed on cal\. (?P<committees>.+?) for', stop=False),
    Rule(r'Taken off notice for cal in s/c (?P<committees>.+)'),
    Rule(r'to be heard in (?P<committees>.+?Comm\.)'),
    Rule(r'Action Def. in S. (?P<committees>.+?Comm.)',
         actor='upper'),
    Rule(r'(?i)Placed on S. (?P<committees>.+?Comm\.) cal. for',
         actor='upper'),
    Rule(r'(?i)Assigned to (?P<committees>.+?comm\.)'),
    Rule(r'(?i)Placed on S. (?P<committees>.+?Comm.) cal.',
         actor='upper'),
    Rule(r'(?i)Taken off Notice For cal\. in s/c.+?\sof\s(?P<committees>.+?)'),
    Rule(r'(?i)Taken off Notice For cal\. in s/c.+?\sof\s(?P<committees>.+?)'),
    Rule(r'(?i)Taken off Notice For cal\. in[: ]+(?!s/c)(?P<committees>.+)'),
    Rule(r'(?i)Re-referred To:\s+(?P<committees>.+)', 'committee:referred'),
    Rule(r'Recalled from S. (?P<committees>.+?Comm.)'),

    # Amendments
    Rule(r'^Am\..+?tabled', 'amendment:tabled'),
    Rule('^Am\. withdrawn\.\(Amendment \d+ \- (?P<version>\S+)',
         'amendment:withdrawn'),
    Rule(r'^Am\. reconsidered(, withdrawn)?\.\(Amendment \d \- (?P<version>.+?\))',
         'amendment:withdrawn'),
    Rule(r'adopted am\.\(Amendment \d+ of \d+ - (?P<version>\S+)\)',
         'amendment:passed'),
    Rule(r'refused to concur.+?in.+?am', 'amendment:failed'),

    # Bill passage
    Rule(r'^Passed H\.', 'bill:passed', actor='lower'),
    Rule(r'^Passed S\.', 'bill:passed', actor='upper'),
    Rule(r'^R/S Adopted', 'bill:passed'),
    Rule(r'R/S Intro., adopted', 'bill:passed'),
    Rule(r'R/S Concurred', 'bill:passed'),

    # Veto
    Rule(r'(?i)veto', 'governor:vetoed'),

    # The existing rules for TN categorization:
    Rule('Amendment adopted', 'amendment:passed'),
    Rule('Amendment failed', 'amendment:failed'),
    Rule('Amendment proposed', 'amendment:introduced'),
    Rule('adopted am.', 'amendment:passed'),
    Rule('Am. withdrawn', 'amendment:withdrawn'),
    Rule('Divided committee report', 'committee:passed'),
    Rule('Filed for intro.', ['bill:introduced', 'bill:reading:1']),
    Rule('Reported back amended, do not pass', 'committee:passed:unfavorable'),
    Rule('Reported back amended, do pass', 'committee:passed:favorable'),
    Rule('Rec. For Pass.', 'committee:passed:favorable'),
    Rule('Rec. For pass.', 'committee:passed:favorable'),
    Rule('Reported back amended, without recommendation', 'committee:passed'),
    Rule('Reported back, do not pass', 'committee:passed:unfavorable'),
    Rule('w/ recommend', 'committee:passed:favorable'),
    Rule('Ref. to', 'committee:referred'),
    Rule('ref. to', 'committee:referred'),
    Rule('Assigned to', 'committee:referred'),
    Rule('Received from House', 'bill:introduced'),
    Rule('Received from Senate', 'bill:introduced'),
    Rule('Adopted, ', ['bill:passed']),
    Rule('Concurred, ', ['bill:passed']),
    Rule('Passed H., ', ['bill:passed']),
    Rule('Passed S., ', ['bill:passed']),
    Rule('Second reading, adopted', ['bill:passed', 'bill:reading:2']),
    Rule('Second reading, failed', ['bill:failed', 'bill:reading:2']),
    Rule('Second reading, passed', ['bill:passed', 'bill:reading:2']),
    Rule('Transmitted to Gov. for action.', 'governor:received'),
    Rule('Transmitted to Governor for his action.', 'governor:received'),
    Rule('Signed by Governor, but item veto', 'governor:vetoed:line-item'),
    Rule('Signed by Governor', 'governor:signed'),
    Rule('Withdrawn', 'bill:withdrawn'),
    Rule('tabled', 'amendment:tabled'),
    Rule('widthrawn', 'amendment:withdrawn'),
    Rule(r'Intro', 'bill:introduced'),
)


def categorize_action(action):
    types = set()
    attrs = {}

    for rule in _categorizer_rules:

        # Try to match the regex.
        m = re.search(rule.regex, action)
        if m or (rule.regex in action):
            # If so, apply its associated types to this action.
            types |= rule.types

            # Also add its specified attrs.
            attrs.update(m.groupdict())
            attrs.update(rule.attrs)

            # Break if the rule says so, otherwise continue testing against
            # other rules.
            if rule.stop is True:
                break

    # Returns types, attrs
    return list(types), attrs


def actions_from_table(bill, actions_table):
    '''
    '''
    action_rows = actions_table.xpath("tr")

    # first row will say "Actions Taken on S|H(B|R|CR)..."
    if 'Actions Taken on S' in action_rows[0].text_content():
        chamber = 'upper'
    else:
        chamber = 'lower'

    for ar in action_rows[1:]:
        tds = ar.xpath('td')
        action_taken = tds[0].text
        strptime = datetime.datetime.strptime
        action_date = strptime(tds[1].text.strip(), '%m/%d/%Y')
        action_types, attrs = categorize_action(action_taken)

        # Overwrite any presumtive fields that are inaccurate, usually chamber.
        action = dict(action=action_taken, date=action_date,
                      type=action_types, actor=chamber)
        action.update(**attrs)

        # Finally, if a vote tally is given, switch the chamber.
        if set(['yes_votes', 'no_votes']) & set(attrs):
            total_votes = int(attrs['yes_votes']) + int(attrs['no_votes'])
            if total_votes > 35:
                action['actor'] = 'lower'
            if total_votes <= 35:
                action['actor'] = 'upper'

        bill.add_action(**action)


class TNBillScraper(BillScraper):
    jurisdiction = 'tn'

    def scrape(self, chamber, term):

        if chamber == 'lower':
            self.warning("skipping bills for lower")
            return

        #types of bills
        abbrs = ['HB', 'HJR', 'HR', 'SB', 'SJR', 'SR']

        for abbr in abbrs:

            if 'B' in abbr:
                bill_type = 'bill'
            elif 'JR' in abbr:
                bill_type = 'joint resolution'
            else:
                bill_type = 'resolution'

            #Checks if current term
            if term == self.metadata["terms"][-1]["sessions"][0]:
                bill_listing = 'http://wapp.capitol.tn.gov/apps/indexes/BillIndex.aspx?StartNum=%s0001&EndNum=%s9999' % (abbr, abbr)
            else:
                bill_listing = 'http://wapp.capitol.tn.gov/apps/archives/BillIndex.aspx?StartNum=%s0001&EndNum=%s9999&Year=%s' % (abbr, abbr, term)

            bill_list_page = self.urlopen(bill_listing)
            bill_list_page = lxml.html.fromstring(bill_list_page)
            for bill_links in bill_list_page.xpath('////div[@id="open"]//a'):
                bill_link = bill_links.attrib['href']
                if '..' in bill_link:
                    bill_link = 'http://wapp.capitol.tn.gov/apps' + bill_link[2:len(bill_link)]
                self.scrape_bill(term, bill_link, bill_type)

    def scrape_bill(self, term, bill_url, bill_type):

        page = self.urlopen(bill_url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(bill_url)

        bill_id = page.xpath('//span[@id="lblBillSponsor"]/a[1]')[0].text
        secondary_bill_id = page.xpath('//span[@id="lblCoBillSponsor"]/a[1]')

        # checking if there is a matching bill
        if secondary_bill_id:
            secondary_bill_id = secondary_bill_id[0].text

            # swap ids if * is in secondary_bill_id
            if '*' in secondary_bill_id:
                bill_id, secondary_bill_id = secondary_bill_id, bill_id
                secondary_bill_id = secondary_bill_id.strip()

        bill_id = bill_id.replace('*', '').strip()

        primary_chamber = 'lower' if 'H' in bill_id else 'upper'
        # secondary_chamber = 'upper' if primary_chamber == 'lower' else 'lower'

        title = page.xpath("//span[@id='lblAbstract']")[0].text
        if title is None:
            msg = '%s detail page was missing title info.'
            self.logger.warning(msg % bill_id)
            return

        # bill subject
        subject_pos = title.find('-')
        subjects = [s.strip() for s in title[:subject_pos - 1].split(',')]
        subjects = filter(None, subjects)

        bill = Bill(term, primary_chamber, bill_id, title, type=bill_type,
                    subjects=subjects)
        if secondary_bill_id:
            bill['alternate_bill_ids'] = [secondary_bill_id]
        bill.add_source(bill_url)

        # Primary Sponsor
        sponsor = page.xpath("//span[@id='lblBillSponsor']")[0].text_content().split("by")[-1]
        sponsor = sponsor.replace('*', '').strip()
        if sponsor:
            bill.add_sponsor('primary', sponsor)

        # bill text
        btext = page.xpath("//span[@id='lblBillSponsor']/a")[0]
        bill.add_version('Current Version', btext.get('href'),
                         mimetype='application/pdf')

        # documents
        summary = page.xpath('//a[contains(@href, "BillSummaryArchive")]')
        if summary:
            bill.add_document('Summary', summary[0].get('href'))
        fiscal = page.xpath('//span[@id="lblFiscalNote"]//a')
        if fiscal:
            bill.add_document('Fiscal Note', fiscal[0].get('href'))
        amendments = page.xpath('//a[contains(@href, "/Amend/")]')
        for amendment in amendments:
            bill.add_document('Amendment ' + amendment.text,
                              amendment.get('href'))
        # amendment notes in image with alt text describing doc inside <a>
        amend_fns = page.xpath('//img[contains(@alt, "Fiscal Memo")]')
        for afn in amend_fns:
            bill.add_document(afn.get('alt'), afn.getparent().get('href'))

        # actions
        atable = page.xpath("//table[@id='tabHistoryAmendments_tabHistory_gvBillActionHistory']")[0]
        actions_from_table(bill, atable)

        # if there is a matching bill
        if secondary_bill_id:
            # secondary sponsor
            secondary_sponsor = page.xpath("//span[@id='lblCoBillSponsor']")[0].text_content().split("by")[-1]
            secondary_sponsor = secondary_sponsor.replace('*', '').replace(')', '').strip()
            # Skip black-name sponsors.
            if secondary_sponsor:
                bill.add_sponsor('primary', secondary_sponsor)

            # secondary actions
            cotable = page.xpath("//table[@id='tabHistoryAmendments_tabHistory_gvCoActionHistory']")[0]
            actions_from_table(bill, cotable)

        # votes
        votes_link = page.xpath("//span[@id='lblBillVotes']/a/@href")
        if len(votes_link) > 0:
            bill = self.scrape_votes(bill, votes_link[0])
        votes_link = page.xpath("//span[@id='lblCompVotes']/a/@href")
        if len(votes_link) > 0:
            bill = self.scrape_votes(bill, votes_link[0])

        bill['actions'].sort(key=lambda a: a['date'])
        self.save_bill(bill)

    def scrape_votes(self, bill, link):
        page = self.urlopen(link)
        page = lxml.html.fromstring(page)
        raw_vote_data = page.xpath("//span[@id='lblVoteData']")[0].text_content()
        raw_vote_data = re.split('\w+? by [\w ]+?\s+-', raw_vote_data.strip())[1:]
        for raw_vote in raw_vote_data:
            raw_vote = raw_vote.split(u'\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0')
            motion = raw_vote[0]

            vote_date = re.search('(\d+/\d+/\d+)', motion)
            if vote_date:
                vote_date = datetime.datetime.strptime(vote_date.group(), '%m/%d/%Y')

            passed = ('Passed' in motion or
                      'Recommended for passage' in motion or
                      'Adopted' in raw_vote[1]
                     )
            vote_regex = re.compile('\d+$')
            aye_regex = re.compile('^.+voting aye were: (.+) -')
            no_regex = re.compile('^.+voting no were: (.+) -')
            other_regex = re.compile('^.+present and not voting were: (.+) -')
            yes_count = 0
            no_count = 0
            other_count = 0
            ayes = []
            nos = []
            others = []

            for v in raw_vote[1:]:
                v = v.strip()
                if v.startswith('Ayes...') and vote_regex.search(v):
                    yes_count = int(vote_regex.search(v).group())
                elif v.startswith('Noes...') and vote_regex.search(v):
                    no_count = int(vote_regex.search(v).group())
                elif v.startswith('Present and not voting...') and vote_regex.search(v):
                    other_count += int(vote_regex.search(v).group())
                elif aye_regex.search(v):
                    ayes = aye_regex.search(v).groups()[0].split(', ')
                elif no_regex.search(v):
                    nos = no_regex.search(v).groups()[0].split(', ')
                elif other_regex.search(v):
                    others += other_regex.search(v).groups()[0].split(', ')

            if 'ChamberVoting=H' in link:
                chamber = 'lower'
            else:
                chamber = 'upper'

            vote = Vote(chamber, vote_date, motion, passed, yes_count,
                        no_count, other_count)
            vote.add_source(link)

            seen = set()
            for a in ayes:
                if a in seen:
                    continue
                vote.yes(a)
                seen.add(a)
            for n in nos:
                if n in seen:
                    continue
                vote.no(n)
                seen.add(n)
            for o in others:
                if o in seen:
                    continue
                vote.other(o)
                seen.add(o)

            # vote.validate()
            bill.add_vote(vote)

        return bill

########NEW FILE########
__FILENAME__ = committees
"""
Archived Committee notes:

Senate committees only avail from 105th forward

Urls are inconsistent
'http://www.capitol.tn.gov/senate/archives/105GA/Committees/scommemb.htm'
'http://www.capitol.tn.gov/senate/archives/106GA/Committees/index.html'

'http://www.capitol.tn.gov/house/archives/99GA/Committees/hcommemb.htm'
'http://www.capitol.tn.gov/house/archives/100GA/hcommemb.htm'
'http://www.capitol.tn.gov/house/archives/101GA/hcommemb.htm'
'http://www.capitol.tn.gov/house/archives/102GA/Committees/HComm.htm'
'http://www.capitol.tn.gov/house/archives/103GA/hcommemb.htm'
'http://www.capitol.tn.gov/house/archives/104GA/hcommemb.htm'
'http://www.capitol.tn.gov/house/archives/105GA/Committees/hcommemb.htm'
'http://www.capitol.tn.gov/house/archives/106GA/Committees/index.html'

"""
import re

from billy.scrape.committees import Committee, CommitteeScraper
import lxml.html


def fix_whitespace(s):
    return re.sub(r'\s+', ' ', s)


class TNCommitteeScraper(CommitteeScraper):
    jurisdiction = 'tn'
    base_href = 'http://www.capitol.tn.gov'
    chambers = {
        'lower': 'house',
        'upper': 'senate'
    }

    def scrape(self, chamber, term):
        self.validate_term(term, latest_only=True)
        url_chamber = self.chambers[chamber]
        url = 'http://www.capitol.tn.gov/%s/committees/' % (url_chamber)
        if chamber == 'upper':
            self.scrape_senate_committees(url)
            self.scrape_joint_committees()
        else:
            self.scrape_house_committees(url)

    #Scrapes all the Senate committees
    def scrape_senate_committees(self, url):

        page = self.urlopen(url)
        # Find individual committee urls
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        find_expr = "//dl[@class='senateCommittees']/dt/a"
        links = [(a.text_content(), a.attrib['href']) for a in
                 page.xpath(find_expr)]

        # title, url
        for committee_name, link in links:
            self.scrape_senate_committee(committee_name, link)

    #Scrapes the individual Senate committee
    def scrape_senate_committee(self, committee_name, link):
        """Scrape individual committee page and add members"""

        com = Committee('upper', committee_name)

        page = self.urlopen(link)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(link)

        # Find individual committee urls
        find_expr = ('//h3[contains(., "Committee Officers")]/'
                     'following-sibling::ul/li/a')
        for a in page.xpath(find_expr):
            if not a.text:
                continue
            member_name = a.text
            role = (a.tail or 'member').strip(', ')
            com.add_member(member_name, role)

        com.add_source(link)
        if com['members']:
            self.save_committee(com)

    #Scrapes all the House Committees
    def scrape_house_committees(self, url):
        # Committees are listed in h3 w/ no attributes.
        # Only indicator is a div w/ 2 classes
        # self.headers['user-agent'] = 'cow'

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        # Committee urls.
        links = doc.xpath('//div[contains(@class, "committeelist")]//a')

        for a in links:
            self.scrape_house_committee(a.text.strip(), a.get('href'))

    #Scrapes the individual House Committee
    def scrape_house_committee(self, committee_name, link):
        """Scrape individual committee page and add members"""

        html = self.urlopen(link)
        doc = lxml.html.fromstring(html)

        subcommittee = False
        for h1 in doc.xpath('//h1/text()'):
            if 'subcommittee' in h1.lower():
                subcommittee = True

        subcomm_name = ('Subcommittee' if subcommittee else None)

        if subcommittee:
            committee_name = committee_name.replace(' Subcommittee', '')
        com = Committee('lower', committee_name, subcomm_name)

        find_expr = "//div[@class='col1']/ul[position()<3]/li/a"
        for a in doc.xpath(find_expr):
            name = a.text
            role = (a.tail or '').strip(', ') or 'member'
            if name:
                com.add_member(name, role)

        com.add_source(link)
        if com['members']:
            self.save_committee(com)

    #Scrapes joint committees
    def scrape_joint_committees(self):
        main_url = 'http://www.capitol.tn.gov/joint/'

        page = self.urlopen(main_url)
        page = lxml.html.fromstring(page)

        for el in page.xpath("//div[@class='col2']/ul/li/a"):
            com_name = el.text
            com_link = el.attrib["href"]
            #cleaning up links
            if '..' in com_link:
                com_link = self.base_href + com_link[2:len(com_link)]
            self.scrape_joint_committee(com_name, com_link)

    #Scrapes the individual joint committee - most of it is special case
    def scrape_joint_committee(self, committee_name, url):
        com = Committee('joint', 'Joint ' + committee_name)
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)

        if 'state.tn.us' in url:
            for el in page.xpath("//div[@class='Blurb']/table//tr[2 <= position() and  position() < 10]/td[1]/a"):
                member_name = el.text
                if 'Senator' in member_name:
                    member_name = member_name[8:len(member_name)]
                elif 'Representative' in member_name:
                    member_name = member_name[15:len(member_name)]
                else:
                    member_name = member_name[17: len(member_name)]
                com.add_member(member_name, 'member')

        elif 'gov-opps' in url:
            links = ['senate', 'house']
            for link in links:
                chamber_link = self.base_href + '/' + link + '/committees/gov-opps.html'
                chamber_page = self.urlopen(chamber_link)
                chamber_page = lxml.html.fromstring(chamber_page)
                for mem in chamber_page.xpath("//div[@class='col1']/ul[position() <= 2]/li/a"):
                    member = [item.strip() for item in mem.text_content().split(',', 1)]
                    if len(member) > 1:
                        member_name, role = member
                    else:
                        member_name, role = member[0], 'member'
                    if member_name != "":
                        com.add_member(member_name, role)
                com.add_source(chamber_link)
        else:
            # If the member sections all state "TBA", skip saving this committee.
            li_text = page.xpath("//div[@class='col1']/ul[position() <= 3]/li/text()")
            if set(li_text) == set(['TBA']):
                return
            for el in page.xpath("//div[@class='col1']/ul[position() <= 3]/li/a"):
                member = [item.strip() for item in el.text_content().split(',', 1)]
                if len(member) > 1:
                    member_name, role = member
                else:
                    member_name, role = member[0], 'member'
                if member_name != "":
                    com.add_member(member_name, role)
        com.add_source(url)
        if com['members']:
            self.save_committee(com)

########NEW FILE########
__FILENAME__ = events
import datetime as dt

from billy.scrape import NoDataForPeriod
from billy.scrape.events import Event, EventScraper

import lxml.html
import pytz

cal_weekly_events = "http://wapp.capitol.tn.gov/apps/schedule/WeeklyView.aspx"
cal_chamber_text = {
    "upper" : "Senate",
    "lower" : "House",
    "other" : "Joint"
}

class TNEventScraper(EventScraper):
    jurisdiction = 'tn'

    _tz = pytz.timezone('US/Eastern')

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def url_xpath(self, url, xpath):
        page = self.lxmlize(url)
        return page.xpath(xpath)

    def _add_agenda_main(self, url, event):
        page = self.lxmlize(url)
        # OK. We get two kinds of links. Either a list to a bunch of agendas
        # or actually a list of agendas. We can check for a <h2> at the top
        # of the generated content
        generated_content = page.xpath("//label[@id='generatedcontent']")[0]
        h2s = generated_content.xpath(".//h2")
        if len(h2s) > 0:
            return self._add_agenda_real(url, event)
        return self._add_agenda_list(url, event)

    def _add_agenda_real(self, url, event):
        trs = self.url_xpath(url, "//tr")
        for tr in trs:
            tds = tr.xpath("./*")
            billinf = tds[0].attrib['id']  # TN uses bill_ids as the id
            descr = tr.xpath("./td//p")[-1].text_content()
            event.add_related_bill(
                billinf,
                description=descr,
                type="consideration"
            )
        event.add_source(url)
        event.add_document(url, "Agenda", type="agenda")
        return event

    def _add_agenda_list(self, url, event):
        trs = self.url_xpath(url, "//tr")
        for tr in trs:
            things = tr.xpath("./td/a")
            for thing in things:
                event = self._add_agenda_real(thing.attrib['href'], event)
        return event

    def add_agenda(self, url, name, event):
        if "CalendarMain" in url:
            return self._add_agenda_main(url, event)
        if "scheduledocs" in url:
            return event.add_document(name, url, "agenda")
        return event.add_document(name, url, "other")

    def scrape(self, chamber, session):
        chmbr = cal_chamber_text[chamber]
        tables = self.url_xpath(cal_weekly_events,
                                "//table[@class='date-table']")
        for table in tables:
            date = table.xpath("../.")[0].getprevious().text_content()
            trs = table.xpath("./tr")
            for tr in trs:
                order = ["time", "chamber", "type", "agenda", "location",
                         "video"]

                tds = tr.xpath("./td")
                metainf = {}

                if not tds:
                    continue

                for el in range(0, len(order)):
                    metainf[order[el]] = tds[el]

                if metainf['chamber'].text_content() == chmbr:
                    self.log("Skipping event based on chamber.")
                    continue

                time = metainf['time'].text_content()
                datetime_string = "%s %s" % (date, time)
                location = metainf['location'].text_content()
                description = metainf['type'].text_content()

                dtfmt = "%A, %B %d, %Y %I:%M %p"
                if time == 'Cancelled':
                    self.log("Skipping cancelled event.")
                    continue
                else:
                    if ' Immediately follows' in datetime_string:
                        datetime_string, _ = datetime_string.split(
                            'Immediately follows')
                        datetime_string = datetime_string.strip()
                        dtfmt = "%A, %B %d, %Y"

                    when = dt.datetime.strptime(datetime_string, dtfmt)
                event = Event(session, when, 'committee:meeting',
                              description, location=location)
                event.add_participant("host", description, 'committee', chamber=chamber)
                event.add_source(cal_weekly_events)

                agenda = metainf['agenda'].xpath(".//a")
                if len(agenda) > 0:
                    agenda = agenda
                    for doc in agenda:
                        if not doc.text_content():
                            continue
                        agenda_url = doc.attrib['href']
                        self.add_agenda(
                            agenda_url, doc.text_content(), event)
                self.save_event(event)

########NEW FILE########
__FILENAME__ = legislators
from billy.scrape.legislators import LegislatorScraper, Legislator
import lxml.html


class TNLegislatorScraper(LegislatorScraper):
    jurisdiction = 'tn'

    def scrape(self, chamber, term):
        self.validate_term(term, latest_only=False)
        root_url = 'http://www.capitol.tn.gov/'
        parties = {'D': 'Democratic', 'R': 'Republican',
                   'CCR': 'Carter County Republican',
                   'I': 'Independent'}

        #testing for chamber
        if chamber == 'upper':
            url_chamber_name = 'senate'
            abbr = 's'
        else:
            url_chamber_name = 'house'
            abbr = 'h'
        if term != self.metadata["terms"][-1]["sessions"][0]:
            chamber_url = root_url + url_chamber_name
            chamber_url += '/archives/' + term + 'GA/Members/index.html'
        else:
            chamber_url = root_url + url_chamber_name + '/members/'

        page = self.urlopen(chamber_url)
        page = lxml.html.fromstring(page)

        for row in page.xpath("//tr")[1:]:

            # Skip any a header row.
            if set(child.tag for child in row) == set(['th']):
                continue

            partyInit = row.xpath('td[2]')[0].text.split()[0]
            party = parties[partyInit]
            district = row.xpath('td[4]/a')[0].text.split()[1]
            address = row.xpath('td[5]')[0].text_content()
            # 301 6th Avenue North Suite
            address = address.replace('LP',
                              'Legislative Plaza\nNashville, TN 37243')
            address = address.replace('WMB',
                              'War Memorial Building\nNashville, TN 37243')
            address = '301 6th Avenue North\nSuite ' + address
            phone = row.xpath('td[6]')[0].text
            #special case for Karen D. Camper
            if phone == None:
                phone = row.xpath('td[6]/div')[0].text
            phone = '615-' + phone.split()[0]
            email = row.xpath('td[7]/a')[0].text
            member_url = (root_url + url_chamber_name + '/members/'
                          + abbr + district + '.html')
            member_photo_url = (root_url + url_chamber_name +
                                '/members/images/' + abbr + district +
                                '.jpg')

            member_page = self.urlopen(member_url)
            member_page = lxml.html.fromstring(member_page)
            name = member_page.xpath('//div[@id="membertitle"]/h2')[0].text
            if 'Speaker' in name:
                full_name = name[8:len(name)]
            elif 'Lt.' in name:
                full_name = name[13:len(name)]
            elif abbr == 'h':
                full_name = name[5: len(name)]
            else:
                full_name = name[8:len(name)]

            leg = Legislator(term, chamber, district, full_name.strip(),
                             party=party, email=email, url=member_url,
                             photo_url=member_photo_url)
            leg.add_source(chamber_url)
            leg.add_source(member_url)

            # TODO: add district address from this page

            leg.add_office('capitol', 'Nashville Address',
                           address=address, phone=phone)

            self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = bills
import os
import re
from urlparse import urljoin
import datetime

from billy.utils import urlescape
from billy.scrape import ScrapeError
from billy.scrape.bills import BillScraper, Bill

from .utils import chamber_name, parse_ftp_listing

import lxml.etree


class TXBillScraper(BillScraper):
    jurisdiction = 'tx'
    _ftp_root = 'ftp://ftp.legis.state.tx.us/'

    def scrape(self, chamber, session):
        """Scrapes information on all bills for a given chamber and session."""
        self.validate_session(session)

        if len(session) == 2:
            session = '%sR' % session

        for bill_type in ['bills', 'concurrent_resolutions',
                          'joint_resolutions', 'resolutions']:
            # This is the billhistory directory for a particular type of bill
            # (e.g. senate resolutions).  It should contain subdirectories
            # with names like "SR00001_SR00099".
            history_dir_url = urljoin(
                self._ftp_root, '/bills/%s/billhistory/%s_%s/' % (
                    session, chamber_name(chamber), bill_type))

            history_groups_listing = self.urlopen(history_dir_url)
            # A group_dir has a name like "HJR00200_HJR00299" and contains
            # the files for a group of 100 bills.
            for group_dir in parse_ftp_listing(history_groups_listing):
                self.scrape_group(
                    chamber, session, history_dir_url, group_dir)

    def scrape_group(self, chamber, session, history_dir_url, group_dir):
        """Scrapes information on all bills in a given group of 100 bills."""
        # Under billhistory, each group dir has a name like HBnnnnn_HBnnnnn,
        # HCRnnnnn_HCRnnnnn, HJRnnnnn_HJRnnnnn, HRnnnnn_HRnnnnn.
        history_group_url = urljoin(history_dir_url, group_dir) + '/'
        # For each group_dir under billhistory, there is a corresponding dir in
        # billtext/html containing the bill versions (texts).  These dirs have
        # similar names, except the prefix is "HC", "HJ", "SC", "SJ" for
        # concurrent/joint resolutions instead of "HCR", "HJR", "SCR", "SJR".

        # Now get the history and version data for each bill.
        histories_list = self.urlopen(history_group_url)
        for history_file in parse_ftp_listing(histories_list):
            url = urljoin(history_group_url, history_file)
            bill_num = int(re.search(r'\d+', history_file).group(0))
            self.scrape_bill(chamber, session, url, history_group_url,
                             bill_num)

    def scrape_bill(self, chamber, session, history_url, history_group_url,
                   billno):
        """Scrapes the information for a single bill."""
        history_xml = self.urlopen(history_url)
        if "Bill does not exist." in history_xml:
            return

        bill = self.parse_bill_xml(chamber, session, history_xml)
        if bill is None:
            return

        bill.add_source(history_url)

        text_group_url = history_group_url.replace(
            '/billhistory/', '/billtext/html/')
        text_group_url = re.sub('([HS][CJ])R', '\\1', text_group_url)
        version_urls = {}
        # Get the list of all the bill versions in this group, and collect
        # the filenames together by bill number.
        versions_list = self.urlopen(text_group_url)
        for version_file in parse_ftp_listing(versions_list):
            url = urljoin(text_group_url, version_file)
            bill_num = int(re.search(r'\d+', version_file).group(0))
            version_urls.setdefault(bill_num, []).append(url)

        if billno in version_urls:
            version_urls = version_urls[billno] #  Sorry :(
            # We need to get the versions from inside here because some bills
            # have XML saying just "no such bill", so we hit an ftp error
            # because there are no bill versions where we expect them.
            #
            # It's a good idea to somehow cache this list, but we need to make
            # sure it exists first. FIXME(nice-to-have)

            for version_url in version_urls:
                bill.add_source(version_url)
                version_name = version_url.split('/')[-1]
                version_name = os.path.splitext(version_name)[0]  # omit '.htm'
                bill.add_version(version_name, version_url,
                                 mimetype='text/html')

        self.save_bill(bill)

    def parse_bill_xml(self, chamber, session, txt):
        root = lxml.etree.fromstring(txt.bytes)
        bill_id = ' '.join(root.attrib['bill'].split(' ')[1:])
        bill_title = root.findtext("caption")
        if bill_title is None:
            return None

        if session[2] == 'R':
            session = session[0:2]

        if bill_id[1] == 'B':
            bill_type = ['bill']
        elif bill_id[1] == 'R':
            bill_type = ['resolution']
        elif bill_id[1:3] == 'CR':
            bill_type = ['concurrent resolution']
        elif bill_id[1:3] == 'JR':
            bill_type = ['joint resolution']
        else:
            raise ScrapeError("Invalid bill_id: %s" % bill_id)

        bill = Bill(session, chamber, bill_id, bill_title, type=bill_type)

        versions = root.xpath("//versions")
        for version in versions:
            versionz = version.xpath(".//version")
            for v in versionz:
                description = v.xpath(".//versionDescription")[0].text
                html_url = v.xpath(".//WebHTMLURL")[0].text
                bill.add_version(description, html_url, 'text/html')

        for action in root.findall('actions/action'):
            act_date = datetime.datetime.strptime(action.findtext('date'),
                                            "%m/%d/%Y").date()

            extra = {}
            extra['action_number'] = action.find('actionNumber').text
            comment = action.find('comment')
            if comment is not None and comment.text:
                extra['comment'] = comment.text.strip()

            actor = {'H': 'lower',
                     'S': 'upper',
                     'E': 'executive'}[extra['action_number'][0]]

            desc = action.findtext('description').strip()

            if desc == 'Scheduled for public hearing on . . .':
                continue

            introduced = False

            if desc == 'Amended':
                atype = 'amendment:passed'
            elif desc == 'Amendment(s) offered':
                atype = 'amendment:introduced'
            elif desc == 'Amendment amended':
                atype = 'amendment:amended'
            elif desc == 'Amendment withdrawn':
                atype = 'amendment:withdrawn'
            elif desc == 'Passed' or desc == 'Adopted':
                atype = 'bill:passed'
            elif re.match(r'^Received (by|from) the', desc):
                if 'Secretary of the Senate' not in desc:
                    atype = 'bill:introduced'
                else:
                    atype = 'bill:filed'
            elif desc.startswith('Sent to the Governor'):
                # But what if it gets lost in the mail?
                atype = 'governor:received'
            elif desc.startswith('Signed by the Governor'):
                atype = 'governor:signed'
            elif desc == 'Vetoed by the Governor':
                atype = 'governor:vetoed'
            elif desc == 'Read first time':
                atype = ['bill:introduced', 'bill:reading:1']
                introduced = True
            elif desc == 'Read & adopted':
                atype = ['bill:passed']
                if not introduced:
                    introduced = True
                    atype.append('bill:introduced')
            elif desc == "Passed as amended":
                atype = 'bill:passed'
            elif desc.startswith('Referred to') or desc.startswith("Recommended to be sent to "):
                atype = 'committee:referred'
            elif desc == "Reported favorably w/o amendment(s)":
                atype = 'committee:passed'
            elif desc == "Filed":
                atype = 'bill:filed'
            elif desc == 'Read 3rd time':
                atype = 'bill:reading:3'
            elif desc == 'Read 2nd time':
                atype = 'bill:reading:2'
            elif desc.startswith('Reported favorably'):
                atype = 'committee:passed:favorable'
            else:
                atype = 'other'

            if 'committee:referred' in atype:
                repls = [
                    'Referred to',
                    "Recommended to be sent to "
                ]
                ctty = desc
                for r in repls:
                    ctty = ctty.replace(r, "").strip()
                extra['committees'] = ctty

            bill.add_action(actor, action.findtext('description'),
                            act_date, type=atype, **extra)

        for author in root.findtext('authors').split(' | '):
            if author != "":
                bill.add_sponsor('primary', author, official_type='author')
        for coauthor in root.findtext('coauthors').split(' | '):
            if coauthor != "":
                bill.add_sponsor('cosponsor', coauthor, official_type='coauthor')
        for sponsor in root.findtext('sponsors').split(' | '):
            if sponsor != "":
                bill.add_sponsor('primary', sponsor, official_type='sponsor')
        for cosponsor in root.findtext('cosponsors').split(' | '):
            if cosponsor != "":
                bill.add_sponsor('cosponsor', cosponsor, official_type='cosponsor')

        bill['subjects'] = []
        for subject in root.iterfind('subjects/subject'):
            bill['subjects'].append(subject.text.strip())

        return bill

########NEW FILE########
__FILENAME__ = events
import re
import datetime as dt

from billy.scrape import NoDataForPeriod
from billy.scrape.events import EventScraper, Event

import pytz
import lxml.html


class TXEventScraper(EventScraper):
    jurisdiction = 'tx'
    _tz = pytz.timezone('US/Central')
    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def scrape(self, chamber, session):
        if not session.startswith('83'):  # XXX: Fixme
            raise NoDataForPeriod(session)

        self.scrape_committee_upcoming(session, chamber)

    def scrape_event_page(self, session, chamber, url, datetime):
        page = self.lxmlize(url)
        info = page.xpath("//p")
        metainf = {}
        plaintext = ""
        for p in info:
            content = re.sub("\s+", " ", p.text_content())
            plaintext += content + "\n"
            if ":" in content:
                key, val = content.split(":", 1)
                metainf[key.strip()] = val.strip()
        ctty = metainf['COMMITTEE']
        where = metainf['PLACE']
        if "CHAIR" in where:
            where, chair = where.split("CHAIR:")
            metainf['PLACE'] = where.strip()
            metainf['CHAIR'] = chair.strip()

        chair = None
        if "CHAIR" in metainf:
            chair = metainf['CHAIR']

        plaintext = re.sub("\s+", " ", plaintext).strip()
        regexp = r"(S|J|H)(B|M|R) (\d+)"
        bills = re.findall(regexp, plaintext)

        event = Event(session,
                      datetime,
                      'committee:meeting',
                      ctty,
                      chamber=chamber,
                      location=where,
                      agenda=plaintext)
        event.add_source(url)
        event.add_participant('host', ctty, 'committee', chamber=chamber)
        if not chair is None:
            event.add_participant('chair', chair, 'legislator', chamber=chamber)

        for bill in bills:
            chamber, type, number = bill
            bill_id = "%s%s %s" % ( chamber, type, number )
            event.add_related_bill(bill_id,
                                   type='consideration',
                                   description='Bill up for discussion')

        self.save_event(event)

    def scrape_page(self, session, chamber, url):
        try:
            page = self.lxmlize(url)
            events = page.xpath("//a[contains(@href, 'schedules/html')]")
            for event in events:
                peers = event.getparent().getparent().xpath("./*")
                date = peers[0].text_content()
                time = peers[1].text_content()
                tad = "%s %s" % ( date, time )
                tad = re.sub(r"(PM|AM).*", r"\1", tad)
                tad_fmt = "%m/%d/%Y %I:%M %p"
                if "AM" not in tad and "PM" not in tad:
                    tad_fmt = "%m/%d/%Y"
                    tad = date

                # Time expressed as 9:00 AM, Thursday, May 17, 2012
                datetime = dt.datetime.strptime(tad, tad_fmt)
                self.scrape_event_page(session, chamber, event.attrib['href'],
                                      datetime)
        except lxml.etree.XMLSyntaxError:
            pass  # lxml.etree.XMLSyntaxError: line 248: htmlParseEntityRef: expecting ';'
            # XXX: Please fix this, future hacker. I think this might be a problem
            # with lxml -- due diligence on this is needed.
            #                                              -- PRT

    def scrape_upcoming_page(self, session, chamber, url):
        page = self.lxmlize(url)
        date = None
        thyme = None

        for row in page.xpath(".//tr"):
            title = row.xpath(".//div[@class='sectionTitle']")
            if len(title) > 0:
                date = title[0].text_content()
                #print "Found date ", date
            time = row.xpath(".//td/strong")
            if len(time) > 0:
                thyme = time[0].text_content()
                #print "Found time ", thyme

            events = row.xpath(".//a[contains(@href, 'schedules/html')]")
            for event in events:
                datetime = "%s %s" % ( date, thyme )
                if "Upon Adjournment" in datetime:
                    continue

                replace = {
                    "or recess": "",
                    "see below": "",
                    "See Below": "",
                    "See below": "",
                    "of the House": "",
                    "of the Senate": "",
                    "Finance Committee": "",
                    "or upon adjournment": "",
                    "9:00 AM Mountain Time": "",
                    "or upon the adjournment": "",
                    "of the House and the Senate": "",
                    "or upon final adjourn./recess": "",
                    "or 30 minutes upon adjournment": "",
                    "or the Senate Finance Committee": "",
                    "During reading and referral of bills": "",
                    "and House Chambers, whichever is later.": "",
                    "or during reading and referral of bills": "",
                    "(or 15 minutes after adjournment of the .*)": "",
                    "Upon final adjourn./recess": "",
                    "or upon recess/adjournment": "",
                }
                for rep in replace:
                    datetime = re.sub(rep, replace[rep], datetime)
                datetime = datetime.strip()

                try:
                    datetime = dt.datetime.strptime(datetime, "%A, %B %d, %Y %I:%M %p")
                except ValueError:
                    datetime = dt.datetime.strptime(datetime, "%A, %B %d, %Y")
                self.scrape_event_page(session, chamber, event.attrib['href'],
                                      datetime)

    def scrape_committee_upcoming(self, session, chamber):
        chid = {'upper': 'S',
                        'lower': 'H',
                        'other': 'J'}[chamber]

        url = "http://www.capitol.state.tx.us/Committees/Committees.aspx" + \
                "?Chamber=" + chid

        page = self.lxmlize(url)
        refs = page.xpath("//div[@id='content']//a")
        for ref in refs:
            self.scrape_page(session, chamber, ref.attrib['href'])


        url = "http://www.capitol.state.tx.us/Committees/MeetingsUpcoming.aspx" + \
                "?Chamber=" + chid
        self.scrape_upcoming_page(session, chamber, url)

########NEW FILE########
__FILENAME__ = legislators
import re

from billy.utils import urlescape
from billy.scrape.legislators import (LegislatorScraper, Legislator,
                                            Person)
from .utils import clean_committee_name

import lxml.html

phone_re = re.compile('\(\d{3}\) \d{3}-\d{4}')

class TXLegislatorScraper(LegislatorScraper):
    jurisdiction = 'tx'

    def scrape(self, chamber, term):
        self.validate_term(term, latest_only=True)

        if chamber == 'upper':
            chamber_type = 'S'
        else:
            chamber_type = 'H'

        url = ("http://www.legdir.legis.state.tx.us/members.aspx?type=%s" %
               chamber_type)
        page = self.urlopen(url)
        root = lxml.html.fromstring(page)

        for li in root.xpath('//ul[@class="options"]/li'):
            member_url = re.match(r"goTo\('(MemberInfo[^']+)'\);",
                                  li.attrib['onclick']).group(1)
            member_url = ("http://www.legdir.legis.state.tx.us/" +
                          member_url)
            self.scrape_member(chamber, term, member_url)

    def scrape_member(self, chamber, term, member_url):
        page = self.urlopen(member_url)
        root = lxml.html.fromstring(page)
        root.make_links_absolute(member_url)

        sdiv = root.xpath('//div[@class="subtitle"]')[0]
        table = sdiv.getnext()

        photo_url = table.xpath('//img[@id="ctl00_ContentPlaceHolder1'
                                '_imgMember"]')[0].attrib['src']

        td = table.xpath('//td[@valign="top"]')[0]

        type = td.xpath('string(//div[1]/strong)').strip()

        full_name = td.xpath('//div/strong/text()')
        full_name = [re.sub(r'\s+', ' ', x).strip() for x in full_name]
        full_name = full_name[-1]

        district = td.xpath('string(//div[3])').strip()
        district = district.replace('District ', '')

        party = td.xpath('string(//div[4])').strip()[0]
        if party == 'D':
            party = 'Democratic'
        elif party == 'R':
            party = 'Republican'

        if type == 'Lt. Gov.':
            leg = Person(full_name)
            leg.add_role('Lt. Governor', term, party=party)
        else:
            leg = Legislator(term, chamber, district, full_name,
                             party=party, photo_url=photo_url,
                             url=member_url)

        leg.add_source(urlescape(member_url))

        # add addresses
        for atype, text in (('capitol', 'Capitol address'),
                            ('district', 'District address')):
            aspan = root.xpath("//span[. = '%s:']" % text)
            addr = ''
            phone = None
            if aspan:
                # cycle through brs
                addr = aspan[0].tail.strip()
                elem = aspan[0].getnext()
                while elem is not None and elem.tag == 'br':
                    if elem.tail:
                        if not phone_re.match(elem.tail):
                            addr += "\n" + elem.tail
                        else:
                            phone = elem.tail
                    elem = elem.getnext()
                # now add the addresses
                leg.add_office(atype, text, address=addr, phone=phone)

        # add committees
        comm_div = root.xpath('//div[string() = "Committee Membership:"]'
                              '/following-sibling::div'
                              '[@class="rcwcontent"]')[0]

        for link in comm_div.xpath('*/a'):
            name = link.text

            if '(Vice Chair)' in name:
                mtype = 'vice chair'
            elif '(Chair)' in name:
                mtype = 'chair'
            else:
                mtype = 'member'

            name = clean_committee_name(link.text)

            # There's no easy way to determine whether a committee
            # is joint or not using the mobile legislator directory
            # (without grabbing a whole bunch of pages, at least)
            # so for now we will hard-code the one broken case
            if (name == "Oversight of HHS Eligibility System" and
                term == '82'):
                comm_chamber = 'joint'
            else:
                comm_chamber = chamber

            if name.startswith('Appropriations-S/C on '):
                sub = name.replace('Appropriations-S/C on ', '')
                leg.add_role('committee member', term,
                             chamber=comm_chamber,
                             committee='Appropriations',
                             subcommittee=sub,
                             position=mtype)
            else:
                leg.add_role('committee member', term,
                             chamber=comm_chamber,
                             committee=name,
                             position=mtype)

        if type == 'Lt. Gov.':
            self.save_object(leg)
        else:
            if district:
                self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = utils
import re


def clean_committee_name(comm_name):
    comm_name = comm_name.strip()
    comm_name = re.sub(r' ?[-,]? ?\(?(Co|Vice)?[- ]?[Cc]hair\)?',
                       '',
                       comm_name)
    comm_name = re.sub(r'\d+/\d+/\d+ - \d+/\d+/\d+', '', comm_name)
    comm_name = re.sub('Appropriations - S/C:', 'Appropriations-S/C on',
                       comm_name).strip()
    if comm_name == 'Appropriations-S/C Stimulus':
        comm_name = 'Appropriations-S/C on Stimulus'

    return comm_name


def parse_ftp_listing(text):
    lines = text.strip().split('\r\n')
    return (' '.join(line.split()[3:]) for line in lines if line)


def chamber_name(chamber):
    if chamber == 'upper':
        return 'senate'
    else:
        return 'house'

########NEW FILE########
__FILENAME__ = votes
import os
import re
import uuid
import urlparse
import datetime

from billy.scrape.votes import VoteScraper, Vote
from .utils import parse_ftp_listing

import lxml.html


def next_tag(el):
    """
    Return next tag, skipping <br>s.
    """
    el = el.getnext()
    while el.tag == 'br':
        el = el.getnext()
    return el


def clean_journal(root):
    # Remove page breaks
    for el in root.xpath('//hr[@noshade and @size=1]'):
        parent = el.getparent()
        previous = el.getprevious()
        if previous:
            parent.remove(previous)
        parent.remove(el)

    # Does lxml not support xpath ends-with?
    for el in root.xpath("//p[contains(text(), 'REGULAR SESSION')]"):
        if el.text.endswith("REGULAR SESSION"):
            parent = el.getparent()
            parent.remove(el)

    for el in root.xpath("//p[contains(text(), 'JOURNAL')]"):
        if (("HOUSE JOURNAL" in el.text or "SENATE JOURNAL" in el.text) and
            "Day" in el.text):

            parent = el.getparent()
            parent.remove(el)

    # Remove empty paragraphs
    for el in root.xpath('//p[not(node())]'):
        if el.tail and el.tail != '\r\n' and el.getprevious() is not None:
            el.getprevious().tail = el.tail
        el.getparent().remove(el)

    # Journal pages sometimes replace spaces with <font color="White">i</font>
    # (or multiple i's for bigger spaces)
    for el in root.xpath('//font[@color="White"]'):
        if el.text:
            el.text = ' ' * len(el.text)


def names(el):
    text = (el.text or '') + (el.tail or '')

    names = []
    for name in text.split(';'):
        name = name.strip().replace('\r\n', '').replace('  ', ' ')

        if not name:
            continue

        if name == 'Gonzalez Toureilles':
            name = 'Toureilles'
        elif name == 'Mallory Caraway':
            name = 'Caraway'
        elif name == 'Martinez Fischer':
            name = 'Fischer'
        elif name == 'Rios Ybarra':
            name = 'Ybarra'

        names.append(name)

    # First name will have stuff to ignore before an mdash
    names[0] = names[0].split(u'\u2014')[1].strip()
    # Get rid of trailing '.'
    names[-1] = names[-1][0:-1]

    return names


def get_motion(match):
    if match.group('type') == 'passed' or match.group('type') == 'adopted':
        return 'final passage'
    else:
        return 'to ' + match.group('to')


def get_type(motion):
    if motion == 'final passage':
        return 'passage'
    else:
        return 'other'


def votes(root, session):
    for vote in record_votes(root, session):
        yield vote
    for vote in viva_voce_votes(root, session):
        yield vote


def record_votes(root, session):
    for el in root.xpath(u'//div[starts-with(., "Yeas \u2014")]'):
        text = ''.join(el.getprevious().getprevious().itertext())
        text.replace('\n', ' ')
        m = re.search(r'(?P<bill_id>\w+\W+\d+)(,?\W+as\W+amended,?)?\W+was\W+'
                      '(?P<type>adopted|passed'
                      '(\W+to\W+(?P<to>engrossment|third\W+reading))?)\W+'
                      'by\W+\(Record\W+(?P<record>\d+)\):\W+'
                      '(?P<yeas>\d+)\W+Yeas,\W+(?P<nays>\d+)\W+Nays,\W+'
                      '(?P<present>\d+)\W+Present', text)
        if m:
            yes_count = int(m.group('yeas'))
            no_count = int(m.group('nays'))
            other_count = int(m.group('present'))

            bill_id = m.group('bill_id')
            bill_id = bill_id.replace(u'\xa0', ' ')
            bill_id = re.sub(r'CS(SB|HB)', r'\1', bill_id)

            if bill_id.startswith('H') or bill_id.startswith('CSHB'):
                bill_chamber = 'lower'
            elif bill_id.startswith('S') or bill_id.startswith('CSSB'):
                bill_chamber = 'upper'
            else:
                continue

            motion = get_motion(m)

            vote = Vote(None, None, motion, True,
                        yes_count, no_count, other_count)
            vote['bill_id'] = bill_id
            vote['bill_chamber'] = bill_chamber
            vote['session'] = session[0:2]
            vote['method'] = 'record'
            vote['record'] = m.group('record')
            vote['type'] = get_type(motion)

            for name in names(el):
                vote.yes(name)

            el = next_tag(el)
            if el.text and el.text.startswith('Nays'):
                for name in names(el):
                    vote.no(name)
                el = next_tag(el)

            while el.text and re.match(r'Present|Absent', el.text):
                for name in names(el):
                    vote.other(name)
                el = next_tag(el)

            vote['other_count'] = len(vote['other_votes'])
            yield vote
        else:
            pass


def viva_voce_votes(root, session):
    prev_id = None
    for el in root.xpath(u'//div[starts-with(., "All Members are deemed")]'):
        text = ''.join(el.getprevious().getprevious().itertext())
        text.replace('\n', ' ')
        m = re.search(r'(?P<bill_id>\w+\W+\d+)(,\W+as\W+amended,)?\W+was\W+'
                      '(?P<type>adopted|passed'
                      '(\W+to\W+(?P<to>engrossment|third\W+reading))?)\W+'
                      'by\W+a\W+viva\W+voce\W+vote', text)
        if m:
            motion = get_motion(m)

            # No identifier, generate our own
            record = str(uuid.uuid1())

            bill_id = m.group('bill_id')
            bill_id = bill_id.replace(u'\xa0', ' ')
            bill_id = re.sub(r'CS(SB|HB)', r'\1', bill_id)

            if bill_id.startswith('H') or bill_id.startswith('CSHB'):
                bill_chamber = 'lower'
            elif bill_id.startswith('S') or bill_id.startswith('CSSB'):
                bill_chamber = 'upper'
            else:
                continue

            vote = Vote(None, None, motion, True, 0, 0, 0)
            vote['bill_id'] = bill_id
            vote['bill_chamber'] = bill_chamber
            vote['session'] = session[0:2]
            vote['method'] = 'viva voce'
            vote['record'] = record
            vote['type'] = get_type(motion)
            yield vote
            continue

        m = re.search('The\W+bill\W+was.+and\W+was\W+'
                      '(?P<type>adopted|passed'
                      '(\W+to\W+(?P<to>engrossment|third\W+reading))?)\W+'
                      'by\W+a\W+viva\W+voce\W+vote', text)
        if m:
            prev_text = ''.join(el.getprevious().getprevious().itertext())
            m2 = re.match('(HB|SB|CSHB|CSSB|HR|SR)\W+\d+', prev_text)
            if m2:
                bill_id = m2.group()
                prev_id = bill_id
            else:
                # This is scary
                bill_id = prev_id

            if not bill_id:
                continue

            if bill_id.startswith('H') or bill_id.startswith('CSHB'):
                bill_chamber = 'lower'
            elif bill_id.startswith('S') or bill_id.startswith('CSSB'):
                bill_chamber = 'upper'
            else:
                continue

            bill_id = bill_id.replace(u'\xa0', ' ')
            motion = get_motion(m)

            record = str(uuid.uuid1())
            vote = Vote(None, None, motion, True, 0, 0, 0)
            vote['bill_id'] = bill_id
            vote['bill_chamber'] = bill_chamber
            vote['session'] = session[0:2]
            vote['method'] = 'viva voce'
            vote['record'] = record
            vote['type'] = get_type(motion)

            yield vote
            continue


class TXVoteScraper(VoteScraper):
    jurisdiction = 'tx'
    _ftp_root = 'ftp://ftp.legis.state.tx.us/'

    def scrape(self, chamber, session):
        self.validate_session(session)

        if session == '821':
            self.warning('no journals for session 821')
            return

        if len(session) == 2:
            session = "%sR" % session

        journal_root = urlparse.urljoin(self._ftp_root, ("/journals/" +
                                                         session +
                                                         "/html/"),
                                        True)

        if chamber == 'lower':
            journal_root = urlparse.urljoin(journal_root, "house/", True)
        else:
            journal_root = urlparse.urljoin(journal_root, "senate/", True)

        listing = self.urlopen(journal_root)
        for name in parse_ftp_listing(listing):
            if not name.startswith(session):
                continue
            url = urlparse.urljoin(journal_root, name)
            self.scrape_journal(url, chamber, session)

    def scrape_journal(self, url, chamber, session):
        year = metadata['session_details'][session]['start_date'].year
        page = self.urlopen(url)
        root = lxml.html.fromstring(page)
        clean_journal(root)

        if chamber == 'lower':
            div = root.xpath("//div[@class = 'textpara']")[0]
            date_str = div.text.split('---')[1].strip()
            date = datetime.datetime.strptime(
                date_str, "%A, %B %d, %Y").date()
        else:
            fname = os.path.split(urlparse.urlparse(url).path)[-1]
            date_str = re.match(r'%sSJ(\d\d-\d\d).*\.htm' % session,
                            fname).group(1) + " %s" % year
            date = datetime.datetime.strptime(date_str,
                                              "%m-%d %Y").date()

        for vote in votes(root, session):
            vote['date'] = date
            vote['chamber'] = chamber
            vote.add_source(url)
            self.save_vote(vote)

########NEW FILE########
__FILENAME__ = bills
import re
import datetime

from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

import lxml.html
import scrapelib
import logging

logger = logging.getLogger('openstates')


SUB_BLACKLIST = [
    "Second Substitute",
    "Third Substitute",
    "Fourth Substitute",
    "Fifth Substitute",
    "Sixth Substitute",
    "Seventh Substitute",
    "Eighth Substitute",
    "Ninth Substitute",
    "Substitute",
]  # Pages are the same, we'll strip this from bills we catch.



class UTBillScraper(BillScraper):
    jurisdiction = 'ut'

    def accept_response(self, response):
        # check for rate limit pages
        normal = super(UTBillScraper, self).accept_response(response)
        return (normal and
                'com.microsoft.jdbc.base.BaseSQLException' not in
                    response.text and
                'java.sql.SQLException' not in
                    response.text)
        # The UT site has been throwing a lot of transiant DB errors, these
        # will backoff and retry if their site has an issue. Seems to happen
        # often enough.

    def scrape(self, chamber, session):
        self.validate_session(session)

        if chamber == 'lower':
            bill_abbrs = r'HB|HCR|HJ|HR'
        else:
            bill_abbrs = r'SB|SCR|SJR|SR'

        bill_list_re = r'(%s).*ht\.htm' % bill_abbrs

        bill_list_url = "http://www.le.state.ut.us/~%s/bills.htm" % (
            session.replace(' ', ''))

        page = self.urlopen(bill_list_url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(bill_list_url)

        for link in page.xpath('//a'):
            if "href" not in link.attrib:
                continue  # XXX: There are some funky <a> tags here.

            if re.search(bill_list_re, link.attrib['href']):
                self.scrape_bill_list(chamber, session,
                                      link.attrib['href'])

    def scrape_bill_list(self, chamber, session, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        for link in page.xpath('//a[contains(@href, "billhtm")]'):
            bill_id = link.xpath('string()').strip()

            self.scrape_bill(chamber, session, bill_id,
                             link.attrib['href'])

    def scrape_bill(self, chamber, session, bill_id, url):
        try:
            page = self.urlopen(url)
        except scrapelib.HTTPError:
            self.warning("couldn't open %s, skipping bill" % url)
            return
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)

        header = page.xpath('//h3/br')[0].tail.replace('&nbsp;', ' ')
        title, primary_sponsor = header.split(' -- ')

        if bill_id.startswith('H.B.') or bill_id.startswith('S.B.'):
            bill_type = ['bill']
        elif bill_id.startswith('H.R.') or bill_id.startswith('S.R.'):
            bill_type = ['resolution']
        elif bill_id.startswith('H.C.R.') or bill_id.startswith('S.C.R.'):
            bill_type = ['concurrent resolution']
        elif bill_id.startswith('H.J.R.') or bill_id.startswith('S.J.R.'):
            bill_type = ['joint resolution']

        for flag in SUB_BLACKLIST:
            if flag in bill_id:
                bill_id = bill_id.replace(flag, " ")
        bill_id = re.sub("\s+", " ", bill_id).strip()

        bill = Bill(session, chamber, bill_id, title, type=bill_type)
        bill.add_sponsor('primary', primary_sponsor)
        bill.add_source(url)

        for link in page.xpath(
            '//a[contains(@href, "bills/") and text() = "HTML"]'):

            name = link.getprevious().tail.strip()
            bill.add_version(name, link.attrib['href'], mimetype="text/html")
            next = link.getnext()
            if next.text == "PDF":
                bill.add_version(name, next.attrib['href'],
                                 mimetype="application/pdf")

        for link in page.xpath(
            "//a[contains(@href, 'fnotes') and text() = 'HTML']"):

            bill.add_document("Fiscal Note", link.attrib['href'])

        subjects = []
        for link in page.xpath("//a[contains(@href, 'RelatedBill')]"):
            subjects.append(link.text.strip())
        bill['subjects'] = subjects

        status_link = page.xpath('//a[contains(@href, "billsta")]')[0]
        self.parse_status(bill, status_link.attrib['href'])

        self.save_bill(bill)

    def parse_status(self, bill, url):
        page = self.urlopen(url)
        bill.add_source(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        uniqid = 0

        for row in page.xpath('//table/tr')[1:]:
            uniqid += 1
            date = row.xpath('string(td[1])')
            date = datetime.datetime.strptime(date, "%m/%d/%Y").date()

            action = row.xpath('string(td[2])')
            actor = bill['chamber']

            if '/' in action:
                actor = action.split('/')[0].strip()

                if actor == 'House':
                    actor = 'lower'
                elif actor == 'Senate':
                    actor = 'upper'
                elif actor == 'LFA':
                    actor = 'Office of the Legislative Fiscal Analyst'

                action = '/'.join(action.split('/')[1:]).strip()

            if action == 'Governor Signed':
                actor = 'executive'
                type = 'governor:signed'
            elif action == 'Governor Vetoed':
                actor = 'executive'
                type = 'governor:vetoed'
            elif action.startswith('1st reading'):
                type = ['bill:introduced', 'bill:reading:1']
            elif action == 'to Governor':
                type = 'governor:received'
            elif action == 'passed 3rd reading':
                type = 'bill:passed'
            elif action.startswith('passed 2nd & 3rd readings'):
                type = 'bill:passed'
            elif action == 'to standing committee':
                comm_link = row.xpath("td[3]/font/font/a")[0]
                comm = re.match(
                    r"writetxt\('(.*)'\)",
                    comm_link.attrib['onmouseover']).group(1)
                action = "to " + comm
                type = 'committee:referred'
            elif action.startswith('2nd reading'):
                type = 'bill:reading:2'
            elif action.startswith('3rd reading'):
                type = 'bill:reading:3'
            elif action == 'failed':
                type = 'bill:failed'
            elif action.startswith('2nd & 3rd readings'):
                type = ['bill:reading:2', 'bill:reading:3']
            elif action == 'passed 2nd reading':
                type = 'bill:reading:2'
            elif 'Comm - Favorable Recommendation' in action:
                type = 'committee:passed:favorable'
            elif action == 'committee report favorable':
                type = 'committee:passed:favorable'
            else:
                type = 'other'

            bill.add_action(actor, action, date, type=type,
                            _vote_id=uniqid)

            # Check if this action is a vote
            vote_links = row.xpath('./td[4]//a')
            for vote_link in vote_links:
                vote_url = vote_link.attrib['href']

                # Committee votes are of a different format that
                # we don't handle yet
                if not vote_url.endswith('txt'):
                    self.parse_html_vote(bill, actor, date, action,
                                         vote_url, uniqid)
                else:
                    self.parse_vote(bill, actor, date, action,
                                    vote_url, uniqid)

    def scrape_committee_vote(self, bill, actor, date, motion, url, uniqid):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        committee = page.xpath("//b")[0].text_content()
        votes = page.xpath("//table")[0]
        rows = votes.xpath(".//tr")[0]
        yno = rows.xpath(".//td")
        if len(yno) < 3:
            yes = yno[0]
            no, other = None, None
        else:
            yes, no, other = rows.xpath(".//td")

        def proc_block(obj):
            if obj is None:
                return {
                    "type": None,
                    "count": None,
                    "votes": []
                }

            typ = obj.xpath("./b")[0].text_content()
            count = obj.xpath(".//b")[0].tail.replace("-", "").strip()
            count = int(count)
            votes = []
            for vote in obj.xpath(".//br"):
                if vote.tail:
                    vote = vote.tail.strip()
                    if vote:
                        votes.append(vote)
            return {
                "type": typ,
                "count": count,
                "votes": votes
            }

        vote_dict = {
            "yes": proc_block(yes),
            "no": proc_block(no),
            "other": proc_block(other),
        }

        yes_count = vote_dict['yes']['count']
        no_count = vote_dict['no']['count'] or 0
        other_count = vote_dict['other']['count'] or 0

        vote = Vote(
            actor,
            date,
            motion,
            (yes_count > no_count),
            yes_count,
            no_count,
            other_count,
            _vote_id=uniqid)
        vote.add_source(url)

        for key in vote_dict:
            for voter in vote_dict[key]['votes']:
                getattr(vote, key)(voter)

        bill.add_vote(vote)

    def parse_html_vote(self, bill, actor, date, motion, url, uniqid):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        descr = page.xpath("//b")[0].text_content()

        if "on voice vote" in descr:
            return

        if "committee" in descr.lower():
            return self.scrape_committee_vote(
                bill, actor, date, motion, url, uniqid
            )

        passed = None

        if "Passed" in descr:
            passed = True
        elif "Failed" in descr:
            passed = False
        elif "UTAH STATE LEGISLATURE" in descr:
            return
        else:
            logger.warning(descr)
            raise NotImplemented("Can't see if we passed or failed")

        headings = page.xpath("//b")[1:]
        votes = page.xpath("//table")
        sets = zip(headings, votes)
        vdict = {}
        for (typ, votes) in sets:
            txt = typ.text_content()
            arr = [x.strip() for x in txt.split("-", 1)]
            if len(arr) != 2:
                continue
            v_txt, count = arr
            v_txt = v_txt.strip()
            count = int(count)
            people = [x.text_content().strip() for x in
                      votes.xpath(".//font[@face='Arial']")]

            vdict[v_txt] = {
                "count": count,
                "people": people
            }

        vote = Vote(actor, date,
                    motion,
                    passed,
                    vdict['Yeas']['count'],
                    vdict['Nays']['count'],
                    vdict['Absent or not voting']['count'],
                    _vote_id=uniqid)
        vote.add_source(url)

        for person in vdict['Yeas']['people']:
            vote.yes(person)
        for person in vdict['Nays']['people']:
            vote.no(person)
        for person in vdict['Absent or not voting']['people']:
            vote.other(person)

        logger.info(vote)
        bill.add_vote(vote)


    def parse_vote(self, bill, actor, date, motion, url, uniqid):
        page = self.urlopen(url)
        bill.add_source(url)
        vote_re = re.compile('YEAS -?\s?(\d+)(.*)NAYS -?\s?(\d+)'
                             '(.*)ABSENT( OR NOT VOTING)? -?\s?'
                             '(\d+)(.*)',
                             re.MULTILINE | re.DOTALL)
        match = vote_re.search(page)
        yes_count = int(match.group(1))
        no_count = int(match.group(3))
        other_count = int(match.group(6))

        if yes_count > no_count:
            passed = True
        else:
            passed = False

        if actor == 'upper' or actor == 'lower':
            vote_chamber = actor
            vote_location = ''
        else:
            vote_chamber = ''
            vote_location = actor

        vote = Vote(vote_chamber, date,
                    motion, passed, yes_count, no_count,
                    other_count,
                    location=vote_location,
                    _vote_id=uniqid)
        vote.add_source(url)

        yes_votes = re.split('\s{2,}', match.group(2).strip())
        no_votes = re.split('\s{2,}', match.group(4).strip())
        other_votes = re.split('\s{2,}', match.group(7).strip())

        for yes in yes_votes:
            if yes:
                vote.yes(yes)
        for no in no_votes:
            if no:
                vote.no(no)
        for other in other_votes:
            if other:
                vote.other(other)

        bill.add_vote(vote)

########NEW FILE########
__FILENAME__ = committees
import re

from billy.scrape import NoDataForPeriod
from billy.scrape.committees import CommitteeScraper, Committee

import lxml.html


class UTCommitteeScraper(CommitteeScraper):
    jurisdiction = 'ut'

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page


    def scrape(self, term, chambers):
        self.validate_term(term, latest_only=True)

        url = "http://le.utah.gov/asp/interim/Main.asp?ComType=All&Year=2014&List=2#Results"
        page = self.lxmlize(url)

        for comm_link in page.xpath("//a[contains(@href, 'Com=')]"):
            comm_name = comm_link.text.strip()

            if "House" in comm_name:
                chamber = "lower"
            if "Senate" in comm_name:
                chamber = "upper"
            else:
                chamber = "joint"


            # Drop leading "House" or "Senate" from name
            comm_name = re.sub(r"^(House|Senate) ", "", comm_name)
            comm = Committee(chamber, comm_name)

            committee_page = self.lxmlize(comm_link.attrib['href'])

            for mbr_link in committee_page.xpath(
                    "//table[@class='memberstable']//a"):

                name = mbr_link.text.strip()
                role = mbr_link.tail.strip().strip(",").strip()
                type = "member"
                if role:
                    type = role

                comm.add_member(name, type)

            comm.add_source(url)
            comm.add_source(comm_link.get('href'))

            self.save_committee(comm)

########NEW FILE########
__FILENAME__ = events
import re
import requests
import datetime as dt
import scrapelib

from billy.scrape.events import EventScraper, Event

import pytz
import lxml.etree
import lxml.html

url = 'http://utahlegislature.granicus.com/ViewPublisherRSS.php?view_id=2&mode=agendas'

class UTEventScraper(EventScraper):
    jurisdiction = 'ut'
    _tz = pytz.timezone('US/Mountain')
    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def scrape_page(self, url, session, chamber):
        try:
            try:
                page = self.lxmlize(url)
            except requests.exceptions.Timeout:
                self.warning("Erm, we got a timeout here. Bailing")
                return

        except lxml.etree.XMLSyntaxError:
            self.warning("Ugh. Invalid HTML")
            return  # Ugh, invalid HTML.
        agendas = page.xpath("//td[@class='numberspace']")

        spans = page.xpath("//center/span")
        ctty = None
        date = None
        time = None
        if len(spans) >= 4:
            ctty = spans[0].text_content().strip()
            date = spans[2].text_content().strip()
            time = spans[3].text_content().strip()

        bills = []
        for agenda in agendas:
            number = agenda.text_content()
            string = agenda.getnext().text_content().strip()
            re_bills = re.findall("(S|H)\.?(B|R|M)\. (\d+)", string)
            for bill in re_bills:
                bill_id = '%s%s %s' % bill
                bills.append({
                    'name': bill_id,
                    'desc': string
                })

        if ctty is None or date is None or time is None:
            return

        datetime = "%s %s" % (
            date.strip(),
            time.strip()
        )
        datetime = re.sub("AGENDA", "", datetime).strip()
        datetime = [ x.strip() for x in datetime.split("\r\n") ]

        if "" in datetime:
            datetime.remove("")

        if len(datetime) == 1:
            datetime.append("state house")

        where = datetime[1]
        translate = {
            "a.m.": "AM",
            "p.m.": "PM"
        }
        for t in translate:
            datetime[0] = datetime[0].replace(t, translate[t])
        datetime = dt.datetime.strptime(datetime[0], "%A, %B %d, %Y %I:%M %p")

        chamber = 'other'
        cLow = ctty.lower()
        if "seante" in cLow:
            chamber = 'upper'
        elif "house" in cLow:
            chamber = 'lower'
        elif "joint" in cLow:
            chamber = 'joint'

        event = Event(session, datetime, 'committee:meeting',
                      ctty, location=where)
        event.add_source(url)
        event.add_participant('host', ctty, 'committee', chamber=chamber)
        for bill in bills:
            event.add_related_bill(bill['name'],
                                   description=bill['desc'],
                                   type='consideration')
        self.save_event(event)

    def scrape(self, chamber, session):
        if chamber != 'other':
            return

        page = self.urlopen(url)
        page = lxml.etree.fromstring(page)

        for p in page.xpath("//link"):
            try:
                self.scrape_page(p.text, session, chamber)
            except scrapelib.HTTPError:
                continue

########NEW FILE########
__FILENAME__ = legislators
from billy.scrape.legislators import LegislatorScraper, Legislator

import lxml.html


class UTLegislatorScraper(LegislatorScraper):
    jurisdiction = 'ut'
    latest_only = True

    def scrape(self, chamber, term):
        if chamber == 'lower':
            self.scrape_lower(term)
        else:
            self.scrape_upper(term)


    def scrape_lower(self, term):
        url = 'http://le.utah.gov/house2/representatives.jsp'
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        for row in doc.xpath('//tr')[1:]:
            tds = row.xpath('td')

            district = tds[0].text_content()
            if tds[1].text_content() == 'Empty':
                self.log('district %s is empty' % district)
                continue
            a = tds[1].xpath('a')[0]
            name = a.text_content()
            leg_url = a.get('href')

            party = tds[2].text_content()
            if party == 'D':
                party = 'Democratic'
            elif party == 'R':
                party = 'Republican'
            else:
                raise ValueError('unknown party')

            # get photo
            leg_html = self.urlopen(leg_url)
            leg_doc = lxml.html.fromstring(leg_html)
            leg_doc.make_links_absolute(leg_url)
            photo_url = leg_doc.xpath('//img[@alt="photo"]/@src')[0]
            email = leg_doc.xpath('//a[starts-with(@href, "mailto")]')[0].text

            address = leg_doc.xpath('//b[text()="Address:"]')[0].tail.strip()
            cell = leg_doc.xpath('//b[text()="Cell Phone:"]')
            if cell:
                cell = cell[0].tail.strip()
            else:
                cell = None

            leg = Legislator(term, 'lower', district, name,
                             party=party, photo_url=photo_url, email=email,
                             url=leg_url)
            leg.add_office('district', 'Home', address=address, phone=cell)

            leg.add_source(url)
            leg.add_source(leg_url)
            self.save_legislator(leg)


    def scrape_upper(self, term):
        url = 'http://www.utahsenate.org/aspx/roster.aspx'
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        for row in doc.xpath('//tr')[1:]:
            tds = row.xpath('td')

            # 1st has district
            district = tds[0].text_content()

            # 3rd has name and email
            person = tds[2].xpath('span[@class="person"]')[0]
            if '(D)' in person.text_content():
                party = 'Democratic'
            elif '(R)' in person.text_content():
                party = 'Republican'
            else:
                raise ValueError('unknown party')
            a = person.xpath('a')[0]
            name = a.text_content()
            leg_url = a.get('href')
            email = tds[2].xpath('span[@class="email"]/a/text()')
            if email:
                email = email[0]
            else:
                email = ''

            # office address
            # text is split by br in 4th td, join with a space
            address = ' '.join(tds[3].xpath('font/text()'))
            numbers = tds[4].xpath('text()')
            phone = None
            fax = None
            for num in numbers:
                if num.startswith(('Cell', 'Home', 'Work')) and not phone:
                    phone = num.split(u'\xa0')[-1]
                elif num.startswith('Fax'):
                    fax = num.split(u'\xa0')[-1]
            numbers = [num.split(u'\xa0') for num in numbers]

            # get photo
            try:
                leg_html = self.urlopen(leg_url)
                leg_doc = lxml.html.fromstring(leg_html)
                leg_doc.make_links_absolute(leg_url)
                photo_url = leg_doc.xpath('//p[@class="photo"]/img/@src')[0]
            except:
                self.warning('could not fetch %s' % leg_url)
                photo_url = ''

            leg = Legislator(term, 'upper', district, name,
                             party=party, email=email, address=address,
                             photo_url=photo_url, url=leg_url)
            leg.add_office('district', 'Home', address=address, phone=phone,
                           fax=fax)
            leg.add_source(url)
            leg.add_source(leg_url)
            self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = bills
import re
import datetime
from collections import defaultdict

from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote
import lxml.html


BASE_URL = 'http://lis.virginia.gov'

class VABillScraper(BillScraper):
    jurisdiction = 'va'

    vote_strip_re = re.compile(r'(.+)\((\d{1,2})-Y (\d{1,2})-N\)')
    actor_map = {'House': 'lower', 'Senate': 'upper', 'Governor': 'governor',
                 'Conference': 'conference'}

    _action_classifiers = (
        ('Approved by Governor', 'governor:signed'),
        ('\s*Amendment(s)? .+ agreed', 'amendment:passed'),
        ('\s*Amendment(s)? .+ withdrawn', 'amendment:withdrawn'),
        ('\s*Amendment(s)? .+ rejected', 'amendment:failed'),
        ('Subject matter referred', 'committee:referred'),
        ('Rereferred to', 'committee:referred'),
        ('Referred to', 'committee:referred'),
        ('Assigned ', 'committee:referred'),
        ('Reported from', 'committee:passed'),
        ('Read third time and passed', ['bill:passed', 'bill:reading:3']),
        ('Read third time and agreed', ['bill:passed', 'bill:reading:3']),
        ('Passed (Senate|House)', 'bill:passed'),
        ('Read third time and defeated', 'bill:failed'),
        ('Presented', 'bill:introduced'),
        ('Prefiled and ordered printed', 'bill:introduced'),
        ('Read first time', 'bill:reading:1'),
        ('Read second time', 'bill:reading:2'),
        ('Read third time', 'bill:reading:3'),
        ('Senators: ', None),
        ('Delegates: ', None),
        ('Committee substitute printed', None),
        ('Bill text as passed', None),
        ('Acts of Assembly', None),
    )

    link_xpath = '//ul[@class="linkSect"]/li/a'

    def accept_response(self, response):
        # check for rate limit pages
        normal = super(VABillScraper, self).accept_response(response)
        return (normal and
                'Sorry, your query could not be processed' not in response.text
                and 'the source database is temporarily unavailable' not in response.text)

    def get_page_bills(self, issue_name, href):
        issue_html = self.urlopen('http://lis.virginia.gov' + href,
                                  retry_on_404=True)
        idoc = lxml.html.fromstring(issue_html)
        for ilink in idoc.xpath(self.link_xpath):
            self.subject_map[ilink.text].append(issue_name)

        more_links = idoc.xpath('//a/b[text()="More..."]/../@href')
        if more_links:
            self.get_page_bills(issue_name, more_links[0])

    def build_subject_map(self):
        url = 'http://lis.virginia.gov/cgi-bin/legp604.exe?%s+sbj+SBJ' % self.site_id
        self.subject_map = defaultdict(list)

        # loop over list of all issue pages
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        for link in doc.xpath(self.link_xpath):
            # get bills from page
            self.get_page_bills(link.text, link.get('href'))


    def scrape(self, chamber, session):
        self.user_agent = 'openstates +mozilla'
        # internal id for the session, store on self so all methods have access
        self.site_id = self.metadata['session_details'][session]['site_id']

        self.build_subject_map()

        # used for skipping bills from opposite chamber
        start_letter = 'H' if chamber == 'lower' else 'S'

        url = 'http://lis.virginia.gov/cgi-bin/legp604.exe?%s+lst+ALL' % self.site_id

        while url:
            html = self.urlopen(url, retry_on_404=True)
            doc = lxml.html.fromstring(html)

            url = None  # no more unless we encounter 'More...'

            bills = doc.xpath('//ul[@class="linkSect"]/li')
            for bill in bills:
                link = bill.getchildren()[0]
                bill_id = str(link.text_content())

                # check if this is the 'More...' link
                if bill_id.startswith('More'):
                    url = BASE_URL + link.get('href')

                # skip bills from the other chamber
                elif not bill_id.startswith(start_letter):
                    continue

                else:
                    # create a bill
                    desc = bill.xpath('text()')[0].strip()
                    bill_type = {'B': 'bill',
                                 'J': 'joint resolution',
                                 'R': 'resolution'}[bill_id[1]]
                    bill = Bill(session, chamber, bill_id, desc,
                                type=bill_type)

                    bill_url = BASE_URL + link.get('href')
                    self.fetch_sponsors(bill)
                    self.scrape_bill_details(bill_url, bill)
                    bill['subjects'] = self.subject_map[bill_id]
                    bill.add_source(bill_url)
                    self.save_bill(bill)


    def scrape_bill_details(self, url, bill):
        html = self.urlopen(url, retry_on_404=True)
        doc = lxml.html.fromstring(html)

        # summary sections
        summary = doc.xpath('//h4[starts-with(text(), "SUMMARY")]/following-sibling::p/text()')
        if summary and summary[0].strip():
            bill['summary'] = summary[0].strip()

        # versions
        for va in doc.xpath('//h4[text()="FULL TEXT"]/following-sibling::ul[1]/li/a[1]'):

            # 11/16/09 \xa0House: Prefiled and ordered printed; offered 01/13/10 10100110D
            date, desc = va.text.split(u' \xa0')
            desc.rsplit(' ', 1)[0]              # chop off last part
            link = va.get('href')
            date = datetime.datetime.strptime(date, '%m/%d/%y')

            # budget bills in VA are searchable but no full text available
            if '+men+' in link:
                self.warning('not adding budget version, bill text not available')
            else:
                # VA duplicates reprinted bills, lets keep the original name
                bill.add_version(desc, BASE_URL+link, date=date,
                                 mimetype='text/html',
                                 on_duplicate='use_old')

        # actions
        for ali in doc.xpath('//h4[text()="HISTORY"]/following-sibling::ul[1]/li'):
            date, action = ali.text_content().split(u' \xa0')
            actor, action = action.split(': ', 1)

            actor = self.actor_map[actor]
            date = datetime.datetime.strptime(date.strip(), '%m/%d/%y')

            # if action ends in (##-Y ##-N) remove that part
            vrematch = self.vote_strip_re.match(action)
            if vrematch:
                action, y, n = vrematch.groups()
                vote = Vote(actor, date, action, int(y) > int(n),
                            int(y), int(n), 0)
                vote_url = ali.xpath('a/@href')
                if vote_url:
                    self.parse_vote(vote, vote_url[0])
                    vote.add_source(BASE_URL + vote_url[0])
                # set other count, it isn't provided
                vote['other_count'] = len(vote['other_votes'])
                #vote.validate()
                bill.add_vote(vote)


            # categorize actions
            for pattern, atype in self._action_classifiers:
                if re.match(pattern, action):
                    break
            else:
                atype = 'other'

            # if matched a 'None' atype, don't add the action
            if atype:
                bill.add_action(actor, action, date, type=atype)


    def fetch_sponsors(self, bill):
        url = "http://lis.virginia.gov/cgi-bin/legp604.exe?%s+mbr+%s" % (
            self.site_id, bill['bill_id'].replace(' ', ''))

        # order of chamber uls
        #if bill['chamber'] == 'lower':
        #    order = ['lower', 'upper']
        #else:
        #    order = ['upper', 'lower']

        html = self.urlopen(url, retry_on_404=True)
        doc = lxml.html.fromstring(html)

        for slist in doc.xpath('//ul[@class="linkSect"]'):
            # note that first ul is origin chamber
            for sponsor in slist.xpath('li'):
                name = sponsor.text_content().strip()
                if name.endswith(u' (chief\xa0patron)'):
                    name = name[:-15]
                    type = 'primary'
                elif name.endswith(u' (chief\xa0co-patron)'):
                    name = name[:-18]
                    type = 'cosponsor'
                else:
                    type = 'cosponsor'
                bill.add_sponsor(type, name)

    def split_vote(self, block):
        if block:
            block = block[0].text.replace('\r\n', ' ')

            pieces = block.split('--')
            # if there are only two pieces, there are no abstentions
            if len(pieces) <= 2:
                return []
            else:
                # lookahead and don't split if comma precedes initials
                # Also, Bell appears as Bell, Richard B. and Bell, Robert P.
                # and so needs the lookbehind assertion.
                return [x.strip() for x in re.split('(?<!Bell), (?!\w\.\w?\.?)', pieces[1]) if x.strip()]
        else:
            return []

    def parse_vote(self, vote, url):
        url = BASE_URL + url

        html = self.urlopen(url, retry_on_404=True)
        doc = lxml.html.fromstring(html)

        yeas = doc.xpath('//p[contains(text(), "YEAS--")]')
        nays = doc.xpath('//p[contains(text(), "NAYS--")]')
        absts = doc.xpath('//p[contains(text(), "ABSTENTIONS")]')
        #no_votes = doc.xpath('//p[contains(text(), "NOT VOTING")]')[0].text

        map(vote.yes, self.split_vote(yeas))
        map(vote.no, self.split_vote(nays))
        map(vote.other, self.split_vote(absts))
        # don't count not voting as anything?
        #map(vote.other, self.split_vote(no_votes))

########NEW FILE########
__FILENAME__ = legislators
from billy.scrape.legislators import Legislator, LegislatorScraper

import re
import lxml.html

class VALegislatorScraper(LegislatorScraper):
    jurisdiction = 'va'

    def scrape(self, chamber, term):
        abbr = {'upper': 'S', 'lower': 'H'}

        sessions = []
        for t in self.metadata['terms']:
            if t['name'] == term:
                session = t['sessions'][-1]

        self.scrape_for_session(chamber, session, term)

    def scrape_for_session(self, chamber, session, term):
        site_id = self.metadata['session_details'][session]['site_id']
        url = 'http://leg6.state.va.us/%s/mbr/MBR.HTM' % site_id

        if chamber == 'lower':
            column = 'lColLt'
        elif chamber == 'upper':
            column = 'lColRt'

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        for link in doc.xpath('//div[@class="%s"]/ul/li/a' % column):
            if 'resigned' in link.text:
                self.log('skipping %s' % link.text)
                continue
            self.fetch_member(link.get('href'), link.text, term, chamber)

    def fetch_member(self, url, name, term, chamber):
        party_map = {'R': 'Republican', 'D': 'Democratic', 'I': 'Independent'}
        party_district_re = re.compile(
            r'\((R|D|I)\) - (?:House|Senate) District\s+(\d+)')

        # handle resignations, special elections
        match = re.search(r'-(Resigned|Member) (\d{1,2}/\d{1,2})?', name)
        if match:
            action, date = match.groups()
            name = name.rsplit('-')[0]
            if action == 'Resigned':
                pass # TODO: set end date
            elif action == 'Member':
                pass # TODO: set start date

        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        party_district_line = doc.xpath('//h3/font/text()')[0]
        party, district = party_district_re.match(party_district_line).groups()

        leg = Legislator(term, chamber, district, name.strip(),
                         party=party_map[party], url=url)
        leg.add_source(url)

        for ul in doc.xpath('//ul[@class="linkNon"]'):
            address = []
            phone = None
            email = None
            for li in ul.getchildren():
                text = li.text_content()
                if re.match('\(\d{3}\)', text):
                    phone = text
                elif text.startswith('email:'):
                    email = text.strip('email: ').strip()
                else:
                    address.append(text)
                type = ('capitol' if 'Capitol Square' in address
                        else 'district')
                name = ('Capitol Office' if type == 'capitol'
                        else 'District Office')
            leg.add_office(type, name, address='\n'.join(address),
                           phone=phone, email=email)

        for com in doc.xpath('//ul[@class="linkSect"][1]/li/a/text()'):
            leg.add_role('committee member', term=term, chamber=chamber,
                         committee=com)

        self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = actions
import re
from functools import partial
from collections import namedtuple, defaultdict
from types import MethodType


class Rule(namedtuple('Rule', 'regexes types stop attrs')):
    '''If anyh of ``regexes`` matches the action text, the resulting
    action's types should include ``types``.

    If stop is true, no other rules should be tested after this one;
    in other words, this rule conclusively determines the action's
    types and attrs.

    The resulting action should contain ``attrs``, which basically
    enables overwriting certain attributes, like the chamber if
    the action was listed in the wrong column.
    '''
    def __new__(_cls, regexes, types=None, stop=False, **kwargs):
        'Create new instance of Rule(regex, types, attrs, stop)'

        # Regexes can be a string or a sequence.
        if isinstance(regexes, basestring):
            regexes = set([regexes])
        regexes = set(regexes or [])

        # Types can be a string or a sequence.
        if isinstance(types, basestring):
            types = set([types])
        types = set(types or [])

        return tuple.__new__(_cls, (regexes, types, stop, kwargs))


class BaseCategorizer(object):
    '''A class that exposes a main categorizer function
    and before and after hooks, in case a state requires specific
    steps that make use of action or category info. The return
    value is a 2-tuple of category types and a dictionary of
    attributes to overwrite on the target action object.
    '''
    rules = []

    def __init__(self):
        before_funcs = []
        after_funcs = []
        for name in dir(self):
            attr = getattr(self, name)
            if isinstance(attr, MethodType):
                # func = partial(attr, self)
                func = attr
                if getattr(attr, 'before', None):
                    before_funcs.append(func)
                if getattr(attr, 'after', None):
                    after_funcs.append(func)
        self._before_funcs = before_funcs
        self._after_funcs = after_funcs

    def categorize(self, text):

        whitespace = partial(re.sub, '\s{1,4}', '\s{,4}')

        # Run the before hook.
        text = self.before_categorize(text)
        for func in self._before_funcs:
            text = func(text)

        types = set()
        attrs = defaultdict(set)
        for rule in self.rules:

            for regex in rule.regexes:

                # Try to match the regex.
                m = re.search(whitespace(regex), text)
                if m or (regex in text):
                    # If so, apply its associated types to this action.
                    types |= rule.types

                    # Also add its specified attrs.
                    for k, v in m.groupdict().items():
                        attrs[k].add(v)

                    for k, v in rule.attrs.items():
                        attrs[k].add(v)

                    # Break if the rule says so, otherwise
                    # continue testing against other rules.
                    if rule.stop is True:
                        break

        # Returns types, attrs
        return_val = (list(types), attrs)
        return_val = self.after_categorize(return_val)
        for func in self._after_funcs:
            return_val = func(*return_val)
        return self.finalize(return_val)

    def before_categorize(self, text):
        '''A precategorization hook. Takes/returns text.
        '''
        return text

    def after_categorize(self, return_val):
        '''A post-categorization hook. Takes, returns
        a tuple like (types, attrs), where types is a sequence
        of categories (e.g., bill:passed), and attrs is a
        dictionary of addition attributes that can be used to
        augment the action (or whatever).
        '''
        return return_val

    def finalize(self, return_val):
        '''Before the types and attrs get passed to the
        importer they need to be altered by converting lists to
        sets, etc.
        '''
        types, attrs = return_val
        _attrs = {}

        # Get rid of defaultdict.
        for k, v in attrs.items():

            # Skip empties.
            if not v:
                continue
            else:
                v = filter(None, v)

            # Get rid of sets.
            if isinstance(v, set):
                v = list(v)

            # Some vals should be strings, not seqs.
            if k == 'actor' and len(v) == 1:
                v = v.pop()

            _attrs[k] = v

        return types, _attrs


def after_categorize(f):
    '''A decorator to mark a function to be run
    before categorization has happened.
    '''
    f.after = True
    return f


def before_categorize(f):
    '''A decorator to mark a function to be run
    before categorization has happened.
    '''
    f.before = True
    return f


# These are regex patterns that map to action categories.
_categorizer_rules = (

    # Capture vote tallies.
    Rule(r'-- Needed (?P<vote_threshold>\d+) of \d+ to '
         r'Pass -- Yeas = (?P<yes_votes>\d+), Nays = (?P<no_votes>\d+)'),

    # Misc. capturing rules.
    Rule([r'(?i)motion by (?P<legislators>.+?) to',
          r'(?P<legislators>.+) moved to',
          r'(?P<legislators>.+) spoke for',
          (r'Amendment as offered by (?P<legislators>.+?) '
           r'of (.+?) ((dis)?agreed to|withdrawn)'),
          r'Amendment of (?P<committees>Committee on.+?) agreed to'
          r'Rep. (?P<legislators>.+?) of',
          r'requested by (?P<legislators>.+), Passed',
          r'(?i)floor amendment by (?P<legislators>.+?)( of .+?) (dis)?agreed to',
          r'(?P<legislators>.+?) explained vote',
          r'(?P<committees>Committee on .+?) relieved',
          r'(?P<legislators>.+?) explained vote',
          r'(?i)as moved by (?P<legislators>.+?) of .+',
          (r'(?i)Proposal of amendment\s+by '
           r'Senator\(s\)(?P<legislators>.+?); text'),
          r'submitted by (Rep\.|Senator) (?P<legislators>.+?) for',
          r'on motion of (Rep\.|Senator) (?P<legislators>.+)',
          r'submitted by (Rep\.|Senator) (?P<legislators>.+) for',
          r'Senator(\(s\))? (?P<legislators>.+)',
          r'Remarks of Senator (?P<legislators>[A-Z].+?) journalized',
          r'Senator\(s\) (?P<legislators>.+?) (motion|on|divided)',
          (r'by Senator(\(s\))? (?P<legislators>.+?),? '
            '(&|Passed|on|divided|;|to|Failed|dis)'),
          (r'(by|of) Senator(\(s\))? (?P<legislators>.+?) '
            '(sustained|overruled|journalized)'),
          r'^Senator(\(s\))? (?P<legislators>.+?) [a-z]']),

    # Readings.
    Rule([r'Read First time',
          r'Read 1st time'], 'bill:reading:1'),

    Rule([r'(?i)read 2nd time',
          r'(?i)read second time',
          r'Second Reading'], 'bill:reading:2'),

    Rule(r'(?i)read (third|3rd) time (and|&) passed',
         ['bill:passed', 'bill:reading:3']),

    # Resolutions.
    Rule([r'^(Read (and|&) )?Adopted',
          r'Read 3rd time & adopted',
          r'^Passed on roll call',
          r'^(?i)roll call results passed',
          r'^(?i)roll call.+?passed',
          r'^Passed',
          r'^(?i)(passed|adopted) in concurrence'], 'bill:passed'),

    # Committees.
    Rule([r'(?i)referred to (?P<committees>.+)',
          r'with the report of (?P<committees>Committee on.+?)\s+intact',
          (r'bill (re)?committed to (?P<committees>Committee on.+?)'
            '\b(with|on)\b'),
          r'Committed to( the)? (?P<committees>Committee .+?) (by|with|on)'],
         'committee:referred'),

    Rule([(r'Reported favorably by Senator (?P<legislators>\S+) '
            'for (?P<committees>Committee on .+?), read 2nd time '
            'and 3rd reading ordered'),
          (r'(?i)favorable report( with recommendation of amendment)? '
           r'by (?P<committees>.+)'),
          (r'(?i)Favorable report with proposal of amendment by '
           r'(?P<committees>Committee on .+)')],
         ('committee:passed:favorable', 'bill:reading:2')),

    Rule((r'Reported favorably by Senator (?P<legislators>.+?) '
          r'for (?P<committees>.+?) with recommendation of amendment'),
         'committee:passed:favorable'),

    Rule([(r'(?i)reported without recommendation by( (?P<legislators>'
            'Senator.+?) for)? (?P<committees>.+)'),
           r'proposal of amendment concurred in',
          (r'(?i)proposal of amendment\s+by (?P<committees>Committee '
            'on .+?) agreed to')],
         'committee:passed'),

    Rule(r'Reported favorably by Senator (?P<legislators>.+?) '
         r'for (?P<committees>.+?)', 'committee:passed:favorable'),

    # Amendments
    Rule([r'floor amendment by (.*) agreed to',
          r'motion to amend bill agreed to',
          r'Proposal of amendment agreed to'
          r'Amendment as offered by Rep\.(?P<legislators>.+?) of .+? agreed',
          r'bill amended as moved by Senator\(s\) (?P<legislators>.+)',
          r'Floor Amendment by Rep\. (?P<legislators>.+?) agreed to',
          (r'Recommendation of amendment by (?P<committees>Committee.+?)'
            '(, as amended,)? agreed'),
          (r'Recommendation of amendment by Senator(\(s\))?(?P<legislators>.+?)'
            ' on behalf of (?P<committees>Committee.+) agreed')],
         'amendment:passed'),

    Rule(['Proposal of amendment disagreed to',
          'Motion to amend disagreed to',
          (r'Amendment as offered by Rep\.(?P<legislators>.+?) '
            'of .+? disagreed'),
          r'Floor Amendment by Rep\. (?P<legislators>.+?) disagreed to'],
          'amendment:failed'),

    Rule((r'Amendment as offered by Rep\.(?P<legislators>.+?) '
           'of .+? withdrawn'), 'amendment:withdrawn'),


    # Governor.
    Rule(r'Signed by Governor on .+', 'governor:signed'),
    Rule([r'(?i)Governor vetoed', '(?i)Vetoed by governor'],
         'governor:vetoed', actor='executive'),
    Rule(r'Delivered to the Governor', 'governor:received'),

    )


class Categorizer(BaseCategorizer):
    rules = _categorizer_rules

    @after_categorize
    def split_legislators(self, types, attrs):
        if 'legislators' in attrs:

            legs = []

            for text in attrs['legislators']:
                if text is None:
                    continue
                text = re.sub(r'; text$', '', text)

                # Split.
                _legs = re.split('(?:, |,? and)', text)
                legs.extend(_legs)
            attrs['legislators'] = legs

        return types, attrs

########NEW FILE########
__FILENAME__ = bills
import re
import datetime

from billy.scrape import NoDataForPeriod, ScrapeError
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

import lxml.html

from .utils import DOUBLED_NAMES
from .actions import Categorizer


def parse_exec_date(date_str):
    """
    Parse dates for executive actions.
    """
    match = re.search(r'((\w+) (\d{1,2}),\s?(\d{4,4}))', date_str)
    if match:
        date_str = "%s %s, %s" % (match.group(2), match.group(3), match.group(4))
        return datetime.datetime.strptime(date_str, "%B %d, %Y")

    match = re.search(r'((\w+), (\d{1,2}),\s?(\d{4,4}))', date_str)
    if match:
        date_str = "%s, %s, %s" % (match.group(2), match.group(3), match.group(4))
        return datetime.datetime.strptime(date_str, "%B, %d, %Y")

    match = re.search(r'(\d{1,2}/\d{1,2}/\d{4,4})', date_str)
    if match:
        return datetime.datetime.strptime(match.group(1), "%m/%d/%Y")

    match = re.search(r'(\d{1,2}/\d{1,2}/\d{2,2})', date_str)
    if match:
        return datetime.datetime.strptime(match.group(1), "%m/%d/%y")

    raise ScrapeError("Invalid executive action date: %s" % date_str)


def clean_action(action):
    action = action.strip()

    # collapse multiple whitespace
    action = ' '.join([w for w in action.split() if w])

    # floating punctuation
    action = re.sub(r'\s([,.;:])(\s)', r'\1\2', action)

    return action


class VTBillScraper(BillScraper):
    jurisdiction = 'vt'
    categorizer = Categorizer()

    def scrape(self, chamber, session, only_bills=None):
        if chamber == 'lower':
            bill_abbr = "H."
        else:
            bill_abbr = "S."


        urls = [
            "http://www.leg.state.vt.us/docs/bills.cfm?Session=%s&Body=%s",
            "http://www.leg.state.vt.us/docs/resolutn.cfm?Session=%s&Body=%s"
        ]

        bill_ids = []

        for url in urls:
            url = url % (session.split('-')[1], bill_abbr[0])
            page = self.urlopen(url)
            page = lxml.html.fromstring(page)
            page.make_links_absolute(url)

            for link in page.xpath("//a[contains(@href, 'summary.cfm')]"):
                bill_id = link.text
                if only_bills is not None and bill_id not in only_bills:
                    self.log("Skipping bill we are not interested in %s" % bill_id)
                    continue

                if bill_id.startswith('JR'):
                    bill_type = 'joint resolution'
                elif bill_id[1:3] == 'CR':
                    bill_type = 'concurrent resolution'
                elif bill_id[0:2] in ['HR', 'SR']:
                    bill_type = 'resolution'
                else:
                    bill_type = 'bill'

                title = link.xpath("string(../../td[2])")

                bill = Bill(session, chamber, bill_id, title,
                            type=bill_type)
                if self.scrape_bill(bill, link.attrib['href']):
                    bill_ids.append(bill_id)
        return bill_ids

    def scrape_bill(self, bill, url):
        page = self.urlopen(url)
        page.replace('&nbsp;', ' ')
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        bill.add_source(url)

        for link in page.xpath("//b[text()='Bill Text:']/"
                               "following-sibling::blockquote[1]//a"):
            if link.attrib['href'].endswith('pdf'):
                mimetype = 'application/pdf'
            elif link.attrib['href'].endswith('htm'):
                mimetype = 'text/html'
            bill.add_version(link.text, link.attrib['href'], mimetype=mimetype)

        more_sponsor_link = page.xpath("//a[text()='More Sponsors']")
        if page.xpath("//a[text()='More Sponsors']"):
            sponsor_url = more_sponsor_link[0].attrib['href']
            self.scrape_sponsors(bill, sponsor_url)
        else:
            for b in page.xpath("//td[text()='Sponsor(s):']/../td[2]/b"):
                bill.add_sponsor("primary", b.text)

        for tr in page.xpath("""
        //b[text()='Detailed Status:']/
        following-sibling::blockquote[1]/table/tr""")[1:]:
            action = tr.xpath("string(td[3])").strip()

            match = re.search('(to|by) Governor on (.*)', action)
            if match:
                date = parse_exec_date(match.group(2).strip()).date()
                actor = 'executive'
            else:
                if tr.attrib['bgcolor'] == 'Salmon':
                    actor = 'lower'
                elif tr.attrib['bgcolor'] == 'LightGreen':
                    actor = 'upper'
                else:
                    raise ScrapeError("Invalid row color: %s" %
                                      tr.attrib['bgcolor'])

                date = tr.xpath("string(td[1])")
                try:
                    date = re.search(
                        r"\d\d?/\d\d?/\d{4,4}", date).group(0)
                except AttributeError:
                    # No date, skip
                    self.warning("skipping action '%s -- %s'" % (
                        date, action))
                    continue

                date = datetime.datetime.strptime(date, "%m/%d/%Y")
                date = date.date()

            types, attrs = self.categorizer.categorize(action)
            action = dict(actor=actor, action=action,
                          date=date, type=types)
            action.update(**attrs)
            bill.add_action(**action)

            for vote_link in tr.xpath("td[4]/a"):
                self.scrape_vote(bill, actor, vote_link.attrib['href'])

        # If nearly all of the bill attributes but the title are blank, this is a bad bill.
        # See Issue #166.
        if all(len(bill[x]) == 0 for x in ('votes', 'alternate_titles', 'sponsors',
            'actions', 'versions', 'documents')):
            return False

        # Get subjects.
        subjects = []
        for subject in page.xpath('//font')[-1].text_content().splitlines():
            subject = subject.strip()
            if subject:
                subjects.append(subject)
        bill['subjects'] = subjects

        self.save_bill(bill)
        return True

    def scrape_sponsors(self, bill, url):
        bill.add_source(url)

        page = self.urlopen(url)
        page = lxml.html.fromstring(page)

        for td in page.xpath("//h3/following-sibling::"
                             "blockquote/table/tr/td"):
            name = td.xpath("string()").strip()
            if name:
                bill.add_sponsor("primary", name)

    def scrape_vote(self, bill, chamber, url):
        page = self.urlopen(url)
        if 'There are no details available for this roll call' in page:
            return
        page = page.replace('&nbsp;', ' ')
        page = lxml.html.fromstring(page)

        info_row = page.xpath("//table[1]/tr[2]")[0]

        date = info_row.xpath("string(td[1])")
        date = datetime.datetime.strptime(date, "%m/%d/%Y")

        motion = info_row.xpath("string(td[2])")
        yes_count = int(info_row.xpath("string(td[3])"))
        no_count = int(info_row.xpath("string(td[4])"))
        other_count = int(info_row.xpath("string(td[5])"))
        passed = info_row.xpath("string(td[6])") == 'Pass'

        if motion == 'Shall the bill pass?':
            type = 'passage'
        elif motion == 'Shall the bill be read the third time?':
            type = 'reading:3'
        elif 'be amended as' in motion:
            type = 'amendment'
        else:
            type = 'other'

        vote = Vote(chamber, date, motion, passed,
                    yes_count, no_count, other_count)
        vote.add_source(url)

        for tr in page.xpath("//table[1]/tr")[3:]:
            if len(tr.xpath("td")) != 2:
                continue


            # avoid splitting duplicate names
            name = tr.xpath("string(td[1])").strip()
            if not name.startswith(DOUBLED_NAMES):
                name = name.split(' of')[0]

            type = tr.xpath("string(td[2])").strip()
            if type.startswith('Yea'):
                vote.yes(name)
            elif type.startswith('Nay'):
                vote.no(name)
            elif type.startswith('Not Voting'):
                pass
            else:
                vote.other(name)

        bill.add_vote(vote)

########NEW FILE########
__FILENAME__ = committees
import re

from billy.scrape.committees import CommitteeScraper, Committee

import lxml.html

from .utils import DOUBLED_NAMES


class VTCommitteeScraper(CommitteeScraper):
    jurisdiction = 'vt'

    def scrape(self, chamber, term):
        self.validate_term(term, latest_only=True)

        chamber_abbr = {'upper': 'S', 'lower': 'H'}[chamber]

        url = ('http://www.leg.state.vt.us/legdir/comms.cfm?Body=%s&Session=2014' %
               chamber_abbr)
        html = self.urlopen(url)
        page = lxml.html.fromstring(html)
        comm = None
        for tr in page.xpath('//table')[3].xpath('tr')[1:]:

            tds = list(tr)
            if len(tds) < 2:
                continue

            if 'COMMITTEE' in tds[1].text_content():
                # Save the current committee.
                if (comm or {}).get('members', []):
                    self.save_committee(comm)
                # Start a new one.
                comm = self.parse_committee_name(tds[1], chamber)
                comm.add_source(url)
                continue

            name = tds[1].text_content().strip()
            match = re.search(
                '^([\w\s\.]+),\s+'
                '(Chair|Vice Chair|Vice-Chair|Ranking Member|Clerk)$',
                name)
            if match:
                name = match.group(1)
                mtype = match.group(2).lower()
            else:
                mtype = 'member'

            # if not name.startswith(DOUBLED_NAMES):
            name = re.sub(r'of [\w\s\.]+$', '', name)
            comm.add_member(name, mtype)

        # And save the last committee.
        self.save_committee(comm)


    def parse_committee_name(self, td, chamber):
        # Strip the room number from the committee name
        comm_name = re.match(r'[^\(]+', td.text_content()).group(0).strip()

        # Strip chamber from beginning of committee name
        comm_name = re.sub(r'^(HOUSE|SENATE) COMMITTEE ON ', '',
                           comm_name)
        # normalize case of committee name
        comm_name = comm_name.title()

        return Committee(chamber, comm_name)

########NEW FILE########
__FILENAME__ = events
import datetime as dt

from billy.scrape.events import Event, EventScraper

import pytz
import lxml.html

class VTEventScraper(EventScraper):
    jurisdiction = 'vt'

    _tz = pytz.timezone('US/Eastern')

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def scrape(self, chamber, session):
        if chamber != "other":
            return
        url = "http://www.leg.state.vt.us/HighlightsMain.cfm"
        page = self.lxmlize(url)
        ps = page.xpath(
            "//p[@class='HighlightsNote' or @class='HighlightsDate']")
        events = {}
        event_set = []
        for p in ps:
            if p.attrib['class'] == "HighlightsNote":
                event_set.append(p)
            else:
                date_time = p.text[len("Posted "):]
                events[date_time] = event_set
                event_set = []
        for date in events:
            date_time = dt.datetime.strptime(date, "%m/%d/%Y")
            for event in events[date]:
                descr = event.text_content()
                e = Event(session, date_time, "other", descr,
                          location="state house")
                e.add_source(url)
                self.save_event(e)

########NEW FILE########
__FILENAME__ = legislators
import re

from billy.scrape import NoDataForPeriod
from billy.scrape.legislators import LegislatorScraper, Legislator

import lxml.html


class VTLegislatorScraper(LegislatorScraper):
    jurisdiction = 'vt'
    latest_only = True

    def scrape(self, chamber, term):
        # What Vermont claims are Word and Excel files are actually
        # just HTML tables
        # What Vermont claims is a CSV file is actually one row of comma
        # separated values followed by a ColdFusion error.
        url = ("http://www.leg.state.vt.us/legdir/"
               "memberdata.cfm/memberdata.doc?FileType=W")

        page = self.urlopen(url)
        page = lxml.html.fromstring(page)

        for tr in page.xpath("//tr")[1:]:
            row_chamber = tr.xpath("string(td[4])")
            if row_chamber == 'S' and chamber == 'lower':
                continue
            elif row_chamber == 'H' and chamber == 'upper':
                continue

            district = tr.xpath("string(td[7])")
            district = district.replace('District', '').strip()
            if not district:
                continue

            first_name = tr.xpath("string(td[8])")
            middle_name = tr.xpath("string(td[9])")
            last_name = tr.xpath("string(td[10])")

            if first_name.endswith(" %s." % middle_name):
                first_name = first_name.split(" %s." % middle_name)[0]

            if middle_name:
                full_name = "%s %s. %s" % (first_name, middle_name,
                                          last_name)
            else:
                full_name = "%s %s" % (first_name, last_name)

            email = tr.xpath("string(td[11])")

            party = tr.xpath("string(td[6])")
            party = re.sub(r'Democrat\b', 'Democratic', party)
            parties = party.split('/')
            if 'Republican' in parties and 'Democratic' in parties:
                pass
            else:
                party = parties.pop(0)

            leg = Legislator(term, chamber, district, full_name,
                             first_name=first_name,
                             middle_name=middle_name,
                             last_name=last_name,
                             party=party,
                             email=email,
            # closest thing we have to a page for legislators, not ideal
            url='http://www.leg.state.vt.us/legdir/LegDirMain.cfm'
                            )
            leg['roles'][0]['other_parties'] = parties
            leg.add_source(url)

            # 12-16: MailingAddress: 1,2,City,State,ZIP
            mail = '%s\n%s\n%s, %s %s' % (tr.xpath('string(td[12])'),
                                          tr.xpath('string(td[13])'),
                                          tr.xpath('string(td[14])'),
                                          tr.xpath('string(td[15])'),
                                          tr.xpath('string(td[16])'))
            leg.add_office('district', 'Mailing Address', address=mail)
            # 17-21: HomeAddress: 1,2,City,State,ZIP, Email, Phone
            home = '%s\n%s\n%s, %s %s' % (tr.xpath('string(td[17])'),
                                          tr.xpath('string(td[18])'),
                                          tr.xpath('string(td[19])'),
                                          tr.xpath('string(td[20])'),
                                          tr.xpath('string(td[21])'))
            home_email = tr.xpath('string(td[22])') or None
            home_phone = tr.xpath('string(td[23])') or None
            leg.add_office('district', 'Home Address', address=home,
                            email=home_email, phone=home_phone)

            self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = utils
DOUBLED_NAMES = ('Smith', 'Johnson', 'Stevens', 'French', 'Lewis', 'Martin')

########NEW FILE########
__FILENAME__ = actions
import re
from billy.scrape.actions import Rule, BaseCategorizer


# http://www.leg.wa.gov/legislature/pages/committeelisting.aspx#
committees_abbrs = {
    u'AGNR': u'Agriculture & Natural Resources',
    # u'APPE': '',
    # u'APPG': '',
    # u'APPH':
    # u'ARED': '',
    u'AWRD': u'Agriculture, Water & Rural Economic Development',
    u'BFS': u'Business & Financial Services',  # u'Early Learning & K-12 Education',
    u'CB': u'Capital Budget',
    u'CDH': u'Community & Economic Development & Housing',
    u'ED': u'Education',  # u'Education Appropriations & Oversight',
    u'EDTI': u'Economic Development, Trade & Innovation',
    u'EDU': u'Education',
    u'ELHS': u'Early Learning & Human Services',  # u'General Government Appropriations & Oversight',
    u'ENRM': u'Energy, Natural Resources & Marine Waters',
    u'ENV': u'Environment',
    u'ENVI': u'Environment',
    u'EWE': u'Health & Human Services Appropriations & Oversight',
    u'FIHI': u'Financial Institutions, Housing & Insurance',  # u'Health & Long-Term Care',
    u'GO': u'Government Operations, Tribal Relations & Elections',
    u'HCW': u'Health Care & Wellness',
    u'HE': u'Higher Education',
    u'HEA': 'Homeowners\' Association Act',
    u'HEWD': u'Higher Education & Workforce Development',
    u'HSC': u'Human Services & Corrections',
    u'JUD': u'Judiciary',
    u'JUDI': u'Judiciary',
    u'LCCP': u'Labor, Commerce & Consumer Protection',
    u'LG': u'Local Government',
    u'LWD': u'Labor & Workforce Development',
    # u'NRMW': '',
    u'PSEP': u'Public Safety & Emergency Preparedness',
    u'SGTA': u'State Government & Tribal Affairs',
    u'TEC': u'Technology, Energy & Communications',
    u'TR': u'Transportation',
    u'TRAN': u'Transportation',
    u'WAYS': u'Ways & Means'
    }
committee_names = committees_abbrs.values()
committees_rgx = '(%s)' % '|'.join(
    sorted(committee_names, key=len, reverse=True))

# These are regex patterns that map to action categories.
_categorizer_rules = (

    Rule(r'yeas, (?P<yes_votes>\d+); nays, (?P<no_votes>\d+); '
         r'absent, (?P<absent_voters>\d+); excused, (?P<excused_voters>\d+)'),
    Rule(r'Committee on (?P<committees>.+?) at \d'),
    Rule(r'(?P<committees>.+?) relieved of further'),
    Rule(r'Passed to (?P<committees>.+?) for \S+ reading'),
    Rule(r'by (?P<committees>.+?) Committee'),

    Rule(r'^Adopted', 'bill:passed'),
    Rule(r'^Introduced', 'bill:introduced'),
    Rule(r'^Introduced', 'bill:introduced'),
    Rule(r'Third reading, adopted', ['bill:reading:3', 'bill:passed']),

    Rule(r'amendment adopted', 'amendment:passed'),
    Rule(r'amendment not adopted', 'amendment:failed'),
    Rule(r"(?i)third reading, (?P<pass_fail>(passed|failed))", 'bill:reading:3'),
    Rule(r'Read first time', 'bill:reading:1'),
    Rule(r"(?i)first reading, referred to (?P<committees>.*)\.", 'bill:reading:1'),
    Rule(r"(?i)And refer to (?P<committees>.*)", 'committee:referred'),
    Rule(r"(?i).* substitute bill substituted.*", 'bill:substituted'),
    Rule(r"(?i)chapter (((\d+),?)+) \d+ laws.( .+)?", "other"),  # XXX: Thom: Code stuff?
    Rule(r"(?i)effective date \d{1,2}/\d{1,2}/\d{4}.*", "other"),
    Rule(r"(?i)(?P<committees>\w+) - majority; do pass with amendment\(s\) (but without amendments\(s\))?.*\.", "committee:passed:favorable", "committee:passed"),
    Rule(r"(?i)Executive action taken in the (House|Senate) committee on (?P<committees>.*) (at)? .*\.", "other"),
    Rule(r"(?i)(?P<committees>\w+) \- Majority; do pass .* \(Majority Report\)", 'bill:passed'),
    Rule(r"(?i)Conference committee appointed.", "other"),
    Rule(r"(?i)Conference committee report;", 'other'),
    Rule(r"(?i).+ - Majority; \d+.+ substitute bill be substituted, do pass", 'bill:passed'),
    Rule(r"(?i)Signed by (?P<signed_chamber>(Representatives|Senators)) (?P<legislators>.*)", "bill:passed"),
    Rule(r"(?i)Referred to (?P<committees>.*)(\.)?"),
    Rule(r"(?i)(?P<from_committee>.*) relieved of further consideration. On motion, referred to (?P<committees>.*)", 'committee:referred'),
    Rule(r"(?i)Governor partially vetoed", 'governor:vetoed:line-item'),
    Rule(r"(?i)Governor vetoed", 'governor:vetoed'),
    Rule(r"(?i)Governor signed", 'governor:signed'),
    Rule(r"(?i)Passed final passage;", 'bill:passed'),
    Rule(r"(?i)Failed final passage;", 'bill:failed'),
#    Rule(r"(?i)"),
#    Rule(r"(?i)"),
    )


class Categorizer(BaseCategorizer):
    rules = _categorizer_rules

    def categorize(self, text):
        '''Wrap categorize and add boilerplate committees.
        '''
        attrs = BaseCategorizer.categorize(self, text)
        if 'committees' in attrs:
            committees = attrs['committees']
            for committee in re.findall(committees_rgx, text, re.I):
                if committee not in committees:
                    committees.append(committee)
        return attrs

########NEW FILE########
__FILENAME__ = bills
import re
import datetime
import scrapelib
from collections import defaultdict

from .actions import Categorizer, committees_abbrs
from .utils import xpath
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

import lxml.etree
import lxml.html
import feedparser


class WABillScraper(BillScraper):
    jurisdiction = 'wa'
    _base_url = 'http://wslwebservices.leg.wa.gov/legislationservice.asmx'
    categorizer = Categorizer()
    _subjects = defaultdict(list)

    def build_subject_mapping(self, year):
        url = 'http://apps.leg.wa.gov/billsbytopic/Results.aspx?year=%s' % year
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute('http://apps.leg.wa.gov/billsbytopic/')
        for link in doc.xpath('//a[contains(@href, "ResultsRss")]/@href'):
            subject = link.rsplit('=', 1)[-1]
            link = link.replace(' ', '%20')

            # Strip invalid characters
            rss = re.sub(r'^[^<]+', '', self.urlopen(link))
            rss = feedparser.parse(rss)
            for e in rss['entries']:
                match = re.match('\w\w \d{4}', e['title'])
                if match:
                    self._subjects[match.group()].append(subject)

    def scrape(self, chamber, session):
        bill_id_list = []
        year = int(session[0:4])

        # first go through API response and get bill list
        for y in (year, year + 1):
            self.build_subject_mapping(y)
            url = "%s/GetLegislationByYear?year=%s" % (self._base_url, y)

            try:
                page = self.urlopen(url)
                page = lxml.etree.fromstring(page.bytes)
            except scrapelib.HTTPError:
                continue  # future years.

            for leg_info in xpath(page, "//wa:LegislationInfo"):
                bill_id = xpath(leg_info, "string(wa:BillId)")
                bill_num = int(bill_id.split()[1])

                # Skip gubernatorial appointments
                if bill_num >= 9000:
                    continue

                # Senate bills are numbered starting at 5000,
                # House at 1000
                if bill_num > 5000:
                    bill_chamber = 'upper'
                else:
                    bill_chamber = 'lower'

                if bill_chamber != chamber:
                    continue

                # normalize bill_id
                bill_id_norm = re.findall('(?:S|H)(?:B|CR|JM|JR|R) \d+',
                                          bill_id)
                if not bill_id_norm:
                    self.warning("illegal bill_id %s" % bill_id)
                    continue

                bill_id_list.append(bill_id_norm[0])

        # de-dup bill_id
        for bill_id in list(set(bill_id_list)):
            bill = self.scrape_bill(chamber, session, bill_id)
            bill['subjects'] = self._subjects[bill_id]
            self.fix_prefiled_action_dates(bill)
            self.save_bill(bill)

    def scrape_bill(self, chamber, session, bill_id):
        biennium = "%s-%s" % (session[0:4], session[7:9])
        bill_num = bill_id.split()[1]

        url = ("%s/GetLegislation?biennium=%s&billNumber"
               "=%s" % (self._base_url, biennium, bill_num))

        page = self.urlopen(url)
        page = lxml.etree.fromstring(page.bytes)
        page = xpath(page, "//wa:Legislation")[0]

        title = xpath(page, "string(wa:LongDescription)")

        bill_type = xpath(
            page,
            "string(wa:ShortLegislationType/wa:LongLegislationType)")
        bill_type = bill_type.lower()

        if bill_type == 'gubernatorial appointment':
            return

        bill = Bill(session, chamber, bill_id, title,
                    type=[bill_type])

        fake_source = ("http://apps.leg.wa.gov/billinfo/"
                       "summary.aspx?bill=%s&year=%s" % (
                           bill_num, session[0:4]))
        bill.add_source(fake_source)

        chamber_name = {'lower': 'House', 'upper': 'Senate'}[chamber]
        mimetype = 'text/html'
        version_url = ("http://www.leg.wa.gov/pub/billinfo/%s/"
                       "Htm/Bills/%s %ss/%s.htm" % (biennium,
                                                    chamber_name,
                                                    bill_type.title(),
                                                    bill_num))

        # Sometimes the measure's version_url isn't guessable. When that happens
        # have to get the url from the source page.
        version_resp = self.get(version_url)
        if version_resp.status_code != 200:
            webpage = self.get(fake_source).text
            webdoc = lxml.html.fromstring(webpage)
            version_url = webdoc.xpath('//a[contains(@href, "billdocs")]/@href')[-1]
            if version_url.lower().endswith('.pdf'):
                mimetype = 'application/pdf'

        bill.add_version(bill_id, version_url, mimetype=mimetype)

        self.scrape_sponsors(bill)
        self.scrape_actions(bill, bill_num)
        self.scrape_votes(bill)
        self.fix_prefiled_action_dates(bill)

        return bill

    def scrape_sponsors(self, bill):
        bill_id = bill['bill_id'].replace(' ', '%20')
        session = bill['session']
        biennium = "%s-%s" % (session[0:4], session[7:9])

        url = "%s/GetSponsors?biennium=%s&billId=%s" % (
            self._base_url, biennium, bill_id)

        page = self.urlopen(url)
        page = lxml.etree.fromstring(page.bytes)

        first = True
        for sponsor in xpath(page, "//wa:Sponsor/wa:Name"):
            args = ('primary' if first else 'cosponsor', sponsor.text)
            bill.add_sponsor(*args)
            first = False

    def scrape_actions(self, bill, bill_num):
        bill_id = bill['bill_id'].replace(' ', '%20')
        session = bill['session']
        biennium = "%s-%s" % (session[0:4], session[7:9])
        begin_date = "%s-01-10T00:00:00" % session[0:4]
        end_date = "%d-01-10T00:00:00" % (int(session[5:9]) + 1)

        chamber = bill['chamber']

        url = "http://apps.leg.wa.gov/billinfo/summary.aspx?bill=%s&year=%s" % (
            bill_num,
            biennium
        )

        page = self.urlopen(url)
        if "Bill Not Found" in page:
            return

        page = lxml.html.fromstring(page)
        actions = page.xpath("//table")[6]
        found_heading = False
        out = False
        curchamber = bill['chamber']
        curday = None
        curyear = None

        for action in actions.xpath(".//tr"):
            if out:
                continue

            if not found_heading:
                if action.xpath(".//td[@colspan='3']//b") != []:
                    found_heading = True
                else:
                    continue

            if action.xpath(".//a[@href='#history']"):
                out = True
                continue

            rows = action.xpath(".//td")
            rows = rows[1:]
            if len(rows) == 1:
                txt = rows[0].text_content().strip()

                session = re.findall(r"(\d{4}) (.*) SESSION", txt)
                chamber = re.findall(r"IN THE (HOUSE|SENATE)", txt)

                if session != []:
                    session = session[0]
                    year, session_type = session
                    curyear = year

                if chamber != []:
                    curchamber = {
                        "SENATE": 'upper',
                        "HOUSE": 'lower'
                    }[chamber[0]]
            else:
                _, day, action = [x.text_content().strip() for x in rows]

                junk = ['(View Original Bill)',
                        '(Committee Materials)']

                for string in junk:
                    action = action.replace(string, '').strip()

                if day != "":
                    curday = day
                if curday is None or curyear is None:
                    continue

                for abbr in re.findall(r'([A-Z]{3,})', action):
                    if abbr in committees_abbrs:
                        action = action.replace(
                            abbr, committees_abbrs[abbr], 1)

                date = "%s %s" % (curyear, curday)
                date = datetime.datetime.strptime(date, "%Y %b %d")
                attrs = dict(actor=curchamber, date=date, action=action)
                attrs.update(self.categorizer.categorize(action))
                bill.add_action(**attrs)

    def fix_prefiled_action_dates(self, bill):
        '''Fix date of 'prefiled' actions.
        '''
        now = datetime.datetime.now()
        for action in bill['actions']:
            date = action['date']
            if now < date:
                if 'prefiled' in action['action'].lower():
                    # Reduce the date by one year for prefiled dated in the
                    # future.
                    action['date'] = datetime.datetime(
                        year=date.year - 1, month=date.month, day=date.day)
                else:
                    # Sometimes an action just refers to meeting that's a
                    # week in the future.`
                    if 'scheduled for' in action['action'].lower():
                        continue
                    msg = 'Found an action date that was in the future.'
                    raise Exception(msg)

    def scrape_votes(self, bill):
        session = bill['session']
        biennium = "%s-%s" % (session[0:4], session[7:9])
        bill_num = bill['bill_id'].split()[1]

        url = ("http://wslwebservices.leg.wa.gov/legislationservice.asmx/"
               "GetRollCalls?billNumber=%s&biennium=%s" % (
                   bill_num, biennium))
        page = self.urlopen(url)
        page = lxml.etree.fromstring(page.bytes)

        for rc in xpath(page, "//wa:RollCall"):
            motion = xpath(rc, "string(wa:Motion)")

            date = xpath(rc, "string(wa:VoteDate)").split("T")[0]
            date = datetime.datetime.strptime(date, "%Y-%m-%d").date()

            yes_count = int(xpath(rc, "string(wa:YeaVotes/wa:Count)"))
            no_count = int(xpath(rc, "string(wa:NayVotes/wa:Count)"))
            abs_count = int(
                xpath(rc, "string(wa:AbsentVotes/wa:Count)"))
            ex_count = int(
                xpath(rc, "string(wa:ExcusedVotes/wa:Count)"))

            other_count = abs_count + ex_count

            agency = xpath(rc, "string(wa:Agency)")
            chamber = {'House': 'lower', 'Senate': 'upper'}[agency]

            vote = Vote(chamber, date, motion,
                        yes_count > (no_count + other_count),
                        yes_count, no_count, other_count)

            for sv in xpath(rc, "wa:Votes/wa:Vote"):
                name = xpath(sv, "string(wa:Name)")
                vtype = xpath(sv, "string(wa:VOte)")

                if vtype == 'Yea':
                    vote.yes(name)
                elif vtype == 'Nay':
                    vote.no(name)
                else:
                    vote.other(name)

            bill.add_vote(vote)

########NEW FILE########
__FILENAME__ = committees
from .utils import xpath
from billy.scrape.committees import CommitteeScraper, Committee

import lxml.etree


class WACommitteeScraper(CommitteeScraper):
    jurisdiction = 'wa'

    _base_url = 'http://wslwebservices.leg.wa.gov/CommitteeService.asmx'

    def scrape(self, chamber, term):
        biennium = "%s-%s" % (term[0:4], term[7:9])

        url = "%s/GetActiveCommittees?biennium=%s" % (self._base_url, biennium)
        page = self.urlopen(url)
        page = lxml.etree.fromstring(page.bytes)

        for comm in xpath(page, "//wa:Committee"):
            agency = xpath(comm, "string(wa:Agency)")
            comm_chamber = {'House': 'lower', 'Senate': 'upper'}[agency]
            if comm_chamber != chamber:
                continue

            name = xpath(comm, "string(wa:Name)")
            comm_id = xpath(comm, "string(wa:Id)")
            # acronym = xpath(comm, "string(wa:Acronym)")
            phone = xpath(comm, "string(wa:Phone)")

            comm = Committee(chamber, name, _code=comm_id,
                             office_phone=phone)
            self.scrape_members(comm, agency)
            comm.add_source(url)
            if comm['members']:
                self.save_committee(comm)

    def scrape_members(self, comm, agency):
        # Can't get them to accept special characters (e.g. &) in URLs,
        # no matter how they're encoded, so we use the SOAP API here.
        template = """
        <?xml version="1.0" encoding="utf-8"?>
        <soap12:Envelope xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:soap12="http://www.w3.org/2003/05/soap-envelope">
          <soap12:Body>
            <GetActiveCommitteeMembers xmlns="http://WSLWebServices.leg.wa.gov/">
              <agency>%s</agency>
              <committeeName>%s</committeeName>
            </GetActiveCommitteeMembers>
          </soap12:Body>
        </soap12:Envelope>
        """.strip()

        body = template % (agency, comm['committee'].replace('&', '&amp;'))
        headers = {'Content-Type': 'application/soap+xml; charset=utf-8'}
        resp = self.post(self._base_url, data=body, headers=headers)
        doc = lxml.etree.fromstring(resp.content)

        if 'subcommittee' in comm['committee'].lower():
            roles = ['chair', 'ranking minority member']
        else:
            roles = ['chair', 'vice chair', 'ranking minority member',
                     'assistant ranking minority member']

        for i, member in enumerate(xpath(doc, "//wa:Member")):
            name = xpath(member, "string(wa:Name)")
            try:
                role = roles[i]
            except IndexError:
                role = 'member'
            comm.add_member(name, role)

########NEW FILE########
__FILENAME__ = events
from datetime import timedelta
import datetime as dt

import lxml.etree
import pytz
import re

from billy.scrape.events import EventScraper, Event

event_page = "http://www.leg.wa.gov/legislature/pages/showagendas.aspx?chamber=%s&start=%s&end=%s"
# 1st arg: [joint|house|senate]
# 2ed arg: start date (5/1/2012)
# 3ed arg: end date (5/31/2012)

class WAEventScraper(EventScraper):
    jurisdiction = 'wa'

    _tz = pytz.timezone('US/Pacific')
    _ns = {'wa': "http://WSLWebServices.leg.wa.gov/"}

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def scrape_agenda(self, ols):
        if len(ols) == 0:
            return []
        ret = []
        # ok. game on.
        for ol in ols:
            try:
                ol = ol[0]
            except IndexError:
                continue
            lis = ol.xpath(".//li")
            regex = r'(S|H)J?(R|B|M) \d{4}'
            for li in lis:
                agenda_item = li.text_content()
                bill = re.search(regex, agenda_item)
                if bill is not None:
                    start, end = bill.regs[0]
                    ret.append({
                        "bill": agenda_item[start:end],
                        "descr": agenda_item
                    })
        return ret

    def scrape(self, chamber, session):

        cha = {
            "upper" : "senate",
            "lower" : "house",
            "other" : "joint"
        }[chamber]

        print_format = "%m/%d/%Y"

        now = dt.datetime.now()
        start = now.strftime(print_format)
        then = now + timedelta(weeks=4)
        end = then.strftime(print_format)
        url = event_page % (
            cha,
            start,
            end
        )

        page = self.lxmlize(url)

        def _split_tr(trs):
            ret = []
            cur = []
            for tr in trs:
                if len(tr.xpath(".//hr")) > 0:
                    ret.append(cur)
                    cur = []
                    continue
                cur.append(tr)
            if cur != []:
                ret.append(cur)
            return ret

        tables = page.xpath("//table[@class='AgendaCommittee']")
        for table in tables:
            # grab agenda, etc
            trs = table.xpath(".//tr")
            events = _split_tr(trs)
            for event in events:
                assert len(event) == 2
                header = event[0]
                body = event[1]
                whowhen = header.xpath(".//h2")[0].text_content()
                blocks = [ x.strip() for x in whowhen.rsplit("-", 1) ]
                who = blocks[0]
                when = blocks[1].replace(u'\xa0', ' ')
                if "TBA" in when:
                    continue  # XXX: Fixme

                cancel = \
                    body.xpath(".//span[@style='color:red;font-weight:bold']")

                if len(cancel) > 0:
                    cancel = True
                else:
                    cancel = False


                descr = body.xpath(".//*")
                flush = False
                where = body.xpath(".//br")[1].tail

                kwargs = {
                    "location": (where or '').strip() or "unknown"
                }

                if cancel:
                    kwargs['cancelled'] = cancel

                when = dt.datetime.strptime(when, "%m/%d/%y  %I:%M %p")

                meeting_title = "Scheduled Meeting of " + who

                agenda = self.scrape_agenda(body.xpath(".//ol"))
                event = Event(session, when, 'committee:meeting',
                              meeting_title, **kwargs)
                event.add_participant(
                    "host",
                    who,
                    'committee',
                    chamber=chamber
                )
                event.add_source(url)

                for item in agenda:
                    bill = item['bill']
                    descr = item['descr']
                    event.add_related_bill(
                        bill,
                        description=descr,
                        type="consideration"
                    )


                self.save_event(event)

########NEW FILE########
__FILENAME__ = legislators
from .utils import xpath
from billy.scrape.legislators import LegislatorScraper, Legislator

import scrapelib
import lxml.html
import lxml.etree


class WALegislatorScraper(LegislatorScraper):
    jurisdiction = 'wa'

    def scrape(self, chamber, term):
        biennium = "%s-%s" % (term[0:4], term[7:9])

        url = ("http://wslwebservices.leg.wa.gov/SponsorService.asmx/"
               "GetSponsors?biennium=%s" % biennium)

        # these pages are useful for checking if a leg is still in office
        if chamber == 'upper':
            cur_members = self.urlopen('http://www.leg.wa.gov/senate/senators/Pages/default.aspx')
        else:
            cur_members = self.urlopen('http://www.leg.wa.gov/house/representatives/Pages/default.aspx')

        page = self.urlopen(url)
        page = lxml.etree.fromstring(page.bytes)

        for member in xpath(page, "//wa:Member"):

            mchamber = xpath(member, "string(wa:Agency)")
            mchamber = {'House': 'lower', 'Senate': 'upper'}[mchamber]

            if mchamber != chamber:
                continue

            name = xpath(member, "string(wa:Name)").strip()

            # if the legislator isn't in the listing, skip them
            if name not in cur_members:
                self.warning('%s is no longer in office' % name)
                continue

            party = xpath(member, "string(wa:Party)")
            party = {'R': 'Republican', 'D': 'Democratic'}.get(
                party, party)

            district = xpath(member, "string(wa:District)")
            if district == '0':
                # Skip phony district 0.
                continue

            email = xpath(member, "string(wa:Email)")
            leg_id = xpath(member, "string(wa:Id)")
            phone = xpath(member, "string(wa:Phone)")

            last = xpath(member, "string(wa:LastName)")
            last = last.lower().replace(' ', '')


            if chamber == 'upper':
                leg_url = ("http://www.leg.wa.gov/senate/senators/"
                           "Pages/%s.aspx" % last)
            else:
                leg_url = ("http://www.leg.wa.gov/house/"
                           "representatives/Pages/%s.aspx" % last)
            scraped_offices = []

            try:
                leg_page = self.urlopen(leg_url)
                leg_page = lxml.html.fromstring(leg_page)
                leg_page.make_links_absolute(leg_url)

                photo_link = leg_page.xpath(
                    "//a[contains(@href, 'publishingimages')]")
                if photo_link:
                    photo_url = photo_link[0].attrib['href']
                offices = leg_page.xpath("//table[@cellspacing='0']/tr/td/b[contains(text(), 'Office')]")
                for office in offices:
                    office_block = office.getparent()
                    office_name = office.text_content().strip().rstrip(":")
                    address_lines = [x.tail for x in office_block.xpath(".//br")]
                    address_lines = filter(lambda a: a is not None, address_lines)
                    phone = address_lines.pop(len(address_lines) - 1)
                    address = "\n".join(address_lines)
                    obj = {
                        "name": office_name,
                        "phone": phone
                    }
                    if address.strip() != '':
                        obj['address'] = address

                    scraped_offices.append(obj)
            except scrapelib.HTTPError:
                # Sometimes the API and website are out of sync
                # with respect to legislator resignations/appointments
                photo_url = ''

            leg = Legislator(term, chamber, district,
                             name, '', '', '', party,
                             _code=leg_id,
                             photo_url=photo_url, url=leg_url)
            leg.add_source(leg_url)

            for office in scraped_offices:
                typ = 'district' if 'District' in office['name'] else 'capitol'
                leg.add_office(typ, office.pop('name'),
                               **office)

            self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = utils
NS = {'wa': "http://WSLWebServices.leg.wa.gov/"}


def xpath(elem, path):
    """
    A helper to run xpath with the proper namespaces for the Washington
    Legislative API.
    """
    return elem.xpath(path, namespaces=NS)

########NEW FILE########
__FILENAME__ = bills
import datetime
import lxml.html
import os
import re
from collections import defaultdict

import scrapelib

from billy.scrape.utils import convert_pdf
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

motion_classifiers = {
    '(Assembly|Senate)( substitute)? amendment': 'amendment',
    'Report (passage|concurrence)': 'passage',
    'Report (adoption|introduction and adoption) of Senate( Substitute)? Amendment': 'amendment',
    'Report Assembly( Substitute)? Amendment': 'amendment',
    'Read a third time': 'passage',
    'Adopted': 'passage'
}

action_classifiers = {
    '(Senate|Assembly)( substitute)? amendment .* offered': 'amendment:introduced',
    '(Senate|Assembly)( substitute)? amendment .* rejected': 'amendment:failed',
    '(Senate|Assembly)( substitute)? amendment .* adopted': 'amendment:passed',
    '(Senate|Assembly)( substitute)? amendment .* laid on table': 'amendment:tabled',
    '(Senate|Assembly)( substitute)? amendment .* withdrawn': 'amendment:withdrawn',
    'Report (passage|concurrence).* recommended': 'committee:passed:favorable',
    'Report approved by the Governor': 'governor:signed',
    '.+ (withdrawn|added) as a co(author|sponsor)': 'other',
    'R(ead (first time )?and r)?eferred to committee': 'committee:referred',
    'Read a third time and (passed|concurred)': 'bill:passed',
    'Adopted': 'bill:passed',
    'Presented to the Governor': 'governor:received',
    'Introduced by': 'bill:introduced',
    'Read a second time': 'bill:reading:2',
}


class WIBillScraper(BillScraper):
    jurisdiction = 'wi'

    def scrape_subjects(self, year, site_id):
        last_url = None
        next_url = 'https://docs.legis.wisconsin.gov/%s/related/subject_index/index/' % year

        # if you visit this page in your browser it is infinite-scrolled
        # but if you disable javascript you'll see the 'Down' links
        # that we use to scrape the data

        self.subjects = defaultdict(list)

        while last_url != next_url:
            html = self.urlopen(next_url)
            doc = lxml.html.fromstring(html)
            doc.make_links_absolute(next_url)

            last_url = next_url
            # get the 'Down' url
            next_url = doc.xpath('//a[text()="Down"]/@href')[0]

            # slug is upper case in links for special sessions
            if site_id != 'reg':
                site_id = site_id.upper()
            a_path = '/document/session/%s/%s/' % (year, site_id)

            # find all bill links to bills in this session
            for bill_a in doc.xpath('//a[contains(@href, "%s")]' % a_path):
                bill_id = bill_a.text_content().split()[-1]

                # subject is in the immediately preceding span
                preceding_subject = bill_a.xpath(
                    './preceding::div[contains(@class,"qsSubject")]/text()')
                # there wasn't a subject get the one from end of the prior page
                if not preceding_subject:
                    preceding_subject = last_subject[0]
                else:
                    preceding_subject = preceding_subject[-1]
                preceding_subject = preceding_subject.replace(u'\xe2\x80\x94',
                                                              '')
                self.subjects[bill_id].append(preceding_subject)

            # last subject on the page, in case we get a bill_id on next page
            last_subject_div = doc.xpath(
                '//div[contains(@class,"qsSubject")]/text()')
            if last_subject_div:
                last_subject = last_subject_div[0]


    def scrape(self, chamber, session):
        # get year
        for t in self.metadata['terms']:
            if session in t['sessions']:
                year = t['name'][0:4]
                break

        site_id = self.metadata['session_details'][session].get('site_id',
                                                                'reg')
        chamber_slug = {'upper': 'sen', 'lower': 'asm'}[chamber]

        self.scrape_subjects(year, site_id)

        types = ('bill', 'joint_resolution', 'resolution')

        for type in types:
            url = 'http://docs.legis.wisconsin.gov/%s/proposals/%s/%s/%s' % (
                year, site_id, chamber_slug, type)

            self.scrape_bill_list(chamber, session, url)

    def scrape_bill_list(self, chamber, session, url):
        if 'joint_resolution' in url:
            bill_type = 'joint resolution'
        elif 'resolution' in url:
            bill_type = 'resolution'
        elif 'bill' in url:
            bill_type = 'bill'

        try:
            data = self.urlopen(url)
        except scrapelib.HTTPError:
            self.warning('skipping URL %s' % url)
            return
        doc = lxml.html.fromstring(data)
        doc.make_links_absolute(url)
        links = doc.xpath('//li//a')
        for link in links:
            bill_url = link.get('href')
            bill_id = bill_url.rsplit('/', 1)[-1]

            title = link.tail.replace(' - Relating to: ', '').strip()

            bill = Bill(session, chamber, bill_id, title,
                        type=bill_type)
            bill['subjects'] = list(set(self.subjects[bill_id]))
            self.scrape_bill_history(bill, bill_url)

    def scrape_bill_history(self, bill, url):
        body = self.urlopen(url)
        doc = lxml.html.fromstring(body)
        doc.make_links_absolute(url)

        bill['status'] = doc.xpath('//div[@class="propStatus"]/h2/text()')[0]

        # add versions
        for a in doc.xpath('//ul[@class="docLinks"]/li//a'):
            # blank ones are PDFs that follow HTML
            if not a.text:
                continue
            elif ('Wisconsin Act' in a.text or
                  'Memo' in a.text or
                  'Government Accountability Board' in a.text or
                  'Redistricting Attachment' in a.text or
                  'Budget Index Report' in a.text or
                  'Veto Message' in a.text
                 ):
                bill.add_document(a.text, a.get('href'))
            elif ('Bill Text' in a.text or
                  'Resolution Text' in a.text or
                  'Enrolled Joint Resolution' in a.text or
                  'Engrossed Resolution' in a.text or
                  'Text as Enrolled' in a.text
                 ):
                bill.add_version(a.text, a.get('href'), mimetype="text/html")

                pdf = a.xpath('following-sibling::span/a/@href')[0]
                bill.add_version(a.text, pdf, mimetype="application/pdf")

            elif a.text in ('Amendments', 'Fiscal Estimates',
                            'Record of Committee Proceedings'):
                extra_doc_url = a.get('href')
                extra_doc = lxml.html.fromstring(self.urlopen(extra_doc_url))
                extra_doc.make_links_absolute(extra_doc_url)
                for extra_a in extra_doc.xpath('//li//a'):
                    if extra_a.text:
                        bill.add_document(extra_a.text, extra_a.get('href'))
            else:
                self.warning('unknown document %s %s' % (bill['bill_id'],
                                                         a.text))

        # add actions (second history dl is the full list)
        hist_table = doc.xpath('//table[@class="history"]')[1]
        for row in hist_table.xpath('.//tr[@class="historyRow"]'):
            date_house, action_td, journal = row.getchildren()

            date, actor = date_house.text_content().split()
            date = datetime.datetime.strptime(date, '%m/%d/%Y')
            actor = {'Asm.': 'lower', 'Sen.': 'upper'}[actor]
            action = action_td.text_content()

            if 'Introduced by' in action:
                self.parse_sponsors(bill, action)

            # classify actions
            atype = 'other'
            for regex, type in action_classifiers.iteritems():
                if re.match(regex, action):
                    atype = type
                    break

            kwargs = {}

            if "committee:referred" in atype:
                kwargs['committees'] = re.sub(
                    'R(ead (first time )?and r)?eferred to committee',
                    '', action)

            bill.add_action(actor, action, date, atype, **kwargs)

            # if this is a vote, add a Vote to the bill
            if 'Ayes' in action:
                vote_url = action_td.xpath('a/@href')
                if vote_url:
                    self.add_vote(bill, actor, date, action, vote_url[0])

        bill.add_source(url)
        self.save_bill(bill)

    def parse_sponsors(self, bill, action):
        if ';' in action:
            lines = action.split(';')
        else:
            lines = [action]

        for line in lines:
            match = re.match(
                '(Introduced|Cosponsored) by (?:joint )?(Senator|Representative|committee|Joint Legislative Council|Law Revision Committee)s?(.*)',
                line)
            if not match:
                # So far, the only one that doens't match is
                # http://docs.legis.wisconsin.gov/2011/proposals/ab568
                # In the following format:
                # Introduced by Representatives Krusick and J. Ott, by ... ;
                match = re.match(
                    'Introduced by (Representatives|Senators) (.*),',
                    line
                )
                if not match:
                    # Nothing to do here :)
                    continue

                type  = "Introduced"
                title, names = match.groups()
                raise Exception("Foo")
            else:
                type, title, people = match.groups()

            if type == 'Introduced':
                sponsor_type = 'primary'
            elif type == 'Cosponsored':
                sponsor_type = 'cosponsor'

            if title == 'Senator':
                sponsor_chamber = 'upper'
            elif title == 'Representative':
                sponsor_chamber = 'lower'
            elif title == 'committee':
                sponsor_chamber = bill['chamber']
                people = 'Committee ' + people
            elif title in ('Joint Legislative Council',
                           'Law Revision Committee'):
                sponsor_chamber = bill['chamber']
                people = title

            for r in re.split(r'\sand\s|\,', people):
                if r.strip():
                    bill.add_sponsor(sponsor_type, r.strip(),
                                     chamber=sponsor_chamber)

    def add_vote(self, bill, chamber, date, text, url):
        votes = re.findall(r'Ayes,? (\d+)[,;]\s+N(?:oes|ays),? (\d+)', text)
        (yes, no) = int(votes[0][0]), int(votes[0][1])

        vtype = 'other'
        for regex, type in motion_classifiers.iteritems():
            if re.match(regex, text):
                vtype = type
                break

        v = Vote(chamber, date, text, yes > no, yes, no, 0, type=vtype)

        # fetch the vote itself
        if url:
            v.add_source(url)

            if 'av' in url:
                self.add_house_votes(v, url)
            elif 'sv' in url:
                self.add_senate_votes(v, url)

        # other count is brute forced
        v['other_count'] = len(v['other_votes'])
        v.validate()
        bill.add_vote(v)


    def add_senate_votes(self, vote, url):
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        # what to do with the pieces
        vfunc = None

        # a game of div-div-table
        for ddt in doc.xpath('//div/div/table'):
            text = ddt.text_content()
            if 'Wisconsin Senate' in text or 'SEQUENCE NO' in text:
                continue
            elif 'AYES -' in text:
                for name in text.split('\n\n\n\n\n')[1:]:
                    if name.strip() and 'AYES' not in name:
                        vote.yes(name.strip())
            elif 'NAYS -' in text:
                for name in text.split('\n\n\n\n\n')[1:]:
                    if name.strip() and 'NAYS' not in name:
                        vote.no(name.strip())
            elif 'NOT VOTING -' in text:
                for name in text.split('\n\n\n\n\n')[1:]:
                    if name.strip():
                        vote.other(name.strip())
            elif text.strip():
                raise ValueError('unexpected block in vote')

    def add_house_votes(self, vote, url):
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)

        header_td = doc.xpath('//td[@align="center"]')[0].text_content()
        ayes_nays = re.findall('AYES - (\d+) .*? NAYS - (\d+)', header_td)
        vote['yes_count'] = int(ayes_nays[0][0])
        vote['no_count'] = int(ayes_nays[0][1])

        for td in doc.xpath('//td[@width="120"]'):
            name = td.text_content()
            if name == 'NAME':
                continue
            for vote_td in td.xpath('./preceding-sibling::td'):
                if vote_td.text_content() == 'Y':
                    vote.yes(name)
                elif vote_td.text_content() == 'N':
                    vote.no(name)
                elif vote_td.text_content() == 'NV':
                    vote.other(name)

########NEW FILE########
__FILENAME__ = committees
from billy.scrape.committees import CommitteeScraper, Committee

import lxml.html

class WICommitteeScraper(CommitteeScraper):
    jurisdiction = 'wi'

    def scrape_committee(self, name, url, chamber):
        com = Committee(chamber, name)
        com.add_source(url)
        data = self.urlopen(url)
        doc = lxml.html.fromstring(data)

        for leg in doc.xpath('//a[contains(@href, "leg-info")]/text()'):
            leg = leg.replace('Representative ', '')
            leg = leg.replace('Senator ', '')
            leg = leg.strip()
            if ' (' in leg:
                leg, role = leg.split(' (')
                if 'Vice-Chair' in role:
                    role = 'vice-chair'
                elif 'Co-Chair' in role:
                    role = 'co-chair'
                elif 'Chair' in role:
                    role = 'chair'
                else:
                    raise Exception('unknown role: %s' % role)
            else:
                role = 'member'
            com.add_member(leg, role)

        self.save_committee(com)


    def scrape(self, term, chambers):
        for chamber in chambers:
            url = 'http://legis.wisconsin.gov/Pages/comm-list.aspx?h='
            url += 's' if chamber == 'upper' else 'a'
            data = self.urlopen(url)
            doc = lxml.html.fromstring(data)
            doc.make_links_absolute(url)

            table = doc.xpath('//table[@class="commList"]')[0]
            for a in table.xpath('.//a[contains(@href, "comm-info")]'):
                self.scrape_committee(a.text, a.get('href'), chamber)

            # also scrape joint committees (once, only on upper)
            if chamber == 'upper':
                table = doc.xpath('//table[@class="commList"]')[1]
                for a in table.xpath('.//a[contains(@href, "comm-info")]'):
                    self.scrape_committee(a.text, a.get('href'), 'joint')

########NEW FILE########
__FILENAME__ = events
import datetime as dt

from billy.scrape import NoDataForPeriod
from billy.scrape.events import Event, EventScraper

import lxml.html
import pytz

calurl = "http://committeeschedule.legis.wisconsin.gov/?filter=Upcoming&committeeID=-1"

class WIEventScraper(EventScraper):
    jurisdiction = 'wi'
    _tz = pytz.timezone('US/Eastern')

    def lxmlize(self, url):
        page = self.urlopen(url)
        page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def scrape_participants(self, session, href):
        page = self.lxmlize(href)
        legs = page.xpath("//a[contains(@href, '/Pages/leg-info.aspx')]/text()")
        role_map = {"participant": "participant",
                    "Chair": "chair",
                    "Co-Chair": "chair",
                    "Vice-Chair": "participant"}
        ret = []
        for leg in legs:
            name = leg
            title = 'participant'
            if "(" and ")" in leg:
                name, title = leg.split("(", 1)
                title = title.replace(")", " ").strip()
                name = name.strip()
            title = role_map[title]
            ret.append({
                "name": name,
                "title": title
            })
        return ret

    def scrape(self, session, chambers):
        page = self.lxmlize(calurl)
        events = page.xpath("//table[@class='agenda-body']//tr")[1:]

        for event in events:
            comit_url = event.xpath(
                ".//a[contains(@href, '/Pages/comm-info.aspx?c=')]")

            if len(comit_url) != 1:
                raise Exception

            comit_url = comit_url[0]
            who = self.scrape_participants(session, comit_url.attrib['href'])

            tds = event.xpath("./*")
            date = tds[0].text_content().strip()
            cttie = tds[1].text_content().strip()
            cttie_chamber, cttie = [x.strip() for x in cttie.split(" - ", 1)]
            info = tds[2]
            name = info.xpath("./a[contains(@href, 'raw')]")[0]
            notice = name.attrib['href']
            name = name.text
            time, where = info.xpath("./i/text()")
            what = tds[3].text_content()
            what = what.replace("Items: ", "")
            if "(None)" in what:
                continue
            what = [x.strip() for x in what.split(";")]

            when = ", ".join([date, str(dt.datetime.now().year), time])
            when = dt.datetime.strptime(when, "%a %b %d, %Y, %I:%M %p")

            event = Event(session, when, 'committee:meeting', name,
                          location=where, link=notice)

            event.add_source(calurl)
            event.add_participant('host', cttie, 'committee',
                                  chamber=cttie_chamber)
            event.add_document("notice", notice, mimetype='application/pdf')

            for thing in who:
                event.add_participant(thing['title'], thing['name'],
                                      'legislator', chamber=cttie_chamber)

            self.save_event(event)

########NEW FILE########
__FILENAME__ = legislators
import datetime
import lxml.html
import re

from billy.scrape.legislators import LegislatorScraper, Legislator


PARTY_DICT = {'D': 'Democratic', 'R': 'Republican', 'I': 'Independent'}

class WILegislatorScraper(LegislatorScraper):
    jurisdiction = 'wi'
    latest_only = True

    def scrape(self, chamber, term):

        if chamber == 'upper':
            url = "http://legis.wisconsin.gov/Pages/leg-list.aspx?h=s"
        else:
            url = "http://legis.wisconsin.gov/Pages/leg-list.aspx?h=a"

        body = self.urlopen(url)
        page = lxml.html.fromstring(body)
        page.make_links_absolute(url)

        for row in page.xpath("//table[@class='legis-list']/tr")[1:]:
            if row.xpath(".//a/@href"):
                rep_url = row.xpath(".//a/@href")[0]
                rep_doc = lxml.html.fromstring(self.urlopen(rep_url))
                rep_doc.make_links_absolute(rep_url)

                first_name = rep_doc.xpath('//h2[@class="given-name"]/text()')[0]
                last_name = rep_doc.xpath('//h2[@class="family-name"]/text()')[0]
                full_name = '%s %s' % (first_name, last_name)
                party = rep_doc.xpath('//div[@class="party"]/text()')[0]
                if party == 'Democrat':
                    party = 'Democratic'

                district = str(int(row.getchildren()[2].text_content()))

                # email
                email = rep_doc.xpath('//a[starts-with(@href, "mailto")]/text()')
                if email:
                    email = email[0]
                else:
                    email = ''

                leg = Legislator(term, chamber, district, full_name,
                                 first_name=first_name, last_name=last_name,
                                 party=party, url=rep_url, email=email)

                img = rep_doc.xpath('//img[@class="photo"]/@src')
                if img:
                    leg['photo_url'] = img[0]

                # office ####
                address = '\n'.join(rep_doc.xpath('//dt[text()="Madison Office"]/following-sibling::dd/div/text()'))
                phone = rep_doc.xpath('//dt[text()="Telephone"]/following-sibling::dd/div/text()')
                if phone:
                    phone = re.sub('\s+', ' ', phone[0]).strip()
                else:
                    phone = None
                fax = rep_doc.xpath('//dt[text()="Fax"]/following-sibling::dd/div/text()')
                if fax:
                    fax = re.sub('\s+', ' ', fax[0]).strip()
                else:
                    fax = None

                leg.add_office('capitol', 'Madison Office', address=address,
                               phone=phone, fax=fax)

                # save legislator
                leg.add_source(rep_url)
                self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = actions
'''

'''
import re
from billy.scrape.actions import Rule, BaseCategorizer


committees = [
    u"Veterans' Affairs",
    u'Agriculture and Agri-business Committee',
    u'Agriculture',
    u'Banking and Insurance',
    u'Banking',
    u'Children, Juveniles and Other Issues',
    u'Constitutional Revision',
    u'Council of Finance and Administration',
    u'Economic Development and Small Business',
    u'Economic Development',
    u'Education Accountability',
    u'Education',
    u'Employee Suggestion Award Board',
    u'Energy, Industry and Labor',
    u'Energy, Industry and Labor/Economic Development and Small Business',
    u'Enrolled Bills',
    u'Equal Pay Commission',
    u'Finance',
    u'Forest Management Review Commission',
    u'Government and Finance',
    u'Government Operations',
    u'Government Organization',
    u'Health and Human Resources Accountability',
    u'Health and Human Resources',
    u'Health',
    u'Homeland Security',
    u'House Rules',
    u'House Select Committee on Redistricting',
    u'Infrastructure',
    u'Insurance',
    u'Intern Committee',
    u'Interstate Cooperation',
    u'Judiciary',
    u'Law Institute',
    u'Minority Issues',
    u'Natural Resources',
    u'Outcomes-Based Funding Models in Higher Education',
    u'Parks, Recreation and Natural Resources',
    u'PEIA, Seniors and Long Term Care',
    u'Pensions and Retirement',
    u'Political Subdivisions',
    u'Post Audits',
    u'Regional Jail and Correctional Facility Authority',
    u'Roads and Transportation',
    u'Rule-Making Review Committee',
    u'Senior Citizen Issues',
    u'Special Investigations',
    u'Technology',
    u'Veterans Affairs',
    u'Veterans Affairs/ Homeland Security',
    u'Water Resources',
    u'Workforce Investment for Economic Development',
    ]


committees_rgx = '(%s)' % '|'.join(sorted(committees, key=len, reverse=True))


rules = (
    Rule(['Communicated to Senate', 'Senate received',
          'Ordered to Senate'], actor='upper'),
    Rule(['Communicated to House', 'House received',
          'Ordered to House'], actor='lower'),

    Rule('Read 1st time', 'bill:reading:1'),
    Rule('Read 2nd time', 'bill:reading:2'),
    Rule('Read 3rd time', 'bill:reading:3'),
    Rule('Filed for introduction', 'bill:filed'),
    Rule('^Introduced in', 'bill:introduced'),
    Rule(['Passed Senate', 'Passed House'], 'bill:passed'),
    Rule(['Reported do pass', 'With amendment, do pass'], 'committee:passed'),

    Rule([u', but first to .+?; then (?P<committees>[^;]+)',
          u'To (?P<committees>.+?) then']),
    Rule(u'(?i)voice vote', voice_vote=True),
    Rule([u'Amendment rejected'], [u'amendment:failed']),
    Rule([u'To Governor'], [u'governor:received']),
    Rule([u'Passed House'], [u'bill:passed']),
    Rule([u'Read 2nd time'], [u'bill:reading:2']),
    Rule([u', but first to (?P<committees>[^;]+)', u'Rejected'], []),
    Rule([u'Approved by Governor \d{1,2}/\d{1,2}/\d{1,2}$'], [u'governor:signed']),
    Rule([u'^Introduced'], [u'bill:introduced']),
    Rule([u'To .+? then (?P<committees>.+)'], []),
    Rule([u'^Filed for intro'], [u'bill:filed']),
    Rule([u'(?i)referred to (?P<committees>.+)'], [u'committee:referred']),
    Rule(u'Senator (?P<legislators>.+? )requests '
         u'to be removed as sponsor of bill'),
    Rule([u'To House (?P<committees>[A-Z].+)'], [u'committee:referred']),
    Rule([u'Passed Senate'], [u'bill:passed']),
    Rule([u'(?i)committed to (?P<committees>.+?) on'], []),
    Rule([u'Vetoed by Governor'], [u'governor:vetoed']),
    Rule([u'(?i)House concurred in senate amendment'], []),
    Rule([u'Be rejected'], [u'bill:failed']),
    Rule([u'To .+? then (?P<committees>.+) then',
          u'reading to (?P<committees>.+)']),
    Rule([u'Adopted by'], [u'bill:passed']),
    Rule([u'House appointed conferees:  (?P<legislators>.+)'], []),
    Rule([u'Read 3rd time'], [u'bill:reading:3']),
    Rule([u'Be adopted$'], [u'bill:passed']),
    Rule([u'(?i)originating in (House|Senate) (?P<committees>.+)',
          u'(?i)to house (?P<committees>.+)']),
    Rule([u'Read 1st time'], [u'bill:reading:1']),
    Rule([u'To .+? then .+? then (?P<committees>.+)']),
    Rule(r'To %s' % committees_rgx, 'committee:referred')
    )


class Categorizer(BaseCategorizer):
    rules = rules

    def categorize(self, text):
        '''Wrap categorize and add boilerplate committees.
        '''
        attrs = BaseCategorizer.categorize(self, text)
        committees = attrs['committees']
        for committee in re.findall(committees_rgx, text, re.I):
            if committee not in committees:
                committees.append(committee)
        return attrs

    def post_categorize(self, attrs):
        res = set()
        if 'legislators' in attrs:
            for text in attrs['legislators']:
                rgx = r'(,\s+(?![a-z]\.)|\s+and\s+)'
                legs = re.split(rgx, text)
                legs = filter(lambda x: x not in [', ', ' and '], legs)
                res |= set(legs)
        attrs['legislators'] = list(res)

        res = set()
        if 'committees' in attrs:
            for text in attrs['committees']:

                # Strip stuff like "Rules on 1st reading"
                for text in text.split('then'):
                    text = re.sub(r' on .+', '', text)
                    text = text.strip()
                    res.add(text)
        attrs['committees'] = list(res)
        return attrs

########NEW FILE########
__FILENAME__ = bills

import os
import re
import datetime
import collections
from urlparse import urlparse, parse_qsl
from urllib import quote, unquote_plus

import lxml.html

from billy.scrape.utils import convert_pdf
from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote
import scrapelib

from .actions import Categorizer


class _Url(object):
    '''A url object that can be compared with other url orbjects
    without regard to the vagaries of casing, encoding, escaping,
    and ordering of parameters in query strings.'''

    def __init__(self, url):
        parts = urlparse(url.lower())
        _query = frozenset(parse_qsl(parts.query))
        _path = unquote_plus(parts.path)
        parts = parts._replace(query=_query, path=_path)
        self.parts = parts

    def __eq__(self, other):
        return self.parts == other.parts

    def __hash__(self):
        return hash(self.parts)


class WVBillScraper(BillScraper):
    jurisdiction = 'wv'
    categorizer = Categorizer()

    bill_types = {'B': 'bill',
                  'R': 'resolution',
                  'CR': 'concurrent resolution',
                  'JR': 'joint resolution'}

    def scrape(self, chamber, session):
        if chamber == 'lower':
            orig = 'h'
        else:
            orig = 's'

        # Scrape the legislature's FTP server to figure out the filenames
        # of bill version documents.
        self._get_version_filenames(session, chamber)

        # scrape bills
        url = ("http://www.legis.state.wv.us/Bill_Status/"
               "Bills_all_bills.cfm?year=%s&sessiontype=RS"
               "&btype=bill&orig=%s" % (session, orig))
        page = lxml.html.fromstring(self.urlopen(url))
        page.make_links_absolute(url)

        for link in page.xpath("//a[contains(@href, 'Bills_history')]"):
            bill_id = link.xpath("string()").strip()
            title = link.xpath("string(../../td[2])").strip()
            self.scrape_bill(session, chamber, bill_id, title,
                             link.attrib['href'])

        # scrape resolutions
        res_url = ("http://www.legis.state.wv.us/Bill_Status/res_list.cfm?"
                   "year=%s&sessiontype=rs&btype=res") % session
        doc = lxml.html.fromstring(self.urlopen(res_url))
        doc.make_links_absolute(res_url)

        # check for links originating in this house
        for link in doc.xpath('//a[contains(@href, "houseorig=%s")]' % orig):
            bill_id = link.xpath("string()").strip()
            title = link.xpath("string(../../td[2])").strip()
            self.scrape_bill(session, chamber, bill_id, title,
                             link.attrib['href'])

    def scrape_bill(self, session, chamber, bill_id, title, url,
                    strip_sponsors=re.compile(r'\s*\(.{,50}\)\s*').sub):

        if '4184' in bill_id:
            import pdb; pdb.set_trace()

        html = self.urlopen(url)

        page = lxml.html.fromstring(html)
        page.make_links_absolute(url)

        bill_type = self.bill_types[bill_id.split()[0][1:]]

        bill = Bill(session, chamber, bill_id, title, type=bill_type)
        bill.add_source(url)

        xpath = ('//strong[contains(., "SUBJECT")]/../'
                 'following-sibling::td/a/text()')
        bill['subjects'] = page.xpath(xpath)

        for version in self.scrape_versions(session, chamber, page, bill_id):
            bill.add_version(**version)

        # Resolution pages have different html.
        values = {}
        trs = page.xpath('//div[@id="bhistcontent"]/table/tr')
        for tr in trs:
            heading = tr.xpath('td/strong/text()')
            if heading:
                heading = heading[0]
            else:
                continue
            value = tr.text_content().replace(heading, '').strip()
            values[heading] = value

        # summary was always same as title
        #bill['summary'] = values['SUMMARY:']

        # Add primary sponsor.
        primary = strip_sponsors('', values.get('LEAD SPONSOR:', ''))
        if primary:
            bill.add_sponsor('primary', primary)

        # Add cosponsors.
        sponsors = strip_sponsors('', values['SPONSORS:'])
        sponsors = re.split(', (?![A-Z]\.)', sponsors)
        for name in sponsors:
            name = name.strip(', \n\r')
            if name:
                # Fix name splitting bug where "Neale, D. Hall"
                match = re.search('(.+?), ([DM]\. Hall)', name)
                if match:
                    for name in match.groups():
                        bill.add_sponsor('cosponsor', name)
                else:
                    bill.add_sponsor('cosponsor', name)

        for link in page.xpath("//a[contains(@href, 'votes/house')]"):
            self.scrape_house_vote(bill, link.attrib['href'])

        for tr in reversed(page.xpath("//table[@class='tabborder']/descendant::tr")[1:]):
            tds = tr.xpath('td')
            if len(tds) < 3:
                continue

            chamber_letter = tds[0].text_content()
            chamber = {'S': 'upper', 'H': 'lower'}[chamber_letter]

            # Index of date info no longer varies on resolutions.
            date = tds[2].text_content().strip()
            date = datetime.datetime.strptime(date, "%m/%d/%y").date()

            action = tds[1].text_content().strip()
            if action.lower().startswith('passed senate'):
                for href in tds[1].xpath('a/@href'):
                    self.scrape_senate_vote(bill, href, date)

            attrs = dict(actor=chamber, action=action, date=date)
            attrs.update(self.categorizer.categorize(action))
            bill.add_action(**attrs)

        self.save_bill(bill)

    def scrape_house_vote(self, bill, url):
        try:
            filename, resp = self.urlretrieve(url)
        except scrapelib.HTTPError:
            self.warning("missing vote file %s" % url)
            return
        text = convert_pdf(filename, 'text')
        os.remove(filename)

        lines = text.splitlines()

        vote_type = None
        votes = collections.defaultdict(list)

        for idx, line in enumerate(lines):
            line = line.rstrip()
            match = re.search(r'(\d+)/(\d+)/(\d{4,4})$', line)
            if match:
                date = datetime.datetime.strptime(match.group(0), "%m/%d/%Y")
                continue

            match = re.match(
                r'\s+YEAS: (\d+)\s+NAYS: (\d+)\s+NOT VOTING: (\d+)',
                line)
            if match:
                motion = lines[idx - 2].strip()
                yes_count, no_count, other_count = [
                    int(g) for g in match.groups()]

                exc_match = re.search(r'EXCUSED: (\d+)', line)
                if exc_match:
                    other_count += int(exc_match.group(1))

                if line.endswith('ADOPTED') or line.endswith('PASSED'):
                    passed = True
                else:
                    passed = False

                continue

            match = re.match(
                r'(YEAS|NAYS|NOT VOTING|PAIRED|EXCUSED):\s+(\d+)\s*$',
                line)
            if match:
                vote_type = {'YEAS': 'yes',
                             'NAYS': 'no',
                             'NOT VOTING': 'other',
                             'EXCUSED': 'other',
                             'PAIRED': 'paired'}[match.group(1)]
                continue

            if vote_type == 'paired':
                for part in line.split('   '):
                    part = part.strip()
                    if not part:
                        continue
                    name, pair_type = re.match(
                        r'([^\(]+)\((YEA|NAY)\)', line).groups()
                    name = name.strip()
                    if pair_type == 'YEA':
                        votes['yes'].append(name)
                    elif pair_type == 'NAY':
                        votes['no'].append(name)
            elif vote_type:
                for name in line.split('   '):
                    name = name.strip()
                    if not name:
                        continue
                    votes[vote_type].append(name)

        vote = Vote('lower', date, motion, passed,
                    yes_count, no_count, other_count)
        vote.add_source(url)

        vote['yes_votes'] = votes['yes']
        vote['no_votes'] = votes['no']
        vote['other_votes'] = votes['other']

        assert len(vote['yes_votes']) == yes_count
        assert len(vote['no_votes']) == no_count
        assert len(vote['other_votes']) == other_count

        bill.add_vote(vote)

    def scrape_senate_vote(self, bill, url, date):
        try:
            filename, resp = self.urlretrieve(url)
        except scrapelib.HTTPError:
            self.warning("missing vote file %s" % url)
            return

        vote = Vote('upper', date, 'Passage', passed=None,
                    yes_count=0, no_count=0, other_count=0)
        vote.add_source(url)

        text = convert_pdf(filename, 'text')
        os.remove(filename)

        if re.search('Yea:\s+\d+\s+Nay:\s+\d+\s+Absent:\s+\d+', text):
            return self.scrape_senate_vote_3col(bill, vote, text, url, date)

        data = re.split(r'(Yea|Nay|Absent)s?:', text)[::-1]
        data = filter(None, data)
        keymap = dict(yeas='yes', nays='no')
        actual_vote = collections.defaultdict(int)
        while True:
            if not data:
                break
            vote_val = data.pop()
            key = keymap.get(vote_val.lower(), 'other')
            values = data.pop()
            for name in re.split(r'(?:[\s,]+and\s|[\s,]{2,})', values):
                if name.lower().strip() == 'none.':
                    continue
                name = name.replace('..', '').strip('-1234567890 \n')
                if not name:
                    continue
                getattr(vote, key)(name)
                actual_vote[vote_val] += 1
                vote[key + '_count'] += 1

        vote['actual_vote'] = actual_vote
        vote['passed'] = vote['no_count'] < vote['yes_count']
        bill.add_vote(vote)

    def scrape_senate_vote_3col(self, bill, vote, text, url, date):
        '''Scrape senate votes like this one:
        http://www.legis.state.wv.us/legisdocs/2013/RS/votes/senate/02-26-0001.pdf
        '''
        counts = dict(re.findall(r'(Yea|Nay|Absent): (\d+)', text))
        text = text.split('\n\n\n')[-2]
        lines = filter(None, text.splitlines())
        actual_vote = collections.defaultdict(int)
        for line in lines:
            vals = re.findall(r'(Y|N|A)\s+((?:\S+ ?)+)', line)
            for vote_val, name in vals:
                vote_val = vote_val.strip()
                name = name.strip()
                if vote_val == 'Y':
                    vote.yes(name)
                    vote['yes_count'] += 1
                elif vote_val == 'N':
                    vote.no(name)
                    vote['no_count'] += 1
                else:
                    vote.other(name)
                    vote['other_count'] += 1
                actual_vote[vote_val] += 1

        vote['passed'] = vote['no_count'] < vote['yes_count']

        assert vote['yes_count'] == int(counts['Yea'])
        assert vote['no_count'] == int(counts['Nay'])
        assert vote['other_count'] == int(counts['Absent'])
        bill.add_vote(vote)

    def _get_version_filenames(self, session, chamber):
        '''All bills have "versions", but for those lacking html documents,
        the .wpd file is available via ftp. Create a dict of those links
        in advance; any bills lacking html versions will get version info
        from this dict.'''

        chamber_name = {'upper': 'senate', 'lower': 'House'}[chamber]
        ftp_url = 'ftp://www.legis.state.wv.us/publicdocs/%s/RS/%s/'
        ftp_url = ftp_url % (session, chamber_name)

        try:
            html = self.urlopen(ftp_url)
        except scrapelib.FTPError:
            # The url doesn't exist. Just set _version_filenames
            # to an empty dict.
            self._version_filenames = {}
            return

        dirs = [' '.join(x.split()[3:]) for x in html.splitlines()]

        split = re.compile(r'\s+').split
        matchwpd = re.compile(r'\.wpd$', re.I).search
        splitext = os.path.splitext
        version_filenames = collections.defaultdict(list)
        for d in dirs:
            url = ('%s%s/' % (ftp_url, d)).replace(' ', '%20')
            html = self.urlopen(url)
            filenames = [split(x, 3)[-1] for x in html.splitlines()]
            filenames = filter(matchwpd, filenames)
            for fn in filenames:
                fn, ext = splitext(fn)
                if ' ' in fn:
                    bill_id, _ = fn.split(' ', 1)
                else:
                    # One bill during 2011 had no spaces
                    # in the filename. Probably a fluke.
                    digits = re.search(r'\d+', fn)
                    bill_id = fn[:digits.end()]

                version_filenames[bill_id.lower()].append((d, fn))

        self._version_filenames = version_filenames

    def _scrape_versions_normally(self, session, chamber, page, bill_id,
                                  get_name=re.compile(r'\"(.+)"').search):
        '''This first method assumes the bills versions are hyperlinked
        on the bill's status page.
        '''
        for link in page.xpath("//a[starts-with(@title, 'HTML -')]"):
            # split name out of HTML - Introduced Version - SB 1
            name = link.getparent().text.strip(' -\r\n|')
            if not name:
                name = link.getprevious().tail.strip(' -\r\n|')
            yield {'name': name, 'url': link.get('href'),
                   'mimetype': 'text/html'}

        for link in page.xpath("//a[contains(@title, 'WordPerfect')]"):
            name = link.getparent().text.strip(' -\r\n|')
            if not name:
                name = link.getprevious().tail.strip(' -\r\n|')
            if not name:
                name = link.getprevious().getprevious().tail.strip(' -\r\n|')
            yield {'name': name, 'url': link.get('href'),
                   'mimetype': 'application/vnd.wordperfect'}

    def _scrape_versions_wpd(self, session, chamber, page, bill_id):
        '''This third method scrapes the .wpd document from the legislature's
        ftp server.
        '''
        chamber_name = {'upper': 'senate', 'lower': 'House'}[chamber]
        _bill_id = bill_id.replace(' ', '').lower()
        url = 'ftp://www.legis.state.wv.us/publicdocs/%s/RS' % session

        try:
            filenames = self._version_filenames[_bill_id]
        except KeyError:
            # There are no filenames in the dict for this bill_id.
            # Skip.
            return

        for folder, filename in filenames:
            _filename = quote(filename + '.wpd')
            _url = '/'.join([url, chamber_name, folder, _filename])

            yield {'name': filename, 'url': _url,
                   'mimetype': 'application/vnd.wordperfect'}

    def scrape_versions(self, session, chamber, page, bill_id):
        '''
        Return all available version documents for this bill_id.
        '''
        res = []
        cache = set()

        # Scrape .htm and .wpd versions listed in the detail page.
        for data in self._scrape_versions_normally(session, chamber, page,
                                                   bill_id):
            _url = _Url(data['url'])
            if _url not in cache:
                cache.add(_url)
                res.append(data)

        # For each .wpd version not already scraped, add a version.
        for data in self._scrape_versions_wpd(session, chamber, page,
                                              bill_id):
            _url = _Url(data['url'])
            if _url not in cache:
                cache.add(_url)
                res.append(data)

        return res

########NEW FILE########
__FILENAME__ = committees
import re

from billy.scrape.committees import CommitteeScraper, Committee

import lxml.html


class WVCommitteeScraper(CommitteeScraper):
    jurisdiction = "wv"

    def scrape(self, chamber, term):
        getattr(self, 'scrape_' + chamber)()

    def scrape_lower(self):
        url = 'http://www.legis.state.wv.us/committees/house/main.cfm'
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        xpath = '//a[contains(@href, "HouseCommittee")]'
        for link in doc.xpath(xpath):
            text = link.text_content().strip()
            if text == '-':
                continue
            committee = self.scrape_lower_committee(link=link, name=text)
            committee.add_source(url)
            self.save_committee(committee)

        url = 'http://www.legis.state.wv.us/committees/interims/interims.cfm'
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)
        xpath = '//a[contains(@href, "committee.cfm")]'
        for link in doc.xpath(xpath):
            text = link.text_content().strip()
            if text == '-':
                continue
            committee = self.scrape_interim_committee(link=link, name=text)
            committee.add_source(url)
            self.save_committee(committee)

    def scrape_lower_committee(self, link, name):
        url = re.sub(r'\s+', '', link.attrib['href'])
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        comm = Committee('lower', name)
        comm.add_source(url)

        xpath = '//a[contains(@href, "?member=")]'
        for link in doc.xpath(xpath):
            name = link.text_content().strip()
            name = re.sub(r'^Delegate\s+', '', name)
            role = link.getnext().text or 'member'
            comm.add_member(name, role.strip())

        return comm

    def scrape_interim_committee(self, link, name):
        url = re.sub(r'\s+', '', link.attrib['href'])
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        comm = Committee('joint', name)
        comm.add_source(url)

        xpath = '//a[contains(@href, "?member=")]'
        for link in doc.xpath(xpath):
            name = link.text_content().strip()
            name = re.sub(r'^Delegate\s+', '', name)
            name = re.sub(r'^Senator\s+', '', name)
            role = link.getnext().text or 'member'
            comm.add_member(name, role.strip())

        return comm

    def scrape_upper(self):
        url = 'http://www.legis.state.wv.us/committees/senate/main.cfm'
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        xpath = '//a[contains(@href, "SenateCommittee")]'
        for link in doc.xpath(xpath):
            text = link.text_content().strip()
            if text == '-':
                continue
            committee = self.scrape_upper_committee(link=link, name=text)
            committee.add_source(url)
            self.save_committee(committee)

    def scrape_upper_committee(self, link, name):
        url = re.sub(r'\s+', '', link.attrib['href'])
        html = self.urlopen(url)
        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(url)

        comm = Committee('upper', name)
        comm.add_source(url)

        xpath = '//a[contains(@href, "?member=")]'
        for link in doc.xpath(xpath):
            name = link.text_content().strip()
            name = re.sub(r'^Delegate\s+', '', name)
            role = link.getnext().text or 'member'
            comm.add_member(name, role.strip())

        return comm

########NEW FILE########
__FILENAME__ = legislators
import re
from collections import defaultdict

from billy.utils import urlescape
from billy.scrape.legislators import LegislatorScraper, Legislator

import lxml.html


class WVLegislatorScraper(LegislatorScraper):
    jurisdiction = 'wv'

    def scrape(self, chamber, term):
        self.validate_term(term, latest_only=True)

        if chamber == 'upper':
            chamber_abbrev = 'Senate1'
        else:
            chamber_abbrev = 'House'

        url = 'http://www.legis.state.wv.us/%s/roster.cfm' % chamber_abbrev
        page = lxml.html.fromstring(self.urlopen(url))
        page.make_links_absolute(url)

        for link in page.xpath("//a[contains(@href, '?member=')]"):
            if not link.text:
                continue
            name = link.xpath("string()").strip()
            leg_url = urlescape(link.attrib['href'])

            if name in ['Members', 'Senate Members', 'House Members',
                        'Vacancy', 'VACANT', 'Vacant']:
                continue

            self.scrape_legislator(chamber, term, name, leg_url)

    def scrape_legislator(self, chamber, term, name, url):
        html = self.urlopen(url)
        page = lxml.html.fromstring(html)
        page.make_links_absolute(url)

        xpath = '//select[@name="sel_member"]/option[@selected]/text()'
        district = page.xpath('//h1[contains(., "DISTRICT")]/text()').pop().split()[1].strip().lstrip('0')

        party = page.xpath('//h2').pop().text_content()
        party = re.search(r'\((R|D)[ \-\]]', party).group(1)

        if party == 'D':
            party = 'Democratic'
        elif party == 'R':
            party = 'Republican'

        photo_url = page.xpath(
            "//img[contains(@src, 'images/members/')]")[0].attrib['src']

        email = page.xpath(
            "//a[contains(@href, 'mailto:')]")[1].attrib['href'].split(
            'mailto:')[1]

        leg = Legislator(term, chamber, district, name, party=party,
                         photo_url=photo_url, email=email, url=url)
        leg.add_source(url)
        self.scrape_offices(leg, page)
        self.save_legislator(leg)

    def scrape_offices(self, legislator, doc):
        text = doc.xpath('//b[contains(., "Capitol Office:")]')[0]
        text = text.getparent().itertext()
        text = filter(None, [t.strip() for t in text])
        officedata = defaultdict(list)
        current = None
        for chunk in text:
            if chunk.lower() == 'biography':
                break
            if chunk.strip().endswith(':'):
                current_key = chunk.strip()
                current = officedata[current_key]
            elif current is not None:
                current.append(chunk)
                if current_key == 'Business Phone:':
                    break

        email = doc.xpath('//a[contains(@href, "mailto:")]/@href')[1]
        email = email[7:]

        office = dict(
            name='Capitol Office',
            type='capitol',
            phone=(officedata['Capitol Phone:'] or [None]).pop(),
            fax=None,
            address='\n'.join(officedata['Capitol Office:']))

        legislator.add_office(**office)

        if officedata['Business Phone:']:
            legislator.add_office(
                name='Business Office',
                type='district',
                phone=officedata['Business Phone:'].pop())


########NEW FILE########
__FILENAME__ = bills
from collections import defaultdict
import datetime
import re

from billy.scrape.bills import BillScraper, Bill
from billy.scrape.votes import Vote

import scrapelib
import lxml.html


def split_names(voters):
    """Representative(s) Barbuto, Berger, Blake, Blikre, Bonner, Botten, Buchanan, Burkhart, Byrd, Campbell, Cannady, Childers, Connolly, Craft, Eklund, Esquibel, K., Freeman, Gingery, Greear, Greene, Harshman, Illoway, Jaggi, Kasperik, Krone, Lockhart, Loucks, Lubnau, Madden, McOmie, Moniz, Nicholas, B., Patton, Pederson, Petersen, Petroff, Roscoe, Semlek, Steward, Stubson, Teeters, Throne, Vranish, Wallis, Zwonitzer, Dn. and Zwonitzer, Dv."""
    voters = voters.split(')',1)[-1]
    # split on comma space as long as it isn't followed by an initial (\w+\.)
    # or split on 'and '
    voters = [x.strip() for x in re.split('(?:, (?!\w+(?:\.|$)))|(?:and )', voters)]
    return voters


def clean_line(line):
    return line.replace(u'\xa0', '').replace('\r\n', ' '
                                            ).replace(u'\u2011', "-").strip()

def categorize_action(action):
    categorizers = (
        ('Introduced and Referred', ('bill:introduced', 'committee:referred')),
        ('Rereferred to', 'committee:referred'),
        ('Do Pass Failed', 'committee:failed'),
        ('Passed 2nd Reading', 'bill:reading:2'),
        ('Passed 3rd Reading', ('bill:reading:3', 'bill:passed')),
        ('Failed 3rd Reading', ('bill:reading:3', 'bill:failed')),
        ('Did Not Adopt', 'amendment:failed'),
        ('Withdrawn by Sponsor', 'bill:withdrawn'),
        ('Governor Signed', 'governor:signed'),
        ('Recommended (Amend and )?Do Pass', 'committee:passed:favorable'),
        ('Recommended (Amend and )?Do NotPass', 'committee:passed:unfavorable'),
    )

    for pattern, types in categorizers:
        if re.findall(pattern, action):
            return types
    return 'other'


class WYBillScraper(BillScraper):
    jurisdiction = 'wy'

    def scrape(self, chamber, session):
        chamber_abbrev = {'upper': 'SF', 'lower': 'HB'}[chamber]

        url = ("http://legisweb.state.wy.us/%s/billindex/"
               "BillCrossRef.aspx?type=%s" % (session, chamber_abbrev))
        page = lxml.html.fromstring(self.urlopen(url))

        for tr in page.xpath("//tr[@valign='middle']")[1:]:
            bill_id = tr.xpath("string(td[1])").strip()
            title = tr.xpath("string(td[2])").strip()

            if bill_id[0:2] in ['SJ', 'HJ']:
                bill_type = 'joint resolution'
            else:
                bill_type = 'bill'

            bill = Bill(session, chamber, bill_id, title, type=bill_type)

            self.scrape_digest(bill)

            # versions
            for a in (tr.xpath('td[6]//a') + tr.xpath('td[9]//a') +
                      tr.xpath('td[10]//a')):
                # skip references to other bills
                if a.text.startswith('See'):
                    continue
                bill.add_version(a.text, a.get('href'),
                                 mimetype='application/pdf')

            # documents
            fnote = tr.xpath('td[7]//a')
            if fnote:
                bill.add_document('Fiscal Note', fnote[0].get('href'))
            summary = tr.xpath('td[12]//a')
            if summary:
                bill.add_document('Summary', summary[0].get('href'))

            bill.add_source(url)
            self.save_bill(bill)

    def scrape_digest(self, bill):
        digest_url = 'http://legisweb.state.wy.us/%(session)s/Digest/%(bill_id)s.htm' % bill

        bill.add_source(digest_url)

        try:
            html = self.urlopen(digest_url)
        except scrapelib.HTTPError:
            self.warning('no digest for %s' % bill['bill_id'])
            return

        doc = lxml.html.fromstring(html)

        ext_title = doc.xpath('//span[@class="billtitle"]')
        if ext_title:
            bill['description'] = ext_title[0].text_content().replace(
                '\r\n', ' ')

        sponsor_span = doc.xpath('//span[@class="sponsors"]')
        sponsors = ''
        if sponsor_span:
            sponsors = sponsor_span[0].text_content().replace('\r\n', ' ')
        else:
            for p in doc.xpath('//p'):
                if p.text_content().lower().startswith('sponsored by'):
                    sponsors = p.text_content().replace('\r\n', ' ')
        if sponsors:
            if 'Committee' in sponsors:
                bill.add_sponsor('primary', sponsors)
            else:
                if bill['chamber'] == 'lower':
                    sp_lists = sponsors.split('and Senator(s)')
                else:
                    sp_lists = sponsors.split('and Representative(s)')
                for spl in sp_lists:
                    for sponsor in split_names(spl):
                        sponsor = sponsor.strip()
                        if sponsor != "":
                            bill.add_sponsor('primary', sponsor)

        action_re = re.compile('(\d{1,2}/\d{1,2}/\d{4})\s+(H |S )?(.+)')
        vote_total_re = re.compile('(Ayes )?(\d*)(\s*)Nays(\s*)(\d+)(\s*)Excused(\s*)(\d+)(\s*)Absent(\s*)(\d+)(\s*)Conflicts(\s*)(\d+)')

        actions_text = [x.text_content() for x in
                        doc.xpath('//p')]
        actions = []
        pastHeader = False
        for action in actions_text:
            action = action.replace(u'\xa0', ' ').replace(u'\xc2', '')
            # XXX: Fix the above, that's a rowdy mess. -- PRT

            if not pastHeader and action_re.match(action):
                pastHeader = True
            if pastHeader:
                actions.append(action)

        # initial actor is bill chamber
        actor = bill['chamber']


        aiter = iter(actions)
        for line in aiter:
            line = clean_line(line)

            # skip blank lines
            if not line:
                continue

            amatch = action_re.match(line)
            if amatch:
                date, achamber, action = amatch.groups()

                # change actor if one is on this action
                if achamber == 'H ':
                    actor = 'lower'
                elif achamber == 'S ':
                    actor = 'upper'

                date = datetime.datetime.strptime(date, '%m/%d/%Y')
                bill.add_action(actor, action, date,
                                type=categorize_action(action))
            elif line == 'ROLL CALL':
                voters = defaultdict(str)
                # if we hit a roll call, use an inner loop to consume lines
                # in a psuedo-state machine manner, 3 types
                # Ayes|Nays|Excused|... - indicates next line is voters
                # : (Senators|Representatives): ... - voters
                # \d+ Nays \d+ Excused ... - totals
                voters_type = None
                for ainext in aiter:
                    nextline = clean_line(ainext)
                    if not nextline:
                        continue

                    breakers = [ "Ayes:", "Nays:", "Nayes:", "Excused:",
                                 "Absent:",  "Conflicts:" ]

                    for breaker in breakers:
                        if nextline.startswith(breaker):
                            voters_type = breaker[:-1]
                            if voters_type == "Nayes":
                                voters_type = "Nays"
                                self.log("Fixed a case of 'Naye-itis'")
                            nextline = nextline[len(breaker)-1:]

                    if nextline.startswith(': '):
                        voters[voters_type] = nextline
                    elif nextline in ('Ayes', 'Nays', 'Excused', 'Absent',
                                      'Conflicts'):
                        voters_type = nextline
                    elif vote_total_re.match(nextline):
                        #_, ayes, _, nays, _, exc, _, abs, _, con, _ = \
                        tupple = vote_total_re.match(nextline).groups()
                        ayes = tupple[1]
                        nays = tupple[4]
                        exc = tupple[7]
                        abs = tupple[10]
                        con = tupple[13]

                        passed = (('Passed' in action or
                                   'Do Pass' in action or
                                   'Did Concur' in action) and
                                  'Failed' not in action)
                        vote = Vote(actor, date, action, passed, int(ayes),
                                    int(nays), int(exc) + int(abs) + int(con))
                        vote.add_source(digest_url)

                        for vtype, voters in voters.iteritems():
                            for voter in split_names(voters):
                                if voter:
                                    if vtype == 'Ayes':
                                        vote.yes(voter)
                                    elif vtype == 'Nays':
                                        vote.no(voter)
                                    else:
                                        vote.other(voter)
                        # done collecting this vote
                        bill.add_vote(vote)
                        break
                    else:
                        # if it is a stray line within the vote, is is a
                        # continuation of the voter list
                        # (sometimes has a newline)
                        voters[voters_type] += ' ' + nextline

########NEW FILE########
__FILENAME__ = committees
from billy.scrape import ScrapeError, NoDataForPeriod
from billy.scrape.committees import CommitteeScraper, Committee

import lxml.html
import re

class WYCommitteeScraper(CommitteeScraper):
    jurisdiction = "wy"

    members = {}
    urls = {
            "list": "http://legisweb.state.wy.us/LegbyYear/CommitteeList.aspx?Year=%s",
            "detail": "http://legisweb.state.wy.us/LegbyYear/%s"
    }

    def scrape(self, chamber, term):
        if chamber == 'lower':
            # Committee members from both houses are listed
            # together. So, we'll only scrape once.
            return None

        year = None

        # Even thought each term spans two years, committee
        # memberships don't appear to change. So we only
        # need to scrape the first year of the term.
        for t in self.metadata["terms"]:
            if term == t["name"]:
                year = t["start_year"]
                break

        if not year:
            raise NoDataForPeriod(term)


        list_url = self.urls["list"] % (year, )
        committees = {}
        page = self.urlopen(list_url)
        page = lxml.html.fromstring(page)
        for el in page.xpath(".//a[contains(@href, 'CommitteeMembers')]"):
            committees[el.text.strip()] = el.get("href")

        for c in committees:
            self.log(c)
            detail_url = self.urls["detail"] % (committees[c],)
            page = self.urlopen(detail_url)
            page = lxml.html.fromstring(page)
            if re.match('\d{1,2}-', c):
                c = c.split('-', 1)[1]
            comm = Committee('joint', c.strip())
            for table in page.xpath(".//table[contains(@id, 'CommitteeMembers')]"):
                rows = table.xpath(".//tr")
                chamber = rows[0].xpath('.//td')[0].text_content().strip()
                chamber = 'upper' if chamber == 'Senator' else 'lower'
                for row in rows[1:]:
                    tds = row.xpath('.//td')
                    name = tds[0].text_content().strip()
                    role = 'chairman' if tds[3].text_content().strip() == 'Chairman' else 'member'
                    comm.add_member(name, role, chamber=chamber)

            comm.add_source(detail_url)
            self.save_committee(comm)

########NEW FILE########
__FILENAME__ = events
import re
import pytz
import datetime
import lxml.html

from billy.scrape.events import EventScraper, Event

class WYEventScraper(EventScraper):
    jurisdiction = 'wy'
    _tz = pytz.timezone('US/Mountain')

    def get_page_from_url(self, url):
        with self.urlopen(url) as page:
            page = lxml.html.fromstring(page)
        page.make_links_absolute(url)
        return page

    def normalize_time(self, time_string):

        time_string = time_string.lower()

        if re.search(r'(upon|after)(\?)? adjournment', time_string):
            time_string = '12:00 am'
        elif re.search(r'(noon (adjournment|recess)|afternoon)', time_string):
            time_string = '12:00 pm'

        if re.search(r'[ap]\.m\.', time_string):
            ap = re.search(r'([ap])\.m\.', time_string).group(1)
            time_string = time_string.replace(ap + '.m.', ap + 'm')

        if re.search(r'[0-9]{1,2}:[0-9]{1,2}[ap]m', time_string):
            hour_minutes, meridiem = re.search(
                r'([0-9]{1,2}:[0-9]{1,2})([ap]m)', time_string).groups()
            time_string = hour_minutes + ' ' + meridiem

        if re.search(
            r'^[0-9]{1,2}:[0-9]{1,2} [ap]m', time_string
        ) and not re.search(r'^[0-9]{1,2}:[0-9]{1,2} [ap]m$', time_string):
            time_string = re.search(
                r'^([0-9]{1,2}:[0-9]{1,2} [ap]m)', time_string).group(1)

        if not re.search(r'^[0-9]{1,2}:[0-9]{1,2} [ap]m$', time_string):
            # if at this point it doesn't match our format return 12:00 am
            time_string = '12:00 am'
        return time_string

    def get_meeting_time(self, meeting_data):
        meeting_time = meeting_data[0].xpath(
            './/p[@class="MsoNormal"]')[0].text_content().strip()
        meeting_time = self.normalize_time(meeting_time)
        return meeting_time

    def get_committee(self, meeting_data):
        committee = meeting_data[0].xpath(
            './/p[@class="MsoNormal"]')[1].text_content().strip()
        if committee == '':
            committee = None
        else:
            committee = re.sub(r'^[0-9]+-','',committee)
            committee = self.clean_string(committee)

        return committee

    def get_location(self, meeting_data):
        tr = meeting_data[0].xpath('.//p[@class="MsoNormal"]')
        room = tr[len(tr)-1].text_content().strip()

        room = self.clean_string(room)
        if room == '':
            room = None

        return room

    def get_meeting_description(self, meeting_data):
        descriptions = ''
        if len(meeting_data) > 1:
            start_at = 1
        else:
            start_at = 0

        for tr in meeting_data[start_at:]:
            description = tr[len(tr)-2].text_content().strip()
            descriptions += ' ' + description

        descriptions = self.clean_string(descriptions).strip()

        return descriptions

    def get_bills(self, meeting_data):

        bill_data = []

        for tr in meeting_data:
            bills = tr.xpath('.//a[contains(@href, "/Bills/")]')
            if bills:
                for bill in bills:
                    bill_id = bill.text_content().strip()
                    bill_description = self.clean_string(
                        tr.xpath('.//td[3]/p')[0].text_content().strip())
                    bill_url = bill.attrib['href'].strip()  #pdf file

                    # dont include bad HTML links for bills. thankfully
                    # they're duplicates and already listed properly
                    if 'href' not in bill_url and '</a>' not in bill_url:
                        bill_data.append({
                            'bill_id': bill_id,
                            'bill_description' : bill_description,
                            'bill_url' : bill_url
                        })
        return bill_data

    def clean_string(self, my_string):
        my_string = my_string.encode('ascii','ignore')
        my_string = re.sub(r'(\n|\r\n)',' ', my_string)
        my_string = re.sub(r'\s{2,}',' ', my_string)
        my_string = my_string.strip()

        return my_string

    def is_row_a_new_meeting(self, row):
        if len(row) == 3:
            td1 = row.xpath('.//td[1]/p[@class="MsoNormal"]')
            td2 = row.xpath('.//td[2]/p[@class="MsoNormal"]')
            td3 = row.xpath('.//td[3]/p[@class="MsoNormal"]')

            if len(td2) == 0:
                td2 = row.xpath('.//td[2]/h1')

            if len(td1) == 0 or len(td2) == 0:
                return False

            if (self.clean_string(td1[0].text_content()) == ''
                    or self.clean_string(td2[0].text_content()) == ''
                    or self.clean_string(td3[0].text_content()) == ''):
                return False
        else:
            return False
        return True

    def scrape(self, chamber, session):
        if chamber == 'other':
            return

        calendar_url = ("http://legisweb.state.wy.us/%s/Calendar/"
            "CalendarMenu/CommitteeMenu.aspx" % str(session))

        page = self.get_page_from_url(calendar_url)

        rows = page.xpath('//table[@id="ctl00_cphContent_gvCalendars"]/tr')

        for i,row in enumerate(rows):

            row_ident = '%02d' % (i + 2)

            date_xpath = ('.//span[@id="ctl00_cphContent_gv'
                'Calendars_ctl%s_lblDate"]' % str(row_ident))
            date_string = row.xpath(date_xpath)[0].text_content()

            chamber_char = self.metadata['chambers'][chamber]['name'][0].upper()
            meeting_xpath = ('.//a[@id="ctl00_cphContent_gv'
                'Calendars_ctl%s_hl%scallink"]' % (
                    str(row_ident), chamber_char
                ))
            meeting_url = row.xpath(meeting_xpath)

            if (len(meeting_url) == 1 and
                    meeting_url[0].text_content().strip() != ''):
                meeting_url = meeting_url[0].attrib['href']
                meeting_page = self.get_page_from_url(meeting_url)
                meetings = meeting_page.xpath(
                    './/table[@class="MsoNormalTable"]/tr')
                meeting_idents = []
                meeting_ident = 0

                # breaking the meetings into arrays (meeting_data) for
                # processing. meeting_ident is the first row of the meeting
                # (time, committee, location)
                for meeting in meetings:
                    if self.is_row_a_new_meeting(meeting):
                        meeting_idents.append(meeting_ident)
                    meeting_ident += 1

                for i,meeting_ident in enumerate(meeting_idents):

                    if len(meeting_idents) == 1 or i + 1 == len(meeting_idents):
                        ident_start, ident_end = [meeting_ident, 0]
                        meeting_data = meetings[ident_start:]
                    else:
                        ident_start, ident_end = [
                            meeting_ident, meeting_idents[i+1] - 1
                        ]

                        if ident_end - ident_start == 1:
                            ident_end = ident_start + 2

                        meeting_data = meetings[ident_start:ident_end]
                    committee = self.get_committee(meeting_data)
                    meeting_time = self.get_meeting_time(meeting_data)
                    meeting_date_time = datetime.datetime.strptime(
                        date_string + ' ' + meeting_time, '%m/%d/%Y %I:%M %p')
                    meeting_date_time = self._tz.localize(meeting_date_time)

                    location = self.get_location(meeting_data)
                    description = self.get_meeting_description(meeting_data)
                    bills = self.get_bills(meeting_data)

                    if description == '':
                        description = committee

                    event = Event(
                        session,
                        meeting_date_time,
                        'committee:meeting',
                        description,
                        location
                    )

                    event.add_source(meeting_url)

                    for bill in bills:

                        if bill['bill_description'] == '':
                            bill['bill_description'] = committee

                        event.add_related_bill(
                            bill_id=bill['bill_id'],
                            description=bill['bill_description'],
                            type='consideration'
                        )
                        event.add_document(
                            name=bill['bill_id'],
                            url=bill['bill_url'],
                            type='bill',
                            mimetype='application/pdf'
                        )

                    event.add_participant(
                        type='host',
                        participant=committee,
                        participant_type='committee',
                        chamber=chamber
                    )

                    self.save_event(event)

########NEW FILE########
__FILENAME__ = legislators
import re

from billy.scrape.legislators import LegislatorScraper, Legislator

import lxml.html


class WYLegislatorScraper(LegislatorScraper):
    jurisdiction = 'wy'

    def scrape(self, chamber, term):
        chamber_abbrev = {'upper': 'S', 'lower': 'H'}[chamber]

        url = ("http://legisweb.state.wy.us/LegislatorSummary/LegislatorList"
               ".aspx?strHouse=%s&strStatus=N" % chamber_abbrev)
        page = lxml.html.fromstring(self.urlopen(url))
        page.make_links_absolute(url)

        for link in page.xpath("//a[contains(@href, 'LegDetail')]"):
            name = link.text.strip()
            leg_url = link.get('href')

            email_address = link.xpath("../../../td[1]//a")[0].attrib['href']
            email_address = link.xpath("../../../td[2]//a")[0].attrib['href']
            email_address = email_address.split('Mailto:')[1]

            party = link.xpath("string(../../../td[3])").strip()
            if party == 'D':
                party = 'Democratic'
            elif party == 'R':
                party = 'Republican'

            district = link.xpath(
                "string(../../../td[4])").strip().lstrip('HS0')

            leg_page = lxml.html.fromstring(self.urlopen(leg_url))
            leg_page.make_links_absolute(leg_url)
            img = leg_page.xpath(
                "//img[contains(@src, 'LegislatorSummary/photos')]")[0]
            photo_url = img.attrib['src']

            office_tds = leg_page.xpath('//table[@id="ctl00_cphContent_tblContact"]/tr/td/text()')
            address = []
            phone = None
            fax = None
            for td in office_tds:
                if td.startswith('Home -'):
                    phone = td.strip('Home - ')
                # only use cell if home isn't present
                elif td.startswith('Cell -') and not phone:
                    phone = td.strip('Cell - ')
                elif td.startswith('Fax -'):
                    fax = td.strip('Fax - ')
                else:
                    address.append(td)

            leg = Legislator(term, chamber, district, name, party=party,
                             email=email_address, photo_url=photo_url,
                             url=leg_url)

            adr = " ".join(address)
            if adr.strip() != "":
                leg.add_office('district', 'Contact Information',
                               address=adr, phone=phone, fax=fax)

            leg.add_source(url)
            leg.add_source(leg_url)

            self.save_legislator(leg)

########NEW FILE########
__FILENAME__ = location
from operator import getitem
from collections import defaultdict


class DefaultdictNode(defaultdict):
    '''A default dict that allows setattr, since
    it needs a `parent` attribute.
    '''
    pass


class LocationSpec(defaultdict):
    '''Basically a recursive defaultdict with helper methods.'''

    def __init__(self, *args, **kwargs):
        super(LocationSpec, self).__init__(*args, **kwargs)
        self.current_node = self
        self._nodetypes = defaultdict(list)

    def __missing__(self, k):
        f = lambda: DefaultdictNode(f)
        res = DefaultdictNode(f)

        # Set self as parent for future reference.
        res.parent = self
        self[k] = res
        return res

    def _setkeys(self, value, *keys):
        keys = list(keys)
        lastkey = keys.pop()
        node = reduce(getitem, [self] + list(keys))
        node[lastkey] = value
        self.current_node = node

    def _getkeys(self, *keys):
        node = reduce(getitem, [self] + list(keys))
        self.current_node = node
        return node

    def _get_recent_nodetype(self, nodetype):
        nodetype = nodetype.lower()
        nodetype = self._nodetypes[nodetype]
        if nodetype:
            # Got most recently added node for this type.
            return nodetype[-1].parent
        else:
            return self

    def _log_node(self, nodetype, child):
        self._nodetypes[nodetype.text.lower()].append(child)

    def add_parallel_nodes(self, nodetype, tokens):
        '''Add tokens containing nodenum information into the
        tree hierarchy of the statute. Log added nodes in the
        _nodetypes dict.
        '''
        node = self._get_recent_nodetype(nodetype.text)
        for token in tokens:
            child = node[token.text]
            self._log_node(nodetype, child)

    def add_path(self, tokens):
        '''Where tokens is a sequence of (nodetype, node_enum) 2-tuples.
        '''
        tokens = list(tokens)
        nodetype, node_enum = tokens[0]
        node = self._get_recent_nodetype(nodetype.text)
        for nodetype, node_enum in tokens:
            node = node[node_enum.text]
            self._log_node(nodetype, node)

    def finalize(self):
        terminal_value = self['impact_verb']

        def _convert_to_dict(_defaultdict, terminal_value=terminal_value):
            res = {}
            for k, v in _defaultdict.items():
                if v:
                    if isinstance(v, dict):
                        res[k] = _convert_to_dict(v)
                    else:
                        res[k] = v
                else:
                    res[k] = terminal_value
            return res

        return _convert_to_dict(self)

########NEW FILE########
__FILENAME__ = parser
import collections
import contextlib
import itertools

from pygments.token import *
import logbook

from .location import LocationSpec


LEVEL = 0
logger = logbook.Logger('parser', level=LEVEL)


class ParseError(Exception):
    pass


Token = collections.namedtuple('Token', 'pos tokentype text')


class Stream(object):

    def __init__(self, iterable, filters=[], reverse=None):

        # Wrap tokens in Token class.
        stream = map(Token._make, iterable)

        # Filter junk from the stream.
        filters = getattr(self, 'filters', filters)
        for func in filters:
            stream = filter(func, stream)

        # Reverse the tokens if necessary.
        if reverse is None:
            reverse = getattr(self, 'reverse', False)
            if reverse:
                stream = stream[::-1]

        self.i = 0
        self.last = len(stream)
        self._stream = stream

    def __iter__(self):
        while True:
            try:
                yield self._stream[self.i]
            except IndexError:
                raise StopIteration
            finally:
                self.i += 1

    def __repr__(self):
        return 'Stream(i=%r, %r)' % (self.i, self._stream)

    def next(self):
        if self.exhausted:
            raise StopIteration
        try:
            return self._stream[self.i]
        except IndexError:
            raise StopIteration
        finally:
            self.i += 1

    def previous(self):
        return self.behind(1)

    def this(self):
        try:
            return self._stream[self.i]
        except IndexError:
            raise StopIteration

    def ahead(self, n=1):
        try:
            return self._stream[self.i + n]
        except IndexError:
            return

    def behind(self, n=1):
        i = self.i - 1
        if i < 0:
            return
        try:
            return self._stream[i]
        except IndexError:
            return

    @property
    def exhausted(self):
        '''Stream iterator is through? Yep/nope
        '''
        return self.last <= self.i


class Parser(object):

    class SequenceMismatch(ParseError):
        '''Raised if expected sequence differs from
        actual sequence found.
        '''

    def __init__(self, tokenstream, parser_state=None):

        # Apply stream class.
        if not isinstance(tokenstream, (Stream,)):
            stream_cls = getattr(self, 'stream_cls', Stream)
            tokenstream = stream_cls(tokenstream)
        self.stream = tokenstream

        if parser_state is not None:
            self.state = parser_state()
        else:
            self.state = ParserState()
        self._compile_rules()

    def _compile_rules(self):
        if not hasattr(self, 'rules'):
            return
        compiled_rules = {}
        parser_state = self._parser_state
        for state, state_rules in self.rules.items():
            compiled_state_rules = {}
            for data in state_rules:
                states = None
                len_data = len(data)
                if len_data == 3:
                    token, func_names, states = data
                elif len_data == 2:
                    token, func_names = data
                funcs = [getattr(parser_state, f) for f in func_names.split()]
                compiled_state_rules[token] = {'funcs': funcs}
                if states is not None:
                    compiled_state_rules[token]['states'] = states
                compiled_rules[state] = compiled_state_rules
        self._rules = compiled_rules

    def parse(self):
        rules = self._rules
        state_stack = []
        ignored_token_types = self.ignored_token_types

        for pos, token, text in self.stream:
            logger.debug('data: %r' % [pos, token, text])

            if token in ignored_token_types:
                logger.debug('ignoring %r %r' % (token, text))
                continue

            try:
                state = state_stack[-1]
            except IndexError:
                state = 'root'
            logger.debug('entering state: %r' % state)

            state_rules = rules[state][token]

            while True:
                try:
                    funcs = state_rules['funcs']
                except KeyError:
                    if state_stack:
                        state = state_stack.pop()
                        logger.debug('pop state: %r' % state)
                    else:
                        msg = 'No rules available for %r in state %r.'
                        raise ParseError(msg % (token, state))
                else:
                    for f in funcs:
                        logger.debug('calling %r(%r)' % (f, text))
                        f(text)
                    break

            if 'states' in state_rules:
                _states = state_rules['states']
                if isinstance(_states, basestring):
                    _states = [_states]
                for st in _states:
                    if st == '#pop':
                        logger.debug('pop state: %r' % state_stack[-1])
                        state_stack.pop()
                    else:
                        logger.debug('push state: %r' % st)
                        state_stack.append(st)

        return dict(self._parser_state)

    def _take_tokentype_sequence(self, sequence, repeat=False,
            flat=False, ignore=None):
        '''Return tokens from the stream if they match the given sequence.
        '''
        result = []
        repeat_sequence = repeat
        repeat_result = []
        ignored_tokentypes = ignore or []
        while True:
            _sequence = list(sequence[::-1])
            while _sequence:
                expected_type = _sequence.pop()
                logger.debug('  expected_type: %r' % expected_type)
                while True:
                    try:
                        token = self.stream.next()
                    except StopIteration:
                        raise self.SequenceMismatch('Reached end of stream.')
                    if token.tokentype not in ignored_tokentypes:
                        logger.debug('  found: %r' % (token,))
                        break
                    logger.debug('  ignoring: %r' % (token,))
                if token.tokentype == expected_type:
                    result.append(token)
                else:
                    msg = '  bailing (wrong tokentype): %r'
                    logger.debug(msg % [token])
                    if repeat_sequence is False:
                        raise self.SequenceMismatch('Seqs didn\'t match.')
                    elif repeat_result:
                        # Manually roll-back the stream index 1 position.
                        self.stream.i -= 1
                        if flat:
                            return _flatten(repeat_result)
                        return repeat_result
                    else:
                        raise self.SequenceMismatch('Seqs didn\'t match.')

            if repeat_sequence is False:
                return result
            else:
                repeat_result.append(result)
                result = []
        if flat:
            return _flatten(repeat_result)
        return repeat_result

    @contextlib.contextmanager
    def expect_seq(self, *tokentypes, **kwargs):
        '''Look ahead into the stream, and if the tokens found match
        the given tokentype sequence, yield; otherwise, return.
        '''
        i = self.stream.i
        try:
            logger.debug('expect_seq: *%r' % (tokentypes,))
            yield self._take_tokentype_sequence(tokentypes, **kwargs)
        except self.SequenceMismatch:
            # Reset the stream position.
            logger.debug('failed: expect_seq: *%r' % (tokentypes,))
            logger.debug('reseting stream index to %d' % i)
            self.stream.i = i
            yield

    @contextlib.contextmanager
    def expect_one(self, tokentype):
        '''Look ahead into the stream, and if the tokens found match
        the given tokentype sequence, yield; otherwise, return.
        '''
        logger.debug('expect_one: %r' % [tokentype])
        token = self.stream.this()
        logger.debug('  found: %r' % [token])
        if token.tokentype == tokentype:
            yield self.stream.next()
        else:
            logger.debug('failed: expect_one: %r' % [tokentype])
            yield

    # @contextlib.contextmanager
    # def expect_subnode_series(self):
    #     i = self.stream.i

    #     seq = (Token.NodeType, Token.NodeEnum)
    #     try:
    #         yield self._take_tokentype_sequence(tokentypes, **kwargs)
    #     except self.SequenceMismatch:
    #         # Reset the stream position.
    #         self.stream.i = i
    #         yield


class ParserState(dict):

    def __init__(self):

        # The location spec.
        self['loc'] = LocationSpec()

    def validate(self):
        '''Check that all the required attrs have been
        added; raise errors if not.
        '''
        required_keys = set(['rsrc', 'loc'])
        undefined_keys = set(required_keys) - set(self)
        if undefined_keys:
            msg = ("The following keys are required but weren't ",
                   "added during the parse: %r")
            raise ParseError(msg % undefined_keys)

    def finalize(self):
        return dict(self)

    def path_set_compilation_name(self, text=None, *args, **kwargs):
        self['type'] = 'statute'
        self['id'] = text

    def path_set_session_law_year(self, text=None, *args, **kwargs):
        self['type'] = 'session_law'
        self['year'] = text

    def path_set_session_law_chapter(self, text=None, *args, **kwargs):
        self['type'] = 'session_law'
        self['chapter'] = text

    def path_set_act_name(self, text=None, *args, **kwargs):
        self['act_name'] = text.strip(', ')

    def amended_by_adding(self, text=None, *args, **kwargs):
        paths = []
        path = []
        node = {}
        path.append(node)
        paths.append(path)
        self._current_node = node
        self._current_path = path
        self['impact'] = 'added'
        self['details'] = paths

    def amended_as_follows(self, text=None, *args, **kwargs):
        self['impact'] = 'amended'

    def renumbered(self, text=None, *args, **kwargs):
        paths = []
        path = []
        node = {}
        path.append(node)
        paths.append(path)
        self._current_node = node
        self._current_path = path
        self['impact'] = 'renumbered'
        self['details'] = paths


def parse(string):
    tokens = ParagraphLexer().get_tokens_unprocessed(string)
    parsed = Parser(tokens).parse()
    return parsed


def _flatten(iterable):
    return list(itertools.chain.from_iterable(iterable))

########NEW FILE########
__FILENAME__ = enumerations
enums = ['paragraph', 'division', 'chapter', 'section', 'clause',
         'article', 'part', 'rule']
enums += ['sub' + s for s in enums]

regex = r'(%s)' % '|'.join(sorted(enums, key=len, reverse=True))

########NEW FILE########
__FILENAME__ = fl
from pygments.lexer import RegexLexer, bygroups, include
from pygments.token import *

from .base import parser
from . import enumerations

t = Token

print enumerations.regex


class Lexer(RegexLexer):

    tokens = {
        'root': [
            include('impact'),
            include('_conjunctions'),
            include('nodetypes'),
            include('junk'),
            (r'(?i)present', t.Present),
            (r'of ', t.Of),
            (r'(?i)respectively', t.Respectively),
            include('junk'),
            ],

        'impact': [
            (r'(?:are|is) (creat|amend|renumber|add|repeal)ed( to )?',
                bygroups(t.ImpactVerb)),
            ],

        'nodetypes': [

            (r'(?i)Section\s+([^ ,]+), (Florida Statutes),',
                bygroups(t.SectionEnum, t.Code)),

            # Match plural path elements.
            (r'(?i)%ss ' % enumerations.regex,
                bygroups(t.NodeType.Plural), 'path'),

            (r'chapter (\s+), Laws of Florida', bygroups(t.ChapterLaw)),

            # Match singular path elements.
            (r'(?i)%s' % enumerations.regex, bygroups(t.NodeType), 'path'),
            ],

        'path': [
            include('impact'),
            include('_conjunctions'),
            include('nodetypes'),
            include('junk'),
            (r'(?i)respectively', t.Respectively, '#pop'),
            (r'(?i)a new', t.New, '#pop'),
            (r'of ', t.Of, '#pop'),
            (r'(?i)(that) (%s)' % enumerations.regex,
                bygroups(t.That, t.NodeType), '#pop'),
            (r'through', t.NodeSpan),
            include('impact'),
            include('_conjunctions'),
            include('nodetypes'),
            (r'\(?([^ \),]+)\)?', bygroups(t.NodeEnum)),
            ],

        '_conjunctions': [
            (r',? and ', t.And),
            (r', ', t.Comma),
            ],

        'junk': [
            (r'to read:', t.Error),
            (r'read:', t.Error),
            ]
    }


class Stream(parser.Stream):
    filters = [lambda token: token.tokentype != Token.Error]


class Parser(parser.Parser):

    stream_cls = Stream

    def parse(self):

        # Section 1003.46, Florida Statutes, is amended to read:
        while True:

            if self.stream.exhausted:
                break

            section_code_verb = self.expect_seq(t.SectionEnum, t.Code, t.ImpactVerb)
            with section_code_verb as tokens:
                if tokens:
                    section_enum, code, impact = tokens
                    self.state['loc']['section_enum'] = section_enum.text
                    self.state['loc']['code'] = 'stat'
                    self.state['loc']['impact_verb'] = impact.text
                    continue

            # Paragraph (d) of subsection (3) of section 1002.20...:
            node_path = self.expect_seq(t.NodeType, t.NodeEnum,
                                        repeat=True, ignore=[t.Of])
            with node_path as tokens:
                if tokens:
                    self.state['loc'].add_path(reversed(tokens))
                    continue

            nodetype_plural = self.expect_one(t.NodeType.Plural)

            node_series = self.expect_seq(t.NodeEnum, repeat=True, flat=True,
                                          ignore=[t.Comma, t.And])
            node_path = self.expect_seq(t.NodeType, t.NodeEnum,
                                        repeat=True, ignore=[t.Of])
            with nodetype_plural as nodetype, node_series as tokens, \
                    self.expect_one(t.Of) as of, node_path as path:
                    if nodetype and tokens and of and path:
                        a = 1
                        import pdb;pdb.set_trace()
                        b = 2
                        continue

            nodetype_plural = self.expect_one(t.NodeType.Plural)

            node_series = self.expect_seq(t.NodeEnum, repeat=True, flat=True,
                                          ignore=[t.Comma, t.And])
            with nodetype_plural as nodetype, node_series as tokens:
                if tokens or nodetype:
                    self.state['loc'].add_parallel_nodes(nodetype, tokens)
                    continue

            try:
                self.stream.next()
            except StopIteration:
                break

########NEW FILE########
__FILENAME__ = ny
import copy

from pygments.lexer import RegexLexer, bygroups
from pygments.token import *

import .base


SectionID = Token.Section.ID
NodeType = Token.Node.Type
NodeID = Token.Node.ID
NodeAndOrComma = Token.Node.AndOrComma
DiffSpec = Token.DiffSpec
AmendedAsFollows = DiffSpec.AmendedAsFollows
AmendedByAdding = DiffSpec.AmendedByAdding
Renumbered = DiffSpec.Renumbered
SessionLawChapter = Token.SessionLawChapter
SessionLawYear = Token.SessionLawYear
ActName = Token.ActName
CompilationName = Token.CompilationName
Junk = Token.Junk

subds = ['paragraph', 'division', 'chapter', 'section', 'clause',
         'article', 'part']
subds += ['sub' + s for s in subds]
subds = r'(%s)' % '|'.join(sorted(subds, key=len, reverse=True))


class Lexer(RegexLexer):

    tokens = {
        'root': [

            # Match 'Section 1' and 'S 2' section headings.
            (r' +Section (1).\s+', bygroups(SectionID)),
            (r' {1,2}S {1,2}(\d+)\.', bygroups(SectionID)),

            # Match singular path elements.
            (r'(?i)(?: of (?:a )?)?(%s) ' % subds,
                bygroups(NodeType), 'path'),

            # Match plural path elements.
            (r'(?i)(%s)s ' % subds, bygroups(NodeType.Plural), 'path'),

            # Match act name.
            (r', constituting the (.{,250}? act),', bygroups(ActName)),

            # Amended as follows variants.
            (r' (is|are) amended to read as follows:', AmendedAsFollows),
            (r'amended to read as follows:', AmendedAsFollows),

            # Amended by adding variants.
            (r' (is|are) amended and \w+ new', AmendedByAdding, 'path'),
            (r' is amended by adding', AmendedByAdding, 'path'),
            (r'amended by adding', AmendedByAdding, 'path'),


            # Compilation name.
            (r'(?i)the ([A-Za-z .&]+ (:?law|rules|code of the city of New York))',
             bygroups(CompilationName)),

            (r'(added|amended|renumbered) by',
            # (r',? (:?(:?as|and) )?(added|amended|renumbered) by',
                Token.RevisionSpec, 'path'),
            # Junk.
            # (r'amending [^,]+', Junk, 'junk'),
            # (r'(added|amended|renumbered) [^,]+', Junk, 'junk'),
            # (r'%s .{,200}? as (?:added|amended) by[^,]+?, ' % subds, Token.Junk),
            # Renumbered variants.
            (r' is renumbered', Renumbered),
            (r'renumbered', Renumbered),
            (r'\band\b', Token.And)
            ],

        'path': [
            (r',? (:?(:?as|and) )?(added|amended|renumbered) by', Token.RevisionSpec),

            (r' local law number (\w+) of the city of (.+?) for the year (\w+)',
                bygroups(Token.LocalLaw.Number,
                         Token.LocalLaw.Jxn,
                         Token.LocalLaw.Year), '#pop'),

            (r' local law number (\w+)',
                bygroups(Token.LocalLaw.Number), '#pop'),

            # "of the codes and ordinances of the city of Yonkers..."
            (r' of the (.+?) of the city of (.+?)(?:,|is)',
                bygroups(Token.MunicipalLaw.Name, Token.MunicipalLaw.Jxn), '#pop'),

            (r' of the laws of (\d{4})', bygroups(SessionLawYear), '#pop'),
            (r'(?i)of the ([A-Za-z \-.&]+ (:?law|rules|code of the city of New York))',
             bygroups(CompilationName), '#pop'),
            (r'(?i)(?: of (?:a )?)?(%s) ' % subds,
                bygroups(NodeType)),
            (r'are added', Token.Added, '#pop'),
            (r'to read as follows', Junk, '#pop'),
            (r'of ', Token.Of, '#pop'),
            (r'[^ ,]+', NodeID),
            (r',? and ', NodeAndOrComma),
            (r', ', NodeAndOrComma),
            ],

        'junk': [
            (r'(?!(is|are) (amended|renumbered|repealed)).', Junk),
            (r'(is|are) amended to read as follows:', AmendedAsFollows, '#pop'),
            (r' (is|are) amended and \w+ new', AmendedByAdding, 'path'),
            (r' is amended by adding', AmendedByAdding, '#pop'),
            (r'is renumbered', Renumbered, '#pop'),
            ]
    }


class ParserState(dict):

    def __init__(self):
        self._current_path = None
        self._current_node = None
        self['paths'] = []

    def finalize(self):
        return dict(self)

    def section_set_id(self, text=None, *args, **kwargs):
        self['section'] = text

    def path_new(self, text=None, *args, **kwargs):
        path = []
        self['paths'].append(path)
        self._current_path = path
        return path

    @property
    def path_current(self, text=None, *args, **kwargs):
        if self._current_path is None:
            return self.path_new()
        else:
            return self._current_path

    def node_new(self, text=None, *args, **kwargs):
        node = {}
        self.path_current.append(node)
        self._current_node = node
        return node

    @property
    def node_current(self, text=None, *args, **kwargs):
        if self._current_node is None:
            return self._new_node()
        else:
            return self._current_node

    def node_set_id(self, text=None, *args, **kwargs):
        text = text.rstrip('.')
        self.node_current['id'] = text

    def node_set_type(self, text=None, *args, **kwargs):
        text = text.lower().rstrip('s')
        node_current = self.node_current
        if 'type' not in node_current:
            node_current['type'] = text
        else:
            self.node_new()['type'] = text

    def path_clone(self, text=None, *args, **kwargs):
        new_path = copy.deepcopy(self.path_current)
        self['paths'].append(new_path)
        self._current_path = new_path
        self._current_node = new_path[-1]
        return new_path

    def path_set_compilation_name(self, text=None, *args, **kwargs):
        self['type'] = 'statute'
        self['id'] = text

    def path_set_session_law_year(self, text=None, *args, **kwargs):
        self['type'] = 'session_law'
        self['year'] = text

    def path_set_session_law_chapter(self, text=None, *args, **kwargs):
        self['type'] = 'session_law'
        self['chapter'] = text

    def path_set_act_name(self, text=None, *args, **kwargs):
        self['act_name'] = text.strip(', ')

    def amended_by_adding(self, text=None, *args, **kwargs):
        paths = []
        path = []
        node = {}
        path.append(node)
        paths.append(path)
        self._current_node = node
        self._current_path = path
        self['impact'] = 'added'
        self['details'] = paths

    def amended_as_follows(self, text=None, *args, **kwargs):
        self['impact'] = 'amended'

    def renumbered(self, text=None, *args, **kwargs):
        paths = []
        path = []
        node = {}
        path.append(node)
        paths.append(path)
        self._current_node = node
        self._current_path = path
        self['impact'] = 'renumbered'
        self['details'] = paths


class Parser(base.Parser):

    ignored_token_types = [Token.Error]

    rules = {
        'root': [
            (SectionID, 'section_set_id'),
            (NodeType, 'path_new node_new node_set_type', 'path'),
            (NodeType.Plural, 'path_new_parallel node_new node_set_type', 'path'),
            (AmendedAsFollows, 'amended_as_follows'),
            (AmendedByAdding, 'amended_by_adding', 'path'),
            (Renumbered, 'renumbered', 'path'),
            (ActName, 'path_set_act_name'),
            (Junk, ''),
            (CompilationName, 'path_set_compilation_name'),
            ],

        'path': [
            (SessionLawYear, 'path_set_session_law_year', '#pop'),
            (CompilationName, 'path_set_compilation_name', '#pop'),
            (NodeID, 'node_set_id'),
            (NodeType, 'node_set_type'),
            (NodeAndOrComma, 'path_clone'),
            (Junk, '', '#pop')
            ],
        }

########NEW FILE########
__FILENAME__ = utils
import os
from os.path import join
import pprint

from pygments.token import *

from billy.conf import settings


def get_billtext(abbr):
    '''A generator of billtext for the given state.
    '''
    DATA = join(settings.BILLY_DATA_DIR, abbr, 'billtext')
    for filename in os.listdir(DATA):
        filename = join(DATA, filename)
        with open(filename) as f:
            text = f.read()
        yield filename, text


def parse(lexer_class, parser_class, parser_state, string):
    print string
    tokens = list(lexer_class().get_tokens_unprocessed(string))
    pprint.pprint(tokens)
    # return tokens
    parser = parser_class(tokens, parser_state)
    parser.parse()
    result = parser.state['loc'].finalize()
    pprint.pprint(result)
    import pdb;pdb.set_trace()

########NEW FILE########
__FILENAME__ = fetch_fulltext
import sys
import os
from os.path import join

from billy import db
from billy.conf import settings
import scrapelib

import logbook


def main(abbr):

    request_defaults = {
        # 'proxies': {"http": "localhost:8888"},
        'timeout': 5.0,
        'headers': {
            'Accept': ('text/html,application/xhtml+xml,application/'
                       'xml;q=0.9,*/*;q=0.8'),
            'Accept-Encoding': 'gzip, deflate',
            'Accept-Language': 'en-us,en;q=0.5',
            'User-Agent': ('Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:10.0.2) '
                           'Gecko/20100101 Firefox/10.0.2'),
            },
        'follow_robots': False,

        # Note, this script needs run in the same dir as billy_settings.py
        }

    logger = logbook.Logger()
    DATA = join(settings.BILLY_DATA_DIR, abbr, 'billtext')

    try:
        os.makedirs(DATA)
    except OSError:
        pass
    logger.info('writing files to %r' % DATA)

    session = scrapelib.Scraper(
        cache_obj=scrapelib.FileCache('cache'),
        cache_write_only=False,
        use_cache_first=True,
        requests_per_minute=0,
        **request_defaults)

    for bill in db.bills.find({'state': abbr}):
        if len(bill['versions']):
            bill_id = bill['bill_id']
            url = bill['versions'][0]['url']
            logger.info('trying %r: %r' % (bill_id, url))
            text = session.get(url).text
            with open(join(DATA, bill['_id']), 'w') as f:
                f.write(text.encode('utf-8'))


if __name__ == '__main__':
    main(sys.argv[1])

########NEW FILE########
__FILENAME__ = fl-debug
import re
import webbrowser
import collections

import lxml.html
import logbook

from core.utils import parse, get_billtext
from core.fl import Lexer, Parser


logger = logbook.Logger('fl-debug')
Section = collections.namedtuple('Section', 'enum content')


def extract_sections(text):
    doc = lxml.html.fromstring(text)
    text = '\n'.join(n.text_content() for n in doc.xpath('//td[2]'))
    text = text.replace(u'\xa0', ' ')
    # Note, currently skips last section (usually effective date).
    matches = re.finditer('     Section (\d\w*)\.\s+(.+?)(?:\n     )', text, re.S)
    for m in matches:
        enum = m.group(1)
        content = re.sub(r'\s+', ' ', m.group(2))
        yield Section(enum, content)


def main():
    for filename, text in get_billtext('fl'):
        logger.info('extracting sections: %r' % filename)
        # webbrowser.open('file:///%s' % filename)
        for section in extract_sections(text):
            section_text = section.content
            print section_text
            if 'repeal' in section_text.lower() or 'add' in section_text.lower():
                # import pdb;pdb.set_trace()
                tokens = parse(Lexer, Parser, None, section_text)
                import pdb;pdb.set_trace()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = fl-test
s = '''
Subsections (1), (2), (3), (4), and (6) and paragraph (c) of subsection (7) of section 1002.69, Florida Statutes, are amended to read:
Section 1003.46, Florida Statutes, is amended to read:
Paragraph (d) of subsection (3) of section 1002.20, Florida Statutes, is amended to read:
Subsection (10) of section 447.203, Florida Statutes, is amended to read:
Paragraph (a) of subsection (4) of section 1001.20, Florida Statutes, is amended to read:
Paragraphs (b) and (e) of subsection (1) and subsections (2) and (4) of section 1006.33, Florida Statutes, are amended to read:
Subsection (1), paragraph (a) of subsection (2), and paragraphs (b) and (e) of subsection (3) of section 1006.28, Florida Statutes, are amended to read:
Subsections (1), (2), (3), and (7) of section 1006.34, Florida Statutes, are amended to read:
Subsection (2), paragraph (a) of subsection (3), and subsection (4) of section 1006.40, Florida Statutes, are amended to read:
Paragraph (p) of subsection (1) and paragraph (b) of subsection (6) of section 1011.62, Florida Statutes, are amended to read:
Paragraph (b) of subsection (3) and subsection (4) of section 1008.33, Florida Statutes, are amended to read:

Subsection (23) of section 1001.42, Florida Statutes, is amended to read:
Paragraph (b) of subsection (5) of section 1002.33, Florida Statutes, is amended to read:
Paragraph (a) of subsection (1) of section 1002.37, Florida Statutes, is amended to read:
Paragraph (f) is added to subsection (3) of section 1002.38, Florida Statutes, to read:
Paragraph (b) of subsection (2) of section 1002.45, Florida Statutes, is amended to read:
Subsection (1) and paragraph (c) of subsection (3) of section 1002.67, Florida Statutes, are amended to read:
Subsection (2) of section 1002.73, Florida Statutes, is amended to read:
Paragraph (c) of subsection (4) of section 1003.03, Florida Statutes, is amended to read:
Subsection (1) of section 1003.4156, Florida Statutes, is amended to read:
Section 1003.4203, Florida Statutes, is created to read:
Subsection (2) of section 1003.428, Florida Statutes, is amended to read:
Subsection (1) of section 1003.492, Florida Statutes, is amended to read:
Section 1003.493, Florida Statutes, is amended to read:
Section 1003.575, Florida Statutes, is amended to read:
Subsection (2) of section 1003.621, Florida Statutes, is amended to read:
Section 1006.29, Florida Statutes, is amended to read:
Section 1006.30, Florida Statutes, is amended to read:
Section 1006.31, Florida Statutes, is amended to read:
Section 1006.32, Florida Statutes, is amended to read:
Subsection (2) of section 1006.35, Florida Statutes, is amended to read:
Section 1006.36, Florida Statutes, is amended to read:
Section 1006.37, Florida Statutes, is repealed.
Subsection (5) of section 1006.39, Florida Statutes, is amended to read:
Section 1006.43, Florida Statutes, is amended to read:
Effective upon this act becoming a law, subsection (2) and paragraph (c) of subsection (3) of section 1008.22, Florida Statutes, are amended to read:
Subsection (3) of section 1008.34, Florida Statutes, is amended to read:
Paragraph (a) of subsection (3) of section 1011.01, Florida Statutes, is amended to read:
Subsection (4) of section 1011.03, Florida Statutes, is amended to read:
Subsection (1) of section 1011.61, Florida Statutes, is amended to read:
Subsection (1) of section 1012.39, Florida Statutes, is amended to read:'''


def main():
    from core.utils import parse
    from core.fl import Lexer, Parser
    for section in filter(None, s.splitlines()):
        tokens = parse(Lexer, Parser, None, section)
        print section

if __name__ == '__main__':
    main()
########NEW FILE########
__FILENAME__ = ny-debug
import os
import re
import pprint
from functools import partial
from os.path import join

from utils import parse
from ny import Lexer, Parser, ParserState


DATA = '/home/thom/data/ny_billtext/data'


def extract_sections(text):

    # Slice beginning crap.
    _, text = text.split('DO ENACT AS FOLLOWS:')

    # Kill line numbers.
    text = re.sub(r' {3,4}\d+ {2}', '', text)

    paragraphs = []
    text = iter(text.splitlines())
    lines = []
    while True:
        try:
            line = next(text)
        except StopIteration:
            paragraphs.append(' '.join(lines))
            break

        lines.append(line)
        if len(line) != 72:
            paragraphs.append(' '.join(lines))
            lines = []

    def filterfunc(s):
        return (not (s.isupper() or ('shall take effect' in s)) \
                and (re.search(r'^  Section +1.', s) or \
                     re.search(r'^  S {1,2}\d+', s)))

    paragraphs = filter(filterfunc, paragraphs)
    paragraphs = map(partial(re.sub, r'\s+', ' '), paragraphs)

    return paragraphs


def main():
    for filename in os.listdir(DATA):
        filename = join(DATA, filename)
        with open(filename) as f:
            text = f.read()
            sections = extract_sections(text)
            for s in sections:
                print s
                parsed = parse(Lexer, Parser, ParserState, s)
                print s
                print filename
                pprint.pprint(parsed)
            import pdb;pdb.set_trace()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = fetch_fulltext
'''
Given a state as the first parameter, this module tries to
fetch bill text for all the state's bills and store them
in settings.BILLY_DATA_DIR. Scrapelib caching is on.
'''
import sys
import os
from os.path import join

from billy import db
from billy.conf import settings
import scrapelib

import logbook


def main(abbr):

    request_defaults = {
        # 'proxies': {"http": "localhost:8888"},
        'timeout': 5.0,
        'headers': {
            'Accept': ('text/html,application/xhtml+xml,application/'
                       'xml;q=0.9,*/*;q=0.8'),
            'Accept-Encoding': 'gzip, deflate',
            'Accept-Language': 'en-us,en;q=0.5',
            'User-Agent': ('Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:10.0.2) '
                           'Gecko/20100101 Firefox/10.0.2'),
            },
        'follow_robots': False,

        # Note, this script needs run in the same dir as billy_settings.py
        }

    logger = logbook.Logger()
    DATA = join(settings.BILLY_DATA_DIR, abbr, 'billtext')

    try:
        os.makedirs(DATA)
    except OSError:
        pass
    logger.info('writing files to %r' % DATA)

    session = scrapelib.Scraper(
        cache_obj=scrapelib.FileCache('cache'),
        cache_write_only=False,
        use_cache_first=True,
        requests_per_minute=0,
        **request_defaults)

    for bill in db.bills.find({'state': abbr}):
        if len(bill['versions']):
            bill_id = bill['bill_id']
            url = bill['versions'][0]['url']
            logger.info('trying %r: %r' % (bill_id, url))
            text = session.get(url).text
            with open(join(DATA, bill['_id']), 'w') as f:
                f.write(text.encode('utf-8'))


if __name__ == '__main__':
    main(sys.argv[1])

########NEW FILE########
__FILENAME__ = ny-tests
import unittest
import pprint

from ny import Lexer, Parser, ParserState
from utils import parse


class TestRelatedCitation(unittest.TestCase):

    maxDiff = None

    def test_parse_all(self):
        for string, data in samples:
            _data = parse(Lexer, Parser, ParserState, string)
            import pdb;pdb.set_trace()
            # pprint.pprint(data)
            # pprint.pprint(_data)
            #self.assertEqual(data, _data)


samples = [

    ('Section 32 of the labor law is amended to read as follows:',
        {
            'type': 'statute',
            'id': 'labor law',
            'paths': [
                [{'type': 'section', 'id': '32'}]
                ],
            'impact': 'amended'
        }
    ),

    (('Section 191-b of the labor law, as added by chapter 451 of '
      'the laws of 1987, is amended to read as follows:'),
        {
            'type': 'statute',
            'id': 'labor law',
            'paths': [
                [{'type': 'section', 'id': '191-b'}]
                ],
            'impact': 'amended'
        }
    ),

    (('Subdivision 1 of section 100 of the labor law, as amended '
      'by chapter 757 of the laws of 1975, is amended to read as follows:'),
        {
            'type': 'statute',
            'id': 'labor law',
            'paths': [
                [{'type': 'subdivision', 'id': '1'},
                 {'type': 'section', 'id': '100'}]
                ],
            'impact': 'amended'
        }
    ),

    (('Subdivision 1 of section 21 of the labor law, added by section '
      '146 of part B of chapter 436 of the laws of 1997 and renumbered by '
      'chapter 214 of the laws of 1998, is amended to read as follows:'),
        {
            'type': 'statute',
            'id': 'labor law',
            'paths': [
                [{'type': 'subdivision', 'id': '1'},
                 {'type': 'section', 'id': '21'}]
                ],
            'impact': 'amended'
        }
    ),

    (('Section 57-0131 of the environmental conservation law, as amended '
      'by chapter 286 of the laws of 1998, is amended to read as follows:'),
        {
            'type': 'statute',
            'id': 'environmental conservation law',
            'paths': [[{'type': 'section', 'id': '57-0131'}]],
            'impact': 'amended'
        }
    ),

    (('Subdivision 4 of section 30 of the labor law, as amended by '
      'chapter 756 of the laws of 1975 and renumbered by chapter 162 '
      'of the laws of 1993, is amended to read as follows:'),
        {
            'id': 'labor law',
            'type': 'statute',
            'paths': [
                [{'type': 'subdivision', 'id': '4'},
                 {'type': 'section', 'id': '30'}]
                ],
            'impact': 'amended'
        }
    ),

    ('Section 30 of the labor law is renumbered section 60.',
        {
            'id': 'labor law',
            'type': 'statute',
            'paths': [
                [{'type': 'section', 'id': '30'}]
                ],
            'impact': 'renumbered',
            'details': [
                [{'type': 'section', 'id': '60'}]
                ]
        }
    ),

    (('Subdivision 1 of section 20 of chapter 784 of the laws of 1951, '
      'constituting the New York state defense emergency act, is '
      'amended to read as follows:'),
      {
          'act_name': 'New York state defense emergency act',
          'impact': 'amended',
          'paths': [
                [{'id': '1', 'type': 'subdivision'},
                 {'id': '20', 'type': 'section'},
                 {'id': '784', 'type': 'chapter'}]
                ],
         'type': 'session_law',
         'year': '1951'}
    ),

    (('Subdivision 1 of section 20 of chapter 784 of the laws of '
      '1951, constituting the New York state defense emergency act, '
      'as amended by chapter 3 of the laws of 1961, is amended to '
      'read as follows:'),
        {
            'year': '1951',
            'type': 'session_law',
            'paths': [
                [{'type': 'subdivision', 'id': '1'},
                 {'type': 'section', 'id': '20'},
                 {'id': '784', 'type': 'chapter'}],
                ],
            'act_name': 'New York state defense emergency act',
            'impact': 'amended',
        }
    ),

    (('Section 4 of chapter 694 of the laws of 1962, relating to the '
      'transfer of judges to the civil court of the city of New York, '
      'is amended to read as follows:'),
        {
            'year': '1962',
            'type': 'session_law',
            'paths': [
                [{'type': 'section', 'id': '4'},
                 {'type': 'chapter', 'id': '694'}],
                ],
            'impact': 'amended',
        }
    ),

    (('Section 4502 of the public health law, as added by a chapter '
      'of the laws of 1989, amending the public health law relating '
      'to health foods, as proposed in legislative bill number S. '
      '3601, is amended to read as follows:'),
        {
            'id': 'public health law',
            'type': 'statute',
            'paths': [
                [{'type': 'section', 'id': '4502'}]
                ],
            'impact': 'amended',
        }
    ),

    (('Section 4508 of the public health law, as added by a chapter '
      'of the laws of 1989, amending the public health law relating '
      'to health foods, as proposed in legislative bill number S. 3601, '
      'is amended to read as follows:'),
        {
            'id': 'public health law',
            'type': 'statute',
            'paths': [
                [{'type': 'section', 'id': '4508'}]
                ],
            'impact': 'amended',
        }
    ),

    (('Section 3 of a chapter 234 of the laws of 1989, amending the public '
      'health law relating to the sale of health foods, as proposed in '
      'legislative bill number A. 5730, is amended to read as follows:'),
        {
            'year': '1989',
            'type': 'session_law',
            'paths': [
                [{'type': 'section', 'id': '3'},
                 {'type': 'chapter', 'id': '234'}]
                ],
            'impact': 'amended',
        }
    ),

    (('Section 4 of a chapter 234 of the laws of 1989, amended the public '
      'health law relating to the sale of health foods, as proposed in '
      'legislative bill number A. 5730, is amended to read as follows:'),
        {
            'year': '1989',
            'type': 'session_law',
            'paths': [
                [{'type': 'section', 'id': '4'},
                 {'type': 'chapter', 'id': '234'}]
                ],
            'impact': 'amended',
        }
    ),

    (('Section 401 of the education law, as amended by a chapter of '
      'the laws of 1989, entitled "AN ACT to amend the civil rights '
      'law, the education law, the executive law and the general '
      'municipal law, in relation to prohibiting discrimination in '
      'employment of physically handicapped persons, making certain '
      'confirming amendments therein and making an appropriation '
      'therefor", is amended to read as follows:'),
        {
            'id': 'education law',
            'type': 'statute',
            'paths': [
                [{'type': 'section', 'id': '401'}]
                ],
            'impact': 'amended',
        }
    ),

    (('Sections 16-a and 18-a of the general construction law, as added '
      'by chapter 917 of the laws of 1920, are amended to read as follows:'),
        {
            'id': 'general construction law',
            'type': 'statute',
            'paths': [
                [{'type': 'section', 'id': '16-a'}],
                [{'type': 'section', 'id': '18-a'}]
                ],
            'impact': 'amended',
        }
    ),

    #
    (('Section 631 of the tax law, as amended by chapter 28 of the laws '
      'of 1987, subsection (a) as amended by chapter 170 of the laws of '
      '1994, subparagraph (c) of paragraph 1 of subsection (b) and '
      'paragraph 2 of subsection (b) as amended, subparagraph (D) of '
      'paragraph 1 of subsection (b) as added by chapter 586 of the laws '
      'of 1999, and paragraph 4 of subsection (b) as amended by chapter '
      '760 of the laws of 1992, is amended to read as follows:'),
        {
            'id': 'tax law',
            'type': 'statute',
            'paths': [
                [{'type': 'section', 'id': '631'}],
                ],
            'impact': 'amended',
        }
    ),


    (('Paragraphs (d) and (f) of section 1513-a of the not-for-profit '
      'corporation law, as added by chapter 478 of the laws of 2003, are '
      'amended and four new paragraphs (i), (j), (k) and (l) are added to '
      'read as follows:'),
        {
            'id': 'not-for-profit corporation law',
            'type': 'statute',
            'paths': [
                [{'type': 'section', 'id': '1513-a'},
                 {'type': 'paragraph', 'id': '(d)'}],
                [{'type': 'section', 'id': '1513-a'},
                 {'type': 'paragraph', 'id': '(f)'}],
                ],
            'impact': 'amended',
            'details': [
                [{'type': 'section', 'id': '1513-a'},
                 {'type': 'paragraph', 'id': '(i)'}],
                [{'type': 'section', 'id': '1513-a'},
                 {'type': 'paragraph', 'id': '(j)'}],
                [{'type': 'section', 'id': '1513-a'},
                 {'type': 'paragraph', 'id': '(k)'}],
                [{'type': 'section', 'id': '1513-a'},
                 {'type': 'paragraph', 'id': '(l)'}],
                ],
        }
    ),

    # Aaaaaaand then there are these two monsters. Let's not worry about them...

    (('Section 27-1018 of the administrative code of the city of New '
      'York, subdivisions c, d and e as added by local law number 61 of '
      'the city of New York for the year 1987, is amended to read as '
      'follows:'),
        {
            'id': 'administrative code of the city of New York',
            'type': 'statute',
            'paths': [
                [{'type': 'section', 'id': '27-1018'}],
                ],
            'impact': 'amended'
        }
    ),

    (('Paragraph 2, subparagraph (A) of paragraph 4, and paragraph 6 of '
      'subsection (b) of section 92-85 of the codes and ordinances of the '
      'city of Yonkers, paragraph 2 and subparagraph (A) of paragraph 4 '
      'as added by local law number 8 and paragraph 6 as amended by local '
      'law number 9 of the city of Yonkers for the year 1984, are amended '
      'to read as follows:'),
        {
            'type': 'statute',
            'id': 'codes and ordinances of the city of Yonkers',
            'paths': [[{'type': 'paragraph', 'id': '2'},
                      {'type': 'subparagraph', 'id': 'A'},
                      {'type': 'paragraph', 'id': '4'},
                      {'type': 'subsection', 'id': 'b'},
                      {'type': 'section', 'id': '92-85'}],

                     [{'type': 'paragraph', 'id': '6'},
                      {'type': 'subsection', 'id': 'b'},
                      {'type': 'section', 'id': '92-85'}]
                    ],
            'impact': 'amended'
        }
    )
]


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = api_test

import httplib2
import pprint

from apiclient.discovery import build
from apiclient.http import MediaFileUpload
from oauth2client.client import OAuth2WebServerFlow

import settings
import ipdb;ipdb.set_trace()

# Check https://developers.google.com/drive/scopes for all available scopes
OAUTH_SCOPE = 'https://www.googleapis.com/auth/drive'


# Path to the file to upload
FILENAME = 'document.txt'

# Run through the OAuth flow and retrieve credentials
flow = OAuth2WebServerFlow(
    settings.CLIENT_ID, settings.CLIENT_SECRET, OAUTH_SCOPE,
    settings.REDIRECT_URI)
authorize_url = flow.step1_get_authorize_url()
print 'Go to the following link in your browser: ' + authorize_url
code = raw_input('Enter verification code: ').strip()
credentials = flow.step2_exchange(code)

# Create an httplib2.Http object and authorize it with our credentials
http = httplib2.Http()
http = credentials.authorize(http)

drive_service = build('drive', 'v2', http=http)

# Insert a file
media_body = MediaFileUpload(FILENAME, mimetype='text/plain', resumable=True)
body = {
  'title': 'My document',
  'description': 'A test document',
  'mimeType': 'text/plain'
}

file = drive_service.files().insert(body=body, media_body=media_body).execute()
pprint.pprint(file)
########NEW FILE########
__FILENAME__ = check
'''
pip install google-api-python-client
'''
import re
import StringIO
import unicodecsv
import unicodedata
#import logging

import logbook
import lxml.html
import scrapelib
import name_tools
from nltk.tokenize import wordpunct_tokenize

import drive_api

#logger = logging.getLogger('billy.ballotpedia-check')
logger = logbook.Logger('ballotpedia-check')

request_defaults = {
    'timeout': 5.0,
    'headers': {
        'Accept': ('text/html,application/xhtml+xml,application/'
                   'xml;q=0.9,*/*;q=0.8'),
        'Accept-Encoding': 'gzip, deflate',
        'Accept-Language': 'en-us,en;q=0.5',
        'Connection': 'keep-alive',
        },
    'follow_robots': False,
    }

session = scrapelib.Scraper(**request_defaults)


def strip_accents(s):
   return ''.join((c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn'))



def clean_html(html):
    """
    Remove HTML markup from the given string. Borrowed from nltk.
    """
    # First we remove inline JavaScript/CSS:
    cleaned = re.sub(r"(?is)<(script|style).*?>.*?(</\1>)", "", html.strip())
    # Then we remove html comments. This has to be done before removing regular
    # tags since comments can contain '>' characters.
    cleaned = re.sub(r"(?s)<!--(.*?)-->[\n]?", "", cleaned)
    # Next we can remove the remaining tags:
    cleaned = re.sub(r"(?s)<.*?>", " ", cleaned)
    # Finally, we deal with whitespace
    cleaned = re.sub(r"&nbsp;", " ", cleaned)
    cleaned = re.sub(r"  ", " ", cleaned)
    cleaned = re.sub(r"  ", " ", cleaned)
    return cleaned.strip()


def fetch(url):
    logger.info('session.get %r' % url)
    resp = session.get(url)
    html = strip_accents(resp.text).encode('utf-8')
    doc = lxml.html.fromstring(html)
    doc.make_links_absolute(url)
    return html, doc


def main():
    import sys

    try:
        with open('status.csv') as f:
            doc = f.read()
    except IOError:
        service = drive_api.get_service()
        files = drive_api.retrieve_all_files(service)
        for file_obj in files:
            if file_obj['title'] == u'open states internal status':
                doc = drive_api.download_csv(service, file_obj)
                with open('status.csv', 'w') as f:
                    f.write(doc)
                break

    data = unicodecsv.DictReader(StringIO.StringIO(doc))

    LOWER_LIST = 'lower members list'
    LOWER_BP = 'lower ballotpedia 2012 results'
    UPPER_BP = 'upper ballotpedia 2012 results'
    UPPER_LIST = 'upper members list'

    for row in data:

        if sys.argv[1:] and not set([row['maintainer'], row['abbr']]) & set(sys.argv[1:]):
            continue

        state_passed = True

        if row['scraping again?'] == 'yes':
            pass

        logger.info('')
        logger.info('')
        logger.info('Starting %r - lower' % row['state'])
        lower = row[LOWER_LIST]
        lower_bp = row[LOWER_BP]
        if lower and lower_bp:

            # Get retiring and newly elected names.
            html, doc = fetch(lower_bp)
            elected = doc.xpath(
                '//a[@title="Won"]/preceding-sibling::a/text()')
            retiring = doc.xpath(
                '//*[@id="bodyContent"]/div[4]/table[3]/tr/td[1]/a/text()')

            # Get the actual legislature's page.
            html, doc = fetch(lower)
            html_lower = clean_html(html).lower()
            html_lower_lines = html_lower.splitlines()

            logger.info('')
            logger.info('  Testing newly elected legislator names:')
            failed = False
            failcount = 0
            for name in elected:
                name = name.lower()
                name = strip_accents(unicode(name)).encode('utf-8')
                try:
                    forms = name_tools.name_forms(name)
                except:
                    logger.warning("Couldn't get name forms for %r" % name)
                    continue
                succeeded = False
                for form in forms:
                    if len(form.split()) > 1:
                        if form in html_lower:
                            succeeded = True
                            # logger.info('    -PASS: elected %r found' % name)
                            break

                    # Try a looser test to assume away middle initials.
                    for line in html_lower_lines:
                        toks = set(wordpunct_tokenize(form))
                        if toks.issubset(set(wordpunct_tokenize(line))):
                            succeeded = True
                            break

                if not succeeded:
                    # This is such epic crap.
                    logger.info('    -FAIL: elected %r not found' % name)
                    failed = True
                    failcount += 1

            if failed:
                state_passed = False
                logger.info('')
                msg = '%r lower FAILED: %d elected not found'
                logger.info(msg % (row['state'], failcount))

            logger.info('  Testing retired incumbent names:')
            failed = False
            failcount = True
            for name in retiring:
                name = name.lower()
                name = strip_accents(unicode(name)).encode('utf-8')
                try:
                    forms = name_tools.name_forms(name)
                except:
                    logger.warning("Couldn't get name forms for %r" % name)
                    continue
                for form in forms:
                    if len(form.split()) > 1:
                        if form in html_lower:
                            logger.info('    -FAIL: retiree %r found' % name)
                            failed = True
                            failcount += 1

            if failed:
                state_passed = False
                logger.info('')
                msg = '%r lower FAILED: %d retirees found'
                logger.info(msg % (row['state'], failcount))

        logger.info('Starting %r - upper' % row['state'])
        upper = row[UPPER_LIST]
        upper_bp = row[UPPER_BP]
        if upper and upper_bp:

            # Get retiring and newly elected names.
            html, doc = fetch(upper_bp)
            elected = doc.xpath(
                '//a[@title="Won"]/preceding-sibling::a/text()')
            retiring = doc.xpath(
                '//*[@id="bodyContent"]/div[4]/table[3]/tr/td[1]/a/text()')

            # Get the actual legislature's page.
            html, doc = fetch(upper)
            html_upper = clean_html(html).lower()
            html_upper_lines = html_upper.splitlines()

            logger.info('')
            logger.info('  Testing newly elected legislator names:')
            failed = False
            failcount = True
            for name in elected:
                name = name.lower()
                # if 'mcdowell' in name:
                #     import ipdb;ipdb.set_trace()
                name = strip_accents(unicode(name)).encode('utf-8')
                try:
                    forms = name_tools.name_forms(name)
                except:
                    logger.warning("Couldn't get name forms for %r" % name)
                    continue

                succeeded = False
                for form in forms:
                    if len(form.split()) > 1:
                        if form in html_upper:
                            succeeded = True
                            # logger.info('    -PASS: elected %r found' % name)
                            break

                    # Try a looser test to assume away middle initials.
                    for line in html_upper_lines:
                        toks = set(wordpunct_tokenize(form))
                        toks = toks - set('.,-();:')
                        line_toks = set(wordpunct_tokenize(line))
                        # if toks & line_toks:
                        #     print '-'  *50
                        #     print toks
                        #     print list(sorted(line_toks))
                        #     import ipdb;ipdb.set_trace()
                        if toks.issubset(line_toks):
                            succeeded = True
                            break

                if not succeeded:
                    # This is such epic crap.
                    logger.info('    -FAIL: elected %r not found' % name)
                    failed = True
                    failcount += 1

            logger.info('')
            if failed:
                state_passed = False
                msg = '%r upper FAILED: %d elected not found'
                logger.info(msg % (row['state'], failcount))

            logger.info('  Testing retired incumbent names:')
            failed = False
            for name in retiring:
                name = name.lower()
                name = strip_accents(unicode(name)).encode('utf-8')
                try:
                    forms = name_tools.name_forms(name)
                except:
                    logger.warning("Couldn't get name forms for %r" % name)
                    continue
                for form in forms:
                    if len(form.split()) > 1:
                        if form in html_lower:
                            logger.info('    -FAIL: retiree %r found' % name)
                            failed = True
                            failcount += 1

            if failed:
                state_passed = False
                logger.info('')
                msg = '%r upper FAILED: %d retirees found'
                logger.info(msg % (row['state'], failcount))

        if state_passed:
            logger.critical('%r PASSED. GOOD TO GO.' % row['state'].upper())
        else:
            logger.critical('%r FAILED. No dice.' % row['state'].upper())

        import pdb;pdb.set_trace()









if __name__ == '__main__':
    main()

# scraper = scrapelib.Scraper()

# jxn = collections.namedtuple('Jurisdiction', 'upper lower upper_bp lower_bp')

# states = map(jxn._make, [

#     ('')

#     ])

########NEW FILE########
__FILENAME__ = drive_api
import re

import httplib2
from apiclient import errors
from apiclient.discovery import build
from oauth2client.client import OAuth2WebServerFlow

import settings


def get_service():

    # Check https://developers.google.com/drive/scopes for all available scopes
    OAUTH_SCOPE = 'https://www.googleapis.com/auth/drive'

    # Run through the OAuth flow and retrieve credentials
    flow = OAuth2WebServerFlow(
        settings.CLIENT_ID, settings.CLIENT_SECRET, OAUTH_SCOPE,
        settings.REDIRECT_URI)
    authorize_url = flow.step1_get_authorize_url()
    print 'Go to the following link in your browser: ' + authorize_url
    code = raw_input('Enter verification code: ').strip()
    credentials = flow.step2_exchange(code)

    # Create an httplib2.Http object and authorize it with our credentials
    http = httplib2.Http()
    http = credentials.authorize(http)

    drive_service = build('drive', 'v2', http=http)
    return drive_service


def retrieve_all_files(service):
  """Retrieve a list of File resources.

  Args:
    service: Drive API service instance.
  Returns:
    List of File resources.
  """
  result = []
  page_token = None
  while True:
    try:
      param = {}
      if page_token:
        param['pageToken'] = page_token
      files = service.files().list(**param).execute()

      result.extend(files['items'])
      page_token = files.get('nextPageToken')
      if not page_token:
        break
    except errors.HttpError, error:
      print 'An error occurred: %s' % error
      break
  return result


def download_csv(service, drive_file):
  """Download a file's content.

  Args:
    service: Drive API service instance.
    drive_file: Drive File instance.

  Returns:
    File's content if successful, None otherwise.
  """
  # Get the csv download link.
  download_url = drive_file['exportLinks']['application/pdf']
  download_url = re.sub(r'=pdf$', '=csv', download_url)

  if download_url:
    resp, content = service._http.request(download_url)
    if resp.status == 200:
      print 'Status: %s' % resp
      return content
    else:
      print 'An error occurred: %s' % resp
      return None
  else:
    # The file doesn't have any content stored on Drive.
    return None
########NEW FILE########
__FILENAME__ = settings
CLIENT_ID = None  # Something like '123467893423.apps.googleusercontent.com'
CLIENT_SECRET = None # Something like 'asdlfkalskdjhfaslkdjhflaksj'
REDIRECT_URI = None # Something like 'urn:ietf:wg:oauth:2.0:oob'

try:
    from local_settings import *
except ImportError:
    pass
########NEW FILE########
__FILENAME__ = check_district_boundaries
import requests
import csv
import json
import glob

urls = ('http://ec2-184-73-61-66.compute-1.amazonaws.com/boundaries/sldu/?limit=20000',
        'http://ec2-184-73-61-66.compute-1.amazonaws.com/boundaries/sldl/?limit=20000')
boundaries = set()
for url in urls:
    resp = json.loads(requests.get(url).content)
    for obj in resp['objects']:
        boundaries.add(obj['url'].replace('/boundaries/', '').rstrip('/'))

csv_boundaries = set()
for file in glob.glob('manual_data/districts/*.csv'):
    file = csv.DictReader(open(file))
    for line in file:
        csv_boundaries.add(unicode(line['boundary_id']))

print 'Districts appearing only in the boundary API:'
for b in boundaries-csv_boundaries:
    if 'not-defined' not in b:
        print '  ', b

print 'Districts not appearing in the boundary API:'
for b in csv_boundaries-boundaries:
    print '  ', b

########NEW FILE########
__FILENAME__ = clean_events
#

from billy.core import db
#import sys

#state = sys.argv[1]

events = db.events.find({
#    "state": state
})

for event in events:
    dupes = db.events.find({
        "when": event['when'],
        "end": event['end'],
        "type": event['type'],
        "description": event['description']
    })
    for dupe in dupes:
        if dupe['_id'] == event['_id']:
            continue
        print "%s => %s (rm)" % (event['_id'], dupe['_id'])
        db.events.remove(dupe, safe=True)

########NEW FILE########
__FILENAME__ = count_bill_key_lengths
from collections import defaultdict, OrderedDict, namedtuple
from decimal import Decimal
from operator import itemgetter

from billy import db


KEYS = 'versions actions documents votes sponsors'.split()


class SaneReprList(list):
    def __repr__(self):
        return '<SaneReprList: %d elements>' % len(self)


class Summarizer(object):

    def __init__(self, spec={}):
        self.spec = spec

    def build(self, keys=KEYS):
        listdict = lambda: defaultdict(SaneReprList)
        counts = defaultdict(listdict)

        keys = 'versions actions documents votes sponsors'.split()
        for bill in db.bills.find(self.spec):
            for k in keys:
                counts[k][len(bill[k])].append(bill['_id'])

        self.counts = dict(counts)
        return dict(counts)

    def count(self):
        return db.bills.find(self.spec).count()

    def max_ids(self):
        '''Yield the key, maximum value length, and the id of the
        bill in which the max was found for each key in KEYS. In
        other words, if TAB0000001 has the most actions (345), then
        one tuple yielded from this generator would be:
        ('actions', 345, 'TAB0000001')
        '''
        for k, v in self.counts.items():
            max_ = max(v)
            id_ = v[max_]
            yield k, max_, id_

    def mean(self, key):
        counts = self.counts[key]
        sum_ = sum(k * len(v) for (k, v) in counts.items())
        return sum_ / self.count()

    def median(self, key):
        counts = self.counts[key]
        if 1 < len(counts):
            counts = self.counts[key]
            div, mod = divmod(len(counts), 2)
            return div
        else:
            return list(counts).pop()

    def mode(self, key):
        counts = self.counts[key]
        if 1 < len(counts):
            return (max(counts) + min(counts)) / 2
        else:
            return list(counts).pop()

    def percentages(self, key):
        '''Returns an OrderedDict where the keys are the numbers of
        actions/votes found and the values are the percentages of how
        many bills had that number of actions out of the total number
        of bills.
        '''
        counts = self.counts[key]
        sum_ = Decimal(self.count())
        items = ((k, (len(v) / sum_) * 100) for (k, v) in counts.items())
        sorter = itemgetter(slice(None, None, -1))
        items = sorted(items, key=sorter, reverse=True)
        return OrderedDict(items)

    def report(self):
        Stats = namedtuple('Stats', 'mean median mode percentages')
        methods = [self.mean, self.median, self.mode, self.percentages]
        return dict((key, Stats(*[meth(key) for meth in methods])) for key in KEYS)

    def print_report(self):
        tab = '    '
        for k, v in self.report().items():
            print
            print repr(k)
            for key in ('mean', 'median', 'mode'):
                print tab, key, '->', getattr(v, key)
            print
            print tab, 'Percentage breakdown'
            for value, percentage in v.percentages.items():
                print tab * 2, value, "{0:.2f}".format(percentage)


if __name__ == '__main__':
    # import pprint
    # pprint.pprint(get_counts())
    x = Summarizer()
    x.build()
    x.print_report()

########NEW FILE########
__FILENAME__ = 2014_dupes
'''
Plan
-----

for each 2013 bill, if id starts with "H " or "S ",

'''
import pymongo
from billy.core import db

def action2tuple(action):
    ac = map(action.get, ['action', 'actor', 'date'])
    ac.append('-'.join(action['type']))
    return tuple(ac)


def main():

    spec = dict(state='fl', session='2014')

    print('fixing bills')
    for dupe in db.bills.find(spec):
        dupe_bill_id = dupe['bill_id']

        letter, number = dupe_bill_id.split(' ', 1)
        if len(letter) is 1:

            regex = ur'%s[A-Z]* %s$' % (letter, number)
            spec = {
                'state': 'fl',
                'session': '2014',
                'bill_id': {'$regex': regex},
                'title': dupe['title']}
            bills_2014 = list(db.bills.find(spec))

            same_actions = []
            dupe_actionset = set(map(action2tuple, dupe['actions']))
            for mergebill in bills_2014:
                if mergebill == dupe:
                    continue
                mergebill_actions = map(action2tuple, mergebill['actions'])
                if dupe_actionset.issubset(mergebill_actions):
                    same_actions.append(mergebill)

            if not same_actions:
                print 'no dupes for', dupe['bill_id']
                continue

            if not len(same_actions) == 1:
                print "CRAAAAAP"
                import pdb; pdb.set_trace()
            else:
                mergebill = same_actions.pop()

            print 'merging %s into %s' % (dupe['bill_id'], mergebill['bill_id'])
            mergebill['_all_ids'].append(dupe['_id'])

            db.bills.save(mergebill, w=1)
            db.bills.remove(dupe['_id'])

        else:
            print("Not merging %s" % dupe['bill_id'])



if __name__ == "__main__":
    main()









########NEW FILE########
__FILENAME__ = 2014_unretire_eisnaugle
from billy.core import db


def main():
    eisnaugle = db.legislators.find_one('FLL000075')

    # Make him active.
    eisnaugle['active'] = True

    # Hack his current roles.
    eisnaugle['roles'].insert(0, {
        "term": "2013-2014",
        "end_date": None,
        "district": "44",
        "chamber": "lower",
        "state": "fl",
        "party": "Republican",
        "type": "member",
        "start_date": None
    })

    # Save this hotness
    db.legislators.save(eisnaugle)


if __name__ == '__main__':
    main()
########NEW FILE########
__FILENAME__ = generate_twitter_spreadsheet
'''
Module to generate a spreadsheet for Nina to enter twitter ids into.
leg_id	name id
------  ---- --
CAL12   Thom twneale
'''

import csv
from billy import db


if __name__ == '__main__':


	writer = csv.DictWriter(open('twitter_ids.csv', 'wb'),
							fieldnames=['id', 'full_name', 'twitter'])

	
	for abbr in 'de fl ga il io mt nb nh nd ri sd tn'.split():
	
		for leg in db.legislators.find({'state': abbr}):
			writer.writerow({'id': leg['_id'].encode('utf-8'), 
				             'full_name': leg['full_name'].encode('utf-8')})
########NEW FILE########
__FILENAME__ = jenkins
import os
import tempfile
import logging
import subprocess
import urllib

from zipfile import ZipFile, BadZipfile
from os.path import split, join
from urllib2 import urlopen, Request, HTTPError

from billy.core import settings


states = {
    #'aa': 'Armed Forces Americas',
    #'ae': 'Armed Forces Middle East',
    'ak': 'Alaska',
    'al': 'Alabama',
    #'ap': 'Armed Forces Pacific',
    'ar': 'Arkansas',
    #'as': 'American Samoa',
    'az': 'Arizona',
    'ca': 'California',
    'co': 'Colorado',
    'ct': 'Connecticut',
    'dc': 'District of Columbia',
    'de': 'Delaware',
    'fl': 'Florida',
    #'fm': 'Federated States of Micronesia',
    'ga': 'Georgia',
    #'gu': 'Guam',
    'hi': 'Hawaii',
    'ia': 'Iowa',
    'id': 'Idaho',
    'il': 'Illinois',
    'in': 'Indiana',
    'ks': 'Kansas',
    'ky': 'Kentucky',
    'la': 'Louisiana',
    'ma': 'Massachusetts',
    'md': 'Maryland',
    'me': 'Maine',
    #'mh': 'Marshall Islands',
    'mi': 'Michigan',
    'mn': 'Minnesota',
    'mo': 'Missouri',
    #'mp': 'Northern Mariana Islands',
    'ms': 'Mississippi',
    'mt': 'Montana',
    'nc': 'North Carolina',
    'nd': 'North Dakota',
    'ne': 'Nebraska',
    'nh': 'New Hampshire',
    'nj': 'New Jersey',
    'nm': 'New Mexico',
    'nv': 'Nevada',
    'ny': 'New York',
    'oh': 'Ohio',
    'ok': 'Oklahoma',
    'or': 'Oregon',
    'pa': 'Pennsylvania',
    'pr': 'Puerto Rico',
    #'pw': 'Palau',
    'ri': 'Rhode Island',
    'sc': 'South Carolina',
    'sd': 'South Dakota',
    'tn': 'Tennessee',
    'tx': 'Texas',
    'ut': 'Utah',
    'va': 'Virginia',
    'vi': 'Virgin Islands',
    'vt': 'Vermont',
    'wa': 'Washington',
    'wi': 'Wisconsin',
    'wv': 'West Virginia',
    'wy': 'Wyoming'}

urls = {'data': ('http://jenkins.openstates.org/job/'
                 '{state}/ws/data/{abbr}/*zip*/in.zip'),
        'cache': ('http://jenkins.openstates.org/job/'
                 '{state}/ws/cache/*zip*/cache.zip')}

# Logging config
logger = logging.getLogger('billy.janky-import')
logger.setLevel(logging.DEBUG)

# create console handler and set level to debug
ch = logging.StreamHandler()
formatter = logging.Formatter('%(name)s %(levelname)s - %(message)s')
ch.setFormatter(formatter)
logger.addHandler(ch)


def _import(abbr, folder):

    # Where to put the files.
    path = split(settings.SCRAPER_PATHS[0])[0]
    path = join(path, folder)

    # Get credentials.
    # auth_header = _get_credentials()

    # Get the data.
    abbr = abbr.lower()
    state = urllib.quote(states.get(abbr))
    zip_url = urls[folder].format(**locals())
    msg = 'requesting {folder} folder for {state}...'
    logger.info(msg.format(**locals()))
    req = Request(zip_url)

    # Save it.
    f = tempfile.NamedTemporaryFile(delete=False)
    try:
        resp = urlopen(req)
    except HTTPError:
        logger.warn('Could\'t fetch from url: %s' % zip_url)
        return

    # Download huge files in chunks to avoid memory error.
    # Thanks @paultag for the tip.
    size = 4096
    read = resp.read
    chunk = read(4096)
    while chunk:
        f.write(chunk)
        chunk = read(4096)
        size += 4096

    logger.info('response ok [%d bytes]. Unzipping files...' % size)

    # Unzip this loaf.
    try:
        os.makedirs(path)
    except OSError:
        pass
    f.seek(0)
    try:
        zipfile = ZipFile(f)
    except BadZipfile:
        logger.warn('%s response wasn\'t a zip file. Skipping.' % state)
        return

    file_count = len(zipfile.namelist())
    zipfile.extractall(path)
    logger.info('Extracted %d files to %s.' % (file_count, path))

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description='Download data and cache files from Jenkins.')

    # Options.
    parser.add_argument('states', help='states to download data for',
                        nargs='+')

    parser.add_argument('--cache', dest='cache', action='store_const',
                       default=False, const='cache',
                       help='Download latest cache build for a state.')
    parser.add_argument('--data', dest='data', action='store_const',
                        default=True, const='data',
                        help='Download latest data build for a state.')
    parser.add_argument('--both', dest='both', action='store_true',
                        default=False,
                        help='Download latest cache, data for a state.')
    parser.add_argument('--alldata', dest='alldata', action='store_true',
                        default=False,
                        help='Download data/cache/both for all states.'),
    parser.add_argument('--import', dest='imp', action='store_true',
                        default=True,
                        help='Run import after downloading data.')

    args = parser.parse_args()

    folders = set()
    for f in ['data', 'cache']:
        if getattr(args, f):
            folders.add(f)

    if args.both:
        folders |= set(['data', 'cache'])

    _states = args.states
    if 'all' in args.states:
        _states = states

    for state in _states:
        for f in folders:
            _import(state, f)

        if args.imp:
            c = 'billy-update %s --import --report' % state
            subprocess.call(c, shell=True)

########NEW FILE########
__FILENAME__ = match_leg_ids
import operator
import difflib
import logging
from collections import defaultdict
from cStringIO import StringIO
import csv
import codecs

from billy.models import db, Metadata

# Logging config
logger = logging.getLogger('match-test')
logger.setLevel(logging.DEBUG)

# create console handler and set level to debug
ch = logging.StreamHandler()
formatter = logging.Formatter('%(name)s %(levelname)s - %(message)s')
ch.setFormatter(formatter)
logger.addHandler(ch)


class UnicodeWriter:
    """
    A CSV writer which will write rows to CSV file "f",
    which is encoded in the given encoding.
    """

    def __init__(self, f, dialect=csv.excel, encoding="utf-8", **kwds):
        # Redirect output to a queue
        self.queue = StringIO()
        self.writer = csv.writer(self.queue, dialect=dialect, **kwds)
        self.stream = f
        self.encoder = codecs.getincrementalencoder(encoding)()

    def writerow(self, row):
        self.writer.writerow([s.encode("utf-8") for s in row])
        # Fetch UTF-8 output from the queue ...
        data = self.queue.getvalue()
        data = data.decode("utf-8")
        # ... and reencode it into the target encoding
        data = self.encoder.encode(data)
        # write to the target stream
        self.stream.write(data)
        # empty queue
        self.queue.truncate(0)

    def writerows(self, rows):
        for row in rows:
            self.writerow(row)


class State(object):

    def __init__(self, abbr, chamber, cutoff=0.6):
        self.abbr = abbr
        self.chamber = chamber
        self.cutoff = cutoff
        self.metadata = Metadata.get_object(abbr)

    @property
    def matched(self):
        return self.name_data[0]

    @property
    def unmatched(self):
        return self.name_data[1]

    @property
    def name_to_ids(self):
        return self.name_data[2]

    @property
    def name_data(self):
        '''Get all unmatched_ids from committees, votes.
        '''
        # Store all the name variations used for each leg_id.

        try:
            return self._matched, self._unmatched, self._name_to_ids
        except AttributeError:
            pass
        matched = defaultdict(set)
        unmatched = set()
        name_to_ids = defaultdict(set)

        logger.debug('Getting all voter names...')
        for bill in self.metadata.bills({'chamber': self.chamber}):
            for vote in bill.votes_manager():
                for type_ in ['yes_votes', 'no_votes', 'other_votes']:
                    for voter in vote[type_]:
                        if voter['leg_id'] is not None:
                            matched[voter['leg_id']].add(voter['name'])
                        else:
                            unmatched.add(voter['name'])

        msg = 'Found %d unmatched, %d matched.'
        logger.debug(msg % (len(unmatched), len(matched)))

        logger.debug('Getting all committee member names.')
        for committee in self.metadata.committees({'chamber': self.chamber}):
            for member in committee['members']:
                if member['leg_id'] is not None:
                            matched[member['leg_id']].add(member['name'])
                else:
                    unmatched.add(member['name'])
        msg = 'Found %d unmatched, %d matched.'
        logger.debug(msg % (len(unmatched), len(matched)))

        logger.debug('Getting all legislator names')
        for legislator in self.metadata.legislators(
                        {'active': True, 'chamber': self.chamber}):
            _id = legislator['leg_id']
            name_to_ids[legislator['full_name'].lower()].add(_id)
            name_to_ids[legislator['last_name'].lower()].add(_id)
            name_to_ids[legislator['_scraped_name'].lower()].add(_id)

        self._matched = matched
        self._unmatched = list(unmatched)
        self._name_to_ids = name_to_ids
        return matched, unmatched, name_to_ids

    def get_name_id(self, namestring):
        matches = difflib.get_close_matches(namestring.lower(),
                                            self.name_to_ids,
                                            cutoff=self.cutoff)
        if matches:
            name = matches[0]
            return self.name_to_ids[name], name, namestring
        else:
            return None, None, namestring

    def csv_rows(self):
        buf = StringIO()
        writer = UnicodeWriter(buf)

        # Cache of data already added.
        added = set()
        for namestring in state.unmatched:
            skip = False
            ids, name, namestring = self.get_name_id(namestring)
            if not ids:
                msg = 'No matches found for %r'
                logger.info(msg % namestring)
                continue

            # Potential prob if there are more than 1 id.
            if 1 < len(ids):
                msg = 'There were %d possible ids for %r'
                logger.warning(msg % (len(ids), namestring))
                legs = db.legislators.find({'active': True, '_id': {'$in': list(ids)}})
                for leg in legs:
                    logger.warning('  -- %r %r' % (leg['_scraped_name'], leg['_id']))
                skip = True

            if skip:
                continue

            for _id in ids:
                legislator = db.legislators.find_one(_id)
                for session in map(operator.itemgetter('term'),
                                   legislator['roles']):
                    vals = [session, legislator['chamber'],
                            namestring, _id]
                    if tuple(vals) in added:
                        continue
                    writer.writerow(vals)
                    msg = 'Wrote row associating %r with %r, %r'
                    logger.debug(msg % (namestring, _id,
                                        legislator['full_name']))
                    added.add(tuple(vals))

        return buf.getvalue()


if __name__ == '__main__':
    import sys
    abbr = sys.argv[1]
    chamber = sys.argv[2]
    try:
        cutoff = float(sys.argv[3])
    except IndexError:
        cutoff = 0.6
    state = State(abbr, chamber)
    print state.csv_rows()
    


########NEW FILE########
__FILENAME__ = fetch
import os
import sys
from os.path import dirname, abspath, join
import json
import shutil
import time
import datetime
import logging
import socket

import feedparser

import scrapelib
from billy.utils import JSONEncoderPlus


PATH = dirname(abspath(__file__))
DATA = 'data'

logger = logging.getLogger('newsblogs.fetch')
logger.setLevel(logging.INFO)

ch = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(message)s',
                              datefmt='%H:%M:%S')
ch.setFormatter(formatter)
logger.addHandler(ch)


if __name__ == '__main__':

    session = scrapelib.Scraper()
    session.headers = {
        'Accept': ('text/html,application/xhtml+xml,application/'
                   'xml;q=0.9,*/*;q=0.8'),
        'Accept-Encoding': 'gzip, deflate',
        'Accept-Language': 'en-us,en;q=0.5',
        'Connection': 'keep-alive',
        }
    session.user_agent = (
        'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:10.0.2) '
        'Gecko/20100101 Firefox/10.0.2')
    session.timeout = 15.0
    session.follow_robots = False

    def fetch(url):
        logger.info('trying %r' % url)
        try:
            return session.get(url)
        except Exception as e:
            logger.exception(e)

    filenames = os.listdir(join(PATH, 'urls'))
    filenames = filter(lambda s: '~' not in s, filenames)
    for urls_filename in filenames:
        abbr = urls_filename.lower().replace('.txt', '')
        if sys.argv[1:] and (abbr not in sys.argv[1:]):
            continue
        with open(join(PATH, 'urls', urls_filename)) as urls:
            urls = urls.read().splitlines()
            ignored = lambda url: not url.strip().startswith('#')
            urls = filter(ignored, urls)
            responses = filter(None, urls)

        STATE_DATA = join(DATA, abbr, 'feeds')
        STATE_DATA_RAW = join(STATE_DATA, 'raw')

        try:
            shutil.rmtree(STATE_DATA_RAW)
        except OSError:
            pass

        for folder in (STATE_DATA, STATE_DATA_RAW):
            try:
                os.makedirs(folder)
            except OSError:
                pass

        for url in urls:

            resp = fetch(url)
            if not resp:
                continue

            try:
                text = resp.text
            except Exception as e:
                logger.exception(e)
                continue

            feed = feedparser.parse(text)
            for entry in feed['entries']:
                # inbox_url = ('https://inbox.influenceexplorer.com/'
                #              'contextualize?apikey=%s&text="%s"')

                # try:
                #     text = entry['summary'].encode('utf-8')
                # except KeyError:
                #     text = entry['title'].encode('utf-8')
                # search_text = urllib.quote(text)
                # inbox_url = inbox_url % (billy_settings.settings.SUNLIGHT_API_KEY,
                #                          search_text)

                # resp2 = session.get(inbox_url)
                # try:
                #     inbox_data = json.loads(resp2.text)
                # except ValueError:
                #     pass
                # else:
                #     entry['_inbox_data'] = inbox_data

                # Patch the entry object to get rid of struct_time.
                for k, v in entry.items():
                    if isinstance(v, time.struct_time):
                        t = time.mktime(entry[k])
                        dt = datetime.datetime.fromtimestamp(t)
                        entry[k] = dt

            fn = join(STATE_DATA_RAW, resp.url.replace('/', ','))
            with open(fn, 'w') as f:
                json.dump(feed['entries'], f, cls=JSONEncoderPlus)

########NEW FILE########
__FILENAME__ = scrape
import re
import json
import os
import operator
import functools
import collections
import htmlentitydefs
import urlparse
import itertools
import contextlib
import logging
import datetime
from os.path import join, dirname, abspath
from operator import itemgetter

import pymongo
from functools import partial

from billy.core import db
from billy.utils import metadata
from billy.core import settings


host = settings.MONGO_HOST
port = settings.MONGO_PORT

conn = pymongo.Connection(host, port)
feed_db = conn.newsblogs


class _CachedAttr(object):
    '''Computes attr value and caches it in the instance.'''

    def __init__(self, method, name=None):
        self.method = method
        self.name = name or method.__name__

    def __get__(self, inst, cls):
        if inst is None:
            return self
        result = self.method(inst)
        setattr(inst, self.name, result)
        return result


def unescape(text):
    '''Removes HTML or XML character references and entities from a text string.
    @param text The HTML (or XML) source text.
    @return The plain text, as a Unicode string, if necessary.
    '''
    def fixup(m):
        text = m.group(0)
        if text[:2] == "&#":
            # character reference
            try:
                if text[:3] == "&#x":
                    return unichr(int(text[3:-1], 16))
                else:
                    return unichr(int(text[2:-1]))
            except ValueError:
                pass
        else:
            # named entity
            try:
                text = unichr(htmlentitydefs.name2codepoint[text[1:-1]])
            except KeyError:
                pass
        return text  # leave as is
    return re.sub("&#?\w+;", fixup, text)


class Trie(dict):

    @_CachedAttr
    def finditer(self):
        return functools.partial(re.finditer, '|'.join(self))


class PseudoMatch(object):
    '''A fake match object that provides the same basic interface
    as _sre.SRE_Match.'''

    def __init__(self, group, start, end):
        self._group = group
        self._start = start
        self._end = end

    def group(self):
        return self._group

    def start(self):
        return self._start

    def end(self):
        return self._end

    def _tuple(self):
        return (self._group, self._start, self._end)

    def __repr__(self):
        return 'PseudoMatch(group=%r, start=%r, end=%r)' % self._tuple()


def trie_add(trie, seq_value_2tuples, terminus=0):
    '''Given a trie (or rather, a dict), add the match terms into the
    trie.
    '''
    for seq, value in seq_value_2tuples:

        this = trie
        w_len = len(seq) - 1
        for i, c in enumerate(seq):

            if c in ",. '&[]":
                continue

            try:
                this = this[c]
            except KeyError:
                this[c] = {}
                this = this[c]

            if i == w_len:
                this[terminus] = value

    return trie


def trie_scan(trie, s,
         _match=PseudoMatch,
         second=itemgetter(1)):
    '''
    Finds all matches for `s` in `trie`.
    '''

    res = []
    match = []

    this = trie
    in_match = False

    for i, c in enumerate(s):

        if c in ",. '&[]":
            if in_match:
                match.append((i, c))
            continue

        if c in this:
            this = this[c]
            match.append((i, c))
            in_match = True
            if 0 in this:
                _matchobj = _match(group=''.join(map(second, match)),
                                   start=match[0][0], end=match[-1][0])
                res.append([_matchobj] + this[0])

        else:
            in_match = False
            if match:
                match = []

            this = trie
            if c in this:
                this = this[c]
                match.append((i, c))
                in_match = True

    # Remove any matches that are enclosed in bigger matches.
    prev = None
    for tpl in reversed(res):
        match, _, _ = tpl
        start, end = match.start, match.end

        if prev:
            a = prev._start <= match._start
            b = match._end <= prev._end
            c = match._group in prev._group
            if a and b and c:
                res.remove(tpl)

        prev = match

    return res


# def trie_scan(trie, string, _match=PseudoMatch,
#               second=itemgetter(1)):

#     this = trie
#     match = []
#     spans = []

#     for matchobj in trie.finditer(string):

#         pos = matchobj.start()
#         this = trie
#         match = []

#         while True:

#             try:
#                 char = string[pos]
#             except IndexError:
#                 break

#             if char in ",. '&[]":
#                 match.append((pos, char))
#                 pos += 1
#                 continue

#             try:
#                 this = this[char]
#             except KeyError:
#                 break
#             else:
#                 match.append((pos, char))
#                 if 0 in this:
#                     start = matchobj.start()
#                     end = pos
#                     pseudo_match = _match(group=''.join(map(second, match)),
#                                           start=start, end=end)

#                     # Don't yeild a match if this match is contained in a
#                     # larger match.
#                     _break = False
#                     for _start, _end in spans:
#                         if (_start <= start) and (end <= _end):
#                             _break = True
#                     if _break:
#                         break

#                     spans.append((start, end))
#                     yield [pseudo_match] + this[0]
#                     break
#                 else:
#                     pos += 1


@contextlib.contextmanager
def cd(path):
    '''Creates the path if it doesn't exist'''
    old_dir = os.getcwd()
    try:
        os.makedirs(path)
    except OSError:
        pass
    os.chdir(path)
    try:
        yield
    finally:
        os.chdir(old_dir)


def cartcat(s_list1, s_list2):
    '''Given two lists of strings, take the cartesian product
    of the lists and concat each resulting 2-tuple.'''
    prod = itertools.product(s_list1, s_list2)
    return map(partial(apply, operator.add), prod)


def clean_html(html):
    """
    Remove HTML markup from the given string. Borrowed from nltk.
    """
    # First we remove inline JavaScript/CSS:
    cleaned = re.sub(r"(?is)<(script|style).*?>.*?(</\1>)", "", html.strip())
    # Then we remove html comments. This has to be done before removing regular
    # tags since comments can contain '>' characters.
    cleaned = re.sub(r"(?s)<!--(.*?)-->[\n]?", "", cleaned)
    # Next we can remove the remaining tags:
    cleaned = re.sub(r"(?s)<.*?>", " ", cleaned)
    # Finally, we deal with whitespace
    cleaned = re.sub(r"&nbsp;", " ", cleaned)
    cleaned = re.sub(r"  ", " ", cleaned)
    cleaned = re.sub(r"  ", " ", cleaned)
    return cleaned.strip()

# ---------------------------------------------------------------------------


# ---------------------------------------------------------------------------
class BogusEntry(Exception):
    '''Raised when an entry lacks a required attribute, like 'link'.'''


def new_feed_id(entry, cache={}):
    '''Generate an entry id using the hash value of the title and link.
    Pad the number to 21 digits.
    '''
    try:
        s = entry['title'] + entry['link']
    except KeyError:
        msg = 'Required key missing: %r'
        raise BogusEntry(msg % entry)

    hashval = hash(s)
    sign = ('A' if 0 < hashval else 'B')
    _id = entry['state'].upper() + 'F' + (str(hashval) + sign).zfill(21)
    return _id

PATH = dirname(abspath(__file__))
DATA = settings.BILLY_DATA_DIR


class Extractor(object):

    full_name = [u' {legislator[first_name]} {legislator[last_name]}']
    trie_terms = {
        'legislators': {
            'upper': cartcat(
                [u'Senator', u'Senate member', u'Senate Member', u'Sen.',
                 u'Council member', u'Councilman', u'Councilwoman',
                 u'Councilperson'],
                [u' {legislator[last_name]}', u' {legislator[full_name]}']
                ) + full_name,

            'lower': cartcat([
                u'Assemblymember',
                u'Assembly Member',
                u'Assembly member',
                u'Assemblyman',
                u'Assemblywoman',
                u'Assembly person',
                u'Assemblymember',
                u'Assembly Member',
                u'Assembly member',
                u'Assemblyman',
                u'Assemblywoman',
                u'Assembly person',
                u'Representative',
                u'Rep.',
                u'Council member', u'Councilman',
                u'Councilwoman', u'Councilperson'],
                [u' {legislator[last_name]}', u' {legislator[full_name]}']
                ) + full_name,
                },

        'bills': [
            ('bill_id', lambda s: s.upper().replace('.', ''))
            ]

        }

    def __init__(self, abbr):
        self.entrycount = 0
        self.abbr = abbr
        self._assemble_ban_data()

        logger = logging.getLogger(abbr)
        logger.setLevel(logging.DEBUG)
        ch = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(message)s')
        ch.setFormatter(formatter)
        logger.addHandler(ch)
        self.logger = logger

    @property
    def metadata(self):
        return metadata(self.abbr)

    def extract_bill(self, m, collection=db.bills, cache={}):
        '''Given a match object m, return the _id of the related bill.
        '''
        def squish(bill_id):
            bill_id = ''.join(bill_id.split())
            bill_id = bill_id.upper().replace('.', '')
            return bill_id

        bill_id = squish(m.group())

        try:
            ids = cache['ids']
        except KeyError:
            # Get a list of (bill_id, _id) tuples like ('SJC23', 'CAB000123')
            ids = collection.find({'state': self.abbr}, {'bill_id': 1})
            ids = dict((squish(r['bill_id']), r['_id']) for r in ids)

            # Cache it in the method.
            cache['ids'] = ids

        if bill_id in ids:
            return ids[bill_id]

    def committee_variations(self, committee):
        '''Compute likely variations for a committee
        Standing Committee on Rules
         - Rules Committee
         - Committee on Rules
         - Senate Rules
         - Senate Rules Committee
         - Rules (useless)
        '''
        name = committee['committee']
        ch = committee['chamber']
        if ch != 'joint':
            chamber_name = self.metadata['chambers'][ch]['name']
        else:
            chamber_name = 'Joint'

        # Arts
        raw = re.sub(r'(Standing|Joint|Select) Committee on ', '', name)
        raw = re.sub(r'\s+Committee$', '', raw)

        # Committee on Arts
        committee_on_1 = 'Committee on ' + raw

        # Arts Committee
        short1 = raw + ' Committee'

        # Assembly Arts Committee
        if not short1.startswith(chamber_name):
            short2 = chamber_name + ' ' + short1
        else:
            short2 = short1

        # Assembly Committee on Arts
        committee_on_2 = chamber_name + ' ' + committee_on_1

        phrases = [name, committee_on_1, committee_on_2, short1, short2]

        # "select Committee weirdness"
        phrases += ['Select ' + committee_on_1]

        # Adjust for ampersand usage like "Senate Public Employment &
        # Retirement Committee"
        phrases += [p.replace(' and ', ' & ') for p in phrases]

        # Exclude phrases less than two words in length.
        return set(filter(lambda s: ' ' in s, phrases))

    @property
    def trie(self):
        try:
            return self._trie
        except AttributeError:
            return self.build_trie()

    def format_trie_term(self, term, vals, collection_name, record):
        if isinstance(term, basestring):
            trie_add_args = (term.format(**vals),
                             [collection_name, record['_id']])
            return trie_add_args

        elif isinstance(term, tuple):
            k, func = term
            trie_add_args = (func(record[k]),
                             [collection_name, record['_id']])
            return trie_add_args

    def build_trie(self):
        '''Interpolate values from this state's mongo records
        into the trie_terms strings. Create a new list of formatted
        strings to use in building the trie, then build.
        '''
        trie = Trie()
        trie_terms = self.trie_terms
        abbr = self.abbr

        for collection_name in trie_terms:
            spec = {'state': abbr}
            trie_data = []
            collection = getattr(db, collection_name)
            if collection_name == 'legislators':
                spec['active'] = True
            cursor = collection.find(spec)
            self.logger.info('compiling %d %r trie term values' % (
                cursor.count(), collection_name))

            for record in cursor:
                k = collection_name.rstrip('s')
                vals = {k: record}

                terms = trie_terms[collection_name]
                if collection_name == 'legislators':
                    terms = terms[record['chamber']]

                for term in terms:
                    args = term, vals, collection_name, record
                    trie_data.append(self.format_trie_term(*args))

            self.logger.info('adding %d %s terms to the trie' %
                             (len(trie_data), collection_name))

            trie = trie_add(trie, trie_data)

        if hasattr(self, 'committee_variations'):

            committee_variations = self.committee_variations
            trie_data = []
            records = db.committees.find({'state': abbr},
                                         {'committee': 1, 'subcommittee': 1,
                                          'chamber': 1})
            self.logger.info('Computing name variations for %d records' %
                             records.count())
            for c in records:
                for variation in committee_variations(c):
                    trie_add_args = (variation, ['committees', c['_id']])
                    trie_data.append(trie_add_args)

        self.logger.info('adding %d \'committees\' terms to the trie' %
                         len(trie_data))

        trie = trie_add(trie, trie_data)
        self._trie = trie
        return trie

    def scan_feed(self, entries):

        for entry in entries:
            self.entrycount += 1

            # Search the trie.
            matches = []
            try:
                summary = clean_html(entry['summary'])
            except KeyError:
                # This entry has no summary. Skip.
                continue
            matches += trie_scan(self.trie, summary)

            yield entry, matches

    def process_feed(self, entries):
        abbr = self.abbr
        feed_entries = feed_db.entries
        third = itemgetter(2)

        # Find matching entities in the feed.
        for entry, matches in self.scan_feed(entries):
            matches = self.extract_entities(matches)

            ids = map(third, matches)
            strings = [m.group() for m, _, _ in matches]
            assert len(ids) == len(strings)

            # Add references and save in mongo.
            entry['state'] = abbr  # list probably wiser
            entry['entity_ids'] = ids or None
            entry['entity_strings'] = strings or None
            entry['save_time'] = datetime.datetime.utcnow()

            try:
                entry['_id'] = new_feed_id(entry)
            except BogusEntry:
                # This entry appears to be malformed somehow. Skip.
                msg = 'Skipping malformed feed: %s'
                msg = msg % repr(entry)[:100] + '...'
                self.logger.info(msg)
                continue

            entry['_type'] = 'feedentry'

            entry['summary'] = unescape(clean_html(entry['summary']))
            try:
                entry['summary_detail']['value'] = unescape(clean_html(
                    entry['summary_detail']['value']))
            except KeyError:
                pass

            entry['title'] = unescape(entry['title'])

            # Kill any keys that contain dots.
            entry = dict((k, v) for (k, v) in entry.items() if '.' not in k)

            entry_set = self._dictitems_to_set(entry)
            for keyval_set in self._banned_keyvals:
                if entry_set & keyval_set:
                    msg = 'Skipped story containing banned key values: %r'
                    self.logger.info(msg % keyval_set)
                    return

            # Skip any entries that are missing required keys:
            required = set('summary source host link published_parsed'.split())
            if required not in set(entry):
                if 'links' not in entry:
                    msg = 'Skipped story lacking required keys: %r'
                    self.logger.info(msg % (required - set(entry)))
                    return
                else:
                    source = entry['links'][-1].get('href')
                    if source:
                        host = urlparse.urlparse(entry['links'][0]['href']).netloc
                        entry['source'] = source
                        entry['host'] = host
                    else:
                        msg = 'Skipped story lacking required keys: %r'
                        self.logger.info(msg % (required - set(entry)))
                        return

            # Save
            feed_entries.save(entry)
            msg = 'Found %d related entities in %r'
            self.logger.info(msg % (len(ids), entry['title']))

    def process_all_feeds(self):
        '''Note to self...possible problems with entries getting
        overwritten?
        '''
        abbr = self.abbr
        STATE_DATA = join(DATA, abbr, 'feeds')
        STATE_DATA_RAW = join(STATE_DATA, 'raw')
        _process_feed = self.process_feed

        with cd(STATE_DATA_RAW):
            for fn in os.listdir('.'):
                with open(fn) as f:
                    entries = json.load(f)
                    _process_feed(entries)

    def extract_entities(self, matches):

        funcs = {}
        for collection_name, method in (('bills', 'extract_bill'),
                                        ('legislators', 'extract_legislator'),
                                        ('committees', 'extract_committees')):

            try:
                funcs[collection_name] = getattr(self, method)
            except AttributeError:
                pass

        processed = []
        for m in matches:

            if len(m) == 2:
                match, collection_name = m
                extractor = funcs.get(collection_name)
                if extractor:
                    _id = extractor(match)
                    processed.append(m + [_id])
            else:
                processed.append(m)

        return processed

    def _assemble_ban_data(self):
        '''Go through this state's file in newblogs/skip, parse each line
        into a JSON object, and store them in the Extractor.
        '''
        here = dirname(abspath(__file__))
        skipfile = join(here,  'skip', '%s.txt' % self.abbr)
        banned_keyvals = []
        try:
            with open(skipfile) as f:
                for line in filter(None, f):
                    data = json.loads(line)
                    data = self._dictitems_to_set(data)
                    banned_keyvals.append(set(data))
        except IOError:
            pass
        self._banned_keyvals = banned_keyvals

    def _dictitems_to_set(self, dict_):
        return set((k, v) for (k, v) in dict_.items()
                   if isinstance(v, collections.Hashable))

'''
To-do:
DONE - Make trie-scan return a pseudo-match object that has same
interface as re.matchobjects.

DONE - Handle A.B. 200 variations for bills.

DONE-ish... Tune committee regexes.

- Add chambers the entry is relevant to so we can query by state
and chamber in addition to entity.

Investigate other jargon and buzz phrase usage i.e.:
 - speaker of the house
 - committee chair

'''

if __name__ == '__main__':

    import sys
    from os.path import dirname, abspath, join

    PATH = dirname(abspath(__file__))

    if sys.argv[1:]:
        states = sys.argv[1:]
    else:
        filenames = os.listdir(join(PATH, 'urls'))
        filenames = [s.replace('.txt', '') for s in filenames]
        states = filter(lambda s: '~' not in s, filenames)

    stats = {}
    for abbr in states:
        ex = Extractor(abbr)
        ex.process_all_feeds()
        stats[ex.abbr] = ex.entrycount

    logger = logging.getLogger('newsblogs fetch')
    logger.setLevel(logging.INFO)

    ch = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(message)s',
                                  datefmt='%H:%M:%S')
    ch.setFormatter(formatter)
    logger.addHandler(ch)

    for abbr, count in stats.items():
        logger.info('%s - scanned %d feed entries' % (abbr, count))

########NEW FILE########
__FILENAME__ = remove_entitydefs
'''HTML entity defs weren't getting unescaped during import until
today. This script retroactively fixes the ones in the feeds_db.
'''
import re, htmlentitydefs

from billy.core import feeds_db


def unescape(text):
    '''Removes HTML or XML character references and entities from a text string.
    @param text The HTML (or XML) source text.
    @return The plain text, as a Unicode string, if necessary.
    from
    '''
    def fixup(m):
        text = m.group(0)
        if text[:2] == "&#":
            # character reference
            try:
                if text[:3] == "&#x":
                    return unichr(int(text[3:-1], 16))
                else:
                    return unichr(int(text[2:-1]))
            except ValueError:
                pass
        else:
            # named entity
            try:
                text = unichr(htmlentitydefs.name2codepoint[text[1:-1]])
            except KeyError:
                pass
        return text  # leave as is
    return re.sub("&#?\w+;", fixup, text)


def main():
    for entry in feeds_db.entries.find():
        entry['title'] = unescape(entry['title'])
        feeds_db.entries.save(entry)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = potential_duped_legislators
#!/usr/bin/env python

from sunlight import openstates
import sys
import codecs
sys.stdout=codecs.getwriter('utf-8')(sys.stdout)

state = sys.argv[1]

kwargs = {
    "state": state
}

legis = openstates.legislators(**kwargs)
for leg in legis:
    search = openstates.legislators(
        first_name=leg['first_name'],
        last_name=leg['last_name'],
        active="false",
        state=state
    )
    for s in search:
        if s['leg_id'] != leg['leg_id']:
            print s['full_name']
            print leg['full_name']
            print "  %s / %s" % (
                s['leg_id'], leg['leg_id']
            )
            print ""

########NEW FILE########
__FILENAME__ = purge_old_committee_ids
from billy.core import db, feeds_db
from billy.core import settings
from billy.core import logging


def main():

    import sys
    abbrs = sys.argv[1:] or [x['abbreviation'] for x in db.metadata.find()]
    logger = logging.getLogger('purge_committee_ids')
    logger.setLevel(logging.DEBUG)

    for abbr in abbrs:
        spec = {settings.LEVEL_FIELD: abbr}
        committee_ids = [c['_id'] for c in db.committees.find(spec, fields=['_id'])]

        # Events with committee participants.
        spec = {
            settings.LEVEL_FIELD: abbr,
            'participants.committee_id': {'$nin': committee_ids}
            }
        for event in db.events.find(spec):
            old_ids = set()
            count = 0
            found = False
            for participant in event['participants']:
                for id_key in 'committee_id', 'id':
                    _id = participant.get(id_key, None)
                    type_ = participant.get('participant_type')
                    if id_key == 'id' and type_ != 'committee':
                        continue
                    if _id and (_id not in committee_ids):
                        found = True
                        msg = 'Removing participant %r from event %r'
                        logger.info(msg % (participant[id_key], event['_id']))

                        # Leave the participant in but set their id to none.
                        # Text will still be displayed without a hyperlink.
                        participant[id_key] = None

            if found:
                msg = 'Removed %d old committee %r ids from %r'
                logger.info(msg % (count, old_ids, event['_id']))
                db.events.save(event, safe=True)

        # Related committees in bill actions.
        spec = {
            settings.LEVEL_FIELD: abbr,
            'actions.related_entities.type': 'committee'
            }
        for bill in db.bills.find(spec):
            old_ids = set()
            count = 0
            found = False
            for action in bill['actions']:
                for entity in action['related_entities']:
                    if entity['type'] == 'committee':
                        if entity['id'] and (entity['id'] not in committee_ids):
                            found = True
                            count += 1
                            old_ids.add(entity['id'])
                            msg = 'Removing entity %r from action in %r'
                            logger.debug(msg % (entity['id'], bill['bill_id']))

                            # Completely remove the related entity. Without an
                            # id it has no other purpose.
                            action['related_entities'].remove(entity)
            if found:
                msg = 'Removed %d old committee %r ids from %r'
                logger.info(msg % (count, old_ids, bill['_id']))
                db.bills.save(bill, safe=True)

        # Legislator old roles.
        spec = {
            settings.LEVEL_FIELD: abbr,
            'old_roles': {'$exists': True}
            }
        for leg in db.legislators.find(spec):
            old_ids = set()
            count = 0
            found = False
            for role in leg['old_roles']:
                if 'committee_id' in role:
                    _id = role['committee_id']
                    if _id and (_id not in committee_ids):
                        found = True
                        count += 1
                        old_ids.add(_id)
                        msg = 'Removing id %r from old_role in %r'
                        logger.info(msg % (role['committee_id'], leg['full_name']))
                        # Set the id to None.
                        role['committee_id'] = None
            if found:
                msg = 'Removed %d old committee %r ids from %r'
                logger.info(msg % (count, old_ids, leg['_id']))
                db.legislators.save(leg, safe=True)

        # Related entities in feeds.
        spec = {
            settings.LEVEL_FIELD: abbr,
            'entity_ids': {'$ne': None}
            }
        for entry in feeds_db.entries.find(spec):
            old_ids = set()
            count = 0
            found = False
            for entity_id in entry['entity_ids']:
                if entity_id[2] == 'C':
                    if entity_id not in committee_ids:
                        found = True
                        count += 1
                        msg = 'Removing id %r from feed %r'
                        logger.info(msg % (entity_id, entry['_id']))

                        # Delete the entity from the feed.
                        old_ids.add(entity_id)
                        index = entry['entity_ids'].index(entity_id)
                        del entry['entity_ids'][index]
                        del entry['entity_strings'][index]
            if found:
                msg = 'Removed %d old committee ids %r from %r'
                logger.info(msg % (count, old_ids, entry['_id']))
                feeds_db.entries.save(entry, safe=True)

        # Nuke any committee sponsors of bills.
        spec = {
            settings.LEVEL_FIELD: abbr,
            'sponsors.committee_id': {'$nin': committee_ids}
            }
        for bill in db.bills.find(spec):
            count = 0
            found = False
            old_ids = set()
            for sponsor in bill.get('sponsors', []):
                if 'committee_id' in sponsor:
                    _id = sponsor['committee_id']
                    old_ids.add(_id)
                    found = True
                    count += 1

                    del sponsor['committee_id']

            if found:
                msg = 'Removed %d old committee ids %r from %r'
                logger.info(msg % (count, old_ids, bill['_id']))
                db.bills.save(bill)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = purge_old_events_bill_ids
import sys
import re
from collections import defaultdict, Counter

from billy.core import db
from billy.core import settings
from billy.core import logging
from billy.utils import fix_bill_id


def main():

    abbrs = sys.argv[1:] or [x['abbreviation'] for x in db.metadata.find()]
    logger = logging.getLogger('billy.purge_committee_ids')
    logger.setLevel(logging.INFO)
    tally = defaultdict(Counter)

    for abbr in abbrs:
        abbr_tally = tally['abbr']
        spec = {
            settings.LEVEL_FIELD: abbr,
            'related_bills': {'$exists': True, '$ne': []},
            }
        for event in db.events.find(spec):
            fixed = []
            for bill in event['related_bills']:

                bill_id = bill.get('bill_id')
                if bill_id is not None:

                    # If "bill_id" is a big id, rename it.
                    if re.match(r'[A-Z]{2}B\d{8}', bill_id):
                        _id = bill.pop('bill_id')
                        bill['id'] = _id
                        logger.info('Renamed "bill_id" to "id"')
                        abbr_tally['bill_id --> id'] += 1

                    # If it's something else, do fix_bill_id to
                    # fix screwed up old ids.
                    else:
                        bill['bill_id'] = fix_bill_id(bill['bill_id'])
                        logger.info('Fixed an un-fixed bill_id')
                        abbr_tally['fix_bill_id'] += 1

                    fixed = True

                if '_scraped_bill_id' in bill:
                    bill_id = fix_bill_id(bill.pop('_scraped_bill_id'))
                    bill['bill_id'] = bill_id
                    logger.info('Renamed "_scraped_bill_id" to "bill_id"')
                    abbr_tally['_scraped_bill_id --> bill_id'] += 1

                    fixed = True

            if fixed:
                msg = 'Updating related_bills on event %r.'
                logger.debug(msg % event['_id'])
                db.events.save(event)

        logger.info(abbr)
        # for item in abbr_tally.items():
        #     logger.info('%s %d' % item)
        # import ipdb;ipdb.set_trace()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = test_filters
from billy.core import db
from billy_settings import LEGISLATOR_FILTERS
from billy.importers.filters import apply_filters
from dictdiffer import diff


filters = LEGISLATOR_FILTERS


for leg in db.legislators.find():
    d1 = leg
    leg = leg.copy()
    d2 = apply_filters(filters, leg)
    changes = list(diff(d1, d2))
    if changes != []:
        print leg['_id'], changes

########NEW FILE########
__FILENAME__ = sessionfix
'''Revise imaginary session 109 back to 108. This should be find because
all bills that got a "109" session value were actually scraped from the
108'th session.
'''
from billy.core import db

def main():

    # Select all votes that had awful 109 or "109" as session.
    spec = dict(state='tn', session={'$in': ['109', 109]})
    votes = db.votes.find(spec)

    for vote in votes:
        print(vote['session'])
        vote['session'] = '108'
        print('Setting vote to 108 on', vote['_id'])
        db.votes.save(vote, w=1)
        # import pdb; pdb.set_trace()


if __name__ == '__main__':
    main()
########NEW FILE########
__FILENAME__ = tnfix
import sys
import plop
import pymongo


def main(state):
    db = pymongo.MongoClient().fiftystates
    index = plop.Index()

    spec = dict(state=state)
    print('adding bills')
    for bill in db.bills.find(spec):
        index.add_object(bill)

    print('adding legislators')
    for obj in db.legislators.find(spec):
        index.add_object(obj)

    print('adding committees')
    for obj in db.committees.find(spec):
        index.add_object(obj)

    print('adding votes')
    for obj in db.votes.find(spec):
        index.add_object(obj)

    import pdb; pdb.set_trace()


if __name__ == "__main__":
    main(*sys.argv[1:])


########NEW FILE########
__FILENAME__ = tn_script
import sys

import pymongo
from billy.core import db

def main(state):

    spec = dict(state=state)

    print('fixing bills')
    bill_spec = dict(spec, session=108)
    for bill_109th in db.bills.find(bill_spec):
        print(bill_109th['bill_id'])
        # print(bill_109th['session'])
        # Reset session and _term to 108.
        bill_109th.update(session='108', _term='108')

        try:
            db.bills.save(bill_109th, w=1)
        except pymongo.errors.DuplicateKeyError:
            # This bill was duped, with the only different attr being
            # session value of 109. Merge the two bills by adding the
            # bad 109 bill id to the 108 bills' _all_ids.

            # First get the 108th session bill.
            spec = dict(spec,
                session='108',
                chamber=bill_109th['chamber'],
                bill_id=bill_109th['bill_id'])
            bill_108th = db.bills.find_one(spec)

            # Add the 109th id to its _all_ids.
            bill_109th_id = bill_109th['_id']
            bill_108th['_all_ids'].append(bill_109th_id)

            # Save.
            db.bills.save(bill_108th)
            db.bills.remove(bill_109th_id)


    # print('adding legislators')
    # for obj in db.legislators.find(spec):
    #     # Remove 108 and 109 from old roles. 108 are the current roles,
    #     # 109 are bogus.
    #     obj.get('old_roles', {}).pop('108', None)
    #     obj.get('old_roles', {}).pop('109', None)

    #     # Remove roles with term: 109.
    #     roles = obj.get('roles', [])
    #     for role in list(roles):
    #         if role['term'] == '109':
    #             roles.remove(role)

    #     db.legislators.save(obj)


if __name__ == "__main__":
    main('tn')









########NEW FILE########
__FILENAME__ = votefix
import sys

import pymongo

try:
    from billy.core import db
except:
    db = pymongo.MongoClient().fiftystates


def main(state):

    spec = dict(state=state)
    for vote in db.votes.find(spec):
        if vote['session'] in ('109'):
            vote['session'] = '108'
            print('fixing', vote['_id'])
            db.votes.save(vote, w=1)



if __name__ == "__main__":
    main('tn')









########NEW FILE########
__FILENAME__ = updated_report
#!/usr/bin/env python

from billy import db

watch = [ 'ma', 'nj', 'il', 'ny' ]

for row in db.reports.find():
    if not row['_id'] in watch:
        continue

    for guy in [ 'bills' ]:
        for thing in [ 'updated_this_month', 'updated_today' ]:
            print "(%s) %s %s: %s" % ( row['_id'], guy, thing, row[guy][thing] )
    print ""

########NEW FILE########
