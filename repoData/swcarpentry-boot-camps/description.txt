Contributing Boot Camp Material
===============================

Software Carpentry is an open source/open access project, and we
welcome contributions of all kinds.  By contributing, you are agreeing
that Software Carpentry may redistribute your work under
[these licenses][licenses].  Please see [this page][creators] for
a list of contributors to date.

Workflow
--------

Software Carpentry uses a development workflow similar to that of
[AstroPy][] and many other open source projects. The AstroPy docs have
excellent sections on:

* [Getting started with git][astropy-git]
* [Developer workflow][astropy-workflow]

File Formats
------------

### Text

Text documents should be in [Markdown][] format and compatible
with [Redcarpet][], the engine GitHub uses to render Markdown.

### Slides

The preferred format for slide presentations is still to be determined.

[AstroPy]: http://astropy.org
[astropy-git]: http://astropy.readthedocs.org/en/latest/development/workflow/index.html#getting-started-with-git
[astropy-workflow]: http://astropy.readthedocs.org/en/latest/development/workflow/development_workflow.html
[creators]: http://software-carpentry.org/badges/creator.html
[licenses]: http://software-carpentry.org/license.html
[Markdown]: http://daringfireball.net/projects/markdown/
[Redcarpet]: https://github.com/vmg/redcarpet

This collection of material is licensed under a Creative Commons - Attribution
license.

You are free:

- to **Share** - to copy, distribute and transmit the work
- to **Remix** - to adapt the work

Under the following conditions:

- **Attribution** - You must attribute the work in the manner specified by the
  author or licensor (but not in any way that suggests that they endorse you or
  your use of the work).

With the understanding that:

- **Waiver** - Any of the above conditions can be waived if you get permission
  from the copyright holder.
- **Other Rights** - In no way are any of the following rights affected by the
  license:
  - Your fair dealing or fair use rights;
  - The authorâ€™s moral rights;
  - Rights other persons may have either in the work itself or in how the work
    is used, such as publicity or privacy rights.
- **Notice** - For any reuse or distribution, you must make clear to others the
  license terms of this work. The best way to do this is with a link to this
  [web page](http://creativecommons.org/licenses/by/3.0/).

For the full legal text of this license, please see
http://creativecommons.org/licenses/by/3.0/legalcode.

This is a (admittedly fake) sample sheet that describes an experiment where six samples (controls vs. biopsies) for three cartoon characters.

Since we had six samples and eight flowcell lanes, we wanted one and a third lanes per sample.
Our sequencing center director told us that we needed at least two samples per lane or the machine would become confused.

So we have a situation where two of our samples are split over two lanes and the rest of our samples are split over three lanes, and we desire to concatenate all of the samples form Donald Duck's biopsy into one (possibly large) data file.    After the fact, we need to check that the concatenated data are valid--that we got all the reads, that there are the number of reads that we exepect, and that the number of reads from read 1 match the number of reads from read 2.

This is the classic case for automation--you can write a script to concatenate the data by hand, or you can solve the more general problem of concatenating all the data form the same sample and changing the names of the files to things that are easier for you and your collaborators to understand.



These is a small sample of sequence data from a Miseq sequencing instrument that has not been demultiplexed.


##What to expect##
We have to **get the data**, **get the data out of its container**, and **do something with the data**.  

###Get the data--NCBI's EUTILS###
One large data source is NCBI.  NCBI provides an interface to allow automated download of various data products using HTTP GET requests.

The documentation for this interface, called EFETCH, is here:
http://www.ncbi.nlm.nih.gov/books/NBK25499/

Before using EUTILS, we might want to know what kinds of things it can do:
#####Search engine for PUBMED: #####
http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=%22Life+with+6000+genes%22&retmax=100
#####Search engine for SRA#####
 http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=sra&term=SRX015714
#####Search engine for Genbank#####
 http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=nucleotide&term=GFAJ1

Ok, you probably get the picture.  These queries return us lists of IDs that don't mean anything to us as humans, but that we can iterate over and retrieve automatically.

Once we've got lists of identifiers, we can retrieve data:
##### Pubmed abstracts:#####
This abstract http://www.ncbi.nlm.nih.gov/pubmed/8849441
can be retrieved in a machine-friendly format with this query:
http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=8849441&rettype=xml
##### SRA metadata bundles#####
This page of metadata about a dataset from Jeff Gordon's twin study  http://www.ncbi.nlm.nih.gov/sra/SRX015714 
can be retrieved from here:
http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=sra&id=16175
##### Genbank records #####
Sequence data deposited by authors (with annotations, when provided by authors or the archive itself) can be retrieved in FASTA or genbank formats using the EUTILS suite.  The following URL should retrieve the human mitochondrial reference sequence (NC_012920) in genbank format:
http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=nuccore&id=251831106&rettype=gb

##### Genome sequences #####
The following query will retrieve the genome of Candidatus Hodgkinia cicadicola (REFseq accession NC_012960.1) in fasta format:
http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=nucleotide&id=NC_012960.1&rettype=fasta

A table describing the supported formats is here:
http://www.ncbi.nlm.nih.gov/books/NBK25499/table/chapter4.chapter4_table1/?report=objectonly
Note that these queries return data in several different formats, some return XML-formatted data structures while
others return files in gb and fasta formats.  Of course, how you handle the data after retrieving
it is going to depend on the data format.

Biopython has subroutines that take EFETCH's options and parameters and return python objects.
This saves us from having to write code that directly talks to NCBI's EFETCH API, freeing us to spend our time elsewhere.  
We just need to find out how to use these subroutines.

Here is a example using the `Entrez.efetch`  biopython method to retrieve fasta sequences:
```python
#!/usr/bin/env python
import os,sys
from Bio import Entrez
from Bio import SeqIO

def downloadstuff(accessionno):
    filename = "%s.fasta" % accessionno        
    print "Trying efectch on %s, writing to %s" % ( accessionno, filename )
    if not os.path.isfile(filename):  
        net_handle = Entrez.efetch(db="nucleotide",id=accessionno,rettype="fasta", retmode="text")
        out_handle = open(filename, "w")
        out_handle.write(net_handle.read() )
        out_handle.close()
        net_handle.close()
    else:
        print "skipping, %s already exists!" % filename

Entrez.email = "swc@example.com        #  Always tell NCBI who you are.  
Entrez.tool = "SoftwareCarpentryBootcamp"

accession = sys.argv[1]     # take the first program argument

downloadstuff(accession)   
```


####Fastq sequence parsing####
FASTQ is the de-facto standard data format for the output of modern high-throughput sequencing machines.
In addition to sequence ID and header, this format includes a quality symbol for each base.

One way to parse fastq is using exactly the same `SeqIO.parse()` method, just with `fastq` instead of `fasta` as the format parameter.
The sequence is in the `seq` attribute and the quality scores (as ints) is in the letter_annotations["phred_quality"] attribute.  

Another approach is to use `FastqGeneralIterator`.  Unlike `SeqIO.parse()` it takes file handles (not file names) and has no format parameter (it only works for fastq).  It returns tuples with the sequence description, the sequence string, and the quality string without additional methods to interpret and format the results.   The following code snippet opens the file tiny.fastq and writes truncated versions of the data to standard out.  (Note that if any of the input sequences are less than 30 base pairs in length, this code breaks.)

```python
from Bio.SeqIO.QualityIO import FastqGeneralIterator
in_handle = open("tiny.fastq")
iterator = FastqGeneralIterator(in_handle)
for triplet in iterator:
     (description, sequence, quality) = triplet
     print "@%s\n%s\n+\n%s" % ( description, sequence[0:30], quality[0:30] )
```

This approach is faster, but doesn't put the FASTQ-format-interpretation methods at your disposal, so you have to decode the quality scores, which are encoded as ASCII, yourself.


Notes on new Biopython module,  SWC boot camp at Tufts June 3-4, 2013, W. Trimble+A. Ahmadia

The initial biopython module included several demonstration examples of "when are we going to use this," several follow-along code snippets, and exercises giving partially-functional code with unimplemented subroutines.

The demonstration examples were	
* download biological data form a HTTP request / REST API and plot it
* produce a composite of 240,000 image files (from which biological big data derive) (http://www.mcs.anl.gov/~trimble/nodi/cell-lg.mp4) (flashy pictures)
The exercises/ examples were
* retrieval of data from NCBI's EUTILS REST API (retrieve data programmatically from a working subroutine)
* iterating over lines of a file and retrieving data programmatically
* looping over records in a fasta-formatted file 
* doing something to the fasta data records -- reverse complimentation method

We didn't get to 
* doing something to the fasta data records -- translation method
* FASTQ parsing / btrimming example and 
* Genbank parsing / format conversion

Backup demonstration examples (not used / not developed yet)
* programmatic concatenation of data files according to a table associating multiple data files with each sample
* "the demultiplexing task" -- sorting data records according string matching of sample label fields with values like ATCACG and GCCAAT

Audience members suggested potentially interesting applications included
* sequence alignment example (BWA bindings)
* multiple sequence alignment example (MUSCLE bindings)
* expensive similarity-search output parsing (BLAST bindings / parsing)

The first exercise (a moderately useful subroutine that is just a binding to a NIH data-delivery REST interface) was well recieved. 

We (Will Trimble + Aron Ahmaidia) went after the second exercise (looping over a data file and running a bio subroutine on data from each line) in a blank ipython notebook.  It seemed to work despite a few hiccups.

The reverse-complimentation method didn't go over a smoothly, in part because the task was in a sense silly ("So you could have done this in one line, calling reverse_complement()?"), and partly it seemed because there wasn't adequate introduction to the syntax.

The students looked tired after a morning of git and an afternoon of biopython, ipython notebook, and biopython debugging.  For this reason, flashy examples (and code that does Neat Stuff) might be good to stack at the end of the session, by which time some of the students are burned out.


Lecture materials originally contributed by Will Trimble. 

This module covers some aspects of programming in Biopython and programming for biological sequence analysis.

##How to program if you must##

If a bioinformatic task is relatively common, it is likely that **someone else has written software to do it already**.

Format conversions, paired-read merging, quality trimming, recruitment to references, diploid SNP calling, haploid
re-sequencing--these are all problems that can be solved by finding out what software purports to do the job, 
figuring out how to install it, getting the data into the right input formats and getting the results out of 
whatever formats the tools write to.  

Depending on the complexity of the task and the ease-of-use and scope of the existing alternatives, it could be 
easier to adapt an existing software library or it could be easier to write the code to do it yourself.  

This section describes programming using the Biopython libraries when you must.
In general you will have your own data, you will need to change its format and do stuff to it, 
you will get some reference data, perform some comparison operation, and then perform
operations on the results of the comparison in view of your hypotheses.

Biopython affords high-level access to subroutines that access the biological sequence data types, databases, and bread-and-butter tools.

###Optimizing time###
Solving any particular data manipulation problem takes a certain amount of time, time that should include the time to 
fix the mistakes and confirm that your code is really is doing what you think it is.   

It is dramatically easier to write a program that you will use once and then throw away than to write a program that is useful to you again and again.  Ask: do you really want to solve a particular (easy) format-conversion problem six times, once for each new collaboration and each new dataset?  If you invest effort in identifying what you need and what parts you will use again and again, you can forget the details of how you solved this (boring) problem in the first place and direct your time to more interesting things.

###An anecdote: using python to get and plot data from a web interface###
One day, a colleague of mine showed me that the MG-RAST website had an interface that would deliver a bundle of data about a dataset in response to an HTTP request.  Specifically, the request 
http://api.metagenomics.anl.gov/metagenome_statistics/mgm4440613.3?verbosity=full
has tables of numbers representing the length distribution, GC-content, and high-level summaries of the taxonomic annotations of an NGS dataset.  The data bundle may not display conveniently in all browsers, but there's a lot of good data in there, encoded in JSON format. http://en.wikipedia.org/wiki/JSON 
Fortunately, there is a python module to painlessly parse JSON into a python dict of dict.
The script `metagenome_statistics-example.py` contains example code that retrieves data from the website, gets some of the data out of the JSON structure, and plots it.  Python code that solves the sub-problems (retrieving data via HTTP, getting data out of JSON objects, and plotting) has already been written, so I spend my time invoking and debugging calls to these subroutines instead of finding out how to write a HTTP client or a JSON parser.  

##Biopython##
Biopython has a large collection of subroutines that do potentially useful things with biological data.
Biopython is described in *Biopython: freely available Python tools for computational molecular biology and bioinformatics.* Bioinformatics. 2009 Jun 1;25(11):1422-3. doi: 10.1093/bioinformatics/btp163. http://www.ncbi.nlm.nih.gov/pubmed/19304878?dopt=Abstract
and has a detailed *Biopython Cookbook and Tutorial*
http://biopython.org/DIST/docs/tutorial/Tutorial.html describing many of the things that it does.  

For these examples to work, you need to be able to run
```python
from Bio import SeqIO
```
from your python environment and not get an error.   Biopython is included with the anaconda python distribution; for the Canopy python distribution it is available as a module. 

We will show 
* an example of how to get data from NCBI using EFETCH

* examples of how Biopython goes through FASTA, FASTQ, and GENBANK data and where the data is inside of the resulting objects 

* we will give some exercises for doing things to sequences one at a time, like retrieving sequences with a given id or automatically selecting one gene from an annotated genome.

###Get reference data--NCBI's EUTILS###
Sequence comparison is at the heart of bioinformatics; to do useful comparisons, you need data (sequences) against which to compare your new, exciting sequences.  NCBI provides an interface to allow automated download of various (sequence) data products using HTTP GET requests.

The documentation for this interface, called EUTILS, is here:
http://www.ncbi.nlm.nih.gov/books/NBK25499/

Pretty much anything you can do at an NCBI web site or search engine can be done using EUTILS -- retrieving pubmed abstracts, searching SRA for NGS datasets, searching for and retrieving reference genomes.  Here we will describe retrieving sequence data (protein sequences, genome sequences, or genomes with annotations) using `efetch`.  Other routines are described in Eutils.md.

Biopython has subroutines that take EFETCH's options and parameters and return python objects.
This saves us from having to write code that directly talks to NCBI's EFETCH API, freeing us to spend our time elsewhere.  
We just need to find out how to use these subroutines.

`retrievegbk.py` contains an example of using the `Entrez.efetch`  biopython method to retrieve genbank records by specifying their accession number.  

```python
#!/usr/bin/env python
'''This is an example of Entrez.efetch. http://www.ncbi.nlm.nih.gov/books/NBK25499/#_chapter4_EFetch_ '''

from Bio import Entrez

accessionno = "NC_012920"   # This is the human mitochondrion reference sequence 
Entrez.email = "swc@example.com        #  Always tell NCBI who you are.  And fix the SytnaxError.
Entrez.tool = "SoftwareCarpentryBootcamp"
assert Entrez.email != None     # Check that you told NCBI who you are before continuing

# Entrez.efetch is the subroutine; 
# db, id, rettype, and retmode are parameters of EFETCH
# and read() is the method that gives us the output as one big string.

genbankdata = Entrez.efetch(db="nucleotide", id=accessionno, rettype="gb", retmode="text").read()

print genbankdata

# Let's write this to a file
filename = "%s.gbk" % accessionno
open(filename, "w").write(genbankdata)
```

Note that all we did was get the data and dump it to a file here; we will go through the data and look at what is inside later.  Note: this code snipped (and `retreivegbk.py`) contain a syntax error--fix the syntax error and give your local version of the script your email address.  You want to tell NCBI who you are as a matter of politeness (they are giving you data for free).  In case your script goes horribly wrong, and you mistakenly launch a denial-of-service attack against NCBI, NCBI might send you an email letting you know.  The guidelines for automated download of data from NCBI include the guidance
>In order not to overload the E-utility servers, NCBI recommends that users post no more than three URL requests per second and limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays. 

Easy Exercise:
Use `retreivegbk.py` to get NC_001422, the genome of Phi-x174, in genbank format.  We will use this below. 

Exercise:
`ladyslipperITSaccessionnumbers.txt` contains 94 accession numbers for the ITS ribosomal marker sequences of certain lady slipper orchids.
Use the `retrievegbk()` subroutine in `retrievegbk.py` to request to download all 94 sequences.  
Once you have the sequences, you can convert them to FASTA, concatenate the FASTA, and run a multiple-sequence-alignment program.  

### Iterating through data records ###

Biopython provides a variety of methods for stepping through data sources one record at a time.  There is variety in the places we

* Data sources can be web interfaces, filenames, or file handles.  

* Data types can include amino acid sequences, short-read nucleic acid sequences with or without qualities, draft genomes in hundreds of contigs,  or complete genomes with gene coordinates, translations, and additional notes about how the genes were identified.    These are normalized into the `SeqRecord` data type.

* The access procedures include opening a data source and loading it all into memory at once, as either a list or a dict, or reading a data source one record at a time. 

####Parsing####

We can open `data/tiny.fasta` using `SeqIO.parse()` 
As a troubleshooting mechanism, python's `dir` method will list the methods and attributes that are defined for this object:

```python
from Bio import SeqIO
generator = SeqIO.parse("data/tiny.fasta", "fasta")
type(generator)
dir(generator)
```
We find that `Bio.SeqIO.parse` method returns a *generator*, an python object with methods to let us access the data.  We can put this generator in a `for` loop and access each record one at a time, or we can call `list(generator)` to load all the records into memory at once, or we can call `SeqIO.to_dict(generator)` to load all the records into a dict at once.     (If we don't have enough memory to load all the data at once--which will often be the case with shotgun data--we need to find ways to write programs that don't require all the data in memory.)  Or we can use `record = generator.next()` to step through the records until we get a `StopIteration`.

`SeqIO.parse` takes the format as a mandatory second parameter.  fasta, fastq, genbank, and embl are among the supported formats.  

Using `next()` just once will give us the first `SeqRecord`, so let's look at it:
```python
firstrecord = SeqIO.parse("data/tiny.fasta", "fasta").next()
print type(firstrecord)
print dir(firstrecord)
```
We can print each of these, and find out whether each is a method or an attribute.
The attributes, `firstrecord.id` and `firstrecord.seq` contain the data we are looking for.
Other attributes such as `firstrecord.annotations` and `firstrecord.features` are only populated for input data types richer than fasta.

To review, `Bio.SeqIO.parse()` returns a generator.  Looping through this will produce `SeqRecord` objects, one for each sequence or contig.  These have `Seq` and `SeqFeature` objects inside of them.  

We can loop through all the records, one at a time, using `for`:
```python
generator = SeqIO.parse("data/tiny.fasta", "fasta")
for seqrecord in generator:
    print seqrecord
```

This print statement gives us a human-readable summary of the contents of the seqrecord, but not necessarily the whole thing. 

While we might expect `seqrecord.seq` to return a string, it returns an object of the type `Bio.Seq.Seq`.  Strings have 
methods like `find()` and `upper()`.  `Seq` objects additionally have methods like `reverse_complement()` and `translate()`.
In some cases, you can use `Seq` objects in place of strings, in other places an explicit format conversion using `str()` is necessary.

```python
print "This is a STRING".upper()
print "This is a STRING".lower()
```

`Seq` objects have these methods and others that perform biological sequence-manipulation tasks:
```python
print firstrecord.seq.upper()
print firstrecord.seq.lower()
print firstrecord.seq.reverse_complement()
print firstrecord.seq.translate()
```
So `Seq` objects know what their sequence is and they know how to translate themselves.  I can iterate over the sequences in the file and print the sequence and the reverse complement like this:
```python
generator = SeqIO.parse("data/tiny.fasta", "fasta")
for seqrecord in generator:
    print seqrecord.id, len(seqrecord.seq)
    print str(seqrecord.seq)
    print str(seqrecord.seq.reverse_complement())
```
Finally, you can get reminders of the recipes for accessing data and descriptions of how to use SeqIO from the manual page on `SeqIO`:
```python
help(SeqIO)
```

Exercise:  
Modify the existing program `exercise-reversecomplement.py` to output fasta whose sequences have been reverse-complemented.  

####Fasta sequence parsing####
The minimal data type for sequence data, this format includes only a text record description and a (possibly long) sequence.  Nucleic acid sequences, partially-ambiguous nucleic acid sequences, and amino acid sequences can all be encoded in this bare-bones format.  

`SeqRecord` data types have the attributes
* `.name`  which is the **fasta id** -- all the text before the first whitespace on the header line

* `.description` the entire header line, including the fasta id and anything after the first whitespace

* `.seq`  the sequence, as a `SeqIO.Seq` object

```python
from Bio import SeqIO
for seqrecord in SeqIO.parse("tiny.fasta", "fasta"):
     print seqrecord.name, seqrecord.seq
```

Exercise:  
Modify the existing program `exercise-allsixframes.py` to read in nucleic acid FASTA and output six sequences, representing the translation of each sequence in all six reading frames.  Hint:  Slicing works on `Seq` objects like it does on strings, so if `seq` is one of the `seqrecord.seq` objects, 
`seq[1:]` and `seq[2:]` the sequence with the first character chopped off, and the sequence with the first two characters chopped off, respectively.  
Hint: `Seq` objects also have  `seq.reverse_complement()` and `seq.translate()` methods that return `Seq` objects with the reverse complement and translation, defaulting to the standard prokaryotic code.


Hard exercise: 
Modify the program `skeleton.py` to count the number of sequences and the number of bases of each type in a fasta file.  (Hint: Decide what type of data structure you want to use to store the base counts before you begin.  What subroutines do you want?)

####Fastq sequence parsing####
FASTQ is the de-facto standard data format for the output of modern high-throughput sequencing machines.
In addition to sequence ID and header, this format includes a quality symbol for each base.

Fastq can be parsed using exactly the same `SeqIO.parse()` method, just with `fastq` instead of `fasta` as the format parameter.
The sequence is in the `seq` attribute and the quality scores (as a list of ints) is in the `letter_annotations["phred_quality"]` attribute.    This snippet just loops through a small example fastq file and prints the data fields:
```python
from Bio import SeqIO
generator = SeqIO.parse("data/tiny.fastq", "fastq")
for sequence in generator:
     print sequence.id
     print sequence.description
     print sequence.seq
     print sequence.letter_annotations["phred_quality"]
```
This snippet will shorten the sequences to include only the first 30 base pairs of each read:
```python
from Bio import SeqIO
generator = SeqIO.parse("data/tiny.fastq", "fastq")
for sequence in generator:
     shortsequence = sequence[0:30]
     sys.stdout.write(shortsequence.format("fastq"))
```

Exercise:  
`exercise-b-trim.py` contains a fastq parser.  Write a subroutine to perform "B-trimming" -- removing bases that have very low quality scores from the end of the reads.  You will need to determine how many bases at the end of the read have quality scores of 2 or below and remove them.   The exercise script has the input and the output in place.

####Genbank sequence parsing####

Unlike FASTA and FASTQ, the Genbank format has many optional fields, and the optional fields may be of variable lengths.  
Consequently, the `SeqRecord` data structures created by Biopython's genbank parser have more fields defined, including variable-length data types for things like gene annotations.

Using `NC_001422.1.gbk` as an example, let us examine the data structures that we get from parsing it.

```python
from Bio import SeqIO
generator = SeqIO.parse("NC_001422.gbk", "genbank")
gbrecord = generator.next()  # This grabs the first record.
```

Now let's look it over:

```python
type(gbrecord)
    Bio.SeqRecord.SeqRecord
dir(gbrecord) 
   ...
   'annotations',
   'dbxrefs',
   'description',
   'features',
   'format',
   'id',
   'letter_annotations',
   'lower',
   'name',
   'reverse_complement',
   'seq',
   'upper'] 
```

Of these, the attributes `id` `name` and `description` are top-level attributes which describe each sequence (contig).  These are accessed by `gbrecord.name` `gbrecord.id` and `gbrecord.description`. `grecord.seq` contains the sequence as a `Seq` object. 
`gbrecord.annotations` is a *dict* containing additional details (submitters, date, organism taxonomy) for the sequence as a whole.   
`gbrecord.features` is a *list* of `SeqFeature` objects, if provided, that include the genes, protein coding and RNAs, that the creator of the genbank record have decided to include.

```python
gbrecord.features
```
The first item in this list is a `SeqFeature` object:
```python
type(gbrecord.features[0])
   Bio.SeqFeature.SeqFeature
```

This is a list, so we can iterate through it:
```python
for i in gbrecord.features:
    print i
```
This shows us that the information is there -- looping through all the features of the gbrecord gives us access to everything except the top-level `seq` and top-level `annotations`, for which there is only one per SeqRecord.  

The `SeqFeature` data type has attributes that include  `id` `location` and `strand`.  The `type` attribute encodes whether the feature represents a gene, tRNA, RNA, or other types, and the available data will usually depend on the value of type.  (RNAs do not have translations; some details are included under "gene" and others under the corresponding "CDS".  A final attribute of `gbrecord.features` is `qualifiers` -- a *dict* of *list*s of additional annotation fields.

```python
for i in gbrecord.features:
    if i.type == "CDS" :
        print i.qualifiers
```
`gbrecord.features[2]` is the first protein-coding annotation "CDS".  Examining it, we see that its `qualifiers` attribute has most of the good stuff:
```python
firstcdsfeature = gbrecord.features[2]
print firstcdsfeature
print firstcdsfeature.qualifiers
print firstcdsfeature.qualifiers.keys()
    ['function', 'locus_tag', 'codon_start', 'product', 'transl_table', 'note', 'db_xref', 'translation', 'gene', 'protein_id']
```
We can output a table of all the CDS features, then, by looping over all the features, testing for CDS, and printing the fields we like:

```python
for i in gbrecord.features:
    if i.type == "CDS" :
        print i.qualifiers["locus_tag"][0], i.qualifiers["protein_id"][0] , i.qualifiers["gene"][0],  i.qualifiers["translation"][0] 
```

Oops.  This doesn't work. This is because not all the CDS records have `qualifiers["gene"]` defined, so python raises a KeyError when I try to access qualifiers["gene"] for the feature that doesn't have the key "gene".  We can correct this problem by putting the line that tries to access `qualifiers["gene"]` inside of a `try... except` conditional.  This lets us fill it in with a default value if "gene" isn't defined.
```python
for i in gbrecord.features:
    if i.type == "CDS" :
        try: 
            gene = i.qualifiers["gene"]
        except KeyError:
            gene = "-"  
        print i.qualifiers["locus_tag"], i.qualifiers["protein_id"] , gene, i.qualifiers["translation"]
```
But the data fields have brackets around them.  Why is that?  The data is being returned as lists, so I can get to the data by asking for the first element of each list.  Remember python indexes from zero, so the first element of a list is accessed using the index `[0]`: 

```python
for i in gbrecord.features:
    if i.type == "CDS" :
        try: 
            gene = i.qualifiers["gene"][0]
        except KeyError:
            gene = "-"  
        print i.qualifiers["locus_tag"][0], i.qualifiers["protein_id"][0] , gene, i.qualifiers["translation"][0]
```

Exercise: 
Parse `NC_001422.1.gbk` and generate an amino acid fasta file containing the translations of all the coding sequences.  Hint: What field do you want to use for the FASTA ID?  Do you want to put anything else in the fasta description line?   Put the results in `NC_001422.1.faa` 

Hard exercise: 
Modify the program `skeleton.py` to generate a table of sequence id, sequence length, and the value of `.annotations['taxonomy']` for all the sequences in a genbank-formatted file specified as an argument on the command line.   Run this to generate a summary table of your lady slipper orchid sequences.


###High-throughput data--getting it###
High-throughput sequencing datasets range in size from a few megabytes to a few hundreds of gigabytes in size.  
Some institutions make raw sequence data available by FTP, but the sequence archive is the largest warehouse of raw sequence data.

The NCBI offers a guide to downloading data here http://www.ncbi.nlm.nih.gov/books/NBK47540/
which includes links to downloading the *SRA toolkit*. 
Linux:  http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=std
You can download precompiled binaries for Mac and Windows here: http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software

The sequence read archive maintains its own formats, and its own library of programs to get data out of the SRA format.  The options for the utilities (and the formats themselves) change from time to time, so it is helpful to update update your copy of the SRA toolkit a few times a year.

To illustrate getting short-read sequencing data form SRA, let's get an Illumina sequencing dataset for with the PhiX control genome described at http://www.ncbi.nlm.nih.gov/sra/SRX017204 .  The SRR accession number is SRR036919, and it's a 1x45 bp sequencing run.

We can download the SRA-formatted dataset from here 
wget ftp://ftp.ncbi.nih.gov/sra/sra-instant/reads/ByRun/litesra/SRR/SRR036/SRR036919/SRR036919.sra 
This is only a 300 Mbyte download.  You can expect NGS datasets (particularly shotgun and metatranscriptomic datasets) to be larger.  
`fastq-dump` is the program in the SRA tools that will extract the data form sra format to fastq:
```
fastq-dump SRR036919.sra
```
will extract the sequence data from SRR036919.sra and create SRR036919.fastq

###What is it good for?###
Scripts using the shell, python, and the standard unix can do things that *you can't do by hand* and they can do things you *could* do by hand faster and with fewer mistakes.

###Being smart### 

Life is short and we have better things to do than solve easy problems.   

Here are some meta-strategies:

* Test the accuracy of the procedure on data with known correct answers.   It's the only way you will know.

* Test that all the parts work with each other first with a *small* subset of the data.  If something isn't working, you want to know now, not after ten hours.  Do as much testing and debugging as you can, when it's cheap, before scaling up the the whole zottobyte dataset.
 
* Estimate how long your tasks are going to take, if you can.    For much of sequence analysis, ten times as much data take ten times as long to move, process.

* Plan like you're going to have to do <any particular task> again.  A lot.  You probably are.  You probably made a mistake.  


### Similarity searching ###
Similarity searching is perhaps the fundamental operation of computational biology; comparisons between known sequences and novel sequences are the bread-and-butter of sequence interpretation.  
You can run BLAST on your laptop, on your department's server, or via the NCBI web interface.  
BLAST against large databases is an expensive operation, so if your computational plan requires running BLAST a million times, you probably need to re-think your plan.  
For small numbers of sequences and for high-value sequences (contigs, genomes)  BLAST is extremely popular.

```python
from Bio.Blast import NCBIWWW
mysterysequence = "GCACTTGTCTCCTGTTTACTCCCCTGAGCTTGAGGGGTTAACATGAAGGTCATCGATAGCAGGATAATAATACAGTA"
blastresults = NCBIWWW.qblast("blastn", "nr", sequence=mysterysequence ) 
print type(blastresults)
```

```python
from Bio.Blast import NCBIXML
result_handle = open("my_blast.xml")
blast_record = NCBIXML.read(result_handle)
```

```python
from Bio.Blast.Applications import NcbiblastxCommandline
help(NcbiblastxCommandline)
```


# Compound Data Types: Lists, Dictionaries, Sets, Tuples, and Reading Files

* * * * *

**Based on lecture materials by Milad Fatenejad, Joshua R. Smith, and Will
Trimble**

Python would be a fairly useless language if it weren't for the compound
data types. The main two are lists and dictionaries, but I'll mention sets
and tuples as well. I'll also go over reading text data from files. 

## Lists

A list is an ordered, indexable collection of data. Lets say you have
collected some current and voltage data that looks like this:

```
voltage:
-2.0
-1.0
0.0
1.0
2.0

current:
-1.0
-0.5
0.0
0.5
1.0
```

So you could put that data into lists like

```python
In [1]: voltageList = [-2.0, -1.0, 0.0, 1.0, 2.0]

In [2]: currentList = [-1.0, -0.5, 0.0, 0.5, 1.0]
```

obviously voltageList is of type list:

```python
In [3]: type(voltageList)
Out[3]: <type 'list'>
```

Python lists have the charming (annoying?) feature that they are indexed
from zero. Therefore, to find the value of the first item in voltageList:

```python
In [4]: voltageList[0]
Out[4]: -2.0
```

And to find the value of the third item

```python
In [5]: voltageList[2]
Out[5]: 0.0
```

Lists can be indexed from the back using a negative index. The last item of
currentList

```python
In [6]: currentList[-1]
Out[6]: 1.0
```

and the next-to-last

```python
In [7]: currentList[-2]
Out[7]: 0.5
```

You can "slice" items from within a list. Lets say we wanted the second
through fourth items from voltageList

```python
In [8]: voltageList[1:4]
Out[8]: [-1.0, 0.0, 1.0]
```

Or from the third item to the end

```python
In [9]: voltageList[2:]
Out[9]: [0.0, 1.0, 2.0]
```

and so on.

### Append and Extend

Just like strings have methods, lists do too.

```python
In [10] dir(list)
```

One useful method is append. Lets say we want to stick the following data
on the end of both our lists.

```
voltage:
3.0
4.0

current:
1.5
2.0
```

If you want to append items to the end of a list, use the append method.

```python
In [11]: voltageList.append(3.)

In [12]: voltageList.append(4.)

In [13]: voltageList
Out[13]: [-2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0]
```

You can see how that approach might be tedious in certain cases. If you
want to concatenate a list onto the end of another one, use extend.

```python
In [14]: currentList.extend([1.5, 2.0])

In [15]: currentList
Out[15]: [-1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]
```

### Length of Lists

Sometimes you want to know how many items are in a list. Use the len command.

```python
In [16]: len(voltageList)
Out[16]: 7
```

### Heterogeneous Data

Lists can contain hetergeneous data.

```python
In [17]: dataList = ["experiment: current vs. voltage", \
   ....:             "run", 47, \
   ....:             "temperature", 372.756, \
   ....:             "current", [-1.0, -0.5, 0.0, 0.5, 1.0], \
   ....:             "voltage", [-2.0, -1.0, 0.0, 1.0, 2.0]]

```

We've got strings, ints, floats, and even other lists in there. The slashes
are there so we can continue on the next line. They aren't necessary but
they can sometimes make things look better.

## Assigning Variables to Other Variables

Something that might cause you headaches in the future is how python deals
with assignment of one variable to another. When you set a variable equal
to another, both variables point to the same thing. Changing the first one
ends up changing the second. Be careful about this fact.

```python
In [19]: a = [1,2]

In [20]: b = a

In [21]: a.append(10)

In [22]: b
Out[22]: [1, 2, 10]
```

There's a ton more to know about lists, but lets press on. Check out Dive
Into Python or the help documentation for more info.

## Reading From Files

At this point it is useful to take a detour regarding files. Lets say you
have a file with some current and voltage data and some metadata.

```
data.dat:

experiment: current vs. voltage
run: 47
temperature: 372.756
current: [-1.0, -0.5, 0.0, 0.5, 1.0]
voltage: [-2.0, -1.0, 0.0, 1.0, 2.0]
```

We can read this data into a list type variable pretty easily.

```python
In [1]: f = open("data.dat")

In [2]: ivdata = f.readlines()

In [3]: f.close()

In [4]: ivdata
Out[4]: 
['experiment: current vs. voltage\n',
 'run: 47\n',
 'temperature: 372.756\n',
 'current: [-1.0, -0.5, 0.0, 0.5, 1.0]\n',
 'voltage: [-2.0, -1.0, 0.0, 1.0, 2.0]\n',
 '\n']
```

Right now the data in ivdata isn't in a particularly useful format, but you
can imagine that with some additional programming we could straighten it
out. We will eventually do that.

## Tuples

Tuples are another of python's basic compound data types that are almost
like lists. The difference is that a tuple is immutable; once you set the
data in it, the tuple cannot be changed. You define a tuple as follows.

```python
In [1]: tup = ("red", "white", "blue")

In [2]: type(tup)
Out[2]: <type 'tuple'>
```

You can slice and index the tuple exactly like you would a list. Tuples are
used in the inner workings of python, and a tuple can be used as a key in a
dictionary, whereas a list cannot as we will see in a moment.

## Sets

Most introductory python courses do not go over sets this early (or at
all), but I've found this data type to be useful. The python set type is
similar to the idea of a mathematical set: it is an unordered collection of
unique things. Consider:

```python
In [3] fruit = set(["apple", "banana", "pear", "banana"]) #You have to use a list to create a set.
```

Since sets contain only unique items, there's only one banana in the set
fruit.

You can do things like intersections, unions, etc. on sets just like in
math. Here's an example of an intersection of two sets (the common items in
both sets).

```python
In [4]: firstBowl = set(["apple", "banana", "pear", "peach"])

In [5]: secondBowl = set(["peach", "watermelon", "orange", "apple"])

In [6]: set.intersection(firstBowl, secondBowl)
Out[6]: set(['apple', 'peach'])
```

You can check out more info using the help docs. We won't be returning to
sets, but its good for you to know they exist.

## Dictionaries

Recall our file data.dat which contained our current-voltage data and also
some metadata. We were able to import the data as a list, but clearly the
list type is not the optimal choice for a data model. The dictionary is a
much better choice. A python dictionary is a collection of key, value
pairs. The key is a way to name the data, and the value is the data itself.
Here's a way to create a dictionary that contains all the data in our
data.dat file in a more sensible way than a list.

```python
In [7] dataDict = {"experiment": "current vs. voltage", \
                   "run": 47, \
                   "temperature": 372.756, \
                   "current": [-1.0, -0.5, 0.0, 0.5, 1.0], \
                   "voltage": [-2.0, -1.0, 0.0, 1.0, 2.0]}
```

This model is clearly better because you no longer have to remember that
the run number is in the second position of the list, you just refer
directly to "run":

```python
In [9]: dataDict["run"]
Out[9]: 47
```

If you wanted the voltage data list:

```python
In [10]: dataDict["voltage"]
Out[10]: [-2.0, -1.0, 0.0, 1.0, 2.0]
```

Or perhaps you wanted the last element of the current data list

```python
In [11]: dataDict["current"][-1]
Out[11]: 1.0
```

Once a dictionary has been created, you can change the values of the data
if you like.

```python
In [12]: dataDict["temperature"] = 3275.39
```

You can also add new keys to the dictionary.

```python
In [13]: dataDict["user"] = "Johann G. von Ulm"
```

Dictionaries, like strings, lists, and all the rest, have built-in methods.
Lets say you wanted all the keys from a particular dictionary.

```python
In [14]: dataDict.keys()
Out[14]: ['run', 'temperature', 'current', 'experiment', 'user', 'voltage']
```

also, values

```python
In [15]: dataDict.values()
Out[15]: 
[47,
 3275.39,
 [-1.0, -0.5, 0.0, 0.5, 1.0],
 'current vs. voltage',
 'Johann G. von Ulm',
 [-2.0, -1.0, 0.0, 1.0, 2.0]]
```

The help documentation has more information about what dictionaries can do.

Its worth mentioning that the value part of a dictionary can be any kind of
data, even another dictionary, or some complex nested structure. The same
is true about a list: they can contain complex data types.

Since tuples are immutable, they can be used as keys for dictionaries.
Lists are mutable, and therefore cannot.

When you architect software in python, most data will end up looking either
like a list or a dictionary. These two data types are very important in
python and you'll end up using them all the time.

# Debugging

**Materials contributed by Anthony Scopatz and Patrick Fuller**

## Exercise: What is debugging?

Before I show you the practice (ahem, _art_) of debugging, separate out into groups of 2-3 
people. Follow these steps:

1.  Come up with a definition of debugging.
2.  Write it down on a strip of paper.
3.  Give me the strip of paper.
4.  ???
5.  Profit.

(Bonus Challenge: Make a new friend!)

Time limit: 5 min.

## Why does debugging matter?

Unless you're perfect, you are bound to make errors. Especially early 
on, expect to spend much more time debugging than actually coding. The process 
fits the Pareto principle - you're going to spend \~20% of your time writing 
~80% of your code, and the other ~80% of your time will be spent screaming 
obscenities at your computer (I think that's what the Pareto principle says, 
anyway). Remember to keep calm, and **LEARN** from your mistakes.

## Debugging 101: exceptions, errors, and tracebacks

When your code errors, Python will stop and return an _exception_ that attempts 
to tell you what's up. There are \~165 exceptions in the Python standard 
library, and you'll be seeing many of them very soon. Exceptions to befriend 
include...

```python
SyntaxError # You're probably missing a parenthesis or colon
NameError   # There's probably a variable name typo somewhere
TypeError   # You're doing something with incompatible variable types
ValueError  # You're calling a function with the wrong parameter
IOError     # You're trying to use a file that doesn't exist
IndexError  # You're trying to reference a list element that doesn't exist
KeyError    # Similar to an IndexError, but for dictionaries
Exception   # This means "an error of any type" - hopefully you don't see it often
```

When code returns an exception, we say that the exception was _thrown_ or
_raised_. These exceptions may be _handled_ or _caught_ by the code. Speaking
of, you can handle exceptions in Python like so:

```python
try:
    a = 1.0 / 0.0
except ZeroDivisionError:
    print "Going from zero to hero."
    a = 1.0
```

That being said, there are some things you should keep in mind.
 * Exception handling in your own code should be seen as a last resort. _Never_
   use exception handling where another approach would work just as well.
 * If you have to handle exceptions, be specific in their type. Writing a blanket
   `except Exception` line provides a place for unintended bugs to hide.

When an exception is printed, it often comes with something called a
_traceback_. This is Python's attempt to tell you where the code errored. It
will look like gibberish for a while, but that impression will go away with time.

So, when your code errors, Python tells you 1. why it errored and 2. where it
errored. "Isn't that enough to debug?", you might ask. Well, yeah. It is. But,
if you debug only by running your code, you're going to be spending a lot more
time in the screaming-obscenities-at-your-computer portion of coding. Every tool
discussed below doesn't add much in terms of functionality (they're still just
pointing out errors), but they all help in decreasing debugging time.

## Linting: catching the stupid errors

As I said before, you can debug by simply attempting to run your code. This,
however, is very annoying. First off, the code will always stop at the first 
exception. This means that, if you have ten errors, you'll have to run the code 
ten times to find all of them. Now, imagine that this is long-running code.
Imagine waiting five minutes for your code to run, only to discover it breaks
because of a typo. Doesn't that sound terrible?

Enter linting. "Linting" is the process of discovering errors in a code (typically 
typos and syntax errors... ie. the dumb stuff) before the code is ever run or 
compiled. In Python, this can be accomplished through using the [pyflakes](http://pypi.python.org/pypi/pyflakes/) 
library. It works by statically analyzing your code without running it. This 
means that it can find multiple errors at once, rather than stopping at the 
first exception.

You can run pyflakes on your code by typing

```
pyflakes my_code.py
```

We can take this a step further with integrated development environments, or
_IDEs_. IDEs are (basically) glorified text editors that dynamically lint,
showing you typos as you write your code. You will find that coders generally
have strong opinions on the use of IDEs, either positive or negative. Regardless,
if you want to play around with one, I recommend [Eclipse](http://www.eclipse.org/) 
with the [PyDev](http://pydev.org/) plugin.

## Coding standards: the details matter!

> The one skill that separates bad programmers from good programmers is attention to detail. 
>
> Zed Shaw, _Learn Python the Hard Way_

In a written natural language, there are many ways to express the same idea. To 
make the consumption of information easier, people define style guides to enforce 
particularly effective ways of writing. This is even more true in coding; 
consistent style choices make scripts much easier to read. They become absolutely 
essential as projects become large (>1 person).

Some programming languages (\*cough\* *Java*) have multiple competing standards, 
and it's easy to imagine how messy this can get. Luckily, Python doesn't have 
this issue. The official standard, [PEP8](http://www.python.org/dev/peps/pep-0008/), 
is used everywhere. Unless you plan on hiding all the code you write from the 
outside world, you should learn it.

To help out coders, there are tools to test for compliance. The aptly named 
`pep8` library shows you where your code deviates from the PEP8 standard, and
`autopep8` goes a step further by trying to fix all of your errors for you.
These are both run from the shell, as

```
pep8 my_code.py
autopep8 my_code.py > my_new_code.py
```

These libraries won't always pick up everything, sadly. Furthermore, due to
the desire to maintain backward compatibility, there is some wiggle room in PEP8 
(see [this powerpoint](www.python.org/doc/essays/ppt/regrets/PythonRegrets.ppt) 
of Python regrets, made by the creator of the language). Here are some additional
rules to remember:

**PEP8 conventions missed by the `pep8` checker**

 * Variables and functions should be named  in `snake_case`. No capital letters.
   Classes are named in `UpperCamelCase`.
 * Multiline comments use `"""`, not `'''`.
 * Private methods and variables should be prefixed with an underscore, ie. 
   `_my_private_method()`
 
**Special rules outside of PEP8**

 * _Never_ use tabs. Ever.
 * Use list comprehensions over `map()`, `reduce()`, and `filter()`.
 * Avoid iterating through lists by index whenever possible.
 * Lambda functions should not be saved to a variable.
 
These rules might seem random (probably because they are), but, trust me: they
make collaborative coding _so much_ easier.

## Debuggers: for the deep-rooted errors

Linting will only catch the really obvious errors. For more complex issues,
(ie. bugs), you're going to want to follow the code's logic line by line. One
lazy way to do this is to put `print` statements everywhere, which allows you
to view variables over time. However, this gets messy quickly, and you lose
control of what variables you can see once you start executing.

This is where the Python DeBugger, or _pdb_, comes into play. With it, you can 
step through your code and watch as variables are changed.

All you have to do to use this is import the `pdb` module and call the 
`set_trace()` method.

```python
import pdb

# [ ... ]
# Your code here
# [ ... ]

pdb.set_trace()
```

Now, when you run the code, it will stop at whatever line you put `set_trace()`.
You'll be prompted to give a command. Some common commands include:

 * `continue` continues on to the next time a `set_trace()` line is hit
 * `print \*variable\*` prints the current value of a specified variable
 * `list` shows the source code around the `set_trace()` line
 * `args` prints the values of all the arguments in the current function

There are a lot more options, which can be found [here](http://docs.python.org/2/library/pdb.html), 
but these few should be enough to get you running with pdb.
 
## Profiling: making code fast

So, you've found your errors, those deep-rooted bugs, and even standardized
your code to conform to PEP8. But, for some reason, it's still really slow.
What can we do about this?

The first idea you might have is to time your code. Analogous to the `print`
statement debugging above, you could write some logic to print run times at
various points in your script.

```python
from time import time

t0 = time()
# run your code
print time() - t0
```

You can also time your entire script with:

```
time python my_code.py
```

While these both work, they're either too messy or not detailed enough. What we
really want is a breakdown of how long the computer spends running each part
of our code.

_Profilers_ provide a way to do just this. With Python, run your script in a
shell with this command

```
python -m cProfile -s time my_code.py
```

This returns the amount of functions called in the execution, along with a
breakdown of the time each function took. The `-s time` part sorts the output
by the time taken (which is usually what you care about). A sample output looks 
like

```
   2530004 function calls in 0.789 seconds
   Ordered by: internal time
   
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
  1980000    0.253    0.000    0.253    0.000 my_code.py:23(<genexpr>)
    10000    0.190    0.000    0.780    0.000 my_code.py:16(evolve)
   220000    0.182    0.000    0.436    0.000 {sum}
   270000    0.150    0.000    0.150    0.000 my_code.py:28(neighbors)
        1    0.009    0.009    0.789    0.789 my_code.py:9(my_func)
    50000    0.004    0.000    0.004    0.000 {method 'add' of 'set' objects}
        1    0.000    0.000    0.000    0.000 {range}
```
With this information, you can go back into your code and adjust the logic to 
improve the speed bottlenecks.

## Segfaults: the scourge of C

Segmentation faults, abbreviated _segfaults_, are the worst debug errors in existence. 
Segfaults occur when the program tries to access a part of memory that it 
expects to be able to get to, and for whatever reason it is not available.
Because of this, segfaults only occur at runtime, so many tools won't even
catch it. To make it worse, a code with a segfault will return the most useless
error in the history of errors:

```
Segmentation fault
```

There's no traceback, and no explanation at all. Nothing.

Luckily, Python (generally) handles the memory issues that could cause a
segfault. Unluckily, many Python scripts interface with lower-level libraries;
these can segfault for, like, no reason.

If you happen upon a segfault in Python, you have two courses of action. If the
segfault is rare and affects a nonvital part of the code, then you can wrap it
in a standard Python exception and handle it in a regular manner. For this, the
[faulthandler](https://github.com/haypo/faulthandler/wiki/) library is useful.
If, however, you absolutely need the function, you're going to have to dive
into the C code and fix it yourself.

The tool you'll need to use is [valgrind](http://valgrind.org/), which is a
debugger + profiler + memory leak checker for compiled C code. It's so important
in C that tutorials in the language often include it within the first few
lessons. 

To use it, first compile the code of interest

```
g++ myCode.cc -o myCode
```

Then, run the compiled code through valgrind

```
valgrind ./myCode
```

Valgrind has a lot of features, each of which can be toggled in the command-line
call, but that's beyond the scope of this tutorial. Keep in mind that these
things exist, and, if you ever find yourself reading through some C, do yourself
a favor and go through a tutorial on all of this. I recommend [Learn C the Hard
Way](http://c.learncodethehardway.org/book/), but feel free to use whatever works.

## In conclusion

If you're new to coding, your head's probably already spinning with entirely
too much new information. That's okay. Remember that everything in this
tutorial is supplemental in nature; learning how to code is much more important
than knowing the tools that can help you code faster. However, keep in mind
that these tools all exist, and make it a goal to eventually come back here and
re-learn them with a clear mind.

If you're an experienced coder, these tools are exactly what you need to up
your game. Debuggers, profilers, and linters all save you valuable time, and
the PEP8 checker will really help in collaborative projects. Get used to them,
and the time investment will pay off.

Please document your module here
================================

Welcome to The Hacker Within!
=============================

This is the University of Chicago Sofware Carperntry Bootcamp 2012

Contents
--------

Indices and tables
==================

-   :ref:\`genindex\`
-   :ref:\`modindex\`
-   :ref:\`search\`


# Documentation

**Material by Anthony Scopatz**

Just like version control and testing, documenting your code is the most
important thing you can do as a software developer. As we have seen in
previous sessions with other tools, good documentation is a sublime
experience that should permeate your code.

Documentation is important because it is [the only way that 90% of
people will ever interact with you or your
code](http://blip.tv/pycon-us-videos-2009-2010-2011/pycon-2011-writing-great-documentation-4899042).
In fact, it is the only way that scales up; there are only so many
emails that you can write.

What is disturbing is that documentation is a forgotten after thought
for most developers. It turns out that being able to write software and
being able to write in your primary spoken language are different
skills. Luckily, we are academics so it is in our nature to publish /
write. We have no excuse for bad documentation.

**The Many Stages of Documentation:**

1.  Readmes
2.  User Guides
3.  Developer Guides
4.  Self-Documenting Code
5.  Code Comments
6.  API Documentation
7.  Auto-Documentation

## Readmes

The omnipresent `README` file is typically a plain text file that sits
next to the code. They typically may contain markup but are often quite
terse. The point of a readme file is to provide only the most basic of
information to the user / developer.

Readme files are so common that GitHub will render and display the
readme file for all directories whenever you are browsing a source tree.
Even Linux itself has a readme:

> Linux kernel release 3.x <http://kernel.org/>
>
> These are the release notes for Linux version 3. Read them carefully,
> as they tell you what this is all about, explain how to install the
> kernel, and what to do if something goes wrong.
>
> WHAT IS LINUX?
>
> Linux is a clone of the operating system Unix, written from scratch
> by Linus Torvalds with assistance from a loosely-knit team of
> hackers across the Net. It aims towards POSIX and Single UNIX
> Specification compliance.
> 
> It has all the features you would expect in a modern fully-fledged
> Unix, including true multitasking, virtual memory, shared libraries,
> demand loading, shared copy-on-write executables, proper memory
> management, and multistack networking including IPv4 and IPv6.
> 
> It is distributed under the GNU General Public License - see the
> accompanying COPYING file for more details.
>
> ...

## User's Guides

The next level of documentation are user's guides. These often take the
form of books or pdfs that aim to explain top level architecture and
functionality to possibly novice users. Such documents are extremely
helpful for bringing in new members to the community, going in depth
into the theory (math, biology, physics, chemistry, engineering), and as
a reference manual for advanced users and developers. However because of
their high level nature, you typically have to wait until the code has
stabilized to be able to write a good comprehensive user's guide.

**Examples:**
[FLASH](http://flash.uchicago.edu/site/flashcode/user_support/flash4b_ug.pdf),
[NumPy](http://www.tramy.us/numpybook.pdf).

## Developer Guides

Developer guides are very similar to user's guides except that they
assume a basic mastery of the project. They are typically for people who
want to *become* developers on a project rather than for existing
developers. They are probably most important for code projects that have
plugin architectures and where the line between user and developer is
less well defined.

**Examples:** [Android](http://developer.android.com/guide/index.html),
[Python](http://docs.python.org/devguide/).

## Self-Documenting Code

Much like in testing where you can simply write perfect code the first
time, there is an analogous philosophy is documentation. This is the
philosophy of [self-documenting
code](http://c2.com/cgi/wiki?SelfDocumentingCode). This ethos makes the
claim that it is often possible to write code in such a way that new
readers can understand what the code does simply by reading it.
Therefore, no extra documentation is required. It is all there in the
code itself.

While there are obvious pitfalls with this approach (assumed knowledge
on the reader's behalf, unavoidable complexities, etc) there are some
merits. By having meaningful naming conventions and structure it does
become possible to infer a lot about a code just by glancing at it.
Coding standards come from the same desire to have readable software.

However using this documentation strategy exclusively is *highly*
inadvisable.

## Code Comments

Every language has a special character (or two) which indicate to the
parser, compiler, or interpreter that whatever comes after or between
these characters should be ignored. This allows the author to write
annotate and explain the code that they are writing *right at the point
that they are writing it!* This is especially helpful if something
weird, obtuse, or obscure is about to happen because it gives the author
a chance to explain themselves to future developers (often themselves in
1, 2, 6 months).

The best part is that you can put literally *anything* in comments:
publication citations, ASCII art, messages to lost loves, and threats to
your collaborators.

In Python, the comment character is the hash symbol `#`. The following
example shows how you might help explain a toaster:

```python
def toast(slices, toastiness, msg=None):
    # make sure the toaster has the right setting
    toastiness = int(toastiness) if 0 < toastiness else 5

    print "Engage the bread warming!"
    for slice in slices:
        slice.toast(toastiness)

    # log the message, making a default if needed
    if msg is None:
        msg = "Toasted to level {}".format(toastiness)
    logging.info(msg)
```

However, it is certainly possible to over-document your code with
comments. Comments should never simply repeat what the code itself is
doing. The goal is to strike the right balance. The appropriate ratio
changes with language. (Typically higher level languages have greater
functionality per line and so have more comments.) Try to avoid the
following:

```python
# init a to 0
a = 0

# make b 'a string'
b = 'a string'

# Add one to a
a = a + 1

# stopping excessive comments
self.fall_on_sword()
```

## API Documentation

The application programming interface (API) is the definition of the
protocol that two pieces of code may use to interact with one another.
Consider the case of functions. All functions have a function signature
which specifies how many arguments they accept and their return values.
This signature along with the module name and function name is the API.
(The function object/pointer itself is the implementation and is
independent of the abstract API.)

Just because you have an argument list, however, does not imply that the
meaning of the arguments is known. For example:

```python
def f(a, b=10):
    ...
```

We know that `f()` accepts two argument `a` and `b` and that `b` should
probably be an integer. But what does `f()` actually do? What do these
arguments mean in this context?

Python allows the user to define API documentation right at the
function, class, module, or variable definition. Every Python object may
have an `__doc__` attribute which is a string representation of the API
docs. This is known as a *docstring*.
[PEP257](http://www.python.org/dev/peps/pep-0257/) describes the
conventions for docstrings. The most important of these is that simple
things should have simple docstrings.

Right below a definition, if the first non-comment, non-whitespace line
is an unassigned string literal, then this string is automatically
loaded in as the docstring. It is this docstring which then read by the
`help()` built-in or the `?` in IPython.

```python
def mean(numlist):
    """Computes the mean of a list of numbers."""
    try:
        total = sum(numlist)
        length = len(numlist)
    except ValueError:
        print "The number list was not a list of numbers."
    except:
        print "There was a problem evaluating the number list."
    return total/length


def fib(n):
    """Determines the nth Fibonacci number where n is 
    a non-negative integer.
    """
    if n < 0 or int(n) != n:
        return NotImplemented
    elif n == 0 or n == 1:
        return n
    else:
        return fib(n - 1) + fib(n - 2)

print help(mean)
print fib.__doc__
```

Most Python docstrings are written in a markup language called
[reStructuredText](http://sphinx.pocoo.org/rest.html) (rST). It is
designed to be easy to read, extensible, and provide enough
natural-looking syntax to be able to render nicely. For example, our
toaster docstring might look like:

```python
def toast(slices, toastiness, msg=None):
    """Toast some bread.

    Parameters
    ----------
    slices : sequence of instance of partial bread
        Slices to toast to toastiness level
    toastiness : int
        The desired toaster setting
    msg : str, optional
        A message for the toaster's usage log.

    """
    # make sure the toaster has the right setting
    toastiness = int(toastiness) if 0 < toastiness else 5

    print "Engage the bread warming!"
    for slice if slices:
        slice.toast(toastiness)

    # log the message, making a default if needed
    if msg is None:
        msg = "Toasted to level {}".format(toastiness)
    logging.info(msg)
```

## Auto-Documentation

Automatic documentation is the powerful concept that the comments and
docstrings that the developer has already written can be scraped from
the code base and placed on a website or into a user's guide. This
significantly reduces the overhead of having to write and maintain may
documents which contain effectively the same information.

Probably the three most popular auto-doc projects are
[javadoc](http://www.oracle.com/technetwork/java/javase/documentation/index-jsp-135444.html)
for Java, [dOxygen](http://www.stack.nl/~dimitri/doxygen/) for most
compiled languages, and [sphinx](http://sphinx.pocoo.org/) for Python.

You can build the sphinx documentation by running the following command
and then navigating to the browser:

    make html

Note, that sphinx also allows you to build to other front ends, such as
LaTeX.

**Example:** Let's take a tour of sphinx now!

## Exercise

Add docstrings to the functions in the `close_line.py` module. Then,
using sphinx, generate a website which auto-documents this module.

# Using Python and FORTRAN with F2py 

**Based on Lecture Material by The Hacker Within and Katy Huff**

Motivation: Why use `f2py`?
=============================

Fortran has been around for a while, and it forms the core of much
scientific software. There's much legacy FORTRAN 77 code out there that
works well, is widely tested and optimized. Being FORTRAN 77, no one
wants to touch it unless absolutely necessary.

Python has many advantages over Fortran, and the few areas it is lacking
(primarily execution speed), Python and Fortran are complementary. Where
Python is streamlined and easy to read, Fortran is klunky and prone to
spaghetti code. Python is dynamic, Fortran types everything, even your
spelling mistakes if you're not careful (implicit typing). Python is
young and spry, Fortran is old, pedigreed and barnacled. Python will
nicely tell you exactly where something is going wrong with an exception
and backtrace; often the best you can hope for from Fortran is a seg
fault or bus error, but data corruptions are common and easy to do.

For all its shortcomings, Fortran code -- especially scientific code
with many arrays, etc -- is usually much faster than the Python
equivalent. It would be wonderful to harness the vast libraries of
tested, fast and well-used Fortran code with a sleek Python wrapper.

And that's where `f2py` comes in.

`f2py` is a Fortran to Python interface generator that allows you to
take Fortran code, generate a Python interface and compile it all
together into an extension module. The front-end is Python, the heavy
lifting is done by Fortran. Best of both worlds.

First Step : Getting Help
=========================

Always, we first learn how to get more help. To get some clues about
`f2py`, type f2py in the terminal and you will see hints about usage.

Among the hints about usage, you will find that there is no ordinary
help flag, though there are help flags concerning fortran compilers and
linking. This is good to know, but not super helpful.

Really, help for f2py can mostly just be found on the web :

-   The User's Guide is your friend : http://cens.ioc.ee/projects/f2py2e/usersguide/
-   Google is also your friend. You'll find that f2py might throw errors
    you don't understand. Google them and you'll find the StackOverflow
    forums are full of people with the same error.

Example 0: Hello World
======================

As usual, we want to start with a very simple example provided by the
user manual.

```fortran
C FILE: FIB1.F
      SUBROUTINE FIB(A,N)
C
C     CALCULATE FIRST N FIBONACCI NUMBERS
C
      INTEGER N
      REAL*8 A(N)
      DO I=1,N
         IF (I.EQ.1) THEN
            A(I) = 0.0D0
         ELSEIF (I.EQ.2) THEN
            A(I) = 1.0D0
         ELSE 
            A(I) = A(I-1) + A(I-2)
         ENDIF
      ENDDO
      END
C END FILE FIB1.F
```

The purpose of this simple file is to fill the array you provide with a
Fibonacci series.

In order to Pythonize this code, try:

    $ f2py -c -m fib1 fib1.f

The configuration of my machine requires that I specify the compiler I want
to use, so the command that I'll call is : `f2py -c -m --fcompiler=gnu95
fib1 fib1.f` Once we have run this command, a shared object file has been
created by f2py.

```python
  import fib1
```
Interestingly, if we don't yet know how to use fib1 or the fib module
within it , we can view the docstrings created by f2py.

```python
  print fib1.__doc__
  print fib1.fib.__doc__
```

So, now we know that in order to use the fib code, we need to provide a
numpy (Numeric) array to fill with Fibonacci numbers.

```python
  import numpy as np
  a=np.zeros(10,'d') 
  fib1.fib(a) 
  print a 
```

Example 1: passing scalar arguments
===================================

Let's try a more interesting example. Suppose we have an implicitly
typed FORTRAN 77 function that takes a number of scalar arguments. This
might be a subroutine in a legacy FORTRAN 77 code, for example.

```fortran
      subroutine scalar_args(int_in, real_in, int_inout, real_inout,
     \ int_out, real_out)
C This doesn't do anything interesting, just for illustration.
      int_inout = int_in
      real_inout = real_in
      int_out = int_inout
      real_out = real_inout
        
      end subroutine scalar_args
```

It is easy to wrap this subroutine with `f2py`. Here's how.

First, it is necessary to tell `f2py` the intent of each subroutine
argument. `f2py` provides multiple ways to specify how to generate the
interface -- the easiest is to put `f2py`-specific comments right in
the FORTRAN code.

[scalar_args.f](https://raw.github.com/thehackerwithin/PyTrieste/master/f2py/scalar_args.f)

```fortran
      subroutine scalar_args(int_in, real_in, int_inout, real_inout,
     \ int_out, real_out)
C Here are the f2py-specific comments.
Cf2py intent(in) :: int_in, real_in }}
Cf2py intent(inout) :: int_inout, real_inout
Cf2py intent(out) :: int_out, real_out

      int_inout = int_in
      real_inout = real_in
      int_out = int_inout
      real_out = real_inout
        
      end subroutine scalar_args
```

You'll notice that the intent specifications are very similar to Fortran
90-style intent statements. The `f2py` specific comments start with
`Cf2py` for FORTRAN 77 code, and `!f2py` for Fortran 9x code.

These intent specifications are necessary for `f2py` to generate the
correct interface. If you're writing Fortran 9x code with intent
specifiers already in place, `f2py` will take care of this for you.

To create the extension module, we invoke `f2py` from the command
line. On UNIX/Linux, assuming the above subroutine is in a source file
'scalar_args.f':

    $ f2py -c -m _scalar_args scalar_args.f

The '-c' switch tells `f2py` to compile an extension module, and the
'-m _scalar_args' specifies the name of the extension module. The
fortran source files follow (in this case just one file).

If everything is setup correctly, the above command will compile the
fortran sources into an extension module named '_scalar_args.so' (the
extension will be different for Mac OS X or Windows) located in the
current directory.

We can test this module from python with a python source file named
'pass_args.py':

[pass_args.py](https://raw.github.com/thehackerwithin/PyTrieste/master/f2py/pass_args.py)

```python
  # pass_args.py
  import numpy as np
  import _scalar_args
  
  print _scalar_args.scalar_args.__doc__
  
  # these are simple python scalars.
  int_in = 1.0
  real_in = 10.0
  
  # since these are intent(inout) variables, these must be arrays
  int_inout = np.zeros((1,), dtype = np.int32)
  real_inout = np.zeros((1,), dtype = np.float32)
  
  # all intent(out) variables are returned in a tuple, so they aren't passed as
  # arguments.
  
  int_out, real_out = _scalar_args.scalar_args(int_in, real_in, int_inout, real_inout)
  
  for name in ('int_inout', 'real_inout', 'int_out', 'real_out'):
      print '%s == %s' % (name, locals()[name])

```

Running the above python script should yield the following output:

    scalar_args - Function signature:
      int_out,real_out = scalar_args(int_in,real_in,int_inout,real_inout)
    Required arguments:
      int_in : input int
      real_in : input float
      int_inout : in/output rank-0 array(int,'i')
      real_inout : in/output rank-0 array(float,'f')
    Return objects:
      int_out : int
      real_out : float

    int=inout == [1]
    real_inout == [ 10.]
    int_out == 1
    real_out == 10.0

One nice feature of `f2py` is that it generates informative docstrings
for the wrapped fortran subroutines & functions. In this case, it tells
us that the subroutine 'scalar_args' has a function signature that
takes 4 inputs and returns a 2-tuple. The first 2 inputs are an int and
a float, respectively. These are the 'intent(in)' variables.

The remaining inputs are 'in/output rank-0 array' objects -- these are
simply numpy arrays with a single element (a rank-0 object). These are
necessary since the fortran objects are intent(inout), and there must be
a place to put the output value.

The intent(out) arguments are converted by `f2py` into a return
2-tuple, and are returned by the wrapper function. This is the case for
any Fortran procedure argument that has an intent(out) attribute.

The remainder of the output shows that the subroutine is behaving
correctly.

Let's move on to passing arrays between Python and Fortran.

Example 2: passing array arguments
==================================

Here's the source of a FORTRAN 77 subroutine that takes array arguments:

```fortran

      subroutine array_args(nx, ny, int_arr_in,
     \ cmplx_arr_inout, 
     \ real_arr_out)

          integer nx, ny
          integer int_arr_in(nx, ny)
          complex cmplx_arr_inout(nx, ny)
          real real_arr_out(nx, ny)

          integer i, j

          do j = 1, ny
              do i = 1, nx
                  cmplx_arr_inout(i,j) = cmplx(int_arr_in(i,j),
     \                   int_arr_in(i,j))
                  real_arr_out(i,j) = real(int_arr_in(i,j))
              enddo
          enddo

      end subroutine array_args
```

Nothing special. This contrived example is designed to be similar to
FORTRAN 77 legacy code that has array arguments, with the array extents
passed in explicitly. You should note that in the loop, the arrays are
iterated through in column-major order (i.e. the first index varies the
fastest). This is known in NumPy & `f2py` parlance as 'fortran order'.
We'll have to keep this in mind when passing multi-dimensional arrays
between Python and Fortran, since Python uses row-major ordering, known
as 'C order'. For 2-dimensional arrays, the orderings are the transpose
of each other, and to index the same element, the array indices need to
be reversed.

Let's add in the `f2py` comments to specify the intent of the
arguments:

[array_args.f](https://raw.github.com/thehackerwithin/PyTrieste/master/f2py/array_args.f)

```fortran
      subroutine array_args(nx, ny, int_arr_in,
     \                      cmplx_arr_inout, 
     \                      real_arr_out)

          integer nx, ny
          integer int_arr_in(nx, ny)
          complex cmplx_arr_inout(nx, ny)
          real real_arr_out(nx, ny)

Cf2py intent(in) nx, ny
Cf2py intent(in) int_arr_in
Cf2py intent(inout) cmplx_arr_inout
Cf2py intent(out) real_arr_out

C ... body of subroutine unchanged ...

      end subroutine array_args

```

As you'd expect. We invoke `f2py` from the commandline:

    $ f2py -c -m _array_args array_args.f

Here's a test script similar to the one we saw before:

[pass_array_args.py](https://raw.github.com/thehackerwithin/PyTrieste/master/f2py/pass_array_args.py)

```python
  # pass_array_args.py
  import numpy as np
  import _array_args
  
  print _array_args.array_args.__doc__
  
  # int_arr is a 10 X 10 array filled with consecutive integers.
  # It is in 'fortran' order.
  int_arr = np.asfortranarray(np.arange(100, dtype = 'i').reshape(10,10))
  
  # cplx_arr is a 10 X 10 complex array filled with zeros.
  # It is in 'fortran' order.
  cplx_arr = np.asfortranarray(np.zeros((10,10), dtype = 'F'))
  
  # We invoke the wrapped fortran subroutine.
  real_arr = _array_args.array_args(int_arr, cplx_arr)
  
  # Here are the results.
  print "int_arr  = %s" %  int_arr
  print "real_arr = %s" % real_arr
  print "cplx_arr = %s" % cplx_arr

```

One thing to note here: the `int_arr` and `cplx_arr` are declared
as fortran arrays, (`np.asfortranarray(...)`) since that's what we
want in this case. Their memory layout is fortran contiguous, and the
fortran subroutine won't have any complaints.

The docstring for the wrapped subroutine is again very helpful:

    array_args - Function signature:
      real_arr_out = array_args(int_arr_in,cmplx_arr_inout,[nx,ny])
    Required arguments:
      int_arr_in : input rank-2 array('i') with bounds (nx,ny)
      cmplx_arr_inout : in/output rank-2 array('F') with bounds (nx,ny)
    Optional arguments:
      nx := shape(int_arr_in,0) input int
      ny := shape(int_arr_in,1) input int
    Return objects:
      real_arr_out : rank-2 array('f') with bounds (nx,ny)

The docstring tells us the subroutine takes 2 arguments, the first a
rank-2 integer array, the second a rank-2 complex array (that's the
`array('F')` part). It is unnecessary to pass in the array extents
explicitly, since the extents can be queried `f2py` from the numpy
arrays themselves.

It also tells us the shape and type of the return array.

The script output gives us the following:

    int_arr  == [[ 0  1  2  3  4  5  6  7  8  9]
     [10 11 12 13 14 15 16 17 18 19]
     [20 21 22 23 24 25 26 27 28 29]
     [30 31 32 33 34 35 36 37 38 39]
     [40 41 42 43 44 45 46 47 48 49]
     [50 51 52 53 54 55 56 57 58 59]
     [60 61 62 63 64 65 66 67 68 69]
     [70 71 72 73 74 75 76 77 78 79]
     [80 81 82 83 84 85 86 87 88 89]
     [90 91 92 93 94 95 96 97 98 99]]
    real_arr == [[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.]
     [ 10.  11.  12.  13.  14.  15.  16.  17.  18.  19.]
     [ 20.  21.  22.  23.  24.  25.  26.  27.  28.  29.]
     [ 30.  31.  32.  33.  34.  35.  36.  37.  38.  39.]
     [ 40.  41.  42.  43.  44.  45.  46.  47.  48.  49.]
     [ 50.  51.  52.  53.  54.  55.  56.  57.  58.  59.]
     [ 60.  61.  62.  63.  64.  65.  66.  67.  68.  69.]
     [ 70.  71.  72.  73.  74.  75.  76.  77.  78.  79.]
     [ 80.  81.  82.  83.  84.  85.  86.  87.  88.  89.]
     [ 90.  91.  92.  93.  94.  95.  96.  97.  98.  99.]]
    cplx_arr == [[  0. +0.j   1. +1.j   2. +2.j   3. +3.j   4. +4.j   5. +5.j   6. +6.j
        7. +7.j   8. +8.j   9. +9.j]
     [ 10.+10.j  11.+11.j  12.+12.j  13.+13.j  14.+14.j  15.+15.j  16.+16.j
       17.+17.j  18.+18.j  19.+19.j]
     [ 20.+20.j  21.+21.j  22.+22.j  23.+23.j  24.+24.j  25.+25.j  26.+26.j
       27.+27.j  28.+28.j  29.+29.j]
     [ 30.+30.j  31.+31.j  32.+32.j  33.+33.j  34.+34.j  35.+35.j  36.+36.j
       37.+37.j  38.+38.j  39.+39.j]
     [ 40.+40.j  41.+41.j  42.+42.j  43.+43.j  44.+44.j  45.+45.j  46.+46.j
       47.+47.j  48.+48.j  49.+49.j]
     [ 50.+50.j  51.+51.j  52.+52.j  53.+53.j  54.+54.j  55.+55.j  56.+56.j
       57.+57.j  58.+58.j  59.+59.j]
     [ 60.+60.j  61.+61.j  62.+62.j  63.+63.j  64.+64.j  65.+65.j  66.+66.j
       67.+67.j  68.+68.j  69.+69.j]
     [ 70.+70.j  71.+71.j  72.+72.j  73.+73.j  74.+74.j  75.+75.j  76.+76.j
       77.+77.j  78.+78.j  79.+79.j]
     [ 80.+80.j  81.+81.j  82.+82.j  83.+83.j  84.+84.j  85.+85.j  86.+86.j
       87.+87.j  88.+88.j  89.+89.j]
     [ 90.+90.j  91.+91.j  92.+92.j  93.+93.j  94.+94.j  95.+95.j  96.+96.j
       97.+97.j  98.+98.j  99.+99.j]]

What if we had not declared the `int_arr` as fortran contiguous?
Let's see.

First, let's turn-on array-copying output in the fortran module. This
requires us to recompile the module with a commandline flag.

    $ f2py -DF2PY_REPORT_ON_ARRAY_COPY=1 -c -m _array_args array_args.f

Let's change the pass_array_args.py file thusly:

```python
  # int_arr is a 10 X 10 array filled with consecutive integers.
  # Now it is in 'C' order.
  int_arr = np.arange(100, dtype = 'i').reshape(10,10)
```

When running the script, you will notice an extra line in the output:

```
  copied an array: size = 100, elsize = 4
  int_arr  == [[ 0  1  2  3  4  5  6  7  8  9]
  ...
  real_arr == [[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.]
  ...
  cplx_arr == [[  0. +0.j   1. +1.j   2. +2.j   3. +3.j   4. +4.j   5. +5.j   6. +6.j
  ...

```

The `-DF2PY_REPORT_ON_ARRAY_COPY=1` switch caused `f2py` to
report that it copied an array (int_arr) on input, since it received a
'C' order array as an argument. To avoid this array copy, it is
necessary to declare the arrays as fortran contiguous, with the
`np.asfortranarray` function.

Example 3: using .pyf files and Python callbacks
================================================

The above 2 examples, while simple, cover a large chunk of calling
Fortran from Python with `f2py`. It is possible to call Python from
Fortran, using callbacks.

As a more interesting example, we'll plot the [logistic
map](http://en.wikipedia.org/wiki/Logistic_map), a classic plot exhibiting
self-similarity and period-doubling yielding chaos and fractal structure.
The logistic map is a fascinating system that shows how very simple
nonlinear systems have nearly unlimited richness. It can be used as a very
simple model of year-to-year populations that are limited by resources or
subject to predator-prey dynamics (I'm a plasma physicist, not an
ecologist, so don't harangue me over the details!).

Let's say we have a Fortran subroutine that calculates the equilibrium
points for an iteratively applied function. It takes a function as an
argument, applies the function iteratively `num_iters` times, and
puts the next `n` results of the function in an array.

[chaos.f](https://raw.github.com/thehackerwithin/PyTrieste/master/f2py/chaos.f)

```fortran
      subroutine iterate_limit(func, x0, num_iters, results, n)
          external func
          double precision func
          double precision x0
          integer num_iters, n
          double precision results(n)

          integer i

          do i = 1, num_iters
              x0 = func(x0)
          enddo

          do i = 1, n
              results(i) = x0
              x0 = func(x0)
          enddo

      end subroutine iterate_limit
```

The above is saved in a file `chaos.f`.

This time, rather than put `Cf2py` comments in the Fortran source,
we'll instead generate an interface file.

Call `f2py` thusly:

    $ f2py -h _chaos.pyf chaos.f

This command instructs `f2py` to extract the necessary information
from the fortran source and create an interface file `_chaos.pyf`
that we'll edit accordingly.

Here's the output:

```fortran

!    -*- f90 -*-
! Note: the context of this file is case sensitive.

python module iterate_limit__user__routines 
    interface iterate_limit_user_interface 
        function func(x0) result (x0) ! in :_chaos:chaos.f:iterate_limit:unknown_interface
            double precision :: x0
        end function func
    end interface iterate_limit_user_interface
end python module iterate_limit__user__routines
python module _chaos ! in 
    interface  ! in :_chaos
        subroutine iterate_limit(func,x0,num_iters,results,n) ! in :_chaos:chaos.f
            use iterate_limit__user__routines
            external func
            double precision :: x0
            integer :: num_iters
            double precision dimension(n) :: results
            integer optional,check(len(results)> = n),depend(results) :: n = len(results)
        end subroutine iterate_limit
    end interface 
end python module _chaos

! This file was auto-generated with f2py (version:2).
! See http://cens.ioc.ee/projects/f2py2e/

```

All that remains in this instance is to add in intent specifications to the
interface file, and to remove the line specifying the `n` argument. Here
are the changed lines
[chaos.pyf](https://raw.github.com/thehackerwithin/PyTrieste/master/f2py/chaos.pyf):

```python
            double precision, intent(inout) :: x0
            integer, intent(in) :: num_iters
            integer, intent(in) :: n
            double precision dimension(n), intent(out) :: results
```

Now, we invoke `f2py` a bit differently, to use the interface file.

    $ f2py -c -m _chaos _chaos.pyf chaos.f

Here is the driver script in Python
[chaos.py](https://raw.github.com/thehackerwithin/PyTrieste/master/f2py/chaos.py):

```python
    #!python
    # chaos.py
    import pylab as pl
    import numpy as np

    # we import the fortran extension module here
    import _chaos

    # here is the logistic function
    # this uses some advanced Python features.
    # Logistic is a function that returns another function.
    # This is known as a 'closure' and is a very powerful feature.
    def logistic(r):
        def _inner(x):
            return r * x * (1.0 - x)
        return _inner

    def sine(r):
        from math import sin, pi
        def _inner(x):
            return r * sin(pi * x)
        return _inner

    def driver(func, lower, upper, N=400):
        # X will scan over the parameter value.
        X = np.linspace(lower, upper, N)
        nresults, niter = 1000, 1000
        for x in X:
            # We call the fortran function, passing the appropriate Python function.
            results = _chaos.iterate_limit(func(x), 0.5, niter, nresults)
            pl.plot([x]*len(results), results, 'k,')

    if __name__ == '__main__':
        pl.figure()
        driver(logistic, 0.0, 4.0)
        pl.xlabel('r')
        pl.ylabel('X limit')
        pl.title('Logistic Map')
        pl.figure()
        driver(sine, 0.0, 1.0)
        pl.xlabel('r')
        pl.ylabel('X limit')
        pl.title('Sine Map')
        pl.show()
```

Running the above script yields [attachment:logistic-map.png] and
[attachment:sine-map.png] .

[[Image(logistic-map.png, 50%, center, top)]]

[[Image(sine-map.png, 50%, center, top)]]

The significance to note here is that we are able to pass an arbitrary
Python function (provided it has the right signature!) to Fortran code,
the Fortran code calls the Python function and does something useful
with it. We can easily change which function is passed from Python, thus
achieving a greater degree of flexibility using Python with Fortran.

Conclusions & External links
============================

There's much more to `f2py` than presented here -- here are some
useful links.

 - The `f2py` documentation -- http://cens.ioc.ee/projects/f2py2e/
 - `f2py` is distributed as part of NumPy --
   [http://numpy.scipy.org](http://numpy.scipy.org)/
 - And `f2py` is used to generate much of the wrappers for SciPy --
   [http://www.scipy.org](http://www.scipy.org)/

`f2py` supports some Fortran 9x specific features, and it is possible
to wrap module procedures with `f2py`. Derived types are not
supported, however. Neither are assumed-shape arrays. In short, `f2py`
excels at wrapping FORTRAN 77 code and supports everything any sane
person would want to do with Python and FORTRAN 77.

# Python 3 : Flow Control - Loops, Conditionals, etc

**Based on Lecture Materials By: Milad Fatenejad and Katy Huff**

Pasting into IPython
====================

This part of the lesson includes a lot of text, but it will be useful to
run it yourself in IPython.

To paste text from another application (i.e. these lecture notes) into
IPython :

1.  select text from the wiki
2.  copy with **ctrl+c**
3.  in IPython, type `%paste`

The code should paste and execute in IPython.

If you also type %autocall to turn autocall OFF, you may be able to
paste with **ctrl+v** though this won't work with all IPython builds.

Conditionals
============

A conditional (if statement) is some statement that in general says :
"When some boolean is true, do the following. Elsewise, do this other
thing."

Many equivalence test statements exist in Python that are similar in
other languages:

```python
i=1
j=2
i==j # i is equal to j : FALSE
i<j  # i is less than j
i<=j # i is less than or equal to j : TRUE
i>j  # i is greater than j
i>=j # i is greater than or equal to j : FALSE
i!=j # i is not equal to j : TRUE
```

However, python has other equivalence test statements that are fairly
unique to python. To check whether an object is contained in a list :

```python
beatle="John"
beatles=["George", "Ringo","John", "Paul"]
print beatle in beatles # is John one of the beatles? : TRUE
print "Katy" not in beatles # this is also TRUE.
```

The `is` keyword tests if two variables refer to the same object. For example:

```python
a = 1234
b = 1234
a == b # True, they have the same value
a is b # False, are different objects
```

It is a common mistake to use `is` to test for equality between two objects, see the code below.  This only works for a small range of integers and strings in CPython, and is a side effect of the implementation that should **not** be relied upon.

```python
a = 1
b = 1
a is b # True - special case for 1
```

A correct use of `is` would be to compare objects like lists, for example the same list could be inserted into two different dictionaries. A comparison with `is` would reveal this:

```python
number_list = [1,2,4,8]
dict1 = {"thing_widths":number_list}
dict2 = {"item_costs":number_list}
dict1["thing_widths"] is dict2["item_costs"]  # True - this is the same list
```

Note that since the two dictionary values are actually the same object, modifying the list in one of the dictionaries will mean that the values in both dictionaries will change:

```python
print dict1, dict2
dict1["thing_widths"][0] = 222
print dict1, dict2
```

Conditionals (`if` statements) are also really easy to use in python. Take
a look at the following example:

```python
i = 4
sign = "zero"
if i < 0:
  sign = "negative"
elif i > 0:
  sign = "positive"
else:
  print "Sign must be zero"
  print "Have a nice day"
print sign
```

The behavior of this code snippet should be pretty clear, but there is
something peculiar. How does Python know where the if-statement ends?
Other languages, like FORTRAN, MatLab, and C/C++ all have some way of
delimiting blocks of code. For example, in MatLab you begin an if
statement with the word `if` and you end it with `end if`. In C/C++ you
delimit blocks with curly braces. Python uses **indentation** to delimit
code blocks. The **indentation** above is NOT just to make things look
pretty - it tells Python what the body of the `if`-statement is. This is
true when ever we create any code blocks, such as the bodies of loops,
functions or classes.

**Aside: Compact if-statement:**

Python has an easy to use `if`-syntax for setting the value of a variable.
Try entering this into IPython:

```python
i = 5
sign = "positive" if i > 0 else "negative"
```

While Loops
===========

Lets start by looking at while loops since they function like while
loops in many other languages. The example below takes a list of
integers and computes the product of each number in the list up to the
-1 element.

A while loop will repeat the instructions within itself until the
conditional that defines it is no longer true.

```python
mult = 1
sequence = [1, 5, 7, 9, 3, -1, 5, 3]
while sequence[0] != -1:
  mult = mult * sequence[0]
  del sequence[0]

print mult
```

Some new syntax has been introduced in this example.

-   On line 4, we compute the product of the elements just to make this
    more interesting.

-   On line 5, we use the `del` keyword to remove the first element of
    the list, shifting every element down one.

**Watch Out**

Since a while loop will continue until its conditional is no longer
true, a **poorly formed** while loop might repeat forever. For example :

```python
i=1
print "Well, there's egg and bacon, egg and spam, egg bacon and"
while i == 1:
  print "spam "
print "or Lobster Thermidor a Crevette with a mornay sauce served in a Provencale manner with shallots..."
```

Since the variable `i` never changes within the while loop, we can
expect that the conditional, `i==1` will remain true forever and the
while loop will just go round and round, as if this restaurant offered
nothing but spam. (If you try this at home, please note that one way to
interrupt a non-terminating process is **ctrl+c** or **ctrl+z**).

To create nested if loops, the indentation (preferably two or four
spaces) should increase for each looping level.

```python
weapons=["surprise","fear","ruthless efficiency","an almost fanatical devotion..."]
tries=0
script=""
while tries < len(weapons) :
  i=0
  while i<tries :
    script += weapons[i]
    script += " and "
    i+=1
  script += weapons[tries]
  script += ". "
  if tries == len(weapons) - 1 :
    script += " and nice red uniforms. Oh damn!"
  tries +=1
print script
```

For Loops
=========

For loops in python operate a little differently from other languages.
Lets start with a simple example which prints all of the numbers from 0
to 9:

```python
for i in range(10):
  print i
```

You may be wondering how this works. Start by using help(range) to see
what the range function does.

    Help on built-in function range in module __builtin__:

    range(...)
        range([start,] stop[, step]) -> list of integers

        Return a list containing an arithmetic progression of integers.
        range(i, j) returns [i, i+1, i+2, ..., j-1]; start (!) defaults to 0.
        When step is given, it specifies the increment (or decrement).
        For example, range(4) returns [0, 1, 2, 3].  The end point is omitted!
        These are exactly the valid indices for a list of 4 elements.

Range is a function that returns a list containing a sequence of
integers. So, `range(10)` returns the list [0,1,2,3,4,5,6,7,8,9]. The for
loop then simply iterates over that list, setting i to each value.

For Loops with Lists and Dictionaries
=====================================

With range, we learned that `for` loops in python are really used to
iterate over sequences of things (they can be used for much more, but
for now this definition will do). Try entering the following to see what
happens:

```python
for c in ["one", 2, "three", 4, "five"]:
    print c
```

this is equivalent to:

```python
c = ["one", 2, "three", 4, "five"]
for i in range(len(c)):
    print c[i]
```

With a list, then, it's clear that we can use the `in` keyword to
indicate a list of things. What about a nested loops around a list of
lists?

```python
italy = ["Rome", "Pisa", "Florence", "Venice", "Trieste"]
argentina = ["Mendoza", "Buenos Aires", "Patagonia"]
india = ["Ahmedabad","Kolkata", "Chennai", "Jaipur", "Surat"]
us = ["Chicago", "Austin", "New York", "San Fran"]
nations = [italy, argentina, india, us]
nationnames = ["italy","argentina", "india", "us"]
for nation in nations :
    print nationnames[nations.index(nation)] + ": "
    for city in nation :
        print "  " + city
```

Of course, this information is better stored in a dictionary, isn't it?
The data makes more sense if the keys were the nation names and the
values were lists of cities. Importantly, python has given us a tool
specifically for dictionary looping.

The syntax for looping through the keys and values of a dictionary is :

    for key, value in dictionary.iteritems():

Importantly, you don't have to use the words key and value. That's just
what will fill those variables. Here, we rewrite the previous loop using
this clever syntax.

```python
italy = ["Rome", "Pisa", "Florence", "Venice", "Trieste"]
argentina = ["Mendoza", "Buenos Aires", "Patagonia"]
india = ["Ahmedabad","Kolkata", "Chennai", "Jaipur", "Surat"]
us = ["Chicago", "Austin", "New York", "San Fran"]
nations = {"italy":italy, "argentina":argentina, "india":india, "us":us}
for nation, cities in nations.iteritems() :
    print nation + " : "
    for city in cities :
        print "  " + city
```

break, continue, and else
=========================

A `break` statement cuts off a loop from within an inner loop. It helps
avoid infinite loops by cutting off loops when they're clearly going
nowhere.

```python
reasonable = 5
for n in range(1,10):
    if n == reasonable :
        break
    print n
```

Something you might want to do instead of breaking is to continue to the
next iteration of a loop, giving up on the current one.

```python
reasonable = 5
for n in range(1,10):
    if n == reasonable :
        continue
    print n
```

What is the difference between the output of these two?

Importantly, Python allows you to use an `else` statement in a for loop.

That is :

```python
knights={"Sir Belvedere":"the Wise", "Sir Lancelot":"the Brave", \
         "Sir Galahad":"the Pure", "Sir Robin":"the Brave", "The Black Knight":"John Cleese"}

favorites=knights.keys()
favorites.remove("Sir Robin")
for name, title in knights.iteritems() :
    string = name + ", "
    for fav in favorites :
        if fav == name :
            string += title
            break
    else:
        string += title + ", but not quite so brave as Sir Lancelot."
    print string
```

Other useful python functions
=============================

###enumerate###

Python lists and dictionaries can easily be iterated through in a for loop by using `in`. As we saw above, this is clearer than writing a for loop over the integers up to the length of the list (or dictionary, or other iterable). However, sometimes you may need the index value at the same time, for example for some calculation. The `enumerate` function generates the integer index for you, which can be used instead of the `range` function. The following two loops are equivalent:

```python
data_list = [23,45,67]

for i in range(len(data_list)):
    print data_list[i], ' is item number ', i, ' in the list'

for i,d in enumerate(data_list):
    print d, ' is item number ', i, ' in the list'
```

###zip###

For iterating through multiple sequences, `zip` can be used to group them together to simultaneous pass through each sequence:

```python
run_numbers = [1,2,3,4]
run_times = [12.1, 33.0, 15.1, 22.9]
directions = ['North', 'South', 'East', 'NorthEast']

for n, t, d in zip(run_numbers, run_times, directions):
    print n, t, d
```

Final Example
=============

We've seen a lot so far. Lets work through a slightly lengthier example
together. I'll use some of the concepts we already saw and introduce a
few new concepts. To run the example, you'll need to locate a short file
containing phone numbers. The file can be found in your
repository within the phonenums directory and is called phonenums.txt.
Now we have to move IPython to that directory so it can find the
phonenums.txt file. You navigate within IPython in the same way that you
navigate in the shell, by entering "cd [path]" .

This example opens a text file containing a list of phone numbers. The
phone numbers are in the format \#\#\#-\#\#\#-\#\#\#\#, one to a line.
The example code loops through each line in the file and counts the
number of times each area code appears. The answer is stored in a
dictionary, where the area code is the key and the number of times it
occurs is the value.

```python

areacodes = {} # Create an empty dictionary
f = open("phonenums.txt") # Open the text file
for line in f: # iterate through the text file, one line at a time (think of the file as a list of lines)
    ac = line.split('-')[0] # Split phone number, first element is the area code
    if not ac in areacodes: # Check if it is already in the dictionary
        areacodes[ac] = 1 # If not, add it to the dictionary
    else:
        areacodes[ac] += 1 # Add one to the dictionary entry

print areacodes # Print the answer
```

Example : Iteritems
-------------------

Use the iteritems dictionary method in combination with a for loop to
print the keys/values of the areacodes dictionary one to a line. In
other words, the goal is to write a loop that prints:

    203 4
    800 4
    608 8
    773 3

This example is a little tricky to figure out, but give it a shot.

Python Functions and Modules
-----------------------

A function is a block of code that performs a specifc task. In this section we
will learn how to utilize available Python functions as well as write our own.

# Sections:

* Python methods for strings

* Writing our own functions

* Importing Python modules


Python
======

[Python](http://www.python.org) is a programming language gaining
popularity in the sciences. It's open source, free, and an array of
existing libraries mean you can often find code that will do some task
for you.

Python is also used outside science and engineering as a general
scripting language, on the web, and powering desktop applications. There
is a large, global community of Python users across many disciplines,
making it a useful language to know when your work starts to intersect
with that of others outside your lab or team.

Useful Python Links
-------------------

### Core Python

 - Main Python Docs
   - [http://docs.python.org](http://docs.python.org)/

 - Global Module Index
   - Built in modules like os, sys, datetime, math, random...
   - [http://docs.python.org/modindex.html](http://docs.python.org/modindex.html)
 - Built-in Functions
   - Built-in, always available functions like open, range, enumerate,
>     zip...
   - [http://docs.python.org/library/functions.html](http://docs.python.org/library/functions.html)

 - String Formatting
   - [http://docs.python.org/library/string.html\#formatstrings](http://docs.python.org/library/string.html#formatstrings)

### Python in Science

 - [NumPy](http://numpy.scipy.org/)
   - Fast arrays, used by almost every scientific Python package

 - [SciPy](http://www.scipy.org/)
   - Minimization, fitting, solvers, statistics, and more

 - [matplotlib](http://matplotlib.sourceforge.net/)
   - 2D and 3D plotting, maps

 - [AstroPy](http://astropy.org) for astronomy
 - [Biopython](http://biopython.org/wiki/Biopython) for bioinformatics
 - [Sage](http://www.sagemath.org/) for mathematic analysis
 - [SymPy](http://sympy.org/en/index.html) for symbolic mathematics
 - [pandas](http://pandas.pydata.org/) data analysis and stats for
    categorical and time series data

IPython Notebook
================

The Introduction to Python tutorial will be taught using the
[IPython](http://ipython.org) notebook.

Launching the IPython Notebook
------------------------------

1.  At the command line, `cd` to the `2-PythonIntro` directory.
2.  Type `ipython notebook`.
   - This should open a new tab in a web browser. (We suggest Firefox.)
   - You should see a list of existing notebooks in the `2-PythonIntro`
     directory. Clicking on one will open it in a new tab.
3.  Click the `Exercises` notebook to open it in a new tab. This
    contains the exercises you will work on today.
4.  Go back to the first tab opened by ipython and click the 'New
    Notebook' button. You can use this notebook as your scratch pad
    today. Feel free to rename and save it.

IPython Notebook Commands
-------------------------

 - `shift-Enter`: run the current cell
 - `control-m h`: show keyboard shortcuts
 - `control-m m`: convert cell to a text cell
 - `control-m c`: convert cell to a code cell
 - `control-m d`: delete cell
 - `control-m a`: insert a new cell above the current cell
 - `control-m b`: insert a new cell below the current cell


# Python 1 : The Shell, Variables, and Basic Data Types 

**Based on Lecture Materials By: Milad Fatenejad** 
**With help from Tommy Guy and Many More**

What is Python ?
================

Python is an interpreted (pre-compiled) language. Its simple, high
level, human readable interface speeds the programming process in
exchange for some computation time overhead. Python is implemented
mostly in the C programming language, so, as python develops, it is
increasingly possible to do everything in Python that is possible in C.
Python is also free and open source, so if you find a bug or generate a
useful module, the Python Software Foundation will likely be happy to
merge your changes into the language.

During this session you are going to learn about some very basics about
how to execute python code as well as some examples of the built-in
Python data types.

Built-in data types are the basic building blocks of Python programs.
They are really basic things like strings and numbers (either integers,
complex or floating point numbers). There are simple containers like
lists (think of lists as arrays or vectors), tuples and dictionaries.

Hello World
===========

First, we will use python ''interactively''. This means that we will
type commands directly into iPython. Once we start performing more
complicated tasks we will start writing Python scripts and programs in a
text editor, outside of the interpreter.

To get to the python shell, type **python** into the terminal.

```python
   >>> print "Hello World"
   Hello World
   >>> exit()
```

To get to the interactive python interpreter, a more sophisticated
python shell, type **ipython** into the terminal.

```python
   In [1]: print "Hello World"
   Hello World
   In [2]: exit
```

You can also put the commands in a **.py** file and execute that file in
the terminal by typing **python [filename]**

    $ gedit myfile.py &
    <edit myfile with the hello world program.>
    $ python myfile.py
    Hello World!

Pasting into iPython
====================

**Note:**

To paste text from another application (i.e. the internet browser) into
iPython:

1.  select text from the wiki
2.  copy with **ctrl+c**
3.  in iPython, type **%paste**

The code should paste and execute in iPython.

Variables
=========

Variables are names, while values are the data assigned to those names.

Questions : Variables and Values
--------------------------------

In the code snippet:

```python
    a=2
    b="string"
    c=a
```

 - What is the value of the variable `c`?
 - What is the value of the variable b ?
 - What is the name given to the variable 2 ?

(The last one is a trick, the value 2 has two names.)

Strings and Numbers
===================

It is really easy to make variables in python. For example, to create a
string, `s`, and print its value, simply type the following into
iPython:

```python
    s = "Hello World"
    print s
```

If you want to see what the type of a variable is, you can use the
built-in python function, `type`. Just enter

```python
   print type(s)
```

into iPython and you should see something like this:

```python
      <type 'str'>
```

This tells us that `s` is of type **str** (i.e. that `s` is a
string). Making numeric variables is equally easy and intuitive. Try
entering the following into IPython. Notice that the \# symbol is used
to start comments so everything after the pound sign is ignored.

```python
   i,r,c = -10, 3.5, 1.0 + 2j  # set i to -10, r to 3.5 and c to 1.0+2j
```

This one line sets the variable `i` to the integer -10 , `r` to the
floating point value 3.5 (a floating point number is just a
real/non-integer number) and `c` to the value 1.0 + 2j (Notice, how
easy and intuitive it is in python to set multiple variables to
something. You'll discover a lot of similar syntax that is designed to
make your life easier). Lets use the built-in type function to determine
the type of each of the three variables we just created:

```python
   print type(i), type(r), type(c)
```

This will give :
```python
    <type 'int'> <type 'float'> <type 'complex'>
```

This tells us that "i" is an integer, "r" is a floating point number,
and "c" is a complex number. As you can see, Python has built-in support
for imaginary numbers!

**Aside: Long integers** Another way python makes our lives easier is by
allowing integers to be arbitrary large. In languages like C/C++ and
FORTRAN integer variables can only store values up to a certain size.
But entering and manipulating the following forty digit number with
iPython is no problem:

```python
   i = 1234567890123456789012345678901234567890
   print i * 6
```

Operations in Python are defined by their type. For instance, look the
difference between these operations:

```python
   In[1]:  1 + 3
     4
   In[2]:  1.0 + 3
     4.0  # This is a float
   In[3]: "Hello " + "world"
     'Hello world'
   In[4]: 1 + "Hello"
   Traceback (most recent call last):
     File "<stdin>", line 1, in <module>
   TypeError: unsupported operand type(s) for +: 'int' and 'str'
```

In the first two cases, addition between numbers meant that 1 was added
to 3 using the standard type rules (float plus int = float). In the
third case, the command was string addition, which concatenates two
strings. The final case broke because an 'int' type can not be added to
a 'str' type. This is because it's unclear how to interpret an int as a
string: should it be the string representation, the ASCII character
code, or something else entirely?

One way to handle this is to explicitly convert the int into a string:

```python
     str(1) + "Hello"
```

Equivalent functions exist for converting to **int**, **float**, and
other types.

Basic data types in Python have a lot of functionality already built in.
For example, lets say that you are reading names from a file one line at
a time and that sometimes the names have leading and trailing spaces
that we want to strip away. We can just use the `strip` string method
to accomplish this. For example, type the following into iPython:

```python

  In[1]: name = "   Milad    "
  In[2]: print name + "is here"
        Milad     is here
```

Now enter `name.strip()` instead of `name`:

```python
  In[1]: print name.strip() + " is here"
   Milad is here
```

Notice that the extra spaces are gone. We used the `strip()` method,
which removes leading and trailing white space from strings. You can
think of a method as being a function that is attached to a particular
variable. You call methods by typing: `<variable>.<method name>`.

**Aside : Tab Completion**

Maybe you've noticed this already, but check out what happens you begin
typing a variable name (the first two letters of name, for example) and
press tab.

Convenient, right? This is also true of many built in functions.

Dynamic Typing
==============

Importantly, python is a **dynamically typed** language. That is, an
explicit type is not needed when creating a variable. Also, this means
that variables in Python which are initialized to a variable of one type
can be re-assigned to a variable of a different type. Try this:

```python
     sillystring = "What is the airspeed velocity of an unladen swallow?"
     print type(sillystring)
```

You'll see:

```python
     <type 'str'>
```

If you reassign silly string to an integer, what happens? That is, when
you type :

```python
    sillystring = 98    
    print type(sillystring)
```

You should see:

```python
     <type 'int'>
```

This is an interesting feature. Can you think of ways it can be helpful?
Are there ways it might be troublesome?

What is the type of sillystring be after this :

```python
    sillystring += 0.1
```

**Aside: In Place Equivalency**

What is the += syntax about? This is an in-place way to write `sillystring =
sillystring + 0.1`. It is common in a number of languages.

Importantly, though we do not explcity state them, variables always have
exactly one type. The number 98 is an **int**. For the variable holding
this value to be treated as a float, it must be assigned as **98.0**.

Questions : Dynamic Typing
--------------------------

Imagine that I first assign :

```python
    a=2
```

Then, I assign :

```python
    a="Welcome to the ministry of silly walks."
```

What has happened to the memory that was pointing to the number 2??

Getting Help
============

One of the really nice features in Python is that a lot of the help and
documentation is built into the code. Practically, this means that much
of the time you don't have to go digging through some web site to find
help. You can get help in Python using the `help` function. Lets look
at an example - enter

```python
    help(str.strip)
```

into IPython. You should then see documentation for the strip method pop
up. (NOTE: if you don't automatically return to the python interpreter,
just hit "`q`" to exit the help screen). You can also use the question
mark, "`?`", character to display the documentation as well. For
example, enter

```python
    str.strip?
```

into IPython to view the documentation.

Now try entering

```python
    help(str)
```

You should see documentation for the entire string type, including all
of the string methods. This can be useful when you are trying to perform
a specific task, but you don't know the right function to call. For
example, lets say we want to convert the string "cooper" to uppercase,
and we want to know if there is a string method which can do the job for
us. Start by typing "`help(str)`" to pull up the string documentation.
You can scroll through the string methods until you find a method called
"upper" which has documentation that looks like:

    |  upper(...)
    |      S.upper() -> string
    |      |      Return a copy of the string S converted to uppercase.

These lines tell us that the string class has a method called "upper"
which can be used to convert strings to uppercase. Now enter:

```python
    name = "cooper"   print name.upper()
```

At which point, you should see the word "COOPER" printed to the screen.

**Aside: Using Methods Directly on Data**

* * * * *

In the previous example, we first created a string variable, `name`,
assigned it the value "cooper", then used the `upper` string method to
obtain the uppercased version of the string. We didn't have to create a
variable, however. We could simply enter:

```python
    print "cooper".upper()
```

To generate the uppercased version.

As we saw above, the **str** type has a lot of documentation associated
with it, and we had to sift through most of it to find the upper method.
If we had a way to simply print all of the **str** methods, we could
have probably figured out that the `upper` method is what we wanted by
the name and in a lot less time. Luckily, python has a built in
function, "`dir`", for just this situation. The `dir` function takes
a type name and prints all of the methods associated. Try entering
"`print dir(str)`" to see a list of every method and variable
associated with the string class. You can ignore the methods that start
and end with double underscores for now. Try printing the methods
associated with the **int**, and **complex** types.

Finally, there are some really basic functions that are built right into
python that we have been using. For example, we used the "float" function
above to convert a string to a floating point number. You can see a list of
built in functions by entering `dir(__builtins__)`.  If you see something
interesting, such as the `zip` function, you can examine what it does using
help(zip).

Example : Manipulating Basic Data Types
---------------------------------------

Use the basic data types we've learned about along with the `help` and
`dir` functions to figure out how to do the following using either one
function or one method call:

- Take the absolute value of the number -1.4
- Begin with the string "a MaN and His DOG" and create the string "A man
  and his dog"
- Return the position of the character 'e' in the string "my test string"
  (The answer is 4, since `m` is is at position 0 not position 1)


# Exercises Accompanying the "Building Blocks" Notebook

## Exercise 1

Read the file `grid2.txt` and display the resulting grid. You should
recognize it if you've done it correctly. :-)

## Exercise 2

Build and show the grid saved in `grid3.txt` reading lines from the file
one at a time.

## Exercise 3

Put your code from Exercise 2 in a function and test it out on `grid1.txt`,
`grid2.txt`, and `grid3.txt`.

## Exercise 4

Use the `glob` function to get a `list` of all the "frame" files in the
`animation` directory. Remember that `glob` works just like `ls`.

## Exercise 5

Use your function from Exercise 3, the list of files from Exercise 4, and
the `.flash()` method to successively display each grid. It will make a short
animation when done correctly!

# Practicing Python with `ipythonblocks`

## Overview

These lessons use [`ipythonblocks`][] to teach the basic usage of Python.
Making the most of `ipythonblocks` requires an understanding of RGB colors
so students should start with the "Color" notebook and spend a little time
practicing mixing their own colors. They may wish to leave this notebook
open as they move on so they can refer back to their color experiments.

The "Playing with Blocks" lesson goes into Python syntax, especially
indexing, `for` loops, and `if` statements. Finally, the "Building Blocks"
lesson goes into reading files and building functions.

[`ipythonblocks.py`][] is packaged here with the lesson so there's nothing
to install.

## Lesson Plans

*Note: These notebooks have some code pre-filled, such as imports.
Explain to the students that even though this code is already written in the
notebook, they must still explicitly execute those cells for them to have
any effect.*

### Color

#### Learning Goals

- Use IPython's help features
- Call functions
- Understand RGB colors

#### Introduction

Two functions are pre-imported: `show_color` and `embed_colorpicker`. Also
imported is a dictionary called `colors`.

#### RGB Colors

- Demonstrate using IPython's help features to see what these do, then explain
  [RGB colors][RGB] using `show_color` or other aids.

- Let them loose on the exercise, then show them how to use the [color picker
  tool][colorpicker] available via the `embed_colorpicker` function (requires an
  internet connection).

    *Hint: The RGB definitions of the colors for the exercise are visible in the
    raw Markdown cell.*

- Finally point out that the `colors` dictionary contains all of the
  [HTML colors][], keyed by camelcase name.
    - Echoing `colors` in the notebook should pretty-print it.

### Playing with Blocks

#### Learning Goals

- Use IPython's help features
- Assign variables
- `for` loops (both iterator and `range` style)
- `if` statements
- indexing

#### Introduction

In this notebook the `BlockGrid` class has been imported for students.
The exercises are in the [play_with_blocks_exercises.md][playing exercises]
file.

#### Variables

- Use IPython's help features to look at `BlockGrid`'s doc string.

- Demonstrate how to make a new `BlockGrid` object and assign it to a variable.
  This is a good chance to explain keyword arguments.

- Show how to display the grid using the interactive echo and the
  `BlockGrid.show()` method.

- Exercise 1

#### Basic Indexing

- Assign the first block of `grid` to a variable and change it's color,
  then display the grid.

        block = grid[0, 0]
        block.rgb = (0, 0, 0)
        grid

- Explain Python's zero based indexing, the coordinate system of the grid
  and that indices are `[row, column]`.

- Exercise 2

- Exercise 3
    - You can use `[-1, -1]` to get the lower-right block and explain
      Python's negative indexing.

- Exercise 4

#### Basic Loops

That's enough changing blocks one at a time, now for loops!

- Set the color of every block to something using a `for` loop:

        for block in grid:
            block.rgb = (0, 0, 0)

    This will probably be the first introduction of Pythonic indentation,
    so talk about that.

    Then demonstrate doing the same thing with the `.animate` attribute,
    which will show the changes as they happen:

        for block in grid.animate:
            block.rgb = (12, 123, 234)

- Exercise 5

#### Introducing If

Now to add logic so we can make some blocks different colors from others.

- Show an example `if` statement by looping over the grid but changing only
  one row. This will involve introducing the `block.row` attribute.

        for block in grid.animate:
            if block.row == 2:
                block.rgb = (0, 0, 0)

    A couple of new things are introduced here:

    - Using `==` for comparison vs. `=` for assignment.
      You might take this opportunity to introduce all of the comparison
      operators.
    - Indenting again for the `if` block.

    Also mention the `block.col` attribute.

- Exercise 6

- What if we want to loop over the grid once and turn the first row black,
  the third row white, and every other row blue? `elif` + `else`! Demo doing
  this.

- Exercise 7

- Now for `and`/`or`. Demo using `or` to change color of multiple columns
  with one loop through. This contrasts with above where we wanted to change
  multiple columns multiple colors, now we want to turn multiple columns the
  same color.

        for block in grid.animate:
            if block.col == 2 or block.col == 4:
                block.rgb = (50, 50, 50)

- Exercise 8

- Show the students that blocks have `.red`, `.green`, and `.blue` attributes
  they can use see the value of individual block color channels. (These can
  also be used to change the color values one at a time.)

- Exercise 9

#### Looping with `range`

So far the students have been looping over the entire grid, but we should also
introduce `range` so they can work on smaller sections of the grid without
looping over the whole thing.

- Take a look at the docstring for `range`.
- Show an example of changing the color of a single row by looping over
  `range(grid.width)` and varying only the column index.

- Exercise 10

- Show an example of using a nested loop to change a 2D subsection of the
  grid somewhere near the middle.

- Exercise 11

#### Slicing

`BlockGrids` support standard Python slicing, for example something like
`grid[2:4, 1:3] = (0, 200, 0)`.

- Demonstrate slicing in various ways, keeping a dialogue with the students
  about what to expect from each statement.

- Excercise 12

#### Free Play

There are many opportunities for [creativity with `ipythonblocks`][fun blocks],
give the students a while to try to make whatever they like. Suggest the
possibilities if they relate block color to block position. Some possible
prompts if they need ideas of things to draw:

- Initials
- Shape like a circle, heart, star, etc.
- Repeating pattern
- Rainbow
- Maze

If they've learned about [GitHub][] and set up accounts there they can put
their notebooks in [gists][] and share them via nbviewer. Demo how to do this
if it seems like something they'd be interested in. You can even show some
of their work!

### Building Blocks

#### Learning Goals

- Work with lists and strings
- Read a Python stacktrace
- Read files
- Use the standard library
- Write a function

#### Introduction

In this set of exercises we'll go into reading simple text files and
encapsulating the reader code into a function so they can reuse it on several
files. There is a sort of "standard" `ipythonblocks` file format that is the
output of calling the `BlockGrid.to_text` method. Here's a small but complete
example from [`grid1.txt`][]:

    # width height
    3 3
    # block size
    20
    # initial color
    0 0 0
    # row column red green blue
    1 1 255 255 255

Lines beginning with `#` are comments. The first six lines specify the
initial state of the grid and at the end are any further modifications,
specified by block index and color.

Reading files introduces a lot of new concepts: files themselves, strings,
and even lists depending how you do it. We'll try to approach these in a
manageable fashion.

Exercises for this section are in the
[building_blocks_exercises.md][building exercises] file.

#### Opening and Reading a File

- Use IPython's help to look at how to use `open`.

- Open `grid1.txt` and use tab completion or `help` to look at the methods
  available on the file, review your favorites.

- Go over `.readlines()` if you haven't already and then use it to read
  `grid1.txt`.

#### Lists and Strings

- Show the result from `.readlines()` and note it's comprised of some new
  things we haven't seen yet: some kind of sequence containing
  character strings.

- Explain the sequence thing is called a list and it works a lot like their
  grids.
    - zero based indexing and negative indices
    - `for thing in sequence`
- Use tab completion just to give the students some idea what lists can do.

- Grab a line from the list and show it, explaining that it's a text sequence
  we call a "string".
- Show that strings are also sequences like lists and grids, e.g. indexable.
- Again, use tab completion to show some of the methods on strings. Mention
  `split()` because we'll be using it soon.
- `print` the string and note the extra empty line that shows up, then echo
  or `repr` the string and note the `\n` at the end.
- Explain `\n` is the symbol for "new-line" and we'll take care of it soon.

#### Recipe for a `BlockGrid`

At this point we can grab things out of the list and we know a little about
strings so let's get started on a "recipe" for building a `BlockGrid` out
of the information in the file. Before getting started on the next step ask
the students to figure out what the resulting grid should look like. You can
lead them to the answer by first looking at the dimensions of the grid, then
the fill color, and finally the modifications.

- Work with the students on the recipe, asking them what to do first,
  second, and so on until you've created a feasible looking block of code.
- Try to run it. It will probably fail because the inputs haven't been
  converted to integers.
- Use this opportunity to introduce and show how to read backtraces.
- Show how to convert strings to integers and floats.
- Work with the students to fix the code and try again.
- Repeat as necessary until you get the desired result.

#### List Slicing

For files with longer lists of grid modifications at the bottom students will
want to use a loop to apply them, but we haven't yet covered list slicing.

- `cat grid2.txt` to make the point that making all those modifications
  one-per-line would be tedious, we want to automate it!
- Show some examples of list slicing, noting common gotchas such as
  exclusive endpoints.

- Exercise 1

#### Reading a File One Line at a Time

In general in Python, and especially for large files, it's common to read
files one line at a time instead of loading the whole thing with `readlines()`.

- Work with the students again to make a new recipe for reading `grid2.txt`
  in which lines are read from the file one at time.
    - You will need to introduce `readline()` and `for line in file`.

- Exercise 2

#### Code Re-use With Functions

In this section the students are going to construct 41 grids from 41 files
and string them together into an animation using the `BlockGrid.flash()`
method. Reading 41 files is obviously not something they'll want to do by
copying out the file reading code 41 times.

- Describe the problem the students are facing.
- Introduce functions and give some demonstrations. Functions that:
    - print something
    - print an argument
    - print multiple arguments
    - return something
    - return multiple somethings
    - does something to an argument and returns the result
- Ask the students what the input and output would be for the function we need.

- Exercise 3

#### Introduction to the Standard Library

Now that we have a reader we need files to pass into it. This is a good
opportunity to introduce the [`glob`][] module and point at the rest of the
[standard library][pystd].

- Talk about `import` and that fact that we've been importing things out of
  the [`ipythonblocks.py`][] file located in this directory all along.
- Python has a vast library of useful code accessible via `import`.
  It may be helpful to open the [docs][pystd]. (IPython should have a link.)
- Go to the [`glob`][] module docs and explain how we'll use the `glob` function
  to get a list of the files.
- Show the students `from glob import glob` and tell them about how it
  works like `ls`.

- Exercise 4

- The students will probably need a reminder that they can loop over a list
  with `for thing in list`.
- Show how to use the `BlockGrid.flash()` method to put a grid on screen for
  a split second.

- Exercise 5

#### More Free Play

If there's more time to kill the students might have ideas for animations to
try out.

[`ipythonblocks`]: https://github.com/jiffyclub/ipythonblocks
[`ipythonblocks.py`]: ./ipythonblocks.py
[RGB]: http://en.wikipedia.org/wiki/RGB_color_model
[colorpicker]: http://www.colorpicker.com
[HTML colors]: http://en.wikipedia.org/wiki/Html_colors#X11_color_names
[playing exercises]: ./playing_with_blocks_exercises.md
[building exercises]: ./building_blocks_exercises.md
[fun blocks]: http://nbviewer.ipython.org/urls/raw.github.com/jiffyclub/ipythonblocks/master/demos/ipythonblocks_fun.ipynb
[GitHub]: http://github.com
[gists]: http://gist.github.com
[`grid1.txt`]: ./grid1.txt
[`glob`]: http://docs.python.org/2/library/glob.html
[pystd]: http://docs.python.org/2/library/index.html

# Exercises Accompanying the "Playing with Blocks" Notebook

## Exercise 1

Make a 5x5 `BlockGrid` with your favorite color (from the "Colors" notebook)
and assign it to a variable called `grid`.

## Exercise 2

Change the color of the block in the third row and fourth column.

## Exercise 3

Change the color of the block in the lower-right corner of the grid.

## Exercise 4

Use negative indexing to change the color of the block in the second row
and first column.

## Exercise 5

Use a `for` loop to change the color of every block in your grid.

## Exercise 6

Use a `for` loop and an `if` statement to change the color of every block
in the fourth *column* of your grid.

## Exercise 7

Augment your code from Exercise 6 with an `elif` and an `else` to change the
color of all blocks in your grid. But make the second column red, the
third column green, and every other block some other color.

## Exercise 8

Use an `if` with an `or` to change the color of the blocks in the first
and fifth rows to black.

## Exercise 9

Use an `if` with an `and` to turn every black block green.

## Exercise 10

Make a new 20 by 20 block grid. Use a `for` loop with `range` to change the
color of every block in the third, 10th, and 18th rows.

## Exercise 11

Use nested `for` loops with `range` to change the color of the bottom-left
quadrant of your 20 by 20 grid.

## Exercise 12

Try to get the same affects as in Exercises 10 & 12, but using slicing instead
of `for` loops.

# Learning Python with `ipythonblocks`

These lessons use [`ipythonblocks`][] to cover several aspects of programming
with Python including:

- Calling functions
- Variables
- Controlling flow with `for` and `if`
- Indexing
- Debugging
- Reading files
- Writing functions
- Importing from the standard library

Notes for instructors are in [`instructor_notes.md`][instructor notes].

Exercises for students are in:

- [`playing_with_blocks_exercises.md`][playing exercises]
- [`building_blocks_exercises.md`][building exercises]

[`ipythonblocks`]: https://github.com/jiffyclub/ipythonblocks
[instructor notes]: ./instructor_notes.md
[playing exercises]: ./playing_with_blocks_exercises.md
[building exercises]: ./building_blocks_exercises.md

Python 7 : Graphing with MatPlotLib
===================================

**Based On Lecture Material By Anthony Scopatz and Katy Huff**

Plotting in python
==================

Do this while I talk. If this is your first use of matplotlib (ever on
this machine), it will take a little longer starting up.

```python
from matplotlib import pyplot 
```

The ability to visually represent data is one of the more important
tools to the scientific user of python. The most popular and arguably
most mature library for producing publication quality 2D plots
matplotlib (MatPlotLib). In the developers own words, MatPlotLib is

> a python 2D plotting library which produces publication quality
> figures in a variety of hardcopy formats and interactive environments
> across platforms. matplotlib can be used in python scripts, the python
> and ipython shell (ala MatLab or mathematica), web application
> servers, and six graphical user interface toolkits."

pyplot, pylab and matplotlib. numpy, MatLab and more...
-------------------------------------------------------

The underlying structure of MatPlotLib is very general and customizable.
Thankfully, you don't actually need to interact with all that power. The
way to generate plots and modify them is through the pyplot interface.
The pyplot interface is much much inspired by Matlab's plotting
commands, so it should if you're familiar with that it should be easy to
pick up.

-   matplotlib - raw access to the plotting library. useful for
    extending matplotlib or doing very custom things
-   pyplot - MatLab-like interface to matplotlib (use this one!)
-   pylab - pyplot + numpy

**Aside: Multiple Interfaces to MatPlotLib**

In addition to the object oriented pyplot interface, pyplot implements a
clone of Matlab's plotting interface. The main difference is that the
Matlab interface has the paradigm of the "current figure." So plotting
and labeling commands implicitly refer to the current figure/subplot.
This is fine so long as your plotting commands are simple and
sequential. However, when your program requires that you modify a figure
in multiple contexts the Matlab interface becomes very difficult to keep
up with.

In this session, we will use the object oriented interface exclusively.
It is trivial to degrade to the Matlab interface if you're feeling lazy,
but the object oriented interface is powerful. It is also more explicit.

```python
import matplotlib.pyplot as plt 
```

First things first. Let's plot something. Open up a file, copy, paste,
execute.

```python
  from matplotlib import pyplot as plt
  x = range(0,10)
  y = [val**2 for val in x]
  ax = plt.subplot(111)
  ax.plot(x,y)
  plt.show()
```

figures, subplots, and axes
---------------------------

MatPlotLib plots are organized into figures, subplots and axes.

![](/python/scipy/images/fig1.jpg)

-   figure = the whole window (Figure 1 above)
-   subplot = the regular grid of four plots (One, Two, Three, and Four
    above) within the window
-   axes = a handle to the place in the subplot to put your curves
    (there are four axes defining the locations of the 6 curves above.)

### Hands On Exercise : Making a Plot

Lets make a plot. Retyping everything is not fun. Open up your favorite
text editor to create a file "plot.py" to hold your plotting script.

0.  from matplotlib import pyplot as plt

1.  Make a figure: **fig = plt.figure()**

2.  Add a subplot to the figure: **ax = fig.add\_subplot(111)**

3.  Plot on the subplot axes: **ax.plot(range(10))**

4.  Show the figure: **plt.show()**

    Your file should look like :

    ```python
      from matplotlib import pyplot as plt
      fig = plt.figure()
      ax = fig.add_subplot(111)
      ax.plot([1,2,3,4,5,6])
    ```

5.  Run your python file from the terminal **python plot.py &**

Customizing your Figure
-----------------------

Other, sometimes more convenient ways to get axes.

```python

  # Useful for figures composed of multiple subplots
  fig = plt.figure()
  ax1 = fig.add_subplot(221)
  ax2 = fig.add_subplot(222)
  ax3 = fig.add_subplot(223)
  ax4 = fig.add_subplot(224)
  
  # Useful for figures with one subplot (ie the subplot is the figure)
  ax2 = subplot(111)
  
  # Useful for overlaying axes, sharing the x axis
  ax3 = plt.twinx(ax2)
```


### Customizing your Plot

Plot is pretty powerful, lets peruse the [MatPlotLib
documentation](http://matplotlib.sourceforge.net).

1.  By editing plot.py and referencing the documentation try to add a
    label to an axis or change the color.
2.  If you're feeling adventurous and you've successfully added a label,
    try ax.legend()

Hints:

-   legend
-   colors
-   linestyles
-   kwargs !

Other Types of Line Plots
=========================

MatPlotLib as all the usual types of plots

log-linear, linear-log, log-log
-------------------------------

```python

  ax.semilogx(x,y)
  ax.semilogy(x,y)
  ax.loglog(x,y)
```

Execute ax.semilogx(range(1,11),range(10)) within your script instead of
ax.plot(x) and run it again to see the effect.

Contour plots and pcolor
------------------------

Notice that for the z argument to pcolor is shorter by one in both
directions. You should think of z as being defined zone centered. If you
forget to do so, it will drop the last row and column.

```python
  
  from matplotlib import pyplot as plt
  import numpy as np
  
  r = np.linspace(0.1,2,200)
  t = np.linspace(np.pi/4,7*np.pi/4,200)
  r,t = np.meshgrid(r,t)
  x = r*np.cos(t)
  y = r*np.sin(t)
  z = (x+0.6)**2 + (y-1.)**2
  
  ax = plt.subplot(111)
  
  ax.pcolor(x,y,z[1:,1:])
  # ax.colorbar()
  
  con = ax.contour(x,y,z)
  ax.clabel(con)

  plt.show()
```

Random useful things
====================

Spy
---

Matlab converts may have used this function to look at the structure of sparse
[matrixes](http://www.merriam-webster.com/medical/matrixes).

```python

  n = 20
  f = diag(ones(n)) + diag(ones(n-1),1) + diag(ones(n-3),-3)
  spy(f)
```

Masked Arrays
-------------

Masked arrays are numpy ndarrays that have a mask to hide elements of
the ndarray. They are really handy to use when plotting and you just
want to make particular parts of the plot "go away." In theory you could
modify the underlying arrays to do the right thing, but that's a lot of
work. You shouldn't have to do it if you don't have to. Masked arrays are
the answer. For more detailed information, check out the [masked arrray
documentation](http://docs.scipy.org/doc/numpy/reference/maskedarray.html)
] and [constructing masked
arrays](http://docs.scipy.org/doc/numpy/reference/maskedarray.generic.html#constructing-masked-arrays).

Starting from the pcolor plot, add the following code after defining z and
before plotting things.

```python
  z = np.ma.masked_where( np.abs(y-x) < 0.2, z )
```

draw() vs show()
----------------

If you're changing plots after they render (ie show()) you have to tell
MatPlotLib to draw the plot. draw() forces a figure to redraw. If you're
doing all your plotting from scripts, you probably won't have to deal
with draw(). If you're programming figures such that they change over
the course of your program, you will need to use draw to refresh the
figure.

How do I make a plot like ... ?
===============================

MatPlotLib has an extensive
[gallery](http://matplotlib.sourceforge.net/gallery.html) of plots with
source code. Take a look through those; they probably have something
similar to what you want to do. If you click on one of these images, you'll
land on a page with the source that created it. Recently, I've noticed that
the server has been throttling my traffic. If you see one you like but get
a "Too many requests warning" go to the matplotlib github repository
(https://github.com/matplotlib/matplotlib/tree/master/examples/pylab_examples)
and look for the source code there.

My favorite, among these, is
[polar_bar.py](https://github.com/matplotlib/matplotlib/tree/master/examples/pylab_examples/polar_bar.py).

Choose your own adventure
=========================

-   keyboard/mouse bindings (interactive plots!)
-   [image
    plotting](http://matplotlib.sourceforge.net/users/image_tutorial.html)
-   Shared Axes Across Subplots

Event binding with MatPlotLib
=============================

MatPlotLib has a built in mechanism for handling keyboard/mouse events
in a platform independent way. This can be useful when you want to
toggle different data on and off in the same figure, or step through
time dependent data, or something else crazy that you dream up. All you
need a a function that draws the figure and some logic to identify
events that your care about. MatPlotLib handles the rest. Lets walk
through the following code.

-   A MovingPlot instance remembers which plot and axes to modify (no
    current figure nonsense!)
-   In the constructor, we draw() the initial frame
-   The attribute cid listens for "key press events" and calls
    self.update(event) when something happens
-   event is a 'key\_press\_event' object, with attributes like "key"
-   update() takes one argument: an event object (other than the
    unavoidable self)
-   Based on the event object passed, update() only does something on
    right and left arrow key presses
-   MatPlotLib has several other [supported events](http://matplotlib.sourceforge.net/api/backend_bases_api.html?highlight=canvas#matplotlib.backend_bases.FigureCanvasBase.mpl_connect)

```python

  from matplotlib import pyplot as plt
  import numpy as np
  
  class MovingPlot(object):
  """ MovingPlot plots a figure that flows across the screen.  Right and left 
  arrows move the figure one step"""
      def __init__(self, r0= [-0.8, 1.0], v0 = [0.1, 0.]):
          # set up grid
          r = np.linspace(0.2,2,20)
          t = np.linspace(np.pi/4, 7*np.pi/4, 20)
          r,t = np.meshgrid(r,t)
          self.x = r*np.cos(t)
          self.y = r*np.sin(t)
  
          self.x0 = r0[0]
          self.y0 = r0[1]
          self.vx = v0[0]
          self.vy = v0[1]
  
          # create figure and axes for reuse
          self.fig = plt.figure()
          self.ax = self.fig.add_subplot(111)
  
          # draw the initial frame
          self.frame = 0
          self.draw(self.frame)
  
          # call self.update everytime a 'key_press_event' happens
          # bind this behavior to an object so it persists
          self.cid = self.fig.canvas.mpl_connect('key_press_event', self.update)
          plt.show()
  
      def update(self, event):
          """Changes the frame number based on event and draws a new frame if the 
          frame changed"""
          if event.key ==  'right':
              print 'forward'
              self.frame += 1
          elif event.key == 'left':
              print 'backward'
              self.frame -= 1
          else:
              return
          self.draw(self.frame)
  
      def draw(self, t):
          """Draws the frame occuring at time=t"""
          x = self.x - self.vx*t
          y = self.y - self.vy*t
          z = (x-self.x0)**2 + (y-self.y0)**2
          self.ax.pcolor(self.x, self.y, z)
          self.fig.canvas.draw()
  
  if __name__ == "__main__":
      mplot = MovingPlot()
```

Image Manipulation
==================

MatPlotLib has the ability to plot images. Check out the following code.
It plots 4 subplots

-   an image of a bunny
-   an image of a bunny with stuff over plotted
-   an image of a generated array
-   a pcolor plot of the same data

```python
    
  from matplotlib import pyplot as plt
  import numpy as np
  import urllib
  
  # read png into img
  try:
      img = plt.imread('bunny.png')
  except RuntimeError:
      print "can't find bunny.png, trying to download"
      # the image probably doesn't exits
      url = 
      'http://github.com/thehackerwithin/PyTrieste.wiki/images/bunny.png'
      urllib.urlretrieve(url, filename='bunny.png')
      img = plt.imread('bunny.png')
  
  fig = plt.figure()
  
  # plot just the bunny
  ax = fig.add_subplot(221)
  ax.imshow(img)
  ax.set_title('just the bunny')
  
  # plot bunny with stuff on top
  ax = fig.add_subplot(223)
  ax.imshow(img)
  xmin,xmax = ax.get_xlim() # plot changes the plotting limits
  ymin,ymax = ax.get_ylim() # snag them here for future uses
  x = np.linspace(100, 700, 10)
  y = np.sin(x/100)*300 + 300
  ax.plot(x,y)
  ax.set_xlim(xmin,xmax) # reset the limits to what imshow would have them be
  ax.set_ylim(ymin,ymax)
  ax.set_title('bunny with overlaid plots')
  
  # plot an array of doubles
  ax = fig.add_subplot(222)
  x = np.linspace(0,200, 10)
  y = np.linspace(0,200, 10)
  x,y = np.meshgrid(x,y)
  z = np.sin(x/200. * 2*np.pi) * np.sin(y/200. * 2*np.pi) * x/200 * y/200
  image = ax.imshow(z)
  plt.colorbar(image)
  ax.set_title('imshow() of array')
  
  # plot pcolor of z
  ax = fig.add_subplot(224)
  image = ax.pcolor(z)
  plt.colorbar(image)
  ax.set_title('pcolor() of same array array')
  
  plt.show()
```

Shared Axes Across Subplots
===========================

Take a look at the following plotting script. It plots 4 subplots. If
the bottom left subplot is the anchor, 1 subplot shares the xaxis (top
left). 1 subplot shares the yaxis (bottom right). 1 subplot shares both
axes (top right).

```python
  
  from matplotlib import pyplot as plt
  import numpy as np
  
  t = np.arange(0.01, 5.0, 0.01)
  s1 = np.sin(2*np.pi*t)
  s2 = np.exp(-t)
  s3 = np.sin(4*np.pi*t)
  
  fig = plt.figure()
  ax1 = fig.add_subplot(223)
  ax1.plot(t,s1)
  ax1.axvline(2, color='k', lw=3)
  ax1.axhline(0, color='r', lw=3)
  
  ## share x only
  ax2= fig.add_subplot(221, sharex
  ax1)
  ax2.plot(t, s2)
  ax2.axvline(2, color='k', lw=3)
  ax2.axhline(0, color='r', lw= 3)
  # make x tick labels invisible
  plt.setp( ax2.get_xticklabels(), visible=False)
  
  ## share x and y
  ax3 = fig.add_subplot(222,  sharex= ax1, sharey=ax1)
  ax3.plot(t, s3)
  ax3.axvline(2, color='k', lw=3)
  ax3.axhline(0, color= 'r', lw=3)
  ax3.set_xlim(0.01,5.0)
  # make tick labels small
  plt.setp( ax3.get_xticklabels(), fontsize=6)
  plt.setp( ax3.get_yticklabels(), fontsize=6)
  
  ## share y only
  ax4 = fig.add=subplot(224, sharey = ax1)
  ax4.plot(t, s3)
  ax4.axvline(2, color= 'k', lw = 3)
  ax4.axhline(0, color= 'r', lw = 3)
  # make y tick labels invisible
  plt.setp( ax4.get_yticklabels(), visible=False)
  
  plt.show()
```

This exercise will use NumPy to create a recommendation engine for
academic papers.

The first step is to decide on recommendation criteria. It would seem
that we want to take into account how highly the paper was rated by
other people, but we also want to consider ratings that are from people
with similar backgrounds to the user of the system. This leads to
another criteria: the similarity between ratings across papers that both
users have already rated.

We will divide the code into three pieces.

First, we will take a list of previous ratings and store them in a NumPy
array.

Second, we will introduce two measures of the similarity between two
papers or between two peopleâ€™s ratings.

Third, we will use those measures to generate recommendations for a
user.

Create a NumPy array with the recommendations
=============================================

Of all the millions of papers out there, most people have only read a
few. Since almost everyone has no opinion on almost every paper, the
data is very sparse. A good way to store sparse data is in a dictionary
for each user, where each ratings is stored as a unique paper identifier
and a rating.

We want to turn this data into an array. For this section, write a class
that includes three public elements:

1.  A numpy array where element i,j is the rating of person i for paper
    j.

2.  A python list where element i is the name of person i.

3.  A python list where element j is the name of paper j.

Calculate Similarity
====================

We can think of each person's ratings as a real vector, so a measure of
the similarity of two researchers is just the distance between their
rating vectors. There are two ratings we will explore.

1. Add a function to your class to compute the 2-norm between two people's
   ratings. However, your function should only consider papers for which *both*
   people have provided a non-zero rating. If there are 4 papers, and Tommy has
   read papers 1,2, and 3 while Katy has read papers 1,3, and 4, then you should
   only compute the similarity using papers 1 and 3. If two people have no
   shared recommendations then return 0.

2. Add a function to compute the Pearson correlation between two
   vectors in the same way you computed the 2-norm above.

3. (Optional) Look up the Tanimoto distance function. Add a function to
   compute the Tanimoto distance.

Generate a Recommendation
=========================

There are a few ways to look at the recommendation data.

First, we could ask which researcher is most like you. Write a function
that takes a researcher id and identifies the 5 researchers whose
ratings are most like the researcher.

Second, we could ask which papers have the most similar ratings. Write a
function that takes a paper id and identifies the 5 paper whose ratings
are most like the paper. (Hint: could we reuse the code we've already
written and use the transpose function?)

Third, we could ask for recommended papers for a researcher. Write a
function that identifies the top 5 papers that a researcher should read.
Keep in mind that the function should only return papers that the
researcher has *not* already rated. In the comment, explain how this
function chooses which papers to return.

Write some tests
================

Use Nose to write some tests for your code. Specifically, think about
how you can test the input code and the distance measurements.

General Advice
==============

There are a few functions you will want to investigate in numpy.

1. numpy.cov

2. numpy.logical\_and

3. numpy.linalg.norm

Data:
=====

Add the following code to a file called inputdata.py. Then you can
import the data directly using:

    import inputdata
    data = inputdata.raw_scores

The file is below:

    raw_scores = {

     'Bhargan Basepair' : {
       'Jackson 1999' : 2.5,
       'Chen 2002' : 3.5,
       'Rollins and Khersau 2002' : 3.0,
       'El Awy 2005' : 3.5,
       'Chen 2008' : 2.5,
       'Falkirk et al 2006' : 3.0
     },

     'Fan Fullerene' : {
       'Jackson 1999' : 3.0,
       'Chen 2002' : 3.5,
       'Rollins and Khersau 2002' : 1.5,
       'El Awy 2005' : 5.0,
       'Falkirk et al 2006' : 3.0,
       'Chen 2008' : 3.5
     },

     'Helen Helmet' : {
       'Jackson 1999' : 2.5,
       'Chen 2002' : 3.0,
       'El Awy 2005' : 3.5,
       'Falkirk et al 2006' : 4.0
     },

     'Mehrdad Mapping' : {
       'Chen 2002' : 3.5,
       'Rollins and Khersau 2002' : 3.0,
       'Falkirk et al 2006' : 4.5,
       'El Awy 2005' : 4.0,
       'Chen 2008' : 2.5
     },

     'Miguel Monopole' : {
       'Jackson 1999' : 3.0,
       'Chen 2002' : 4.0,
       'Rollins and Khersau 2002' : 2.0,
       'El Awy 2005' : 3.0,
       'Falkirk et al 2006' : 3.0,
       'Chen 2008' : 2.0
     },

     'Gail Graphics' : {
       'Jackson 1999' : 3.0,
       'Chen 2002' : 4.0,
       'Falkirk et al 2006' : 3.0,
       'El Awy 2005' : 5.0,
       'Chen 2008' : 3.5
     },

     'Stephen Scanner' : {
       'Chen 2002' :4.5,
       'Chen 2008' :1.0,
       'El Awy 2005' :4.0
     }
    }

Python 9 : NumPy
================

**Based on Lecture Materials By Matthew Terry and Katy Huff**

This class will cover a broad overview of NumPy with brief illustrative
examples geared towards getting people familiar with the basic use
cases. Since NumPy has many advanced features that may be useful to
experienced programmers, these notes will occasionally link to more
advanced examples that readers can peruse on their own time.

**Aside: Code Examples**

In all the examples below, we assume that import numpy has already been
executed. If any other modules are needed, we will import them
explicitly.

NumPy basics
============

What is NumPy ?
---------------

NumPy is a Python package implementing efficient collections of specific
types of data (generally numerical), similar to the standard array
module (but with many more features). NumPy arrays differ from lists and
tuples in that the data is contiguous in memory. A Python **list**, 
```[0, 1, 2]```, in contrast, is actually an array of pointers to Python
objects representing each number. This allows NumPy arrays to be
considerably faster for numerical operations than Python lists/tuples.

Creating NumPy Arrays
---------------------

Creating a NumPy array is as simple as passing a sequence to
numpy.array. You can also explicitly specify the data-type if the
automatically-chosen one would be unsuitable.

```python    

  >>> A = numpy.array([1, 2.3, 4])   
  >>> A.dtype 
  dtype('float64')   
  >>> B= numpy.array([1, 2.3, 4], dtype int)   
  >>> B.dtype   
  dtype('int32') 
```

As you might expect, creating a NumPy array this way can be slow, since
it must manually convert each element of a list into its equivalent C
type (int objects become C ints, etc). There are many other ways to
create NumPy arrays, such as **numpy.identity**, **numpy.zeros**,
**numpy.zeros\_like**, or by manually specifying the dimensions and type
of the array with the low-level creation function:

```python    

  numpy.ndarray((2, 3, 4), dtype=complex) # new 2x3x4 array of complex numbers 
```

For many of the examples below, we will be using **numpy.arange** which,
similar to the Python built-in function **range**, returns a NumPy array
of integers from 0 to N-1, inclusive. Like **range**, you can also
specify a starting value and a step.


```python

  >>> numpy.arange(2, 5)
  array([2, 3, 4])
  >>> numpy.arange(1, 5, 2)
  array([1, 3])
  >>> numpy.arange(1, 10, 2)
  array([1, 3, 5, 7, 9])
```

### Exercise : Create an Array

Create a NumPy array with values ranging from 0 to 10, in increments of
0.5. Don't forget to use **help()** to find useful functions!

Data types
----------

When creating a NumPy array, you supply a dtype ("data type"), or one is
chosen for you. There are a total of 21 different array scalar types,
which can be used to specify the dtype. In addition to the scalar type,
you may also specify byte order (little- or big-endian) or even multiple
scalar types to be used as a light-weight tuple (similar to a C struct).
For everyday use, however, you can just pass in the appropriate scalar
type and NumPy will figure it out. Some common scalar types include:

    int     # Python-compatible int (usually a C long)
    intc    # C int
    float   # Python-compatible float (usually a C double)
    single  # C float
    double  # C double
    complex # Python-compatible complex

List operations
---------------

For basic operations, NumPy arrays can be accessed just like Python
lists and tuples. This means that you can use the square brackets to
access elements, **len()** to access the size of the array, and so on.

```python

  >>> A = numpy.arange(5)
  >>> A
  array([0, 1, 2, 3, 4])
  >>> A[3]
  3
  >>> A[3] = 42
  >>> A
  array([ 0,  1,  2, 42,  4])
  >>> len(A)
  5
```

Arithmetic
----------

Since NumPy exists to perform efficient numerical operations in Python,
it stands to reason that NumPy arrays have all the usual arithmetic
operations available to them. These operations are performed
element-wise (i.e. the same operation is performed independently on each
element of the array).


```python

  >>> A = numpy.arange(5)
  >>> B = numpy.arange(5, 10)
  >>> A
  array([0, 1, 2, 3, 4])
  >>> B
  array([5, 6, 7, 8, 9])
  >>> A+B
  array([ 5,  7,  9, 11, 13])
  >>> B-A
  array([5, 5, 5, 5, 5])
  >>> A*B
  array([ 0,  6, 14, 24, 36])
```

In addition, if one of the arguments is a scalar, that value will be
applied to all the elements of the array.

```python
  
  >>> A = numpy.arange(5)
  >>> 2*A
  array([0, 2, 4, 6, 8])
  >>> A**2
  array([ 0,  1,  4,  9, 16])
  >>> A+10
  array([10, 11, 12, 13, 14])
```

Comparison
----------

Much like the basic arithmetic operations we discussed above, comparison
operations are perfomed element-wise. That is, rather than returning a
single boolean, comparison operators compare each element in both arrays
pairwise, and return an **array** of booleans (if the sizes of the input
arrays are incompatible, the comparison will simply return False). For
example:

```python

  >>> A = numpy.array([1, 2, 3, 4, 5])
  >>> B = numpy.array([1, 1, 3, 3, 5])
  >>> A == B
  array([ True, False,  True, False,  True], dtype=bool)
```

From here, you can use the methods .any() and .all() to return a single
boolean indicating whether any or all values in the array are True,
respectively.

Advanced Indexing
-----------------

In addition to the usual methods of indexing lists with an integer (or
with a series of colon-separated integers for a slice), NumPy allows you
to index arrays in a wide variety of different ways for more advanced
operations.

Multi-dimensional Indexing
--------------------------

Unlike Python lists and tuples, NumPy arrays can be multidimensional.
This complicates somewhat how they are indexed. To access a single
element, you simply pass in a comma-separated list of indices as a
subscript. However, there are many other things you can do when indexing
multidimensional arrays.

For instance, suppose you want the elements of a 2D array where the
first dimension is 1. NumPy makes this extremely simple:

```python

  >>> A = numpy.arange(16).reshape(4, 4)
  >>> A
  array([[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11],
         [12, 13, 14, 15]])
  >>> A[1]
  array([4, 5, 6, 7])
```

Now suppose you want the elements of that array where the **second**
dimension is 1. To do this, you can use "slices" of an entire dimension
as a placeholder by typing : as your first "index". Continuing from
above:

```python

  >>> A[:, 1]
  array([ 1,  5,  9, 13])
```

As you'd expect, this type of indexing can become quite complicated with
arrays of high dimension.

### Exercise : Selective Array Display

Using what we've learned about slicing and indexing, print just the
upper-left quarter of the array A above.

Indexing with Arrays
--------------------

NumPy arrays can be indexed with other arrays, using either an array of
indices, or an array of booleans of the same length. In the former case,
NumPy returns a view of the data in the specified indices as a new
array. In the latter, NumPy returns a view of the array with only the
elements where the index array is True. (We'll discuss the difference
between views and copies in a moment.) This makes normally-tedious
operations like clamping extremely simple.

Indexing with an array of indices:

```python

  >>> A = numpy.arange(5, 10)
  >>> A
  array([5, 6, 7, 8, 9])
  >>> A[[0, 2, 3]]
  array([5, 7, 8])
  >>> A[[0, 2, 3]] = 0
  >>> A
  array([0, 6, 0, 0, 9])
```

Indexing with an array of booleans:

```python
  
  >>> import random
  >>> A = numpy.array([random.randint(0, 10) for i in range(10)])
  >>> A
  array([10,  5,  1,  2,  3,  9,  3,  4,  9,  8])
  >>> A[A>5] = 5
  >>> A
  array([5, 5, 1, 2, 3, 5, 3, 4, 5, 5])
```

NumPy Gotchas
=============

NumPy has some important interface differences from Python lists and
tuples that can confuse new users of the library. Below are the most
notable of these.

Multiplication and Addition
---------------------------

As you may have noticed above, since NumPy arrays are modeled more
closely after vectors and matrices, multiplying by a scalar will
multiply each element of the array, whereas multiplying a list by a
scalar will repeat that list N times.

```python
  
  >>> numpy.arange(5)*2
  array([0, 2, 4, 6, 8])
  >>> range(5)*2
  [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]
```

Similarly, when adding two NumPy arrays together, we get the vector sum
back, whereas when adding two lists together, we get the concatenation
back.

```python
  
  >>> numpy.arange(5) + numpy.arange(5)
  array([0, 2, 4, 6, 8])
  >>> range(5) + range(5)
  [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]
```

Views vs. Copies
----------------

In order to be as efficient as possible, NumPy uses "views" instead of
copies wherever possible. That is, NumPy arrays derived from another
base array generally refer to the ''exact same data'' as the base array.
The consequence of this is that modification of these derived arrays
will also modify the base array. You saw this above in how the result of
an array indexed by an array of indices is a ''copy'', but an array
indexed by an array of booleans is a ''view''. (Phew!)

Specifically, slices of arrays are always views, unlike slices of lists
or tuples, which are always copies.

```python

  >>> A = numpy.arange(5)
  >>> B = A[0:1]
  >>> B[0] = 42
  >>> A
  array([42,  1,  2,  3,  4])
  >>> >>> A = range(5)
  >>> B = A[0:1]
  >>> B[0] = 42
  >>> A
  [0, 1, 2, 3, 4]
```

### Exercise : Copy a NumPy Array

Figure out how to create a copy of a NumPy array. Remember: since NumPy
slices are views, you can't use the trick you'd use for Python lists,
i.e. copy = list[:].

Mathematics with NumPy
======================

Being designed for scientific computing, NumPy also contains a host of
common mathematical functions, including linear algebra functions, fast
Fourier transforms, and probability/statistics functions. While there
isn't space to go over ''all'' of these in detail, we will provide an
overview of the most common/essential of these.

Basics
------

All NumPy arrays have a collection of basic operations built-in. Most of
these can be used to operate only on a particular axis of the array, but
for simplicity, we will only show them in action on one-dimensional
arrays.

```python
  
  >>> import random
  >>> A = numpy.array([random.randint(0, 10) for i in range(10)])
  >>> A
  array([6, 9, 9, 4, 9, 8, 7, 9, 0, 3])
  >>> A.min()
  0
  >>> A.max()
  9
  >>> A.mean()
  6.4000000000000004
  >>> A.std() # standard deviation
  2.9732137494637012
  >>> A.sum()
  64
```

For 2-dimensional (or more) arrays, there are some other common
operations:

```python
  
  >>> A = numpy.arange(16).reshape(4, 4)
  >>> A
  array([[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11],
         [12, 13, 14, 15]])
  >>> A.T # transpose
  array([[ 0,  4,  8, 12],
         [ 1,  5,  9, 13],
       [ 2,  6, 10, 14],
       [ 3,  7, 11, 15]])
  >>> A.trace()
  30
```

There are many more methods like these available with NumPy arrays. Be
sure to consult the NumPy documentation before writing your own
versions!

Matrices
--------

So far, we've used two-dimensional arrays to represent matrix-like
objects. However, NumPy provides a specialized class for this. The
matrix class is almost identical to a two-dimensional NumPy array, but
has a few changes to the interface to simplify common linear algebraic
tasks. These are: \* The `*` operator is performs matrix multiplication
\* The `**` operator performs matrix exponentiation \* The property `.I`
(or the method `.getI()`) returns the matrix inverse \* The property
`.H` (or the method `.getH()`) returns the conjugate transpose

### Example: Solving a System of Linear Equations

```python

  >>> import numpy.linalg
  >>> A = numpy.matrix([[3, 2, -1], [2, -2, 4], [-1, .5, -1]])
  >>> B = numpy.array([1, -2, 0])
  >>> numpy.linalg.solve(A, B)
  array([ 1., -2., -2.])
```

Universal Functions
===================

Universal functions (also called ufuncs) are high-speed, element-wise
operations on NumPy arrays. They are, in essence, what allows you to
operate on NumPy arrays efficiently. There are a large number of
universal functions available covering most of the basic operations that
get performed on data, like addition, subtraction, logarithms, and so
on. Calling a ufunc is a simple matter:

```python

  >>> A = numpy.arange(1,10)
  >>> numpy.log10(A)
  array([ 0.        ,  0.30103   ,  0.47712125,  0.60205999,  0.69897   ,
          0.77815125,  0.84509804,  0.90308999,  0.95424251])
```

In addition to basic operation like above, ufuncs that take two input
arrays and return an output array can be used in more advanced ways.

ufuncs
------

### Exercise : Elementwise Operations

Using ufuncs, calculate the reciprocals of each element in the following
array:

```python
  
  [8.1, 1.6, 0.9, 4.3, 7.0, 7.3, 4.7, 8.2, 7.2, 3.0,
  1.4, 9.8, 5.7, 0.7, 8.7, 4.6, 8.8, 0.9, 4.4, 4.4]
```

External Resources
==================

NumPy has too many features to discuss here. However, there are plenty
of resources on the web that describe NumPy in detail. Here is a
selection of them:

 * [NumPy user's guide](http://docs.scipy.org/doc/numpy/user)
 * [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/)
 * [NumPy For Matlab Users](http://www.scipy.org/NumPy_for_Matlab_Users)
 * [NumPy CookBook](http://www.scipy.org/Cookbook) 

Python 6: Classes and Objects
=============================

**Materials orignially by Tommy Guy**

Basic Classes
-------------

The file manipulation example in the last lecture hid a pretty amazing
idea. When we opened a file using open(), it returned a new type we
hadn't seen before. The type had methods that we could use to access the
file all at once or one character at a time. It had other methods to
move around in the file, to close it, and to update it. It also had
data: what file is open and where in the file is the next readable byte.
The magic is that you, the programmer, don't have to think about the
details of the file implementation. You just have to use the methods
available to access the file. This thought process is the basis of
objects and object oriented programming.

Object oriented (OO) programming revolves around the create and
manipulation of objects that have attributes and can do things. They can
be as simple as a coordinate with x and y values or as complicated as a
dynamic webpage framework. They are a useful way to organize programs.
C++ and Java are OO languages. Even fortran is adding OO constructs in
newer standards. Here is the code for making a very simple class that
sets an attribute. Start a new file, call it myclass.py and type this
in.

```python
    class MyClass(object):
      def setA(self, A):
        self.A = A
```

Now, in the Python shell, lets import and use MyClass:

```python
    > import myclass
    > anObject = myclass.MyClass()  # The MyClass object is in the myclass module.
    > type(anObject)
    <class 'myclass.MyClass'>  # See, it's a new type!
    > anObject.A = 34          # Set the class variable A directly.
    > print anObject.A
    > anObject.setA('hello')   # Set the class variable A with the setter method.
    > print anObject.A
```

It will help to have a bit of object-oriented vocabulary to understand what just happened:
 - Class - user defined type (MyClass)
 - object - instance of a Class (tmp = MyClass(), tmp is an object)
 - method - a Class function, also called a member function (tmp.getA())
 - attribute - a Class variable (tmp.A)

Remember: you *write* a class and you *create* and object.

**Hands on Example**

Write an Atom class with functions set_number(number_string) and
area_code().

```python
    """Matrix defines a real, 2-d matrix."""

    class Matrix(object):
      """I am a matrix of real numbers"""

      def __init__(self,h,w):
          self._nrows = h
          self._ncols = w
          self._data = [0] * (self._nrows * self._ncols)

      def __str__(self):
          return "Matrix: " + str(self._nrows) + " by " + str(self._ncols)

      def setnrows(self, w):
           self._nrows = w
           self.reinit()

      def getnrows(self):
           return self._nrows

      def getncols(self):
           return self._ncols

      def reinit(self):
           self._data = [0] * (self._nrows * self._ncols)

      def setncols(self, h):
           self._ncols = h
           self.reinit()

      def setValue(self,i,j, value):
         if i < self._nrows and j < self._ncols:
             self._data[i * self._nrows + j] = value
         else:
             raise Exception("Out of range")

      def multiply(self, otherMatrix):
         ''' Perform matrix multiplication and return a new matrix.
         The new matrix is on the left. '''
         result = Matrix(self._nrows, otherMatrix.getncols())
         # Do multiplication...
         return result

      def inv(self):
         ''' Invert matrix '''
         if self._ncols != self._nrows: raise Exception("Only square matrices are invertible")
         invertedMatrix = Matrix(self._ncols, self._nrows)
         invertedMatrix.setncols(self._ncols)
         invertedMatrix.setnrows(self._ncols)
         # INVERT!
         return invertedMatrix
```

Note the "self" argument in all of the class methods. This is a pointer
to the current object. You have to use self to reference methods and
data in an object.

The Big Idea: Interface vs. Implementation
------------------------------------------

Users shouldn't have to know *how* your program works in order to use
it.

The interface is a *contract* saying what a class knows how to do. The
code above defines matrix multiplication, which means that
mat1.multiply(mat2) should always return the right answer. It turns out
there are many ways to multiply matrices, and there are whole Ph.Ds
written on performing efficient matrix inversion. The implementation is
the way in which the *contract* is carried out.

Constructors
------------

Usually you want to create an object with a set of initial values for
things. Perhaps an object needs certain information to be created. For
this you write a "constructor." In python, constructors are just methods
with a special name:

```python
    class MyClass(object):
      def __init__(self):
          ''' Initialize things '''
```

**Aside: Magic functions**

Methods with leading and trailing double underscores are "magic
functions" in python.

 - Iteration (for x in sequence) uses \_\_next\_\_
 - Slicing ie brackets) (a[1:2]) uses \_\_get\_\_
 - Calling ie parentheses (a(3)) uses \_\_call\_\_
 - Help uses \_\_doc\_\_

*Write an initializer for the Matrix class that sets the height and
width. Change the multiply and inv methods to use this compiler*

**Aside: Old vs New Style Classes**

It is worth noting that there are two types of classes in python: Old
style classes (OSC) and new style classes (NSC). NSC fix some conceptual
problems with OSC (typing, diamond inheritance, subclassing built in
types, etc). Consequently OSC are gone in python 3. This is not a cause
for concern or confusion as the difference are subtle and will not
affect you until you have written enough python to be comfortable with
the distinction. Or you can always subclass object and render the issue
moot. Below illustrates the fix in the typing system.

```python
    class OldStyleClass: # don't use this one
        def __init__(self):
            print "Old Style"

    class NewStyleClass(object): # <- use this one
        def __init__(self):
            print "New Style"

    ns = NewStyleClass()
    os = OldStyleClass()

    print type(os)
    print type(ns)
```

Class methods with variable numbers of arguments
------------------------------------------------

In the previous session you learned about the power of python functions.
The full power of functions (keyword arguments, variable length
arguments, etc) are available in classes. For converts from other
languages, be aware that python functions do not have a type signature
so function overloading is not available.

Subclassing
-----------

If you want a to create a Class that behaves mostly like another class,
you should not have to copy code. What you do is subclass and change the
things that need changing. When we created classes we were already
subclassing the built in python class "object."

For example, let's say you want to write a sparse matrix class, which
means that you don't explicitly store zero elements. You can create a
subclass of the Matrix class that redefines the matrix operations.

```python
    class SparseMatrix(Matrix):
    """I am a matrix of real numbers"""

    def __str__(self):
      return "SparseMatrix: " + str(self._nrows) + " by " + str(self._ncols)

    def reinit(self):
      self._data = {}

    def setValue(self,i,j, value):
       self._data[(i,j)] = value


    def multiply(self, otherMatrix):
       ''' Perform matrix multiplication and return a new matrix.
       The new matrix is on the left. '''
       result = SparseMatrix(self._nrows, otherMatrix.getncols())
       # Do multiplication...
       return result

    def inv(self):
       ''' Invert matrix '''
       if self._nrows != self._rcols: raise Exception("Only square matrices are invertible")
       invertedMatrix = SparseMatrix(self._ncols, self._nrows)
```

The SparseMatrix object is a Matrix but some methods are defined in the
*super class* Matrix.

Python guidelines for code formatting and pythonic conventions on class
behavior, naming, etc.  [python
conventions](http://www.python.org/dev/peps/pep-0008)

These notebooks involve using Python for simple math exercises.

* A Math Trick
  * A series of simple, integer arithmetic steps which result in a silly
    math trick. Intended as a first step into Python and the notebook so
    students become familiar with pressing `shift-Enter` and entering code
    into cells.

Python 8 : Using Scipy
======================

**Based on Lecture Materials By Anthony Scopatz**

Scientific Python (SciPy) is a very robust package. It encompasses
several modules ranging from the eternally practical to the quirky and
cool. No matter what your discipline is, you can probably find a
pre-made short cut using SciPy. With its considerable scope, there is no
way we can cover all of its functionality in a single tutorial. That is
what documentation is for http://docs.scipy.org/doc/! However, we can talk about
the basics, show off some of its neater features, and generally teach the
'SciPy-way of doing things.'

The presentation on SciPy will follow five sections:

1.  SciPy Constants,
2.  Special Functions,
3.  Integration & ODEs,
4.  Pade Approximates,
5.  and Image Tricks.

In general, you can grab SciPy functionality via

```python
  import scipy 
```

What you get is (from
[http://docs.scipy.org/doc/scipy/reference/](http://docs.scipy.org/doc/scipy/reference/)):

```python
  -- Clustering package (**scipy.cluster**)
  -- Constants (**scipy.constants**)
  -- Fourier transforms (**scipy.fftpack**)
  -- Integration and ODEs (**scipy.integrate**)
  -- Interpolation (**scipy.interpolate**)
  -- Input and output (**scipy.io**)
  -- Linear algebra (**scipy.linalg**)
  -- Maximum entropy models (**scipy.maxentropy**)
  -- Miscellaneous routines (**scipy.misc**)
  -- Multi-dimensional image processing (**scipy.ndimage**)
  -- Orthogonal distance regression (**scipy.odr**)
  -- Optimization and root finding (**scipy.optimize**)
  -- Signal processing (**scipy.signal**)
  -- Sparse matrices (**scipy.sparse**)
  -- Sparse linear algebra (**scipy.sparse.linalg**)
  -- Spatial algorithms and data structures (**scipy.spatial**)
  -- Special functions (**scipy.special**)
  -- Statistical functions (**scipy.stats**)
  -- Image Array Manipulation and Convolution (**scipy.stsci**)
  -- C/C++ integration (**scipy.weave**)
```

SciPy Constants, crawl before you walk!
=======================================

A plethora of important, fundamental constants can be found in
**scipy.constants**. NOTE: However, this module is not automatically
included when you **import scipy**. Still, some very basic pieces of
information are given as module attributes.

The following, for example:

```python
  import scipy.constants 
  import math
  
  print("SciPy thinks that pi = %.16f"%scipy.constants.pi) 
  print("While math thinks that pi = %.16f"%math.pi) 
  print("SciPy also thinks that the speed of light is c = %.1F"%scipy.constants.c) 
```

will return

```python
  >>> SciPy thinks that pi = 3.1415926535897931 
  >>> While math thinks that pi = 3.1415926535897931 
  >>> SciPy also thinks that the speed of light is c = 299792458.0 
```

However, the real value of SciPy Constants is its enormous physical
constant database. These are of the form:
**scipy.constants.physical\_constants[name] = (value, units,
uncertainty)**.

For example, the mass of an alpha particle is:

```python

  >>> scipy.constants.physical_constants["alpha particle mass"] 
  >>> (6.6446564999999997e-27, 'kg', 1.1e-33) 
```

How can you tell what the key is for this function? My favorite way is
with the scipy.constants.find() method.

```python
  scipy.constants.find("light")
```

gives :

```python
  ['speed of light in vacuum']
```

But buyer beware! Let's look at the speed of light again.

```python
  >>> print("c = %s"%str(scipy.constants.physical_constants["speed of light in vacuum"]))
  >>> c = (299792458.0, 'm s^-1', 0.0)
```

The uncertainty in c should not be zero! Right? Wrong. The
[meter](http://en.wikipedia.org/wiki/Metre) is in fact set by the speed of light
in a vacuum. So there is, by definition, no error. However, this has not always
been the case.  Moreover, any actual determination of the **c** or the meter has
a measurement uncertainty, but SciPy does not acknowledge it.

**BASIC LESSON:** As always, pay attention.

Check http://docs.scipy.org/doc/scipy/reference/constants.html for a complete
constants listing.

The above code is reproduced concisely in the constants.py file found in
the SciPy directory of your PyTrieste repository.

SciPy Special Functions, walk before you run!
=============================================

Code that numerically approximates common (and some not-so-common)
special functions can be found in **scipy.special**. Here you can find
things like error functions, gamma functions, Legendre polynomials, etc.
But as a example let's focus on my favorites: the Bessel functions.

```python
  from scipy.special import * 
  from pylab import *
  
  x = arange(0.0, 10.1, 0.1)
  
  for n in range(4): 
    j = jn(n, x) 
    plot(x, j, 'k-') 
    text(x[10*(n+1)+1], j[10*(n+1)], r'$J_%r$'%n)
  
  for n in range(3): 
    y = yn(n, x) plot(x, y, 'k--') 
    text(x[10*(n)+6], y[10*(n)+5], r'$Y_%r$'%n)
  
  axis([0, 10, -2, 1.25]) 
  xlabel(r'$x$') 
  ylabel("Bessel Functions")
  
  show() 
```

These 20-ish lines of code should produce :

![](/images/BesselFigure.png)

Note that the figure that was created here is a reproduction of Figure
6.5.1 in ''Numerical Recipes'' by W. H. Press, et al...
(http://www.nr.com/).

Check out http://docs.scipy.org/doc/scipy/reference/special.html for a complete
listing of special functions.

The above code is reproduced concisely in special\_functions.py which
can be found in the SciPy directory of your PyTrieste repository.

SciPy Integration, run before you glide!
========================================

Tools used to calculate numerical, definite integrals may be found in
the **integrate** module. There are two basic ways you can integrate in
SciPy:

1.  Integrate a function,
2.  Integrate piecewise data.

First, let's deal with integration of functions. Recall that in Python,
functions are also objects. Therefore you can pass functions as
arguments to other functions! Just make sure that the function that you
want to integrate returns a float, or at the very least, an object that
has a **__float__()** method.

Integration Example: 1D
-----------------------

The simplest way to compute a functions definite integral is via the
**quad(...)** function. The script

```python
  import scipy.integrate #For kicks, let's also grab
  import scipy.special
  import numpy
  def CrazyFunc(x):
    return (scipy.special.i1(x) - 1)**3

  print("Try integrating CrazyFunc on the range [-5, 10]...")
  
  val, err = scipy.integrate.quad(CrazyFunc, -5, 10)

  print("A Crazy Function integrates to %.8E"%val)
  print("And with insanely low error of %.8E"%err)
```

will return


```python
  >>> Try integrating CrazyFunc on the range [-5, 10]...  
  >>> A Crazy Function integrates to 6.65625226E+09 
  >>> And with insanely low error of 3.21172897E-03
```

Integration Example: Infinite Limits
------------------------------------

You can also use **scipy.integrate.Inf** for infinity in the limits of
integration. For example, try integrating e\^x on `[-inf, 0]`:

```python
  >>> print("(val, err) = " + 
      str( scipy.integrate.quad(scipy.exp, -scipy.integrate.Inf, 0.0) ))
```

will return :

```python
  >>> (val, err) = (1.0000000000000002, 5.8426067429060041e-11)
```

Integration Example: 2D
-----------------------

Two dimensional integrations follow similarly to the 1D case. However,
now we need to use the **dblquad( f(y,x), ...)** function instead of
simply **quad( f(x), ...)**. Because a picture is worth 10\^3 words,
SciPy computes the right-hand side of the following equation:

![](/images/dblquad.png)

More information on the justification of this integral may be found at
http://en.wikipedia.org/wiki/Order\_of\_integration\_(calculus). For
example, let's try to integrate the surface area of a unit sphere:

```python
  def dA_Sphere(phi, theta):
    return  scipy.sin(phi)

  print("Integrate the surface area of the unit sphere...")
  val, err = scipy.integrate.dblquad(dA_Sphere, 0.0, 2.0*scipy.pi, 
    lambda theta: 0.0, 
    lambda theta: scipy.pi )
  print("val = %.8F"%val)
  print("err = %.8E"%err)
```

This will return :

```python
  >>> Integrate the surface area of the unit sphere...  
  >>> val = 12.56637061
  >>> err = 1.39514740E-13
```

There are a couple of subtleties here. First is that the function you
are integrating over is defined as **f(y,x)** and NOT the more standard
**f(x,y)**. Moreover, while **x**'s limits of integration are given
directly **[0, 2*pi]**,**y**'s limits have to be functions**[g(x),
h(x)]*\* (given by the 'lambdas' here). This method of doing double
integrals allows for**y*\* to have a more complicated edge in**x*\* than
a simple point. This is great for some functions but a little annoying
for simple integrations. In any event, the above integral computes the
surface are of a unit sphere to be **4*pi*\* to within floating point
error.

Integration Example: 3D
-----------------------

Three dimensional integration is more similar to 2D than 1D. Once again,
we define our function variables in reverse order, **f(z, y, x)**, and
integrate using **tplquad( f(z, y, x) )**. Moreover, **z** has limits of
integration defined by surfaces give as functions **[q(x,y), r(x,y)]**.
Thus, **tplquad(...)** integrates the right-hand side of :

![](/images/trplquad.png)

To continue with the previous example, let's try integrating the volume
of a sphere. Take the radius here to be 3.5.

```python
  def dV_Sphere(phi, theta, r):
    return r * r * dA_Sphere(phi, theta)

  print("Integrate the volume of a sphere with r=3.5...")
  val, err = scipy.integrate.tplquad(dV_Sphere, 0.0, 3.5, lambda r: 0.0,
    lambda r: 2.0*scipy.pi, lambda x, y: 0.0, lambda x, y: scipy.pi)
  print("val = %.8F"%val)
  print("err = %.8E"%err)
```

will return:

```python
  >>> Integrate the volume of a sphere with r=3.5...
  >>> val = 179.59438003
  >>> err = 1.99389816E-12
```

A simple hand calculation verifies this result.

Integration Example: Trapazoidal
--------------------------------

Now, only very rarely will scientists (and even more rarely engineers)
will truely 'know' the function that they wish to integrate. Much more
often we'll have piecewise data that we wish numerically integrate (ie
sum an array y(x), biased by array x). This can be done in SciPy through
the **trapz(...)** function.

```python
  y = range(0, 11)
  print("Trapazoidally integrate y = x on [0,10]...")
  val = scipy.integrate.trapz(y)
  print("val = %F"%val)
```

will return:

```python
  >>> Trapazoidally integrate y = x on [0,10]...
  >>> val = 50.000000
```

The above takes a series of y-values that are implicitly spaced 1-unit
apart in x and 'trapazoidally integrates' them. Basically, just a sum.

However, you can use the **x= [0, 3,...]** or **dx = 3** argument
keywords to explicitly declare different spacings in x. For example,
with y = x\^2:

```python
  x = numpy.arange(0.0, 20.5, 0.5)
  y = x * x
  print("Trapazoidally integrate y = x^2 on [0,20] with half steps...")
  val = scipy.integrate.trapz(y, x)
  print("val = %F"%val)
```

```python
  >>> Trapazoidally integrate y = x^2 on [0,20] with half steps...
  >>> val = 2667.500000
```

```python
  print("Trapazoidally integrate y = x^2 with dx = 0.5...")
  val = scipy.integrate.trapz(y, dx = 0.5)
  print("val = %F"%val)
```

```python
  >>> Trapazoidally integrate y = x^2 with dx = 0.5...
  >>> val = 2667.500000
```

Integration Example: Ordinary Differential Equations
----------------------------------------------------

Say that you have an ODE of the form **dy/dt = f(y, t)** that you
''really'' need integrated. Then you, my friend, are in luck! SciPy can
do this for you using the **scipy.integrate.odeint** function. This is
of the form:

```python
  odeint( f, y0, [t0, t1, ...])
```

For example take the decay equation:
   (y, t) = - lambda * y 

We can then try integrating this using a decay constant of 0.2`:

```python
  def dDecay(y, t, lam):
    return -lam*y

  vals = scipy.integrate.odeint( lambda y, t: dDecay(y, t, 0.2), 1.0, [0.0, 10.0] )
  print("If you start with a mass of y(0) = %F"%vals[0][0])
  print("you'll only have y(t= 10) = %F left."%vals[1][0])
```

This will return

```python
  >>> If you start with a mass of y(0) = 1.000000
  >>> you'll only have y(t= 10) = 0.135335 left.
```

Check out http://docs.scipy.org/doc/scipy/reference/integrate.html for more
information on integreation.

The above code is reproduced concisely in integrate.py, which can be
found in the SciPy direcotry of your PyTreiste repository.

SciPy Pade, glide before you fly!
=================================

As you have seen, SciPy has some really neat functionality that comes
stock. Oddly, some of the best stuff is in the ```miscellaneous``` module
accessed via **import scipy.misc**. What follows is a brief explanation of the
Pade Approximant (http://en.wikipedia.org/wiki/PadÃ©\_approximant) and how to
utilize it in SciPy.

Most people are familar with the polynomial expansions of a function:

    f(x) = a + bx + cx^2 + ...

Or a Taylor expansion:

    f(x) = sum( d^n f(a) / dx^n (x-a)^n /n! )

However, there exists the lesser known, more exact Pade approximation.
This basically splits up a function into a numerator and a denominator.

    f(x) = p(x) / q(x)

Then, you can approximate p(x) and q(x) using a power series. However to
generate p(x) and q(x) of order ```N```, you first need the
polynomial approximation of f(x) to order ```2N+1``` (for when p and
q are expanded to the same order). Thus the Pade approximate is a way to
squeeze greater orders of accuracy out of another, more expensively
gained approximation. A more complete treatment is available in Section
5.12 in ''Numerical Recipes'' by W. H. Press, et al...
(http://www.nr.com/). The strength of this method
is demonstrated though examples and figures.

First let's take the case for where f(x) = e\^x.

```python
  import scipy.misc
  from pylab import *
  
  #Let's expand e^x to fifth order and record the coefficents 
  e_exp = [1.0, 1.0, 1.0/2.0, 1.0/6.0, 1.0/24.0, 1.0/120.0]
  
  #The Pade coefficients are given simply by, 
  p, q = scipy.misc.pade(e_exp, 2)
  #p and q are of numpy's polynomial class
  #So the Pade approximation is given by 
  def PadeAppx(x):
      return p(x) / q(x)
  
  #Let's test it...
  x = arange(0.0, 3.1, 0.1)
  
  e_exp.reverse()
  e_poly = poly1d(e_exp)
  
  plot(x, PadeAppx(x), 'k--', label="Pade Approximation")
  plot(x, scipy.e**x, 'k-', label=r'$e^x$')
  plot(x, e_poly(x), 'r-', label="Power Series")
  
  xlabel(r'$x$')
  ylabel("Exponential Functions")
  
  legend(loc=0)

  show()
```

The above script, pade1.py, generates the following figure :

![](/images/PadeFigure1.png)

Now the gains from using the Pade approximant for an exponential here seem
slight. This is because the original exponential expansion is rather well
defined. Still the Pade version in this case has two important advantages:

 1. The power series version of e\^x would still require about twice as many
    terms to gain that slight increase in accuracy that the Pade gives.

 2. If instead x were replaced by the matrix A, then e\^A could be calculated
    with about half as many multiplications and sums using Pade rather than a
    polynomial. Success!

However, Pade's real power can be seen when approximating a different
function. Let's try approximating a rougher function...

```python

  import scipy.misc
  from pylab import *
  
  def f(x):
          return (7.0 + (1+x)**(4.0/3.0))**(1.0/3.0)
  
  #Through someone else's labors we know the expansion to be... 
  f_exp = [2.0, 1.0/9.0, 1.0/81.0, -49.0/8748.0, 175.0/78732.0]
  
  #The Pade coefficients are given simply by, 
  p, q = scipy.misc.pade(f_exp, (5-1)/2)
  #p and q are of numpy's polynomial class
  #So the Pade approximation is given by 
  def PadeAppx(x):
          return p(x) / q(x)
  
  #Let's test it...
  x = arange(0.0, 10.01, 0.01)
  
  f_exp.reverse()
  f_poly = poly1d(f_exp)
  
  plot(x, PadeAppx(x), 'k--', label="Pade Approximation")
  plot(x, f(x), 'k-', label=r'$f(x)$')
  plot(x, f_poly(x), 'r-', label="Power Series")
  
  xlabel(r'$x$')
  ylabel("Polynomial Function")

  legend(loc=0)

  show()
```

The above script, pade2.py, generates the following figure
![](/images/PadeFigure2.png)

Note, that this example is again taken from Section 5.12 in Numerical
Recipes by W. H. Press, et
al (http://www.nr.com/).

Check out http://docs.scipy.org/doc/scipy/reference/misc.html#scipy.misc.pade
for more details on SciPy's Pade interface.

SciPy Image Tricks, fly before you....You can do that?!
=======================================================

For some reason that has yet to be explained to me, SciPy has the
ability to treat 2D & 3D arrays as images. You can even convert PIL
images or read in external files as numpy arrays! From here, you can
fool around with the raw image data at will. Naturally, this
functionality is buried (again) within the ```miscellaneous``` module.

First let's read in an image file. For now make it a JPEG. From
Wikipedia. Of a bunny.  http://en.wikipedia.org/wiki/File:JumpingRabbit.JPG

![](/images/image.jpg)

```python
  import scipy.misc
  img = scipy.misc.imread("image.jpg")
  #Note that this really is an array!
  print(str(img))
```

```python

  >>> [[[130 174  27]
  >>>   [129 173  24]
  >>>   [127 171  22]
  >>>   ..., 
  >>>   [147 192  41]
  >>>   [146 190  41]
  >>>   [146 190  41]]
  >>> 
  >>>  [[137 177  29]
  
  >>>   [133 175  27]
  >>>   [130 173  21]
  >>>   ..., 
  >>>   [147 192  37]
  >>>   [149 194  41]
  >>>   [149 194  41]]
  >>> 
  >>>  [[141 177  29]
  >>>   [137 176  25]
  >>>   [130 174  19]
  >>>   ..., 
  >>>   [148 194  34]
  >>>   [149 195  35]
  >>>   [149 195  35]]
  >>> 
  >>>  ..., 
  >>>  [[ 77 126   1]
  >>>   [ 87 137  14]
  >>>   [ 85 139   0]
  >>>   ..., 
  >>>   [ 77 128   0]
  >>>   [ 99 159   3]
  >>>   [128 183  17]]
  
  >>>  [[ 86 139   0]
  >>>   [ 90 141  12]
  >>>   [ 82 136   0]
  >>>   ..., 
  >>>   [ 66 114   2]
  >>>   [102 160   0]
  >>>   [119 174  11]]
  >>> 
  >>>  [[ 89 140   0]
  >>>   [ 79 129   8]
  >>>   [ 87 139   2]
  >>>   ..., 
  >>>   [ 67 117   4]
  >>>   [ 81 138   0]
  >>>   [117 172  18]]]
```

We can now apply some basic filters...

```python
  img = scipy.misc.imfilter(img, 'blur')
```

We can even rotate the image, counter-clockwise by degrees.

```python
  img = scipy.misc.imrotate(img, 45)
```

And then, we can rewrite the array to an image file.

```python
  scipy.misc.imsave("image1.jpg", img)
```

These functions produce the following image:

![](/images/image1.jpg)

Because the array takes integer values from 0 - 255, we can easily
define our own filters as well! For instance, you could write a two-line
function to inverse the image...

```python
  def InverseImage(imgarr):
     return 255 - imgarr
  #Starting fresh we get...
  img = scipy.misc.imread("image.jpg")
  img = scipy.misc.imrotate(img, 330)
  img = InverseImage(img)
  scipy.misc.imsave("image2.jpg", img)
```

Having this much fun, the rabbit becomes a twisted shade of its former
self!

![](/images/image2.jpg)

Check out http://docs.scipy.org/doc/scipy/reference/misc.html for a complete
listing of associated image functions.

The code above can be found in the image\_tricks.py file in the SciPy
directory of your repository..

Python 5: Strings and File I/O
==============================

**Materials originaly by Tommy Guy**

Lesson goals:

1.  Examine the string class in greater detail.
2.  Use open() to open, read, and write to files.

Strings
-------

To start understanding the String class, let's use the built in help
system.

```python
  > help(str)
 Help on class str in module __builtin__:

 class str(basestring)
  |  str(object) -> string
  |  
  |  Return a nice string representation of the object.
  |  If the argument is a string, the return value is the same object.
  |  
  |  Method resolution order:
  |      str
  |      basestring
  |      object
  |  
  |  Methods defined here:
  |  
  |  __add__(...)
  |      x.__add__(y) <==> x+y
  |  
  |  __contains__(...)
  |      x.__contains__(y) <==> y in x
  |  
  |  __eq__(...)
  |      x.__eq__(y) <==> x==y
  |  
  |  __format__(...)
  |      S.__format__(format_spec) -> unicode
  |  
  |   __ge__(...)
  |      x.__ge__(y) <==> x>=y
  |  
  |  __getattribute__(...)
  |      x.__getattribute__('name') <==> x.name
  |  ]]
  |  __getitem__(...)
  |      x.__getitem__(y) <==> x[y]
  |  
  |  __getnewargs__(...)
  |  
  |  __getslice__(...)
  |      x.__getslice__(i, j) <==> x[i:j]
  |      
  |      Use of negative indices is not supported.
  |  
  ...
```

The help page for string is very long, and it may be easier to keep it open
in a browser window by going to the [online Python
documentation](http://docs.python.org/library/stdtypes.html#sequence-types-str-unicode-list-tuple-bytearray-buffer-xrange)
while we talk about its properties.

At its heart, a string is just a sequence of characters. Basic strings are
defined using single or double quotes.

```python
    > s = "This is a string."
    > s2 = 'This is another string that uses single quotes'
```

The reason for having two types of quotes to define a string is
emphasized in these examples:

```python
    > s = "Bob's mom called to say hello."
    > s = 'Bob's mom called to say hello.'
```

The second one should be an error: Python interprets it as `s = 'Bob'` then the
rest of the line breaks the language standard.

Characters in literal strings must come from the ASCII character set,
which is a set of 127 character codes that is used by all modern
programming languages and computers. Unfortunately, ASCII does not have
room for non-Roman characters like accents or Eastern scripts. Unicode
strings in Python are specified with a leading u:

```python
    > u = u'abcdÃ©'
```

For the rest of this lecture, we will deal with ASCII strings, because
most scientific data that is stored as text is stored with ASCII.

Working with Strings
--------------------

Strings are iterables, which means many of the ideas from lists can also
be applied directly to string manipulation. For instance, characters can
be accessed individually or in sequences:

```python
    > s = 'abcdefghijklmnopqrstuvwxyz'
    > s[0]
    'a'
    > s[-1]
    'z'
    > s[1:4]
    'bcd'
```

They can also be compared using sort and equals.

```python
    > 'str1' == 'str2'
    False
    > 'str1' == 'str1'
    True
    > 'str1' < 'str2'
    True
```

In the help screen, which we looked at above, there are lots of
functions that look like this:

```python
    |  __add__(...)
    |      x.__add__(y) <==> x+y

    |  __le__(...)
    |      x.__le__(y) <==> x<y
```

These are special Python functions that interpret operations like \< and \+.
We'll talk more about these in the next lecture on Classes.

Some special functions introduce handy text functions.

**Hands on example**

Try each of the following functions on a few strings. What does the
function do?

```python
    > s = "This is a string"
    > s.startswith("This")
    > s.split(" ")
    > s.strip() # This won't change every string!
    > s.capitalize()
    > s.capwords()
    > s.lower()
    > s.upper()
```

File I/O
--------

Python has a built-in function called "open()" that can be used to
manipulate files. The help information for open is below:

    > help(open)
     Help on built-in function open in module __builtin__:

     open(...)
       open(name[, mode[, buffering]]) -> file object

       Open a file using the file() type, returns a file object.  This is the
       preferred way to open a file.

The main two parameters we'll need to worry about are the name of the
file and the mode, which determines whether we can read from or write to
the file. open returns a file object, acts like a pointer into the file.
An example will make this clear. In the code below, I've opened a file
that contains one line:

    (unix shell) $ cat testFile.txt
    abcde
    fghij

Now let's open this file in Python:

```python
    > fileHandle = open('testFile.txt','r')
```

The second input, 'r' means I want to open the file for reading only. I
can not write to this handle. The read() command will read a specified
number of bytes:

```python
    > s = fileHandle.read(3)
    > print s
    abc
```

We read the first three characters, where each character is a byte long.
We can see that the file handle points to the 4th byte (index number 3)
in the file:

```python
    > fileHandle.tell()
    3L
    > fileHandle.read(1)
    'd'
```

The file we are using is a long series of characters, but two of the
characters are new line characters. If we looked at the file in
sequence, it would look like "abcdenfghijn". Separating a file into
lines is popular enough that there are two ways to read whole lines in a
file. The first is to use the readlines() method:

```python
    > fileHandle.close() # close the old handle
    > fileHandle = open('testFile.txt','r')
    > lineArr = fileHandle.readlines()
    > lineArr
    ['abcde\n', 'fghij\n']
```

A very important point about the readline method is that it *keeps* the
newline character at the end of each line. You can use the strip()
method to get rid of the string.

File handles are also iterable, which means we can use them in for loops
or list extensions:

```python
    > f = open('testFile.txt','r')
    > l = [s.strip() for s in f]
    > l
    ['abcde', 'fghij']
    > f.close()
    > l = []
    > f = open('testFile.txt','r')
    > for s in f:
         l.append(s.strip())
```

These are equivalent operations. It's often best to handle a file one
line at a time, particularly when the file is so large it might not fit
in memory.

The other half of the story is writing output to files. We'll talk about
two techniques: writing to the shell and writing to files directly.

If your program only creates one stream of output, it's often a good
idea to write to the shell using the print function. There are several
advantages to this strategy, including the fact that it allows the user
to select where they want to store the output without worrying about any
command line flags. You can use "\>" to direct the output of your
program to a file or use "|" to pipe it to another program.

Sometimes, you need to direct your output directly to a file handle. For
instance, if your program produces two output streams, you may want to
assign two open file handles. Opening a file for reading simply requires
changing the second option from 'r' to 'w' or 'a'.

*Caution!* Opening a file with the 'w' option means start writing *at
the beginning*, which may overwrite old material. If you want to append
to the file without losing what is already there, open it with 'a'.

Writing to a file uses the write() command, which accepts a string.

```python
    > outFile = open('outfile.txt','w')
    > outFile.write('This is the first line!')
```

Another way to write to a file is to use writelines(), which accepts a
list of strings and writes them in order. *Caution!* writelines does not
append newlines. If you really want to write a newline at the end of
each string in the list, add it yourself.

Aside: The first exercise
=========================

Yesterday, we asked you to edit a file in place. Many of you asked how
this was possible. The answer is that it is not. You can use f.seek()
and f.tell() to verify that even if your file handle is pointing to the
middle of a file, write commands go to the end of the file in append
mode. The best way to change a file is to open a temporary file in
/tmp/, fill it, and then move it to overwrite the original. On large
clusters, /tmp/ is often local to each node, which means it reduces I/O
bottlenecks associated with writing large amounts of data.

SWIG

**Based on Lecture Material by Kurt Smith**

[SWIG](http://www.swig.org) is a Simple Wrapper Interface Generator. SWIG parses
C/C++ header files and generates the appropriate wrapper code to make your C/C++
code directly callable from python (among other languages). This makes it a very
convenient way of exposing C/C++ code to python. For more detailed info, check
out SWIG's python
[documentation](http://www.swig.org/Doc1.3/Python.html#Python).

In C/C++, if you have the header files, you have everything you need to
link correctly. In theory, if you have the header files for some C/C++
code, you can generate the glue code to such that python can call that
code. All you need to do is parse the header, figure out the name
mangling, write the appropriate C code to make it python aware, and
write the appropriate python to load the C code correctly. To me this
sounds like a large undertaking. Thankfully, SWIG can do most of that
for you.

To use SWIG, you write a config file that tells SWIG which header files
to parse. When you invoke SWIG, you pass it the config file and the
desired output language. SWIG generates the appropriate C/C++ and python
glue code. You compile and link the generated C/C++ and import the
generated python module. Sounds simple enough, lets work through and
example.

**Note regarding C vs C++**

I realize that C and C++ are not the same language and this tutorial
will work with C++ and not C. C coders should console yourselves with
the a sense of smug superiority that there is no C analog to the
C++-python problems we will address. The techniques introduced should
still transfer toward making your C API more pythonic.

**Note regarding*nix vs Windows**

I have no experience with SWIG and Windows. I'm told it isn't
[difficult](http://www.swig.org/Doc1.3/Windows.html). All my programming and
building experience is in linux and I wasn't up to the challenge of figuring out
declspec just for this tutorial (extra credit for anyone that can
explain/justify declspec to me).

Illuminating Example
====================

Deciding what to have for breakfast is a computationally difficult
problem. Breakfast calculations taking excessively long have been known
to cause the phenomenon of "brunch." Since it is computationally
expensive, python is the the appropriate language for writing a
Breakfast library. Something closer to the metal, like C++, is. So we
write our library in C++ and then decide to generate python bindings
using SWIG.

To generate the python bindings to breakfast (called pyfast), we must
write a SWIG config file. This file names the python module that you
will be generating and tells SWIG which headers to parse and what
additional steps should be taken. For the sake of example, we have three
header files Bfast.hpp, Spam.hpp and Eggs.hpp, which define classes
Bfast, Spam and Eggs, respectively. Spam and Eggs inherit from Bfast.
Here is our initial pyfast.swg configuration file. (`` `*.i ``\` is also
a common extension for SWIG config files)

```c 
  %module pyfast
  %{
  #include "Bfast.hpp"
  #include "Eggs.hpp"
  #include "Spam.hpp"
  %}

  %include "std_string.i"

  %include "Bfast.hpp"
  %include "Eggs.hpp"
  %include "Spam.hpp"
```

The \#include's are normal preprocessor directives. The %include's are SWIG
directives that tell to actually generate the python bindings.  Notice that you
first \#include the file, then you tell swig to generate the bindings with the
%include directive. Lets start building things.  Below is a simple makefile for
%building our swig pyshapes module.  [Similar
%examples](http://www.dabeaz.com/cgi-bin/wiki.pl?SwigFaq/SharedLibraries) exist
%for other platforms.

```Makefile
    #!Lineno
    #!Makefile
    PYTHON_INCLUDE_DIR = "/usr/include/python2.6"

    pyfast:
            swig -python -c++ pyfast.swg
            g++ -fPIC -shared \
                    -o _pyfast.so \
                    food.cpp \
                    pyfast_wrap.cxx \
                    -I $(PYTHON_INCLUDE_DIR)
```

-   **Line 4**: We call swig on pyfast.swg, telling it that the headers
    are C++ (swig assumes C by default) and that we want to generate
    python bindings. This generates two files: pyfast.py (the python
    bindings) and pyfast_wrap.cxx (the C++ glue code).
-   **Line 5**: We build the shared library. -fPIC and -shared are
    compiler options for position independent code and building a shared
    library.
-   **Line 6**: Continuation of build command. We specify that the name
    of the shared library will be _pyfast.so. If you look in the
    pyfast.py file you will see that this is the assumed name for the
    shared library.
-   **Line 7**: Continuation of build command. We specify the source
    code being wrapped. If libbfast.so already exists, we can link to
    that rather than compiling in.
-   **Line 8**: Continuation of build command. We compile the actual
    glue code.
-   **Line 9**: Continuation of build command. We tell the compiler
    where to find the python header files. Depending on your platform
    configuration, you may need to explicitly tell the compiler where
    the python runtime library is.

If all went well, we should now have _pyfast.so and pyfast.py. Lets run
a test script to see how things worked. Download it here.

Python and C++ are Different Languages
======================================

Python and C++ are different languages (surprise!). They have different
conventions and different features. Don't be surprised if there isn't a
direct analog of some C++ feature in python (or vice versa).

Templates and SWIG
------------------

Metaprogramming is really nice in C++ because it allows you to write
general algorithms, but get specialized performance. It is nice that the
compiler generates templated code for you, but it is awkward when you
want to like to that generated code. SWIG only knows how to link python
to compiled object code. The way around this is that for every template
that occurs in the python bindings, you have to manually create an
instance. So if your API takes a vector of doubles, you have to make a
vector of doubles in the your swig config. Add the following code to
your swig config.

    %include "std_vector.i"
    %template std::vector<double>;

SWIG is aware of deque, list, map, pair, set, string, and vector. The [SWIG STL
documentation](http://www.swig.org/Doc1.3/Library.html#Library_stl_cpp_library)
is quite helpful.

Renaming
--------

You might have a function in C++ that shares a name with a python
keyword (print perhaps?). One solution around this problem is to rename
your function. You add the following code to your swig config:

    %rename('new_name') old_name

SWIG can be fairly aggressive when renaming things. The above code will
rename all functions named old_name (including class member functions).
You can make the renaming more specific by adding a function signature
and/or class resolution. Lets add the following renaming to our project.

    %rename("__str__") Bfast::string_rep() const;

Ignoring
--------

Sometimes you don't want to expose certain functionality to python.
Sometimes you can't get something to work and you want the problem to
Just Go Away. SWIG can ignore the problem

    %ignore lotsOfSpam(const Bfast&);

Lets put this into our project and see what happens.

You should be careful with %ignore's and %rename's as they tend to be
greedy. If you have class member functions that have the same signature,
they will get renamed/ignored. Also be careful with typedef's as SWIG
doesn't always know that (typedef int Int) Int's are int's.

## [Cross language polymorphism](http://www.swig.org/Doc1.3/Python.html#Python_directors) 

Typical SWIG wrapping consists of generating a proxy class in python
that handles dispatching calls to the compiled C++ library. This works
great so long as your interaction with the library is one way. Function
calls, class instantiations, normal things are all one way
communication. Using python to subclass C++ classes with virtual
functions requires two way communication. The feature you want to
investigate in this case is called "directors".

Auxillary code
--------------

SWIG has the capability of including code directly in the swig config
file.

[C/C++ Helper Functions](http://www.swig.org/Doc1.3/Python.html#Python_nn41)

    %inline

[Additional Python Code](http://www.swig.org/Doc1.3/Python.html#Python_nn42)

    %pythoncode

More Info
=========

The SWIG [C++ documentation](http://www.swig.org/Doc1.3/SWIGPlus.html) is quite
helpful. It discusses everything covered here in greater detail and has sections
specifically dealing with

-   [Swig and C++](http://www.swig.org/Doc1.3/SWIGPlus.html)
-   [Swig and Python](http://www.swig.org/Doc1.3/Python.html)
-   [Wrapping Overloaded Functions and
    Methods](http://www.swig.org/Doc1.3/SWIGPlus.html#SWIGPlus_overloaded_methods)
-   [Templates](http://www.swig.org/Doc1.3/SWIGPlus.html#SWIGPlus_nn30)
-   [Exception Handling](http://www.swig.org/Doc1.3/Customization.html#exception)
-   [STL Exceptions](http://www.swig.org/Doc1.3/Library.html#Library_stl_exceptions)


## Exercise 1

In the episode on [Randomness](http://software-carpentry.org/4_0/invperc/random)
we discussed how it is important (and saves time!) to use well-tested library
routines to generate random numbers.  This principle of reuse is also true for
any other task you need to carry out.  Python is packaged with an [extensive
library of modules](http://docs.python.org/library/index.html). What module
could you use to...
	
* parse a string containing a date or time (e.g. "Thursday, 27 May, 2010")? 
   > [`datetime`](http://docs.python.org/library/datetime.html) and in particular,
   > the function [`datetime.strptime()`](http://docs.python.org/library/datetime.html#datetime.datetime.strptime)


* inspect a bunch of files in a folder or subfolders?
   > One module for doing this is the
   > [`os.path`](http://docs.python.org/library/os.path.html).  It has lots of
   > functions for manipulating path names, and also this gem of a function:
   > [`os.path.walk`](http://docs.python.org/library/os.path.html#os.path.walk).

* manage command line arguments to your program?
   > [`argparse`](http://docs.python.org/library/argparse.html)

* access data on the web?
   > For very basic operations like downloading files, check out:
   > [`urllib`](http://docs.python.org/library/urllib.html)


## Exercise 2

In this lecture we wrote roughly the following snippet to parse the command line arguments.
    
```python
import sys

def fail(message):
 print message
 sys.exit(1)

def parse_arguments(arguments):
  '''Parse strings to get controlling parameters.'''

  try:
    grid_size   = int(arguments[0])
    value_range = int(arguments[1])
    random_seed = int(arguments[2])
  except IndexError:
    fail('Expected 3 arguments, got %d' % len(arguments))
  except ValueError:
    fail('Expected int arguments, got %s' % str(arguments))

  return grid_size, value_range, random_seed

if __name__ == '__main__':
 arguments = sys.argv[1:]
 grid_size, value_range, random_seed = parse_arguments(arguments)

  # print out the arguments
  print "grid size = %i \t value range = %i \t seed = %i" % \
  Â   (grid_size, value_range, random_seed)
```

As we learned in Exercise 1, python comes with a library,
[`argparse`](http://docs.python.org/library/argparse.html), to do this in a way
that's easier to extend and with better error messages.Â  

In this exercise, you'll rework the above code to use the argparse library.
You'll need to import the `argparse` module, and rewrite `parse_arguments` to
use it.

**A hint on getting started:**

> You'll need to create an ArgumentParser object, and then call it's
> `add_argument` method for each of the arguments.


**A hint on setting up `argparse`:**

> The arguments are positional arguments so when you call `add_argument` you just
> need to supply a
> [name](http://docs.python.org/library/argparse.html#name-or-flags), and a type
> (extra hint: `int`).  Passing in a help message, default value, and so on is not
> mandatory.

**A hint if you're really stuck:**

> ```python
> parser.add_argument('grid_size', type=int, help="Grid size")
> ```

**A hint on accessing the parsed arguments:**

> If you created an argument named "foo", you can access it by calling:
> 
> ```python
> args = parser.parse_args()
> print args.foo
> ```

**Our answer:**
> ```python
> import sys, argparse
> 
> def parse_arguments(arguments):
>   '''Parse strings to get controlling parameters.'''
> 
>   parser = argparse.ArgumentParser()
>   parser.add_argument('grid_size',   type=int, help='Grid size')
>   parser.add_argument('value_range', type=int, help='Value range')
>   parser.add_argument('random_seed', type=int, help='Random seed')
>   args   = parser.parse_args(arguments)
> 
>   return args.grid_size, args.value_range, args.random_seed
> 
> if __name__ == '__main__':
>   arguments = sys.argv[1:]
>   grid_size, value_range, random_seed = parse_arguments(arguments)
>   print "grid size = %i \t value range = %i \t seed = %i" % \
>     (grid_size, value_range, random_seed)
> ```
>
> The `fail()` function is no longer necessary since `argparse.parse_args`
> exits and prints a help message if incorrect arguments are passed in.  Try
> running your program with only the argument "-h" to get a more verbose help
> message.

Dev Notes
=========

Things we want to demonstrate:

- write lots of small tools (functions)
- document your code
- test your code, including TDD
- debugging
  -   build in a crash somewhere and explain tracebacks
  -   do an import pdb; pdb.set_trace()

Create a few utilities to work with the `*animal.txt` files from
introductory Python.

Put all the functionality in one file and make small scripts that import
from that file, parse command line arguments, and do stuff.

- average number of an animal seen per sighting

Students may want an IPython notebook open as a scratch pad.

Lesson Plan
-----------

We're going to make a command line script that will print the average
group size in a given file for a given animal.

0.  Discuss libraries and re-using code.
1.  Demonstrate importing a module (maybe `math` or `glob`), then
    demonstrate putting a function in a file and importing it.

    Exercise: Make a new text file called `animals.py`. Copy the file
    reading function from yesterday's IPython notebook into the file and
    modify it so that it returns the columns of the file as lists (instead
    of printing certain lines). (They may want to develop the function in
    the IPython notebook and then copy it over.)

2.  Go over the exercise and talk about documentation as you do.
3.  How do we know the function works correctly? Try importing and
    running it on a small file.
4.  But what if we want to make changes and make sure it still works
    afterward, or we want to make sure it isn't broken when we add new
    stuff later?
    - Demonstrate a potential test solution comparing output in an `if` and
      printing a message if something doesn't match.
    - Explain `assert` and demonstrate
    - Explain unit tests and show how to run with nosetests.

5.  Explain test driven development.
    Exercise: We're going to make a function to calculate the mean of all
    the values in a list, but we're going to write the tests for it first.
    Make a new text file called `test_animals.py`. Make a function called
    `test_mean` that runs your theoretical mean function through several
    tests.

6.  When going over this with the students, put in a test with an empty
    list. Also put in tests with lists that contain all ints.

    Exercise: Write the mean function in `animals.py` and verify that it
    passes your tests.

7.  When going over this with the students do not put in a test for an
    empty list. The error when running the tests will give a chance to
    teach tracebacks. Also do not put in any coercion to float so some
    means will be wrong due to integer truncation. More debugging,
    `--pdb-failure`.

8.  The last piece we'll need is a function that takes the output of the
    file reader function and returns only the data relevant to a given
    animal. Write this function as a live exercise with the students
    participating, though they can do it on their own if they want.

    Exercise: Write tests for a function that will take a file name and
    animal name as arguments, and return the average number of animals per
    sighting.

    Exercise: Write a function that takes a file name and animal name and
    returns the average number of animals per sighting. Make sure it passes
    your tests.

9.  After going over exercises, conclude by showing how to make a
    command line script that takes a file name and animal name as
    arguments and prints the average number of animals per sighting.

10. If time permits, could demonstrate `pdb.set_trace()` somewhere in
    the script execution.

Building a Library of Code you Trust
====================================

Suppose we're going to be dealing a lot with these animal count files,
and doing many different kinds of analysis with them. In the
introduction to Python lesson we wrote a function that reads these files
but it's stuck off in an IPython notebook. We could copy and paste it
into a new notebook every time we want to use it but that gets tedious
and makes it difficult to add features to the function. The ideal
solution would be to keep the function in one spot and use it over and
over again from many different places. Python modules to the rescue!

We're going to move beyond the IPython notebook. Most Python code is
stored in `.py` files and then used in other `.py` files where it
has been pulled in using an `import` statement. Today we'll show you
how to do that.

Exercises
=========

Exercise 1
----------

Make a new text file called `animals.py`. Copy the file reading
function from yesterday's IPython notebook into the file and modify it
so that it returns the columns of the file as lists (instead of printing
certain lines).

Exercise 2
----------

We're going to make a function to calculate the mean of all the values
in a list, but we're going to write the tests for it first. Make a new
text file called `test\_animals.py`. Make a function called
`test\_mean` that runs your theoretical mean function through several
tests.

Exercise 3
----------

Write the mean function in `animals.py` and verify that it passes your
tests.

Exercise 4
----------

Write tests for a function that will take a file name and animal name as
arguments, and return the average number of animals per sighting.

Exercise 5
----------

Write a function that takes a file name and animal name and returns the
average number of animals per sighting. Make sure it passes your tests.

Python Testing Cheat Sheet
==========================

Why testing?
------------

1. Helps you to think about expected behavior, especially boundary cases,
2. documents expected behavior,
3. confidence recent changes didn't break anything that worked before,
4. confidence code is correct.


Defensive programming
---------------------

Using an assertion to ensure input is acceptable:

    def some_function(x):
        assert x >= 0
        # ... continue safe in knowledge that x > 0

Adding an explanatory message to the assertion:

        assert x >= 0, "Function not defined for negative x."

Alternatively, raise an exception to indicate what the problem is:

    def some_function(x):
        if x < 0:
            raise TypeError, "Function not defined for negative x."
        return 0


Unit testing with Nose
----------------------

To run tests, at the shell prompt, type

    nosetests

By default, Nose will

* look for test functions that have a name starting with `test`
* look for them in files with names starting with `test`
* look for such files in the current working directory, and in subdirectories with names starting with `test`

There are some additional rules, and you can configure your own, but this should be enough to get started.

### A simple test

    from nose.tools import assert_equal

    from mystatscode import mean

    def test_single_value():
        observed = mean([1.0])
        expected = 1.0
        assert_equal(observed, expected)

### Other assertions

Nose provides a range of assertions that can be used when a test is not just checking a simple equality, e.g.

    from nose.tools import assert_items_equal

    from mycode import find_factors

    def test_6():
        observed = find_factors(6)
        expected = [2, 3]
        assert_items_equal(observed, expected) # order of factors is not guaranteed

To see the available assertions, and get help with them:

    import nose.tools
    dir(nose.tools)                   # list assertions and other classes/functions
    help(nose.tools.assert_set_equal) # get information about one of them

### Floating point tests

When comparing floating-point numbers for equality, allow some tolerance for small differences due to
the way values are represented and rounded.
* assertGreater, assertLess

    from nose.tools import assert_almost_equal

    from mycode import hypotenuse

    def test_hypotenuse_345():
        observed = hypotenuse(3.0, 4.0)
        expected = 5.0
        assert_almost_equal(observed, expected)

### Testing exceptions

Testing that a method raises the appropriate exception when the input is invalid:

    from nose.tools import raises

    from mystatscode import mean

    @raises(TypeError)
    def test_not_a_list():
        observed = mean(1)

### Fixtures

A *fixture* is what the test function uses as input, e.g. values, objects and arrays.

To set up a fixture once before any tests are run, define a method called `setup` in the same files
as the test functions. This can assign values to global variables for use in the test functions.

    long_list = None

    def setup():
        long_list = [0]
        # append more values to long_list...

If the global variables assigned in `setup` might be modified by some of the test functions, the set-up
step must be executed once before each test function is called:

    from nose.tools import with_setup

    from mycode import mean, clear

    long_list = None

    def setup_each():
        long_list = [0]
        # append more values to long_list...

    @with_setup(setup_each)
    def test_mean_long_list():
        observed = mean(long_list)
        expected = 0.0
        assert_equal(observed, expected)

    @with_setup(setup_each)
    def test_clear_long_list():
        clear(long_list)
	assert_equal(len(long_list), 0)



Test-driven deveopment
----------------------

***Red.*** Write test function that checks one new functionality you want to add to your code. -- tests have to fail.

***Green.*** Write minimal code that implements desired features until all tests pass.

***Refactor.*** Improve code wrt. readability and speed. Constantly check that tests still pass.

***Commit.*** Commit working code to version control.

Repeat.


General advice
--------------

* Perfect test-case coverage is impossible.
* Try to test distinct functionalities.
* If you find a bug yet undiscovered by previous test, make it a new test case.




**Materials originally by John Blischak**

## A TDD Example

To illustrate TDD, let's return to the function you wrote yesterday,
`calculate_gc`. We'll start from scratch and develop the function
by meeting test specifications. 

The beginning of the function is contained in the file `calculate_gc.py`
in this directory. It currently takes one argument as input, but does
nothing.

```python
def calculate_gc(x):
    '''
    Calculates the GC content of DNA sequence x.
    '''
    pass
```

The tests that we must pass are contained in the file
`test_calculate_gc.py`. We can run the tests using `nosetests`.

    nosetests -v test_calculate_gc.py

As expected, we fail all the tests! What is the bare minimum 
functionality we must add to pass the first test below?

```python
def test_only_G_and_C():
    '''
    Sequence of only G's and C's has fraction 1.0
    '''
    fixture = 'GGCGCCGGC'
    result = calculate_gc(fixture)
    assert_equal(result, 1.0)
```

And the second test?

```python
def test_half():
    '''
    Sequence with half G and C has fraction 0.5
    '''
    fixture = 'ATGC'
    result = calculate_gc(fixture)
    assert_equal(result, 0.5)
```

Test number three?

```python
def test_lower_case():
    '''
    Sequence with lower case letters
    '''
    fixture = 'atgc'
    result = calculate_gc(fixture)
    assert_equal(result, 0.5)
```

Test number four?

```python
def test_not_DNA():
    '''
    Raise TypeError if not DNA
    '''
    fixture = 'qwerty'
    assert_raises(TypeError, calculate_gc, fixture)
```

Through this cycle of writing tests and modifying the function to pass 
the tests, we have developed a function that behaves exactly as we 
expect and nothing more. And the tests not only serve as documentation 
of what the function does, but can also be easily ran again if we made 
further modifications (regression tests). What would be the next test 
you would write for our function?

## Exercise: Test function that transcribes DNA to RNA

In the lesson yesterday on functions, `05-python-functions`, one exercise
asked you to write a function to transcribe DNA to RNA. An example of
that function is implemented in this directory in a file named
`transcribe.py`. In that lesson, there were two tests to check your
work:

```python
transcribe('ATGC') == 'UACG'
transcribe('ATGCAGTCAGTGCAGTCAGT') == 'UACGUCAGUCACGUCAGUCA'
```

Convert these to a proper test and place it the file `test_transcribe.py`.
Next, add a few tests of your own and run the test suite with nosetests.

# Testing

* * * * *

**Based on materials by Katy Huff, Rachel Slaybaugh, and Anthony
Scopatz**

![image](https://github.com/thehackerwithin/UofCSCBC2012/raw/scopz/5-Testing/test_prod.jpg)
# What is testing?

Software testing is a process by which one or more expected behaviors
and results from a piece of software are exercised and confirmed. Well
chosen tests will confirm expected code behavior for the extreme
boundaries of the input domains, output ranges, parametric combinations,
and other behavioral **edge cases**.

# Why test software?

Unless you write flawless, bug-free, perfectly accurate, fully precise,
and predictable code **every time**, you must test your code in order to
trust it enough to answer in the affirmative to at least a few of the
following questions:

-   Does your code work?
-   **Always?**
-   Does it do what you think it does? ([Patriot Missile Failure](http://www.ima.umn.edu/~arnold/disasters/patriot.html))
-   Does it continue to work after changes are made?
-   Does it continue to work after system configurations or libraries
    are upgraded?
-   Does it respond properly for a full range of input parameters?
-   What about **edge or corner cases**?
-   What's the limit on that input parameter?
-   How will it affect your
    [publications](http://www.nature.com/news/2010/101013/full/467775a.html)?

## Verification

*Verification* is the process of asking, "Have we built the software
correctly?" That is, is the code bug free, precise, accurate, and
repeatable?

## Validation

*Validation* is the process of asking, "Have we built the right
software?" That is, is the code designed in such a way as to produce the
answers we are interested in, data we want, etc.

## Uncertainty Quantification

*Uncertainty Quantification* is the process of asking, "Given that our
algorithm may not be deterministic, was our execution within acceptable
error bounds?" This is particularly important for anything which uses
random numbers, for example, Monte Carlo methods.

# Where are tests?

Say we have an averaging function:

```python
def mean(numlist):
    total = sum(numlist)
    length = len(numlist)
    return total/length
```

Tests could be implemented as runtime **exceptions in the function**:

```python
def mean(numlist):
    try:
        total = sum(numlist)
        length = len(numlist)
    except TypeError:
        raise TypeError("The number list was not a list of numbers.")
    except:
        print "There was a problem evaluating the number list."
    return total/length
```

Sometimes tests they are functions alongside the function definitions
they are testing.

```python
def mean(numlist):
    try:
        total = sum(numlist)
        length = len(numlist)
    except TypeError:
        raise TypeError("The number list was not a list of numbers.")
    except:
        print "There was a problem evaluating the number list."
    return total/length


def test_mean():
    assert mean([0, 0, 0, 0]) == 0
    assert mean([0, 200]) == 100
    assert mean([0, -200]) == -100
    assert mean([0]) == 0


def test_floating_mean():
    assert mean([1, 2]) == 1.5
```

Sometimes they are in an executable independent of the main executable.

```python
def mean(numlist):
    try:
        total = sum(numlist)
        length = len(numlist)
    except TypeError:
        raise TypeError("The number list was not a list of numbers.")
    except:
        print "There was a problem evaluating the number list."
    return total/length
```

Where, in a different file exists a test module:

```python
import mean

def test_mean():
    assert mean([0, 0, 0, 0]) == 0
    assert mean([0, 200]) == 100
    assert mean([0, -200]) == -100
    assert mean([0]) == 0


def test_floating_mean():
    assert mean([1, 2]) == 1.5
```

# When should we test?

The three right answers are:

-   **ALWAYS!**
-   **EARLY!**
-   **OFTEN!**

The longer answer is that testing either before or after your software
is written will improve your code, but testing after your program is
used for something important is too late.

If we have a robust set of tests, we can run them before adding
something new and after adding something new. If the tests give the same
results (as appropriate), we can have some assurance that we didn't
wreak anything. The same idea applies to making changes in your system
configuration, updating support codes, etc.

Another important feature of testing is that it helps you remember what
all the parts of your code do. If you are working on a large project
over three years and you end up with 200 classes, it may be hard to
remember what the widget class does in detail. If you have a test that
checks all of the widget's functionality, you can look at the test to
remember what it's supposed to do.

# Who should test?

In a collaborative coding environment, where many developers contribute
to the same code base, developers should be responsible individually for
testing the functions they create and collectively for testing the code
as a whole.

Professionals often test their code, and take pride in test coverage,
the percent of their functions that they feel confident are
comprehensively tested.

# How are tests written?

The type of tests that are written is determined by the testing
framework you adopt. Don't worry, there are a lot of choices.

## Types of Tests

**Exceptions:** Exceptions can be thought of as type of runtime test.
They alert the user to exceptional behavior in the code. Often,
exceptions are related to functions that depend on input that is unknown
at compile time. Checks that occur within the code to handle exceptional
behavior that results from this type of input are called Exceptions.

**Unit Tests:** Unit tests are a type of test which test the fundamental
units of a program's functionality. Often, this is on the class or
function level of detail. However what defines a *code unit* is not
formally defined.

To test functions and classes, the interfaces (API) - rather than the
implementation - should be tested. Treating the implementation as a
black box, we can probe the expected behavior with boundary cases for
the inputs.

**System Tests:** System level tests are intended to test the code as a
whole. As opposed to unit tests, system tests ask for the behavior as a
whole. This sort of testing involves comparison with other validated
codes, analytical solutions, etc.

**Regression Tests:** A regression test ensures that new code does
change anything. If you change the default answer, for example, or add a
new question, you'll need to make sure that missing entries are still
found and fixed.

**Integration Tests:** Integration tests query the ability of the code
to integrate well with the system configuration and third party
libraries and modules. This type of test is essential for codes that
depend on libraries which might be updated independently of your code or
when your code might be used by a number of users who may have various
versions of libraries.

**Test Suites:** Putting a series of unit tests into a collection of
modules creates, a test suite. Typically the suite as a whole is
executed (rather than each test individually) when verifying that the
code base still functions after changes have been made.

# Elements of a Test

**Behavior:** The behavior you want to test. For example, you might want
to test the fun() function.

**Expected Result:** This might be a single number, a range of numbers,
a new fully defined object, a system state, an exception, etc. When we
run the fun() function, we expect to generate some fun. If we don't
generate any fun, the fun() function should fail its test.
Alternatively, if it does create some fun, the fun() function should
pass this test. The the expected result should known *a priori*. For
numerical functions, this is result is ideally analytically determined
even if the function being tested isn't.

**Assertions:** Require that some conditional be true. If the
conditional is false, the test fails.

**Fixtures:** Sometimes you have to do some legwork to create the
objects that are necessary to run one or many tests. These objects are
called fixtures as they are not really part of the test themselves but
rather involve getting the computer into the appropriate state.

For example, since fun varies a lot between people, the fun() function
is a method of the Person class. In order to check the fun function,
then, we need to create an appropriate Person object on which to run
fun().

**Setup and teardown:** Creating fixtures is often done in a call to a
setup function. Deleting them and other cleanup is done in a teardown
function.

**The Big Picture:** Putting all this together, the testing algorithm is
often:

```python
setup()
test()
teardown()
```

But, sometimes it's the case that your tests change the fixtures. If so,
it's better for the setup() and teardown() functions to occur on either
side of each test. In that case, the testing algorithm should be:

```python
setup()
test1()
teardown()

setup()
test2()
teardown()

setup()
test3()
teardown()
```

* * * * *

# Nose: A Python Testing Framework

The testing framework we'll discuss today is called nose. However, there
are several other testing frameworks available in most languages. Most
notably there is [JUnit](http://www.junit.org/) in Java which can
arguably attributed to inventing the testing framework.

## Where do nose tests live?

Nose tests are files that begin with `Test-`, `Test_`, `test-`, or
`test_`. Specifically, these satisfy the testMatch regular expression
`[Tt]est[-_]`. (You can also teach nose to find tests by declaring them
in the unittest.TestCase subclasses that you create in your code. You
can also create test functions which are not unittest.TestCase
subclasses if they are named with the configured testMatch regular
expression.)

## Nose Test Syntax

To write a nose test, we make assertions.

```python
assert should_be_true()
assert not should_not_be_true()
```

Additionally, nose itself defines number of assert functions which can
be used to test more specific aspects of the code base.

```python
from nose.tools import *

assert_equal(a, b)
assert_almost_equal(a, b)
assert_true(a)
assert_false(a)
assert_raises(exception, func, *args, **kwargs)
assert_is_instance(a, b)
# and many more!
```

Moreover, numpy offers similar testing functions for arrays:

```python
from numpy.testing import *

assert_array_equal(a, b)
assert_array_almost_equal(a, b)
# etc.
```

## Exercise: Writing tests for mean()

There are a few tests for the mean() function that we listed in this
lesson. What are some tests that should fail? Add at least three test
cases to this set. Edit the `test_mean.py` file which tests the mean()
function in `mean.py`.

*Hint:* Think about what form your input could take and what you should
do to handle it. Also, think about the type of the elements in the list.
What should be done if you pass a list of integers? What if you pass a
list of strings?

**To run the tests**:

    nosetests -v test_mean.py

# Test Driven Development

Test driven development (TDD) is a philosophy whereby the developer
creates code by **writing the tests first**. That is to say you write the
tests *before* writing the associated code!

This is an iterative process whereby you write a test then write the
minimum amount code to make the test pass. If a new feature is needed,
another test is written and the code is expanded to meet this new use
case. This continues until the code does what is needed.

TDD operates on the YAGNI principle (You Ain't Gonna Need It). People
who diligently follow TDD swear by its effectiveness. This development
style was put forth most strongly by [Kent Beck in
2002](http://www.amazon.com/Test-Driven-Development-By-Example/dp/0321146530).

## A TDD Example

Say you want to write a fib() function which generates values of the
Fibonacci sequence of given indexes. You would - of course - start by
writing the test, possibly testing a single value:

```python
from nose.tools import assert_equal

from pisa import fib

def test_fib1():
    obs = fib(2)
    exp = 1
    assert_equal(obs, exp)
```

You would *then* go ahead and write the actual function:

```python
def fib(n):
    # you snarky so-and-so
    return 1
```

And that is it right?! Well, not quite. This implementation fails for
most other values. Adding tests we see that:

```python
def test_fib1():
    obs = fib(2)
    exp = 1
    assert_equal(obs, exp)


def test_fib2():
    obs = fib(0)
    exp = 0
    assert_equal(obs, exp)

    obs = fib(1)
    exp = 1
    assert_equal(obs, exp)
```

This extra test now requires that we bother to implement at least the
initial values:

```python
def fib(n):
    # a little better
    if n == 0 or n == 1:
        return n
    return 1
```

However, this function still falls over for `2 < n`. Time for more
tests!

```python
def test_fib1():
    obs = fib(2)
    exp = 1
    assert_equal(obs, exp)


def test_fib2():
    obs = fib(0)
    exp = 0
    assert_equal(obs, exp)

    obs = fib(1)
    exp = 1
    assert_equal(obs, exp)


def test_fib3():
    obs = fib(3)
    exp = 2
    assert_equal(obs, exp)

    obs = fib(6)
    exp = 8
    assert_equal(obs, exp)
```

At this point, we had better go ahead and try do the right thing...

```python
def fib(n):
    # finally, some math
    if n == 0 or n == 1:
        return n
    else:
        return fib(n - 1) + fib(n - 2)
```

Here it becomes very tempting to take an extended coffee break or
possibly a power lunch. But then you remember those pesky negative
numbers and floats. Perhaps the right thing to do here is to just be
undefined.

```python
def test_fib1():
    obs = fib(2)
    exp = 1
    assert_equal(obs, exp)


def test_fib2():
    obs = fib(0)
    exp = 0
    assert_equal(obs, exp)

    obs = fib(1)
    exp = 1
    assert_equal(obs, exp)


def test_fib3():
    obs = fib(3)
    exp = 2
    assert_equal(obs, exp)

    obs = fib(6)
    exp = 8
    assert_equal(obs, exp)


def test_fib3():
    obs = fib(13.37)
    exp = NotImplemented
    assert_equal(obs, exp)

    obs = fib(-9000)
    exp = NotImplemented
    assert_equal(obs, exp)
```

This means that it is time to add the appropriate case to the function
itself:

```python
def fib(n):
    # sequence and you shall find
    if n < 0 or int(n) != n:
        return NotImplemented
    elif n == 0 or n == 1:
        return n
    else:
        return fib(n - 1) + fib(n - 2)
```

# Quality Assurance Exercise

Can you think of other tests to make for the Fibonacci function? I promise there 
are at least two. 

Implement one new test in test_fib.py, run nosetests, and if it fails, implement 
a more robust function for that case.

And thus - finally - we have a robust function together with working
tests!

# Exercise

**The Problem:** In 2D or 3D, we have two points (p1 and p2) which
define a line segment. Additionally there exists experimental data which
can be anywhere in the domain. Find the data point which is closest to
the line segment.

In the `close_line.py` file there are four different implementations
which all solve this problem. [You can read more about them
here.](http://inscight.org/2012/03/31/evolution_of_a_solution/) However,
there are no tests! Please write from scratch a `test_close_line.py`
file which tests the closest\_data\_to\_line() functions.

*Hint:* you can use one implementation function to test another. Below
is some sample data to help you get started.

![image](https://github.com/thehackerwithin/UofCSCBC2012/raw/scopz/5-Testing/evo_sol1.png)
> -

```python
import numpy as np

p1 = np.array([0.0, 0.0])
p2 = np.array([1.0, 1.0])
data = np.array([[0.3, 0.6], [0.25, 0.5], [1.0, 0.75]])
```


# Python 1: The iPython Shell, Variables, and Basic Data Types

[Back To The
Shell](http://github.com/thehackerwithin/UofCSCBC2012/tree/master/1-Shell/)
- [Forward to Lists, dictionaries, sets, and
tuples](http://github.com/thehackerwithin/UofCSCBC2012/tree/master/2b-PythonDataStructures/)

* * * * *

**Based on Lecture Materials By: Milad Fatenejad with contributions from Katy Huff, Tommy Guy and Many More**

# What is Python ?

Python is an interpreted (pre-compiled) language. Its simple, high
level, human readable interface speeds the programming process in
exchange for some computation time overhead. Python is implemented
mostly in the C programming language, so, as python develops, it is
increasingly possible to do everything in Python that is possible in C.
Python is also free and open source, so if you find a bug or generate a
useful module, the Python Software Foundation will likely be happy to
merge your changes into the language.

During this session you are going to learn about some very basics about
how to execute python code as well as some examples of the built-in
Python data types.

Built-in data types are the basic building blocks of Python programs.
They are really basic things like strings and numbers (either integers,
complex or floating point numbers). There are simple containers like
lists (think of lists as arrays or vectors), tuples and dictionaries.

# Hello World

First, we will use python ''interactively''. This means that we will
type commands directly into iPython. Once we start performing more
complicated tasks we will start writing Python scripts and programs in a
text editor, outside of the interpreter.

To get to the python shell, type **python** into the terminal.

```python
>>> print "Hello World"
Hello World
>>> exit()
```

To get to the interactive python interpreter, a more sophisticated
python shell, type **ipython** into the terminal:

```python
In [1]: print "Hello World"
Hello World
In [2]: exit
```

You can also put the commands in a **.py** file and execute that file in
the terminal by typing **python [filename]**

    $ gedit myfile.py &
    <edit myfile with the hello world program.>
    $ python myfile.py
    Hello World!

# Pasting into iPython

**Note:**

To paste text from another application (i.e. the internet browser) into
iPython:

1.  select text from the wiki
2.  copy with **ctrl+c**
3.  in iPython, type **%paste**

The code should paste and execute in iPython.

# Variables

Variables are names, while values are the data assigned to those names.

## Questions : Variables and Values

In the code snippet :

```python
a=2
b="string"
c=a
```

-   What is the value of the variable \`c\`?
-   What is the value of the variable b ?
-   What is the name given to the variable 2 ?

(The last one is a trick, the value 2 has two names.)

# Strings and Numbers

It is really easy to make variables in python. For example, to create a
string, \`s\`, and print its value, simply type the following into
iPython:

```python
s = "Hello World"
print s
```

If you want to see what the type of a variable is, you can use the
built-in python function, \`type\`. Just enter

```python
print type(s)
```

into iPython and you should see something like this:

```python
<type 'str'>
```

This tells us that \`s\` is of type **str** (i.e. that \`s\` is a
string). Making numeric variables is equally easy and intuitive. Try
entering the following into IPython. Notice that the \# symbol is used
to start comments so everything after the pound sign is ignored.

```python
i,r,c = -10, 3.5, 1.0 + 2j  # set i to -10, r to 3.5 and c to 1.0+2j
```

This one line sets the variable \`i\` to the integer -10 , \`r\` to the
floating point value 3.5 (a floating point number is just a
real/non-integer number) and \`c\` to the value 1.0 + 2j (Notice, how
easy and intuitive it is in python to set multiple variables to
something. You'll discover a lot of similar syntax that is designed to
make your life easier). Lets use the built-in type function to determine
the type of each of the three variables we just created:

```python
print type(i), type(r), type(c) 
```

This will give : 

```python
<type 'int'\> <type 'float'\> <type 'complex'\>
```

This tells us that "i" is an integer, "r" is a floating point number,
and "c" is a complex number. As you can see, Python has built-in support
for imaginary numbers!

**Aside: Long integers** Another way python makes our lives easier is by
allowing integers to be arbitrary large. In languages like C/C++ and
FORTRAN integer variables can only store values up to a certain size.
But entering and manipulating the following forty digit number with
iPython is no problem:

```python
i = 1234567890123456789012345678901234567890 
print i * 6
```

Operations in Python are defined by their type. For instance, look the
difference between these operations:

```python
In[1]:  1 + 3
  4
In[2]:  1.0 + 3
  4.0  # This is a float
In[3]: "Hello " + "world"
  'Hello world'
In[4]: 1 + "Hello"
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: unsupported operand type(s) for +: 'int' and 'str'
```

In the first two cases, addition between numbers meant that 1 was added
to 3 using the standard type rules (float plus int = float). In the
third case, the command was string addition, which concatenates two
strings. The final case broke because an 'int' type can not be added to
a 'str' type. This is because it's unclear how to interpret an int as a
string: should it be the string representation, the ASCII character
code, or something else entirely?

One way to handle this is to explicitly convert the int into a string:

```python
str(1) + "Hello"
```

Equivalent functions exist for converting to **int**, **float**, and
other types.

Basic data types in Python have a lot of functionality already built in.
For example, lets say that you are reading names from a file one line at
a time and that sometimes the names have leading and trailing spaces
that we want to strip away. We can just use the \`strip\` string method
to accomplish this. For example, type the following into iPython:

```python
In[1]: name = "   Milad    "
In[2]: print name + "is here"
      Milad     is here
```

Now enter \`name.strip()\` instead of \`name\`:

```python
In[1]: print name.strip() + " is here"
 Milad is here
```

Notice that the extra spaces are gone. We used the \`strip()\` method,
which removes leading and trailing white space from strings. You can
think of a method as being a function that is attached to a particular
variable. You call methods by typing: \`<variable\>.<method name\>\`.

**Aside : Tab Completion**

Maybe you've noticed this already, but check out what happens you begin
typing a variable name (the first two letters of name, for example) and
press tab.

Convenient, right? This is also true of many built in functions.

# Dynamic Typing

Importantly, python is a **dynamically typed** language. That is, an
explicit type is not needed when creating a variable. Also, this means
that variables in Python which are initialized to a variable of one type
can be re-assigned to a variable of a different type. Try this:

```python
sillystring = "What is the airspeed velocity of an unladen swallow?"
print type(sillystring)
```

You'll see:

```python
<type 'str'>
```

If you reassign silly string to an integer, what happens? That is, when
you type :

```python
sillystring = 98
print type(sillystring)
```

You should see:

```python
<type 'int'>
```

This is an interesting feature. Can you think of ways it can be helpful?
Are there ways it might be troublesome?

What is the type of sillystring be after this :

```python
sillystring += 0.1
```

**Aside: In Place Equivalency**

What is the += syntax about? This is an in-place way to write
`` `sillystring =  sillystring + 0.1 ``\`. It is common in a number of
languages.

Importantly, though we do not explcity state them, variables always have
exactly one type. The number 98 is an **int**. For the variable holding
this value to be treated as a float, it must be assigned as **98.0**.

## Questions : Dynamic Typing

Imagine that I first assign :

```python
a=2
```

Then, I assign :

```python
a="Welcome to the ministry of silly walks."
```

What has happened to the memory that was pointing to the number 2??

# Getting Help

One of the really nice features in Python is that a lot of the help and
documentation is built into the code. Practically, this means that much
of the time you don't have to go digging through some web site to find
help. You can get help in Python using the \`help\` function. Lets look
at an example - enter

```python
help(str.strip) 
```

into IPython. You should then see documentation for the strip method pop
up. (NOTE: if you don't automatically return to the python interpreter,
just hit "\`q\`" to exit the help screen). You can also use the question
mark, "\`?\`", character to display the documentation as well. For
example, enter

```python
str.strip?
```

into IPython to view the documentation.

Now try entering

```python
help(str)
```

You should see documentation for the entire string type, including all
of the string methods. This can be useful when you are trying to perform
a specific task, but you don't know the right function to call. For
example, lets say we want to convert the string "cooper" to uppercase,
and we want to know if there is a string method which can do the job for
us. Start by typing "\`help(str)\`" to pull up the string documentation.
You can scroll through the string methods until you find a method called
"upper" which has documentation that looks like:

    |  upper(...)
    |      S.upper() -> string
    |      |      Return a copy of the string S converted to uppercase.

These lines tell us that the string class has a method called "upper"
which can be used to convert strings to uppercase. Now enter:

```python
name = "cooper"
print name.upper()
```

At which point, you should see the word "COOPER" printed to the screen.

**Aside: Using Methods Directly on Data**

* * * * *

In the previous example, we first created a string variable, \`name\`,
assigned it the value "cooper", then used the \`upper\` string method to
obtain the uppercased version of the string. We didn't have to create a
variable, however. We could simply enter:

```python
print "cooper".upper()
```

To generate the uppercased version.

As we saw above, the **str** type has a lot of documentation associated
with it, and we had to sift through most of it to find the upper method.
If we had a way to simply print all of the **str** methods, we could
have probably figured out that the \`upper\` method is what we wanted by
the name and in a lot less time. Luckily, python has a built in
function, "\`dir\`", for just this situation. The \`dir\` function takes
a type name and prints all of the methods associated. Try entering
"\`print dir(str)\`" to see a list of every method and variable
associated with the string class. You can ignore the methods that start
and end with double underscores for now. Try printing the methods
associated with the **int**, and **complex** types.

Finally, there are some really basic functions that are built right into
python that we have been using. For example, we used the "float"
function above to convert a string to a floating point number. You can
see a list of built in functions by entering
\`dir(\_[\_builtins](____________________________________________________________________Python%201%20:%20The%20Shell,%20Variables,%20and%20Basic%20Data%20Types))\`.
If you see something interesting, such as the \`zip\` function, you can
examine what it does using help(zip).

## Example : Manipulating Basic Data Types

Use the basic data types we've learned about along with the \`help\` and
\`dir\` functions to figure out how to do the following using either one
function or one method call:

-   Take the absolute value of the number -1.4
-   Begin with the string "a MaN and His DOG" and create the string "A
    man and his dog"
-   Return the position of the character 'e' in the string "my test
    string" (The answer is 4, since \`m\` is is at position 0 not
    position 1)


# Python, iPython, and the basics

* * * * *


**Based on Lecture Materials By: Milad Fatenejad, Katy Huff, Tommy Guy, Joshua 
R. Smith, Will Trimble, and many more**

## Introduction
This lecture is on basic programming in python. In order to do the examples, we are going to use an environment called iPython notebook.  I expect this lecture to be interactive, so stop me at any point if you have questions. The correct power dynamic is that people are the masters and the machines are servants. The computer is a hammer; it exists to help us get things done.  We can hammer nails with the handle, with the claw of the hammer; some of us even hammer nails with bricks.  But when you learn what part of the hammer works best with nails, and have some experience swinging it, you spend less time worrying about the hammering and more time worrying about your furniture.

So now would be a good time to roll out [PEP 20, The Zen of Python] (http://www.python.org/dev/peps/pep-0020/)

> Beautiful is better than ugly.
> Explicit is better than implicit.
> Simple is better than complex.
> Complex is better than complicated.
> Flat is better than nested.
> Sparse is better than dense.
> Readability counts.
> Special cases aren't special enough to break the rules.
> Although practicality beats purity.
> Errors should never pass silently.
> Unless explicitly silenced.
> In the face of ambiguity, refuse the temptation to guess.
> There should be one-- and preferably only one --obvious way to do it.
> Although that way may not be obvious at first unless you're Dutch.
> Now is better than never.
> Although never is often better than *right* now.
> If the implementation is hard to explain, it's a bad idea.
> If the implementation is easy to explain, it may be a good idea.
> Namespaces are one honking great idea -- let's do more of those!

This lecture will be structured as follows: I will be teaching the basics of two things: the python programming language (to a greater extent) and the ipython interpreter (to a lesser extent). The ipython interpreter is one of many different ways to implement python code. As far as the python component, I'll shoot for a layered approach: I'l continue building on my previous concepts. It turns out that like any sufficiently complex topic, its not really possible to force the pedagogy into a serial stream. Also, we have a pretty serious time constraint. I'm just going to drop it on you. Because of the brief nature of this tutorial, I've included links to some excellent reference material. Also, if we have time, I'll take questions based on the specific programming needs of this class.

Here is the reference material.

* [Dive into Python] (http://www.diveintopython.net/toc/index.html)
* [Software Carpentry's Python Lectures] (http://software-carpentry.org/4_0/python/)
* [IPython: A System for Interactive Scientific Computing] (http://dx.doi.org/10.1109/MCSE.2007.53)
* [How to Think Like a Computer Scientist] (http://www.greenteapress.com/thinkpython/thinkpython.html)

Once we briefly deal with ipython, I'll cover python in the following order:

## What I'll cover
### Lesson 1
* print statements
* variables
* integers
* floats
* strings
* types
* type coersion
* basic operations: add numbers, concatenate strings, basic data type functionality

### Lesson 2
* list
* dictionary 
* set 
* tuple
* file reading

### Lesson 3
* for loop
* conditional (if) statements
* while loops
* iteration
* writing to files

### Lesson 4
* methods
* modules

## iPython
You can run python commands in a handful of ways; you can create executable scripts, you can run the python interpreter, you can run iPython, or you can run iPython notebook.  iPython is an alternative to the built-in Python interpreter with some nice features.  iPython notebook gives you interactive access to the python interpreter from within a browser window, and it allows you to save your commands as a "notebook".
Lets give the built-in interpreter a spin just this once.

```
swc@swc:~$ python
Python 2.7.3 (default, Apr 20 2012, 22:44:07) 
[GCC 4.6.3] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> print "hello world"
hello world
>>> quit() 
```

We can also write python commands in a file and execute them from the command line. You will notice that the print command above is located in the file hello.py. Execute the following command at the command line

```
swc@swc:~$ python hello.py
```

iPython has more useful features for interactive use than the standard python interpreter, so we'll use it from here on out.

```python
swc@swc:~$ ipython notebook
In [1]: print "hello world"
hello world
In [2]: 
```

You may notice if you hit ENTER your current command does not execute.  ENTER puts a line break in your current command, and allows you to write multi-line commands and have them all executed at once.  
SHIFT-ENTER sends the line that your cursor is on to the interpreter.  The output of the command, or the error message, appears below the line you entered it on.

```
In [2]: print "hello"
        print "world"
hello
world
In [3]: 
```

### Pasting

You can paste things into the ipython console by copying text from your machine with **ctrl+c** and typing **%paste** at the iPython prompt.

### History

iPython has a history. If you press the up and down keys, you can access the history.

### Tab Completion

iPython also has tab completion of previous commands. Try typing "print" and then hit the tab key.

### Getting Help

iPython has some nice help features. Lets say we want to know more about the integer data type. There are at least two ways to do this task:

```python
In [1] help(int)
```

or 

```python
In [1] int?
```

If you wanted to see all the commands available for something, use the dir command. Check out all of the methods of the str type.

```python
In [1] dir(str)
```

### Executing code in files

If your code is in a file, you can execute it from the iPython shell with the **%run** command. Execute hello.py like so

```python
In [1] %run hello.py
```

### Clearing iPython

To clear everything from iPython, use the reset command.

```python
In [1] reset
Once deleted, variables cannot be recovered. Proceed (y/[n])?
```

## Variables

All programming languages have variables, and python is no different. To create a variable, just name it and set it with the equals sign. One important caveat: variable names can only contain letters, numbers, and the underscore character. Lets set a variable.

```python
In [1]: experiment = "current vs. voltage"

In [2]: print experiment
current vs. voltage

In [3]: voltage = 2

In [4]: current = 0.5

In [5]: print voltage, current
2 0.5
```

## Types and Dynamic Typing

Like most programming languages, things in python are typed. The type refers to the type of data. We've already defined three different types of data in experiment, voltage, and current. The types are string, integer, and float. You can inspect the type of a variable by using the type command.

```python
In [6]: type(experiment)
Out[6]: <type 'str'>

In [7]: type(voltage)
Out[7]: <type 'int'>

In [8]: type(current)
Out[8]: <type 'float'>
```

Python is a dynamically typed language (unlike, say, C++). If you know what that means, you may be feeling some fear and loathing right now. If you don't know what dynamic typing means, the next stuff may seem esoteric and pedantic. Its actually important, but its importance may not be clear to you until long after this class is over.

Dynamic typing means that you don't have to declare the type of a variable when you define it; python just figures it out based on how you are setting the variable. Lets say you set a variable. Sometime later you can just change the type of data assigned to a variable and python is perfectly happy about that. Since it won't be obvious until (possibly much) later why that's important, I'll let you marinate on that idea for a second. 

Here's an example of dynamic typing. What's the type of data assigned to voltage?

```python
In [9]: type(voltage)
Out[9]: <type 'int'>
```

Lets assign a value of 2.7 (which is clearly a float) to voltage. What happens to the type?

```python
In [10]: voltage = 2.7

In [11]: type(voltage)
Out[11]: <type 'float'>
```

You can even now assign a string to the variable voltage and python would be happy to comply.

```python
In [12]: voltage = "2.7 volts"

In [13]: type(voltage)
Out[13]: <type 'str'>
```

I'll let you ruminate on the pros and cons of this construction while I change the value of voltage back to an int:

```python
In [14]: voltage = 2
```

## Coersion
It is possible to coerce (a fancy and slightly menacing way to say "convert") certain types of data to other types. For example, its pretty straightforward to coerce numerical data to strings.

```python
In [19]: voltageString = str(voltage)

In [20]: currentString = str(current)

In [21]: voltageString
Out[21]: '2'

In [22]: type(voltageString)
Out[22]: <type 'str'>
``` 

As you might imagine, you can go the other way in certain cases. Lets say you had numerical data in a string.

```python
In [23]: resistanceString = "4.0"

In [24]: resistance = float(resistanceString)

In [25]: resistance
Out[25]: 4.0

In [26]: type(resistance)
Out[26]: <type 'float'>
```

What would happen if you tried to coerce resistanceString to an int? What about coercing resistance to an int? Consider the following:

```python
In [27] resistanceString = "4.0 ohms"
```

Do you think you can coerce that string to a numerical type?

## On Being Precise with floats and ints

Again, the following may seem esoteric and pedantic, but it is very important. So bear with me.

Lets say you had some voltage data that looks like the following

```
0
0.5
1
1.5
2
```

Obviously, if you just assigned this data individually to a variable, you'd end up with the following types

```
0   -> int
0.5 -> float
1   -> int
1.5 -> float
2   -> int
```

But what if you wanted all of that data to be floats on its way in? You could assign the variable and then coerce it to type float:

```python
In [28]: voltage = float(1)
```

But that's ugly. If you want whats otherwise an integer to be a float, just add a period at the end

```python
In [29]: voltage = 1.

In [30]: type(voltage)
Out[30]: <type 'float'>
```

This point becomes important when we start operating on data in the next section.

## Data Operations

In this section all of the discussion in the previous section becomes important. I don't know if I'd call this stuff fundamental to the language, but its pretty important and it will zing you if you aren't careful. The takeaway is that you need to be precise with what you are doing. Lets say you want to add some integers.

```python
In [31]: a = 1

In [32]: b = 2

In [33]: c = a+b

In [34]: c
Out[34]: 3

In [38]: type(a), type(b), type(c)
Out[38]: (<type 'int'>, <type 'int'>, <type 'int'>)
```

So we got a vale of three for the sum, which also happens to be an integer. Any operation between two integers is another integer. Makes sense.

So what about the case where a is an integer and b is a float?

```python
In [39]: a = 1

In [40]: b = 2.

In [41]: c = a + b

In [42]: c
Out[42]: 3.0

In [43]: type(a), type(b), type(c)
Out[43]: (<type 'int'>, <type 'float'>, <type 'float'>)
```

You can do multiplication on numbers as well.

```python
In [44]: a = 2

In [45]: b = 3

In [46]: c = a * b

In [47]: c
Out[47]: 6

In [48]: type(a), type(b), type(c)
Out[48]: (<type 'int'>, <type 'int'>, <type 'int'>)
```

Also division.

```python
In [49]: a = 1

In [50]: b = 2

In [51]: c = a / b

In [52]: c
Out[52]: 0
```

**ZING!**

Here's why type is important. Divding two integers returnes an integer: this operation calculates the quotient and floors the result to get the answer.

If everything was a float, the division is what you would expect.

```python
In [53]: a = 1.

In [54]: b = 2.

In [55]: c = a / b

In [56]: c
Out[56]: 0.5

In [57]: type(a), type(b), type(c)
Out[57]: (<type 'float'>, <type 'float'>, <type 'float'>)
```

There are operations that can be done with strings.

```python
In [58]: firstName = "Johann"

In [59]: lastName = "Gambolputty"

In [60]: fullName = firstName + lastName

In [61]: print fullName
JohannGambolputty
```

When concatenating strings, you have to be explicit since computers don't understand context.

```python
In [62]: fullName = firstName + " " + lastName

In [63]: print fullName
Johann Gambolputty
```

There are other operations deined on string data. Use the dir comnand to find them. One example I'll show is the upper method. Lets take a look at the documentation.

```python
In [64]: str.upper?
Type:           method_descriptor
Base Class:     <type 'method_descriptor'>
String Form:    <method 'upper' of 'str' objects>
Namespace:      Python builtin
Docstring:
    S.upper() -> string                                                                                                                        
    
    Return a copy of the string S converted to uppercase.
```

So we can use it to upper-caseify a string. 

```python
In [65]: fullName.upper()
Out[65]: 'JOHANN GAMBOLPUTTY'
```

You have to use the parenthesis at the end because upper is a method of the string class.

For what its worth, you don't need to have a variable to use the upper() method, you could use it on the string itself.

```python
In [66]: "Johann Gambolputty".upper()
Out[66]: 'JOHANN GAMBOLPUTTY'
```

What do you think should happen when you take upper of an int?  What about a string representation of an int?

That wraps up this lesson. We tried out the iPython shell and got some experience with ints, floats, and strings. Along the way we talked about some philosophy and how programming is about people.


The individual scripts we wrote on Day 1 of an R-flavored SWC Boot Camp, run in two instances: NESCent and Duke in May 2013.

Here the files are referred to w/o reflecting the creating of subdirectories, which happened on Day 2, and is the current state of the repo.

Instructor has done whatever is necessary to ensure that RStudio and R launches as "virginally" as possible. Suppress usage of custom `.Rprofile`. Ensure will launch with empty workspace and history. Ensure will launch with user's home directory as working directory.

block01: Students fired up RStudio. Basic exploration of the environment RStudio provides. Entered commands live in the R console. Discussed organizing a project, especially an R analytical project, and R's notions of workspace and working directory. Created an RStudio project to use for the remainder of the bootcamp. Sent commands from the History to the source editor. Eventually saved these as the stand-alone script `block01_toyExample.R`. Practiced grooming code and using RStudio's facilities for sending code from the source editor to the console. The R markdown file `block01_basicsWorkspaceWorkingDirProject.rmd` (in `code/`) creates the html file `block01_basicsWorkspaceWorkingDirProject.html`; that gives Jenny's "script" for block 1. 
  * inputs: none
  * code: `block01_preProject.R`, `block01_postProject.R`, `block01_toyExample.R`
  * outputs: `avgX.txt`, `niftyPlot.pdf`
  
block02: Basic care and feeding of the most common R objects. Special emphasis on `data.frames`. `read.table` and friends for import. Bit of figure-making with the `lattice` package. Using the `subset()` and `with()` functions and the `data=` and `subset=` arguments found in many functions to do computations _in situ_ with added bonus of readable code. How to access various bits of various R objects, i.e. indexing. Accurate transcript can be found in the script `block02_careFeedingData.R`, which is NOT meant to be run as a whole -- it's for interactive use.
  * inputs: `gapminderDataFiveYear.txt`
  * code: `block02_careFeedingData.R`
  * output: none
  
block03: Data aggregation = doing something repetitive for various logical bits of an R object. E.g. taking means of rows in a matrix, computing statistical summaries for variables in a `data.frame`, fitting a model to sub-`data.frames` induced by separating the Gapminder data out by country. Used the `apply` family of functions in base R and also introduced the add-on package `plyr`. Accurate transcript can be found in the script `block03_dataAggregation.R`, which is NOT meant to be run as a whole -- it's for interactive use.
  * inputs: `gapminderDataFiveYear.txt`
  * code: `block03_dataAggregation.R`
  * output: none

block04: Sort of a capstone "putting it all together" piece. Revisiting country specific linear models of life expectancy against year. Before writing those results to file for later use, reordering the continent factor rationally (based on rate of life expectancy gains) and dropping Oceania (too few countries). Purpose was to demonstrate typical hygiene for factor variables. Different ways to write rectangular data to file with various pros/cons: `write.table`, `dput`, `saveRDS`, which have natural relationships with `read.table`, `dget`, `readRDS`. Accurate transcript of live work can be found in the script `block04_puttingAllTogether.R`, which is NOT meant to be run as a whole -- it's for interactive use. We did package some of our work nicely as scripts that could be `knit` and/or `source`'d or put into a pipeline (see below).
  * `block04_puttingAllTogether.R`
    - inputs: `gapminderDataFiveYear.txt`
    - output: none
  * `01_countrySpecificInterceptSlope.R`
    - inputs: `gapminderDataFiveYear.txt`
    - output: `gCoef.txt`, `gCoef.rds`
  * `02_slopeComparisonAsiaVsAmericas.R` (we used this to demonstrate the super-lightweight dynamic report generation capability of RStudio: "File --> Compile notebook", also available as a button; later accomplished same from command line in a `Makefile`)
    - inputs: `gCoef.rds`
    - outputs (after compiling notebook): `02_slopeComparisonAsiaVsAmericas.html`
  * `03_slopeComparisonAsiaVsAmericas.R` (we used this to demonstrate how a stand-alone script could leave files behind for later use, such as a PDF and the results of a two-sample t-test; essentially equivalent to `02_slopeComparionsAsiaVsAmericas.R` but optimized for running in a hands-off way using old-school techniques, e.g. `sink()`)
      - inputs: `gCoef.rds`
      - outputs: `slopes_AsiaVsAmericas.pdf`, `02_slopeComparisonAsiaVsAmericas_fromSink.txt`
      
block99: Challenges issued for further work.


This repository gives the R content from Duke software Carpentry May 20 - 21 2013, which was very similar to the NESCent boot camp which ran May 17 - 18,2013.

Here are pages with set-up instructions, etc.:
  * [NESCent boot camp](http://swcarpentry.github.io/boot-camps/2013-05-16-nescent/)
  * [Duke boot camp](http://swcarpentry.github.io/boot-camps/2013-05-20-duke/)

Day 1 was essentially all R, using RStudio. Main instructor Jenny Bryan. At the end of the day, we had some input data (an excerpt from the Gapminder data), several R scripts, and various outputs that these scripts left behind, e.g. figures, numerical results, analytical reports. The first commit to this repo is a snapshot of where we are at the end of Day 1. The README.md associated with the `code` subdirectory explains what each file does.

Our goal was to introduce students to data analysis in R, so focus was not on programming *per se*. Coverage of writing functions, control structures, package development etc. is essentially non-existent. That was intentional. Had to do some visualization, which we did with `lattice`, since that's what Jenny knows. More graphics would be good -- but how to fit in? Also `ggplot2` is perhaps more the way of the future?

Day 2 was mostly led by Ben Morris and Elliot Hauser. They went over the shell and version control, in particular git and github. We kept operating on our R work from Day 1. Ben also presented `make` and we continued to use our R work to demonstrate its power.

First task: tidy up the day 1 stuff. We create subdirectories: data, code, figures, results, prose. Then we move files into their respective homes. The second commit to this repo is a snapshot right after this. This was an exercise for the students, i.e. an opportunity for them to use many of the shell commands they just learned.

The file re-organization then requires us to visit the code files and prepend subdirectories, so the scripts still work. Making these incremental changes and documenting why they are necessary is how we started to demonstrate the power of version control. By this point Jenny had put up a public github repo (this one) and students were cloning / pulling from it. We got to see how `git pull` would not work if some of the student's local changes would be destroyed. We did not get into merging (for the students), so students were advised to discard their local changes, if they wanted to pull from this repo again.

Jenny and Ben continued to make changes to the repo, e.g. adding README.md and LICENSE, and providing various live demonstrations of using git and github to collaborate on this project. You will see some commits that are silly but are just us demonstrating something. Jenny and Ben purposely edited the same file and committed, so we could show them how merging worked.

After Ben had covered `make`, we set about making final preparations to some of our R scripts so they could be used in an automated pipeline managed by `make`. There are several commits related to these preparations. There is one remaining "gremlin" in which one of our automated reports is not successfully including a figure. Fixing this is on Jenny's to do list, it's probably merely a path issue relating to where the figures are being stored and sought.

Overall, we were quite pleased with how this all worked together. Day 1 stood well by itself and then that material gave us a great opportunity to apply everything we were learning on Day 2: file and directory manipulation from the command line, version control and collaboration, and automating a workflow with `make`.

PS: Jenny showed a couple slides from her UBC courses with helpful visuals for various R concepts. Students requested those and they are in `prose/slides.pdf`.

This repository is no longer being updated.  Please use the 'bc' repo
at https://github.com/swcarpentry/bc as a starting point for new
bootcamp web pages.

boot-camps
==========
![SWC logo](http://software-carpentry.org/img/software-carpentry-banner.png)

Welcome!

# University of Chicago - January 12-13, 2013
Software Carpentry boot camp material
for the January 12-13, 2013 boot camp at the University of Chicago.

For a schedule and installation instructions check out the bootcamp webpage at
http://swcarpentry.github.com/boot-camps/2013-01-12-chicago.

Students
========

This directory contains scripts for testing your machine to make sure
you have the software you'll need for your boot camp installed.  See
the comments at the head of each script for more details, but you'll
basically want to see something like:

    $ python swc-installation-test-1.py
    Passed
    $ python swc-installation-test-2.py
    check virtual-shell...  pass
    â€¦
    Successes:

    virtual-shell Bourne Again Shell (bash) 4.2.37
    â€¦

If you see something like:

    $ python swc-installation-test-2.py
    check virtual-shell...  fail
    â€¦
    check for command line shell (virtual-shell) failed:
      command line shell (virtual-shell) requires at least one of the following dependencies
      For instructions on installing an up-to-date version, see
      http://software-carpentry.org/setup/
      causes:
      check for Bourne Again Shell (bash) failed:
        could not find 'bash' executable for Bourne Again Shell (bash)
        For instructions on installing an up-to-date version, see
        http://software-carpentry.org/setup/
    â€¦

follow the suggestions to try and install any missing software.  For
additional troubleshooting information, you can use the `--verbose`
option:

    $ python swc-installation-test-2.py --verbose
    check virtual-shell...  fail
    â€¦
    ==================
    System information
    ==================
    os.name            : posix
    â€¦

Instructors
===========

`swc-installation-test-1.py` is pretty simple, and just checks that
the students have a recent enough version of Python installed that
they'll be able to parse `swc-installation-test-2.py`.  The latter
checks for a list of dependencies and prints error messages if a
package is not installed, or if the installed version is not current
enough.  By default, the script checks for pretty much anything that
has ever been used at a Software Carpentry boot camp, which is
probably not what you want for your particular boot camp.

Before your boot camp, you should go through
`swc-installation-test-2.py` and comment any dependencies you don't
need out of the `CHECKS` list.  You might also want to skim through
the minimum version numbers listed where particular dependencies are
defined (e.g. `('git', 'Git', (1, 7, 0), None)`).  For the most part,
fairly conservative values have been selected, so students with modern
machines should be fine.  If your boot camp has stricter version
requirements, feel free to bump them accordingly.

Similarly, the virtual dependencies can be satisfied by any of several
packages.  If you don't want to support a particular package (e.g. if
you have no Emacs experience and don't want to be responsible for
students who show up with Emacs as their only editor), you can comment
out that particular `or_dependency`.

Finally, don't forget to post your modified scripts somewhere where
your students can download them!

# The Shell

**Material by Paul Wilson, Milad Fatenejad, Sasha Wood, and Radhika Khetani**

# What is the shell? How do I access the shell?

The *shell* is a program that presents a command line interface
which allows you to control your computer using commands entered
with a keyboard instead of controlling graphical user interfaces
(GUIs) with a mouse/keyboard combination.

Use a browser to open the tutorial on github, located at:

    https://github.com/swcarpentry/boot-camps/tree/YYYY-MM-PLACE

Click on the directory named `shell`.

A *terminal* is a program you run that gives you access to the
shell. There are many different terminal programs that vary across
operating systems.

There are many reasons to learn about the shell. In my opinion, the
most important reasons are that:

1.  It is very common to encounter the shell and
    command-line-interfaces in scientific computing, so you will
    probably have to learn it eventually

2.  The shell is a really powerful way of interacting with your
    computer. GUIs and the shell are complementary - by knowing both
    you will greatly expand the range of tasks you can accomplish with
    your computer. You will also be able to perform many tasks more
    efficiently.

The shell is just a program and there are many different shell
programs that have been developed. The most common shell (and the one
we will use) is called the Bourne-Again SHell (bash). Even if bash is
not the default shell, it is usually installed on most systems and can be
started by typing `bash` in the terminal. Many commands, especially a
lot of the basic ones, work across the various shells but many things
are different. I recommend sticking with bash and learning it well.
([Here is a link for more information](http://en.wikipedia.org/wiki/Bash_(Unix_shell))

To open a terminal, just single click on the "Terminal" icon on the
Desktop.

# The Example: Manipulating Experimental Data Files

We will spend most of our time learning about the basics of the shell
by manipulating some experimental data from a hearing test. To get
the data for this test, you will need internet access. Just enter the
command:

    git clone -b YYYY-MM-PLACE --single-branch git://github.com/swcarpentry/boot-camps.git

This command will grab all of the data needed for this workshop from
the internet.  (We will talk about the `git` command later in the
workshop.)

# Let's get started

One very basic command is `echo`. This command just prints text to
the terminal. Try the command:

    echo Hello, World

Then press enter. You should see the text "Hello, World" printed back
to you. The echo command is useful for printing from a shell script,
for displaying variables, and for generating known values to pass
to other programs.

## Moving around the file system

Let's learn how to move around the file system using command line
programs. This is really easy to do using a GUI (just click on
things). Once you learn the basic commands, you'll see that it is
really easy to do in the shell too.

First we have to know where we are. The program `pwd` (print working
directory) tells you where you are sitting in the directory tree. The
command `ls` will list the files in files in the current
directory. Directories are often called "folders" because of how they
are represented in GUIs. Directories are just listings of files. They
can contain other files or directories.

Whenever you start up a terminal, you will start in a special
directory called the *home* directory. Every user has their own home
directory where they have full access to do whatever they want. In
this case, the `pwd` command tells us that we are in the `/home/swc`
directory. This is the home directory for the `swc` user. That is our
user name. You can always find out your user name by entering the
command `whoami`.

## File Types

When you enter the `ls` command lists the contents of the current
directory. There are several items in the home directory, notice that
they are all colored blue. This tells us that all of these items are
directories as opposed to files.

Lets create an empty file using the `touch` command. Enter the
command:

    touch testfile

Then list the contents of the directory again. You should see that a
new entry, called `testfile`, exists. It is colored white meaning that
it is a file, as opposed to a directory. The `touch` command just
creates an empty file.

Some terminals will not color the directory entries in this very
convenient way. In those terminals, use `ls -F` instead of `ls`. The
`-F` argument modifies the results so that a slash is placed at the
end of directories. If the file is *executable* meaning that it can be
run like a program, then a star will be placed at the end of of the
file name.

You can also use the command `ls -l` to see whether items in a
directory are files or directories. `ls -l` gives a lot more
information too, such as the size of the file and information about
the owner. If the entry is a directory, then the first letter will be
a "d". The fifth column shows you the size of the entries in
bytes. Notice that `testfile` has a size of zero.

Now, let's get rid of `testfile`. To remove a file, just enter the
command:

    rm testfile

The `rm` command can be used to remove files. If you enter `ls` again,
you will see that `testfile` is gone.


## Changing Directories

Now, let's move to a different directory. The command `cd` (change
directory) is used to move around. Let's move into the `boot-camps`
directory. Enter the following command:

    cd boot-camps

Use the `ls` command to see what is inside this directory.  This
directory contains all of the material for this boot camp. Now move to
the directory containing the data for the shell tutorial:

    cd shell

Now use the `ls` command to see what is inside this directory. You
will see that there is an entry which is green. This means that this
is an executable. If you use `ls -F` you will see that this file ends
with a star.

If you enter the `cd` command by itself, you will return to the home
directory. Try this, and then navigate back to the `shell`
directory.

## Arguments

Most programs take additional arguments that control their exact
behavior. For example, `-F` and `-l` are arguments to `ls`.  The `ls`
program, like many programs, take a lot of arguments. But how do we
know what the options are to particular commands?

Most commonly used shell programs have a manual. You can access the
manual using the `man` program. Try entering:

    man ls

This will open the manual page for `ls`. Use the space key to go
forward and b to go backwards. When you are done reading, just hit `q`
to quit.

Programs that are run from the shell can get extremely complicated. To
see an example, open up the manual page for the `find` program,
which we will use later this session. No one can possibly learn all of
these arguments, of course. So you will probably find yourself
referring back to the manual page frequently.

* * * *
**Short Exercise**

1. Use the manual page for `ls` to guess what you would expect from
using the arguments `-l`, '-t', '-r' at the same time.
2. Try the following and see if you can figure out what they do, either by examining the results or consulting the manual page.
   * `ls -lS` (equivalent to `ls -l -S`)
   * `ls -lt` (equivalent to `ls -l -t`)
   * `ls -1`  (that's the number one, not a letter 'ell')

* * * *


## Examining the contents of other directories

By default, the `ls` commands lists the contents of the working
directory (i.e. the directory you are in). You can always find the
directory you are in using the `pwd` command. However, you can also
give `ls` the names of other directories to view. Navigate to the
home directory if you are not already there. Then enter the
command:

    ls boot-camps

This will list the contents of the `boot-camps` directory without
you having to navigate there. Now enter:

    ls boot-camps/shell

This prints the contents of `shell`. The `cd` command works in a
similar way. Try entering:

    cd boot-camps/shell

and you will jump directly to `shell` without having to go through
the intermediate directory.

## Full vs. Relative Paths

The `cd` command takes an argument which is the directory
name. Directories can be specified using either a *relative* path a
full *path*. The directories on the computer are arranged into a
hierarchy. The full path tells you where a directory is in that
hierarchy. Navigate to the home directory. Now, enter the `pwd`
command and you should see:

    /home/swc

which is the full name of your home directory. This tells you that you
are in a directory called `swc`, which sits inside a directory called
`home` which sits inside the very top directory in the hierarchy. The
very top of the hierarchy is a directory called `/` which is usually
referred to as the *root directory*. So, to summarize: `swc` is a
directory in `home` which is a directory in `/`.

Now enter the following command:

    cd /home/swc/boot-camps/shell

This jumps to `shell`. Now go back to the home directory. We saw
earlier that the command:

    cd boot-camps/shell

had the same effect - it took us to the `shell` directory. But,
instead of specifying the full path
(`/home/swc/boot-camps/shell`), we specified a *relative path*. In
other words, we specified the path relative to our current
directory. A full path always starts with a `/`. A relative path does
not. You can usually use either a full path or a relative path
depending on what is most convenient. If we are in the home directory,
it is more convenient to just enter the relative path since it
involves less typing.

Over time, it will become easier for you to keep a mental note of the
structure of the directories that you are using hand how to quickly
navigate amongst them.

* * * *
**Short Exercise**

Now, list the contents of the /bin directory. Do you see anything
familiar in there?

* * * *

## Saving time with shortcuts, wild cards, and tab completion

### Shortcuts

There are some shortcuts which you should know about. Dealing with the
home directory is very common. So, in the shell the tilde character,
`~`, is a shortcut for your home directory. Navigate to the `shell`
directory, then enter the command:

    ls ~

This prints the contents of your home directory, without you having to
type the full path. The shortcut `..` always refers to the directory
above your current directory. Thus:

    ls ..

prints the contents of the `/home/swc/boot-camps`. You can chain
these together, so:

    ls ../../

prints the contents of `/home/swc` which is your home
directory. Finally, the special directory `.` always refers to your
current directory. So, `ls`, `ls .`, and `ls ././././.` all do the
same thing, they print the contents of the current directory. This may
seem like a useless shortcut right now, but we'll see when it is
needed in a little while.

To summarize, while you are in the `shell` directory, the commands
`ls ~`, `ls ~/.`, `ls ../../`, and `ls /home/swc` all do exactly the
same thing. These shortcuts are not necessary, they are provided for
your convenience.

### Our data set: Cochlear Implants

A cochlear implant is a small electronic device that is surgically
implanted in the inner ear to give deaf people a sense of
hearing. More than a quarter of a million people have them, but there
is still no widely-accepted benchmark to measure their effectiveness.
In order to establish a baseline for such a benchmark, our supervisor
got teenagers with CIs to listen to audio files on their computer and
report:

1.  the quietest sound they could hear
2.  the lowest and highest tones they could hear
3.  the narrowest range of frequencies they could discriminate

To participate, subjects attended our laboratory and one of our lab
techs played an audio sample, and recorded their data - when they
first heard the sound, or first heard a difference in the sound.  Each
set of test results were written out to a text file, one set per file.
Each participant has a unique subject ID, and a made-up subject name.
Each experiment has a unique experiment ID. The experiment has
collected 351 files so far.

The data is a bit of a mess! There are inconsistent file names, there
are extraneous "NOTES" files that we'd like to get rid of, and the
data is spread across many directories. We are going to use shell
commands to get this data into shape. By the end we would like to:

1.  Put all of the data into one directory called "alldata"

2.  Have all of the data files in there, and ensure that every file
    has a ".txt" extension

3.  Get rid of the extraneous "NOTES" files

If we can get through this example in the available time, we will move
onto more advanced shell topics...

### Wild cards

Navigate to the `~/boot-camps/shell/data/THOMAS` directory. This
directory contains our hearing test data for THOMAS. If we type `ls`,
we will see that there are a bunch of files which are just four digit
numbers. By default, `ls` lists all of the files in a given
directory. The `*` character is a shortcut for "everything". Thus, if
you enter `ls *`, you will see all of the contents of a given
directory. Now try this command:

    ls *1

This lists every file that ends with a `1`. This command:

    ls /usr/bin/*.sh

Lists every file in `/usr/bin` that ends in the characters `.sh`. And
this command:

    ls *4*1

lists every file in the current directory whose name contains the
number `4`, and ends with the number `1`. There are four such files:
`0241`, `0341`, `0431`, and `0481`.

So how does this actually work? Well...when the shell (bash) sees a
word that contains the `*` character, it automatically looks for filenames
that match the given pattern. In this case, it identified four such
files. Then, it replaced the `*4*1` with the list of files, separated
by spaces. In other words, the two commands:

    ls *4*1
    ls 0241 0341 0431 0481

are exactly identical. The `ls` command cannot tell the difference
between these two things.

* * * *
**Short Exercise**

Do each of the following using a single `ls` command without
navigating to a different directory.

1.  List all of the files in `/bin` that contain the letter `a`
2.  List all of the files in `/bin` that contain the letter `a` or the letter `b`
3.  List all of the files in `/bin` that contain the letter `a` AND the letter `b`

* * * *

### Tab Completion

Navigate to the home directory. Typing out directory names can waste a
lot of time. When you start typing out the name of a directory, then
hit the tab key, the shell will try to fill in the rest of the
directory name. For example, enter:

    cd b<tab>

The shell will fill in the rest of the directory name for
`boot-camps`. Now enter:

    ls s<tab><tab>

When you hit the first tab, nothing happens. The reason is that there
are multiple directories in the home directory which start with
`s`. Thus, the shell does not know which one to fill in. When you hit
tab again, the shell will list the possible choices.

Tab completion can also fill in the names of programs. For example,
enter `e<tab><tab>`. You will see the name of every program that
starts with an `e`. One of those is `echo`. If you enter `ec<tab>` you
will see that tab completion works.

## Command History

You can easily access previous commands.  Hit the up arrow.
Hit it again.  You can step backwards through your command history.
The down arrow takes your forwards in the command history.

^-C will cancel the command you are writing, and give you a fresh prompt.

^-R will do a reverse-search through your command history.  This
is very useful.

You can also review your recent commands with the `history` command.  Just enter:

    history

to see a numbered list of recent commands, including this just issues
`history` command.  You can reuse one of these commands directly by
referring to the number of that command.

If your history looked like this:

    259  ls *!
    260  ls /usr/bin/*.sh
    261  ls *4*1

then you could repeat command #260 by simply entering:

    !260

(that's an exclamation mark).

* * * *
**Short Exercise**

1. Find the line number in your history for the last exercise (listing
files in /bin) and reissue that command.

* * * *

## Which program?

Commands like `ls`, `rm`, `echo`, and `cd` are just ordinary programs
on the computer. A program is just a file that you can *execute*. The
program `which` tells you the location of a particular program. For
example:

    which ls

Will return "/bin/ls". Thus, we can see that `ls` is a program that
sits inside of the `/bin` directory. Now enter:

    which find

You will see that `find` is a program that sits inside of the
`/usr/bin` directory.

So ... when we enter a program name, like `ls`, and hit enter, how
does the shell know where to look for that program? How does it know
to run `/bin/ls` when we enter `ls`. The answer is that when we enter
a program name and hit enter, there are a few standard places that the
shell automatically looks. If it can't find the program in any of
those places, it will print an error saying "command not found". Enter
the command:

    echo $PATH

This will print out the value of the `PATH` environment variable. More
on environment variables later. Notice that a list of directories,
separated by colon characters, is listed. These are the places the
shell looks for programs to run. If your program is not in this list,
then an error is printed. The shell ONLY checks in the places listed
in the `PATH` environment variable.

Navigate to the `shell` directory and list the contents. You will
notice that there is a program (executable file) called `hello` in
this directory. Now, try to run the program by entering:

    hello

You should get an error saying that hello cannot be found. That is
because the directory `/home/swc/boot-camps/shell` is not in the
`PATH`. You can run the `hello` program by entering:

    ./hello

Remember that `.` is a shortcut for the current working
directory. This tells the shell to run the `hello` program which is
located right here. So, you can run any program by entering the path
to that program. You can run `hello` equally well by specifying:

    /home/swc/boot-camps/shell/hello

Or by entering:

    ../shell/hello

When there are no `/` characters, the shell assumes you want to look
in one of the default places for the program.


## Examining Files

We now know how to switch directories, run programs, and look at the
contents of directories, but how do we look at the contents of files?

The easiest way to examine a file is to just print out all of the
contents using the program `cat`. Enter the following command:

    cat ex_data.txt

This prints out the contents of the `ex_data.txt` file. If you enter:

    cat ex_data.txt ex_data.txt

It will print out the contents of `ex_data.txt` twice. `cat` just
takes a list of file names and writes them out one after another (this
is where the name comes from, `cat` is short for concatenate).

* * * *
**Short Exercises**

1.  Print out the contents of the `~/boot-camps/shell/dictionary.txt`
    file. What does this file contain?

2.  Without changing directories, (you should still be in `shell`),
    use one short command to print the contents of all of the files in
    the `/home/swc/boot-camps/shell/data/THOMAS` directory.

* * * *

`cat` is a terrific program, but when the file is really big, it can
be annoying to use. The program, `less`, is useful for this
case. Enter the following command:

    less ~/boot-camps/shell/dictionary.txt

`less` opens the file, and lets you navigate through it. The commands
are identical to the `man` program.

**Some commands in `less`**

| key     | action |
| ------- | ---------- |
| "space" | to go forward |
|  "b"    | to go backwards |
|  "g"    | to go to the beginning |
|  "G"    | to go to the end |
|  "q"    | to quit |

`less` also gives you a way of searching through files. Just hit the
"/" key to begin a search. Enter the name of the word you would like
to search for and hit enter. It will jump to the next location where
that word is found. Try searching the `dictionary.txt` file for the
word "cat". If you hit "/" then "enter", `less` will just repeat
the previous search. `less` searches from the current location and
works its way forward. If you are at the end of the file and search
for the word "cat", `less` will not find it. You need to go to the
beginning of the file and search.

Remember, the `man` program actually uses `less` internally and
therefore uses the same commands, so you can search documentation
using "/" as well!

* * * *
**Short Exercise**

Use the commands we've learned so far to figure out how to search
in reverse while using `less`.

* * * *


## Redirection

Let's turn to the experimental data from the hearing tests that we
began with. This data is located in the `~/boot-camps/shell/data`
directory. Each subdirectory corresponds to a particular participant
in the study. Navigate to the `Bert` subdirectory in `data`.  There
are a bunch of text files which contain experimental data
results. Lets print them all:

    cat au*

Now enter the following command:

    cat au* > ../all_data

This tells the shell to take the output from the `cat au*` command and
dump it into a new file called `../all_data`. To verify that this
worked, examine the `all_data` file. If `all_data` had already
existed, we would overwritten it. So the `>` character tells the shell
to take the output from what ever is on the left and dump it into the
file on the right. The `>>` characters do almost the same thing,
except that they will append the output to the file if it already
exists.

* * * *
**Short Exercise**

Use `>>`, to append the contents of all of the files whose names
contain the number 4 in the directory:

    /home/swc/boot-camps/shell/data/gerdal

to the existing `all_data` file. Thus, when you are done `all_data`
should contain all of the experiment data from Bert and any
experimental data file from gerdal with filenames that contain the
number 4.

* * * *


## Creating, moving, copying, and removing

We've created a file called `all_data` using the redirection operator
`>`. This file is critical - it's our analysis results - so we want to
make copies so that the data is backed up.
Lets copy the file using the `cp` command. The `cp`
command backs up the file. Navigate to the `data` directory and enter:

    cp all_data all_data_backup

Now `all_data_backup` has been created as a copy of `all_data`. We can
move files around using the command `mv`. Enter this command:

    mv all_data_backup /tmp/

This moves `all_data_backup` into the directory `/tmp`. The directory
`/tmp` is a special directory that all users can write to. It is a
temporary place for storing files. Data stored in `/tmp` is
automatically deleted when the computer shuts down.

The `mv` command is also how you rename files. Since this file is so
important, let's rename it:

    mv all_data all_data_IMPORTANT

Now the file name has been changed to all_data_IMPORTANT. Let's delete
the backup file now:

    rm /tmp/all_data_backup

The `mkdir` command is used to make a directory. Just enter `mkdir`
followed by a space, then the directory name.

* * * *
**Short Exercise**

Do the following:

1.  Rename the `all_data_IMPORTANT` file to `all_data`.
2.  Create a directory in the `data` directory called `foo`
3.  Then, copy the `all_data` file into `foo`

* * * *

By default, `rm`, will NOT delete directories. You can tell `rm` to
delete a directory using the `-r` option. Enter the following command:

    rm -r foo


## Count the words

The `wc` program (word count) counts the number of lines, words, and
characters in one or more files. Make sure you are in the `data`
directory, then enter the following command:

    wc Bert/* gerdal/*4*

For each of the files indicated, `wc` has printed a line with three
numbers. The first is the number of lines in that file. The second is
the number of words. Finally, the total number of characters is
indicated. The final line contains this information summed over all of
the files. Thus, there were 10445 characters in total.

Remember that the `Bert/*` and `gerdal/*4*` files were merged
into the `all_data` file. So, we should see that `all_data` contains
the same number of characters:

    wc all_data

Every character in the file takes up one byte of disk space. Thus, the
size of the file in bytes should also be 10445. Let's confirm this:

    ls -l all_data

Remember that `ls -l` prints out detailed information about a file and
that the fifth column is the size of the file in bytes.

* * * *
**Short Exercise**

Suppose you are running low on hard drive space.  Examine the man 
page and figure out how to get `ls` to sort the directory listing 
by size, putting the largest file last. 

* * * *

## The awesome power of the Pipe

Suppose I wanted to only see the total number of character, words, and
lines across the files `Bert/*` and `gerdal/*4*`. I don't want to
see the individual counts, just the total. Of course, I could just do:

    wc all_data

Since this file is a concatenation of the smaller files. Sure, this
works, but I had to create the `all_data` file to do this. Thus, I
have wasted a precious 10445 bytes of hard disk space. We can do this
*without* creating a temporary file, but first I have to show you two
more commands: `head` and `tail`. These commands print the first few,
or last few, lines of a file, respectively. Try them out on
`all_data`:

    head all_data
    tail all_data

The `-n` option to either of these commands can be used to print the
first or last `n` lines of a file. To print the first/last line of the
file use:

    head -n 1 all_data
    tail -n 1 all_data

Let's turn back to the problem of printing only the total number of
lines in a set of files without creating any temporary files. To do
this, we want to tell the shell to take the output of the `wc Bert/*
gerdal/*4*` and send it into the `tail -n 1` command. The `|`
character (called pipe) is used for this purpose. Enter the following
command:

    wc Bert/* gerdal/Data0559 | tail -n 1

This will print only the total number of lines, characters, and words
across all of these files. What is happening here? Well, `tail`, like
many command line programs will read from the *standard input* when it
is not given any files to operate on. In this case, it will just sit
there waiting for input. That input can come from the user's keyboard
*or from another program*. Try this:

    tail -n 2

Notice that your cursor just sits there blinking. Tail is waiting for
data to come in. Now type:

    French
    fries
    are
    good

then CONTROL+d. You should see the lines:

    are
    good

printed back at you. The CONTROL+d keyboard shortcut inserts an
*end-of-file* character. It is sort of the standard way of telling the
program "I'm done entering data". The `|` character is replaces the
data from the keyboard with data from another command. You can string
all sorts of commands together using the pipe.

The philosophy behind these command line programs is that none of them
really do anything all that impressive. BUT when you start chaining
them together, you can do some really powerful things really
efficiently. If you want to be proficient at using the shell, you must
learn to become proficient with the pipe and redirection operators:
`|`, `>`, `>>`.


### A sorting example

Let's create a file with some words to sort for the next example. We
want to create a file which contains the following names:

    Bob
    Alice
    Diane
    Charles

To do this, we need a program which allows us to create text
files. There are many such programs, the easiest one which is
installed on almost all systems is called `nano`. Navigate to `/tmp`
and enter the following command:

    nano toBeSorted

Now enter the four names as shown above. When you are done, press
CONTROL+O to write out the file. Press enter to use the file name
`toBeSorted`. Then press CONTROL+x to exit `nano`.

When you are back to the command line, enter the command:

    sort toBeSorted

Notice that the names are now printed in alphabetical order.

* * * *
**Short Exercise**

Use the `echo` command and the append operator, `>>`, to append your
name to the file, then sort it and make a new file called Sorted.

* * * *

Let's navigate back to `~/boot-camps/shell/data`. Enter the following command:

    wc Bert/* | sort -k 3 -n

We are already familiar with what the first of these two commands
does: it creates a list containing the number of characters, words,
and lines in each file in the `Bert` directory. This list is then
piped into the `sort` command, so that it can be sorted. Notice there
are two options given to sort:

1.  `-k 3`: Sort based on the third column
2.  `-n`: Sort in numerical order as opposed to alphabetical order

Notice that the files are sorted by the number of characters.

* * * *
**Short Exercise**

1. Use the `man` command to find out how to sort the output from `wc` in
reverse order.

2. Combine the `wc`, `sort`, `head` and `tail` commands so that only the
`wc` information for the largest file is listed

Hint: To print the smallest file, use:

    wc Bert/* | sort -k 3 -n | head -n 1

# Writing a shell script

Printing the smallest file seems pretty useful. We don't want to type
out that long command often. Let's create a simple script, a simple
program, to run this command. The program will look at all of the
files in the current directory and print the information about the
smallest one. Let's call the script `smallest`. We'll use `nano` to
create this file. Navigate to the `data` directory, then:

    nano smallest

Then enter the following text:

    #!/bin/bash
    wc * | sort -k 3 -n | head -n 1

Now, `cd` into the `Bert` directory and enter the command
`../smallest`. Notice that it says permission denied. This happens
because we haven't told the shell that this is an executable
file. If you do `ls -l ../smallest`, it will show you the permissions on
the left of the listing.

Enter the following commands:

    chmod a+x ../smallest
    ../smallest

The `chmod` command is used to modify the permissions of a file. This
particular command modifies the file `../smallest` by giving all users
(notice the `a`) permission to execute (notice the `x`) the file. If
you enter:

    ls -l ../smallest

You will see that the file name is green and the permissions have changed.
Congratulations, you just created your first shell script!

# Searching files

You can search the contents of a file using the command `grep`. The
`grep` program is very powerful and useful especially when combined
with other commands by using the pipe. Navigate to the `Bert`
directory. Every data file in this directory has a line which says
"Range". The range represents the smallest frequency range that can be
discriminated. Lets list all of the ranges from the tests that Bert
conducted:

    grep Range *

* * * *
**Short Exercise**

Create an executable script called `smallestrange` in the `data`
directory, that is similar to the `smallest` script, but prints the
file containing the file with the smallest Range. Use the commands
`grep`, `sort`, and `tail` to do this.

* * * *


# Finding files

The `find` program can be used to find files based on arbitrary
criteria. Navigate to the `data` directory and enter the following
command:

    find . -print

This prints the name of every file or directory, recursively, starting
from the current directory. Let's exclude all of the directories:

    find . -type f -print

This tells `find` to locate only files. Now try these commands:

    find . -type f -name "*1*"
    find . -type f -name "*1*" -or -name "*2*" -print
    find . -type f -name "*1*" -and -name "*2*" -print

The `find` command can acquire a list of files and perform some
operation on each file. Try this command out:

    find . -type f -exec grep Volume {} \;

This command finds every file starting from `.`. Then it searches each
file for a line which contains the word "Volume". The `{}` refers to
the name of each file. The trailing `\;` is used to terminate the
command.  This command is slow, because it is calling a new instance
of `grep` for each item the `find` returns.

A faster way to do this is to use the `xargs` command:

    find . -type f -print | xargs grep Volume

`find` generates a list of all the files we are interested in,
then we pipe them to `xargs`.  `xargs` takes the items given to it
and passes them as arguments to `grep`.  `xargs` generally only creates
a single instance of `grep` (or whatever program it is running).

* * * *
**Short Exercise**

Navigate to the `data` directory. Use one `find` command to perform each
of the operations listed below (except number 2, which does not
require a `find` command):

1.  Find any file whose name is "NOTES" within `data` and delete it

2.  Create a new directory called `cleaneddata`

3.  Move all of the files within `data` to the `cleaneddata` directory

4.  Rename all of the files to ensure that they end in `.txt` (note:
    it is ok for the file name to end in `.txt.txt`

Hint: If you make a mistake and need to start over just do the
following:

1.  Navigate to the `shell` directory

2.  Delete the `data` directory

3.  Enter the command: `git checkout -- data` You should see that the
    data directory has reappeared in its original state

**BONUS**

Redo exercise 4, except rename only the files which do not already end
in `.txt`. You will have to use the `man` command to figure out how to
search for files which do not match a certain name.

* * * *



## Bonus:

**backtick, xargs**: Example find all files with certain text

**alias** -> rm -i

**variables** -> use a path example

**.bashrc**

**du**

**ln**

**ssh and scp**

**Regular Expressions**

**Permissions**

**Chaining commands together**

# Basic Shell Commands
***

## 1. Shell Basics:

| Command        | Definition                                                                                                     |
|----------------|----------------------------------------------------------------------------------------------------------------|  
| `.`            | a single period refers to the current directory                                                                |  
| `..`           | a double period refers to the directory immediately above the current directory                                |  
| `~`            | refers to your home directory. _Note:_ this command does NOT work on Windows machines (Mac and Linux are okay) |  
| `cd ./dirname` | changes the current directory to the directory `dirname`                                                       |  
| `ls -F`        | tells you what files and directories are in the current directory                                              |  
|  `pwd`         | tells you what directory you are in (`pwd` stands for *p*rint *w*orking *d*irectory)                           |  
|  `history`     | lists previous commands you have entered. `history | less` lets you page through the list.                     |  
|  `man` *cmd*   | displays the *man*ual page for a command.                                                                      |  



## 2. Creating Things:
### a) How to create new files and directories..

| Command           | Definition                                                                                                                                                                                                                                                                                                                                            |  
|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|  
| `mkdir ./dirname` | makes a new directory called dirname below the current directory. _Note:_ Windows users will need to use `\` instead of `/` for the path separator                                                                                                                                                                                                    |  
| `nano filename`   | if `filename` does not exist, `nano` creates it and opens the `nano` text editor. If the file exists, `nano` opens it. _Note:_ _(i)_ You can use a different text editor if you like.  In gnome Linux, `gedit` works really well too. _(ii)_ `nano` (or `gedit`) create text files. It doesn't matter what the file extension is (or if there is one) |  

### b) How to delete files and directories...
#### _Remember that deleting is forever. There is NO going back_

| Command           | Definition                                                                                                       |  
|-------------------|------------------------------------------------------------------------------------------------------------------|
| `rm ./filename`   | deletes a file called `filename` from the current directory                                                      |  
| `rmdir ./dirname` |  deletes the directory `dirname` from the current directory. _Note:_ `dirname` must be empty for `rmdir` to run. |  

### c) How to copy and rename files and directories...

| Command | Definition                                                                                                                                                                                                                    |  
|---------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|  
| `mv tmp/filename .` | moves the file `filename` from the directory `tmp` to the current directory. _Note:_ _(i)_ the original `filename` in `tmp` is deleted. _(ii)_ `mv` can also be used to rename files (e.g., `mv filename newname` |  
| `cp tmp/filename .` | copies the file `filename` from the directory `tmp` to the current directory. _Note:_ _(i)_ the original file is still there                                                                                      |  



## 3. Pipes and Filters
### a) How to use wildcards to match filenames...
Wildcards are a shell feature that makes the command line much more powerful than any GUI file managers. 
Wildcards are particularly useful when you are looking for directories, files, or file content that can
vary along a given dimension.  These wildcards can be used with any command that accepts file names or 
text strings as arguments.

#### Table of commonly used wildcards 

| Wildcard               | Matches                                        |  
|------------------------|------------------------------------------------|  
| `*`                    | zero or more characters                        |  
| `?`                    | exactly one character                          |  
| `[abcde]`              | exactly one of the characters listed           |  
| `[a-e]`                | exactly one character in the given range       |  
| `[!abcde]`             | any character not listed                       |  
| `[!a-e]`               | any character that is not in the given range   |  
| `{software,carpentry}` | exactly one entire word from the options given |  

See the cheatsheet on regular expressions for more "wildcard" shortcuts.

### b) How to redirect to a file and get input from a file ...
Redirection operators can be used to redirect the output from a program from the display screen to a file where it is saved (or many other places too, like your printer or to another program where it can be used as input).


| Command | Description                                                                                                                     |  
|---------|---------------------------------------------------------------------------------------------------------------------------------|  
| `>`     | write `stdout` to a new file; overwrites any file with that name (e.g., `ls *.md > mardkownfiles.txt`)                          |  
| `>>`    | append `stdout` to a previously existing file; if the file does not exist, it is created (e.g., `ls *.md >> markdownfiles.txt`) |  
| `<`     | assigns the information in a file to a variable, loop, etc (e.g., `n < markdownfiles.md`)                                       | 



#### b.1) How to use the output of one command as the input to another with a pipe...
A special kind of redirection is called a pipe and is denoted by `|`. 


| Command | Description                                                                                                                                           |  
|---------|-------------------------------------------------------------------------------------------------------------------------------------------------------|  
| &#124;     | Output from one command line program can be used as input to another one (e.g. ls \*.md &#124; head gives you the first 5 `*.md` files in your directory) |  





##### Example:   

    ls *.md | head | sed -i `s/markdown/software/g`
   
changes all the instances of the word `markdown` to `software` in the first 5 `*.md` files in your current directory.


 

## 4. How to repeat operations using a loop...
Loops assign a value in a list or counter to a variable that takes on a different value each time through the loop.
There are 2 primary kinds of loops: `for` loops and `while` loops.

### a) For loop
For loops loop through variables in a list


    for varname in list
    do
        command1 $varname
        command2 $varname
    done

where,

*  `for`, `in`, `do`, and `done` are keywords
*  `list` contains a list of values separated by spaces. e.g. `list` can be replaced by `1 2 3 4 5 6` or by `Bob Mary Sue Greg`. `list` can also be a variable:
*  `varname` is assigned a value without using a `$` and the value is retrieved using `$varname`

--

    list[0]=Sam
    list[1]=Lynne
    list[2]=Dhavide
    list[3]=Trevor
    .
    .
    .
    list[n]=Mark
    
which is referenced in the loop by:

    for varname in ${list[@]}
    do
        command1 $varname
        command2 $varname
    done


_Note:_ Bash is zero indexed, so counting always starts at `0`, not `1`.
    

### b) While Loop
While loops loop through the commands until a condition is met. For example
    
    COUNTER=0
    while [ ${COUNTER} -lt 10 ]; do
        command 1
        command 2
        COUNTER=`expr ${COUNTER} + 1` 
    done

continues the loop as long as the value in the variable COUNTER is less than 10 (incremented by 1 on each iteration of the loop).

*  `while`, `do`, and `done` are keywords


#### b.1) Commonly used conditional operators

| Operator | Definition               |  
|----------|--------------------------|  
| `-eq`    | is equal to              |  
| `-ne`    | is not equal to          |  
| `-gt`    | greater than             |
| `-ge`    | greater than or equal to |
| `-lt`    | less than                |
| `-le`    | less than or equal to    |

Use `man bash` or `man test` to learn about other operators you can use.



## 6. Finding Things
### a) How to select lines matching patterns in text files...
To find information within files, you use a command called `grep`.

| Example command                | Description                                                                                    |
|--------------------------------|------------------------------------------------------------------------------------------------|
| `grep [options] day haiku.txt` | finds every instance of the string `day` in the file haiku.txt and pipes it to standard output |                                                                                                                                                                                                                |  

#### a.1) Commonly used `grep` options

|      | `grep` options                                                                                                                                                                                                       |  
|------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|  
| `-E` | tells grep you will be using a regular expression. Enclose the regular expression in quotes. _Note:_ the power of `grep` comes from using regular expressions. Please see the regular expressions sheet for examples |  
| `-i` | makes matching case-insensitive                                                                                                                                                                                      |  
| `-n` | limits the number of lines that match to the first n matches                                                                                                                                                         |   
| `-v` | shows lines that do not match the pattern (inverts the match)                                                                                                                                                        |  
| `-w` | outputs instances where the pattern is a whole word                                                                                                                                                                  |  

### b) How to find files with certain properties...
To find file and directory names, you use a command called `find`

| Example command  | Description |  
|------------------|-------------|
| `find . -type d` | `find` recursively descends the directory tree for each path listed to match the expression given in the command line with file or directory names in the search path |  


#### b.1) Commonly used `find` options

|               | `find` options                                                                                                                                          |  
|---------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|  
| `-type [df]`  | `d` lists directories; `f` lists files                                                                                                                  |  
| `-maxdepth n` | `find` automatically searches subdirectories. If you don't want that, specify the number of levels below the working directory you would like to search |
| `-mindepth n` | starts `find`'s search `n` levels below the working directory                                                                                           |



SQL Cheat Sheet
===============

Basic queries
-------------

Select one or more columns of data from a table:

    SELECT column_name_1, column_name_2 FROM table_name;

Select all of the columns in a table:

    SELECT * FROM table_name;

Get only unique lines in a query:

    SELECT DISTINCT column_name FROM table_name;

Perform calculations in a query:

    SELECT column_name_1, ROUND(column_name_2 / 1000.0) FROM table_name;


Filtering
---------

Select only the data meeting certain criteria:

    SELECT * FROM table_name WHERE column_name = 'Hello World';

Combine conditions:

    SELECT * FROM table_name WHERE (column_name_1 >= 1000) AND (column_name_2 = 'A' OR column_name_2 = 'B');


Sorting
-------

Sort results using `ASC` for ascending order or `DESC` for descending order:

    SELECT * FROM table_name ORDER BY column_name_1 ASC, column_name_2 DESC;


Aggregation
-----------

Combine data into groups and calculate combined values in groups:

    SELECT column_name_1, SUM(column_name_2), COUNT(*) FROM table_name GROUP BY column_name_1;


Joins
-----

Join data from two tables:

    SELECT * FROM table_name_1 JOIN table_name_2 ON table_name_1.column_name = table_name_2.column_name;


Combining commands
------------------

SQL commands can be combined in the following order:
`SELECT`, `FROM`, `JOIN`, `ON`, `WHERE`, `GROUP BY`, `ORDER BY`.

SQL Outline
===========

* Basic queries - selecting data from a table and performing basic calculations
* Filtering - selecting only the data that matches a set of conditions
* Sorting - ordering results
* Aggregation - grouping the output of queries and performing calculations within groups
* Joins - combing data from multiple tables
* Basic database design - how and why to split data into multiple tables
Testing Outline
===============

* Testing: how to save time and write better code
* Exceptions: how to handle errors and detect bugs
* Unit tests: how to isolate components of your code and make sure they work
* Floating-point numbers: how to test results that have limited precision

This repository is no longer being updated.  Please use the 'bc' repo
at https://github.com/swcarpentry/bc as a starting point for new
bootcamp web pages.

# git/GitHub

The goal of this lesson is to introduce the students to [git][] via
collaboration on [GitHub][].

## Introduction

- Say some introductory stuff about version control in general, and git/GitHub
  in particular.

*Note: The figures in the [Visual Git Reference][visual git] can be a good
stand-in if you have nowhere to draw.*

## Setup and Signup

- Have everyone configure git:

        $ git config --global user.name "User Name"
        $ git config --global user.email "user@email.com"
        $ git config --global core.editor "nano"
        $ git config --global color.ui "auto"

- Give a little tour of [GitHub][].
- Have everyone make [GitHub][] accounts.

### Make and Clone

- Make a new demo repo on [GitHub][] explaining the process as you go
  (probably on your personal account).
    - Have [GitHub][] put in a README so it can be cloned.
- Explain that much like a browser navigates to websites using a URL, git talks
  to remote repositories using a URL.
- Explain the different URL options:
    - Read/write `ssh` protocol requires [ssh keys][], which make it so you
      don't have to enter username/password.
    - `https` protocol takes username/password.
    - `git` protocol is read-only.
- Now we want to get a copy of this down on all of our computers -- `git clone`!
    - Have everyone do this via the `https` URL.
- `ls` to show the new directory and `cd` into it.
- Compare the local listing to what you see on [GitHub][]. Same for now, but
  not for long!

## Basics

### Local Basics

**IMPORTANT:** Make sure you tell people *not* to make their own local changes,
that will make things really complicated later when people pull. Alternatively,
you can go ahead and let them do whatever they like and use it as a teaching
moment on `git reset --hard` in a few minutes when it's time to start the
collaboration.

- On the white board draw a box representing the working area and explain that
  this is where you work and make changes.
- Make a new file called `top-things-to-learn.md` and put the title
  "Top Things We Want to Learn" in it.
- `git status` -- highlight the "Untracked files" section and that git tells
  you how to add a file to the next commit.

### Composing the Snapshot

- On the white board draw a box representing the staging area (index) and
  explain that this is where we set up the next snapshot of our project.
    - Like a photographer in a studio, we're putting together a shot before
      we actually snap the picture.
    - Connect the working area box and the staging box with `git add`.
- Run `git add top-things-to-learn.md`.
- `git status` -- highlight the "Changes to be committed" section
  and git telling you how to unstage the new file.

### Taking the Snapshot

- On the white board draw a box representing the project history. Once we take
  a snapshot of the project that snapshot becomes a permanent reference point
  in the project's history that we can always go back to.
    - The history is like a photo album of changes, and each snapshot has a
      time stamp, the name of the photographer, and a description.
    - Connect the staging area to the history with `git commit`.
- Run `git commit` and explain log messages.
    - Summary message at the top, longer one below.
- `git status` -- nothing going on!

### Looking at the History

- `git log` -- Shows description of what we've done.
    - `git log --oneline` -- Abbreviated version.
- Explain the commit hash.
    - Unique identifier of that commit if we ever want to refer to it.
    - Comes from "hashing" stuff associated with the commit, like the changes
      we made.
    - Can demo hashing with Python's `hashlib.sha1`.

### Previewing Changes

- The file we're making is going to be a list of the top things everyone wants
  to learn in the boot camp. Add your item (e.g. everyone's names) and save.
- `git status` -- point out that now we have a modified file instead of an
  untracked file, but the procedure for adding it to the next snapshot is
  the same.
- Want to see the changes you're about to add? `git diff`!
- `git add`
- `git diff` -- now it doesn't show anything. `git diff` shows differences
  between the working area and the staging area.
    - To see differences between the staging area and the most recent commit
      use `git diff --cached`.
- `git commit -m` -- This time use the `-m` option and show that for short
  commit messages you can just enter them at the command line.

### Undoing Changes

- Say I want to redo the previous commit...
- `git log --oneline` -- grab the commit has for the point we want to go back to.
- `git reset COMMIT`
- `git log --oneline` -- highlight that the latest commit is gone
- `git status` -- But the changes haven't gone anywhere.
- I can now edit the file to fix whatever was wrong and re-commit.

## Sharing

- Now I want to share my changes with everyone so they can start working on
  it too.

### Remotes

- As we said back at the beginning, git uses URLs to point repositories on other
  computers, in this case [GitHub's][GitHub] servers.
- We can give these remote repositories names so that we don't have to type
  in the full URL all the time, and in fact git has already set one up for us.
- `git remote` -- there's a remote called "origin".
- `git remote -v` -- we can see that it points to the same URL we cloned from,
  git sets this up automatically.

### Branches

- On the [GitHub][] view of the repo highlight the branches pull-down -- right
  now it only has one branch called "master", this is another thing git makes
  for us by default.
- What branch are we on locally? `git branch`.
- Give a short explanation of branches and mention that we will come back to
  them later.
    - Isolated development environments.
- When git communicates with a remote repository it needs to know what branch
  is changing, in this case we'll just stick with "master".

### Pushing

- Use `push` command to send data to a remote repository, and we also have to
  specify the remote name and branch name: `git push origin master`.
- Refresh the [GitHub][] view.

### Pulling

**IMPORTANT:** If students have been making local commits, this is the time at
which they will need to use `git reset --hard` to get back in sync with you.

- `pull` is the reciprocal command, must specify remote and branch.
- Have everyone `git pull origin master`.

### Collaborate

- Pick a student to add their top thing to learn to the list:
    - Add them to the collaborator list on the demo repo.
    - edit, save, `add`, `commit`, `push`
- Have everyone `pull`.

### Rebase

#### No Conflict

- Have another student add their thing and push.
- Make a change to the README file before pulling.
- Try to push.
- On the white board draw the situation: my repo and the remote repo have
  different development histories and git doesn't know how to pull things
  together.
- It would be nice if I could move my local change after the remote change.
  (Draw picture.) There's a command for that!
- `git fetch origin` -- This gets information from the remote repository
  but doesn't integrate it with your local repo like `pull` does.
- `git rebase origin/master` -- `origin/master` is how we specify the fetched
  data for the remote named "origin" and it's branch named "master".
    - This replays my local changes on top of the state of the remote repo.
- `git log --oneline` -- Now my change is after the remote change.
- `git push origin master`
- Have everyone pull.

#### With Conflicts

- Again, have a student add their thing and push.
- Before pulling make a change in the same place in the same file.
- Try to rebase as above.
- Explain the conflict message git prints out.
- Show the conflict messages in the file and how to clean it up.
- Continue the rebase and push the result.
- Have everyone pull.

## Developing in Branches

Often you want to leave a stable version of your code alone while you make some
potentially disruptive changes. Or you and someone else are working on the code
and you want to be able to work without worrying what others are doing.

- It's going to take a long time to get everyone's top thing to learn onto the
  list one at a time, so the room is going to break out into groups and each
  come up with their own list.
- So that they can all do this and then push their result to [GitHub][] each
  is going to work in their own, isolated branch.

### Making a New Branch

*Note: The [Learn Git Branching][] app can be a useful way to
illustrate this part of the lesson.*

- Make a new branch: `git branch matt-list`.
- `git branch` -- highlight the asterisk showing the branch we're currently on.
- Move to that branch: `git checkout matt-list`.
- `git branch` -- asterisk moved!
- Make a change and push.
    - **IMPORTANT:** have to specify new branch named when pushing, not "master".
- `git checkout master` -- show that your change is *not* in master.
- Show how to browse to the other branch on [GitHub][].
- Have each group pick a unique branch name, switch to that branch, and add
  all of their top things to learn to the list and push the result.

### Resolving the Changes

- Browse all of the new branches on [GitHub][].
- Illustrate the situation on the [Learn Git Branching][] app.
- Could just pick one of these branches as the new one "master" and move on,
  but we're more adventurous than that.
- Make sure you're in "master".
- `git fetch origin` -- without a branch name it grabs all of the new branches.
- Pick a branch and `git merge branch-name`.
    - Should be a smooth fast-forward.
    - Illustrate on the [Learn Git Branching][] app.
- Pick another branch and try to merge.
    - Resolve conflicts, add, and commit.
    - Illustrate on the [Learn Git Branching][] app.
- Repeat as necessary.
- Push the final result to [GitHub][].

[git]: http://git-scm.com/
[GitHub]: http://github.com
[ssh keys]: https://help.github.com/articles/generating-ssh-keys
[visual git]: http://marklodato.github.com/visual-git-guide/index-en.html
[Learn Git Branching]: http://pcottle.github.com/learnGitBranching/?NODEMO

# Local Version Control
----

**Based on materials by Katy Huff, Anthony Scopatz, Joshua R. Smith, and Sri 
Hari Krishna Narayanan**

## Example: A Slide Deck for a Presentation

Imagine you have two computers: the first is a big, powerful desktop machine 
with a big, dual monitor setup. The second is a tiny lightweight netbook you 
take with you when you travel. Imagine also that you have a presentation at a 
conference overseas. You know that no matter how "finished" you think you are 
with your slide deck, inevitably you will realize you need to add an extra figure 
or recalculate some data AFTER leaving the country and your main desktop 
workstation. How can you be sure your slide deck, data, and the programs you've 
written are synchronized between your two machines? How can you be sure that 
every little change you've made on your netbook is reflected on your desktop? 
What happens if you start making changes on your slide deck, only to realize you 
need to drop back to the slide deck you had five days ago?

## git : What is Version Control ?

Very briefly, version control is a way to *keep a backup of changing
files*, to *store a history of those changes*, and most importantly to
*allow many people to make changes* to the same files
concurrently. There are a lot of version control systems. Wikipedia
provides both a nice vocabulary list and a fairly complete table of some
popular version control systems and their equivalent commands.

What problems does version control solve?
* undo mistakes by rolling back to earlier versions
* run and test with older versions for debugging (when DID it break?)
* allows you to keep and switch between multiple verisons of code
* automatic merging of edits by different people
* can distrubute/publish analysis code and workflows
* archive or back up your thesis so when your laptop goes away, your thesis doesn't

Today, we'll be using git. Git is an example of a distributed version
control system, distinct from centralized version control systems. I'll
make the distinction clear later, but for now, the table below will
suffice.

Version Control System Tool Options

- **Distributed** 
  - Decentralized CVS (dcvs)  
  - mercurial (hg)
  - git (git) 
  - bazaar (bzr)
- **Centralized**
  - concurrent versions system (cvs)
  - subversion (svn)

## git clone : we've seen git already
Yesterday morning, after installing git, we asked everyone to run
```
git clone http://github.com/swcarpentry/boot-camps -b YYYY-MM-PLACE --single-branch
```
This created a copy of the software carpentry repository materials on
each of your hard drives yesterday morning.   If you did this yesterday,
you don't need to to it again.

*But*, last night, the instructors changed the content on github, so now
the repositories on all our hard drives are out of date.

```
cd
cd boot-camps
git pull
```
will try to retrieve all of last night's changes and update your local
copies.  Note: git commands work only when executed from within the directory
that contains the repository.

## git --help : Getting Help

The first thing I like to know about any tool is how to get help. From
the command line type

    $ man git

The manual entry for the git version control system will appear before
you. You may scroll through it using arrows, or you can search for
keywords by typing **/** followed by the search term. I'm interested in
help, so I type **/help** and then hit enter. It looks like the syntax
for getting help with git is **git --help**.

To exit the manual page, type q.

Let's see what happens when we type :

    $ git --help

Excellent, it gives a list of commands it is able to help with, as well
as their descriptions.

    $ git --help
    usage: git [--version] [--exec-path[=<path>]] [--html-path]
               [-p|--paginate|--no-pager] [--no-replace-objects]
               [--bare] [--git-dir=<path>] [--work-tree=<path>]
               [-c name=value] [--help]
               <command> [<args>]

    The most commonly used git commands are:
       add        Add file contents to the index
       bisect     Find by binary search the change that introduced a bug
       branch     List, create, or delete branches
       checkout   Checkout a branch or paths to the working tree
       clone      Clone a repository into a new directory
       commit     Record changes to the repository
       diff       Show changes between commits, commit and working tree, etc
       fetch      Download objects and refs from another repository
       grep       Print lines matching a pattern
       init       Create an empty git repository or reinitialize an existing one
       log        Show commit logs
       merge      Join two or more development histories together
       mv         Move or rename a file, a directory, or a symlink
       pull       Fetch from and merge with another repository or a local branch
       push       Update remote refs along with associated objects
       rebase     Forward-port local commits to the updated upstream head
       reset      Reset current HEAD to the specified state
       rm         Remove files from the working tree and from the index
       show       Show various types of objects
       status     Show the working tree status
       tag        Create, list, delete or verify a tag object signed with GPG

    See 'git help <command>' for more information on a specific command.

## git config : Controls the behavior of git
Since we're going to be making commits, we need to perform 
a one-time per computer configuation of git so git knows 
who to credit for our contributions to the version control 
system.

     $ git config --global user.name "YOUR NAME"
     $ git config --global user.email "YOUR EMAIL"
     
## git init : Creating a Local Repository

To keep track of numerous versions of your work without saving numerous
copies, you can make a local repository for it on your computer. What git
does is to save the first version, then for each subsequent version it
saves only the changes. That is, git only records the difference between
the new version and the one before it. With this compact information,
git is able to recreate any version on demand by adding the changes to
the original in order up to the version of interest.

To create your own local (on your own machine) repository, you must
initialize the repository with the infrastructure git needs in order to
keep a record of things within the repository that you're concerned
about. The command to do this is **git init** .

### Exercise : Create a Local Repository

Step 1 : Initialize your repository.

    $ cd
    $ mkdir good_science
    $ cd good_science
    $ git init
    Initialized empty Git repository in /home/swc/good_science/.git/

Step 2 : Browse the directory's hidden files to see what happened here.
Open directories, browse file contents. Learn what you can in a minute.

    $ ls -A
    .git
    $ cd .git
    $ ls -A
    HEAD        config      description hooks       info        objects     refs 

Step 3 : Use what you've learned. You may have noticed the file called
description. You can describe your repository by opening the description
file and replacing the text with a name for the repository. Mine will be
called "Reproducible Science". You may call yours anything you like.

    $ nano description &

Step 4 : Applications sometimes create files that are not needed. For
example, kate creates a temporary file called 'filename~' when you edit
the file 'filename', and latex produces .aux and .dvi files that you 
may not need to keep in version control. You can ask git to ignore such files by editing
the file '.git/info/exclude'. Edit the file to ignore files the end with '~'.

     git ls-files --others --exclude-from=.git/info/exclude
    # Lines that start with '#' are comments.
    # For a project mostly in C, the following would be a good set of
    # exclude patterns (uncomment them if you want to use them):
    # *.[oa]
    # *~
    
## git add : Adding a File To Version Control

For the git repository to know which files within this directory you
would like to keep track of, you must add them. First, you'll need to
create one, then we'll learn the **git add** command.

### Exercise : Add a File to Your Local Repository

Step 1 : Create a file to add to your repository.

    $ nano readme.rst &

Step 2 : Inform git that you would like to keep track of future changes
in this file.

    $ git add readme.rst

## git status : Checking the Status of Your Local Copy

The files you've created on your machine are your local "working" copy.
The changes your make in this local copy aren't backed up online
automatically. Until you commit them, the changes you make are local
changes. When you change anything, your set of files becomes different
from the files in the official repository copy. To find out what's
different about them in the terminal, try:

    $ git status
    # On branch master
    #
    # Initial commit
    #
    # Changes to be committed:

    #   (use "git rm --cached <file>..." to unstage)
    #
    #       new file:   readme.rst
    #

The null result means that you're up to date with the current version of
the repository online. This result indicates that the current difference
between the repository HEAD (which, so far, is empty) and your
good\_science directory is this new readme.rst file.

## git commit : Saving a Snapshot

In order to save a snapshot of the current state (revision) of the
repository, we use the commit command. This command is always associated
with a message describing the changes since the last commit and
indicating their purpose. Informative commit messages will serve you
well someday, so make a habit of never committing changes without at
least a full sentence description.

**ADVICE: Commit often**

In the same way that it is wise to often save a document that you are
working on, so too is it wise to save numerous revisions of your code.
More frequent commits increase the granularity of your **undo** button.

**ADVICE: Write good commit messages**

There are no hard and fast rules, but good commits are atomic: 
they are the smallest change that remain meaningful. A good 
commit message usually contains a one-line description followed 
by a longer explanation if necessary.  Remember, you're writing 
commit messages for yourself as much as for anyone else.  

[Our repo](https://github.com/swcarpentry/boot-camps/commits/YYYY-MM-PLACE) has some good commit messages.

### Exercise : Commit Your Changes

Step 1 : Commit the file you've added to your repository.

    $ git commit -am "This is the first commit. It adds a readme file."
    [master (root-commit) 1863aef] This is the first commit. It adds a readme file.
     1 files changed, 2 insertions(+), 0 deletions(-)
     create mode 100644 readme.rst

Step 2 : Admire your work.

    $ git status
    # On branch master
    nothing to commit (working directory clean)

## git diff : Viewing the Differences

There are many diff tools.

If you have a favorite you can set your default git diff tool to execute
that one. Git, however, comes with its own diff system.

Let's recall the behavior of the diff command on the command line.
Choosing two files that are similar, the command :

    $ diff file1 file2

will output the lines that differ between the two files. This
information can be saved as what's known as a patch, but we won't go
deeply into that just now.

The only difference between the command line diff tool and git's diff
tool is that the git tool is aware of all of the revisions in your
repository, allowing each revision of each file to be treated as a full
file.

Thus, git diff will output the changes in your working directory that
are not yet staged for a commit. To see how this works, make a change in
your readme.rst file, but don't yet commit it.

    $ git diff

A summarized version of this output can be output with the --stat flag :

    $ git diff --stat

To see only the differences in a certain path, try:

    $ git diff HEAD -- [path]

To see what IS staged for commit (that is, what will be committed if you
type git commit without the -a flag), you can try :

    $ git diff --cached

## git log : Viewing the History

A log of the commit messages is kept by the repository and can be
reviewed with the log command.

    $ git log
    commit 1863aefd7db752f58226264e5f4282bda641ddb3
    Author: Joshua Smith <joshua.r.smith@gmail.com>
    Date:   Wed Feb 8 16:08:08 2012 -0600

        This is the first commit. It adds a readme file.

There are some useful flags for this command, such as

    -p
    -3
    --stat
    --oneline
    --graph
    --pretty=short/full/fuller/oneline
    --since=X.minutes/hours/days/weeks/months/years or YY-MM-DD-HH:MM
    --until=X.minutes/hours/days/weeks/months/years or YY-MM-DD-HH:MM
    --author=<pattern>

## git reset : Unstaging a staged file
    git reset filename     (opposite of 'git add filename')

## git checkout : Discarding unstaged modifications (git checkout has other purposes)
    git checkout -- filename     
    
## git rm : Removing a file
   git rm filename   (Removes a file from the repository)
   
### Exercise : 
    1) Create 5 files in your directory with one line of content in each file.
    2) Commit the files to the repository.
    3) Change 2 of the 5 files and commit them.
    4) Undo the changes in step 3)
    5) Print out the last entry in the log.
    
## git branch : Listing, Creating, and Deleting Branches

Branches are pointers to a version of a repository that can be edited and
version controlled in parallel. They are useful for pursuing various
implementations experimentally or maintaining a stable core while
developing separate sections of a code base.

Without an argument, the **branch** command lists the branches that
exist in your repository.

    $ git branch
    * master

The master branch is created when the repository is initialized. With an
argument, the **branch** command creates a new branch with the given
name.

    $ git branch experimental
    $ git branch
    * master
      experimental

To delete a branch, use the **-d** flag.

    $ git branch -d experimental
    $ git branch
    * master

## git checkout : Switching Between Branches, Abandoning Local Changes

The **git checkout** command allows context switching between branches
as well as abandoning local changes.

To switch between branches, try

    $ git branch newbranch 
    $ git checkout newbranch 
    $ git branch

How can you tell we've switched between branches? When we used the
branch command before there was an asterisk next to the master branch.
That's because the asterisk indicates which branch you're currently in.

## git merge : Merging Branches

At some point, the experimental branch may be ready to become part of
the core or two testing branches may be ready to be combined for further
integration testing. The method for combining the changes in two
parallel branches is the **merge** command.

### Exercise : Create and Merge Branches

Step 1 : Create two new branches and list them

    $ git branch first
    $ git branch second

Step 2 : Make changes in each new branch and commit them.

    $ git checkout first
    Switched to branch 'first'
    $ touch firstnewfile
    $ git add firstnewfile
    $ git commit -am "Added firstnewfile to the first branch."
    [first 68eba44] Added firstnewfile to first branch.
     0 files changed, 0 insertions(+), 0 deletions(-)
     create mode 100644 firstnewfile
    $ git checkout second
    Switched to branch 'second'
    $ touch secondnewfile
    $ git add secondnewfile
    $ git commit -am "Added secondnewfile to the second branch."
    [second 45dd34c] Added secondnewfile to the second branch.
     0 files changed, 0 insertions(+), 0 deletions(-)
     create mode 100644 secondnewfile

Step 3 : Merge the two branches into the core

    $ git checkout first
    Switched to branch 'first'
    $ git merge second
    Merge made by recursive.
     0 files changed, 0 insertions(+), 0 deletions(-)
      create mode 100644 secondnewfile
    $ git checkout master
    Switched to branch 'master'
    $ git merge first
    Updating 1863aef..ce7e4b5
    Fast-forward
     0 files changed, 0 insertions(+), 0 deletions(-)
     create mode 100644 firstnewfile
     create mode 100644 secondnewfile

## git clone : Copying a Repository

Yesterday, you checked out a git type repository at
https://github.com/swcarpentry/boot-camps/tree/YYYY-MM-PLACE

When you clone the Original repository, the one that is created on your
local machine is a copy, and contains both the contents and the history.
With the right configuration, you can share your changes with your
collaborators and import changes that others made in their versions.  You
can also update the Original repository with your changes.

We'll get to that soon, but for now, let's move on to a fairly
easy-to-use system for managing repositories.

### Exercise : Cloning a Repository from GitHub

Step 1 : Pick any repository you like. There are many cool projects
hosted on github. Take a few minutes here, and pick a piece of code.

Step 2 : Clone it. If you didn't find anything cool, you can chose the
"instructional" Spoon-Knife repository:
    
    $ cd    
    $ git clone git@github.com/octocat/Spoon-Knife.git
    Cloning into Spoon-Knife...
    remote: Counting objects: 24, done.
    remote: Compressing objects: 100% (21/21), done.
    remote: Total 24 (delta 7), reused 17 (delta 1)
    Receiving objects: 100% (24/24), 74.36 KiB, done.
    Resolving deltas: 100% (7/7), done.

Step 3 : You should see many files download themselves onto your
machine. Let's make sure it worked. Change directories to the source
code and list the contents.

    $ cd Spoon-Knife
    $ ls 

## git pull : Pulling updates from the Original Repository

Updating your repository is like voting. You should update early and
often especially if you intend to contribute back to the upstream
repository and particularly before you make or commit any changes. This
will ensure you're working with the most up-to-date version of the
repository. Updating won't overwrite any changes you've made locally
without asking, so don't get nervous. When in doubt, update.

    $ git pull 
    Already up-to-date.

Since we just pulled the repository down, we will be up to date unless
there has been a commit by someone else to the Original repository in
the meantime.

## Resources

[git book](http://git-scm.com/book)

Notes on teaching git remote with toy (occasion-built) repositories.

Aron Ahmadia created a toy repository for 2013-06-tufts and
walked the audience through forking, committing, and pushing 
to the twenty forks of the toy repository.

I'm not a fan of Fork and Spoon; if we could use the boot-camps
repository for examples more this would be great.

I tried the same approach at 2013-06-chicago and 2013-07-notredame
and asked for pull requests to update the upstream repository
with content.  I suggested that there might be a universe in 
which students submit their homework by pull request.

By the end of the class at Notre Dame, most of the class had 
sent pull requests, but it looks like only a few updated their
forks with all the changes.

Requirements:
Instructor sets up a toy repository with placeholder content.

Teaching assistant has access to the toy repository and can approve
pull requests (after the instructor approves the first one or two)
so that the instructor's toy repository gets commits from most
of the class.

Hint:  It is helpful to have a TA give the instructor push access to 
the TA's fork of the repository.  This allows the instructor to do 
exactly as the students -- updating a repository (the TA's)  that is 
NOT upstream.

origin    https://github.com/TA/testrepo-YYYY-MM-PLACE
upstream  https://github.com/INSTRUCTOR/testrepo-YYYY-MM-PLACE

the placeholders USERNAME and TESTREPO should be replaced by 
the instructor's username and test repository name.

We used 
https://github.com/ahmadia/bio-pipeline
https://github.com/wltrimbl/testrepo-2013-06-chicago
https://github.com/wltrimbl/testrepo-2013-07-notredame

W. Trimble 2013-07-31


Version Control

----

**Original material by Katy Huff, Anthony Scopatz, and Sri Hari Krishna
Narayanan**

## github.com?

GitHub is a site where many people store their open (and closed) source
code repositories. It provides tools for browsing, collaborating on and
documenting code. Your home institution may have a repository hosting
system of it's own. To find out, ask your system administrator.  GitHub,
much like other forge hosting services (
[launchpad](https://launchpad.net), [bitbucket](https://bitbucket.org),
[googlecode](http://code.google.com), [sourceforge](http://sourceforge.net)
etc.) provides :

-   landing page support 
-   wiki support
-   network graphs and time histories of commits
-   code browser with syntax highlighting
-   issue (ticket) tracking
-   user downloads
-   varying permissions for various groups of users
-   commit triggered mailing lists
-   other service hooks (twitter, etc.)

## github login 

Setting up github requires a github user name and password.  Please take a
moment to [create a free github account](https://github.com/signup/free) (if you
want to start paying, you can add that to your account some other day).

## git remote : Steps for Forking a Repository

A key step to interacting with an online repository that you have forked
is adding the original as a remote repository. By adding the remote
repository, you inform git of a new option for fetching updates and
pushing commits.

The **git remote** command allows you to add, name, rename, list, and
delete repositories such as the original one **upstream** from your
fork, others that may be **parallel** to your fork, and so on.

### Exercise : Fork Our GitHub Repository

In step 1, you will make a copy "fork" of our test repository TESTREPO on github.  This gives you a copy of this repository that you control.

In step 2, you will make a copy of **your** fork of the repository on your hard drive.

In step 3, you will let git know that in addition to your local copy and your fork on github, there is another github repository (called "upstream") that you might want to get updates from.

Step 1 : Go to our
[repository](https://github.com/USERNAME/TESTREPO)
from your browser, and click on the Fork button. Choose to fork it to your
username rather than any organizations.

Step 2 : Clone it. From your terminal :

    $ git clone https://github.com/YOU/TESTREPO.git
    $ cd TESTREPO
Note: YOU is a placeholder for YOUR github username.  If git asks you for a password here, it probably means there isn't any repository a the url that you provided.

Step 3 : 

    $ git remote add upstream https://github.com/USERNAME/boot-camps.git
    $ git remote -v
    origin  https://github.com/YOU/TESTREPO.git (fetch)
    origin  https://github.com/YOU/TESTREPO.git (push)
    upstream        https://github.com/USERNAME/boot-camps.git (fetch)
    upstream        https://github.com/USERNAME/boot-camps.git (push)
    $

All repositories that are clones begin with a remote called origin.
### What's going on here?
The **git remote add** merely defines a nickname and a location that git will be able to communicate with for making copies of your repository.  "origin" and "upstream" are nicknames for your fork of TESTREPO and the "original" TESTREPO, respectively.

## git fetch : Fetching the contents of a remote

Now that you have alerted your repository to the presence of others, it
is able to pull in updates from those repositories. In this case, if you
want your master branch to track updates in the original TESTREPO
repository, you simply **git fetch** that repository into the master
branch of your current repository.

The fetch command alone merely pulls down information recent changes
from the original master (upstream) repository. By itself, the fetch
command does not change your local working copy. To update your local
working copy to include recent changes in the original (upstream)
repository, it is necessary to also merge.

## git merge : Merging the contents of a remote

To incorporate upstream changes from the original master repository (in
this case USERNAME/TESTREPO) into your local working copy, you
must both fetch and merge. The process of merging may result in
conflicts, so pay attention. This is where version control is both at
its most powerful and its most complicated.

### Exercise : Fetch and Merge the Contents of Our GitHub Repository

Step 1 : Fetch the recent remote repository history

    $ git fetch upstream

Step 2 : Make certain you are in the master branch and merge the
upstream master branch into your master branch

    $ git checkout master
    $ git merge upstream/master

Step 3 : Check out what happened by browsing the directory.

## git pull : Pull = Fetch + Merge

The command **git pull** is the same as executing **git fetch** followed
by **git merge**. Though it is not recommened for cases in which there
are many branches to consider, the pull command is shorter and simpler
than fetching and merging as it automates the branch matching.
Specificially, to perform the same task as we did in the previous
exercise, the pull command would be :

    $ git pull upstream
    Already up-to-date.

When there have been remote changes, the pull will apply those changes
to your local branch, unless there are conflicts with your local
changes.

## git push : Sending Your Commits to Remote Repositories

The **git push** command pushes commits in a local working copy to a
remote repository. The syntax is git push [remote] [local branch].
Before pushing, a developer should always pull (or fetch + merge), so
that there is an opportunity to resolve conflicts before pushing to the
remote.

### Exercise : Push a change to github
We'll talk about conflicts later, but first, let's make a small change
that won't have any conflicts and send our changes to
your fork, the "origin."

1. Create a file in the `messages` directory whose filename is your github
id.  (This is to ensure no conflicts just yet!)  Add a line of text, perhaps a
description of how you use / how you expect to use programming in your
work.

2.  commit your change with `git add <USERNAME>` and `git commit -m "commit message"`

3.  Update your fork ("origin") with your new changes:
    $ git push origin master

This will update your github fork with any changes you've committed.
Once you do this, you can see your changes on the github web interface
to your repository, along with the time you made the change and
your commit message.

If you have permission to push to the upstream repository, sending
commits to that remote is exactly analagous.

    $ git push upstream master

In the case of the upstream push, new developer accounts will not allow
this push to succeed. You're welcome to try it though.

There is now a hierarchy of git repositories.  There was the upstream
repository that you can't write to, there is your fork of that repository
that you have updated, and there is the local copy on your hard drive.

## github pull requests 

One protocol for updating repositories that we use at software carpentry
is the "pull request."   This is a bundle of updates to the repository
that can be accepted and merged into the upstream repository or rejected
and not merged.  If you would like to share your changes with the
upstream repository, click the green "compare and review" button, and
github will show you a summary of your commits.  If you then
click on "Click to create a pull request for this comparison," your
request will be sent to the upstream repository for acceptance or
rejection.

## git merge : Conflicts

This is the trickiest part of version control, so let's take it very
carefully.

Conflicts happen when git tries to combine changes from two different
branches (local and remote, development and master) but finds that 
changes in the two branches interfere with each other and can't be
automatically merged.

Branches are a tool that git uses to facilitate managing changes.  
They allow us to switch between states of the repository and refer to
states that we desire to merge.  

You'll often want to start a new branch for development, make your changes there,
and then merge these changes into your main branch. It's a good convention
to think of your master branch as
the "production branch," typically by keeping that branch clean of your
local edits until they are ready for release. Developers typically use the
master branch of their local fork to track other developers' changes in the
remote repository until their own local development branch changes are
ready for production.

Note, your fork contains a file called Readme.md.  This is a
standard documentation file that appears rendered on the landing page
for the repository in github. To see the rendered version, visit your
fork on github, (https://github.com/YOU/TESTREPO/README.md).  We
can edit this file on two different (local) branches and cause a merge
conflict, and if we try to update with remote changes that conflict with
our local changes we will also trigger a merge conflict.


### Exercise : Experience a Conflict

Step 1 : Make a new branch, edit the readme file in that branch, and
commit your changes.  Let's change the greeting "Vanakkam" to some other
greeting.

    $ git branch development
    $ git checkout development
    Switched to branch 'development'
    $ kate Readme.md &
    <edit the readme file and exit kate>
    $ git commit -am "Changed the welcome message to ... "

Step 2 : Mirror the remote upstream repository in your master branch 
by pulling down my changes

    $ git checkout master
    Switched to branch 'master'
    $ git fetch upstream
    $ git merge upstream/master
    Updating 43844ea..3b36a87
    Fast-forward
     README.rst |   2 +-
     1 files changed, 1 insertions(+), 1 deletions(-)

Step 3 : You want to push it to the internet eventually, so you pull
updates from the upstream repository, but will experience a conflict.

    $ git merge development
    Auto-merging Readme.md
    CONFLICT (content): Merge conflict in Readme.md
    Automatic merge failed; fix conflicts and then commit the result.

## git resolve : Resolving Conflicts

Now what?

Git has paused the merge. You can see this with the **git status**
command.

    # On branch master
    # Unmerged paths:
    #   (use "git add/rm <file>..." as appropriate to mark resolution)
    #
    #       unmerged:      Readme.md
    #
    no changes added to commit (use "git add" and/or "git commit -a")

The only thing that has changed is the Readme.md file. Opening it,
you'll see something like this at the beginning of the file.

    =====================
    <<<<<<< HEAD
    Vanakkam
    =======
    Willkommen
    >>>>>>> development
    =====================

The intent is for you to edit the file, knowing now that I wanted the
Welcome to say Vanakkam. If you want it to say Willkommen, you should
delete the other lines. However, if you want to be inclusive, you may
want to change it to read Vanakkam and Willkommen. Decisions such as this
one must be made by a human, and why conflict resolution is not handled
more automatically by the version control system.

    Vanakkam and Willkommen

This results in a status To alert git that you have made appropriate
alterations,

    $ git add Readme.md
    $ git commit
    Merge branch 'development'

    Conflicts:
      Readme.md
    #
    # It looks like you may be committing a MERGE.
    # If this is not correct, please remove the file
    # .git/MERGE_HEAD
    # and try again.
    #
    $ git push origin master
    Counting objects: 10, done.
    Delta compression using up to 2 threads.
    Compressing objects: 100% (6/6), done.
    Writing objects: 100% (6/6), 762 bytes, done.
    Total 6 (delta 2), reused 0 (delta 0)
    To git@github.com:username/TESTREPO.git

## synchronizing 
Now that lots of us created files and put in pull requests,
we begin to suspect that the upstream repository might have
new content and we are out of date. Try
    $ git pull upstream master
to fetch, merge, and commit the changes from upstream repository--
including other people's changes that have been added to upstream.
In this way we can all get updates of what the rest of us are 
working on.

But now our forks -- on github -- are out of date.  We can push
to update those
    $ git push origin master
And all is synchronized.

## gitolite

[Gitolite](https://github.com/sitaramc/gitolite) is a way for you to host
your own multi-user git repositories. I'm not going to go into details
here, but all you need is a machine with some drive space and network
access. You can install [minimal
ubuntu](https://help.ubuntu.com/community/Installation/MinimalCD), then
sudo apt-get install gitolite will pull in everything you need. At that
point, your collaborators will only need to send you their public ssh keys
for you to configure pull and push access to the repos.

