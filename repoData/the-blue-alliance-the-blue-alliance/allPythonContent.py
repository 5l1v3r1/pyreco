__FILENAME__ = admin_main
#!/usr/bin/env python
import os
import webapp2

import tba_config

from controllers.admin.admin_event_controller import AdminEventAddAllianceSelections, AdminEventAddTeams, AdminEventAddWebcast, AdminEventCreate, AdminEventCreateTest, AdminEventDelete, AdminEventDetail, AdminEventEdit, AdminEventList
from controllers.admin.admin_main_controller import AdminDebugHandler, AdminMain, AdminTasksHandler
from controllers.admin.admin_award_controller import AdminAwardDashboard, AdminAwardEdit, AdminAwardAdd
from controllers.admin.admin_match_controller import AdminVideosAdd, AdminMatchCleanup, AdminMatchDashboard, AdminMatchDelete, AdminMatchDetail, AdminMatchAdd, AdminMatchEdit
from controllers.admin.admin_media_controller import AdminMediaDashboard, AdminMediaAdd

from controllers.admin.admin_memcache_controller import AdminMemcacheMain
from controllers.admin.admin_offseason_scraper_controller import AdminOffseasonScraperController
from controllers.admin.admin_sitevar_controller import AdminSitevarCreate, AdminSitevarEdit, AdminSitevarList
from controllers.admin.suggestions.admin_event_webcast_suggestions_review_controller import AdminEventWebcastSuggestionsReviewController
from controllers.admin.suggestions.admin_match_video_suggestions_review_controller import AdminMatchVideoSuggestionsReviewController
from controllers.admin.suggestions.admin_media_suggestions_review_controller import AdminMediaSuggestionsReviewController
from controllers.admin.admin_team_controller import AdminTeamDetail, AdminTeamList
from controllers.admin.admin_migration_controller import AdminMigration
from controllers.admin.admin_user_controller import AdminUserList, AdminUserEdit, AdminUserDetail

app = webapp2.WSGIApplication([('/admin/', AdminMain),
                               ('/admin/debug', AdminDebugHandler),
                               ('/admin/events', AdminEventList),
                               ('/admin/events/([0-9]*)', AdminEventList),
                               ('/admin/event/add_alliance_selections/(.*)', AdminEventAddAllianceSelections),
                               ('/admin/event/add_teams/(.*)', AdminEventAddTeams),
                               ('/admin/event/add_webcast/(.*)', AdminEventAddWebcast),
                               ('/admin/event/create', AdminEventCreate),
                               ('/admin/event/create/test', AdminEventCreateTest),
                               ('/admin/event/delete/(.*)', AdminEventDelete),
                               ('/admin/event/edit/(.*)', AdminEventEdit),
                               ('/admin/event/(.*)', AdminEventDetail),
                               ('/admin/awards', AdminAwardDashboard),
                               ('/admin/award/add', AdminAwardAdd),
                               ('/admin/award/edit/(.*)', AdminAwardEdit),
                               ('/admin/matches', AdminMatchDashboard),
                               ('/admin/match/add', AdminMatchAdd),
                               ('/admin/match/cleanup', AdminMatchCleanup),
                               ('/admin/match/delete/(.*)', AdminMatchDelete),
                               ('/admin/match/edit/(.*)', AdminMatchEdit),
                               ('/admin/match/(.*)', AdminMatchDetail),
                               ('/admin/media', AdminMediaDashboard),
                               ('/admin/media/add_media', AdminMediaAdd),
                               ('/admin/memcache', AdminMemcacheMain),
                               ('/admin/migration', AdminMigration),
                               ('/admin/offseasons', AdminOffseasonScraperController),
                               ('/admin/sitevars', AdminSitevarList),
                               ('/admin/sitevar/create', AdminSitevarCreate),
                               ('/admin/sitevar/edit/(.*)', AdminSitevarEdit),
                               ('/admin/suggestions/event/webcast/review', AdminEventWebcastSuggestionsReviewController),
                               ('/admin/suggestions/match/video/review', AdminMatchVideoSuggestionsReviewController),
                               ('/admin/suggestions/media/review', AdminMediaSuggestionsReviewController),
                               ('/admin/tasks', AdminTasksHandler),
                               ('/admin/teams', AdminTeamList),
                               ('/admin/team/(.*)', AdminTeamDetail),
                               ('/admin/users', AdminUserList),
                               ('/admin/user/edit/(.*)', AdminUserEdit),
                               ('/admin/user/(.*)', AdminUserDetail),
                               ('/admin/videos/add', AdminVideosAdd),
                               ],
                              debug=tba_config.DEBUG)

########NEW FILE########
__FILENAME__ = api_main
#!/usr/bin/env python
import webapp2

import tba_config

from controllers.api_controller import ApiEventsShow, ApiTeamDetails, ApiTeamsShow, \
                                       ApiEventList, ApiEventDetails, ApiMatchDetails, \
                                       CsvTeamsAll
from controllers.api.api_team_controller import ApiTeamController
from controllers.api.api_event_controller import ApiEventController, ApiEventTeamsController, \
                                                 ApiEventMatchesController, ApiEventStatsController, \
                                                 ApiEventRankingsController, ApiEventAwardsController, ApiEventListController
from controllers.api.api_trusted_controller import ApiTrustedAddMatchYoutubeVideo


app = webapp2.WSGIApplication([('/api/v1/team/details', ApiTeamDetails),
                               ('/api/v1/teams/show', ApiTeamsShow),
                               ('/api/v1/events/show', ApiEventsShow),
                               ('/api/v1/events/list', ApiEventList),
                               ('/api/v1/event/details', ApiEventDetails),
                               ('/api/v1/match/details', ApiMatchDetails),
                               ('/api/csv/teams/all', CsvTeamsAll),
                               webapp2.Route(r'/api/v2/team/<team_key:>',
                                             ApiTeamController,
                                             methods=['GET']),
                               webapp2.Route(r'/api/v2/team/<team_key:>/<year:([0-9]*)>',
                                             ApiTeamController,
                                             methods=['GET']),
                               webapp2.Route(r'/api/v2/event/<event_key:>',
                                             ApiEventController,
                                             methods=['GET']),
                               webapp2.Route(r'/api/v2/event/<event_key:>/teams',
                                             ApiEventTeamsController,
                                             methods=['GET']),
                               webapp2.Route(r'/api/v2/event/<event_key:>/matches',
                                             ApiEventMatchesController,
                                             methods=['GET']),
                               webapp2.Route(r'/api/v2/event/<event_key:>/stats',
                                             ApiEventStatsController,
                                             methods=['GET']),
                               webapp2.Route(r'/api/v2/event/<event_key:>/rankings',
                                             ApiEventRankingsController,
                                             methods=['GET']),
                               webapp2.Route(r'/api/v2/event/<event_key:>/awards',
                                            ApiEventAwardsController,
                                            methods=['GET']),
                               webapp2.Route(r'/api/v2/events/<year:([0-9]*)>',
                                             ApiEventListController,
                                             methods=['GET']),
                               webapp2.Route(r'/api/trusted/v1/match/add_youtube_video',
                                             ApiTrustedAddMatchYoutubeVideo,
                                             methods=['POST']),
                               ], debug=tba_config.DEBUG)

########NEW FILE########
__FILENAME__ = appengine_config
appstats_CALC_RPC_COSTS = True


def webapp_add_wsgi_middleware(app):
    from google.appengine.ext.appstats import recording
    app = recording.appstats_wsgi_middleware(app)
    return app

########NEW FILE########
__FILENAME__ = BeautifulSoup
"""Beautiful Soup
Elixir and Tonic
"The Screen-Scraper's Friend"
http://www.crummy.com/software/BeautifulSoup/

Beautiful Soup parses a (possibly invalid) XML or HTML document into a
tree representation. It provides methods and Pythonic idioms that make
it easy to navigate, search, and modify the tree.

A well-formed XML/HTML document yields a well-formed data
structure. An ill-formed XML/HTML document yields a correspondingly
ill-formed data structure. If your document is only locally
well-formed, you can use this library to find and process the
well-formed part of it.

Beautiful Soup works with Python 2.2 and up. It has no external
dependencies, but you'll have more success at converting data to UTF-8
if you also install these three packages:

* chardet, for auto-detecting character encodings
  http://chardet.feedparser.org/
* cjkcodecs and iconv_codec, which add more encodings to the ones supported
  by stock Python.
  http://cjkpython.i18n.org/

Beautiful Soup defines classes for two main parsing strategies:

 * BeautifulStoneSoup, for parsing XML, SGML, or your domain-specific
   language that kind of looks like XML.

 * BeautifulSoup, for parsing run-of-the-mill HTML code, be it valid
   or invalid. This class has web browser-like heuristics for
   obtaining a sensible parse tree in the face of common HTML errors.

Beautiful Soup also defines a class (UnicodeDammit) for autodetecting
the encoding of an HTML or XML document, and converting it to
Unicode. Much of this code is taken from Mark Pilgrim's Universal Feed Parser.

For more than you ever wanted to know about Beautiful Soup, see the
documentation:
http://www.crummy.com/software/BeautifulSoup/documentation.html

Here, have some legalese:

Copyright (c) 2004-2010, Leonard Richardson

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

  * Redistributions of source code must retain the above copyright
    notice, this list of conditions and the following disclaimer.

  * Redistributions in binary form must reproduce the above
    copyright notice, this list of conditions and the following
    disclaimer in the documentation and/or other materials provided
    with the distribution.

  * Neither the name of the the Beautiful Soup Consortium and All
    Night Kosher Bakery nor the names of its contributors may be
    used to endorse or promote products derived from this software
    without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE, DAMMIT.

"""
from __future__ import generators

__author__ = "Leonard Richardson (leonardr@segfault.org)"
__version__ = "3.0.8.1"
__copyright__ = "Copyright (c) 2004-2010 Leonard Richardson"
__license__ = "New-style BSD"

from sgmllib import SGMLParser, SGMLParseError
import codecs
import markupbase
import types
import re
import sgmllib
try:
  from htmlentitydefs import name2codepoint
except ImportError:
  name2codepoint = {}
try:
    set
except NameError:
    from sets import Set as set

#These hacks make Beautiful Soup able to parse XML with namespaces
sgmllib.tagfind = re.compile('[a-zA-Z][-_.:a-zA-Z0-9]*')
markupbase._declname_match = re.compile(r'[a-zA-Z][-_.:a-zA-Z0-9]*\s*').match

DEFAULT_OUTPUT_ENCODING = "utf-8"

def _match_css_class(str):
    """Build a RE to match the given CSS class."""
    return re.compile(r"(^|.*\s)%s($|\s)" % str)

# First, the classes that represent markup elements.

class PageElement(object):
    """Contains the navigational information for some part of the page
    (either a tag or a piece of text)"""

    def setup(self, parent=None, previous=None):
        """Sets up the initial relations between this element and
        other elements."""
        self.parent = parent
        self.previous = previous
        self.next = None
        self.previousSibling = None
        self.nextSibling = None
        if self.parent and self.parent.contents:
            self.previousSibling = self.parent.contents[-1]
            self.previousSibling.nextSibling = self

    def replaceWith(self, replaceWith):
        oldParent = self.parent
        myIndex = self.parent.index(self)
        if hasattr(replaceWith, "parent")\
                  and replaceWith.parent is self.parent:
            # We're replacing this element with one of its siblings.
            index = replaceWith.parent.index(replaceWith)
            if index and index < myIndex:
                # Furthermore, it comes before this element. That
                # means that when we extract it, the index of this
                # element will change.
                myIndex = myIndex - 1
        self.extract()
        oldParent.insert(myIndex, replaceWith)

    def replaceWithChildren(self):
        myParent = self.parent
        myIndex = self.parent.index(self)
        self.extract()
        reversedChildren = list(self.contents)
        reversedChildren.reverse()
        for child in reversedChildren:
            myParent.insert(myIndex, child)

    def extract(self):
        """Destructively rips this element out of the tree."""
        if self.parent:
            try:
                del self.parent.contents[self.parent.index(self)]
            except ValueError:
                pass

        #Find the two elements that would be next to each other if
        #this element (and any children) hadn't been parsed. Connect
        #the two.
        lastChild = self._lastRecursiveChild()
        nextElement = lastChild.next

        if self.previous:
            self.previous.next = nextElement
        if nextElement:
            nextElement.previous = self.previous
        self.previous = None
        lastChild.next = None

        self.parent = None
        if self.previousSibling:
            self.previousSibling.nextSibling = self.nextSibling
        if self.nextSibling:
            self.nextSibling.previousSibling = self.previousSibling
        self.previousSibling = self.nextSibling = None
        return self

    def _lastRecursiveChild(self):
        "Finds the last element beneath this object to be parsed."
        lastChild = self
        while hasattr(lastChild, 'contents') and lastChild.contents:
            lastChild = lastChild.contents[-1]
        return lastChild

    def insert(self, position, newChild):
        if isinstance(newChild, basestring) \
            and not isinstance(newChild, NavigableString):
            newChild = NavigableString(newChild)

        position =  min(position, len(self.contents))
        if hasattr(newChild, 'parent') and newChild.parent is not None:
            # We're 'inserting' an element that's already one
            # of this object's children.
            if newChild.parent is self:
                index = self.index(newChild)
                if index > position:
                    # Furthermore we're moving it further down the
                    # list of this object's children. That means that
                    # when we extract this element, our target index
                    # will jump down one.
                    position = position - 1
            newChild.extract()

        newChild.parent = self
        previousChild = None
        if position == 0:
            newChild.previousSibling = None
            newChild.previous = self
        else:
            previousChild = self.contents[position-1]
            newChild.previousSibling = previousChild
            newChild.previousSibling.nextSibling = newChild
            newChild.previous = previousChild._lastRecursiveChild()
        if newChild.previous:
            newChild.previous.next = newChild

        newChildsLastElement = newChild._lastRecursiveChild()

        if position >= len(self.contents):
            newChild.nextSibling = None

            parent = self
            parentsNextSibling = None
            while not parentsNextSibling:
                parentsNextSibling = parent.nextSibling
                parent = parent.parent
                if not parent: # This is the last element in the document.
                    break
            if parentsNextSibling:
                newChildsLastElement.next = parentsNextSibling
            else:
                newChildsLastElement.next = None
        else:
            nextChild = self.contents[position]
            newChild.nextSibling = nextChild
            if newChild.nextSibling:
                newChild.nextSibling.previousSibling = newChild
            newChildsLastElement.next = nextChild

        if newChildsLastElement.next:
            newChildsLastElement.next.previous = newChildsLastElement
        self.contents.insert(position, newChild)

    def append(self, tag):
        """Appends the given tag to the contents of this tag."""
        self.insert(len(self.contents), tag)

    def findNext(self, name=None, attrs={}, text=None, **kwargs):
        """Returns the first item that matches the given criteria and
        appears after this Tag in the document."""
        return self._findOne(self.findAllNext, name, attrs, text, **kwargs)

    def findAllNext(self, name=None, attrs={}, text=None, limit=None,
                    **kwargs):
        """Returns all items that match the given criteria and appear
        after this Tag in the document."""
        return self._findAll(name, attrs, text, limit, self.nextGenerator,
                             **kwargs)

    def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):
        """Returns the closest sibling to this Tag that matches the
        given criteria and appears after this Tag in the document."""
        return self._findOne(self.findNextSiblings, name, attrs, text,
                             **kwargs)

    def findNextSiblings(self, name=None, attrs={}, text=None, limit=None,
                         **kwargs):
        """Returns the siblings of this Tag that match the given
        criteria and appear after this Tag in the document."""
        return self._findAll(name, attrs, text, limit,
                             self.nextSiblingGenerator, **kwargs)
    fetchNextSiblings = findNextSiblings # Compatibility with pre-3.x

    def findPrevious(self, name=None, attrs={}, text=None, **kwargs):
        """Returns the first item that matches the given criteria and
        appears before this Tag in the document."""
        return self._findOne(self.findAllPrevious, name, attrs, text, **kwargs)

    def findAllPrevious(self, name=None, attrs={}, text=None, limit=None,
                        **kwargs):
        """Returns all items that match the given criteria and appear
        before this Tag in the document."""
        return self._findAll(name, attrs, text, limit, self.previousGenerator,
                           **kwargs)
    fetchPrevious = findAllPrevious # Compatibility with pre-3.x

    def findPreviousSibling(self, name=None, attrs={}, text=None, **kwargs):
        """Returns the closest sibling to this Tag that matches the
        given criteria and appears before this Tag in the document."""
        return self._findOne(self.findPreviousSiblings, name, attrs, text,
                             **kwargs)

    def findPreviousSiblings(self, name=None, attrs={}, text=None,
                             limit=None, **kwargs):
        """Returns the siblings of this Tag that match the given
        criteria and appear before this Tag in the document."""
        return self._findAll(name, attrs, text, limit,
                             self.previousSiblingGenerator, **kwargs)
    fetchPreviousSiblings = findPreviousSiblings # Compatibility with pre-3.x

    def findParent(self, name=None, attrs={}, **kwargs):
        """Returns the closest parent of this Tag that matches the given
        criteria."""
        # NOTE: We can't use _findOne because findParents takes a different
        # set of arguments.
        r = None
        l = self.findParents(name, attrs, 1)
        if l:
            r = l[0]
        return r

    def findParents(self, name=None, attrs={}, limit=None, **kwargs):
        """Returns the parents of this Tag that match the given
        criteria."""

        return self._findAll(name, attrs, None, limit, self.parentGenerator,
                             **kwargs)
    fetchParents = findParents # Compatibility with pre-3.x

    #These methods do the real heavy lifting.

    def _findOne(self, method, name, attrs, text, **kwargs):
        r = None
        l = method(name, attrs, text, 1, **kwargs)
        if l:
            r = l[0]
        return r

    def _findAll(self, name, attrs, text, limit, generator, **kwargs):
        "Iterates over a generator looking for things that match."

        if isinstance(name, SoupStrainer):
            strainer = name
        # (Possibly) special case some findAll*(...) searches
        elif text is None and not limit and not attrs and not kwargs:
            # findAll*(True)
            if name is True:
                return [element for element in generator()
                        if isinstance(element, Tag)]
            # findAll*('tag-name')
            elif isinstance(name, basestring):
                return [element for element in generator()
                        if isinstance(element, Tag) and
                        element.name == name]
            else:
                strainer = SoupStrainer(name, attrs, text, **kwargs)
        # Build a SoupStrainer
        else:
            strainer = SoupStrainer(name, attrs, text, **kwargs)
        results = ResultSet(strainer)
        g = generator()
        while True:
            try:
                i = g.next()
            except StopIteration:
                break
            if i:
                found = strainer.search(i)
                if found:
                    results.append(found)
                    if limit and len(results) >= limit:
                        break
        return results

    #These Generators can be used to navigate starting from both
    #NavigableStrings and Tags.
    def nextGenerator(self):
        i = self
        while i is not None:
            i = i.next
            yield i

    def nextSiblingGenerator(self):
        i = self
        while i is not None:
            i = i.nextSibling
            yield i

    def previousGenerator(self):
        i = self
        while i is not None:
            i = i.previous
            yield i

    def previousSiblingGenerator(self):
        i = self
        while i is not None:
            i = i.previousSibling
            yield i

    def parentGenerator(self):
        i = self
        while i is not None:
            i = i.parent
            yield i

    # Utility methods
    def substituteEncoding(self, str, encoding=None):
        encoding = encoding or "utf-8"
        return str.replace("%SOUP-ENCODING%", encoding)

    def toEncoding(self, s, encoding=None):
        """Encodes an object to a string in some encoding, or to Unicode.
        ."""
        if isinstance(s, unicode):
            if encoding:
                s = s.encode(encoding)
        elif isinstance(s, str):
            if encoding:
                s = s.encode(encoding)
            else:
                s = unicode(s)
        else:
            if encoding:
                s  = self.toEncoding(str(s), encoding)
            else:
                s = unicode(s)
        return s

class NavigableString(unicode, PageElement):

    def __new__(cls, value):
        """Create a new NavigableString.

        When unpickling a NavigableString, this method is called with
        the string in DEFAULT_OUTPUT_ENCODING. That encoding needs to be
        passed in to the superclass's __new__ or the superclass won't know
        how to handle non-ASCII characters.
        """
        if isinstance(value, unicode):
            return unicode.__new__(cls, value)
        return unicode.__new__(cls, value, DEFAULT_OUTPUT_ENCODING)

    def __getnewargs__(self):
        return (NavigableString.__str__(self),)

    def __getattr__(self, attr):
        """text.string gives you text. This is for backwards
        compatibility for Navigable*String, but for CData* it lets you
        get the string without the CData wrapper."""
        if attr == 'string':
            return self
        else:
            raise AttributeError, "'%s' object has no attribute '%s'" % (self.__class__.__name__, attr)

    def __unicode__(self):
        return str(self).decode(DEFAULT_OUTPUT_ENCODING)

    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
        if encoding:
            return self.encode(encoding)
        else:
            return self

class CData(NavigableString):

    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
        return "<![CDATA[%s]]>" % NavigableString.__str__(self, encoding)

class ProcessingInstruction(NavigableString):
    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
        output = self
        if "%SOUP-ENCODING%" in output:
            output = self.substituteEncoding(output, encoding)
        return "<?%s?>" % self.toEncoding(output, encoding)

class Comment(NavigableString):
    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
        return "<!--%s-->" % NavigableString.__str__(self, encoding)

class Declaration(NavigableString):
    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
        return "<!%s>" % NavigableString.__str__(self, encoding)

class Tag(PageElement):

    """Represents a found HTML tag with its attributes and contents."""

    def _invert(h):
        "Cheap function to invert a hash."
        i = {}
        for k,v in h.items():
            i[v] = k
        return i

    XML_ENTITIES_TO_SPECIAL_CHARS = { "apos" : "'",
                                      "quot" : '"',
                                      "amp" : "&",
                                      "lt" : "<",
                                      "gt" : ">" }

    XML_SPECIAL_CHARS_TO_ENTITIES = _invert(XML_ENTITIES_TO_SPECIAL_CHARS)

    def _convertEntities(self, match):
        """Used in a call to re.sub to replace HTML, XML, and numeric
        entities with the appropriate Unicode characters. If HTML
        entities are being converted, any unrecognized entities are
        escaped."""
        x = match.group(1)
        if self.convertHTMLEntities and x in name2codepoint:
            return unichr(name2codepoint[x])
        elif x in self.XML_ENTITIES_TO_SPECIAL_CHARS:
            if self.convertXMLEntities:
                return self.XML_ENTITIES_TO_SPECIAL_CHARS[x]
            else:
                return u'&%s;' % x
        elif len(x) > 0 and x[0] == '#':
            # Handle numeric entities
            if len(x) > 1 and x[1] == 'x':
                return unichr(int(x[2:], 16))
            else:
                return unichr(int(x[1:]))

        elif self.escapeUnrecognizedEntities:
            return u'&amp;%s;' % x
        else:
            return u'&%s;' % x

    def __init__(self, parser, name, attrs=None, parent=None,
                 previous=None):
        "Basic constructor."

        # We don't actually store the parser object: that lets extracted
        # chunks be garbage-collected
        self.parserClass = parser.__class__
        self.isSelfClosing = parser.isSelfClosingTag(name)
        self.name = name
        if attrs is None:
            attrs = []
        self.attrs = attrs
        self.contents = []
        self.setup(parent, previous)
        self.hidden = False
        self.containsSubstitutions = False
        self.convertHTMLEntities = parser.convertHTMLEntities
        self.convertXMLEntities = parser.convertXMLEntities
        self.escapeUnrecognizedEntities = parser.escapeUnrecognizedEntities

        # Convert any HTML, XML, or numeric entities in the attribute values.
        convert = lambda(k, val): (k,
                                   re.sub("&(#\d+|#x[0-9a-fA-F]+|\w+);",
                                          self._convertEntities,
                                          val))
        self.attrs = map(convert, self.attrs)

    def getString(self):
        if (len(self.contents) == 1
            and isinstance(self.contents[0], NavigableString)):
            return self.contents[0]

    def setString(self, string):
        """Replace the contents of the tag with a string"""
        self.clear()
        self.append(string)

    string = property(getString, setString)

    def getText(self, separator=u""):
        if not len(self.contents):
            return u""
        stopNode = self._lastRecursiveChild().next
        strings = []
        current = self.contents[0]
        while current is not stopNode:
            if isinstance(current, NavigableString):
                strings.append(current.strip())
            current = current.next
        return separator.join(strings)

    text = property(getText)

    def get(self, key, default=None):
        """Returns the value of the 'key' attribute for the tag, or
        the value given for 'default' if it doesn't have that
        attribute."""
        return self._getAttrMap().get(key, default)

    def clear(self):
        """Extract all children."""
        for child in self.contents[:]:
            child.extract()

    def index(self, element):
        for i, child in enumerate(self.contents):
            if child is element:
                return i
        raise ValueError("Tag.index: element not in tag")

    def has_key(self, key):
        return self._getAttrMap().has_key(key)

    def __getitem__(self, key):
        """tag[key] returns the value of the 'key' attribute for the tag,
        and throws an exception if it's not there."""
        return self._getAttrMap()[key]

    def __iter__(self):
        "Iterating over a tag iterates over its contents."
        return iter(self.contents)

    def __len__(self):
        "The length of a tag is the length of its list of contents."
        return len(self.contents)

    def __contains__(self, x):
        return x in self.contents

    def __nonzero__(self):
        "A tag is non-None even if it has no contents."
        return True

    def __setitem__(self, key, value):
        """Setting tag[key] sets the value of the 'key' attribute for the
        tag."""
        self._getAttrMap()
        self.attrMap[key] = value
        found = False
        for i in range(0, len(self.attrs)):
            if self.attrs[i][0] == key:
                self.attrs[i] = (key, value)
                found = True
        if not found:
            self.attrs.append((key, value))
        self._getAttrMap()[key] = value

    def __delitem__(self, key):
        "Deleting tag[key] deletes all 'key' attributes for the tag."
        for item in self.attrs:
            if item[0] == key:
                self.attrs.remove(item)
                #We don't break because bad HTML can define the same
                #attribute multiple times.
            self._getAttrMap()
            if self.attrMap.has_key(key):
                del self.attrMap[key]

    def __call__(self, *args, **kwargs):
        """Calling a tag like a function is the same as calling its
        findAll() method. Eg. tag('a') returns a list of all the A tags
        found within this tag."""
        return apply(self.findAll, args, kwargs)

    def __getattr__(self, tag):
        #print "Getattr %s.%s" % (self.__class__, tag)
        if len(tag) > 3 and tag.rfind('Tag') == len(tag)-3:
            return self.find(tag[:-3])
        elif tag.find('__') != 0:
            return self.find(tag)
        raise AttributeError, "'%s' object has no attribute '%s'" % (self.__class__, tag)

    def __eq__(self, other):
        """Returns true iff this tag has the same name, the same attributes,
        and the same contents (recursively) as the given tag.

        NOTE: right now this will return false if two tags have the
        same attributes in a different order. Should this be fixed?"""
        if other is self:
            return True
        if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):
            return False
        for i in range(0, len(self.contents)):
            if self.contents[i] != other.contents[i]:
                return False
        return True

    def __ne__(self, other):
        """Returns true iff this tag is not identical to the other tag,
        as defined in __eq__."""
        return not self == other

    def __repr__(self, encoding=DEFAULT_OUTPUT_ENCODING):
        """Renders this tag as a string."""
        return self.__str__(encoding)

    def __unicode__(self):
        return self.__str__(None)

    BARE_AMPERSAND_OR_BRACKET = re.compile("([<>]|"
                                           + "&(?!#\d+;|#x[0-9a-fA-F]+;|\w+;)"
                                           + ")")

    def _sub_entity(self, x):
        """Used with a regular expression to substitute the
        appropriate XML entity for an XML special character."""
        return "&" + self.XML_SPECIAL_CHARS_TO_ENTITIES[x.group(0)[0]] + ";"

    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING,
                prettyPrint=False, indentLevel=0):
        """Returns a string or Unicode representation of this tag and
        its contents. To get Unicode, pass None for encoding.

        NOTE: since Python's HTML parser consumes whitespace, this
        method is not certain to reproduce the whitespace present in
        the original string."""

        encodedName = self.toEncoding(self.name, encoding)

        attrs = []
        if self.attrs:
            for key, val in self.attrs:
                fmt = '%s="%s"'
                if isinstance(val, basestring):
                    if self.containsSubstitutions and '%SOUP-ENCODING%' in val:
                        val = self.substituteEncoding(val, encoding)

                    # The attribute value either:
                    #
                    # * Contains no embedded double quotes or single quotes.
                    #   No problem: we enclose it in double quotes.
                    # * Contains embedded single quotes. No problem:
                    #   double quotes work here too.
                    # * Contains embedded double quotes. No problem:
                    #   we enclose it in single quotes.
                    # * Embeds both single _and_ double quotes. This
                    #   can't happen naturally, but it can happen if
                    #   you modify an attribute value after parsing
                    #   the document. Now we have a bit of a
                    #   problem. We solve it by enclosing the
                    #   attribute in single quotes, and escaping any
                    #   embedded single quotes to XML entities.
                    if '"' in val:
                        fmt = "%s='%s'"
                        if "'" in val:
                            # TODO: replace with apos when
                            # appropriate.
                            val = val.replace("'", "&squot;")

                    # Now we're okay w/r/t quotes. But the attribute
                    # value might also contain angle brackets, or
                    # ampersands that aren't part of entities. We need
                    # to escape those to XML entities too.
                    val = self.BARE_AMPERSAND_OR_BRACKET.sub(self._sub_entity, val)

                attrs.append(fmt % (self.toEncoding(key, encoding),
                                    self.toEncoding(val, encoding)))
        close = ''
        closeTag = ''
        if self.isSelfClosing:
            close = ' /'
        else:
            closeTag = '</%s>' % encodedName

        indentTag, indentContents = 0, 0
        if prettyPrint:
            indentTag = indentLevel
            space = (' ' * (indentTag-1))
            indentContents = indentTag + 1
        contents = self.renderContents(encoding, prettyPrint, indentContents)
        if self.hidden:
            s = contents
        else:
            s = []
            attributeString = ''
            if attrs:
                attributeString = ' ' + ' '.join(attrs)
            if prettyPrint:
                s.append(space)
            s.append('<%s%s%s>' % (encodedName, attributeString, close))
            if prettyPrint:
                s.append("\n")
            s.append(contents)
            if prettyPrint and contents and contents[-1] != "\n":
                s.append("\n")
            if prettyPrint and closeTag:
                s.append(space)
            s.append(closeTag)
            if prettyPrint and closeTag and self.nextSibling:
                s.append("\n")
            s = ''.join(s)
        return s

    def decompose(self):
        """Recursively destroys the contents of this tree."""
        self.extract()
        if len(self.contents) == 0:
            return
        current = self.contents[0]
        while current is not None:
            next = current.next
            if isinstance(current, Tag):
                del current.contents[:]
            current.parent = None
            current.previous = None
            current.previousSibling = None
            current.next = None
            current.nextSibling = None
            current = next

    def prettify(self, encoding=DEFAULT_OUTPUT_ENCODING):
        return self.__str__(encoding, True)

    def renderContents(self, encoding=DEFAULT_OUTPUT_ENCODING,
                       prettyPrint=False, indentLevel=0):
        """Renders the contents of this tag as a string in the given
        encoding. If encoding is None, returns a Unicode string.."""
        s=[]
        for c in self:
            text = None
            if isinstance(c, NavigableString):
                text = c.__str__(encoding)
            elif isinstance(c, Tag):
                s.append(c.__str__(encoding, prettyPrint, indentLevel))
            if text and prettyPrint:
                text = text.strip()
            if text:
                if prettyPrint:
                    s.append(" " * (indentLevel-1))
                s.append(text)
                if prettyPrint:
                    s.append("\n")
        return ''.join(s)

    #Soup methods

    def find(self, name=None, attrs={}, recursive=True, text=None,
             **kwargs):
        """Return only the first child of this Tag matching the given
        criteria."""
        r = None
        l = self.findAll(name, attrs, recursive, text, 1, **kwargs)
        if l:
            r = l[0]
        return r
    findChild = find

    def findAll(self, name=None, attrs={}, recursive=True, text=None,
                limit=None, **kwargs):
        """Extracts a list of Tag objects that match the given
        criteria.  You can specify the name of the Tag and any
        attributes you want the Tag to have.

        The value of a key-value pair in the 'attrs' map can be a
        string, a list of strings, a regular expression object, or a
        callable that takes a string and returns whether or not the
        string matches for some custom definition of 'matches'. The
        same is true of the tag name."""
        generator = self.recursiveChildGenerator
        if not recursive:
            generator = self.childGenerator
        return self._findAll(name, attrs, text, limit, generator, **kwargs)
    findChildren = findAll

    # Pre-3.x compatibility methods
    first = find
    fetch = findAll

    def fetchText(self, text=None, recursive=True, limit=None):
        return self.findAll(text=text, recursive=recursive, limit=limit)

    def firstText(self, text=None, recursive=True):
        return self.find(text=text, recursive=recursive)

    #Private methods

    def _getAttrMap(self):
        """Initializes a map representation of this tag's attributes,
        if not already initialized."""
        if not getattr(self, 'attrMap'):
            self.attrMap = {}
            for (key, value) in self.attrs:
                self.attrMap[key] = value
        return self.attrMap

    #Generator methods
    def childGenerator(self):
        # Just use the iterator from the contents
        return iter(self.contents)

    def recursiveChildGenerator(self):
        if not len(self.contents):
            raise StopIteration
        stopNode = self._lastRecursiveChild().next
        current = self.contents[0]
        while current is not stopNode:
            yield current
            current = current.next


# Next, a couple classes to represent queries and their results.
class SoupStrainer:
    """Encapsulates a number of ways of matching a markup element (tag or
    text)."""

    def __init__(self, name=None, attrs={}, text=None, **kwargs):
        self.name = name
        if isinstance(attrs, basestring):
            kwargs['class'] = _match_css_class(attrs)
            attrs = None
        if kwargs:
            if attrs:
                attrs = attrs.copy()
                attrs.update(kwargs)
            else:
                attrs = kwargs
        self.attrs = attrs
        self.text = text

    def __str__(self):
        if self.text:
            return self.text
        else:
            return "%s|%s" % (self.name, self.attrs)

    def searchTag(self, markupName=None, markupAttrs={}):
        found = None
        markup = None
        if isinstance(markupName, Tag):
            markup = markupName
            markupAttrs = markup
        callFunctionWithTagData = callable(self.name) \
                                and not isinstance(markupName, Tag)

        if (not self.name) \
               or callFunctionWithTagData \
               or (markup and self._matches(markup, self.name)) \
               or (not markup and self._matches(markupName, self.name)):
            if callFunctionWithTagData:
                match = self.name(markupName, markupAttrs)
            else:
                match = True
                markupAttrMap = None
                for attr, matchAgainst in self.attrs.items():
                    if not markupAttrMap:
                         if hasattr(markupAttrs, 'get'):
                            markupAttrMap = markupAttrs
                         else:
                            markupAttrMap = {}
                            for k,v in markupAttrs:
                                markupAttrMap[k] = v
                    attrValue = markupAttrMap.get(attr)
                    if not self._matches(attrValue, matchAgainst):
                        match = False
                        break
            if match:
                if markup:
                    found = markup
                else:
                    found = markupName
        return found

    def search(self, markup):
        #print 'looking for %s in %s' % (self, markup)
        found = None
        # If given a list of items, scan it for a text element that
        # matches.
        if hasattr(markup, "__iter__") \
                and not isinstance(markup, Tag):
            for element in markup:
                if isinstance(element, NavigableString) \
                       and self.search(element):
                    found = element
                    break
        # If it's a Tag, make sure its name or attributes match.
        # Don't bother with Tags if we're searching for text.
        elif isinstance(markup, Tag):
            if not self.text:
                found = self.searchTag(markup)
        # If it's text, make sure the text matches.
        elif isinstance(markup, NavigableString) or \
                 isinstance(markup, basestring):
            if self._matches(markup, self.text):
                found = markup
        else:
            raise Exception, "I don't know how to match against a %s" \
                  % markup.__class__
        return found

    def _matches(self, markup, matchAgainst):
        #print "Matching %s against %s" % (markup, matchAgainst)
        result = False
        if matchAgainst is True:
            result = markup is not None
        elif callable(matchAgainst):
            result = matchAgainst(markup)
        else:
            #Custom match methods take the tag as an argument, but all
            #other ways of matching match the tag name as a string.
            if isinstance(markup, Tag):
                markup = markup.name
            if markup and not isinstance(markup, basestring):
                markup = unicode(markup)
            #Now we know that chunk is either a string, or None.
            if hasattr(matchAgainst, 'match'):
                # It's a regexp object.
                result = markup and matchAgainst.search(markup)
            elif hasattr(matchAgainst, '__iter__'): # list-like
                result = markup in matchAgainst
            elif hasattr(matchAgainst, 'items'):
                result = markup.has_key(matchAgainst)
            elif matchAgainst and isinstance(markup, basestring):
                if isinstance(markup, unicode):
                    matchAgainst = unicode(matchAgainst)
                else:
                    matchAgainst = str(matchAgainst)

            if not result:
                result = matchAgainst == markup
        return result

class ResultSet(list):
    """A ResultSet is just a list that keeps track of the SoupStrainer
    that created it."""
    def __init__(self, source):
        list.__init__([])
        self.source = source

# Now, some helper functions.

def buildTagMap(default, *args):
    """Turns a list of maps, lists, or scalars into a single map.
    Used to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and
    NESTING_RESET_TAGS maps out of lists and partial maps."""
    built = {}
    for portion in args:
        if hasattr(portion, 'items'):
            #It's a map. Merge it.
            for k,v in portion.items():
                built[k] = v
        elif hasattr(portion, '__iter__'): # is a list
            #It's a list. Map each item to the default.
            for k in portion:
                built[k] = default
        else:
            #It's a scalar. Map it to the default.
            built[portion] = default
    return built

# Now, the parser classes.

class BeautifulStoneSoup(Tag, SGMLParser):

    """This class contains the basic parser and search code. It defines
    a parser that knows nothing about tag behavior except for the
    following:

      You can't close a tag without closing all the tags it encloses.
      That is, "<foo><bar></foo>" actually means
      "<foo><bar></bar></foo>".

    [Another possible explanation is "<foo><bar /></foo>", but since
    this class defines no SELF_CLOSING_TAGS, it will never use that
    explanation.]

    This class is useful for parsing XML or made-up markup languages,
    or when BeautifulSoup makes an assumption counter to what you were
    expecting."""

    SELF_CLOSING_TAGS = {}
    NESTABLE_TAGS = {}
    RESET_NESTING_TAGS = {}
    QUOTE_TAGS = {}
    PRESERVE_WHITESPACE_TAGS = []

    MARKUP_MASSAGE = [(re.compile('(<[^<>]*)/>'),
                       lambda x: x.group(1) + ' />'),
                      (re.compile('<!\s+([^<>]*)>'),
                       lambda x: '<!' + x.group(1) + '>')
                      ]

    ROOT_TAG_NAME = u'[document]'

    HTML_ENTITIES = "html"
    XML_ENTITIES = "xml"
    XHTML_ENTITIES = "xhtml"
    # TODO: This only exists for backwards-compatibility
    ALL_ENTITIES = XHTML_ENTITIES

    # Used when determining whether a text node is all whitespace and
    # can be replaced with a single space. A text node that contains
    # fancy Unicode spaces (usually non-breaking) should be left
    # alone.
    STRIP_ASCII_SPACES = { 9: None, 10: None, 12: None, 13: None, 32: None, }

    def __init__(self, markup="", parseOnlyThese=None, fromEncoding=None,
                 markupMassage=True, smartQuotesTo=XML_ENTITIES,
                 convertEntities=None, selfClosingTags=None, isHTML=False):
        """The Soup object is initialized as the 'root tag', and the
        provided markup (which can be a string or a file-like object)
        is fed into the underlying parser.

        sgmllib will process most bad HTML, and the BeautifulSoup
        class has some tricks for dealing with some HTML that kills
        sgmllib, but Beautiful Soup can nonetheless choke or lose data
        if your data uses self-closing tags or declarations
        incorrectly.

        By default, Beautiful Soup uses regexes to sanitize input,
        avoiding the vast majority of these problems. If the problems
        don't apply to you, pass in False for markupMassage, and
        you'll get better performance.

        The default parser massage techniques fix the two most common
        instances of invalid HTML that choke sgmllib:

         <br/> (No space between name of closing tag and tag close)
         <! --Comment--> (Extraneous whitespace in declaration)

        You can pass in a custom list of (RE object, replace method)
        tuples to get Beautiful Soup to scrub your input the way you
        want."""

        self.parseOnlyThese = parseOnlyThese
        self.fromEncoding = fromEncoding
        self.smartQuotesTo = smartQuotesTo
        self.convertEntities = convertEntities
        # Set the rules for how we'll deal with the entities we
        # encounter
        if self.convertEntities:
            # It doesn't make sense to convert encoded characters to
            # entities even while you're converting entities to Unicode.
            # Just convert it all to Unicode.
            self.smartQuotesTo = None
            if convertEntities == self.HTML_ENTITIES:
                self.convertXMLEntities = False
                self.convertHTMLEntities = True
                self.escapeUnrecognizedEntities = True
            elif convertEntities == self.XHTML_ENTITIES:
                self.convertXMLEntities = True
                self.convertHTMLEntities = True
                self.escapeUnrecognizedEntities = False
            elif convertEntities == self.XML_ENTITIES:
                self.convertXMLEntities = True
                self.convertHTMLEntities = False
                self.escapeUnrecognizedEntities = False
        else:
            self.convertXMLEntities = False
            self.convertHTMLEntities = False
            self.escapeUnrecognizedEntities = False

        self.instanceSelfClosingTags = buildTagMap(None, selfClosingTags)
        SGMLParser.__init__(self)

        if hasattr(markup, 'read'):        # It's a file-type object.
            markup = markup.read()
        self.markup = markup
        self.markupMassage = markupMassage
        try:
            self._feed(isHTML=isHTML)
        except StopParsing:
            pass
        self.markup = None                 # The markup can now be GCed

    def convert_charref(self, name):
        """This method fixes a bug in Python's SGMLParser."""
        try:
            n = int(name)
        except ValueError:
            return
        if not 0 <= n <= 127 : # ASCII ends at 127, not 255
            return
        return self.convert_codepoint(n)

    def _feed(self, inDocumentEncoding=None, isHTML=False):
        # Convert the document to Unicode.
        markup = self.markup
        if isinstance(markup, unicode):
            if not hasattr(self, 'originalEncoding'):
                self.originalEncoding = None
        else:
            dammit = UnicodeDammit\
                     (markup, [self.fromEncoding, inDocumentEncoding],
                      smartQuotesTo=self.smartQuotesTo, isHTML=isHTML)
            markup = dammit.unicode
            self.originalEncoding = dammit.originalEncoding
            self.declaredHTMLEncoding = dammit.declaredHTMLEncoding
        if markup:
            if self.markupMassage:
                if not hasattr(self.markupMassage, "__iter__"):
                    self.markupMassage = self.MARKUP_MASSAGE
                for fix, m in self.markupMassage:
                    markup = fix.sub(m, markup)
                # TODO: We get rid of markupMassage so that the
                # soup object can be deepcopied later on. Some
                # Python installations can't copy regexes. If anyone
                # was relying on the existence of markupMassage, this
                # might cause problems.
                del(self.markupMassage)
        self.reset()

        SGMLParser.feed(self, markup)
        # Close out any unfinished strings and close all the open tags.
        self.endData()
        while self.currentTag.name != self.ROOT_TAG_NAME:
            self.popTag()

    def __getattr__(self, methodName):
        """This method routes method call requests to either the SGMLParser
        superclass or the Tag superclass, depending on the method name."""
        #print "__getattr__ called on %s.%s" % (self.__class__, methodName)

        if methodName.startswith('start_') or methodName.startswith('end_') \
               or methodName.startswith('do_'):
            return SGMLParser.__getattr__(self, methodName)
        elif not methodName.startswith('__'):
            return Tag.__getattr__(self, methodName)
        else:
            raise AttributeError

    def isSelfClosingTag(self, name):
        """Returns true iff the given string is the name of a
        self-closing tag according to this parser."""
        return self.SELF_CLOSING_TAGS.has_key(name) \
               or self.instanceSelfClosingTags.has_key(name)

    def reset(self):
        Tag.__init__(self, self, self.ROOT_TAG_NAME)
        self.hidden = 1
        SGMLParser.reset(self)
        self.currentData = []
        self.currentTag = None
        self.tagStack = []
        self.quoteStack = []
        self.pushTag(self)

    def popTag(self):
        tag = self.tagStack.pop()

        #print "Pop", tag.name
        if self.tagStack:
            self.currentTag = self.tagStack[-1]
        return self.currentTag

    def pushTag(self, tag):
        #print "Push", tag.name
        if self.currentTag:
            self.currentTag.contents.append(tag)
        self.tagStack.append(tag)
        self.currentTag = self.tagStack[-1]

    def endData(self, containerClass=NavigableString):
        if self.currentData:
            currentData = u''.join(self.currentData)
            if (currentData.translate(self.STRIP_ASCII_SPACES) == '' and
                not set([tag.name for tag in self.tagStack]).intersection(
                    self.PRESERVE_WHITESPACE_TAGS)):
                if '\n' in currentData:
                    currentData = '\n'
                else:
                    currentData = ' '
            self.currentData = []
            if self.parseOnlyThese and len(self.tagStack) <= 1 and \
                   (not self.parseOnlyThese.text or \
                    not self.parseOnlyThese.search(currentData)):
                return
            o = containerClass(currentData)
            o.setup(self.currentTag, self.previous)
            if self.previous:
                self.previous.next = o
            self.previous = o
            self.currentTag.contents.append(o)


    def _popToTag(self, name, inclusivePop=True):
        """Pops the tag stack up to and including the most recent
        instance of the given tag. If inclusivePop is false, pops the tag
        stack up to but *not* including the most recent instqance of
        the given tag."""
        #print "Popping to %s" % name
        if name == self.ROOT_TAG_NAME:
            return

        numPops = 0
        mostRecentTag = None
        for i in range(len(self.tagStack)-1, 0, -1):
            if name == self.tagStack[i].name:
                numPops = len(self.tagStack)-i
                break
        if not inclusivePop:
            numPops = numPops - 1

        for i in range(0, numPops):
            mostRecentTag = self.popTag()
        return mostRecentTag

    def _smartPop(self, name):

        """We need to pop up to the previous tag of this type, unless
        one of this tag's nesting reset triggers comes between this
        tag and the previous tag of this type, OR unless this tag is a
        generic nesting trigger and another generic nesting trigger
        comes between this tag and the previous tag of this type.

        Examples:
         <p>Foo<b>Bar *<p>* should pop to 'p', not 'b'.
         <p>Foo<table>Bar *<p>* should pop to 'table', not 'p'.
         <p>Foo<table><tr>Bar *<p>* should pop to 'tr', not 'p'.

         <li><ul><li> *<li>* should pop to 'ul', not the first 'li'.
         <tr><table><tr> *<tr>* should pop to 'table', not the first 'tr'
         <td><tr><td> *<td>* should pop to 'tr', not the first 'td'
        """

        nestingResetTriggers = self.NESTABLE_TAGS.get(name)
        isNestable = nestingResetTriggers != None
        isResetNesting = self.RESET_NESTING_TAGS.has_key(name)
        popTo = None
        inclusive = True
        for i in range(len(self.tagStack)-1, 0, -1):
            p = self.tagStack[i]
            if (not p or p.name == name) and not isNestable:
                #Non-nestable tags get popped to the top or to their
                #last occurance.
                popTo = name
                break
            if (nestingResetTriggers is not None
                and p.name in nestingResetTriggers) \
                or (nestingResetTriggers is None and isResetNesting
                    and self.RESET_NESTING_TAGS.has_key(p.name)):

                #If we encounter one of the nesting reset triggers
                #peculiar to this tag, or we encounter another tag
                #that causes nesting to reset, pop up to but not
                #including that tag.
                popTo = p.name
                inclusive = False
                break
            p = p.parent
        if popTo:
            self._popToTag(popTo, inclusive)

    def unknown_starttag(self, name, attrs, selfClosing=0):
        #print "Start tag %s: %s" % (name, attrs)
        if self.quoteStack:
            #This is not a real tag.
            #print "<%s> is not real!" % name
            attrs = ''.join([' %s="%s"' % (x, y) for x, y in attrs])
            self.handle_data('<%s%s>' % (name, attrs))
            return
        self.endData()

        if not self.isSelfClosingTag(name) and not selfClosing:
            self._smartPop(name)

        if self.parseOnlyThese and len(self.tagStack) <= 1 \
               and (self.parseOnlyThese.text or not self.parseOnlyThese.searchTag(name, attrs)):
            return

        tag = Tag(self, name, attrs, self.currentTag, self.previous)
        if self.previous:
            self.previous.next = tag
        self.previous = tag
        self.pushTag(tag)
        if selfClosing or self.isSelfClosingTag(name):
            self.popTag()
        if name in self.QUOTE_TAGS:
            #print "Beginning quote (%s)" % name
            self.quoteStack.append(name)
            self.literal = 1
        return tag

    def unknown_endtag(self, name):
        #print "End tag %s" % name
        if self.quoteStack and self.quoteStack[-1] != name:
            #This is not a real end tag.
            #print "</%s> is not real!" % name
            self.handle_data('</%s>' % name)
            return
        self.endData()
        self._popToTag(name)
        if self.quoteStack and self.quoteStack[-1] == name:
            self.quoteStack.pop()
            self.literal = (len(self.quoteStack) > 0)

    def handle_data(self, data):
        self.currentData.append(data)

    def _toStringSubclass(self, text, subclass):
        """Adds a certain piece of text to the tree as a NavigableString
        subclass."""
        self.endData()
        self.handle_data(text)
        self.endData(subclass)

    def handle_pi(self, text):
        """Handle a processing instruction as a ProcessingInstruction
        object, possibly one with a %SOUP-ENCODING% slot into which an
        encoding will be plugged later."""
        if text[:3] == "xml":
            text = u"xml version='1.0' encoding='%SOUP-ENCODING%'"
        self._toStringSubclass(text, ProcessingInstruction)

    def handle_comment(self, text):
        "Handle comments as Comment objects."
        self._toStringSubclass(text, Comment)

    def handle_charref(self, ref):
        "Handle character references as data."
        if self.convertEntities:
            data = unichr(int(ref))
        else:
            data = '&#%s;' % ref
        self.handle_data(data)

    def handle_entityref(self, ref):
        """Handle entity references as data, possibly converting known
        HTML and/or XML entity references to the corresponding Unicode
        characters."""
        data = None
        if self.convertHTMLEntities:
            try:
                data = unichr(name2codepoint[ref])
            except KeyError:
                pass

        if not data and self.convertXMLEntities:
                data = self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)

        if not data and self.convertHTMLEntities and \
            not self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):
                # TODO: We've got a problem here. We're told this is
                # an entity reference, but it's not an XML entity
                # reference or an HTML entity reference. Nonetheless,
                # the logical thing to do is to pass it through as an
                # unrecognized entity reference.
                #
                # Except: when the input is "&carol;" this function
                # will be called with input "carol". When the input is
                # "AT&T", this function will be called with input
                # "T". We have no way of knowing whether a semicolon
                # was present originally, so we don't know whether
                # this is an unknown entity or just a misplaced
                # ampersand.
                #
                # The more common case is a misplaced ampersand, so I
                # escape the ampersand and omit the trailing semicolon.
                data = "&amp;%s" % ref
        if not data:
            # This case is different from the one above, because we
            # haven't already gone through a supposedly comprehensive
            # mapping of entities to Unicode characters. We might not
            # have gone through any mapping at all. So the chances are
            # very high that this is a real entity, and not a
            # misplaced ampersand.
            data = "&%s;" % ref
        self.handle_data(data)

    def handle_decl(self, data):
        "Handle DOCTYPEs and the like as Declaration objects."
        self._toStringSubclass(data, Declaration)

    def parse_declaration(self, i):
        """Treat a bogus SGML declaration as raw data. Treat a CDATA
        declaration as a CData object."""
        j = None
        if self.rawdata[i:i+9] == '<![CDATA[':
             k = self.rawdata.find(']]>', i)
             if k == -1:
                 k = len(self.rawdata)
             data = self.rawdata[i+9:k]
             j = k+3
             self._toStringSubclass(data, CData)
        else:
            try:
                j = SGMLParser.parse_declaration(self, i)
            except SGMLParseError:
                toHandle = self.rawdata[i:]
                self.handle_data(toHandle)
                j = i + len(toHandle)
        return j

class BeautifulSoup(BeautifulStoneSoup):

    """This parser knows the following facts about HTML:

    * Some tags have no closing tag and should be interpreted as being
      closed as soon as they are encountered.

    * The text inside some tags (ie. 'script') may contain tags which
      are not really part of the document and which should be parsed
      as text, not tags. If you want to parse the text as tags, you can
      always fetch it and parse it explicitly.

    * Tag nesting rules:

      Most tags can't be nested at all. For instance, the occurance of
      a <p> tag should implicitly close the previous <p> tag.

       <p>Para1<p>Para2
        should be transformed into:
       <p>Para1</p><p>Para2

      Some tags can be nested arbitrarily. For instance, the occurance
      of a <blockquote> tag should _not_ implicitly close the previous
      <blockquote> tag.

       Alice said: <blockquote>Bob said: <blockquote>Blah
        should NOT be transformed into:
       Alice said: <blockquote>Bob said: </blockquote><blockquote>Blah

      Some tags can be nested, but the nesting is reset by the
      interposition of other tags. For instance, a <tr> tag should
      implicitly close the previous <tr> tag within the same <table>,
      but not close a <tr> tag in another table.

       <table><tr>Blah<tr>Blah
        should be transformed into:
       <table><tr>Blah</tr><tr>Blah
        but,
       <tr>Blah<table><tr>Blah
        should NOT be transformed into
       <tr>Blah<table></tr><tr>Blah

    Differing assumptions about tag nesting rules are a major source
    of problems with the BeautifulSoup class. If BeautifulSoup is not
    treating as nestable a tag your page author treats as nestable,
    try ICantBelieveItsBeautifulSoup, MinimalSoup, or
    BeautifulStoneSoup before writing your own subclass."""

    def __init__(self, *args, **kwargs):
        if not kwargs.has_key('smartQuotesTo'):
            kwargs['smartQuotesTo'] = self.HTML_ENTITIES
        kwargs['isHTML'] = True
        BeautifulStoneSoup.__init__(self, *args, **kwargs)

    SELF_CLOSING_TAGS = buildTagMap(None,
                                    ('br' , 'hr', 'input', 'img', 'meta',
                                    'spacer', 'link', 'frame', 'base', 'col'))

    PRESERVE_WHITESPACE_TAGS = set(['pre', 'textarea'])

    QUOTE_TAGS = {'script' : None, 'textarea' : None}

    #According to the HTML standard, each of these inline tags can
    #contain another tag of the same type. Furthermore, it's common
    #to actually use these tags this way.
    NESTABLE_INLINE_TAGS = ('span', 'font', 'q', 'object', 'bdo', 'sub', 'sup',
                            'center')

    #According to the HTML standard, these block tags can contain
    #another tag of the same type. Furthermore, it's common
    #to actually use these tags this way.
    NESTABLE_BLOCK_TAGS = ('blockquote', 'div', 'fieldset', 'ins', 'del')

    #Lists can contain other lists, but there are restrictions.
    NESTABLE_LIST_TAGS = { 'ol' : [],
                           'ul' : [],
                           'li' : ['ul', 'ol'],
                           'dl' : [],
                           'dd' : ['dl'],
                           'dt' : ['dl'] }

    #Tables can contain other tables, but there are restrictions.
    NESTABLE_TABLE_TAGS = {'table' : [],
                           'tr' : ['table', 'tbody', 'tfoot', 'thead'],
                           'td' : ['tr'],
                           'th' : ['tr'],
                           'thead' : ['table'],
                           'tbody' : ['table'],
                           'tfoot' : ['table'],
                           }

    NON_NESTABLE_BLOCK_TAGS = ('address', 'form', 'p', 'pre')

    #If one of these tags is encountered, all tags up to the next tag of
    #this type are popped.
    RESET_NESTING_TAGS = buildTagMap(None, NESTABLE_BLOCK_TAGS, 'noscript',
                                     NON_NESTABLE_BLOCK_TAGS,
                                     NESTABLE_LIST_TAGS,
                                     NESTABLE_TABLE_TAGS)

    NESTABLE_TAGS = buildTagMap([], NESTABLE_INLINE_TAGS, NESTABLE_BLOCK_TAGS,
                                NESTABLE_LIST_TAGS, NESTABLE_TABLE_TAGS)

    # Used to detect the charset in a META tag; see start_meta
    CHARSET_RE = re.compile("((^|;)\s*charset=)([^;]*)", re.M)

    def start_meta(self, attrs):
        """Beautiful Soup can detect a charset included in a META tag,
        try to convert the document to that charset, and re-parse the
        document from the beginning."""
        httpEquiv = None
        contentType = None
        contentTypeIndex = None
        tagNeedsEncodingSubstitution = False

        for i in range(0, len(attrs)):
            key, value = attrs[i]
            key = key.lower()
            if key == 'http-equiv':
                httpEquiv = value
            elif key == 'content':
                contentType = value
                contentTypeIndex = i

        if httpEquiv and contentType: # It's an interesting meta tag.
            match = self.CHARSET_RE.search(contentType)
            if match:
                if (self.declaredHTMLEncoding is not None or
                    self.originalEncoding == self.fromEncoding):
                    # An HTML encoding was sniffed while converting
                    # the document to Unicode, or an HTML encoding was
                    # sniffed during a previous pass through the
                    # document, or an encoding was specified
                    # explicitly and it worked. Rewrite the meta tag.
                    def rewrite(match):
                        return match.group(1) + "%SOUP-ENCODING%"
                    newAttr = self.CHARSET_RE.sub(rewrite, contentType)
                    attrs[contentTypeIndex] = (attrs[contentTypeIndex][0],
                                               newAttr)
                    tagNeedsEncodingSubstitution = True
                else:
                    # This is our first pass through the document.
                    # Go through it again with the encoding information.
                    newCharset = match.group(3)
                    if newCharset and newCharset != self.originalEncoding:
                        self.declaredHTMLEncoding = newCharset
                        self._feed(self.declaredHTMLEncoding)
                        raise StopParsing
                    pass
        tag = self.unknown_starttag("meta", attrs)
        if tag and tagNeedsEncodingSubstitution:
            tag.containsSubstitutions = True

class StopParsing(Exception):
    pass

class ICantBelieveItsBeautifulSoup(BeautifulSoup):

    """The BeautifulSoup class is oriented towards skipping over
    common HTML errors like unclosed tags. However, sometimes it makes
    errors of its own. For instance, consider this fragment:

     <b>Foo<b>Bar</b></b>

    This is perfectly valid (if bizarre) HTML. However, the
    BeautifulSoup class will implicitly close the first b tag when it
    encounters the second 'b'. It will think the author wrote
    "<b>Foo<b>Bar", and didn't close the first 'b' tag, because
    there's no real-world reason to bold something that's already
    bold. When it encounters '</b></b>' it will close two more 'b'
    tags, for a grand total of three tags closed instead of two. This
    can throw off the rest of your document structure. The same is
    true of a number of other tags, listed below.

    It's much more common for someone to forget to close a 'b' tag
    than to actually use nested 'b' tags, and the BeautifulSoup class
    handles the common case. This class handles the not-co-common
    case: where you can't believe someone wrote what they did, but
    it's valid HTML and BeautifulSoup screwed up by assuming it
    wouldn't be."""

    I_CANT_BELIEVE_THEYRE_NESTABLE_INLINE_TAGS = \
     ('em', 'big', 'i', 'small', 'tt', 'abbr', 'acronym', 'strong',
      'cite', 'code', 'dfn', 'kbd', 'samp', 'strong', 'var', 'b',
      'big')

    I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS = ('noscript',)

    NESTABLE_TAGS = buildTagMap([], BeautifulSoup.NESTABLE_TAGS,
                                I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS,
                                I_CANT_BELIEVE_THEYRE_NESTABLE_INLINE_TAGS)

class MinimalSoup(BeautifulSoup):
    """The MinimalSoup class is for parsing HTML that contains
    pathologically bad markup. It makes no assumptions about tag
    nesting, but it does know which tags are self-closing, that
    <script> tags contain Javascript and should not be parsed, that
    META tags may contain encoding information, and so on.

    This also makes it better for subclassing than BeautifulStoneSoup
    or BeautifulSoup."""

    RESET_NESTING_TAGS = buildTagMap('noscript')
    NESTABLE_TAGS = {}

class BeautifulSOAP(BeautifulStoneSoup):
    """This class will push a tag with only a single string child into
    the tag's parent as an attribute. The attribute's name is the tag
    name, and the value is the string child. An example should give
    the flavor of the change:

    <foo><bar>baz</bar></foo>
     =>
    <foo bar="baz"><bar>baz</bar></foo>

    You can then access fooTag['bar'] instead of fooTag.barTag.string.

    This is, of course, useful for scraping structures that tend to
    use subelements instead of attributes, such as SOAP messages. Note
    that it modifies its input, so don't print the modified version
    out.

    I'm not sure how many people really want to use this class; let me
    know if you do. Mainly I like the name."""

    def popTag(self):
        if len(self.tagStack) > 1:
            tag = self.tagStack[-1]
            parent = self.tagStack[-2]
            parent._getAttrMap()
            if (isinstance(tag, Tag) and len(tag.contents) == 1 and
                isinstance(tag.contents[0], NavigableString) and
                not parent.attrMap.has_key(tag.name)):
                parent[tag.name] = tag.contents[0]
        BeautifulStoneSoup.popTag(self)

#Enterprise class names! It has come to our attention that some people
#think the names of the Beautiful Soup parser classes are too silly
#and "unprofessional" for use in enterprise screen-scraping. We feel
#your pain! For such-minded folk, the Beautiful Soup Consortium And
#All-Night Kosher Bakery recommends renaming this file to
#"RobustParser.py" (or, in cases of extreme enterprisiness,
#"RobustParserBeanInterface.class") and using the following
#enterprise-friendly class aliases:
class RobustXMLParser(BeautifulStoneSoup):
    pass
class RobustHTMLParser(BeautifulSoup):
    pass
class RobustWackAssHTMLParser(ICantBelieveItsBeautifulSoup):
    pass
class RobustInsanelyWackAssHTMLParser(MinimalSoup):
    pass
class SimplifyingSOAPParser(BeautifulSOAP):
    pass

######################################################
#
# Bonus library: Unicode, Dammit
#
# This class forces XML data into a standard format (usually to UTF-8
# or Unicode).  It is heavily based on code from Mark Pilgrim's
# Universal Feed Parser. It does not rewrite the XML or HTML to
# reflect a new encoding: that happens in BeautifulStoneSoup.handle_pi
# (XML) and BeautifulSoup.start_meta (HTML).

# Autodetects character encodings.
# Download from http://chardet.feedparser.org/
try:
    import chardet
#    import chardet.constants
#    chardet.constants._debug = 1
except ImportError:
    chardet = None

# cjkcodecs and iconv_codec make Python know about more character encodings.
# Both are available from http://cjkpython.i18n.org/
# They're built in if you use Python 2.4.
try:
    import cjkcodecs.aliases
except ImportError:
    pass
try:
    import iconv_codec
except ImportError:
    pass

class UnicodeDammit:
    """A class for detecting the encoding of a *ML document and
    converting it to a Unicode string. If the source encoding is
    windows-1252, can replace MS smart quotes with their HTML or XML
    equivalents."""

    # This dictionary maps commonly seen values for "charset" in HTML
    # meta tags to the corresponding Python codec names. It only covers
    # values that aren't in Python's aliases and can't be determined
    # by the heuristics in find_codec.
    CHARSET_ALIASES = { "macintosh" : "mac-roman",
                        "x-sjis" : "shift-jis" }

    def __init__(self, markup, overrideEncodings=[],
                 smartQuotesTo='xml', isHTML=False):
        self.declaredHTMLEncoding = None
        self.markup, documentEncoding, sniffedEncoding = \
                     self._detectEncoding(markup, isHTML)
        self.smartQuotesTo = smartQuotesTo
        self.triedEncodings = []
        if markup == '' or isinstance(markup, unicode):
            self.originalEncoding = None
            self.unicode = unicode(markup)
            return

        u = None
        for proposedEncoding in overrideEncodings:
            u = self._convertFrom(proposedEncoding)
            if u: break
        if not u:
            for proposedEncoding in (documentEncoding, sniffedEncoding):
                u = self._convertFrom(proposedEncoding)
                if u: break

        # If no luck and we have auto-detection library, try that:
        if not u and chardet and not isinstance(self.markup, unicode):
            u = self._convertFrom(chardet.detect(self.markup)['encoding'])

        # As a last resort, try utf-8 and windows-1252:
        if not u:
            for proposed_encoding in ("utf-8", "windows-1252"):
                u = self._convertFrom(proposed_encoding)
                if u: break

        self.unicode = u
        if not u: self.originalEncoding = None

    def _subMSChar(self, orig):
        """Changes a MS smart quote character to an XML or HTML
        entity."""
        sub = self.MS_CHARS.get(orig)
        if isinstance(sub, tuple):
            if self.smartQuotesTo == 'xml':
                sub = '&#x%s;' % sub[1]
            else:
                sub = '&%s;' % sub[0]
        return sub

    def _convertFrom(self, proposed):
        proposed = self.find_codec(proposed)
        if not proposed or proposed in self.triedEncodings:
            return None
        self.triedEncodings.append(proposed)
        markup = self.markup

        # Convert smart quotes to HTML if coming from an encoding
        # that might have them.
        if self.smartQuotesTo and proposed.lower() in("windows-1252",
                                                      "iso-8859-1",
                                                      "iso-8859-2"):
            markup = re.compile("([\x80-\x9f])").sub \
                     (lambda(x): self._subMSChar(x.group(1)),
                      markup)

        try:
            # print "Trying to convert document to %s" % proposed
            u = self._toUnicode(markup, proposed)
            self.markup = u
            self.originalEncoding = proposed
        except Exception, e:
            # print "That didn't work!"
            # print e
            return None
        #print "Correct encoding: %s" % proposed
        return self.markup

    def _toUnicode(self, data, encoding):
        '''Given a string and its encoding, decodes the string into Unicode.
        %encoding is a string recognized by encodings.aliases'''

        # strip Byte Order Mark (if present)
        if (len(data) >= 4) and (data[:2] == '\xfe\xff') \
               and (data[2:4] != '\x00\x00'):
            encoding = 'utf-16be'
            data = data[2:]
        elif (len(data) >= 4) and (data[:2] == '\xff\xfe') \
                 and (data[2:4] != '\x00\x00'):
            encoding = 'utf-16le'
            data = data[2:]
        elif data[:3] == '\xef\xbb\xbf':
            encoding = 'utf-8'
            data = data[3:]
        elif data[:4] == '\x00\x00\xfe\xff':
            encoding = 'utf-32be'
            data = data[4:]
        elif data[:4] == '\xff\xfe\x00\x00':
            encoding = 'utf-32le'
            data = data[4:]
        newdata = unicode(data, encoding)
        return newdata

    def _detectEncoding(self, xml_data, isHTML=False):
        """Given a document, tries to detect its XML encoding."""
        xml_encoding = sniffed_xml_encoding = None
        try:
            if xml_data[:4] == '\x4c\x6f\xa7\x94':
                # EBCDIC
                xml_data = self._ebcdic_to_ascii(xml_data)
            elif xml_data[:4] == '\x00\x3c\x00\x3f':
                # UTF-16BE
                sniffed_xml_encoding = 'utf-16be'
                xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')
            elif (len(xml_data) >= 4) and (xml_data[:2] == '\xfe\xff') \
                     and (xml_data[2:4] != '\x00\x00'):
                # UTF-16BE with BOM
                sniffed_xml_encoding = 'utf-16be'
                xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')
            elif xml_data[:4] == '\x3c\x00\x3f\x00':
                # UTF-16LE
                sniffed_xml_encoding = 'utf-16le'
                xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')
            elif (len(xml_data) >= 4) and (xml_data[:2] == '\xff\xfe') and \
                     (xml_data[2:4] != '\x00\x00'):
                # UTF-16LE with BOM
                sniffed_xml_encoding = 'utf-16le'
                xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')
            elif xml_data[:4] == '\x00\x00\x00\x3c':
                # UTF-32BE
                sniffed_xml_encoding = 'utf-32be'
                xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')
            elif xml_data[:4] == '\x3c\x00\x00\x00':
                # UTF-32LE
                sniffed_xml_encoding = 'utf-32le'
                xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')
            elif xml_data[:4] == '\x00\x00\xfe\xff':
                # UTF-32BE with BOM
                sniffed_xml_encoding = 'utf-32be'
                xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')
            elif xml_data[:4] == '\xff\xfe\x00\x00':
                # UTF-32LE with BOM
                sniffed_xml_encoding = 'utf-32le'
                xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')
            elif xml_data[:3] == '\xef\xbb\xbf':
                # UTF-8 with BOM
                sniffed_xml_encoding = 'utf-8'
                xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')
            else:
                sniffed_xml_encoding = 'ascii'
                pass
        except:
            xml_encoding_match = None
        xml_encoding_match = re.compile(
            '^<\?.*encoding=[\'"](.*?)[\'"].*\?>').match(xml_data)
        if not xml_encoding_match and isHTML:
            regexp = re.compile('<\s*meta[^>]+charset=([^>]*?)[;\'">]', re.I)
            xml_encoding_match = regexp.search(xml_data)
        if xml_encoding_match is not None:
            xml_encoding = xml_encoding_match.groups()[0].lower()
            if isHTML:
                self.declaredHTMLEncoding = xml_encoding
            if sniffed_xml_encoding and \
               (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',
                                 'iso-10646-ucs-4', 'ucs-4', 'csucs4',
                                 'utf-16', 'utf-32', 'utf_16', 'utf_32',
                                 'utf16', 'u16')):
                xml_encoding = sniffed_xml_encoding
        return xml_data, xml_encoding, sniffed_xml_encoding


    def find_codec(self, charset):
        return self._codec(self.CHARSET_ALIASES.get(charset, charset)) \
               or (charset and self._codec(charset.replace("-", ""))) \
               or (charset and self._codec(charset.replace("-", "_"))) \
               or charset

    def _codec(self, charset):
        if not charset: return charset
        codec = None
        try:
            codecs.lookup(charset)
            codec = charset
        except (LookupError, ValueError):
            pass
        return codec

    EBCDIC_TO_ASCII_MAP = None
    def _ebcdic_to_ascii(self, s):
        c = self.__class__
        if not c.EBCDIC_TO_ASCII_MAP:
            emap = (0,1,2,3,156,9,134,127,151,141,142,11,12,13,14,15,
                    16,17,18,19,157,133,8,135,24,25,146,143,28,29,30,31,
                    128,129,130,131,132,10,23,27,136,137,138,139,140,5,6,7,
                    144,145,22,147,148,149,150,4,152,153,154,155,20,21,158,26,
                    32,160,161,162,163,164,165,166,167,168,91,46,60,40,43,33,
                    38,169,170,171,172,173,174,175,176,177,93,36,42,41,59,94,
                    45,47,178,179,180,181,182,183,184,185,124,44,37,95,62,63,
                    186,187,188,189,190,191,192,193,194,96,58,35,64,39,61,34,
                    195,97,98,99,100,101,102,103,104,105,196,197,198,199,200,
                    201,202,106,107,108,109,110,111,112,113,114,203,204,205,
                    206,207,208,209,126,115,116,117,118,119,120,121,122,210,
                    211,212,213,214,215,216,217,218,219,220,221,222,223,224,
                    225,226,227,228,229,230,231,123,65,66,67,68,69,70,71,72,
                    73,232,233,234,235,236,237,125,74,75,76,77,78,79,80,81,
                    82,238,239,240,241,242,243,92,159,83,84,85,86,87,88,89,
                    90,244,245,246,247,248,249,48,49,50,51,52,53,54,55,56,57,
                    250,251,252,253,254,255)
            import string
            c.EBCDIC_TO_ASCII_MAP = string.maketrans( \
            ''.join(map(chr, range(256))), ''.join(map(chr, emap)))
        return s.translate(c.EBCDIC_TO_ASCII_MAP)

    MS_CHARS = { '\x80' : ('euro', '20AC'),
                 '\x81' : ' ',
                 '\x82' : ('sbquo', '201A'),
                 '\x83' : ('fnof', '192'),
                 '\x84' : ('bdquo', '201E'),
                 '\x85' : ('hellip', '2026'),
                 '\x86' : ('dagger', '2020'),
                 '\x87' : ('Dagger', '2021'),
                 '\x88' : ('circ', '2C6'),
                 '\x89' : ('permil', '2030'),
                 '\x8A' : ('Scaron', '160'),
                 '\x8B' : ('lsaquo', '2039'),
                 '\x8C' : ('OElig', '152'),
                 '\x8D' : '?',
                 '\x8E' : ('#x17D', '17D'),
                 '\x8F' : '?',
                 '\x90' : '?',
                 '\x91' : ('lsquo', '2018'),
                 '\x92' : ('rsquo', '2019'),
                 '\x93' : ('ldquo', '201C'),
                 '\x94' : ('rdquo', '201D'),
                 '\x95' : ('bull', '2022'),
                 '\x96' : ('ndash', '2013'),
                 '\x97' : ('mdash', '2014'),
                 '\x98' : ('tilde', '2DC'),
                 '\x99' : ('trade', '2122'),
                 '\x9a' : ('scaron', '161'),
                 '\x9b' : ('rsaquo', '203A'),
                 '\x9c' : ('oelig', '153'),
                 '\x9d' : '?',
                 '\x9e' : ('#x17E', '17E'),
                 '\x9f' : ('Yuml', ''),}

#######################################################################


#By default, act as an HTML pretty-printer.
if __name__ == '__main__':
    import sys
    soup = BeautifulSoup(sys.stdin)
    print soup.prettify()

########NEW FILE########
__FILENAME__ = api_utils
# Copyright 2013 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
# either express or implied. See the License for the specific
# language governing permissions and limitations under the License.

"""Util functions and classes for cloudstorage_api."""



__all__ = ['set_default_retry_params',
           'RetryParams',
          ]

import copy
import httplib
import logging
import math
import os
import threading
import time
import urllib


try:
  from google.appengine.api import app_identity
  from google.appengine.api import urlfetch
  from google.appengine.datastore import datastore_rpc
  from google.appengine.ext import ndb
  from google.appengine.ext.ndb import eventloop
  from google.appengine.ext.ndb import tasklets
  from google.appengine.ext.ndb import utils
  from google.appengine import runtime
  from google.appengine.runtime import apiproxy_errors
except ImportError:
  from google.appengine.api import app_identity
  from google.appengine.api import urlfetch
  from google.appengine.datastore import datastore_rpc
  from google.appengine import runtime
  from google.appengine.runtime import apiproxy_errors
  from google.appengine.ext import ndb
  from google.appengine.ext.ndb import eventloop
  from google.appengine.ext.ndb import tasklets
  from google.appengine.ext.ndb import utils


_RETRIABLE_EXCEPTIONS = (urlfetch.DownloadError,
                         apiproxy_errors.Error,
                         app_identity.InternalError,
                         app_identity.BackendDeadlineExceeded)

_thread_local_settings = threading.local()
_thread_local_settings.default_retry_params = None


def set_default_retry_params(retry_params):
  """Set a default RetryParams for current thread current request."""
  _thread_local_settings.default_retry_params = copy.copy(retry_params)


def _get_default_retry_params():
  """Get default RetryParams for current request and current thread.

  Returns:
    A new instance of the default RetryParams.
  """
  default = getattr(_thread_local_settings, 'default_retry_params', None)
  if default is None or not default.belong_to_current_request():
    return RetryParams()
  else:
    return copy.copy(default)


def _quote_filename(filename):
  """Quotes filename to use as a valid URI path.

  Args:
    filename: user provided filename. /bucket/filename.

  Returns:
    The filename properly quoted to use as URI's path component.
  """
  return urllib.quote(filename)


def _unquote_filename(filename):
  """Unquotes a valid URI path back to its filename.

  This is the opposite of _quote_filename.

  Args:
    filename: a quoted filename. /bucket/some%20filename.

  Returns:
    The filename unquoted.
  """
  return urllib.unquote(filename)


def _should_retry(resp):
  """Given a urlfetch response, decide whether to retry that request."""
  return (resp.status_code == httplib.REQUEST_TIMEOUT or
          (resp.status_code >= 500 and
           resp.status_code < 600))


class _RetryWrapper(object):
  """A wrapper that wraps retry logic around any tasklet."""

  def __init__(self,
               retry_params,
               retriable_exceptions=_RETRIABLE_EXCEPTIONS,
               should_retry=lambda r: False):
    """Init.

    Args:
      retry_params: an RetryParams instance.
      retriable_exceptions: a list of exception classes that are retriable.
      should_retry: a function that takes a result from the tasklet and returns
        a boolean. True if the result should be retried.
    """
    self.retry_params = retry_params
    self.retriable_exceptions = retriable_exceptions
    self.should_retry = should_retry

  @ndb.tasklet
  def run(self, tasklet, **kwds):
    """Run a tasklet with retry.

    The retry should be transparent to the caller: if no results
    are successful, the exception or result from the last retry is returned
    to the caller.

    Args:
      tasklet: the tasklet to run.
      **kwds: keywords arguments to run the tasklet.

    Raises:
      The exception from running the tasklet.

    Returns:
      The result from running the tasklet.
    """
    start_time = time.time()
    n = 1

    while True:
      e = None
      result = None
      got_result = False

      try:
        result = yield tasklet(**kwds)
        got_result = True
        if not self.should_retry(result):
          raise ndb.Return(result)
      except runtime.DeadlineExceededError:
        logging.debug(
            'Tasklet has exceeded request deadline after %s seconds total',
            time.time() - start_time)
        raise
      except self.retriable_exceptions as e:
        pass

      if n == 1:
        logging.debug('Tasklet is %r', tasklet)

      delay = self.retry_params.delay(n, start_time)

      if delay <= 0:
        logging.debug(
            'Tasklet failed after %s attempts and %s seconds in total',
            n, time.time() - start_time)
        if got_result:
          raise ndb.Return(result)
        elif e is not None:
          raise e
        else:
          assert False, 'Should never reach here.'

      if got_result:
        logging.debug(
            'Got result %r from tasklet.', result)
      else:
        logging.debug(
            'Got exception "%r" from tasklet.', e)
      logging.debug('Retry in %s seconds.', delay)
      n += 1
      yield tasklets.sleep(delay)


class RetryParams(object):
  """Retry configuration parameters."""

  _DEFAULT_USER_AGENT = 'App Engine Python GCS Client'

  @datastore_rpc._positional(1)
  def __init__(self,
               backoff_factor=2.0,
               initial_delay=0.1,
               max_delay=10.0,
               min_retries=3,
               max_retries=6,
               max_retry_period=30.0,
               urlfetch_timeout=None,
               save_access_token=False,
               _user_agent=None):
    """Init.

    This object is unique per request per thread.

    Library will retry according to this setting when App Engine Server
    can't call urlfetch, urlfetch timed out, or urlfetch got a 408 or
    500-600 response.

    Args:
      backoff_factor: exponential backoff multiplier.
      initial_delay: seconds to delay for the first retry.
      max_delay: max seconds to delay for every retry.
      min_retries: min number of times to retry. This value is automatically
        capped by max_retries.
      max_retries: max number of times to retry. Set this to 0 for no retry.
      max_retry_period: max total seconds spent on retry. Retry stops when
        this period passed AND min_retries has been attempted.
      urlfetch_timeout: timeout for urlfetch in seconds. Could be None,
        in which case the value will be chosen by urlfetch module.
      save_access_token: persist access token to datastore to avoid
        excessive usage of GetAccessToken API. Usually the token is cached
        in process and in memcache. In some cases, memcache isn't very
        reliable.
      _user_agent: The user agent string that you want to use in your requests.
    """
    self.backoff_factor = self._check('backoff_factor', backoff_factor)
    self.initial_delay = self._check('initial_delay', initial_delay)
    self.max_delay = self._check('max_delay', max_delay)
    self.max_retry_period = self._check('max_retry_period', max_retry_period)
    self.max_retries = self._check('max_retries', max_retries, True, int)
    self.min_retries = self._check('min_retries', min_retries, True, int)
    if self.min_retries > self.max_retries:
      self.min_retries = self.max_retries

    self.urlfetch_timeout = None
    if urlfetch_timeout is not None:
      self.urlfetch_timeout = self._check('urlfetch_timeout', urlfetch_timeout)
    self.save_access_token = self._check('save_access_token', save_access_token,
                                         True, bool)
    self._user_agent = _user_agent or self._DEFAULT_USER_AGENT

    self._request_id = os.getenv('REQUEST_LOG_ID')

  def __eq__(self, other):
    if not isinstance(other, self.__class__):
      return False
    return self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not self.__eq__(other)

  @classmethod
  def _check(cls, name, val, can_be_zero=False, val_type=float):
    """Check init arguments.

    Args:
      name: name of the argument. For logging purpose.
      val: value. Value has to be non negative number.
      can_be_zero: whether value can be zero.
      val_type: Python type of the value.

    Returns:
      The value.

    Raises:
      ValueError: when invalid value is passed in.
      TypeError: when invalid value type is passed in.
    """
    valid_types = [val_type]
    if val_type is float:
      valid_types.append(int)

    if type(val) not in valid_types:
      raise TypeError(
          'Expect type %s for parameter %s' % (val_type.__name__, name))
    if val < 0:
      raise ValueError(
          'Value for parameter %s has to be greater than 0' % name)
    if not can_be_zero and val == 0:
      raise ValueError(
          'Value for parameter %s can not be 0' % name)
    return val

  def belong_to_current_request(self):
    return os.getenv('REQUEST_LOG_ID') == self._request_id

  def delay(self, n, start_time):
    """Calculate delay before the next retry.

    Args:
      n: the number of current attempt. The first attempt should be 1.
      start_time: the time when retry started in unix time.

    Returns:
      Number of seconds to wait before next retry. -1 if retry should give up.
    """
    if (n > self.max_retries or
        (n > self.min_retries and
         time.time() - start_time > self.max_retry_period)):
      return -1
    return min(
        math.pow(self.backoff_factor, n-1) * self.initial_delay,
        self.max_delay)


def _run_until_rpc():
  """Eagerly evaluate tasklets until it is blocking on some RPC.

  Usually ndb eventloop el isn't run until some code calls future.get_result().

  When an async tasklet is called, the tasklet wrapper evaluates the tasklet
  code into a generator, enqueues a callback _help_tasklet_along onto
  the el.current queue, and returns a future.

  _help_tasklet_along, when called by the el, will
  get one yielded value from the generator. If the value if another future,
  set up a callback _on_future_complete to invoke _help_tasklet_along
  when the dependent future fulfills. If the value if a RPC, set up a
  callback _on_rpc_complete to invoke _help_tasklet_along when the RPC fulfills.
  Thus _help_tasklet_along drills down
  the chain of futures until some future is blocked by RPC. El runs
  all callbacks and constantly check pending RPC status.
  """
  el = eventloop.get_event_loop()
  while el.current:
    el.run0()


def _eager_tasklet(tasklet):
  """Decorator to turn tasklet to run eagerly."""

  @utils.wrapping(tasklet)
  def eager_wrapper(*args, **kwds):
    fut = tasklet(*args, **kwds)
    _run_until_rpc()
    return fut

  return eager_wrapper

########NEW FILE########
__FILENAME__ = cloudstorage_api
# Copyright 2012 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
# either express or implied. See the License for the specific
# language governing permissions and limitations under the License.

"""File Interface for Google Cloud Storage."""



from __future__ import with_statement



__all__ = ['delete',
           'listbucket',
           'open',
           'stat',
          ]

import logging
import StringIO
import urllib
import xml.etree.cElementTree as ET
from . import api_utils
from . import common
from . import errors
from . import storage_api



def open(filename,
         mode='r',
         content_type=None,
         options=None,
         read_buffer_size=storage_api.ReadBuffer.DEFAULT_BUFFER_SIZE,
         retry_params=None,
         _account_id=None):
  """Opens a Google Cloud Storage file and returns it as a File-like object.

  Args:
    filename: A Google Cloud Storage filename of form '/bucket/filename'.
    mode: 'r' for reading mode. 'w' for writing mode.
      In reading mode, the file must exist. In writing mode, a file will
      be created or be overrode.
    content_type: The MIME type of the file. str. Only valid in writing mode.
    options: A str->basestring dict to specify additional headers to pass to
      GCS e.g. {'x-goog-acl': 'private', 'x-goog-meta-foo': 'foo'}.
      Supported options are x-goog-acl, x-goog-meta-, cache-control,
      content-disposition, and content-encoding.
      Only valid in writing mode.
      See https://developers.google.com/storage/docs/reference-headers
      for details.
    read_buffer_size: The buffer size for read. Read keeps a buffer
      and prefetches another one. To minimize blocking for large files,
      always read by buffer size. To minimize number of RPC requests for
      small files, set a large buffer size. Max is 30MB.
    retry_params: An instance of api_utils.RetryParams for subsequent calls
      to GCS from this file handle. If None, the default one is used.
    _account_id: Internal-use only.

  Returns:
    A reading or writing buffer that supports File-like interface. Buffer
    must be closed after operations are done.

  Raises:
    errors.AuthorizationError: if authorization failed.
    errors.NotFoundError: if an object that's expected to exist doesn't.
    ValueError: invalid open mode or if content_type or options are specified
      in reading mode.
  """
  common.validate_file_path(filename)
  api = storage_api._get_storage_api(retry_params=retry_params,
                                     account_id=_account_id)
  filename = api_utils._quote_filename(filename)

  if mode == 'w':
    common.validate_options(options)
    return storage_api.StreamingBuffer(api, filename, content_type, options)
  elif mode == 'r':
    if content_type or options:
      raise ValueError('Options and content_type can only be specified '
                       'for writing mode.')
    return storage_api.ReadBuffer(api,
                                  filename,
                                  buffer_size=read_buffer_size)
  else:
    raise ValueError('Invalid mode %s.' % mode)


def delete(filename, retry_params=None, _account_id=None):
  """Delete a Google Cloud Storage file.

  Args:
    filename: A Google Cloud Storage filename of form '/bucket/filename'.
    retry_params: An api_utils.RetryParams for this call to GCS. If None,
      the default one is used.
    _account_id: Internal-use only.

  Raises:
    errors.NotFoundError: if the file doesn't exist prior to deletion.
  """
  api = storage_api._get_storage_api(retry_params=retry_params,
                                     account_id=_account_id)
  common.validate_file_path(filename)
  filename = api_utils._quote_filename(filename)
  status, resp_headers, content = api.delete_object(filename)
  errors.check_status(status, [204], filename, resp_headers=resp_headers,
                      body=content)


def stat(filename, retry_params=None, _account_id=None):
  """Get GCSFileStat of a Google Cloud storage file.

  Args:
    filename: A Google Cloud Storage filename of form '/bucket/filename'.
    retry_params: An api_utils.RetryParams for this call to GCS. If None,
      the default one is used.
    _account_id: Internal-use only.

  Returns:
    a GCSFileStat object containing info about this file.

  Raises:
    errors.AuthorizationError: if authorization failed.
    errors.NotFoundError: if an object that's expected to exist doesn't.
  """
  common.validate_file_path(filename)
  api = storage_api._get_storage_api(retry_params=retry_params,
                                     account_id=_account_id)
  status, headers, content = api.head_object(
      api_utils._quote_filename(filename))
  errors.check_status(status, [200], filename, resp_headers=headers,
                      body=content)
  file_stat = common.GCSFileStat(
      filename=filename,
      st_size=headers.get('content-length'),
      st_ctime=common.http_time_to_posix(headers.get('last-modified')),
      etag=headers.get('etag'),
      content_type=headers.get('content-type'),
      metadata=common.get_metadata(headers))

  return file_stat


def _copy2(src, dst, metadata=None, retry_params=None):
  """Copy the file content from src to dst.

  Internal use only!

  Args:
    src: /bucket/filename
    dst: /bucket/filename
    metadata: a dict of metadata for this copy. If None, old metadata is copied.
      For example, {'x-goog-meta-foo': 'bar'}.
    retry_params: An api_utils.RetryParams for this call to GCS. If None,
      the default one is used.

  Raises:
    errors.AuthorizationError: if authorization failed.
    errors.NotFoundError: if an object that's expected to exist doesn't.
  """
  common.validate_file_path(src)
  common.validate_file_path(dst)

  if metadata is None:
    metadata = {}
    copy_meta = 'COPY'
  else:
    copy_meta = 'REPLACE'
  metadata.update({'x-goog-copy-source': src,
                   'x-goog-metadata-directive': copy_meta})

  api = storage_api._get_storage_api(retry_params=retry_params)
  status, resp_headers, content = api.put_object(
      api_utils._quote_filename(dst), headers=metadata)
  errors.check_status(status, [200], src, metadata, resp_headers, body=content)


def listbucket(path_prefix, marker=None, prefix=None, max_keys=None,
               delimiter=None, retry_params=None, _account_id=None):
  """Returns a GCSFileStat iterator over a bucket.

  Optional arguments can limit the result to a subset of files under bucket.

  This function has two modes:
  1. List bucket mode: Lists all files in the bucket without any concept of
     hierarchy. GCS doesn't have real directory hierarchies.
  2. Directory emulation mode: If you specify the 'delimiter' argument,
     it is used as a path separator to emulate a hierarchy of directories.
     In this mode, the "path_prefix" argument should end in the delimiter
     specified (thus designates a logical directory). The logical directory's
     contents, both files and subdirectories, are listed. The names of
     subdirectories returned will end with the delimiter. So listbucket
     can be called with the subdirectory name to list the subdirectory's
     contents.

  Args:
    path_prefix: A Google Cloud Storage path of format "/bucket" or
      "/bucket/prefix". Only objects whose fullpath starts with the
      path_prefix will be returned.
    marker: Another path prefix. Only objects whose fullpath starts
      lexicographically after marker will be returned (exclusive).
    prefix: Deprecated. Use path_prefix.
    max_keys: The limit on the number of objects to return. int.
      For best performance, specify max_keys only if you know how many objects
      you want. Otherwise, this method requests large batches and handles
      pagination for you.
    delimiter: Use to turn on directory mode. str of one or multiple chars
      that your bucket uses as its directory separator.
    retry_params: An api_utils.RetryParams for this call to GCS. If None,
      the default one is used.
    _account_id: Internal-use only.

  Examples:
    For files "/bucket/a",
              "/bucket/bar/1"
              "/bucket/foo",
              "/bucket/foo/1", "/bucket/foo/2/1", "/bucket/foo/3/1",

    Regular mode:
    listbucket("/bucket/f", marker="/bucket/foo/1")
    will match "/bucket/foo/2/1", "/bucket/foo/3/1".

    Directory mode:
    listbucket("/bucket/", delimiter="/")
    will match "/bucket/a, "/bucket/bar/" "/bucket/foo", "/bucket/foo/".
    listbucket("/bucket/foo/", delimiter="/")
    will match "/bucket/foo/1", "/bucket/foo/2/", "/bucket/foo/3/"

  Returns:
    Regular mode:
    A GCSFileStat iterator over matched files ordered by filename.
    The iterator returns GCSFileStat objects. filename, etag, st_size,
    st_ctime, and is_dir are set.

    Directory emulation mode:
    A GCSFileStat iterator over matched files and directories ordered by
    name. The iterator returns GCSFileStat objects. For directories,
    only the filename and is_dir fields are set.

    The last name yielded can be used as next call's marker.
  """
  if prefix:
    common.validate_bucket_path(path_prefix)
    bucket = path_prefix
  else:
    bucket, prefix = common._process_path_prefix(path_prefix)

  if marker and marker.startswith(bucket):
    marker = marker[len(bucket) + 1:]

  api = storage_api._get_storage_api(retry_params=retry_params,
                                     account_id=_account_id)
  options = {}
  if marker:
    options['marker'] = marker
  if max_keys:
    options['max-keys'] = max_keys
  if prefix:
    options['prefix'] = prefix
  if delimiter:
    options['delimiter'] = delimiter

  return _Bucket(api, bucket, options)


class _Bucket(object):
  """A wrapper for a GCS bucket as the return value of listbucket."""

  def __init__(self, api, path, options):
    """Initialize.

    Args:
      api: storage_api instance.
      path: bucket path of form '/bucket'.
      options: a dict of listbucket options. Please see listbucket doc.
    """
    self._init(api, path, options)

  def _init(self, api, path, options):
    self._api = api
    self._path = path
    self._options = options.copy()
    self._get_bucket_fut = self._api.get_bucket_async(
        self._path + '?' + urllib.urlencode(self._options))
    self._last_yield = None
    self._new_max_keys = self._options.get('max-keys')

  def __getstate__(self):
    options = self._options
    if self._last_yield:
      options['marker'] = self._last_yield.filename[len(self._path) + 1:]
    if self._new_max_keys is not None:
      options['max-keys'] = self._new_max_keys
    return {'api': self._api,
            'path': self._path,
            'options': options}

  def __setstate__(self, state):
    self._init(state['api'], state['path'], state['options'])

  def __iter__(self):
    """Iter over the bucket.

    Yields:
      GCSFileStat: a GCSFileStat for an object in the bucket.
        They are ordered by GCSFileStat.filename.
    """
    total = 0
    max_keys = self._options.get('max-keys')

    while self._get_bucket_fut:
      status, resp_headers, content = self._get_bucket_fut.get_result()
      errors.check_status(status, [200], self._path, resp_headers=resp_headers,
                          body=content, extras=self._options)

      if self._should_get_another_batch(content):
        self._get_bucket_fut = self._api.get_bucket_async(
            self._path + '?' + urllib.urlencode(self._options))
      else:
        self._get_bucket_fut = None

      root = ET.fromstring(content)
      dirs = self._next_dir_gen(root)
      files = self._next_file_gen(root)
      next_file = files.next()
      next_dir = dirs.next()

      while ((max_keys is None or total < max_keys) and
             not (next_file is None and next_dir is None)):
        total += 1
        if next_file is None:
          self._last_yield = next_dir
          next_dir = dirs.next()
        elif next_dir is None:
          self._last_yield = next_file
          next_file = files.next()
        elif next_dir < next_file:
          self._last_yield = next_dir
          next_dir = dirs.next()
        elif next_file < next_dir:
          self._last_yield = next_file
          next_file = files.next()
        else:
          logging.error(
              'Should never reach. next file is %r. next dir is %r.',
              next_file, next_dir)
        if self._new_max_keys:
          self._new_max_keys -= 1
        yield self._last_yield

  def _next_file_gen(self, root):
    """Generator for next file element in the document.

    Args:
      root: root element of the XML tree.

    Yields:
      GCSFileStat for the next file.
    """
    for e in root.getiterator(common._T_CONTENTS):
      st_ctime, size, etag, key = None, None, None, None
      for child in e.getiterator('*'):
        if child.tag == common._T_LAST_MODIFIED:
          st_ctime = common.dt_str_to_posix(child.text)
        elif child.tag == common._T_ETAG:
          etag = child.text
        elif child.tag == common._T_SIZE:
          size = child.text
        elif child.tag == common._T_KEY:
          key = child.text
      yield common.GCSFileStat(self._path + '/' + key,
                               size, etag, st_ctime)
      e.clear()
    yield None

  def _next_dir_gen(self, root):
    """Generator for next directory element in the document.

    Args:
      root: root element in the XML tree.

    Yields:
      GCSFileStat for the next directory.
    """
    for e in root.getiterator(common._T_COMMON_PREFIXES):
      yield common.GCSFileStat(
          self._path + '/' + e.find(common._T_PREFIX).text,
          st_size=None, etag=None, st_ctime=None, is_dir=True)
      e.clear()
    yield None

  def _should_get_another_batch(self, content):
    """Whether to issue another GET bucket call.

    Args:
      content: response XML.

    Returns:
      True if should, also update self._options for the next request.
      False otherwise.
    """
    if ('max-keys' in self._options and
        self._options['max-keys'] <= common._MAX_GET_BUCKET_RESULT):
      return False

    elements = self._find_elements(
        content, set([common._T_IS_TRUNCATED,
                      common._T_NEXT_MARKER]))
    if elements.get(common._T_IS_TRUNCATED, 'false').lower() != 'true':
      return False

    next_marker = elements.get(common._T_NEXT_MARKER)
    if next_marker is None:
      self._options.pop('marker', None)
      return False
    self._options['marker'] = next_marker
    return True

  def _find_elements(self, result, elements):
    """Find interesting elements from XML.

    This function tries to only look for specified elements
    without parsing the entire XML. The specified elements is better
    located near the beginning.

    Args:
      result: response XML.
      elements: a set of interesting element tags.

    Returns:
      A dict from element tag to element value.
    """
    element_mapping = {}
    result = StringIO.StringIO(result)
    for _, e in ET.iterparse(result, events=('end',)):
      if not elements:
        break
      if e.tag in elements:
        element_mapping[e.tag] = e.text
        elements.remove(e.tag)
    return element_mapping

########NEW FILE########
__FILENAME__ = common
# Copyright 2012 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
# either express or implied. See the License for the specific
# language governing permissions and limitations under the License.

"""Helpers shared by cloudstorage_stub and cloudstorage_api."""





__all__ = ['CS_XML_NS',
           'CSFileStat',
           'dt_str_to_posix',
           'local_api_url',
           'LOCAL_GCS_ENDPOINT',
           'local_run',
           'get_access_token',
           'get_metadata',
           'GCSFileStat',
           'http_time_to_posix',
           'memory_usage',
           'posix_time_to_http',
           'posix_to_dt_str',
           'set_access_token',
           'validate_options',
           'validate_bucket_name',
           'validate_bucket_path',
           'validate_file_path',
          ]


import calendar
import datetime
from email import utils as email_utils
import logging
import os
import re

try:
  from google.appengine.api import runtime
except ImportError:
  from google.appengine.api import runtime


_GCS_BUCKET_REGEX_BASE = r'[a-z0-9\.\-_]{3,63}'
_GCS_BUCKET_REGEX = re.compile(_GCS_BUCKET_REGEX_BASE + r'$')
_GCS_BUCKET_PATH_REGEX = re.compile(r'/' + _GCS_BUCKET_REGEX_BASE + r'$')
_GCS_PATH_PREFIX_REGEX = re.compile(r'/' + _GCS_BUCKET_REGEX_BASE + r'.*')
_GCS_FULLPATH_REGEX = re.compile(r'/' + _GCS_BUCKET_REGEX_BASE + r'/.*')
_GCS_METADATA = ['x-goog-meta-',
                 'content-disposition',
                 'cache-control',
                 'content-encoding']
_GCS_OPTIONS = _GCS_METADATA + ['x-goog-acl']
CS_XML_NS = 'http://doc.s3.amazonaws.com/2006-03-01'
LOCAL_GCS_ENDPOINT = '/_ah/gcs'
_access_token = ''


_MAX_GET_BUCKET_RESULT = 1000


def set_access_token(access_token):
  """Set the shared access token to authenticate with Google Cloud Storage.

  When set, the library will always attempt to communicate with the
  real Google Cloud Storage with this token even when running on dev appserver.
  Note the token could expire so it's up to you to renew it.

  When absent, the library will automatically request and refresh a token
  on appserver, or when on dev appserver, talk to a Google Cloud Storage
  stub.

  Args:
    access_token: you can get one by run 'gsutil -d ls' and copy the
      str after 'Bearer'.
  """
  global _access_token
  _access_token = access_token


def get_access_token():
  """Returns the shared access token."""
  return _access_token


class GCSFileStat(object):
  """Container for GCS file stat."""

  def __init__(self,
               filename,
               st_size,
               etag,
               st_ctime,
               content_type=None,
               metadata=None,
               is_dir=False):
    """Initialize.

    For files, the non optional arguments are always set.
    For directories, only filename and is_dir is set.

    Args:
      filename: a Google Cloud Storage filename of form '/bucket/filename'.
      st_size: file size in bytes. long compatible.
      etag: hex digest of the md5 hash of the file's content. str.
      st_ctime: posix file creation time. float compatible.
      content_type: content type. str.
      metadata: a str->str dict of user specified options when creating
        the file. Possible keys are x-goog-meta-, content-disposition,
        content-encoding, and cache-control.
      is_dir: True if this represents a directory. False if this is a real file.
    """
    self.filename = filename
    self.is_dir = is_dir
    self.st_size = None
    self.st_ctime = None
    self.etag = None
    self.content_type = content_type
    self.metadata = metadata

    if not is_dir:
      self.st_size = long(st_size)
      self.st_ctime = float(st_ctime)
      if etag[0] == '"' and etag[-1] == '"':
        etag = etag[1:-1]
      self.etag = etag

  def __repr__(self):
    if self.is_dir:
      return '(directory: %s)' % self.filename

    return (
        '(filename: %(filename)s, st_size: %(st_size)s, '
        'st_ctime: %(st_ctime)s, etag: %(etag)s, '
        'content_type: %(content_type)s, '
        'metadata: %(metadata)s)' %
        dict(filename=self.filename,
             st_size=self.st_size,
             st_ctime=self.st_ctime,
             etag=self.etag,
             content_type=self.content_type,
             metadata=self.metadata))

  def __cmp__(self, other):
    if not isinstance(other, self.__class__):
      raise ValueError('Argument to cmp must have the same type. '
                       'Expect %s, got %s', self.__class__.__name__,
                       other.__class__.__name__)
    if self.filename > other.filename:
      return 1
    elif self.filename < other.filename:
      return -1
    return 0

  def __hash__(self):
    if self.etag:
      return hash(self.etag)
    return hash(self.filename)


CSFileStat = GCSFileStat


def get_metadata(headers):
  """Get user defined options from HTTP response headers."""
  return dict((k, v) for k, v in headers.iteritems()
              if any(k.lower().startswith(valid) for valid in _GCS_METADATA))


def validate_bucket_name(name):
  """Validate a Google Storage bucket name.

  Args:
    name: a Google Storage bucket name with no prefix or suffix.

  Raises:
    ValueError: if name is invalid.
  """
  _validate_path(name)
  if not _GCS_BUCKET_REGEX.match(name):
    raise ValueError('Bucket should be 3-63 characters long using only a-z,'
                     '0-9, underscore, dash or dot but got %s' % name)


def validate_bucket_path(path):
  """Validate a Google Cloud Storage bucket path.

  Args:
    path: a Google Storage bucket path. It should have form '/bucket'.

  Raises:
    ValueError: if path is invalid.
  """
  _validate_path(path)
  if not _GCS_BUCKET_PATH_REGEX.match(path):
    raise ValueError('Bucket should have format /bucket '
                     'but got %s' % path)


def validate_file_path(path):
  """Validate a Google Cloud Storage file path.

  Args:
    path: a Google Storage file path. It should have form '/bucket/filename'.

  Raises:
    ValueError: if path is invalid.
  """
  _validate_path(path)
  if not _GCS_FULLPATH_REGEX.match(path):
    raise ValueError('Path should have format /bucket/filename '
                     'but got %s' % path)


def _process_path_prefix(path_prefix):
  """Validate and process a Google Cloud Stoarge path prefix.

  Args:
    path_prefix: a Google Cloud Storage path prefix of format '/bucket/prefix'
      or '/bucket/' or '/bucket'.

  Raises:
    ValueError: if path is invalid.

  Returns:
    a tuple of /bucket and prefix. prefix can be None.
  """
  _validate_path(path_prefix)
  if not _GCS_PATH_PREFIX_REGEX.match(path_prefix):
    raise ValueError('Path prefix should have format /bucket, /bucket/, '
                     'or /bucket/prefix but got %s.' % path_prefix)
  bucket_name_end = path_prefix.find('/', 1)
  bucket = path_prefix
  prefix = None
  if bucket_name_end != -1:
    bucket = path_prefix[:bucket_name_end]
    prefix = path_prefix[bucket_name_end + 1:] or None
  return bucket, prefix


def _validate_path(path):
  """Basic validation of Google Storage paths.

  Args:
    path: a Google Storage path. It should have form '/bucket/filename'
      or '/bucket'.

  Raises:
    ValueError: if path is invalid.
    TypeError: if path is not of type basestring.
  """
  if not path:
    raise ValueError('Path is empty')
  if not isinstance(path, basestring):
    raise TypeError('Path should be a string but is %s (%s).' %
                    (path.__class__, path))


def validate_options(options):
  """Validate Google Cloud Storage options.

  Args:
    options: a str->basestring dict of options to pass to Google Cloud Storage.

  Raises:
    ValueError: if option is not supported.
    TypeError: if option is not of type str or value of an option
      is not of type basestring.
  """
  if not options:
    return

  for k, v in options.iteritems():
    if not isinstance(k, str):
      raise TypeError('option %r should be a str.' % k)
    if not any(k.lower().startswith(valid) for valid in _GCS_OPTIONS):
      raise ValueError('option %s is not supported.' % k)
    if not isinstance(v, basestring):
      raise TypeError('value %r for option %s should be of type basestring.' %
                      (v, k))


def http_time_to_posix(http_time):
  """Convert HTTP time format to posix time.

  See http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.3.1
  for http time format.

  Args:
    http_time: time in RFC 2616 format. e.g.
      "Mon, 20 Nov 1995 19:12:08 GMT".

  Returns:
    A float of secs from unix epoch.
  """
  if http_time is not None:
    return email_utils.mktime_tz(email_utils.parsedate_tz(http_time))


def posix_time_to_http(posix_time):
  """Convert posix time to HTML header time format.

  Args:
    posix_time: unix time.

  Returns:
    A datatime str in RFC 2616 format.
  """
  if posix_time:
    return email_utils.formatdate(posix_time, usegmt=True)


_DT_FORMAT = '%Y-%m-%dT%H:%M:%S'


def dt_str_to_posix(dt_str):
  """format str to posix.

  datetime str is of format %Y-%m-%dT%H:%M:%S.%fZ,
  e.g. 2013-04-12T00:22:27.978Z. According to ISO 8601, T is a separator
  between date and time when they are on the same line.
  Z indicates UTC (zero meridian).

  A pointer: http://www.cl.cam.ac.uk/~mgk25/iso-time.html

  This is used to parse LastModified node from GCS's GET bucket XML response.

  Args:
    dt_str: A datetime str.

  Returns:
    A float of secs from unix epoch. By posix definition, epoch is midnight
    1970/1/1 UTC.
  """
  parsable, _ = dt_str.split('.')
  dt = datetime.datetime.strptime(parsable, _DT_FORMAT)
  return calendar.timegm(dt.utctimetuple())


def posix_to_dt_str(posix):
  """Reverse of str_to_datetime.

  This is used by GCS stub to generate GET bucket XML response.

  Args:
    posix: A float of secs from unix epoch.

  Returns:
    A datetime str.
  """
  dt = datetime.datetime.utcfromtimestamp(posix)
  dt_str = dt.strftime(_DT_FORMAT)
  return dt_str + '.000Z'


def local_run():
  """Whether we should hit GCS dev appserver stub."""
  server_software = os.environ.get('SERVER_SOFTWARE')
  if server_software is None:
    return True
  if 'remote_api' in server_software:
    return False
  if server_software.startswith(('Development', 'testutil')):
    return True
  return False


def local_api_url():
  """Return URL for GCS emulation on dev appserver."""
  return 'http://%s%s' % (os.environ.get('HTTP_HOST'), LOCAL_GCS_ENDPOINT)


def memory_usage(method):
  """Log memory usage before and after a method."""
  def wrapper(*args, **kwargs):
    logging.info('Memory before method %s is %s.',
                 method.__name__, runtime.memory_usage().current())
    result = method(*args, **kwargs)
    logging.info('Memory after method %s is %s',
                 method.__name__, runtime.memory_usage().current())
    return result
  return wrapper


def _add_ns(tagname):
  return '{%(ns)s}%(tag)s' % {'ns': CS_XML_NS,
                              'tag': tagname}


_T_CONTENTS = _add_ns('Contents')
_T_LAST_MODIFIED = _add_ns('LastModified')
_T_ETAG = _add_ns('ETag')
_T_KEY = _add_ns('Key')
_T_SIZE = _add_ns('Size')
_T_PREFIX = _add_ns('Prefix')
_T_COMMON_PREFIXES = _add_ns('CommonPrefixes')
_T_NEXT_MARKER = _add_ns('NextMarker')
_T_IS_TRUNCATED = _add_ns('IsTruncated')

########NEW FILE########
__FILENAME__ = errors
# Copyright 2012 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
# either express or implied. See the License for the specific
# language governing permissions and limitations under the License.

"""Google Cloud Storage specific Files API calls."""





__all__ = ['AuthorizationError',
           'check_status',
           'Error',
           'FatalError',
           'FileClosedError',
           'ForbiddenError',
           'InvalidRange',
           'NotFoundError',
           'ServerError',
           'TimeoutError',
           'TransientError',
          ]

import httplib


class Error(Exception):
  """Base error for all gcs operations.

  Error can happen on GAE side or GCS server side.
  For details on a particular GCS HTTP response code, see
  https://developers.google.com/storage/docs/reference-status#standardcodes
  """


class TransientError(Error):
  """TransientError could be retried."""


class TimeoutError(TransientError):
  """HTTP 408 timeout."""


class FatalError(Error):
  """FatalError shouldn't be retried."""


class FileClosedError(FatalError):
  """File is already closed.

  This can happen when the upload has finished but 'write' is called on
  a stale upload handle.
  """


class NotFoundError(FatalError):
  """HTTP 404 resource not found."""


class ForbiddenError(FatalError):
  """HTTP 403 Forbidden.

  While GCS replies with a 403 error for many reasons, the most common one
  is due to bucket permission not correctly setup for your app to access.
  """


class AuthorizationError(FatalError):
  """HTTP 401 authentication required.

  Unauthorized request has been received by GCS.

  This error is mostly handled by GCS client. GCS client will request
  a new access token and retry the request.
  """


class InvalidRange(FatalError):
  """HTTP 416 RequestRangeNotSatifiable."""


class ServerError(TransientError):
  """HTTP >= 500 server side error."""


def check_status(status, expected, path, headers=None,
                 resp_headers=None, body=None, extras=None):
  """Check HTTP response status is expected.

  Args:
    status: HTTP response status. int.
    expected: a list of expected statuses. A list of ints.
    path: filename or a path prefix.
    headers: HTTP request headers.
    resp_headers: HTTP response headers.
    body: HTTP response body.
    extras: extra info to be logged verbatim if error occurs.

  Raises:
    AuthorizationError: if authorization failed.
    NotFoundError: if an object that's expected to exist doesn't.
    TimeoutError: if HTTP request timed out.
    ServerError: if server experienced some errors.
    FatalError: if any other unexpected errors occurred.
  """
  if status in expected:
    return

  msg = ('Expect status %r from Google Storage. But got status %d.\n'
         'Path: %r.\n'
         'Request headers: %r.\n'
         'Response headers: %r.\n'
         'Body: %r.\n'
         'Extra info: %r.\n' %
         (expected, status, path, headers, resp_headers, body, extras))

  if status == httplib.UNAUTHORIZED:
    raise AuthorizationError(msg)
  elif status == httplib.FORBIDDEN:
    raise ForbiddenError(msg)
  elif status == httplib.NOT_FOUND:
    raise NotFoundError(msg)
  elif status == httplib.REQUEST_TIMEOUT:
    raise TimeoutError(msg)
  elif status == httplib.REQUESTED_RANGE_NOT_SATISFIABLE:
    raise InvalidRange(msg)
  elif (status == httplib.OK and 308 in expected and
        httplib.OK not in expected):
    raise FileClosedError(msg)
  elif status >= 500:
    raise ServerError(msg)
  else:
    raise FatalError(msg)

########NEW FILE########
__FILENAME__ = rest_api
# Copyright 2012 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
# either express or implied. See the License for the specific
# language governing permissions and limitations under the License.

"""Base and helper classes for Google RESTful APIs."""





__all__ = ['add_sync_methods']

import random
import time

from . import api_utils

try:
  from google.appengine.api import app_identity
  from google.appengine.ext import ndb
except ImportError:
  from google.appengine.api import app_identity
  from google.appengine.ext import ndb



def _make_sync_method(name):
  """Helper to synthesize a synchronous method from an async method name.

  Used by the @add_sync_methods class decorator below.

  Args:
    name: The name of the synchronous method.

  Returns:
    A method (with first argument 'self') that retrieves and calls
    self.<name>, passing its own arguments, expects it to return a
    Future, and then waits for and returns that Future's result.
  """

  def sync_wrapper(self, *args, **kwds):
    method = getattr(self, name)
    future = method(*args, **kwds)
    return future.get_result()

  return sync_wrapper


def add_sync_methods(cls):
  """Class decorator to add synchronous methods corresponding to async methods.

  This modifies the class in place, adding additional methods to it.
  If a synchronous method of a given name already exists it is not
  replaced.

  Args:
    cls: A class.

  Returns:
    The same class, modified in place.
  """
  for name in cls.__dict__.keys():
    if name.endswith('_async'):
      sync_name = name[:-6]
      if not hasattr(cls, sync_name):
        setattr(cls, sync_name, _make_sync_method(name))
  return cls


class _AE_TokenStorage_(ndb.Model):
  """Entity to store app_identity tokens in memcache."""

  token = ndb.StringProperty()
  expires = ndb.FloatProperty()


@ndb.tasklet
def _make_token_async(scopes, service_account_id):
  """Get a fresh authentication token.

  Args:
    scopes: A list of scopes.
    service_account_id: Internal-use only.

  Raises:
    An ndb.Return with a tuple (token, expiration_time) where expiration_time is
    seconds since the epoch.
  """
  rpc = app_identity.create_rpc()
  app_identity.make_get_access_token_call(rpc, scopes, service_account_id)
  token, expires_at = yield rpc
  raise ndb.Return((token, expires_at))


class _RestApi(object):
  """Base class for REST-based API wrapper classes.

  This class manages authentication tokens and request retries.  All
  APIs are available as synchronous and async methods; synchronous
  methods are synthesized from async ones by the add_sync_methods()
  function in this module.

  WARNING: Do NOT directly use this api. It's an implementation detail
  and is subject to change at any release.
  """

  _TOKEN_EXPIRATION_HEADROOM = random.randint(60, 600)

  def __init__(self, scopes, service_account_id=None, token_maker=None,
               retry_params=None):
    """Constructor.

    Args:
      scopes: A scope or a list of scopes.
      service_account_id: Internal use only.
      token_maker: An asynchronous function of the form
        (scopes, service_account_id) -> (token, expires).
      retry_params: An instance of api_utils.RetryParams. If None, the
        default for current thread will be used.
    """

    if isinstance(scopes, basestring):
      scopes = [scopes]
    self.scopes = scopes
    self.service_account_id = service_account_id
    self.make_token_async = token_maker or _make_token_async
    if not retry_params:
      retry_params = api_utils._get_default_retry_params()
    self.retry_params = retry_params
    self.user_agent = {'User-Agent': retry_params._user_agent}

  def __getstate__(self):
    """Store state as part of serialization/pickling."""
    return {'scopes': self.scopes,
            'id': self.service_account_id,
            'a_maker': (None if self.make_token_async == _make_token_async
                        else self.make_token_async),
            'retry_params': self.retry_params}

  def __setstate__(self, state):
    """Restore state as part of deserialization/unpickling."""
    self.__init__(state['scopes'],
                  service_account_id=state['id'],
                  token_maker=state['a_maker'],
                  retry_params=state['retry_params'])

  @ndb.tasklet
  def do_request_async(self, url, method='GET', headers=None, payload=None,
                       deadline=None, callback=None):
    """Issue one HTTP request.

    It performs async retries using tasklets.

    Args:
      url: the url to fetch.
      method: the method in which to fetch.
      headers: the http headers.
      payload: the data to submit in the fetch.
      deadline: the deadline in which to make the call.
      callback: the call to make once completed.

    Yields:
      The async fetch of the url.
    """
    retry_wrapper = api_utils._RetryWrapper(
        self.retry_params,
        retriable_exceptions=api_utils._RETRIABLE_EXCEPTIONS,
        should_retry=api_utils._should_retry)
    resp = yield retry_wrapper.run(
        self.urlfetch_async,
        url=url,
        method=method,
        headers=headers,
        payload=payload,
        deadline=deadline,
        callback=callback,
        follow_redirects=False)
    raise ndb.Return((resp.status_code, resp.headers, resp.content))

  @ndb.tasklet
  def get_token_async(self, refresh=False):
    """Get an authentication token.

    The token is cached in memcache, keyed by the scopes argument.

    Args:
      refresh: If True, ignore a cached token; default False.

    Yields:
      An authentication token. This token is guaranteed to be non-expired.
    """
    key = '%s,%s' % (self.service_account_id, ','.join(self.scopes))
    ts = yield _AE_TokenStorage_.get_by_id_async(
        key, use_cache=True, use_memcache=True,
        use_datastore=self.retry_params.save_access_token)
    if refresh or ts is None or ts.expires < (
        time.time() + self._TOKEN_EXPIRATION_HEADROOM):
      token, expires_at = yield self.make_token_async(
          self.scopes, self.service_account_id)
      timeout = int(expires_at - time.time())
      ts = _AE_TokenStorage_(id=key, token=token, expires=expires_at)
      if timeout > 0:
        yield ts.put_async(memcache_timeout=timeout,
                           use_datastore=self.retry_params.save_access_token,
                           use_cache=True, use_memcache=True)
    raise ndb.Return(ts.token)

  @ndb.tasklet
  def urlfetch_async(self, url, method='GET', headers=None,
                     payload=None, deadline=None, callback=None,
                     follow_redirects=False):
    """Make an async urlfetch() call.

    This is an async wrapper around urlfetch(). It adds an authentication
    header.

    Args:
      url: the url to fetch.
      method: the method in which to fetch.
      headers: the http headers.
      payload: the data to submit in the fetch.
      deadline: the deadline in which to make the call.
      callback: the call to make once completed.
      follow_redirects: whether or not to follow redirects.

    Yields:
      This returns a Future despite not being decorated with @ndb.tasklet!
    """
    headers = {} if headers is None else dict(headers)
    headers.update(self.user_agent)
    self.token = yield self.get_token_async()
    headers['authorization'] = 'OAuth ' + self.token

    deadline = deadline or self.retry_params.urlfetch_timeout

    ctx = ndb.get_context()
    resp = yield ctx.urlfetch(
        url, payload=payload, method=method,
        headers=headers, follow_redirects=follow_redirects,
        deadline=deadline, callback=callback)
    raise ndb.Return(resp)


_RestApi = add_sync_methods(_RestApi)

########NEW FILE########
__FILENAME__ = storage_api
# Copyright 2012 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
# either express or implied. See the License for the specific
# language governing permissions and limitations under the License.

"""Python wrappers for the Google Storage RESTful API."""





__all__ = ['ReadBuffer',
           'StreamingBuffer',
          ]

import collections
import os
import urlparse

from . import api_utils
from . import common
from . import errors
from . import rest_api

try:
  from google.appengine.api import urlfetch
  from google.appengine.ext import ndb
except ImportError:
  from google.appengine.api import urlfetch
  from google.appengine.ext import ndb



def _get_storage_api(retry_params, account_id=None):
  """Returns storage_api instance for API methods.

  Args:
    retry_params: An instance of api_utils.RetryParams. If none,
     thread's default will be used.
    account_id: Internal-use only.

  Returns:
    A storage_api instance to handle urlfetch work to GCS.
    On dev appserver, this instance by default will talk to a local stub
    unless common.ACCESS_TOKEN is set. That token will be used to talk
    to the real GCS.
  """


  api = _StorageApi(_StorageApi.full_control_scope,
                    service_account_id=account_id,
                    retry_params=retry_params)
  if common.local_run() and not common.get_access_token():
    api.api_url = common.local_api_url()
  if common.get_access_token():
    api.token = common.get_access_token()
  return api


class _StorageApi(rest_api._RestApi):
  """A simple wrapper for the Google Storage RESTful API.

  WARNING: Do NOT directly use this api. It's an implementation detail
  and is subject to change at any release.

  All async methods have similar args and returns.

  Args:
    path: The path to the Google Storage object or bucket, e.g.
      '/mybucket/myfile' or '/mybucket'.
    **kwd: Options for urlfetch. e.g.
      headers={'content-type': 'text/plain'}, payload='blah'.

  Returns:
    A ndb Future. When fulfilled, future.get_result() should return
    a tuple of (status, headers, content) that represents a HTTP response
    of Google Cloud Storage XML API.
  """

  api_url = 'https://storage.googleapis.com'
  read_only_scope = 'https://www.googleapis.com/auth/devstorage.read_only'
  read_write_scope = 'https://www.googleapis.com/auth/devstorage.read_write'
  full_control_scope = 'https://www.googleapis.com/auth/devstorage.full_control'

  def __getstate__(self):
    """Store state as part of serialization/pickling.

    Returns:
      A tuple (of dictionaries) with the state of this object
    """
    return (super(_StorageApi, self).__getstate__(), {'api_url': self.api_url})

  def __setstate__(self, state):
    """Restore state as part of deserialization/unpickling.

    Args:
      state: the tuple from a __getstate__ call
    """
    superstate, localstate = state
    super(_StorageApi, self).__setstate__(superstate)
    self.api_url = localstate['api_url']

  @api_utils._eager_tasklet
  @ndb.tasklet
  def do_request_async(self, url, method='GET', headers=None, payload=None,
                       deadline=None, callback=None):
    """Inherit docs.

    This method translates urlfetch exceptions to more service specific ones.
    """
    if headers is None:
      headers = {}
    if 'x-goog-api-version' not in headers:
      headers['x-goog-api-version'] = '2'
    headers['accept-encoding'] = 'gzip, *'
    try:
      resp_tuple = yield super(_StorageApi, self).do_request_async(
          url, method=method, headers=headers, payload=payload,
          deadline=deadline, callback=callback)
    except urlfetch.DownloadError, e:
      raise errors.TimeoutError(
          'Request to Google Cloud Storage timed out.', e)

    raise ndb.Return(resp_tuple)


  def post_object_async(self, path, **kwds):
    """POST to an object."""
    return self.do_request_async(self.api_url + path, 'POST', **kwds)

  def put_object_async(self, path, **kwds):
    """PUT an object."""
    return self.do_request_async(self.api_url + path, 'PUT', **kwds)

  def get_object_async(self, path, **kwds):
    """GET an object.

    Note: No payload argument is supported.
    """
    return self.do_request_async(self.api_url + path, 'GET', **kwds)

  def delete_object_async(self, path, **kwds):
    """DELETE an object.

    Note: No payload argument is supported.
    """
    return self.do_request_async(self.api_url + path, 'DELETE', **kwds)

  def head_object_async(self, path, **kwds):
    """HEAD an object.

    Depending on request headers, HEAD returns various object properties,
    e.g. Content-Length, Last-Modified, and ETag.

    Note: No payload argument is supported.
    """
    return self.do_request_async(self.api_url + path, 'HEAD', **kwds)

  def get_bucket_async(self, path, **kwds):
    """GET a bucket."""
    return self.do_request_async(self.api_url + path, 'GET', **kwds)


_StorageApi = rest_api.add_sync_methods(_StorageApi)


class ReadBuffer(object):
  """A class for reading Google storage files."""

  DEFAULT_BUFFER_SIZE = 1024 * 1024
  MAX_REQUEST_SIZE = 30 * DEFAULT_BUFFER_SIZE

  def __init__(self,
               api,
               path,
               buffer_size=DEFAULT_BUFFER_SIZE,
               max_request_size=MAX_REQUEST_SIZE):
    """Constructor.

    Args:
      api: A StorageApi instance.
      path: Quoted/escaped path to the object, e.g. /mybucket/myfile
      buffer_size: buffer size. The ReadBuffer keeps
        one buffer. But there may be a pending future that contains
        a second buffer. This size must be less than max_request_size.
      max_request_size: Max bytes to request in one urlfetch.
    """
    self._api = api
    self._path = path
    self.name = api_utils._unquote_filename(path)
    self.closed = False

    assert buffer_size <= max_request_size
    self._buffer_size = buffer_size
    self._max_request_size = max_request_size
    self._offset = 0
    self._buffer = _Buffer()
    self._etag = None

    self._request_next_buffer()

    status, headers, content = self._api.head_object(path)
    errors.check_status(status, [200], path, resp_headers=headers, body=content)
    self._file_size = long(headers['content-length'])
    self._check_etag(headers.get('etag'))
    if self._file_size == 0:
      self._buffer_future = None

  def __getstate__(self):
    """Store state as part of serialization/pickling.

    The contents of the read buffer are not stored, only the current offset for
    data read by the client. A new read buffer is established at unpickling.
    The head information for the object (file size and etag) are stored to
    reduce startup and ensure the file has not changed.

    Returns:
      A dictionary with the state of this object
    """
    return {'api': self._api,
            'path': self._path,
            'buffer_size': self._buffer_size,
            'request_size': self._max_request_size,
            'etag': self._etag,
            'size': self._file_size,
            'offset': self._offset,
            'closed': self.closed}

  def __setstate__(self, state):
    """Restore state as part of deserialization/unpickling.

    Args:
      state: the dictionary from a __getstate__ call

    Along with restoring the state, pre-fetch the next read buffer.
    """
    self._api = state['api']
    self._path = state['path']
    self.name = api_utils._unquote_filename(self._path)
    self._buffer_size = state['buffer_size']
    self._max_request_size = state['request_size']
    self._etag = state['etag']
    self._file_size = state['size']
    self._offset = state['offset']
    self._buffer = _Buffer()
    self.closed = state['closed']
    self._buffer_future = None
    if self._remaining() and not self.closed:
      self._request_next_buffer()

  def __iter__(self):
    """Iterator interface.

    Note the ReadBuffer container itself is the iterator. It's
    (quote PEP0234)
    'destructive: they consumes all the values and a second iterator
    cannot easily be created that iterates independently over the same values.
    You could open the file for the second time, or seek() to the beginning.'

    Returns:
      Self.
    """
    return self

  def next(self):
    line = self.readline()
    if not line:
      raise StopIteration()
    return line

  def readline(self, size=-1):
    """Read one line delimited by '\n' from the file.

    A trailing newline character is kept in the string. It may be absent when a
    file ends with an incomplete line. If the size argument is non-negative,
    it specifies the maximum string size (counting the newline) to return.
    A negative size is the same as unspecified. Empty string is returned
    only when EOF is encountered immediately.

    Args:
      size: Maximum number of bytes to read. If not specified, readline stops
        only on '\n' or EOF.

    Returns:
      The data read as a string.

    Raises:
      IOError: When this buffer is closed.
    """
    self._check_open()
    if size == 0 or not self._remaining():
      return ''

    data_list = []
    newline_offset = self._buffer.find_newline(size)
    while newline_offset < 0:
      data = self._buffer.read(size)
      size -= len(data)
      self._offset += len(data)
      data_list.append(data)
      if size == 0 or not self._remaining():
        return ''.join(data_list)
      self._buffer.reset(self._buffer_future.get_result())
      self._request_next_buffer()
      newline_offset = self._buffer.find_newline(size)

    data = self._buffer.read_to_offset(newline_offset + 1)
    self._offset += len(data)
    data_list.append(data)

    return ''.join(data_list)

  def read(self, size=-1):
    """Read data from RAW file.

    Args:
      size: Number of bytes to read as integer. Actual number of bytes
        read is always equal to size unless EOF is reached. If size is
        negative or unspecified, read the entire file.

    Returns:
      data read as str.

    Raises:
      IOError: When this buffer is closed.
    """
    self._check_open()
    if not self._remaining():
      return ''

    data_list = []
    while True:
      remaining = self._buffer.remaining()
      if size >= 0 and size < remaining:
        data_list.append(self._buffer.read(size))
        self._offset += size
        break
      else:
        size -= remaining
        self._offset += remaining
        data_list.append(self._buffer.read())

        if self._buffer_future is None:
          if size < 0 or size >= self._remaining():
            needs = self._remaining()
          else:
            needs = size
          data_list.extend(self._get_segments(self._offset, needs))
          self._offset += needs
          break

        if self._buffer_future:
          self._buffer.reset(self._buffer_future.get_result())
          self._buffer_future = None

    if self._buffer_future is None:
      self._request_next_buffer()
    return ''.join(data_list)

  def _remaining(self):
    return self._file_size - self._offset

  def _request_next_buffer(self):
    """Request next buffer.

    Requires self._offset and self._buffer are in consistent state
    """
    self._buffer_future = None
    next_offset = self._offset + self._buffer.remaining()
    if not hasattr(self, '_file_size') or next_offset != self._file_size:
      self._buffer_future = self._get_segment(next_offset,
                                              self._buffer_size)

  def _get_segments(self, start, request_size):
    """Get segments of the file from Google Storage as a list.

    A large request is broken into segments to avoid hitting urlfetch
    response size limit. Each segment is returned from a separate urlfetch.

    Args:
      start: start offset to request. Inclusive. Have to be within the
        range of the file.
      request_size: number of bytes to request.

    Returns:
      A list of file segments in order
    """
    if not request_size:
      return []

    end = start + request_size
    futures = []

    while request_size > self._max_request_size:
      futures.append(self._get_segment(start, self._max_request_size))
      request_size -= self._max_request_size
      start += self._max_request_size
    if start < end:
      futures.append(self._get_segment(start, end-start))
    return [fut.get_result() for fut in futures]

  @ndb.tasklet
  def _get_segment(self, start, request_size):
    """Get a segment of the file from Google Storage.

    Args:
      start: start offset of the segment. Inclusive. Have to be within the
        range of the file.
      request_size: number of bytes to request. Have to be small enough
        for a single urlfetch request. May go over the logical range of the
        file.

    Yields:
      a segment [start, start + request_size) of the file.

    Raises:
      ValueError: if the file has changed while reading.
    """
    end = start + request_size - 1
    content_range = '%d-%d' % (start, end)
    headers = {'Range': 'bytes=' + content_range}
    status, resp_headers, content = yield self._api.get_object_async(
        self._path, headers=headers)
    errors.check_status(status, [200, 206], self._path, headers, resp_headers,
                        body=content)
    self._check_etag(resp_headers.get('etag'))
    raise ndb.Return(content)

  def _check_etag(self, etag):
    """Check if etag is the same across requests to GCS.

    If self._etag is None, set it. If etag is set, check that the new
    etag equals the old one.

    In the __init__ method, we fire one HEAD and one GET request using
    ndb tasklet. One of them would return first and set the first value.

    Args:
      etag: etag from a GCS HTTP response. None if etag is not part of the
        response header. It could be None for example in the case of GCS
        composite file.

    Raises:
      ValueError: if two etags are not equal.
    """
    if etag is None:
      return
    elif self._etag is None:
      self._etag = etag
    elif self._etag != etag:
      raise ValueError('File on GCS has changed while reading.')

  def close(self):
    self.closed = True
    self._buffer = None
    self._buffer_future = None

  def __enter__(self):
    return self

  def __exit__(self, atype, value, traceback):
    self.close()
    return False

  def seek(self, offset, whence=os.SEEK_SET):
    """Set the file's current offset.

    Note if the new offset is out of bound, it is adjusted to either 0 or EOF.

    Args:
      offset: seek offset as number.
      whence: seek mode. Supported modes are os.SEEK_SET (absolute seek),
        os.SEEK_CUR (seek relative to the current position), and os.SEEK_END
        (seek relative to the end, offset should be negative).

    Raises:
      IOError: When this buffer is closed.
      ValueError: When whence is invalid.
    """
    self._check_open()

    self._buffer.reset()
    self._buffer_future = None

    if whence == os.SEEK_SET:
      self._offset = offset
    elif whence == os.SEEK_CUR:
      self._offset += offset
    elif whence == os.SEEK_END:
      self._offset = self._file_size + offset
    else:
      raise ValueError('Whence mode %s is invalid.' % str(whence))

    self._offset = min(self._offset, self._file_size)
    self._offset = max(self._offset, 0)
    if self._remaining():
      self._request_next_buffer()

  def tell(self):
    """Tell the file's current offset.

    Returns:
      current offset in reading this file.

    Raises:
      IOError: When this buffer is closed.
    """
    self._check_open()
    return self._offset

  def _check_open(self):
    if self.closed:
      raise IOError('Buffer is closed.')

  def seekable(self):
    return True

  def readable(self):
    return True

  def writable(self):
    return False


class _Buffer(object):
  """In memory buffer."""

  def __init__(self):
    self.reset()

  def reset(self, content='', offset=0):
    self._buffer = content
    self._offset = offset

  def read(self, size=-1):
    """Returns bytes from self._buffer and update related offsets.

    Args:
      size: number of bytes to read starting from current offset.
        Read the entire buffer if negative.

    Returns:
      Requested bytes from buffer.
    """
    if size < 0:
      offset = len(self._buffer)
    else:
      offset = self._offset + size
    return self.read_to_offset(offset)

  def read_to_offset(self, offset):
    """Returns bytes from self._buffer and update related offsets.

    Args:
      offset: read from current offset to this offset, exclusive.

    Returns:
      Requested bytes from buffer.
    """
    assert offset >= self._offset
    result = self._buffer[self._offset: offset]
    self._offset += len(result)
    return result

  def remaining(self):
    return len(self._buffer) - self._offset

  def find_newline(self, size=-1):
    """Search for newline char in buffer starting from current offset.

    Args:
      size: number of bytes to search. -1 means all.

    Returns:
      offset of newline char in buffer. -1 if doesn't exist.
    """
    if size < 0:
      return self._buffer.find('\n', self._offset)
    return self._buffer.find('\n', self._offset, self._offset + size)


class StreamingBuffer(object):
  """A class for creating large objects using the 'resumable' API.

  The API is a subset of the Python writable stream API sufficient to
  support writing zip files using the zipfile module.

  The exact sequence of calls and use of headers is documented at
  https://developers.google.com/storage/docs/developer-guide#unknownresumables
  """

  _blocksize = 256 * 1024

  _flushsize = 8 * _blocksize

  _maxrequestsize = 9 * 4 * _blocksize

  def __init__(self,
               api,
               path,
               content_type=None,
               gcs_headers=None):
    """Constructor.

    Args:
      api: A StorageApi instance.
      path: Quoted/escaped path to the object, e.g. /mybucket/myfile
      content_type: Optional content-type; Default value is
        delegate to Google Cloud Storage.
      gcs_headers: additional gs headers as a str->str dict, e.g
        {'x-goog-acl': 'private', 'x-goog-meta-foo': 'foo'}.
    Raises:
      IOError: When this location can not be found.
    """
    assert self._maxrequestsize > self._blocksize
    assert self._maxrequestsize % self._blocksize == 0
    assert self._maxrequestsize >= self._flushsize

    self._api = api
    self._path = path

    self.name = api_utils._unquote_filename(path)
    self.closed = False

    self._buffer = collections.deque()
    self._buffered = 0
    self._written = 0
    self._offset = 0

    headers = {'x-goog-resumable': 'start'}
    if content_type:
      headers['content-type'] = content_type
    if gcs_headers:
      headers.update(gcs_headers)
    status, resp_headers, content = self._api.post_object(path, headers=headers)
    errors.check_status(status, [201], path, headers, resp_headers,
                        body=content)
    loc = resp_headers.get('location')
    if not loc:
      raise IOError('No location header found in 201 response')
    parsed = urlparse.urlparse(loc)
    self._path_with_token = '%s?%s' % (self._path, parsed.query)

  def __getstate__(self):
    """Store state as part of serialization/pickling.

    The contents of the write buffer are stored. Writes to the underlying
    storage are required to be on block boundaries (_blocksize) except for the
    last write. In the worst case the pickled version of this object may be
    slightly larger than the blocksize.

    Returns:
      A dictionary with the state of this object

    """
    return {'api': self._api,
            'path': self._path,
            'path_token': self._path_with_token,
            'buffer': self._buffer,
            'buffered': self._buffered,
            'written': self._written,
            'offset': self._offset,
            'closed': self.closed}

  def __setstate__(self, state):
    """Restore state as part of deserialization/unpickling.

    Args:
      state: the dictionary from a __getstate__ call
    """
    self._api = state['api']
    self._path_with_token = state['path_token']
    self._buffer = state['buffer']
    self._buffered = state['buffered']
    self._written = state['written']
    self._offset = state['offset']
    self.closed = state['closed']
    self._path = state['path']
    self.name = api_utils._unquote_filename(self._path)

  def write(self, data):
    """Write some bytes.

    Args:
      data: data to write. str.

    Raises:
      TypeError: if data is not of type str.
    """
    self._check_open()
    if not isinstance(data, str):
      raise TypeError('Expected str but got %s.' % type(data))
    if not data:
      return
    self._buffer.append(data)
    self._buffered += len(data)
    self._offset += len(data)
    if self._buffered >= self._flushsize:
      self._flush()

  def flush(self):
    """Flush as much as possible to GCS.

    GCS *requires* that all writes except for the final one align on
    256KB boundaries. So the internal buffer may still have < 256KB bytes left
    after flush.
    """
    self._check_open()
    self._flush(finish=False)

  def tell(self):
    """Return the total number of bytes passed to write() so far.

    (There is no seek() method.)
    """
    return self._offset

  def close(self):
    """Flush the buffer and finalize the file.

    When this returns the new file is available for reading.
    """
    if not self.closed:
      self.closed = True
      self._flush(finish=True)
      self._buffer = None

  def __enter__(self):
    return self

  def __exit__(self, atype, value, traceback):
    self.close()
    return False

  def _flush(self, finish=False):
    """Internal API to flush.

    Buffer is flushed to GCS only when the total amount of buffered data is at
    least self._blocksize, or to flush the final (incomplete) block of
    the file with finish=True.
    """
    while ((finish and self._buffered >= 0) or
           (not finish and self._buffered >= self._blocksize)):
      tmp_buffer = []
      tmp_buffer_len = 0

      excess = 0
      while self._buffer:
        buf = self._buffer.popleft()
        size = len(buf)
        self._buffered -= size
        tmp_buffer.append(buf)
        tmp_buffer_len += size
        if tmp_buffer_len >= self._maxrequestsize:
          excess = tmp_buffer_len - self._maxrequestsize
          break
        if not finish and (
            tmp_buffer_len % self._blocksize + self._buffered <
            self._blocksize):
          excess = tmp_buffer_len % self._blocksize
          break

      if excess:
        over = tmp_buffer.pop()
        size = len(over)
        assert size >= excess
        tmp_buffer_len -= size
        head, tail = over[:-excess], over[-excess:]
        self._buffer.appendleft(tail)
        self._buffered += len(tail)
        if head:
          tmp_buffer.append(head)
          tmp_buffer_len += len(head)

      data = ''.join(tmp_buffer)
      file_len = '*'
      if finish and not self._buffered:
        file_len = self._written + len(data)
      self._send_data(data, self._written, file_len)
      self._written += len(data)
      if file_len != '*':
        break

  def _send_data(self, data, start_offset, file_len):
    """Send the block to the storage service.

    This is a utility method that does not modify self.

    Args:
      data: data to send in str.
      start_offset: start offset of the data in relation to the file.
      file_len: an int if this is the last data to append to the file.
        Otherwise '*'.
    """
    headers = {}
    end_offset = start_offset + len(data) - 1

    if data:
      headers['content-range'] = ('bytes %d-%d/%s' %
                                  (start_offset, end_offset, file_len))
    else:
      headers['content-range'] = ('bytes */%s' % file_len)

    status, response_headers, content = self._api.put_object(
        self._path_with_token, payload=data, headers=headers)
    if file_len == '*':
      expected = 308
    else:
      expected = 200
    errors.check_status(status, [expected], self._path, headers,
                        response_headers, content,
                        {'upload_path': self._path_with_token})

  def _get_offset_from_gcs(self):
    """Get the last offset that has been written to GCS.

    This is a utility method that does not modify self.

    Returns:
      an int of the last offset written to GCS by this upload, inclusive.
      -1 means nothing has been written.
    """
    headers = {'content-range': 'bytes */*'}
    status, response_headers, content = self._api.put_object(
        self._path_with_token, headers=headers)
    errors.check_status(status, [308], self._path, headers,
                        response_headers, content,
                        {'upload_path': self._path_with_token})
    val = response_headers.get('range')
    if val is None:
      return -1
    _, offset = val.rsplit('-', 1)
    return int(offset)

  def _force_close(self, file_length=None):
    """Close this buffer on file_length.

    Finalize this upload immediately on file_length.
    Contents that are still in memory will not be uploaded.

    This is a utility method that does not modify self.

    Args:
      file_length: file length. Must match what has been uploaded. If None,
        it will be queried from GCS.
    """
    if file_length is None:
      file_length = self._get_offset_from_gcs() + 1
    self._send_data('', 0, file_length)

  def _check_open(self):
    if self.closed:
      raise IOError('Buffer is closed.')

  def seekable(self):
    return False

  def readable(self):
    return False

  def writable(self):
    return True

########NEW FILE########
__FILENAME__ = test_utils
# Copyright 2013 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
# either express or implied. See the License for the specific
# language governing permissions and limitations under the License.

"""Utils for testing."""


class MockUrlFetchResult(object):

  def __init__(self, status, headers, body):
    self.status_code = status
    self.headers = headers
    self.content = body
    self.content_was_truncated = False
    self.final_url = None

########NEW FILE########
__FILENAME__ = my_filters
from google.appengine.ext import webapp
import re

# More info on custom Django template filters here:
# https://docs.djangoproject.com/en/dev/howto/custom-template-tags/#registering-custom-filters

register = webapp.template.create_template_register()


@register.filter
def digits(value):
    return re.sub('[^0-9]', '', value)

########NEW FILE########
__FILENAME__ = award_type
class AwardType(object):
    """
    An award type defines a logical type of award that an award falls into.
    These types are the same across both years and competitions within a year.
    In other words, an industrial design award from 2013casj and
    2010cmp will be of award type AwardType.INDUSTRIAL_DESIGN.

    An award type must be enumerated for every type of award ever awarded.
    ONCE A TYPE IS ENUMERATED, IT MUST NOT BE CHANGED.

    Award types don't care about what type of event (Regional, District,
    District Championship, Championship Division, Championship Finals, etc.)
    the award is from. In other words, RCA and CCA are of the same award type.
    """
    CHAIRMANS = 0
    WINNER = 1
    FINALIST = 2

    WOODIE_FLOWERS = 3
    DEANS_LIST = 4
    VOLUNTEER = 5
    FOUNDERS = 6
    BART_KAMEN_MEMORIAL = 7
    MAKE_IT_LOUD = 8

    ENGINEERING_INSPIRATION = 9
    ROOKIE_ALL_STAR = 10
    GRACIOUS_PROFESSIONALISM = 11
    COOPERTITION = 12
    JUDGES = 13
    HIGHEST_ROOKIE_SEED = 14
    ROOKIE_INSPIRATION = 15
    INDUSTRIAL_DEESIGN = 16
    QUALITY = 17
    SAFETY = 18
    SPORTSMANSHIP = 19
    CREATIVITY = 20
    ENGINEERING_EXCELLENCE = 21
    ENTREPRENEURSHIP = 22
    EXCELLENCE_IN_DESIGN = 23
    EXCELLENCE_IN_DESIGN_CAD = 24
    EXCELLENCE_IN_DESIGN_ANIMATION = 25
    DRIVING_TOMORROWS_TECHNOLOGY = 26
    IMAGERY = 27
    MEDIA_AND_TECHNOLOGY = 28
    INNOVATION_IN_CONTROL = 29
    SPIRIT = 30
    WEBSITE = 31
    VISUALIZATION = 32
    AUTODESK_INVENTOR = 33
    FUTURE_INNOVATOR = 34
    RECOGNITION_OF_EXTRAORDINARY_SERVICE = 35
    OUTSTANDING_CART = 36
    WSU_AIM_HIGHER = 37
    LEADERSHIP_IN_CONTROL = 38
    NUM_1_SEED = 39
    INCREDIBLE_PLAY = 40
    PEOPLES_CHOICE_ANIMATION = 41
    VISUALIZATION_RISING_STAR = 42
    BEST_OFFENSIVE_ROUND = 43
    BEST_PLAY_OF_THE_DAY = 44
    FEATHERWEIGHT_IN_THE_FINALS = 45
    MOST_PHOTOGENIC = 46
    OUTSTANDING_DEFENSE = 47
    POWER_TO_SIMPLIFY = 48
    AGAINST_ALL_ODDS = 49
    RISING_STAR = 50
    CHAIRMANS_HONORABLE_MENTION = 51
    CONTENT_COMMUNICATION_HONORABLE_MENTION = 52
    TECHNICAL_EXECUTION_HONORABLE_MENTION = 53
    REALIZATION = 54
    REALIZATION_HONORABLE_MENTION = 55
    DESIGN_YOUR_FUTURE = 56
    DESIGN_YOUR_FUTURE_HONORABLE_MENTION = 57
    SPECIAL_RECOGNITION_CHARACTER_ANIMATION = 58
    HIGH_SCORE = 59
    TEACHER_PIONEER = 60
    BEST_CRAFTSMANSHIP = 61
    BEST_DEFENSIVE_MATCH = 62
    PLAY_OF_THE_DAY = 63

    BLUE_BANNER_AWARDS = {CHAIRMANS, WINNER}
    INDIVIDUAL_AWARDS = {WOODIE_FLOWERS, DEANS_LIST, VOLUNTEER, FOUNDERS,
                         BART_KAMEN_MEMORIAL, MAKE_IT_LOUD}

########NEW FILE########
__FILENAME__ = district_type
class DistrictType(object):
    """
    Constants for what district an Event is a part of
    """
    # Event is not a district event
    NO_DISTRICT = 0

    MICHIGAN = 1
    MID_ATLANTIC = 2
    NEW_ENGLAND = 3
    PACIFIC_NORTHWEST = 4

    # Used for rendering
    type_names = {
        NO_DISTRICT: None,
        MICHIGAN: 'Michigan',
        MID_ATLANTIC: 'Mid Atlantic',
        NEW_ENGLAND: 'New England',
        PACIFIC_NORTHWEST: 'Pacific Northwest',
    }

    # Names used on the FIRST website
    names = {
        'FIRST in Michigan': MICHIGAN,
        'Mid-Atlantic Robotics': MID_ATLANTIC,
        'New England': NEW_ENGLAND,
        'Pacific Northwest': PACIFIC_NORTHWEST,
    }

########NEW FILE########
__FILENAME__ = event_type
class EventType(object):
    REGIONAL = 0
    DISTRICT = 1
    DISTRICT_CMP = 2
    CMP_DIVISION = 3
    CMP_FINALS = 4
    OFFSEASON = 99
    PRESEASON = 100
    UNLABLED = -1

    type_names = {
        REGIONAL: 'Regional',
        DISTRICT: 'District',
        DISTRICT_CMP: 'District Championship',
        CMP_DIVISION: 'Championship Division',
        CMP_FINALS: 'Championship Finals',
        OFFSEASON: 'Offseason',
        PRESEASON: 'Preseason',
        UNLABLED: '--',
    }

    NON_CMP_EVENT_TYPES = {
        REGIONAL,
        DISTRICT,
        DISTRICT_CMP,
    }

    CMP_EVENT_TYPES = {
        CMP_DIVISION,
        CMP_FINALS,
    }

########NEW FILE########
__FILENAME__ = media_type
class MediaType(object):
    # ndb keys are based on these! Don't change!
    YOUTUBE = 0
    CD_PHOTO_THREAD = 1

    type_names = {
        YOUTUBE: 'YouTube Video',
        CD_PHOTO_THREAD: 'Chief Delphi Photo Thread',
    }

########NEW FILE########
__FILENAME__ = account_controller
import os

from google.appengine.ext.webapp import template

from base_controller import LoggedInHandler

from models.account import Account


class AccountOverview(LoggedInHandler):
    def get(self):
        self._require_login('/account')
        # Redirects to registration page if account not registered
        if not self.user_bundle.account.registered:
            self.redirect('/account/register')
            return None
        path = os.path.join(os.path.dirname(__file__), '../templates/account_overview.html')
        self.response.out.write(template.render(path, self.template_values))


class AccountEdit(LoggedInHandler):
    def get(self):
        self._require_login('/account/edit')
        if not self.user_bundle.account.registered:
            self.redirect('/account/register')
            return None

        path = os.path.join(os.path.dirname(__file__), '../templates/account_edit.html')
        self.response.out.write(template.render(path, self.template_values))

    def post(self):
        self._require_login('/account/edit')
        if not self.user_bundle.account.registered:
            self.redirect('/account/register')
            return None

        # Check to make sure that they aren't trying to edit another user
        real_account_id = self.user_bundle.account.key.id()
        check_account_id = self.request.get('account_id')
        if check_account_id == real_account_id:
            user = Account.get_by_id(self.user_bundle.account.key.id())
            user.display_name = self.request.get('display_name')
            user.put()
            self.redirect('/account')
        else:
            self.redirect('/')


class AccountRegister(LoggedInHandler):
    def get(self):
        self._require_login('/account/register')
        # Redirects to account overview page if already registered
        if self.user_bundle.account.registered:
            self.redirect('/account')
            return None

        path = os.path.join(os.path.dirname(__file__), '../templates/account_register.html')
        self.response.out.write(template.render(path, self.template_values))

    def post(self):
        self._require_login('/account/register')
        if self.user_bundle.account.registered:
            self.redirect('/account')
            return None

        # Check to make sure that they aren't trying to edit another user
        real_account_id = self.user_bundle.account.key.id()
        check_account_id = self.request.get('account_id')
        if check_account_id == real_account_id:
            account = Account.get_by_id(self.user_bundle.account.key.id())
            account.display_name = self.request.get('display_name')
            account.registered = True
            account.put()
            self.redirect('/account')
        else:
            self.redirect('/')


class AccountLogout(LoggedInHandler):
    def get(self):
        if os.environ.get('SERVER_SOFTWARE', '').startswith('Development/'):
            self.redirect(self.user_bundle.logout_url)
            return

        # Deletes the session cookies pertinent to TBA without touching Google session(s)
        # Reference: http://ptspts.blogspot.ca/2011/12/how-to-log-out-from-appengine-app-only.html
        response = self.redirect('/')
        response.delete_cookie('ACSID')
        response.delete_cookie('SACSID')

        return response

########NEW FILE########
__FILENAME__ = admin_award_controller
import json
import logging
import os

from google.appengine.ext import ndb
from google.appengine.ext.webapp import template

from controllers.base_controller import LoggedInHandler
from datafeeds.csv_awards_parser import CSVAwardsParser
from helpers.award_manipulator import AwardManipulator
from models.award import Award
from models.event import Event
from models.team import Team


class AdminAwardDashboard(LoggedInHandler):
    """
    Show stats about Awards
    """
    def get(self):
        self._require_admin()
        award_count = Award.query().count()

        self.template_values.update({
            "award_count": award_count
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/award_dashboard.html')
        self.response.out.write(template.render(path, self.template_values))


class AdminAwardAdd(LoggedInHandler):
    """
    Add Award from CSV.
    """
    def post(self):
        self._require_admin()
        awards_csv = self.request.get('awards_csv')

        events = {}  # for reducing datastore fetches of events and teams
        awards = []
        for award in CSVAwardsParser.parse(awards_csv):
            event_key_name = '{}{}'.format(award['year'], award['event_short'])
            if event_key_name in events:
                event = events[event_key_name]
            else:
                event = Event.get_by_id(event_key_name)
                if event is None:
                    logging.warning("Event: {} doesn't exist!".format(event_key_name))
                    continue
                events[event_key_name] = event

            awards.append(Award(
                id=Award.render_key_name(event.key_name, award['award_type_enum']),
                name_str=award['name_str'],
                award_type_enum=award['award_type_enum'],
                year=event.year,
                event=event.key,
                event_type_enum=event.event_type_enum,
                team_list=[ndb.Key(Team, 'frc{}'.format(team_number)) for team_number in award['team_number_list']],
                recipient_json_list=award['recipient_json_list']
            ))

        new_awards = AwardManipulator.createOrUpdate(awards)
        if type(new_awards) != list:
            new_awards = [new_awards]

        template_values = {
            'awards': new_awards,
        }
        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/awards_add.html')
        self.response.out.write(template.render(path, template_values))


class AdminAwardEdit(LoggedInHandler):
    """
    Edit an Award.
    """
    def get(self, award_key):
        self._require_admin()
        award = Award.get_by_id(award_key)

        self.template_values.update({
            "award": award
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/award_edit.html')
        self.response.out.write(template.render(path, self.template_values))

    def post(self, award_key):
        self._require_admin()

        event_key_name = self.request.get('event_key_name')

        recipient_json_list = []
        team_list = []
        for recipient in json.loads(self.request.get('recipient_list_json')):
            recipient_json_list.append(json.dumps(recipient))
            if recipient['team_number'] is not None:
                team_list.append(ndb.Key(Team, 'frc{}'.format(recipient['team_number'])))

        award = Award(
            id=award_key,
            name_str=self.request.get('name_str'),
            award_type_enum=int(self.request.get('award_type_enum')),
            event=ndb.Key(Event, event_key_name),
            event_type_enum=int(self.request.get('event_type_enum')),
            team_list=team_list,
            recipient_json_list=recipient_json_list,
        )
        award = AwardManipulator.createOrUpdate(award, auto_union=False)
        self.redirect("/admin/event/" + event_key_name)

########NEW FILE########
__FILENAME__ = admin_event_controller
from datetime import datetime
import json
import logging
import os

from google.appengine.ext import ndb
from google.appengine.ext.webapp import template

from controllers.base_controller import LoggedInHandler
from datafeeds.csv_alliance_selections_parser import CSVAllianceSelectionsParser
from datafeeds.csv_teams_parser import CSVTeamsParser
from helpers.event.event_test_creator import EventTestCreator
from helpers.event.event_webcast_adder import EventWebcastAdder
from helpers.event_helper import EventHelper
from helpers.event_manipulator import EventManipulator
from helpers.event_team_manipulator import EventTeamManipulator
from helpers.team_manipulator import TeamManipulator
from helpers.match_manipulator import MatchManipulator
from helpers.memcache.memcache_webcast_flusher import MemcacheWebcastFlusher
from models.award import Award
from models.event import Event
from models.event_team import EventTeam
from models.match import Match
from models.team import Team

import tba_config


class AdminEventAddAllianceSelections(LoggedInHandler):
    """
    Add alliance selections to an Event.
    """
    def post(self, event_key_id):
        self._require_admin()
        event = Event.get_by_id(event_key_id)

        alliance_selections_csv = self.request.get('alliance_selections_csv')
        alliance_selections = CSVAllianceSelectionsParser.parse(alliance_selections_csv)

        if alliance_selections and event.alliance_selections != alliance_selections:
            event.alliance_selections_json = json.dumps(alliance_selections)
            event._alliance_selections = None
            event.dirty = True

        EventManipulator.createOrUpdate(event)

        self.redirect("/admin/event/" + event.key_name)


class AdminEventAddTeams(LoggedInHandler):
    """
    Add a teams to an Event. Useful for legacy and offseason events.
    """
    def post(self, event_key_id):
        self._require_admin()
        event = Event.get_by_id(event_key_id)

        teams_csv = self.request.get('teams_csv')
        team_numbers = CSVTeamsParser.parse(teams_csv)

        event_teams = []
        teams = []
        for team_number in team_numbers:
            event_teams.append(EventTeam(id=event.key.id() + '_frc{}'.format(team_number),
                                         event=event.key,
                                         team=ndb.Key(Team, 'frc{}'.format(team_number)),
                                         year=event.year))
            teams.append(Team(id='frc{}'.format(team_number),
                              team_number=int(team_number)))

        EventTeamManipulator.createOrUpdate(event_teams)
        TeamManipulator.createOrUpdate(teams)

        self.redirect("/admin/event/" + event.key_name)


class AdminEventAddWebcast(LoggedInHandler):
    """
    Add a webcast to an Event.
    """
    def post(self, event_key_id):
        self._require_admin()

        webcast = dict()
        webcast["type"] = self.request.get("webcast_type")
        webcast["channel"] = self.request.get("webcast_channel")
        if self.request.get("webcast_file"):
            webcast["file"] = self.request.get("webcast_file")

        event = Event.get_by_id(event_key_id)
        EventWebcastAdder.add_webcast(event, webcast)

        self.redirect("/admin/event/" + event.key_name)


class AdminEventCreate(LoggedInHandler):
    """
    Create an Event. POSTs to AdminEventEdit.
    """
    def get(self):
        self._require_admin()

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/event_create.html')
        self.response.out.write(template.render(path, self.template_values))


class AdminEventCreateTest(LoggedInHandler):
    """
    Create a test event that is happening now.
    """
    def get(self):
        self._require_admin()

        if tba_config.CONFIG["env"] != "prod":
            EventTestCreator.createPastEvent()
            EventTestCreator.createFutureEvent()
            EventTestCreator.createPresentEvent()
            self.redirect("/events/")
        else:
            logging.error("{} tried to create test events in prod! No can do.".format(
                self.user_bundle.user.email()))
            self.redirect("/admin/")


class AdminEventDelete(LoggedInHandler):
    """
    Delete an Event.
    """
    def get(self, event_key_id):
        self._require_admin()

        event = Event.get_by_id(event_key_id)

        self.template_values.update({
            "event": event
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/event_delete.html')
        self.response.out.write(template.render(path, self.template_values))

    def post(self, event_key_id):
        self._require_admin()

        logging.warning("Deleting %s at the request of %s / %s" % (
            event_key_id,
            self.user_bundle.user.user_id(),
            self.user_bundle.user.email()))

        event = Event.get_by_id(event_key_id)

        matches = Match.query(Match.event == event.key).fetch(5000)
        MatchManipulator.delete(matches)

        event_teams = EventTeam.query(EventTeam.event == event.key).fetch(5000)
        EventTeamManipulator.delete(event_teams)

        EventManipulator.delete(event)

        self.redirect("/admin/events?deleted=%s" % event_key_id)


class AdminEventDetail(LoggedInHandler):
    """
    Show an Event.
    """
    def get(self, event_key):
        self._require_admin()

        event = Event.get_by_id(event_key)
        event.prepAwardsMatchesTeams()

        self.template_values.update({
            "event": event
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/event_details.html')
        self.response.out.write(template.render(path, self.template_values))


class AdminEventEdit(LoggedInHandler):
    """
    Edit an Event.
    """
    def get(self, event_key):
        self._require_admin()

        event = Event.get_by_id(event_key)

        self.template_values.update({
            "event": event
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/event_edit.html')
        self.response.out.write(template.render(path, self.template_values))

    def post(self, event_key):
        self._require_admin()

        # Note, we don't actually use event_key.

        start_date = None
        if self.request.get("start_date"):
            start_date = datetime.strptime(self.request.get("start_date"), "%Y-%m-%d")

        end_date = None
        if self.request.get("end_date"):
            end_date = datetime.strptime(self.request.get("end_date"), "%Y-%m-%d")

        event = Event(
            id=str(self.request.get("year")) + str.lower(str(self.request.get("event_short"))),
            end_date=end_date,
            event_short=self.request.get("event_short"),
            event_type_enum=EventHelper.parseEventType(self.request.get("event_type_str")),
            event_district_enum=EventHelper.parseDistrictName(self.request.get("event_district_str")),
            location=self.request.get("location"),
            name=self.request.get("name"),
            short_name=self.request.get("short_name"),
            start_date=start_date,
            website=self.request.get("website"),
            year=int(self.request.get("year")),
            official={"true": True, "false": False}.get(self.request.get("official").lower()),
            facebook_eid=self.request.get("facebook_eid"),
            webcast_json=self.request.get("webcast_json"),
            alliance_selections_json=self.request.get("alliance_selections_json"),
            rankings_json=self.request.get("rankings_json"),
        )
        event = EventManipulator.createOrUpdate(event)

        MemcacheWebcastFlusher.flushEvent(event.key_name)

        self.redirect("/admin/event/" + event.key_name)


class AdminEventList(LoggedInHandler):
    """
    List all Events.
    """
    VALID_YEARS = range(1992, datetime.now().year + 1)

    def get(self, year=None):
        self._require_admin()

        if year is not None:
            year = int(year)
        else:
            year = datetime.now().year

        events = Event.query(Event.year == year).order(Event.start_date).fetch(10000)

        self.template_values.update({
            "valid_years": self.VALID_YEARS,
            "selected_year": year,
            "events": events,
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/event_list.html')
        self.response.out.write(template.render(path, self.template_values))

########NEW FILE########
__FILENAME__ = admin_main_controller
import os
import json
import re
import logging
import datetime

from google.appengine.api import memcache
from google.appengine.ext.webapp import template

from controllers.base_controller import LoggedInHandler
from models.account import Account
from models.suggestion import Suggestion


class AdminMain(LoggedInHandler):
    def get(self):
        self._require_admin()

        self.template_values['memcache_stats'] = memcache.get_stats()

        # Gets the 5 recently created users
        users = Account.query().order(-Account.created).fetch(5)
        self.template_values['users'] = users

        # Retrieves the number of pending suggestions
        video_suggestions = Suggestion.query().filter(
            Suggestion.review_state == Suggestion.REVIEW_PENDING).filter(
            Suggestion.target_model == "match").count()
        self.template_values['video_suggestions'] = video_suggestions

        webcast_suggestions = Suggestion.query().filter(
            Suggestion.review_state == Suggestion.REVIEW_PENDING).filter(
            Suggestion.target_model == "event").count()
        self.template_values['webcast_suggestions'] = webcast_suggestions

        media_suggestions = Suggestion.query().filter(
            Suggestion.review_state == Suggestion.REVIEW_PENDING).filter(
            Suggestion.target_model == "media").count()
        self.template_values['media_suggestions'] = media_suggestions

        # version info
        try:
            fname = os.path.join(os.path.dirname(__file__), '../../version_info.json')

            with open(fname, 'r') as f:
                data = json.loads(f.read().replace('\r\n', '\n'))

            self.template_values['git_branch_name'] = data['git_branch_name']
            self.template_values['build_time'] = data['build_time']

            commit_parts = re.split("[\n]+", data['git_last_commit'])
            self.template_values['commit_hash'] = commit_parts[0].split(" ")
            self.template_values['commit_author'] = commit_parts[1]
            self.template_values['commit_date'] = commit_parts[2]
            self.template_values['commit_msg'] = commit_parts[3]

        except Exception, e:
            logging.warning("version_info.json parsing failed: %s" % e)
            pass

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/index.html')
        self.response.out.write(template.render(path, self.template_values))


class AdminDebugHandler(LoggedInHandler):
    def get(self):
        self._require_admin()
        self.template_values['cur_year'] = datetime.datetime.now().year
        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/debug.html')
        self.response.out.write(template.render(path, self.template_values))


class AdminTasksHandler(LoggedInHandler):
    def get(self):
        self._require_admin()
        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/tasks.html')
        self.response.out.write(template.render(path, self.template_values))

########NEW FILE########
__FILENAME__ = admin_match_controller
import json
import logging
import os

from google.appengine.ext import ndb
from google.appengine.ext.webapp import template

from controllers.base_controller import LoggedInHandler
from datafeeds.offseason_matches_parser import OffseasonMatchesParser
from helpers.firebase.firebase_pusher import FirebasePusher
from helpers.match_manipulator import MatchManipulator
from models.event import Event
from models.match import Match


class AdminMatchCleanup(LoggedInHandler):
    """
    Given an Event, clean up all Matches that don't have the Event's key as their key prefix.
    Used to clean up 2011 Matches, where we had dupes of "2011new_qm1" and "2011newton_qm1".
    """
    def get(self):
        self._require_admin()
        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/matches_cleanup.html')
        self.response.out.write(template.render(path, self.template_values))

    def post(self):
        self._require_admin()
        event = Event.get_by_id(self.request.get("event_key_name"))
        matches_to_delete = list()
        match_keys_to_delete = list()
        if event is not None:
            for match in Match.query(Match.event == event.key):
                if match.key.id() != match.key_name:
                    matches_to_delete.append(match)
                    match_keys_to_delete.append(match.key_name)

            MatchManipulator.delete(matches_to_delete)

        self.template_values.update({
            "match_keys_deleted": match_keys_to_delete,
            "tried_delete": True
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/matches_cleanup.html')
        self.response.out.write(template.render(path, self.template_values))


class AdminMatchDashboard(LoggedInHandler):
    """
    Show stats about Matches
    """
    def get(self):
        self._require_admin()

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/match_dashboard.html')
        self.response.out.write(template.render(path, self.template_values))


class AdminMatchDelete(LoggedInHandler):
    """
    Delete a Match.
    """
    def get(self, event_key_id):
        self._require_admin()

        match = Match.get_by_id(event_key_id)

        self.template_values.update({
            "match": match
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/match_delete.html')
        self.response.out.write(template.render(path, self.template_values))

    def post(self, match_key_id):
        self._require_admin()

        logging.warning("Deleting %s at the request of %s / %s" % (
            match_key_id,
            self.user_bundle.user.user_id(),
            self.user_bundle.user.email()))

        match = Match.get_by_id(match_key_id)
        event_key_id = match.event.id()

        MatchManipulator.delete(match)

        self.redirect("/admin/event/%s?deleted=%s" % (event_key_id, match_key_id))


class AdminMatchDetail(LoggedInHandler):
    """
    Show a Match.
    """
    def get(self, match_key):
        self._require_admin()
        match = Match.get_by_id(match_key)

        self.template_values.update({
            "match": match
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/match_details.html')
        self.response.out.write(template.render(path, self.template_values))


class AdminMatchAdd(LoggedInHandler):
    """
    Add Matches from CSV.
    """
    def post(self):
        self._require_admin()
        event_key = self.request.get('event_key')
        matches_csv = self.request.get('matches_csv')
        matches = OffseasonMatchesParser.parse(matches_csv)

        event = Event.get_by_id(event_key)
        matches = [Match(
            id=Match.renderKeyName(
                event.key.id(),
                match.get("comp_level", None),
                match.get("set_number", 0),
                match.get("match_number", 0)),
            event=event.key,
            game=Match.FRC_GAMES_BY_YEAR.get(event.year, "frc_unknown"),
            set_number=match.get("set_number", 0),
            match_number=match.get("match_number", 0),
            comp_level=match.get("comp_level", None),
            team_key_names=match.get("team_key_names", None),
            alliances_json=match.get("alliances_json", None)
            )
            for match in matches]
        MatchManipulator.createOrUpdate(matches)

        try:
            FirebasePusher.updated_event(event.key_name)
        except:
            logging.warning("Enqueuing Firebase push failed!")

        self.redirect('/admin/event/{}'.format(event_key))


class AdminMatchEdit(LoggedInHandler):
    """
    Edit a Match.
    """
    def get(self, match_key):
        self._require_admin()
        match = Match.get_by_id(match_key)

        self.template_values.update({
            "match": match
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/match_edit.html')
        self.response.out.write(template.render(path, self.template_values))

    def post(self, match_key):
        self._require_admin()
        alliances_json = self.request.get("alliances_json")
        alliances = json.loads(alliances_json)
        team_key_names = list()

        for alliance in alliances:
            team_key_names.extend(alliances[alliance].get('teams', None))

        match = Match(
            id=match_key,
            event=Event.get_by_id(self.request.get("event_key_name")).key,
            game=self.request.get("game"),
            set_number=int(self.request.get("set_number")),
            match_number=int(self.request.get("match_number")),
            comp_level=self.request.get("comp_level"),
            team_key_names=team_key_names,
            alliances_json=alliances_json,
            # no_auto_update = str(self.request.get("no_auto_update")).lower() == "true", #TODO
        )
        match = MatchManipulator.createOrUpdate(match)

        self.redirect("/admin/match/" + match.key_name)


class AdminVideosAdd(LoggedInHandler):
    """
    Add a lot of youtube_videos to Matches at once.
    """
    def get(self):
        self._require_admin()
        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/videos_add.html')
        self.response.out.write(template.render(path, self.template_values))

    def post(self):
        self._require_admin()

        additions = json.loads(self.request.get("youtube_additions_json"))
        match_keys, youtube_videos = zip(*additions["videos"])
        matches = ndb.get_multi([ndb.Key(Match, match_key) for match_key in match_keys])

        matches_to_put = []
        results = {"existing": [], "bad_match": [], "added": []}
        for (match, match_key, youtube_video) in zip(matches, match_keys, youtube_videos):
            if match:
                if youtube_video not in match.youtube_videos:
                    match.youtube_videos.append(youtube_video)
                    match.dirty = True  # hacky
                    matches_to_put.append(match)
                    results["added"].append(match_key)
                else:
                    results["existing"].append(match_key)
            else:
                results["bad_match"].append(match_key)

        MatchManipulator.createOrUpdate(matches_to_put)

        self.template_values.update({
            "results": results,
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/videos_add.html')
        self.response.out.write(template.render(path, self.template_values))

########NEW FILE########
__FILENAME__ = admin_media_controller
import json
import logging
import os

from google.appengine.ext.webapp import template

from controllers.base_controller import LoggedInHandler
from helpers.media_helper import MediaParser
from helpers.media_manipulator import MediaManipulator
from models.media import Media


class AdminMediaDashboard(LoggedInHandler):
    """
    Show stats about Media
    """
    def get(self):
        self._require_admin()
        media_count = Media.query().count()

        self.template_values.update({
            "media_count": media_count
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/media_dashboard.html')
        self.response.out.write(template.render(path, self.template_values))


class AdminMediaAdd(LoggedInHandler):
    def post(self):
        self._require_admin()

        media_dict = MediaParser.partial_media_dict_from_url(self.request.get('media_url').strip())
        if media_dict is not None:
            year_str = self.request.get('year')
            if year_str == '':
                year = None
            else:
                year = int(year_str.strip())

            media = Media(
                id=Media.render_key_name(media_dict['media_type_enum'], media_dict['foreign_key']),
                foreign_key=media_dict['foreign_key'],
                media_type_enum=media_dict['media_type_enum'],
                details_json=media_dict.get('details_json', None),
                year=year,
                references=[Media.create_reference(
                    self.request.get('reference_type'),
                    self.request.get('reference_key'))],
            )
            MediaManipulator.createOrUpdate(media)

        self.redirect(self.request.get('originating_url'))

########NEW FILE########
__FILENAME__ = admin_memcache_controller
import os

from google.appengine.api import memcache
from google.appengine.ext.webapp import template

from controllers.base_controller import LoggedInHandler
from helpers.memcache.memcache_webcast_flusher import MemcacheWebcastFlusher

# Main memcache view.


class AdminMemcacheMain(LoggedInHandler):
    def post(self):
        self._require_admin()
        flushed = list()

        if self.request.get("all_keys") == "all_keys":
            memcache.flush_all()
            flushed.append("all memcache values")

        if self.request.get("webcast_keys") == "webcast_keys":
            flushed.append(MemcacheWebcastFlusher.flush())

        if self.request.get('memcache_key') is not "":
            memcache.delete(self.request.get("memcache_key"))
            flushed.append(self.request.get("memcache_key"))

        self.template_values.update({
            "flushed": flushed,
            "memcache_stats": memcache.get_stats(),
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/memcache_index.html')
        self.response.out.write(template.render(path, self.template_values))

    def get(self):
        self._require_admin()

        self.template_values.update({
            "memcache_stats": memcache.get_stats(),
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/memcache_index.html')
        self.response.out.write(template.render(path, self.template_values))

########NEW FILE########
__FILENAME__ = admin_migration_controller
import os
from google.appengine.ext import ndb
from google.appengine.ext.webapp import template
from consts.event_type import EventType
from controllers.base_controller import LoggedInHandler
from models.award import Award
from models.event import Event
from helpers.event_helper import EventHelper


class AdminMigration(LoggedInHandler):
  def get(self):
    self._require_admin()
    path = os.path.join(os.path.dirname(__file__), '../../templates/admin/migration.html')
    self.response.out.write(template.render(path, self.template_values))

########NEW FILE########
__FILENAME__ = admin_offseason_scraper_controller
from datetime import datetime
import logging
import os

from google.appengine.ext import ndb
from google.appengine.ext.webapp import template

from controllers.base_controller import LoggedInHandler
from datafeeds.datafeed_usfirst_offseason import DatafeedUsfirstOffseason

from consts.event_type import EventType
from helpers.event_manipulator import EventManipulator
from models.event import Event


class AdminOffseasonScraperController(LoggedInHandler):
    """
    View and add un-added offseasons from FIRST's site
    """
    def get(self):
        self._require_admin()

        df = DatafeedUsfirstOffseason()
        new_events = df.getEventList()
        old_events = Event.query().filter(
            Event.event_type_enum == EventType.OFFSEASON).filter(
            Event.year == 2014).filter(
            Event.first_eid != None).fetch(100)

        old_first_eids = [event.first_eid for event in old_events]
        truly_new_events = [event for event in new_events if event.first_eid not in old_first_eids]

        self.template_values.update({
            "events": truly_new_events,
            "event_key": self.request.get("event_key"),
            "success": self.request.get("success"),

        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/offseasons.html')
        self.response.out.write(template.render(path, self.template_values))

    def post(self):
        self._require_admin()

        if self.request.get("submit") == "duplicate":
            old_event = Event.get_by_id(self.request.get("duplicate_event_key"))
            old_event.first_eid = self.request.get("event_first_eid")
            old_event.dirty = True  # TODO: hacky
            EventManipulator.createOrUpdate(old_event)

            self.redirect("/admin/offseasons?success=duplicate&event_key=%s" % self.request.get("duplicate_event_key"))
            return

        if self.request.get("submit") == "create":

            start_date = None
            if self.request.get("event_start_date"):
                start_date = datetime.strptime(self.request.get("event_start_date"), "%Y-%m-%d")

            end_date = None
            if self.request.get("event_end_date"):
                end_date = datetime.strptime(self.request.get("event_end_date"), "%Y-%m-%d")

            event_key = str(self.request.get("event_year")) + str.lower(str(self.request.get("event_short")))

            event = Event(
                id=event_key,
                event_type_enum=int(self.request.get("event_type_enum")),
                event_short=self.request.get("event_short"),
                first_eid=self.request.get("event_first_eid"),
                name=self.request.get("event_name"),
                year=int(self.request.get("event_year")),
                start_date=start_date,
                end_date=end_date,
                location=self.request.get("event_location"),
                )
            event = EventManipulator.createOrUpdate(event)

            self.redirect("/admin/offseasons?success=create&event_key=%s" % event_key)
            return

        self.redirect("/admin/offseasons")

########NEW FILE########
__FILENAME__ = admin_sitevar_controller
import os
import logging

from google.appengine.ext.webapp import template

from controllers.base_controller import LoggedInHandler
from models.sitevar import Sitevar


class AdminSitevarList(LoggedInHandler):
    """
    List all Sitevars.
    """
    def get(self):
        self._require_admin()
        sitevars = Sitevar.query().fetch(10000)

        self.template_values.update({
            "sitevars": sitevars,
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/sitevar_list.html')
        self.response.out.write(template.render(path, self.template_values))


class AdminSitevarCreate(LoggedInHandler):
    """
    Create an Sitevar. POSTs to AdminSitevarEdit.
    """
    def get(self):
        self._require_admin()

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/sitevar_create.html')
        self.response.out.write(template.render(path, self.template_values))


class AdminSitevarEdit(LoggedInHandler):
    """
    Edit a Sitevar.
    """
    def get(self, sitevar_key):
        self._require_admin()
        sitevar = Sitevar.get_by_id(sitevar_key)

        success = self.request.get("success")

        self.template_values.update({
            "sitevar": sitevar,
            "success": success,
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/sitevar_edit.html')
        self.response.out.write(template.render(path, self.template_values))

    def post(self, sitevar_key):
        self._require_admin()

        # note, we don't use sitevar_key

        sitevar = Sitevar(
            id=self.request.get("key"),
            description=self.request.get("description"),
            values_json=self.request.get("values_json"),
        )
        sitevar.put()

        self.redirect("/admin/sitevar/edit/" + sitevar.key.id() + "?success=true")

########NEW FILE########
__FILENAME__ = admin_team_controller
import os

from google.appengine.ext.webapp import template

from controllers.base_controller import LoggedInHandler
from models.event_team import EventTeam
from models.team import Team
from models.media import Media


class AdminTeamList(LoggedInHandler):
    """
    The view of a list of teams.
    """
    def get(self):
        self._require_admin()

        teams = Team.query().order(Team.team_number)

        self.template_values.update({
            "teams": teams,
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/team_list.html')
        self.response.out.write(template.render(path, self.template_values))


class AdminTeamDetail(LoggedInHandler):
    """
    The view of a single Team.
    """
    def get(self, team_number):
        self._require_admin()

        team = Team.get_by_id("frc" + team_number)
        event_teams = EventTeam.query(EventTeam.team == team.key).fetch(500)
        team_medias = Media.query(Media.references == team.key).fetch(500)

        team_medias_by_year = {}
        for media in team_medias:
            if media.year in team_medias_by_year:
                team_medias_by_year[media.year].append(media)
            else:
                team_medias_by_year[media.year] = [media]

        self.template_values.update({
            'event_teams': event_teams,
            'team': team,
            'team_medias_by_year': team_medias_by_year,
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/team_details.html')
        self.response.out.write(template.render(path, self.template_values))

########NEW FILE########
__FILENAME__ = admin_user_controller
from datetime import datetime
import logging
import os

from google.appengine.api import users
from google.appengine.ext import ndb
from google.appengine.ext.webapp import template

from controllers.base_controller import LoggedInHandler
from models.account import Account


class AdminUserList(LoggedInHandler):
    """
    List all Users.
    """
    def get(self):
        self._require_admin()
        users = Account.query().order(Account.created).fetch(10000)

        self.template_values.update({
            "users": users,
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/user_list.html')
        self.response.out.write(template.render(path, self.template_values))


class AdminUserDetail(LoggedInHandler):
    """
    Show a User.
    """
    def get(self, user_id):
        self._require_admin()
        user = Account.get_by_id(user_id)

        self.template_values.update({
            "user": user
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/user_details.html')
        self.response.out.write(template.render(path, self.template_values))


class AdminUserEdit(LoggedInHandler):
    """
    Edit a User.
    """
    def get(self, user_id):
        self._require_admin()
        user = Account.get_by_id(user_id)
        self.template_values.update({
            "user": user
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/admin/user_edit.html')
        self.response.out.write(template.render(path, self.template_values))

    def post(self, user_id):
        self._require_admin()
        user = Account.get_by_id(user_id)

        user.display_name = self.request.get("display_name")
        user.put()

        self.redirect("/admin/user/" + user_id)

########NEW FILE########
__FILENAME__ = admin_event_webcast_suggestions_review_controller
import datetime
import os
import logging

from google.appengine.ext import ndb
from google.appengine.ext.webapp import template

from controllers.base_controller import LoggedInHandler
from helpers.event.event_webcast_adder import EventWebcastAdder
from helpers.memcache.memcache_webcast_flusher import MemcacheWebcastFlusher


from models.event import Event
from models.suggestion import Suggestion


class AdminEventWebcastSuggestionsReviewController(LoggedInHandler):
    """
    View the list of suggestions.
    """
    def get(self):
        self._require_admin()

        suggestions = Suggestion.query().filter(
            Suggestion.review_state == Suggestion.REVIEW_PENDING).filter(
            Suggestion.target_model == "event")

        self.template_values.update({
            "event_key": self.request.get("event_key"),
            "success": self.request.get("success"),
            "suggestions": suggestions,
        })

        path = os.path.join(os.path.dirname(__file__), '../../../templates/admin/event_webcast_suggestion_list.html')
        self.response.out.write(template.render(path, self.template_values))

    def post(self):
        self._require_admin()

        if self.request.get("verdict") == "accept":
            webcast = dict()
            webcast["type"] = self.request.get("webcast_type")
            webcast["channel"] = self.request.get("webcast_channel")
            if self.request.get("webcast_file"):
                webcast["file"] = self.request.get("webcast_file")

            event = Event.get_by_id(self.request.get("event_key"))
            suggestion = Suggestion.get_by_id(int(self.request.get("suggestion_key")))

            EventWebcastAdder.add_webcast(event, webcast)
            MemcacheWebcastFlusher.flush()

            suggestion.review_state = Suggestion.REVIEW_ACCEPTED
            suggestion.reviewer = self.user_bundle.account.key
            suggestion.reviewer_at = datetime.datetime.now()
            suggestion.put()

            self.redirect("/admin/suggestions/event/webcast/review?success=accept&event_key=%s" % event.key.id())
            return

        elif self.request.get("verdict") == "reject":
            suggestion = Suggestion.get_by_id(int(self.request.get("suggestion_key")))

            suggestion.review_state = Suggestion.REVIEW_REJECTED
            suggestion.reviewer = self.user_bundle.account.key
            suggestion.reviewer_at = datetime.datetime.now()
            suggestion.put()

            self.redirect("/admin/suggestions/event/webcast/review?success=reject")
            return

        self.redirect("/admin/suggestions/event/webcast/review")




########NEW FILE########
__FILENAME__ = admin_match_video_suggestions_review_controller
import datetime
import os

from google.appengine.ext import ndb
from google.appengine.ext.webapp import template

from controllers.base_controller import LoggedInHandler
from helpers.suggestions.match_suggestion_accepter import MatchSuggestionAccepter
from models.suggestion import Suggestion


class AdminMatchVideoSuggestionsReviewController(LoggedInHandler):
    """
    View the list of suggestions.
    """
    def get(self):
        self._require_admin()

        suggestions = Suggestion.query().filter(
            Suggestion.review_state == Suggestion.REVIEW_PENDING).filter(
            Suggestion.target_model == "match")

        self.template_values.update({
            "suggestions": suggestions,
        })

        path = os.path.join(os.path.dirname(__file__), '../../../templates/admin/match_video_suggestion_list.html')
        self.response.out.write(template.render(path, self.template_values))

    def post(self):
        self._require_admin()

        accept_keys = map(int, self.request.POST.getall("accept_keys[]"))
        reject_keys = map(int, self.request.POST.getall("reject_keys[]"))

        accepted_suggestion_futures = [Suggestion.get_by_id_async(key) for key in accept_keys]
        rejected_suggestion_futures = [Suggestion.get_by_id_async(key) for key in reject_keys]
        accepted_suggestions = map(lambda a: a.get_result(), accepted_suggestion_futures)
        rejected_suggestions = map(lambda a: a.get_result(), rejected_suggestion_futures)

        MatchSuggestionAccepter.accept_suggestions(accepted_suggestions)

        all_suggestions = accepted_suggestions
        all_suggestions.extend(rejected_suggestions)

        for suggestion in all_suggestions:
            if suggestion.key.id() in accept_keys:
                suggestion.review_state = Suggestion.REVIEW_ACCEPTED
            if suggestion.key.id() in reject_keys:
                suggestion.review_state = Suggestion.REVIEW_REJECTED
            suggestion.reviewer = self.user_bundle.account.key
            suggestion.reviewer_at = datetime.datetime.now()

        ndb.put_multi(all_suggestions)

        self.redirect("/admin/suggestions/match/video/review")

########NEW FILE########
__FILENAME__ = admin_media_suggestions_review_controller
import datetime
import os
import json

from google.appengine.ext import ndb
from google.appengine.ext.webapp import template

from controllers.base_controller import LoggedInHandler
from helpers.media_manipulator import MediaManipulator
from models.media import Media
from models.suggestion import Suggestion


class AdminMediaSuggestionsReviewController(LoggedInHandler):
    """
    View the list of suggestions.
    """
    def get(self):
        self._require_admin()

        suggestions = Suggestion.query().filter(
            Suggestion.review_state == Suggestion.REVIEW_PENDING).filter(
            Suggestion.target_model == "media")

        reference_keys = []
        for suggestion in suggestions:
            reference_keys.append(Media.create_reference(
                suggestion.contents['reference_type'],
                suggestion.contents['reference_key']))
            if 'details_json' in suggestion.contents:
                suggestion.details = json.loads(suggestion.contents['details_json'])
                suggestion.details['thumbnail'] = suggestion.details['image_partial'].replace('_l', '_m')

        reference_futures = ndb.get_multi_async(reference_keys)
        references = map(lambda r: r.get_result(), reference_futures)

        suggestions_and_references = zip(suggestions, references)

        self.template_values.update({
            "suggestions_and_references": suggestions_and_references,
        })

        path = os.path.join(os.path.dirname(__file__), '../../../templates/admin/media_suggestion_list.html')
        self.response.out.write(template.render(path, self.template_values))

    def post(self):
        self._require_admin()

        accept_keys = map(int, self.request.POST.getall("accept_keys[]"))
        reject_keys = map(int, self.request.POST.getall("reject_keys[]"))

        accepted_suggestion_futures = [Suggestion.get_by_id_async(key) for key in accept_keys]
        rejected_suggestion_futures = [Suggestion.get_by_id_async(key) for key in reject_keys]
        accepted_suggestions = map(lambda a: a.get_result(), accepted_suggestion_futures)
        rejected_suggestions = map(lambda a: a.get_result(), rejected_suggestion_futures)

        for suggestion in accepted_suggestions:
            media = Media(
                id=Media.render_key_name(suggestion.contents['media_type_enum'], suggestion.contents['foreign_key']),
                foreign_key=suggestion.contents['foreign_key'],
                media_type_enum=suggestion.contents['media_type_enum'],
                details_json=suggestion.contents.get('details_json', None),
                year=int(suggestion.contents['year']),
                references=[Media.create_reference(
                    suggestion.contents['reference_type'],
                    suggestion.contents['reference_key'])],
            )
            MediaManipulator.createOrUpdate(media)

        all_suggestions = accepted_suggestions
        all_suggestions.extend(rejected_suggestions)

        for suggestion in all_suggestions:
            if suggestion.key.id() in accept_keys:
                suggestion.review_state = Suggestion.REVIEW_ACCEPTED
            if suggestion.key.id() in reject_keys:
                suggestion.review_state = Suggestion.REVIEW_REJECTED
            suggestion.reviewer = self.user_bundle.account.key
            suggestion.reviewer_at = datetime.datetime.now()

        ndb.put_multi(all_suggestions)

        self.redirect("/admin/suggestions/media/review")

########NEW FILE########
__FILENAME__ = ajax_controller
import os
import urllib2
import json
import time

from google.appengine.api import memcache
from google.appengine.ext.webapp import template

from base_controller import CacheableHandler

from models.event import Event
from models.sitevar import Sitevar
from models.typeahead_entry import TypeaheadEntry


class LiveEventHandler(CacheableHandler):
    """
    Returns the necessary details to render live components
    Uses timestamp for aggressive caching
    """
    CACHE_VERSION = 1
    CACHE_KEY_FORMAT = "live-event:{}:{}"  # (event_key, timestamp)

    def __init__(self, *args, **kw):
        super(LiveEventHandler, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 10

    def get(self, event_key, timestamp):
        if int(timestamp) > time.time():
            self.abort(404)
        self._cache_key = self.CACHE_KEY_FORMAT.format(event_key, timestamp)
        super(LiveEventHandler, self).get(event_key, timestamp)

    def _render(self, event_key, timestamp):
        self.response.headers['Cache-Control'] = 'public, max-age=%d' % self._cache_expiration
        self.response.headers['Pragma'] = 'Public'
        self.response.headers['content-type'] = 'application/json; charset="utf-8"'

        event = Event.get_by_id(event_key)

        matches = []
        for match in event.matches:
            matches.append({
                'name': match.short_name,
                'alliances': match.alliances,
                'order': match.play_order,
                'time_str': match.time_string,
            })

        event_dict = {
#             'rankings': event.rankings,
#             'matchstats': event.matchstats,
            'matches': matches,
        }

        return json.dumps(event_dict)


class TypeaheadHandler(CacheableHandler):
    """
    Currently just returns a list of all teams and events
    Needs to be optimized at some point.
    Tried a trie but the datastructure was too big to
    fit into memcache efficiently
    """
    CACHE_VERSION = 1
    CACHE_KEY_FORMAT = "typeahead_entries:{}"  # (search_key)

    def __init__(self, *args, **kw):
        super(TypeaheadHandler, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24

    def get(self, search_key):
        search_key = urllib2.unquote(search_key)
        self._cache_key = self.CACHE_KEY_FORMAT.format(search_key)
        super(TypeaheadHandler, self).get(search_key)

    def _render(self, search_key):
        self.response.headers['Cache-Control'] = 'public, max-age=%d' % self._cache_expiration
        self.response.headers['Pragma'] = 'Public'
        self.response.headers['content-type'] = 'application/json; charset="utf-8"'

        entry = TypeaheadEntry.get_by_id(search_key)
        if entry is None:
            return '[]'
        else:
            if self._has_been_modified_since(entry.updated):
                return entry.data_json
            else:
                return None


class WebcastHandler(CacheableHandler):
    """
    Returns the HTML necessary to generate the webcast embed for a given event
    """
    CACHE_VERSION = 1
    CACHE_KEY_FORMAT = "webcast_{}_{}"  # (event_key)

    def __init__(self, *args, **kw):
        super(WebcastHandler, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24

    def get(self, event_key, webcast_number):
        self._cache_key = self.CACHE_KEY_FORMAT.format(event_key, webcast_number)
        super(WebcastHandler, self).get(event_key, webcast_number)

    def _render(self, event_key, webcast_number):
        self.response.headers['Cache-Control'] = "public, max-age=%d" % (5 * 60)
        self.response.headers['Pragma'] = 'Public'
        self.response.headers.add_header('content-type', 'application/json', charset='utf-8')

        output = {}
        if not webcast_number.isdigit():
            return json.dumps(output)
        webcast_number = int(webcast_number) - 1

        event = Event.get_by_id(event_key)
        if event and event.webcast:
            webcast = event.webcast[webcast_number]
            if 'type' in webcast and 'channel' in webcast:
                output['player'] = self._renderPlayer(webcast)
        else:
            special_webcasts_future = Sitevar.get_by_id_async('gameday.special_webcasts')
            special_webcasts = special_webcasts_future.get_result()
            if special_webcasts:
                special_webcasts = special_webcasts.contents
            else:
                special_webcasts = {}
            if event_key in special_webcasts:
                webcast = special_webcasts[event_key]
                if 'type' in webcast and 'channel' in webcast:
                    output['player'] = self._renderPlayer(webcast)

        return json.dumps(output)

    def _renderPlayer(self, webcast):
        webcast_type = webcast['type']
        template_values = {'webcast': webcast}

        path = os.path.join(os.path.dirname(__file__), '../templates/webcast/' + webcast_type + '.html')
        return template.render(path, template_values)

    def memcacheFlush(self, event_key):
        keys = [self.CACHE_KEY_FORMAT.format(event_key, n) for n in range(10)]
        memcache.delete_multi(keys)
        return keys

########NEW FILE########
__FILENAME__ = api_base_controller
import json
import logging
import urllib
import uuid
import webapp2

from google.appengine.api import urlfetch
from google.appengine.ext import deferred

from controllers.base_controller import CacheableHandler
from helpers.validation_helper import ValidationHelper
from models.sitevar import Sitevar


# used for deferred call
def track_call(api_action, api_label, x_tba_app_id):
    """
    For more information about GAnalytics Protocol Parameters, visit
    https://developers.google.com/analytics/devguides/collection/protocol/v1/parameters
    """
    analytics_id = Sitevar.get_by_id("google_analytics.id")
    if analytics_id is None:
        logging.warning("Missing sitevar: google_analytics.id. Can't track API usage.")
    else:
        GOOGLE_ANALYTICS_ID = analytics_id.contents['GOOGLE_ANALYTICS_ID']
        params = urllib.urlencode({
            'v': 1,
            'tid': GOOGLE_ANALYTICS_ID,
            'cid': uuid.uuid3(uuid.NAMESPACE_X500, str(x_tba_app_id)),
            't': 'event',
            'ec': 'api-v02',
            'ea': api_action,
            'el': api_label,
            'cd1': x_tba_app_id,  # custom dimension 1
            'ni': 1,
            'sc': 'end',  # forces tracking session to end
        })

        analytics_url = 'http://www.google-analytics.com/collect?%s' % params
        urlfetch.fetch(
            url=analytics_url,
            method=urlfetch.GET,
            deadline=10,
        )


class ApiBaseController(CacheableHandler):

    def __init__(self, *args, **kw):
        super(ApiBaseController, self).__init__(*args, **kw)
        self.response.headers['content-type'] = 'application/json; charset="utf-8"'
        self.response.headers['Access-Control-Allow-Origin'] = '*'

    def handle_exception(self, exception, debug):
        """
        Handle an HTTP exception and actually writeout a
        response.
        Called by webapp when abort() is called, stops code excution.
        """
        logging.info(exception)
        if isinstance(exception, webapp2.HTTPException):
            self.response.set_status(exception.code)
            self.response.out.write(self._errors)
        else:
            self.response.set_status(500)

    def get(self, *args, **kw):
        self._validate_tba_app_id()
        self._errors = ValidationHelper.validate(self._validators)
        if self._errors:
            self.abort(400)

        self._track_call(*args, **kw)
        super(ApiBaseController, self).get(*args, **kw)

    def _track_call_defer(self, api_action, api_label):
        deferred.defer(track_call, api_action, api_label, self.x_tba_app_id)

    def _validate_tba_app_id(self):
        """
        Tests the presence of a X-TBA-App-Id header or URL param.
        """
        self.x_tba_app_id = self.request.headers.get("X-TBA-App-Id")
        if self.x_tba_app_id is None:
            self.x_tba_app_id = self.request.get('X-TBA-App-Id')

        logging.info("X-TBA-App-ID: {}".format(self.x_tba_app_id))
        if not self.x_tba_app_id:
            self._errors = json.dumps({"Error": "X-TBA-App-Id is a required header or URL param. Please see http://www.thebluealliance.com/apidocs for more info."})
            self.abort(400)
        if len(self.x_tba_app_id.split(':')) != 3:
            self._errors = json.dumps({"Error": "X-TBA-App-Id must follow a specific format. Please see http://www.thebluealliance.com/apidocs for more info."})
            self.abort(400)

    def _set_cache_header_length(self, seconds):
        if type(seconds) is not int:
            logging.error("Cache-Control max-age is not integer: {}".format(seconds))
            return

        self.response.headers['Cache-Control'] = "public, max-age=%d" % seconds
        self.response.headers['Pragma'] = 'Public'

########NEW FILE########
__FILENAME__ = api_event_controller
import json
import logging
import webapp2

from datetime import datetime
from google.appengine.ext import ndb

from controllers.api.api_base_controller import ApiBaseController

from helpers.model_to_dict import ModelToDict

from models.event import Event


class ApiEventController(ApiBaseController):
    CACHE_KEY_FORMAT = "apiv2_event_controller_{}"  # (event_key)
    CACHE_VERSION = 0

    def __init__(self, *args, **kw):
        super(ApiEventController, self).__init__(*args, **kw)
        self.event_key = self.request.route_kwargs["event_key"]
        self._cache_key = self.CACHE_KEY_FORMAT.format(self.event_key)

    @property
    def _validators(self):
        return [("event_id_validator", self.event_key)]

    def _set_event(self, event_key):
        self.event = Event.get_by_id(event_key)
        if self.event is None:
            self._errors = json.dumps({"404": "%s event not found" % self.event_key})
            self.abort(404)

    def _track_call(self, event_key):
        self._track_call_defer('event', event_key)

    def _render(self, event_key):
        self._set_cache_header_length(60 * 60)
        self._set_event(event_key)

        event_dict = ModelToDict.eventConverter(self.event)

        return json.dumps(event_dict, ensure_ascii=True)


class ApiEventTeamsController(ApiEventController):
    CACHE_KEY_FORMAT = "apiv2_event_teams_controller_{}"  # (event_key)
    CACHE_VERSION = 0

    def __init__(self, *args, **kw):
        super(ApiEventTeamsController, self).__init__(*args, **kw)
        self._cache_key = self.CACHE_KEY_FORMAT.format(self.event_key)

    def _track_call(self, event_key):
        self._track_call_defer('event/teams', event_key)

    def _render(self, event_key):
        self._set_cache_header_length(60 * 60)
        self._set_event(event_key)

        teams = self.event.teams
        team_dicts = [ModelToDict.teamConverter(team) for team in teams]

        return json.dumps(team_dicts, ensure_ascii=True)


class ApiEventMatchesController(ApiEventController):
    CACHE_KEY_FORMAT = "apiv2_event_matches_controller_{}"  # (event_key)
    CACHE_VERSION = 0

    def __init__(self, *args, **kw):
        super(ApiEventMatchesController, self).__init__(*args, **kw)
        self._cache_key = self.CACHE_KEY_FORMAT.format(self.event_key)

    def _track_call(self, event_key):
        self._track_call_defer('event/matches', event_key)

    def _render(self, event_key):
        self._set_cache_header_length(61)
        self._set_event(event_key)

        matches = self.event.matches
        match_dicts = [ModelToDict.matchConverter(match) for match in matches]

        return json.dumps(match_dicts, ensure_ascii=True)


class ApiEventStatsController(ApiEventController):
    CACHE_KEY_FORMAT = "apiv2_event_stats_controller_{}"  # (event_key)
    CACHE_VERSION = 0

    def __init__(self, *args, **kw):
        super(ApiEventStatsController, self).__init__(*args, **kw)
        self._cache_key = self.CACHE_KEY_FORMAT.format(self.event_key)

    def _track_call(self, event_key):
        self._track_call_defer('event/stats', event_key)

    def _render(self, event_key):
        self._set_cache_header_length(61)
        self._set_event(event_key)

        return json.dumps(Event.get_by_id(event_key).matchstats)

class ApiEventRankingsController(ApiEventController):
    CACHE_KEY_FORMAT = "apiv2_event_rankings_controller_{}"  # (event_key)
    CACHE_VERSION = 0

    def __init__(self, *args, **kw):
        super(ApiEventRankingsController, self).__init__(*args, **kw)
        self._cache_key = self.CACHE_KEY_FORMAT.format(self.event_key)

    def _track_call(self, event_key):
        self._track_call_defer('event/rankings', event_key)

    def _render(self, event_key):
        self._set_cache_header_length(61)
        self._set_event(event_key)

        ranks = json.dumps(Event.get_by_id(event_key).rankings)
        if ranks is None or ranks == 'null':
            return '[]'
        else:
            return ranks

class ApiEventAwardsController(ApiEventController):
    CACHE_KEY_FORMAT = "apiv2_event_awards_controller_{}"  # (event_key)
    CACHE_VERSION = 0

    def __init__(self, *args, **kw):
        super(ApiEventAwardsController, self).__init__(*args, **kw)
        self._cache_key = self.CACHE_KEY_FORMAT.format(self.event_key)

    def _track_call(self, event_key):
        self._track_call_defer('event/awards', event_key)

    def _render(self,event_key):
        self._set_cache_header_length(61)
        self._set_event(event_key)

        award_list = self.event.awards
        award_dicts = [ModelToDict.awardConverter(award) for award in award_list]
        return json.dumps(award_dicts, ensure_ascii=True)


class ApiEventListController(ApiBaseController):
    CACHE_KEY_FORMAT = "apiv2_event_list_controller_{}"  # (year)
    CACHE_VERSION = 0

    def __init__(self, *args, **kw):
        super(ApiEventListController, self).__init__(*args, **kw)
        self.year = int(self.request.route_kwargs.get("year") or datetime.now().year)
        self._cache_key = self.CACHE_KEY_FORMAT.format(self.year)

    @property
    def _validators(self):
        return []

    def _track_call(self, *args, **kw):
        self._track_call_defer('event/list', self.year)

    def _render(self, year=None):
        self._set_cache_header_length(60 * 60 * 24 * 3)

        if self.year < 1992 or self.year > datetime.now().year + 1:
            self._errors = json.dumps({"404": "No events found for %s" % self.year})
            self.abort(404)

        keys = Event.query(Event.year == self.year).fetch(1000, keys_only=True)
        events = ndb.get_multi(keys)

        event_list = [ModelToDict.eventConverter(event) for event in events]

        return json.dumps(event_list, ensure_ascii=True)

########NEW FILE########
__FILENAME__ = api_team_controller
import json
import webapp2

from datetime import datetime

from controllers.api.api_base_controller import ApiBaseController

from helpers.model_to_dict import ModelToDict
from helpers.data_fetchers.team_details_data_fetcher import TeamDetailsDataFetcher

from models.team import Team


class ApiTeamController(ApiBaseController):
    CACHE_KEY_FORMAT = "apiv2_team_controller_{}_{}"  # (team_key, year)
    CACHE_VERSION = 0

    def __init__(self, *args, **kw):
        super(ApiTeamController, self).__init__(*args, **kw)
        self.team_key = self.request.route_kwargs["team_key"]
        self.year = int(self.request.route_kwargs.get("year") or datetime.now().year)
        self._cache_key = self.CACHE_KEY_FORMAT.format(self.team_key, self.year)

    @property
    def _validators(self):
        return [("team_id_validator", self.team_key)]

    def _track_call(self, team_key, year=None):
        api_label = team_key
        if year is not None:
            api_label += '/{}'.format(year)
        self._track_call_defer('team', api_label)

    def _render(self, team_key, year=None):
        self._set_cache_header_length(61)

        self.team = Team.get_by_id(self.team_key)
        if self.team is None:
            self._errors = json.dumps({"404": "%s team not found" % self.team_key})
            self.abort(404)

        events_sorted, matches_by_event_key, awards_by_event_key, _ = TeamDetailsDataFetcher.fetch(self.team, self.year)
        team_dict = ModelToDict.teamConverter(self.team)

        team_dict["events"] = list()
        for event in events_sorted:
            event_dict = ModelToDict.eventConverter(event)
            event_dict["matches"] = [ModelToDict.matchConverter(match) for match in matches_by_event_key.get(event.key, [])]
            event_dict["awards"] = [ModelToDict.awardConverter(award) for award in awards_by_event_key.get(event.key, [])]
            team_dict["events"].append(event_dict)

        return json.dumps(team_dict, ensure_ascii=True)

########NEW FILE########
__FILENAME__ = api_trusted_controller
import json
import webapp2

from helpers.match_manipulator import MatchManipulator

from models.match import Match
from models.sitevar import Sitevar


class ApiTrustedAddMatchYoutubeVideo(webapp2.RequestHandler):
    def post(self):
        trusted_api_secret = Sitevar.get_by_id("trusted_api.secret")
        if trusted_api_secret is None:
            raise Exception("Missing sitevar: trusted_api.secret. Can't accept YouTube Videos.")

        secret = self.request.get('secret', None)
        if secret is None:
            self.response.set_status(400)
            self.response.out.write(json.dumps({"400": "No secret given"}))
            return

        if str(trusted_api_secret.values_json) != str(secret):
            self.response.set_status(400)
            self.response.out.write(json.dumps({"400": "Incorrect secret"}))
            return

        match_key = self.request.get('match_key', None)
        if match_key is None:
            self.response.set_status(400)
            self.response.out.write(json.dumps({"400": "No match_key given"}))
            return

        youtube_id = self.request.get('youtube_id', None)
        if youtube_id is None:
            self.response.set_status(400)
            self.response.out.write(json.dumps({"400": "No youtube_id given"}))
            return

        match = Match.get_by_id(match_key)
        if match is None:
            self.response.set_status(400)
            self.response.out.write(json.dumps({"400": "Match {} does not exist!".format(match_key)}))
            return

        if youtube_id not in match.youtube_videos:
            match.youtube_videos.append(youtube_id)
            match.dirty = True  # This is so hacky. -fangeugene 2014-03-06
            MatchManipulator.createOrUpdate(match)

########NEW FILE########
__FILENAME__ = api_controller
import csv
import json
import logging
import StringIO
import os
import urllib
import uuid
import webapp2

from datetime import datetime

from google.appengine.api import memcache, urlfetch
from google.appengine.ext import deferred, ndb
from google.appengine.ext.webapp import template


import tba_config
from helpers.api_helper import ApiHelper

from models.event import Event
from models.sitevar import Sitevar
from models.team import Team


# used for deferred call
def track_call(api_action, api_details, x_tba_app_id):
    analytics_id = Sitevar.get_by_id("google_analytics.id")
    if analytics_id is None:
        logging.warning("Missing sitevar: google_analytics.id. Can't track API usage.")
    else:
        GOOGLE_ANALYTICS_ID = analytics_id.contents['GOOGLE_ANALYTICS_ID']
        params = urllib.urlencode({
            'v': 1,
            'tid': GOOGLE_ANALYTICS_ID,
            'cid': uuid.uuid3(uuid.NAMESPACE_X500, str(x_tba_app_id)),
            't': 'event',
            'ec': 'api',
            'ea': api_action,
            'el': api_details,
            'cd1': x_tba_app_id,  # custom dimension 1
            'ni': 1,
            'sc': 'end',  # forces tracking session to end
        })

        # Sets up the call
        analytics_url = 'http://www.google-analytics.com/collect?%s' % params
        urlfetch.fetch(
            url=analytics_url,
            method=urlfetch.GET,
            deadline=10,
        )


# Note: generally caching for the API happens in ApiHelper
class MainApiHandler(webapp2.RequestHandler):

    def __init__(self, request, response):
        # Need to initialize a webapp2 instance
        self.initialize(request, response)
        self.response.headers.add_header("content-type", "application/json")
        self.response.headers['Access-Control-Allow-Origin'] = '*'

    def handle_exception(self, exception, debug):
        """
        Handle an HTTP exception and actually write out a response.
        Called by webapp when abort() is called, stops code excution.
        """
        logging.info(exception)
        if isinstance(exception, webapp2.HTTPException):
            self.response.set_status(exception.code)
            self.response.out.write(self._errors)
        else:
            self.response.set_status(500)

    def _track_call_defer(self, api_action, api_details=''):
        deferred.defer(track_call, api_action, api_details, self.x_tba_app_id)

    def _validate_tba_app_id(self):
        """
        Tests the presence of a X-TBA-App-Id header or URL param.
        """
        self.x_tba_app_id = self.request.headers.get("X-TBA-App-Id")
        if self.x_tba_app_id is None:
            self.x_tba_app_id = self.request.get('X-TBA-App-Id')

        logging.info("X-TBA-App-ID: {}".format(self.x_tba_app_id))
        if not self.x_tba_app_id:
            self._errors = json.dumps({"Error": "X-TBA-App-Id is a required header or URL param. Please see http://www.thebluealliance.com/apidocs for more info."})
            self.abort(400)
        if len(self.x_tba_app_id.split(':')) != 3:
            self._errors = json.dumps({"Error": "X-TBA-App-Id must follow a specific format. Please see http://www.thebluealliance.com/apidocs for more info."})
            self.abort(400)


class ApiTeamsShow(MainApiHandler):
    """
    Information about teams.
    """
    def get(self):
        self._validate_tba_app_id()
        teams = []
        team_keys = self.request.get('teams').split(',')

        for team_key in team_keys:
            try:
                team_info = ApiHelper.getTeamInfo(team_key)
                teams.append(team_info)
            except IndexError:
                pass

        if teams:
            response_json = teams
        else:
            response_json = {"Property Error": "No teams found for any key given"}
            self.response.set_status(404)

        self.response.out.write(json.dumps(response_json))

        team_keys_sorted = sorted(team_keys)
        track_team_keys = ",".join(team_keys_sorted)
        self._track_call_defer('teams/show', track_team_keys)


class ApiTeamDetails(MainApiHandler):
    """
    Information about a Team in a particular year, including full Event and Match objects
    """
    def get(self):
        self._validate_tba_app_id()
        team_key = self.request.get('team')
        year = self.request.get('year')

        response_json = {}

        try:
            response_json = ApiHelper.getTeamInfo(team_key)
            if self.request.get('events'):
                response_json = ApiHelper.addTeamEvents(response_json, year)

            # TODO: matches

            self.response.out.write(json.dumps(response_json))

            track_team_key = team_key
            if year:
                track_team_key = track_team_key + ' (' + year + ')'
            self._track_call_defer('teams/details', track_team_key)

        except IndexError:
            response_json = {"Property Error": "No team found for the key given"}
            self.response.set_status(404)
            self.response.out.write(json.dumps(response_json))


class ApiEventsShow(MainApiHandler):
    """
    Information about events.
    Deprecation notice. Please use ApiEventList, or ApiEventDetails.
    """
    def get(self):
        self._validate_tba_app_id()
        response = {"API Method Removed": "ApiEventsShow is no longer available. Please use ApiEventDetails, and ApiEventList instead."}
        self.response.set_status(410)
        self.response.out.write(json.dumps(response))


class ApiEventList(MainApiHandler):
    """
    Returns a list of events for a year with top level information
    """

    def get(self):
        self._validate_tba_app_id()
        if self.request.get("year") is '':
            year = datetime.now().year
        else:
            year = int(self.request.get("year"))

        memcache_key = "api_event_list_%s" % year
        event_list = memcache.get(memcache_key)

        if event_list is None:
            event_list = []
            event_keys = Event.query(Event.year == year).fetch(500, keys_only=True)
            events = ndb.get_multi(event_keys)
            for event in events:
                event_dict = {}
                event_dict["key"] = event.key_name
                event_dict["name"] = event.name
                event_dict["short_name"] = event.short_name
                event_dict["official"] = event.official

                if event.start_date:
                    event_dict["start_date"] = event.start_date.isoformat()
                else:
                    event_dict["start_date"] = None
                if event.end_date:
                    event_dict["end_date"] = event.end_date.isoformat()
                else:
                    event_dict["end_date"] = None

                event_list.append(event_dict)

            if tba_config.CONFIG["memcache"]:
                memcache.set(memcache_key, event_list, (30 * ((60 * 60) * 24)))

        self.response.out.write(json.dumps(event_list))

        self._track_call_defer('events/list')


class ApiEventDetails(MainApiHandler):
    """
    Return a specific event with details.
    """

    def get(self):
        self._validate_tba_app_id()
        event_key = str(self.request.get("event"))
        if event_key is "" or event_key is None:
            error_message = {"Parameter Error": "'event' is a required parameter."}
            self.response.out.write(json.dumps(error_message))
            return False

        event_dict = ApiHelper.getEventInfo(event_key)

        self.response.out.write(json.dumps(event_dict))

        self._track_call_defer('events/details', event_key)


class ApiMatchDetails(MainApiHandler):
    """
    Returns specific matches with details.
    """
    def get(self):
        self._validate_tba_app_id()
        value = self.request.get('match') or self.request.get('matches')
        matches = []
        if value is not '':
            match_keys = value.split(',')
            for match_key in match_keys:
                if match_key == '':
                    continue
                mjson = ApiHelper.getMatchDetails(match_key)
                if mjson is not None:
                    matches.append(mjson)

        if matches != []:
            response = matches
        else:
            response = {"Property Error": "No matches found for any key given"}
            self.response.set_status(404)

        self.response.out.write(json.dumps(response))

        self._track_call_defer('matches/details')

class CsvTeamsAll(MainApiHandler):
    """
    Outputs a CSV of all team information in the database, designed for other apps to bulk-import data.
    """
    def get(self):
        self._validate_tba_app_id()
        memcache_key = "csv_teams_all"
        output = memcache.get(memcache_key)

        if output is None:
            team_keys = Team.query().order(Team.team_number).fetch(10000, keys_only=True)
            team_futures = ndb.get_multi_async(team_keys)

            sio = StringIO.StringIO()
            writer = csv.writer(sio, delimiter=',')
            writer.writerow(['team_number','name','nickname','location','website'])

            for team_future in team_futures:
                team = team_future.get_result()
                row = [team.team_number, team.name, team.nickname, team.location, team.website]
                row_utf8 = [unicode(e).encode('utf-8') for e in row]
                writer.writerow(row_utf8)

            output = sio.getvalue()

            if tba_config.CONFIG["memcache"]:
                memcache.set(memcache_key, output, 86400)

        self.response.headers["content-type"] = "text/csv"
        self.response.out.write(output)

        self._track_call_defer('teams/list')

########NEW FILE########
__FILENAME__ = backup_controller
import cloudstorage
import csv
import datetime
import json
import logging
import os
import StringIO
import tba_config

from google.appengine.api import taskqueue
from google.appengine.api import urlfetch
from google.appengine.ext import ndb
from google.appengine.ext import webapp
from google.appengine.ext.webapp import template

from helpers.award_manipulator import AwardManipulator
from helpers.event_manipulator import EventManipulator
from helpers.match_manipulator import MatchManipulator

from models.award import Award
from models.event import Event
from models.match import Match
from models.team import Team

from datafeeds.csv_alliance_selections_parser import CSVAllianceSelectionsParser
from datafeeds.csv_awards_parser import CSVAwardsParser
from datafeeds.offseason_matches_parser import OffseasonMatchesParser


class TbaCSVBackupEventsEnqueue(webapp.RequestHandler):
    """
    Enqueues CSV backup
    """
    def get(self, year=None):
        if year is None:
            years = range(1992, datetime.datetime.now().year + 1)
            for y in years:
                taskqueue.add(
                    url='/tasks/enqueue/csv_backup_events/{}'.format(y),
                    method='GET')
            self.response.out.write("Enqueued backup for years: {}".format(years))
        else:
            event_keys = Event.query(Event.year == int(year)).fetch(None, keys_only=True)

            for event_key in event_keys:
                taskqueue.add(
                    url='/tasks/do/csv_backup_event/{}'.format(event_key.id()),
                    method='GET')

            template_values = {'event_keys': event_keys}
            path = os.path.join(os.path.dirname(__file__), '../templates/backup/csv_backup_enqueue.html')
            self.response.out.write(template.render(path, template_values))


class TbaCSVBackupEventDo(webapp.RequestHandler):
    """
    Backs up event awards, matches, team list, rankings, and alliance selection order
    """

    AWARDS_FILENAME_PATTERN = '/tbatv-prod-hrd.appspot.com/tba-data-backup/events/{}/{}/{}_awards.csv'  # % (year, event_key, event_key)
    MATCHES_FILENAME_PATTERN = '/tbatv-prod-hrd.appspot.com/tba-data-backup/events/{}/{}/{}_matches.csv'  # % (year, event_key, event_key)
    TEAMS_FILENAME_PATTERN = '/tbatv-prod-hrd.appspot.com/tba-data-backup/events/{}/{}/{}_teams.csv'  # % (year, event_key, event_key)
    RANKINGS_FILENAME_PATTERN = '/tbatv-prod-hrd.appspot.com/tba-data-backup/events/{}/{}/{}_rankings.csv'  # % (year, event_key, event_key)
    ALLIANCES_FILENAME_PATTERN = '/tbatv-prod-hrd.appspot.com/tba-data-backup/events/{}/{}/{}_alliances.csv'  # % (year, event_key, event_key)

    def get(self, event_key):
        event = Event.get_by_id(event_key)

        event.prepAwardsMatchesTeams()

        if event.awards:
            with cloudstorage.open(self.AWARDS_FILENAME_PATTERN.format(event.year, event_key, event_key), 'w') as awards_file:
                writer = csv.writer(awards_file, delimiter=',')
                for award in event.awards:
                    for recipient in award.recipient_list:
                        team = recipient['team_number']
                        if type(team) == int:
                            team = 'frc{}'.format(team)
                        self._writerow_unicode(writer, [award.key.id(), award.name_str, team, recipient['awardee']])

        if event.matches:
            with cloudstorage.open(self.MATCHES_FILENAME_PATTERN.format(event.year, event_key, event_key), 'w') as matches_file:
                writer = csv.writer(matches_file, delimiter=',')
                for match in event.matches:
                    red_score = match.alliances['red']['score']
                    blue_score = match.alliances['blue']['score']
                    self._writerow_unicode(writer, [match.key.id()] + match.alliances['red']['teams'] + match.alliances['blue']['teams'] + [red_score, blue_score])

        if event.teams:
            with cloudstorage.open(self.TEAMS_FILENAME_PATTERN.format(event.year, event_key, event_key), 'w') as teams_file:
                writer = csv.writer(teams_file, delimiter=',')
                self._writerow_unicode(writer, [team.key.id() for team in event.teams])

        if event.rankings:
            with cloudstorage.open(self.RANKINGS_FILENAME_PATTERN.format(event.year, event_key, event_key), 'w') as rankings_file:
                writer = csv.writer(rankings_file, delimiter=',')
                for row in event.rankings:
                    self._writerow_unicode(writer, row)

        if event.alliance_selections:
            with cloudstorage.open(self.ALLIANCES_FILENAME_PATTERN.format(event.year, event_key, event_key), 'w') as alliances_file:
                writer = csv.writer(alliances_file, delimiter=',')
                for alliance in event.alliance_selections:
                    self._writerow_unicode(writer, alliance['picks'])

        self.response.out.write("Done backing up {}!".format(event_key))

    def _writerow_unicode(self, writer, row):
        unicode_row = []
        for s in row:
            try:
                unicode_row.append(s.encode("utf-8"))
            except:
                unicode_row.append(s)
        writer.writerow(unicode_row)


class TbaCSVRestoreEventsEnqueue(webapp.RequestHandler):
    """
    Enqueues CSV restore
    """
    def get(self, year=None):
        if tba_config.CONFIG["env"] == "prod":  # disable in prod for now
            logging.error("Tried to restore {} from CSV in prod! No can do.".format(event_key))
            return

        if year is None:
            years = range(1992, datetime.datetime.now().year + 1)
            for y in years:
                taskqueue.add(
                    url='/tasks/enqueue/csv_restore_events/{}'.format(y),
                    method='GET')
            self.response.out.write("Enqueued restore for years: {}".format(years))
        else:
            event_keys = Event.query(Event.year == int(year)).fetch(None, keys_only=True)

            for event_key in event_keys:
                taskqueue.add(
                    url='/tasks/do/csv_restore_event/{}'.format(event_key.id()),
                    method='GET')

            template_values = {'event_keys': event_keys}
            path = os.path.join(os.path.dirname(__file__), '../templates/backup/csv_restore_enqueue.html')
            self.response.out.write(template.render(path, template_values))


class TbaCSVRestoreEventDo(webapp.RequestHandler):
    """
    Restores event awards, matches, team list, rankings, and alliance selection order
    """

    BASE_URL = 'https://raw.githubusercontent.com/the-blue-alliance/tba-data-backup/master/events/{}/{}/'  # % (year, event_key)
    ALLIANCES_URL = BASE_URL + '{}_alliances.csv'  # % (year, event_key, event_key)
    AWARDS_URL = BASE_URL + '{}_awards.csv'  # % (year, event_key, event_key)
    MATCHES_URL = BASE_URL + '{}_matches.csv'  # % (year, event_key, event_key)
    RANKINGS_URL = BASE_URL + '{}_rankings.csv'  # % (year, event_key, event_key)
    # TEAMS_URL = BASE_URL + '{}_teams.csv'  # % (year, event_key, event_key)  # currently unused

    def get(self, event_key):
        if tba_config.CONFIG["env"] == "prod":  # disable in prod for now
            logging.error("Tried to restore {} from CSV in prod! No can do.".format(event_key))
            return

        event = Event.get_by_id(event_key)

        # alliances
        result = urlfetch.fetch(self.ALLIANCES_URL.format(event.year, event_key, event_key))
        if result.status_code != 200:
            logging.warning('Unable to retreive url: ' + (self.ALLIANCES_URL.format(event.year, event_key, event_key)))
        else:
            data = result.content.replace('frc', '')
            alliance_selections = CSVAllianceSelectionsParser.parse(data)
            if alliance_selections and event.alliance_selections != alliance_selections:
                event.alliance_selections_json = json.dumps(alliance_selections)
                event._alliance_selections = None
                event.dirty = True
            EventManipulator.createOrUpdate(event)

        # awards
        result = urlfetch.fetch(self.AWARDS_URL.format(event.year, event_key, event_key))
        if result.status_code != 200:
            logging.warning('Unable to retreive url: ' + (self.AWARDS_URL.format(event.year, event_key, event_key)))
        else:
            # convert into expected input format
            data = StringIO.StringIO()
            writer = csv.writer(data, delimiter=',')
            for row in csv.reader(StringIO.StringIO(result.content), delimiter=','):
                writer.writerow([event.year, event.event_short, row[1], row[2].replace('frc', ''), row[3]])

            awards = []
            for award in CSVAwardsParser.parse(data.getvalue()):
                awards.append(Award(
                    id=Award.render_key_name(event.key_name, award['award_type_enum']),
                    name_str=award['name_str'],
                    award_type_enum=award['award_type_enum'],
                    year=event.year,
                    event=event.key,
                    event_type_enum=event.event_type_enum,
                    team_list=[ndb.Key(Team, 'frc{}'.format(team_number)) for team_number in award['team_number_list']],
                    recipient_json_list=award['recipient_json_list']
                ))
            AwardManipulator.createOrUpdate(awards)

        # matches
        result = urlfetch.fetch(self.MATCHES_URL.format(event.year, event_key, event_key))
        if result.status_code != 200:
            logging.warning('Unable to retreive url: ' + (self.MATCHES_URL.format(event.year, event_key, event_key)))
        else:
            data = result.content.replace('frc', '').replace('{}_'.format(event_key), '')
            match_dicts = OffseasonMatchesParser.parse(data)
            matches = [
                Match(
                    id=Match.renderKeyName(
                        event.key.id(),
                        match.get("comp_level", None),
                        match.get("set_number", 0),
                        match.get("match_number", 0)),
                    event=event.key,
                    game=Match.FRC_GAMES_BY_YEAR.get(event.year, "frc_unknown"),
                    set_number=match.get("set_number", 0),
                    match_number=match.get("match_number", 0),
                    comp_level=match.get("comp_level", None),
                    team_key_names=match.get("team_key_names", None),
                    alliances_json=match.get("alliances_json", None)
                )
            for match in match_dicts]
            MatchManipulator.createOrUpdate(matches)

        # rankings
        result = urlfetch.fetch(self.RANKINGS_URL.format(event.year, event_key, event_key))
        if result.status_code != 200:
            logging.warning('Unable to retreive url: ' + (self.RANKINGS_URL.format(event.year, event_key, event_key)))
        else:
            # convert into expected input format
            rankings = list(csv.reader(StringIO.StringIO(result.content), delimiter=','))
            if rankings and event.rankings != rankings:
                event.rankings_json = json.dumps(rankings)
                event._rankings = None
                event.dirty = True
            EventManipulator.createOrUpdate(event)

        self.response.out.write("Done restoring {}!".format(event_key))


class TbaCSVBackupTeamsEnqueue(webapp.RequestHandler):
    """
    Enqueues CSV teams backup
    """
    def get(self):
        taskqueue.add(
            url='/tasks/do/csv_backup_teams',
            method='GET')
        self.response.out.write("Enqueued CSV teams backup")


class TbaCSVBackupTeamsDo(webapp.RequestHandler):
    """
    Backs up teams
    """
    TEAMS_FILENAME_PATTERN = '/tbatv-prod-hrd.appspot.com/tba-data-backup/teams/teams.csv'

    def get(self):
        team_keys = Team.query().order(Team.team_number).fetch(None, keys_only=True)
        team_futures = ndb.get_multi_async(team_keys)

        if team_futures:
            with cloudstorage.open(self.TEAMS_FILENAME_PATTERN, 'w') as teams_file:
                writer = csv.writer(teams_file, delimiter=',')
                for team_future in team_futures:
                    team = team_future.get_result()
                    self._writerow_unicode(writer, [team.key.id(), team.nickname, team.name, team.address, team.website, team.rookie_year])

        self.response.out.write("Done backing up teams!")

    def _writerow_unicode(self, writer, row):
        unicode_row = []
        for s in row:
            try:
                unicode_row.append(s.encode("utf-8"))
            except:
                unicode_row.append(s)
        writer.writerow(unicode_row)

########NEW FILE########
__FILENAME__ = base_controller
import logging
import webapp2

from time import mktime
from wsgiref.handlers import format_date_time

from google.appengine.api import memcache

import tba_config

from helpers.user_bundle import UserBundle


class CacheableHandler(webapp2.RequestHandler):
    """
    Provides a standard way of caching the output of pages.
    Currently only supports logged-out pages.
    """
    CACHE_KEY_FORMAT = ''

    def __init__(self, *args, **kw):
        super(CacheableHandler, self).__init__(*args, **kw)
        self._cache_expiration = 0
        if not hasattr(self, '_cache_key'):
            self._cache_key = self.CACHE_KEY_FORMAT

        # Cache all pages for 61 seconds, unless overwritten.
        if self.response is not None:
            self.response.headers['Cache-Control'] = 'public, max-age=61'
            self.response.headers['Pragma'] = 'Public'

    @property
    def full_cache_key(self):
        return self._get_full_cache_key(self._cache_key)

    @classmethod
    def _get_full_cache_key(cls, cache_key):
        return "{}:{}:{}".format(
            cache_key,
            cls.CACHE_VERSION,
            tba_config.CONFIG["static_resource_version"])

    def get(self, *args, **kw):
        cached_response = self._read_cache()
        if cached_response:
            self.response.out.write(cached_response.body)
            self.response.headers = cached_response.headers
        else:
            self.response.out.write(self._render(*args, **kw))
            self._write_cache(self.response)

    def _has_been_modified_since(self, datetime):
        last_modified = format_date_time(mktime(datetime.timetuple()))
        if_modified_since = self.request.headers.get('If-Modified-Since')
        if if_modified_since and if_modified_since == last_modified:
            self.response.set_status(304)
            return False
        else:
            self.response.headers['Last-Modified'] = last_modified
            return True

    def memcacheFlush(self):
        memcache.delete(self.full_cache_key)
        return self.full_cache_key

    @classmethod
    def clear_cache(cls, *args):
        full_cache_key = cls._get_full_cache_key(cls.CACHE_KEY_FORMAT.format(*args))
        memcache.delete(full_cache_key)
        logging.info("Deleting cache key: {}".format(full_cache_key))

    def _read_cache(self):
        return memcache.get(self.full_cache_key)

    def _render(self):
        raise NotImplementedError("No _render method.")

    def _write_cache(self, response):
        if tba_config.CONFIG["memcache"]:
            memcache.set(self.full_cache_key, response, self._cache_expiration)


class LoggedInHandler(webapp2.RequestHandler):
    """
    Provides a base set of functionality for pages that need logins.
    Currently does not support caching as easily as CacheableHandler.
    """

    def __init__(self, *args, **kw):
        super(LoggedInHandler, self).__init__(*args, **kw)
        self.user_bundle = UserBundle()
        self.template_values = {
            "user_bundle": self.user_bundle
        }

    def _require_admin(self):
        self._require_login()
        if not self.user_bundle.is_current_user_admin:
            return self.redirect(self.user_bundle.login_url, abort=True)

    def _require_login(self, target_url="/"):
        if not self.user_bundle.user:
            return self.redirect(
                self.user_bundle.create_login_url(target_url),
                abort=True
            )

########NEW FILE########
__FILENAME__ = cron_controller
import datetime
import logging
import os
import json

from google.appengine.api import taskqueue

from google.appengine.ext import ndb

from google.appengine.ext import webapp
from google.appengine.ext.webapp import template

from helpers.event_helper import EventHelper
from helpers.event_manipulator import EventManipulator

from helpers.event_team_manipulator import EventTeamManipulator
from helpers.event_team_repairer import EventTeamRepairer
from helpers.event_team_updater import EventTeamUpdater

from helpers.insight_manipulator import InsightManipulator
from helpers.team_manipulator import TeamManipulator
from helpers.matchstats_helper import MatchstatsHelper
from helpers.insights_helper import InsightsHelper

from helpers.match_manipulator import MatchManipulator

from models.event import Event
from models.event_team import EventTeam
from models.match import Match
from models.team import Team
from models.typeahead_entry import TypeaheadEntry


class EventShortNameCalcEnqueue(webapp.RequestHandler):
    """
    Enqueues Event short_name computation for official events
    """
    def get(self, year):
        event_keys = Event.query(Event.official == True, Event.year == int(year)).fetch(200, keys_only=True)
        events = ndb.get_multi(event_keys)

        for event in events:
            taskqueue.add(
                url='/tasks/math/do/event_short_name_calc_do/{}'.format(event.key.id()),
                method='GET')

        template_values = {'events': events}
        path = os.path.join(os.path.dirname(__file__), '../templates/math/event_short_name_calc_enqueue.html')
        self.response.out.write(template.render(path, template_values))


class EventShortNameCalcDo(webapp.RequestHandler):
    """
    Computes Event short_name
    """
    def get(self, event_key):
        event = Event.get_by_id(event_key)
        event.short_name = EventHelper.getShortName(event.name)
        event.dirty = True  # TODO: hacky
        EventManipulator.createOrUpdate(event)

        template_values = {'event': event}
        path = os.path.join(os.path.dirname(__file__), '../templates/math/event_short_name_calc_do.html')
        self.response.out.write(template.render(path, template_values))


class EventTeamRepairDo(webapp.RequestHandler):
    """
    Repair broken EventTeams.
    """
    def get(self):
        event_teams_keys = EventTeam.query(EventTeam.year == None).fetch(keys_only=True)
        event_teams = ndb.get_multi(event_teams_keys)

        event_teams = EventTeamRepairer.repair(event_teams)
        event_teams = EventTeamManipulator.createOrUpdate(event_teams)

        # sigh. -gregmarra
        if type(event_teams) == EventTeam:
            event_teams = [event_teams]

        template_values = {
            'event_teams': event_teams,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/math/eventteam_repair_do.html')
        self.response.out.write(template.render(path, template_values))


class EventTeamUpdate(webapp.RequestHandler):
    """
    Task that updates the EventTeam index for an Event.
    Can only update or delete EventTeams for unregistered teams.
    ^^^ Does it actually do this? Eugene -- 2013/07/30
    """
    def get(self, event_key):
        teams, event_teams, et_keys_to_del = EventTeamUpdater.update(event_key)

        teams = TeamManipulator.createOrUpdate(teams)

        if teams:
            event_teams = EventTeamManipulator.createOrUpdate(event_teams)

        if et_keys_to_del:
            EventTeamManipulator.delete_keys(et_keys_to_del)

        template_values = {
            'event_teams': event_teams,
            'deleted_event_teams_keys': et_keys_to_del
        }

        path = os.path.join(os.path.dirname(__file__),
                            '../templates/math/eventteam_update_do.html')
        self.response.out.write(template.render(path, template_values))


class EventTeamUpdateEnqueue(webapp.RequestHandler):
    """
    Handles enqueing building attendance for Events.
    """
    def get(self, when):
        if when == "all":
            event_keys = Event.query().fetch(10000, keys_only=True)
        else:
            event_keys = Event.query(Event.year == int(when)).fetch(10000, keys_only=True)

        for event_key in event_keys:
            taskqueue.add(
                url='/tasks/math/do/eventteam_update/' + event_key.id(),
                method='GET')

        template_values = {
            'event_keys': event_keys,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/math/eventteam_update_enqueue.html')
        self.response.out.write(template.render(path, template_values))


class EventMatchstatsDo(webapp.RequestHandler):
    """
    Calculates match stats (OPR/DPR/CCWM) for an event
    """
    def get(self, event_key):
        event = Event.get_by_id(event_key)
        matchstats_dict = MatchstatsHelper.calculate_matchstats(event.matches)
        if matchstats_dict != {}:
            event.matchstats_json = json.dumps(matchstats_dict)
            event.dirty = True  # TODO: hacky
            EventManipulator.createOrUpdate(event)
        else:
            logging.warn("Matchstat calculation for {} failed!".format(event_key))

        template_values = {
            'matchstats_dict': matchstats_dict,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/math/event_matchstats_do.html')
        self.response.out.write(template.render(path, template_values))

    def post(self):
        self.get()


class EventMatchstatsEnqueue(webapp.RequestHandler):
    """
    Enqueues Matchstats calculation
    """
    def get(self, when):
        if when == "now":
            events = EventHelper.getEventsWithinADay()
        else:
            events = Event.query(Event.year == int(when)).fetch(500)

        for event in events:
            taskqueue.add(
                url='/tasks/math/do/event_matchstats/' + event.key_name,
                method='GET')

        template_values = {
            'event_count': len(events),
            'year': when
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/math/event_matchstats_enqueue.html')
        self.response.out.write(template.render(path, template_values))


class FinalMatchesRepairDo(webapp.RequestHandler):
    """
    Repairs zero-indexed final matches
    """
    def get(self, year):
        year_event_keys = Event.query(Event.year == int(year)).fetch(1000, keys_only=True)

        final_match_keys = []
        for event_key in year_event_keys:
            final_match_keys.extend(Match.query(Match.event == event_key, Match.comp_level == 'f').fetch(100, keys_only=True))

        match_keys_to_repair = []
        for match_key in final_match_keys:
            key_name = match_key.id()
            if '_f0m' in key_name:
                match_keys_to_repair.append(match_key)

        deleted_keys = []
        matches_to_repair = ndb.get_multi(match_keys_to_repair)
        for match in matches_to_repair:
            deleted_keys.append(match.key)

            event = ndb.get_multi([match.event])[0]
            match.set_number = 1
            match.key = ndb.Key(Match, Match.renderKeyName(
                event.key.id(),
                match.comp_level,
                match.set_number,
                match.match_number))
            match.dirty = True  # hacky

        MatchManipulator.createOrUpdate(matches_to_repair)
        MatchManipulator.delete_keys(deleted_keys)

        template_values = {'deleted_keys': deleted_keys,
                           'new_matches': matches_to_repair}

        path = os.path.join(os.path.dirname(__file__), '../templates/math/final_matches_repair_do.html')
        self.response.out.write(template.render(path, template_values))


class YearInsightsEnqueue(webapp.RequestHandler):
    """
    Enqueues Insights calculation of a given kind for a given year
    """
    def get(self, kind, year):
        taskqueue.add(
            url='/tasks/math/do/insights/{}/{}'.format(kind, year),
            method='GET')

        template_values = {
            'kind': kind,
            'year': year
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/math/year_insights_enqueue.html')
        self.response.out.write(template.render(path, template_values))


class YearInsightsDo(webapp.RequestHandler):
    """
    Calculates insights of a given kind for a given year.
    Calculations of a given kind should reuse items fetched from the datastore.
    """

    def get(self, kind, year):
        year = int(year)

        insights = None
        if kind == 'matches':
            insights = InsightsHelper.doMatchInsights(year)
        elif kind == 'awards':
            insights = InsightsHelper.doAwardInsights(year)

        if insights != None:
            InsightManipulator.createOrUpdate(insights)

        template_values = {
            'insights': insights,
            'year': year,
            'kind': kind,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/math/year_insights_do.html')
        self.response.out.write(template.render(path, template_values))

    def post(self):
        self.get()


class OverallInsightsEnqueue(webapp.RequestHandler):
    """
    Enqueues Overall Insights calculation for a given kind.
    """
    def get(self, kind):
        taskqueue.add(
            url='/tasks/math/do/overallinsights/{}'.format(kind),
            method='GET')

        template_values = {
            'kind': kind,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/math/overall_insights_enqueue.html')
        self.response.out.write(template.render(path, template_values))


class OverallInsightsDo(webapp.RequestHandler):
    """
    Calculates overall insights of a given kind.
    Calculations of a given kind should reuse items fetched from the datastore.
    """

    def get(self, kind):
        insights = None
        if kind == 'matches':
            insights = InsightsHelper.doOverallMatchInsights()
        elif kind == 'awards':
            insights = InsightsHelper.doOverallAwardInsights()

        if insights != None:
            InsightManipulator.createOrUpdate(insights)

        template_values = {
            'insights': insights,
            'kind': kind,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/math/overall_insights_do.html')
        self.response.out.write(template.render(path, template_values))

    def post(self):
        self.get()


class TypeaheadCalcEnqueue(webapp.RequestHandler):
    """
    Enqueues typeahead calculations
    """
    def get(self):
        taskqueue.add(url='/tasks/math/do/typeaheadcalc', method='GET')
        template_values = {}
        path = os.path.join(os.path.dirname(__file__), '../templates/math/typeaheadcalc_enqueue.html')
        self.response.out.write(template.render(path, template_values))


class TypeaheadCalcDo(webapp.RequestHandler):
    """
    Calculates typeahead entries
    """
    def get(self):
        @ndb.tasklet
        def get_events_async():
            event_keys = yield Event.query().order(-Event.year).order(Event.name).fetch_async(keys_only=True)
            events = yield ndb.get_multi_async(event_keys)
            raise ndb.Return(events)

        @ndb.tasklet
        def get_teams_async():
            team_keys = yield Team.query().order(Team.team_number).fetch_async(keys_only=True)
            teams = yield ndb.get_multi_async(team_keys)
            raise ndb.Return(teams)

        @ndb.toplevel
        def get_events_and_teams():
            events, teams = yield get_events_async(), get_teams_async()
            raise ndb.Return((events, teams))

        events, teams = get_events_and_teams()

        results = {}
        for team in teams:
            if not team.nickname:
                nickname = "Team %s" % team.team_number
            else:
                nickname = team.nickname
            data = '%s | %s' % (team.team_number, nickname)
            if TypeaheadEntry.ALL_TEAMS_KEY in results:
                results[TypeaheadEntry.ALL_TEAMS_KEY].append(data)
            else:
                results[TypeaheadEntry.ALL_TEAMS_KEY] = [data]

        for event in events:
            data = '%s %s [%s]' % (event.year, event.name, event.event_short.upper())
            # all events
            if TypeaheadEntry.ALL_EVENTS_KEY in results:
                results[TypeaheadEntry.ALL_EVENTS_KEY].append(data)
            else:
                results[TypeaheadEntry.ALL_EVENTS_KEY] = [data]
            # events by year
            if TypeaheadEntry.YEAR_EVENTS_KEY.format(event.year) in results:
                results[TypeaheadEntry.YEAR_EVENTS_KEY.format(event.year)].append(data)
            else:
                results[TypeaheadEntry.YEAR_EVENTS_KEY.format(event.year)] = [data]

        # Prepare to remove old entries
        old_entry_keys_future = TypeaheadEntry.query().fetch_async(keys_only=True)

        # Add new entries
        entries = []
        for key_name, data in results.items():
            entries.append(TypeaheadEntry(id=key_name, data_json=json.dumps(data)))
        ndb.put_multi(entries)

        # Remove old entries
        old_entry_keys = set(old_entry_keys_future.get_result())
        new_entry_keys = set([ndb.Key(TypeaheadEntry, key_name) for key_name in results.keys()])
        keys_to_delete = old_entry_keys.difference(new_entry_keys)
        logging.info("Removing the following unused TypeaheadEntries: {}".format([key.id() for key in keys_to_delete]))
        ndb.delete_multi(keys_to_delete)

        template_values = {'results': results}
        path = os.path.join(os.path.dirname(__file__), '../templates/math/typeaheadcalc_do.html')
        self.response.out.write(template.render(path, template_values))

########NEW FILE########
__FILENAME__ = datafeed_controller
import logging
import os
import datetime
import time
import json

from google.appengine.api import taskqueue
from google.appengine.ext import ndb
from google.appengine.ext import webapp
from google.appengine.ext.webapp import template

from consts.event_type import EventType

from datafeeds.datafeed_fms import DatafeedFms
from datafeeds.datafeed_tba import DatafeedTba
from datafeeds.datafeed_usfirst import DatafeedUsfirst
from datafeeds.datafeed_usfirst_legacy import DatafeedUsfirstLegacy
from datafeeds.datafeed_offseason import DatafeedOffseason
from datafeeds.datafeed_twitter import DatafeedTwitter

from helpers.event_helper import EventHelper
from helpers.event_manipulator import EventManipulator
from helpers.event_team_manipulator import EventTeamManipulator
from helpers.match_manipulator import MatchManipulator
from helpers.match_helper import MatchHelper
from helpers.award_manipulator import AwardManipulator
from helpers.team_manipulator import TeamManipulator

from models.event import Event
from models.event_team import EventTeam
from models.team import Team

from helpers.firebase.firebase_pusher import FirebasePusher


class FmsEventListGet(webapp.RequestHandler):
    """
    Fetch basic data about all current season events at once.
    """
    def get(self):
        df = DatafeedFms()
        events = df.getFmsEventList()

        # filter if first_eid is too high, meaning its a Championship Division
        # (we manually add these due to naming issues)
        events = filter(lambda e: int(e.first_eid) < 100000, events)

        events = EventManipulator.createOrUpdate(events)

        template_values = {
            "events": events
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/fms_event_list_get.html')
        self.response.out.write(template.render(path, template_values))


class FmsTeamListGet(webapp.RequestHandler):
    """
    Fetch basic data about all current season teams at once.
    Doesn't get tpids or full data.
    """
    def get(self):
        df = DatafeedFms()
        teams = df.getFmsTeamList()
        TeamManipulator.createOrUpdate(teams)

        template_values = {
            "teams": teams
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/fms_team_list_get.html')
        self.response.out.write(template.render(path, template_values))


class TbaVideosEnqueue(webapp.RequestHandler):
    """
    Handles enqueing grabing tba_videos for Matches at individual Events.
    """
    def get(self):
        events = Event.query()

        for event in events:
            taskqueue.add(
                url='/tasks/get/tba_videos/' + event.key_name,
                method='GET')

        template_values = {
            'event_count': Event.query().count(),
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/tba_videos_enqueue.html')
        self.response.out.write(template.render(path, template_values))


class TbaVideosGet(webapp.RequestHandler):
    """
    Handles reading a TBA video listing page and updating the match objects in the datastore as needed.
    """
    def get(self, event_key):
        df = DatafeedTba()

        event = Event.get_by_id(event_key)
        match_filetypes = df.getVideos(event)
        if match_filetypes:
            matches_to_put = []
            for match in event.matches:
                if match.tba_videos != match_filetypes.get(match.key_name, []):
                    match.tba_videos = match_filetypes.get(match.key_name, [])
                    match.dirty = True
                    matches_to_put.append(match)

            MatchManipulator.createOrUpdate(matches_to_put)

            tbavideos = match_filetypes.items()
        else:
            logging.info("No tbavideos found for event " + event.key_name)
            tbavideos = []

        template_values = {
            'tbavideos': tbavideos,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/tba_videos_get.html')
        self.response.out.write(template.render(path, template_values))


class UsfirstEventDetailsEnqueue(webapp.RequestHandler):
    """
    Handles enqueing updates to individual USFIRST events.
    """
    def get(self, year):
        event_keys = Event.query(Event.first_eid != None, Event.year == int(year)).fetch(200, keys_only=True)
        events = ndb.get_multi(event_keys)

        for event in events:
            taskqueue.add(
                queue_name='usfirst',
                url='/tasks/get/usfirst_event_details/%s/%s' % (year, event.first_eid),
                method='GET')

        template_values = {
            'event_count': len(events),
            'year': year,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/usfirst_events_details_enqueue.html')
        self.response.out.write(template.render(path, template_values))


class UsfirstEventDetailsGet(webapp.RequestHandler):
    """
    Handles reading a USFIRST event page and creating or updating the model as needed.
    Includes registered Teams.
    """
    def get(self, year, first_eid):
        df = DatafeedUsfirst()
        df_legacy = DatafeedUsfirstLegacy()

        event = df.getEventDetails(first_eid)
        if not event:
            logging.warning("getEventDetails with DatafeedUsfirst for event id {} failed. Retrying with DatafeedUsfirstLegacy.".format(first_eid))
            event = df_legacy.getEventDetails(int(year), first_eid)

        if self.request.get('event_district_enum'):
            event.event_district_enum = int(self.request.get('event_district_enum'))
        event = EventManipulator.createOrUpdate(event)

        teams = df.getEventTeams(int(year), first_eid)
        if not teams:
            logging.warning("getEventTeams with DatafeedUsfirst for event id {} failed. Retrying with DatafeedUsfirstLegacy.".format(first_eid))
            teams = df_legacy.getEventTeams(int(year), first_eid)
            if not teams:
                logging.warning("getEventTeams with DatafeedUsfirstLegacy for event id {} failed.".format(first_eid))
                teams = []

        teams = TeamManipulator.createOrUpdate(teams)

        if teams:
            if type(teams) is not list:
                teams = [teams]

            event_teams = [EventTeam(
                id=event.key.id() + "_" + team.key.id(),
                event=event.key,
                team=team.key,
                year=event.year)
                for team in teams]

            # Delete eventteams of teams that unregister from an event
            if event.future:
                existing_event_team_keys = set(EventTeam.query(EventTeam.event == event.key).fetch(1000, keys_only=True))
                event_team_keys = set([et.key for et in event_teams])
                et_keys_to_delete = existing_event_team_keys.difference(event_team_keys)
                EventTeamManipulator.delete_keys(et_keys_to_delete)

            event_teams = EventTeamManipulator.createOrUpdate(event_teams)
            if type(event_teams) is not list:
                event_teams = [event_teams]
        else:
            event_teams = []

        template_values = {
            'event': event,
            'event_teams': event_teams,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/usfirst_event_details_get.html')
        self.response.out.write(template.render(path, template_values))


class UsfirstAwardsEnqueue(webapp.RequestHandler):
    """
    Handles enqueing getting awards for USFIRST events.
    """
    def get(self, when):
        if when == "now":
            events = EventHelper.getEventsWithinADay()
        else:
            event_keys = Event.query(Event.official == True).filter(Event.year == int(when)).fetch(500, keys_only=True)
            events = ndb.get_multi(event_keys)

        for event in events:
            taskqueue.add(
                queue_name='usfirst',
                url='/tasks/get/usfirst_awards/%s' % (event.key_name),
                method='GET')
        template_values = {
            'events': events,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/usfirst_awards_enqueue.html')
        self.response.out.write(template.render(path, template_values))


class UsfirstAwardsGet(webapp.RequestHandler):
    """
    Handles reading a USFIRST match results page and updating the datastore as needed.
    Also creates EventTeams.
    """
    def get(self, event_key):
        datafeed = DatafeedUsfirst()

        event = Event.get_by_id(event_key)
        new_awards = AwardManipulator.createOrUpdate(datafeed.getEventAwards(event))
        if new_awards is None:
            new_awards = []
        elif type(new_awards) != list:
            new_awards = [new_awards]

        # create EventTeams
        team_ids = set()
        for award in new_awards:
            for team in award.team_list:
                team_ids.add(team.id())
        teams = TeamManipulator.createOrUpdate([Team(
            id=team_id,
            team_number=int(team_id[3:]))
            for team_id in team_ids])
        if teams:
            if type(teams) is not list:
                teams = [teams]
            event_teams = EventTeamManipulator.createOrUpdate([EventTeam(
                id=event_key + "_" + team.key.id(),
                event=event.key,
                team=team.key,
                year=event.year)
                for team in teams])

        template_values = {
            'awards': new_awards,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/usfirst_awards_get.html')
        self.response.out.write(template.render(path, template_values))


class UsfirstEventAlliancesEnqueue(webapp.RequestHandler):
    """
    Handles enqueing getting alliances for USFIRST events.
    """
    def get(self, when):
        if when == "now":
            events = EventHelper.getEventsWithinADay()
        else:
            event_keys = Event.query(Event.official == True).filter(Event.year == int(when)).fetch(500, keys_only=True)
            events = ndb.get_multi(event_keys)

        for event in events:
            taskqueue.add(
                queue_name='usfirst',
                url='/tasks/get/usfirst_event_alliances/' + event.key_name,
                method='GET')

        template_values = {
            'events': events,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/usfirst_event_alliances_enqueue.html')
        self.response.out.write(template.render(path, template_values))


class UsfirstEventAlliancesGet(webapp.RequestHandler):
    """
    Handles reading a USFIRST alliances page and updating the datastore as needed.
    """
    def get(self, event_key):
        df = DatafeedUsfirst()

        event = Event.get_by_id(event_key)

        if event.event_type_enum == EventType.CMP_FINALS:
            logging.info("Skipping Einstein alliance selections")
            return

        alliance_selections = df.getEventAlliances(event)
        if alliance_selections and event.alliance_selections != alliance_selections:
            event.alliance_selections_json = json.dumps(alliance_selections)
            event._alliance_selections = None
            event.dirty = True

        EventManipulator.createOrUpdate(event)

        template_values = {'alliance_selections': alliance_selections,
                           'event_name': event.key_name}

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/usfirst_event_alliances_get.html')
        self.response.out.write(template.render(path, template_values))


class UsfirstEventListGet(webapp.RequestHandler):
    """
    Handles reading the USFIRST event list.
    Enqueues a bunch of detailed reads that actually establish Event objects.
    """
    def get(self, year):
        df = DatafeedUsfirst()
        events = df.getEventList(int(year))

        for event in events:
            taskqueue.add(
                queue_name='usfirst',
                url='/tasks/get/usfirst_event_details/%s/%s' % (year, event.first_eid),
                params={'event_district_enum': event.event_district_enum},  # district info is not available on event detail pages
                method='GET')

        template_values = {
            'events': events,
            'year': year
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/usfirst_event_list_get.html')
        self.response.out.write(template.render(path, template_values))


class UsfirstMatchesEnqueue(webapp.RequestHandler):
    """
    Handles enqueing getting match results for USFIRST events.
    """
    def get(self, when):
        if when == "now":
            events = EventHelper.getEventsWithinADay()
        else:
            event_keys = Event.query(Event.official == True).filter(Event.year == int(when)).fetch(500, keys_only=True)
            events = ndb.get_multi(event_keys)

        for event in events:
            taskqueue.add(
                queue_name='usfirst',
                url='/tasks/get/usfirst_matches/' + event.key_name,
                method='GET')

        template_values = {
            'events': events,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/usfirst_matches_enqueue.html')
        self.response.out.write(template.render(path, template_values))


class UsfirstMatchesGet(webapp.RequestHandler):
    """
    Handles reading a USFIRST match results page and updating the datastore as needed.
    """
    def get(self, event_key):
        df = DatafeedUsfirst()

        event = Event.get_by_id(event_key)
        new_matches = MatchManipulator.createOrUpdate(df.getMatches(event))

        if new_matches:
            for match in new_matches:
                if hasattr(match, 'dirty') and match.dirty:
                    # Enqueue push notification
                    try:
                        FirebasePusher.updated_event(event.key_name)
                    except:
                        logging.warning("Enqueuing Firebase push failed!")
                    # Enqueue task to calculate matchstats
                    taskqueue.add(
                            url='/tasks/math/do/event_matchstats/' + event.key_name,
                            method='GET')
                    break


        template_values = {
            'matches': new_matches,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/usfirst_matches_get.html')
        self.response.out.write(template.render(path, template_values))


class UsfirstEventRankingsEnqueue(webapp.RequestHandler):
    """
    Handles enqueing getting rankings for USFIRST events.
    """
    def get(self, when):
        if when == "now":
            events = EventHelper.getEventsWithinADay()
        else:
            event_keys = Event.query(Event.official == True).filter(Event.year == int(when)).fetch(500, keys_only=True)
            events = ndb.get_multi(event_keys)

        for event in events:
            taskqueue.add(
                queue_name='usfirst',
                url='/tasks/get/usfirst_event_rankings/' + event.key_name,
                method='GET')

        template_values = {
            'events': events,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/usfirst_event_rankings_enqueue.html')
        self.response.out.write(template.render(path, template_values))


class UsfirstEventRankingsGet(webapp.RequestHandler):
    """
    Handles reading a USFIRST ranking page and updating the datastore as needed.
    """
    def get(self, event_key):
        df = DatafeedUsfirst()

        event = Event.get_by_id(event_key)
        rankings = df.getEventRankings(event)
        if event.rankings_json != json.dumps(rankings):
            event.rankings_json = json.dumps(rankings)
            event.dirty = True

        EventManipulator.createOrUpdate(event)

        template_values = {'rankings': rankings,
                           'event_name': event.key_name}

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/usfirst_event_rankings_get.html')
        self.response.out.write(template.render(path, template_values))


class UsfirstTeamDetailsEnqueue(webapp.RequestHandler):
    """
    Handles enqueing updates to individual USFIRST teams.
    """
    def get(self):
        offset = int(self.request.get("offset", 0))

        team_keys = Team.query().fetch(1000, offset=int(offset), keys_only=True)
        teams = ndb.get_multi(team_keys)
        for team in teams:
            taskqueue.add(
                queue_name='usfirst',
                url='/tasks/get/usfirst_team_details/' + team.key_name,
                method='GET')

        # FIXME omg we're just writing out? -gregmarra 2012 Aug 26
        self.response.out.write("%s team gets have been enqueued offset from %s.<br />" % (len(teams), offset))
        self.response.out.write("Reload with ?offset=%s to enqueue more." % (offset + len(teams)))


class UsfirstTeamDetailsRollingEnqueue(webapp.RequestHandler):
    """
    Handles enqueing updates to individual USFIRST teams.
    Enqueues a certain fraction of teams so that all teams will get updated
    every PERIOD days.
    """
    PERIOD = 14  # a particular team will be updated every PERIOD days

    def get(self):
        now_epoch = time.mktime(datetime.datetime.now().timetuple())
        bucket_num = int((now_epoch / (60 * 60 * 24)) % self.PERIOD)

        highest_team_key = Team.query().order(-Team.team_number).fetch(1, keys_only=True)[0]
        highest_team_num = int(highest_team_key.id()[3:])
        bucket_size = int(highest_team_num / (self.PERIOD)) + 1

        min_team = bucket_num * bucket_size
        max_team = min_team + bucket_size
        team_keys = Team.query(Team.team_number >= min_team, Team.team_number < max_team).fetch(1000, keys_only=True)

        teams = ndb.get_multi(team_keys)
        for team in teams:
            taskqueue.add(
                queue_name='usfirst',
                url='/tasks/get/usfirst_team_details/' + team.key_name,
                method='GET')

        # FIXME omg we're just writing out? -fangeugene 2013 Nov 6
        self.response.out.write("Bucket number {} out of {}<br>".format(bucket_num, self.PERIOD))
        self.response.out.write("{} team gets have been enqueued in the interval [{}, {}).".format(len(teams), min_team, max_team))


class UsfirstTeamDetailsGet(webapp.RequestHandler):
    """
    Handles reading a USFIRST team information page and updating the
    model accordingly.
    """
    def get(self, key_name):
        df = DatafeedUsfirst()
        legacy_df = DatafeedUsfirstLegacy()

        team = df.getTeamDetails(Team.get_by_id(key_name))
        if not team:
            logging.warning("getTeamDetails with DatafeedUsfirst for event id {} failed. Retrying with DatafeedUsfirstLegacy.".format(key_name))
            team = legacy_df.getTeamDetails(Team.get_by_id(key_name))
        else:
            legacy_team = legacy_df.getTeamDetails(Team.get_by_id(key_name))
            if legacy_team is not None:
                team.rookie_year = legacy_team.rookie_year  # only available on legacy df

        if team:
            team = TeamManipulator.createOrUpdate(team)
            success = True
        else:
            success = False

        template_values = {
            'key_name': key_name,
            'team': team,
            'success': success,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/usfirst_team_details_get.html')
        self.response.out.write(template.render(path, template_values))


class UsfirstPre2003TeamEventsEnqueue(webapp.RequestHandler):
    def get(self):
        """
        Enqueues TeamEventsGet for teams numbers <= 999 (these teams participated
        in events from 2002 and prior, which we can't scrape normally)
        """
        team_keys = Team.query(Team.team_number <= 999).fetch(10000, keys_only=True)
        teams = ndb.get_multi(team_keys)
        for team in teams:
            taskqueue.add(
                queue_name='usfirst',
                url='/tasks/get/usfirst_pre2003_team_events/{}'.format(team.key_name),
                method='GET')

        self.response.out.write("Pre 2003 event gets have been enqueued for %s teams." % (len(teams)))


class UsfirstPre2003TeamEventsGet(webapp.RequestHandler):
    """
    Handles reading a USFIRST team information page and enqueues tasks to
    create events that the team has attended if the event does not exist in the db.
    Also creates appropriate eventteams.
    Doesn't create Championship Event or Championship Divisions
    """
    def get(self, key_name):
        team_key = ndb.Key(Team, key_name)

        df = DatafeedUsfirst()
        first_eids = df.getPre2003TeamEvents(Team.get_by_id(key_name))

        new_eids = []
        for eid in first_eids:
            event_keys = Event.query(Event.first_eid == eid).fetch(10, keys_only=True)
            if len(event_keys) == 0:  # only create events if event not already in db
                try:
                    event = df.getEventDetails(eid)
                except:
                    logging.warning("getEventDetails for eid {} failed.".format(eid))
                    continue

                if event.event_type_enum in {EventType.CMP_DIVISION, EventType.CMP_FINALS}:
                    if event.year >= 2001:
                        # Divisions started in 2001; need to manually create championship events
                        continue
                    else:
                        # No divisions; force event type to be finals
                        event.event_type_enum = EventType.CMP_FINALS

                event = EventManipulator.createOrUpdate(event)
                new_eids.append(eid)
            else:
                event = event_keys[0].get()

            event_team_key_name = event.key.id() + "_" + team_key.id()
            existing_event_team = ndb.Key(EventTeam, event_team_key_name).get()
            if existing_event_team is None:
                event_team = EventTeam(
                    id=event_team_key_name,
                    event=event.key,
                    team=team_key,
                    year=event.year)
                EventTeamManipulator.createOrUpdate(event_team)

        template_values = {'first_eids': first_eids,
                           'new_eids': new_eids}

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/usfirst_team_events_get.html')
        self.response.out.write(template.render(path, template_values))


class UsfirstTeamsTpidsGet(webapp.RequestHandler):
    """
    A run-as-needed function that instantiates new Team objects based on
    FIRST's full team list.
    """
    def get(self, year):
        df = DatafeedUsfirst()
        skip = 0

        try:
            skip = self.request.get("skip")
            if skip == '':
                skip = 0
        except Exception, detail:
            logging.error('Failed to get skip value')

        logging.info("YEAR: %s", year)
        df.getTeamsTpids(int(year), skip)

        team_count = Team.query().count()

        template_values = {
            'team_count': team_count
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/usfirst_teams_tpids.html')
        self.response.out.write(template.render(path, template_values))


class OffseasonMatchesGet(webapp.RequestHandler):
    """
    Handles reading an offseason match results page and updating the datastore as needed.
    """
    def get(self, event_key):
        df = DatafeedOffseason()

        event = Event.get_by_id(event_key)
        url = self.request.get('url')

        new_matches = MatchManipulator.createOrUpdate(df.getMatches(event, url))

        template_values = {
            'matches': new_matches,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/offseason_matches_get.html')
        self.response.out.write(template.render(path, template_values))


class TwitterFrcfmsMatchesGet(webapp.RequestHandler):
    """
    Handles getting matches from @FRCFMS on Twitter, and returns a table of
    matches that can be manually manipulated and added
    """
    def get(self):
        df = DatafeedTwitter()

        event_matches = df.getMatches()

        template_values = {
            'event_matches': event_matches,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/datafeeds/twitter_frcfms_matches_get.html')
        self.response.out.write(template.render(path, template_values))

########NEW FILE########
__FILENAME__ = event_controller
import datetime
import os

from google.appengine.api import memcache
from google.appengine.ext import ndb
from google.appengine.ext.webapp import template

from base_controller import CacheableHandler
from helpers.match_helper import MatchHelper
from helpers.award_helper import AwardHelper
from helpers.team_helper import TeamHelper
from helpers.event_helper import EventHelper

from models.event import Event


class EventList(CacheableHandler):
    """
    List all Events.
    """
    MAX_YEAR = 2014
    VALID_YEARS = list(reversed(range(1992, MAX_YEAR + 1)))
    CACHE_VERSION = 4
    CACHE_KEY_FORMAT = "event_list_{}_{}"  # (year, explicit_year)

    def __init__(self, *args, **kw):
        super(EventList, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24

    def get(self, year=None, explicit_year=False):
        if year == '':
            return self.redirect("/events")

        if year:
            if not year.isdigit():
                self.abort(404)
            year = int(year)
            if year not in self.VALID_YEARS:
                self.abort(404)
            explicit_year = True
        else:
            year = datetime.datetime.now().year
            explicit_year = False

        self._cache_key = self.CACHE_KEY_FORMAT.format(year, explicit_year)
        super(EventList, self).get(year, explicit_year)

    def _render(self, year=None, explicit_year=False):
        event_keys = Event.query(Event.year == year).fetch(1000, keys_only=True)
        events = ndb.get_multi(event_keys)
        EventHelper.sort_events(events)

        week_events = EventHelper.groupByWeek(events)

        template_values = {
            "events": events,
            "explicit_year": explicit_year,
            "selected_year": year,
            "valid_years": self.VALID_YEARS,
            "week_events": week_events,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/event_list.html')
        return template.render(path, template_values)

    def memcacheFlush(self):
        year = datetime.datetime.now().year
        keys = [self.CACHE_KEY_FORMAT.format(year, True), self.CACHE_KEY_FORMAT.format(year, False)]
        memcache.delete_multi(keys)
        return keys


class EventDetail(CacheableHandler):
    """
    Show an Event.
    event_code like "2010ct"
    """
    LONG_CACHE_EXPIRATION = 60 * 60 * 24
    SHORT_CACHE_EXPIRATION = 60 * 5
    CACHE_VERSION = 3
    CACHE_KEY_FORMAT = "event_detail_{}"  # (event_key)

    def __init__(self, *args, **kw):
        super(EventDetail, self).__init__(*args, **kw)
        self._cache_expiration = self.LONG_CACHE_EXPIRATION

    def get(self, event_key):
        if not event_key:
            return self.redirect("/events")

        self._cache_key = self.CACHE_KEY_FORMAT.format(event_key)
        super(EventDetail, self).get(event_key)

    def _render(self, event_key):
        event = Event.get_by_id(event_key)

        if not event:
            self.abort(404)

        event.prepAwardsMatchesTeams()

        awards = AwardHelper.organizeAwards(event.awards)
        cleaned_matches = MatchHelper.deleteInvalidMatches(event.matches)
        matches = MatchHelper.organizeMatches(cleaned_matches)
        teams = TeamHelper.sortTeams(event.teams)

        num_teams = len(teams)
        middle_value = num_teams / 2
        if num_teams % 2 != 0:
            middle_value += 1
        teams_a, teams_b = teams[:middle_value], teams[middle_value:]

        oprs = [i for i in event.matchstats['oprs'].items()] if (event.matchstats is not None and 'oprs' in event.matchstats) else []
        oprs = sorted(oprs, key=lambda t: t[1], reverse=True)  # sort by OPR
        oprs = oprs[:15]  # get the top 15 OPRs

        if event.within_a_day:
            matches_recent = MatchHelper.recentMatches(cleaned_matches)
            matches_upcoming = MatchHelper.upcomingMatches(cleaned_matches)
        else:
            matches_recent = None
            matches_upcoming = None

        bracket_table = MatchHelper.generateBracket(matches, event.alliance_selections)

        template_values = {
            "event": event,
            "matches": matches,
            "matches_recent": matches_recent,
            "matches_upcoming": matches_upcoming,
            "awards": awards,
            "teams_a": teams_a,
            "teams_b": teams_b,
            "num_teams": num_teams,
            "oprs": oprs,
            "bracket_table": bracket_table,
        }

        if event.within_a_day:
            self._cache_expiration = self.SHORT_CACHE_EXPIRATION

        path = os.path.join(os.path.dirname(__file__), '../templates/event_details.html')
        return template.render(path, template_values)


class EventRss(CacheableHandler):
    """
    Generates a RSS feed for the matches in a event
    """
    CACHE_VERSION = 2
    CACHE_KEY_FORMAT = "event_rss_{}"  # (event_key)

    def __init__(self, *args, **kw):
        super(EventRss, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 5

    def get(self, event_key):
        if not event_key:
            return self.redirect("/events")

        self._cache_key = self.CACHE_KEY_FORMAT.format(event_key)
        super(EventRss, self).get(event_key)

    def _render(self, event_key):
        event = Event.get_by_id(event_key)
        if not event:
            self.abort(404)

        matches = MatchHelper.organizeMatches(event.matches)

        template_values = {
            "event": event,
            "matches": matches,
            "datetime": datetime.datetime.now()
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/event_rss.xml')
        self.response.headers['content-type'] = 'application/xml; charset=UTF-8'
        return template.render(path, template_values)

########NEW FILE########
__FILENAME__ = firebase_controller
import logging

from google.appengine.ext import webapp
from google.appengine.api import urlfetch

from models.sitevar import Sitevar

import tba_config
import json


class FirebasePushDo(webapp.RequestHandler):
    """
    Pushes data to Firebase
    """
    SUCCESS_STATUS_CODES = {200, 204}

    def post(self):
        payload = json.loads(self.request.body)
        key = payload['key']
        payload_json = json.dumps(payload['data'])

        firebase_secrets = Sitevar.get_by_id("firebase.secrets")
        if firebase_secrets is None:
            raise Exception("Missing sitevar: firebase.secrets. Can't write to Firebase.")
        FIREBASE_SECRET = firebase_secrets.contents['FIREBASE_SECRET']

        url = tba_config.CONFIG['firebase-url'].format(key, FIREBASE_SECRET)
        result = urlfetch.fetch(url, payload_json, 'PUT')
        if result.status_code not in self.SUCCESS_STATUS_CODES:
            logging.warning("Error pushing data to Firebase: {}. ERROR {}: {}".format(payload_json, result.status_code, result.content))

########NEW FILE########
__FILENAME__ = gameday2_controller
import os

from google.appengine.ext.webapp import template

from base_controller import CacheableHandler

from helpers.event_helper import EventHelper


class Gameday2Controller(CacheableHandler):
    CACHE_VERSION = 1
    CACHE_KEY_FORMAT = "main_gameday2"

    def __init__(self, *args, **kw):
        super(Gameday2Controller, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24 * 7

    def _render(self, *args, **kw):
        events = EventHelper.getWeekEvents()

        webcasts = []
        for event in events:
            webcasts.append({
                "key": event.key_name,
                "name": event.name,
                "webcast": event.webcast,
                })

        template_values = {'webcasts': webcasts}

        path = os.path.join(os.path.dirname(__file__), '../templates/gameday2.html')
        return template.render(path, template_values)

########NEW FILE########
__FILENAME__ = insights_controller
import os

from google.appengine.ext import ndb
from google.appengine.ext.webapp import template

from base_controller import CacheableHandler

from models.insight import Insight

MAX_YEAR = 2014
VALID_YEARS = list(reversed(range(1992, MAX_YEAR + 1)))


class InsightsOverview(CacheableHandler):
    """
    Show Insights Overview
    """
    CACHE_VERSION = 2
    CACHE_KEY_FORMAT = "insights_overview"

    def __init__(self, *args, **kw):
        super(InsightsOverview, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24

    def _render(self):
        template_values = {
            'valid_years': VALID_YEARS,
        }

        insights = ndb.get_multi([ndb.Key(Insight, Insight.renderKeyName(0, insight_name)) for insight_name in Insight.INSIGHT_NAMES.values()])
        for insight in insights:
            if insight:
                template_values[insight.name] = insight

        path = os.path.join(os.path.dirname(__file__), '../templates/insights.html')
        return template.render(path, template_values)


class InsightsDetail(CacheableHandler):
    """
    Show Insights for a particular year
    """
    CACHE_VERSION = 2
    CACHE_KEY_FORMAT = "insight_detail_{}"  # (year)

    def __init__(self, *args, **kw):
        super(InsightsDetail, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24

    def get(self, year):
        if year == '':
            return self.redirect("/insights")
        if not year.isdigit():
            self.abort(404)
        year = int(year)
        if year not in VALID_YEARS:
            self.abort(404)

        self._cache_key = self.CACHE_KEY_FORMAT.format(year)
        super(InsightsDetail, self).get(year)

    def _render(self, year):
        template_values = {
            'valid_years': VALID_YEARS,
            'selected_year': year,
        }

        insights = ndb.get_multi([ndb.Key(Insight, Insight.renderKeyName(year, insight_name)) for insight_name in Insight.INSIGHT_NAMES.values()])
        for insight in insights:
            if insight:
                template_values[insight.name] = insight

        path = os.path.join(os.path.dirname(__file__), '../templates/insights_details.html')
        return template.render(path, template_values)

########NEW FILE########
__FILENAME__ = main_controller
import os
import logging
import datetime
import webapp2

from google.appengine.api import memcache
from google.appengine.ext import ndb
from google.appengine.ext.webapp import template

import tba_config

from base_controller import CacheableHandler
from consts.event_type import EventType
from helpers.event_helper import EventHelper

from models.event import Event
from models.insight import Insight
from models.team import Team
from models.sitevar import Sitevar


def render_static(page):
    memcache_key = "main_%s" % page
    html = memcache.get(memcache_key)

    if html is None:
        path = os.path.join(os.path.dirname(__file__), "../templates/%s.html" % page)
        html = template.render(path, {})
        if tba_config.CONFIG["memcache"]:
            memcache.set(memcache_key, html, 86400)

    return html


class MainKickoffHandler(CacheableHandler):
    CACHE_VERSION = 3
    CACHE_KEY_FORMAT = "main_kickoff"

    def __init__(self, *args, **kw):
        super(MainKickoffHandler, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24

    def _render(self, *args, **kw):
        kickoff_datetime_est = datetime.datetime(2014, 1, 4, 10, 30)
        kickoff_datetime_utc = kickoff_datetime_est + datetime.timedelta(hours=5)

        is_kickoff = datetime.datetime.now() >= kickoff_datetime_est - datetime.timedelta(days=1)  # turn on 1 day before

        path = os.path.join(os.path.dirname(__file__), "../templates/index_kickoff.html")
        return template.render(path, {'is_kickoff': is_kickoff,
                                      'kickoff_datetime_est': kickoff_datetime_est,
                                      'kickoff_datetime_utc': kickoff_datetime_utc,
                                      })


class MainBuildseasonHandler(CacheableHandler):
    CACHE_VERSION = 1
    CACHE_KEY_FORMAT = "main_buildseason"

    def __init__(self, *args, **kw):
        super(MainBuildseasonHandler, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24 * 7

    def _render(self, *args, **kw):
        endbuild_datetime_est = datetime.datetime(2014, 2, 18, 23, 59)
        endbuild_datetime_utc = endbuild_datetime_est + datetime.timedelta(hours=5)

        path = os.path.join(os.path.dirname(__file__), "../templates/index_buildseason.html")
        return template.render(path, {'endbuild_datetime_est': endbuild_datetime_est,
                                      'endbuild_datetime_utc': endbuild_datetime_utc
                                      })


class MainChampsHandler(CacheableHandler):
    CACHE_VERSION = 1
    CACHE_KEY_FORMAT = "main_champs"

    def __init__(self, *args, **kw):
        super(MainChampsHandler, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24

    def _render(self, *args, **kw):
        year = datetime.datetime.now().year
        event_keys = Event.query(Event.year == year, Event.event_type_enum.IN(EventType.CMP_EVENT_TYPES)).fetch(100, keys_only=True)
        events = [event_key.get() for event_key in event_keys]
        template_values = {
            "events": events,
        }

        insights = ndb.get_multi([ndb.Key(Insight, Insight.renderKeyName(year, insight_name)) for insight_name in Insight.INSIGHT_NAMES.values()])
        for insight in insights:
            if insight:
                template_values[insight.name] = insight

        path = os.path.join(os.path.dirname(__file__), '../templates/index_champs.html')
        return template.render(path, template_values)


class MainCompetitionseasonHandler(CacheableHandler):
    CACHE_VERSION = 5
    CACHE_KEY_FORMAT = "main_competitionseason"

    def __init__(self, *args, **kw):
        super(MainCompetitionseasonHandler, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60

    def _render(self, *args, **kw):
        week_events = EventHelper.getWeekEvents()
        template_values = {
            "events": week_events,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/index_competitionseason.html')
        return template.render(path, template_values)


class MainInsightsHandler(CacheableHandler):
    CACHE_VERSION = 2
    CACHE_KEY_FORMAT = "main_insights"

    def __init__(self, *args, **kw):
        super(MainInsightsHandler, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24

    def _render(self, *args, **kw):
        week_events = EventHelper.getWeekEvents()
        template_values = {
            "events": week_events,
        }

        insights = ndb.get_multi([ndb.Key(Insight, Insight.renderKeyName(2014, insight_name)) for insight_name in Insight.INSIGHT_NAMES.values()])
        for insight in insights:
            if insight:
                template_values[insight.name] = insight

        path = os.path.join(os.path.dirname(__file__), '../templates/index_insights.html')
        return template.render(path, template_values)


class MainOffseasonHandler(CacheableHandler):
    CACHE_VERSION = 2
    CACHE_KEY_FORMAT = "main_offseason"

    def __init__(self, *args, **kw):
        super(MainOffseasonHandler, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24

    def _render(self, *args, **kw):
        week_events = EventHelper.getWeekEvents()
        template_values = {
            "events": week_events,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/index_offseason.html')
        return template.render(path, template_values)


class ContactHandler(CacheableHandler):
    CACHE_VERSION = 1
    CACHE_KEY_FORMAT = "main_contact"

    def __init__(self, *args, **kw):
        super(ContactHandler, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24 * 7

    def _render(self, *args, **kw):
        path = os.path.join(os.path.dirname(__file__), "../templates/contact.html")
        return template.render(path, {})


class HashtagsHandler(CacheableHandler):
    CACHE_VERSION = 1
    CACHE_KEY_FORMAT = "main_hashtags"

    def __init__(self, *args, **kw):
        super(HashtagsHandler, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24 * 7

    def _render(self, *args, **kw):
        path = os.path.join(os.path.dirname(__file__), "../templates/hashtags.html")
        return template.render(path, {})


class AboutHandler(CacheableHandler):
    CACHE_VERSION = 1
    CACHE_KEY_FORMAT = "main_about"

    def __init__(self, *args, **kw):
        super(AboutHandler, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24 * 7

    def _render(self, *args, **kw):
        path = os.path.join(os.path.dirname(__file__), "../templates/about.html")
        return template.render(path, {})


class ThanksHandler(CacheableHandler):
    CACHE_VERSION = 1
    CACHE_KEY_FORMAT = "main_thanks"

    def __init__(self, *args, **kw):
        super(ThanksHandler, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24 * 7

    def _render(self, *args, **kw):
        path = os.path.join(os.path.dirname(__file__), "../templates/thanks.html")
        return template.render(path, {})


class OprHandler(CacheableHandler):
    CACHE_VERSION = 1
    CACHE_KEY_FORMAT = "main_opr"

    def __init__(self, *args, **kw):
        super(OprHandler, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24 * 7

    def _render(self, *args, **kw):
        path = os.path.join(os.path.dirname(__file__), "../templates/opr.html")
        return template.render(path, {})


class SearchHandler(webapp2.RequestHandler):
    def get(self):
        try:
            q = self.request.get("q")
            logging.info("search query: %s" % q)
            if q.isdigit():
                team_id = "frc%s" % q
                team = Team.get_by_id(team_id)
                if team:
                    self.redirect(team.details_url)
                    return None
            elif len(q) in {3, 4, 5}:  # event shorts are between 3 and 5 characters long
                year = datetime.datetime.now().year  # default to current year
                event_id = "%s%s" % (year, q)
                event = Event.get_by_id(event_id)
                if event:
                    self.redirect(event.details_url)
                    return None
        except Exception, e:
            logging.warning("warning: %s" % e)
        finally:
            self.response.out.write(render_static("search"))


class GamedayHandler(CacheableHandler):
    CACHE_VERSION = 1
    CACHE_KEY_FORMAT = "main_gameday"

    def __init__(self, *args, **kw):
        super(GamedayHandler, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24 * 7

    def _render(self, *args, **kw):
        special_webcasts_future = Sitevar.get_by_id_async('gameday.special_webcasts')
        special_webcasts_temp = special_webcasts_future.get_result()
        if special_webcasts_temp:
            special_webcasts_temp = special_webcasts_temp.contents
        else:
            special_webcasts_temp = {}
        special_webcasts = []
        for webcast in special_webcasts_temp.values():
            toAppend = {}
            for key, value in webcast.items():
                toAppend[str(key)] = str(value)
            special_webcasts.append(toAppend)

        ongoing_events = []
        ongoing_events_w_webcasts = []
        week_events = EventHelper.getWeekEvents()
        for event in week_events:
            if event.within_a_day:
                ongoing_events.append(event)
                if event.webcast:
                    valid = []
                    for webcast in event.webcast:
                        if 'type' in webcast and 'channel' in webcast:
                            event_webcast = {'event': event}
                            valid.append(event_webcast)
                    # Add webcast numbers if more than one for an event
                    if len(valid) > 1:
                        count = 1
                        for event in valid:
                            event['count'] = count
                            count += 1
                    ongoing_events_w_webcasts += valid

        template_values = {'special_webcasts': special_webcasts,
                           'ongoing_events': ongoing_events,
                           'ongoing_events_w_webcasts': ongoing_events_w_webcasts}

        path = os.path.join(os.path.dirname(__file__), '../templates/gameday.html')
        return template.render(path, template_values)


class PageNotFoundHandler(webapp2.RequestHandler):
    def get(self, *args):
        self.error(404)
        self.response.out.write(render_static("404"))


class WebcastsHandler(CacheableHandler):
    CACHE_VERSION = 2
    CACHE_KEY_FORMAT = "main_webcasts"

    def __init__(self, *args, **kw):
        super(WebcastsHandler, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24 * 7

    def _render(self, *args, **kw):
        year = datetime.datetime.now().year
        event_keys = Event.query(Event.year == year).order(Event.start_date).fetch(500, keys_only=True)
        events = ndb.get_multi(event_keys)

        template_values = {
            'events': events,
            'year': year,
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/webcasts.html')
        return template.render(path, template_values)


class RecordHandler(CacheableHandler):
    CACHE_VERSION = 1
    CACHE_KEY_FORMAT = "main_record"

    def __init__(self, *args, **kw):
        super(RecordHandler, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24 * 7

    def _render(self, *args, **kw):
        path = os.path.join(os.path.dirname(__file__), "../templates/record.html")
        return template.render(path, {})


class ApiDocumentationHandler(CacheableHandler):
    CACHE_VERSION = 1
    CACHE_KEY_FORMAT = "api_docs"

    def __init__(self, *args, **kw):
        super(ApiDocumentationHandler, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24 * 7

    def _render(self, *args, **kw):
        path = os.path.join(os.path.dirname(__file__), "../templates/apidocs.html")
        return template.render(path, {})

########NEW FILE########
__FILENAME__ = match_controller
import os

from google.appengine.ext.webapp import template

from base_controller import CacheableHandler
from models.event import Event
from models.match import Match


class MatchDetail(CacheableHandler):
    """
    Display a Match.
    """
    LONG_CACHE_EXPIRATION = 60 * 60 * 24
    SHORT_CACHE_EXPIRATION = 60 * 5
    CACHE_VERSION = 4
    CACHE_KEY_FORMAT = "match_detail_{}"  # (match_key)

    def __init__(self, *args, **kw):
        super(MatchDetail, self).__init__(*args, **kw)
        self._cache_expiration = self.LONG_CACHE_EXPIRATION

    def get(self, match_key):
        if not match_key:
            return self.redirect("/")

        self._cache_key = self.CACHE_KEY_FORMAT.format(match_key)
        super(MatchDetail, self).get(match_key)

    def _render(self, match_key):
        try:
            match_future = Match.get_by_id_async(match_key)
            event_future = Event.get_by_id_async(match_key.split("_")[0])
            match = match_future.get_result()
            event = event_future.get_result()
        except Exception, e:
            self.abort(404)

        if not match:
            self.abort(404)

        template_values = {
            "event": event,
            "match": match,
        }

        if event.within_a_day:
            self._cache_expiration = self.SHORT_CACHE_EXPIRATION

        path = os.path.join(os.path.dirname(__file__), '../templates/match_details.html')
        return template.render(path, template_values)

########NEW FILE########
__FILENAME__ = suggest_event_webcast_controller
import os

from google.appengine.ext.webapp import template

from controllers.base_controller import LoggedInHandler
from models.event import Event
from models.suggestion import Suggestion


class SuggestEventWebcastController(LoggedInHandler):
    """
    Allow users to suggest webcasts for TBA to add to events.
    """

    def get(self):
        self._require_login("/suggest/event/webcast?event=%s" % self.request.get("event_key"))

        if not self.request.get("event_key"):
            self.redirect("/", abort=True)

        event = Event.get_by_id(self.request.get("event_key"))

        self.template_values.update({
            "result": self.request.get("result"),
            "event": event,
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/suggest_event_webcast.html')
        self.response.out.write(template.render(path, self.template_values))

    def post(self):
        self._require_login()

        event_key = self.request.get("event_key")
        webcast_url = self.request.get("webcast_url")

        if not webcast_url:
            self.redirect('/suggest/event/webcast?event_key=%s&result=blank_webcast' % event_key, abort=True)

        suggestion = Suggestion(
            author=self.user_bundle.account.key,
            target_key=event_key,
            target_model="event",
            )
        suggestion.contents = {"webcast_url": webcast_url}
        suggestion.put()

        self.redirect('/suggest/event/webcast?event_key=%s&result=success' % event_key)

########NEW FILE########
__FILENAME__ = suggest_match_video_controller
import os
import re

from google.appengine.ext.webapp import template

from controllers.base_controller import LoggedInHandler
from models.event import Event
from models.match import Match
from models.suggestion import Suggestion


class SuggestMatchVideoController(LoggedInHandler):
    """
    Allow users to suggest videos for TBA to add to matches.
    """

    def get(self):
        self._require_login("/suggest/match/video?match=%s" % self.request.get("match_key"))

        if not self.request.get("match_key"):
            self.redirect("/", abort=True)

        match_future = Match.get_by_id_async(self.request.get("match_key"))
        event_future = Event.get_by_id_async(self.request.get("match_key").split("_")[0])
        match = match_future.get_result()
        event = event_future.get_result()

        self.template_values.update({
            "success": self.request.get("success"),
            "event": event,
            "match": match,
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/suggest_match_video.html')
        self.response.out.write(template.render(path, self.template_values))

    def post(self):
        self._require_login()

        match_key = self.request.get("match_key")
        youtube_url = self.request.get("youtube_url")

        youtube_id = None
        regex1 = re.match(r".*youtu\.be\/(.*)", youtube_url)
        if regex1 is not None:
            youtube_id = regex1.group(1)
        else:
            regex2 = re.match(r".*v=([a-zA-Z0-9_-]*)", youtube_url)
            if regex2 is not None:
                youtube_id = regex2.group(1)

        if youtube_id is not None:
            suggestion = Suggestion(
                author=self.user_bundle.account.key,
                target_key=match_key,
                target_model="match",
                )
            suggestion.contents = {"youtube_videos": [youtube_id]}
            suggestion.put()

        self.redirect('/suggest/match/video?match_key=%s&success=1' % match_key)

########NEW FILE########
__FILENAME__ = suggest_team_media_controller
import os

from google.appengine.ext import ndb
from google.appengine.ext.webapp import template

from controllers.base_controller import LoggedInHandler
from helpers.media_helper import MediaHelper, MediaParser
from models.media import Media
from models.suggestion import Suggestion
from models.team import Team


class SuggestTeamMediaController(LoggedInHandler):
    """
    Allow users to suggest media for TBA to add to teams.
    """

    def get(self):
        team_key = self.request.get("team_key")
        year_str = self.request.get("year")

        self._require_login("/suggest/team/media?team_key=%s&year=%s" % (team_key, year_str))

        if not team_key or not year_str:
            self.redirect("/", abort=True)

        year = int(year_str)
        team_future = Team.get_by_id_async(self.request.get("team_key"))
        team = team_future.get_result()
        media_key_futures = Media.query(Media.references == team.key, Media.year == year).fetch_async(500, keys_only=True)
        media_futures = ndb.get_multi_async(media_key_futures.get_result())
        medias_by_slugname = MediaHelper.group_by_slugname([media_future.get_result() for media_future in media_futures])

        self.template_values.update({
            "success": self.request.get("success"),
            "team": team,
            "year": year,
            "medias_by_slugname": medias_by_slugname
        })

        path = os.path.join(os.path.dirname(__file__), '../../templates/suggest_team_media.html')
        self.response.out.write(template.render(path, self.template_values))

    def post(self):
        self._require_login()

        team_key = self.request.get("team_key")
        year_str = self.request.get("year")

        success_code = 0
        media_dict = MediaParser.partial_media_dict_from_url(self.request.get('media_url').strip())
        if media_dict is not None:
            existing_media = Media.get_by_id(Media.render_key_name(media_dict['media_type_enum'], media_dict['foreign_key']))
            if existing_media is None or team_key not in [reference.id() for reference in existing_media.references]:
                media_dict['year'] = int(year_str)
                media_dict['reference_type'] = 'team'
                media_dict['reference_key'] = team_key

                suggestion = Suggestion(
                    author=self.user_bundle.account.key,
                    target_model="media",
                    )
                suggestion.contents = media_dict
                suggestion.put()
            success_code = 1

        self.redirect('/suggest/team/media?team_key=%s&year=%s&success=%s' % (team_key, year_str, success_code))

########NEW FILE########
__FILENAME__ = team_controller
import datetime
import os

from google.appengine.ext import ndb
from google.appengine.ext.webapp import template

from base_controller import CacheableHandler

from models.team import Team

from renderers.team_renderer import TeamRenderer


class TeamList(CacheableHandler):
    VALID_PAGES = [1, 2, 3, 4, 5, 6]
    CACHE_VERSION = 1
    CACHE_KEY_FORMAT = "team_list_{}"  # (page)

    def __init__(self, *args, **kw):
        super(TeamList, self).__init__(*args, **kw)
        self._cache_expiration = 60 * 60 * 24 * 7

    def get(self, page='1'):
        if page == '':
            return self.redirect("/teams")
        page = int(page)
        if page not in self.VALID_PAGES:
            self.abort(404)

        self._cache_key = self.CACHE_KEY_FORMAT.format(page)
        super(TeamList, self).get(page)

    def _render(self, page=''):
        page_labels = []
        for curPage in self.VALID_PAGES:
            if curPage == 1:
                label = '1-999'
            else:
                label = "{}'s".format((curPage - 1) * 1000)
            page_labels.append(label)
            if curPage == page:
                cur_page_label = label

        start = (page - 1) * 1000
        stop = start + 999

        if start == 0:
            start = 1

        team_keys = Team.query().order(Team.team_number).filter(
          Team.team_number >= start).filter(Team.team_number < stop).fetch(10000, keys_only=True)
        teams = ndb.get_multi(team_keys)

        num_teams = len(teams)
        middle_value = num_teams / 2
        if num_teams % 2 != 0:
            middle_value += 1
        teams_a, teams_b = teams[:middle_value], teams[middle_value:]

        template_values = {
            "teams_a": teams_a,
            "teams_b": teams_b,
            "num_teams": num_teams,
            "page_labels": page_labels,
            "cur_page_label": cur_page_label,
            "current_page": page
        }

        path = os.path.join(os.path.dirname(__file__), '../templates/team_list.html')
        return template.render(path, template_values)


class TeamCanonical(CacheableHandler):
    LONG_CACHE_EXPIRATION = 60 * 60 * 24
    SHORT_CACHE_EXPIRATION = 60 * 5
    CACHE_VERSION = 1
    CACHE_KEY_FORMAT = "team_canonical_{}"  # (team_number)

    def __init__(self, *args, **kw):
        super(TeamCanonical, self).__init__(*args, **kw)
        self._cache_expiration = self.LONG_CACHE_EXPIRATION

    def get(self, team_number):
        # /team/0201 should redirect to /team/201
        if str(int(team_number)) != team_number:
            return self.redirect("/team/%s" % int(team_number))

        self._cache_key = self.CACHE_KEY_FORMAT.format("frc{}".format(team_number))
        super(TeamCanonical, self).get(team_number)

    def _render(self, team_number):
        team = Team.get_by_id("frc{}".format(team_number))
        if not team:
            self.abort(404)

        year = datetime.datetime.now().year

        rendered_result = TeamRenderer.render_team_details(self, team, year, True)
        if rendered_result is None:
            return TeamRenderer.render_team_history(self, team, True)
        else:
            return rendered_result


class TeamDetail(CacheableHandler):
    LONG_CACHE_EXPIRATION = 60 * 60 * 24
    SHORT_CACHE_EXPIRATION = 60 * 5
    CACHE_VERSION = 2
    CACHE_KEY_FORMAT = "team_detail_{}_{}"  # (team_number, year)

    def __init__(self, *args, **kw):
        super(TeamDetail, self).__init__(*args, **kw)
        self._cache_expiration = self.LONG_CACHE_EXPIRATION

    def get(self, team_number, year):
        # /team/0201 should redirect to /team/201
        if str(int(team_number)) != team_number:
            return self.redirect("/team/%s/%s" % (int(team_number), year))

        self._cache_key = self.CACHE_KEY_FORMAT.format("frc{}".format(team_number), year)
        super(TeamDetail, self).get(team_number, year)

    def _render(self, team_number, year):
        team = Team.get_by_id("frc{}".format(team_number))
        if not team:
            self.abort(404)

        rendered_result = TeamRenderer.render_team_details(self, team, int(year), False)
        if rendered_result is None:
            self.abort(404)
        else:
            return rendered_result


class TeamHistory(CacheableHandler):
    LONG_CACHE_EXPIRATION = 60 * 60 * 24
    SHORT_CACHE_EXPIRATION = 60 * 5
    CACHE_VERSION = 2
    CACHE_KEY_FORMAT = "team_history_{}"  # (team_number)

    def __init__(self, *args, **kw):
        super(TeamHistory, self).__init__(*args, **kw)
        self._cache_expiration = self.LONG_CACHE_EXPIRATION

    def get(self, team_number):
        # /team/0604/history should redirect to /team/604/history
        if str(int(team_number)) != team_number:
            return self.redirect("/team/%s/history" % int(team_number))

        self._cache_key = self.CACHE_KEY_FORMAT.format("frc" + team_number)
        super(TeamHistory, self).get(team_number)

    def _render(self, team_number):
        team = Team.get_by_id("frc" + team_number)
        if not team:
            self.abort(404)

        return TeamRenderer.render_team_history(self, team, False)

########NEW FILE########
__FILENAME__ = cron_main
#!/usr/bin/env python
import webapp2

import tba_config

from controllers.backup_controller import TbaCSVBackupEventsEnqueue, TbaCSVBackupEventDo, TbaCSVRestoreEventsEnqueue, TbaCSVRestoreEventDo
from controllers.backup_controller import TbaCSVBackupTeamsEnqueue, TbaCSVBackupTeamsDo

from controllers.datafeed_controller import TbaVideosGet, TbaVideosEnqueue
from controllers.datafeed_controller import FmsEventListGet, FmsTeamListGet
from controllers.datafeed_controller import OffseasonMatchesGet
from controllers.datafeed_controller import TwitterFrcfmsMatchesGet
from controllers.datafeed_controller import UsfirstEventDetailsEnqueue, UsfirstEventDetailsGet, UsfirstEventListGet
from controllers.datafeed_controller import UsfirstAwardsEnqueue, UsfirstAwardsGet
from controllers.datafeed_controller import UsfirstEventAlliancesEnqueue, UsfirstEventAlliancesGet
from controllers.datafeed_controller import UsfirstMatchesEnqueue, UsfirstMatchesGet, UsfirstEventRankingsEnqueue, UsfirstEventRankingsGet
from controllers.datafeed_controller import UsfirstTeamDetailsEnqueue, UsfirstTeamDetailsRollingEnqueue, UsfirstTeamDetailsGet, UsfirstTeamsTpidsGet
from controllers.datafeed_controller import UsfirstPre2003TeamEventsEnqueue, UsfirstPre2003TeamEventsGet

from controllers.cron_controller import EventShortNameCalcEnqueue, EventShortNameCalcDo
from controllers.cron_controller import EventTeamRepairDo, EventTeamUpdate, EventTeamUpdateEnqueue
from controllers.cron_controller import EventMatchstatsDo, EventMatchstatsEnqueue
from controllers.cron_controller import FinalMatchesRepairDo
from controllers.cron_controller import YearInsightsEnqueue, YearInsightsDo, OverallInsightsEnqueue, OverallInsightsDo, TypeaheadCalcEnqueue, TypeaheadCalcDo
from controllers.firebase_controller import FirebasePushDo


app = webapp2.WSGIApplication([('/tasks/enqueue/csv_backup_events', TbaCSVBackupEventsEnqueue),
                               ('/tasks/enqueue/csv_backup_events/([0-9]*)', TbaCSVBackupEventsEnqueue),
                               ('/tasks/do/csv_backup_event/(.*)', TbaCSVBackupEventDo),
                               ('/tasks/enqueue/csv_restore_events', TbaCSVRestoreEventsEnqueue),
                               ('/tasks/enqueue/csv_restore_events/([0-9]*)', TbaCSVRestoreEventsEnqueue),
                               ('/tasks/do/csv_restore_event/(.*)', TbaCSVRestoreEventDo),
                               ('/tasks/enqueue/csv_backup_teams', TbaCSVBackupTeamsEnqueue),
                               ('/tasks/do/csv_backup_teams', TbaCSVBackupTeamsDo),
                               ('/tasks/enqueue/tba_videos', TbaVideosEnqueue),
                               ('/tasks/enqueue/usfirst_event_alliances/(.*)', UsfirstEventAlliancesEnqueue),
                               ('/tasks/enqueue/usfirst_event_details/([0-9]*)', UsfirstEventDetailsEnqueue),
                               ('/tasks/enqueue/usfirst_event_rankings/(.*)', UsfirstEventRankingsEnqueue),
                               ('/tasks/enqueue/usfirst_awards/(.*)', UsfirstAwardsEnqueue),
                               ('/tasks/enqueue/usfirst_matches/(.*)', UsfirstMatchesEnqueue),
                               ('/tasks/enqueue/usfirst_team_details', UsfirstTeamDetailsEnqueue),
                               ('/tasks/enqueue/usfirst_team_details_rolling', UsfirstTeamDetailsRollingEnqueue),
                               ('/tasks/enqueue/usfirst_pre2003_team_events', UsfirstPre2003TeamEventsEnqueue),
                               ('/tasks/get/fms_event_list', FmsEventListGet),
                               ('/tasks/get/fms_team_list', FmsTeamListGet),
                               ('/tasks/get/offseason_matches/(.*)', OffseasonMatchesGet),
                               ('/tasks/get/tba_videos/(.*)', TbaVideosGet),
                               ('/tasks/get/twitter_frcfms_matches', TwitterFrcfmsMatchesGet),
                               ('/tasks/get/usfirst_event_alliances/(.*)', UsfirstEventAlliancesGet),
                               ('/tasks/get/usfirst_event_list/([0-9]*)', UsfirstEventListGet),
                               ('/tasks/get/usfirst_event_details/([0-9]*)/([0-9]*)', UsfirstEventDetailsGet),
                               ('/tasks/get/usfirst_event_rankings/(.*)', UsfirstEventRankingsGet),
                               ('/tasks/get/usfirst_awards/(.*)', UsfirstAwardsGet),
                               ('/tasks/get/usfirst_matches/(.*)', UsfirstMatchesGet),
                               ('/tasks/get/usfirst_team_details/(.*)', UsfirstTeamDetailsGet),
                               ('/tasks/get/usfirst_teams_tpids/([0-9]*)', UsfirstTeamsTpidsGet),
                               ('/tasks/get/usfirst_pre2003_team_events/(.*)', UsfirstPre2003TeamEventsGet),
                               ('/tasks/math/enqueue/event_short_name_calc_enqueue/([0-9]*)', EventShortNameCalcEnqueue),
                               ('/tasks/math/do/event_short_name_calc_do/(.*)', EventShortNameCalcDo),
                               ('/tasks/math/enqueue/event_matchstats/(.*)', EventMatchstatsEnqueue),
                               ('/tasks/math/enqueue/eventteam_update/(.*)', EventTeamUpdateEnqueue),
                               ('/tasks/math/do/event_matchstats/(.*)', EventMatchstatsDo),
                               ('/tasks/math/do/eventteam_repair', EventTeamRepairDo),
                               ('/tasks/math/do/eventteam_update/(.*)', EventTeamUpdate),
                               ('/tasks/math/do/final_matches_repair/([0-9]*)', FinalMatchesRepairDo),
                               ('/tasks/math/enqueue/overallinsights/(.*)', OverallInsightsEnqueue),
                               ('/tasks/math/do/overallinsights/(.*)', OverallInsightsDo),
                               ('/tasks/math/enqueue/insights/(.*)/([0-9]*)', YearInsightsEnqueue),
                               ('/tasks/math/do/insights/(.*)/([0-9]*)', YearInsightsDo),
                               ('/tasks/math/enqueue/typeaheadcalc', TypeaheadCalcEnqueue),
                               ('/tasks/math/do/typeaheadcalc', TypeaheadCalcDo),
                               ('/tasks/posts/firebase_push', FirebasePushDo),
                               ],
                              debug=tba_config.DEBUG)

########NEW FILE########
__FILENAME__ = csv_alliance_selections_parser
import csv
import StringIO

from datafeeds.parser_base import ParserBase


class CSVAllianceSelectionsParser(ParserBase):
    @classmethod
    def parse(self, data):
        """
        Parse CSV that contains teams
        Format is as follows:
        captain1, pick1-1, pick1-2
        captain2, pick2-1, pick2-2
        ...
        """
        alliances = []
        csv_data = list(csv.reader(StringIO.StringIO(data), delimiter=',', skipinitialspace=True))
        for row in csv_data:
            alliances.append({'picks': ['frc' + team.strip() for team in row], 'declines': []})
        return alliances

########NEW FILE########
__FILENAME__ = csv_awards_parser
import csv
import json
import StringIO

from datafeeds.parser_base import ParserBase
from helpers.award_helper import AwardHelper
from models.award import Award


class CSVAwardsParser(ParserBase):
    @classmethod
    def parse(cls, data):
        """
        Parse CSV that contains awards
        Format is as follows:
        year, event_short, award_name_str, team_number (can be blank), awardee (can be blank)
        Example:
        2000,mi,Regional Finalist,45,
        """
        awards_by_key = {}
        csv_data = list(csv.reader(StringIO.StringIO(data), delimiter=',', skipinitialspace=True))
        for award in csv_data:
            year = int(award[0])
            event_short = award[1]
            name_str = award[2]
            team_number = award[3]
            awardee = award[4]

            if team_number == '':
                team_number = None
            else:
                team_number = int(team_number)
            if awardee == '':
                awardee = None
            # an award must have either an awardee or a team_number
            if awardee is None and team_number is None:
                continue

            if team_number is not None:
                team_number_list = [team_number]
            else:
                team_number_list = []

            recipient_json = json.dumps({
                'team_number': team_number,
                'awardee': awardee,
            })

            award_type_enum = AwardHelper.parse_award_type(name_str)
            if award_type_enum is None:
                continue
            award_key_name = Award.render_key_name('{}{}'.format(year, event_short), award_type_enum)

            if award_key_name in awards_by_key:
                if team_number is not None:
                    awards_by_key[award_key_name]['team_number_list'].append(team_number)
                awards_by_key[award_key_name]['recipient_json_list'].append(recipient_json)
            else:
                awards_by_key[award_key_name] = {
                    'year': year,
                    'event_short': event_short,
                    'name_str': name_str,
                    'award_type_enum': award_type_enum,
                    'team_number_list': team_number_list,
                    'recipient_json_list': [recipient_json],
                }
        return awards_by_key.values()

########NEW FILE########
__FILENAME__ = csv_teams_parser
import csv
import StringIO

from datafeeds.parser_base import ParserBase


class CSVTeamsParser(ParserBase):
    @classmethod
    def parse(self, data):
        """
        Parse CSV that contains teams
        Format is as follows:
        team1, team2, ... teamN
        Example:
        254, 1114, 100, 604, 148
        """
        teams = set()
        csv_data = list(csv.reader(StringIO.StringIO(data), delimiter=',', skipinitialspace=True))
        for row in csv_data:
            for team in row:
                if team.isdigit():
                    teams.add(int(team))
        return teams

########NEW FILE########
__FILENAME__ = datafeed_base
import logging

from google.appengine.api import urlfetch


class DatafeedBase(object):
    """
    Provides structure for fetching and parsing pages from websites.
    Other Datafeeds inherit from here.
    """
    def parse(self, url, parser, usfirst_session_key=None):
        headers = {
            'Cache-Control': 'no-cache, max-age=10',
            'Pragma': 'no-cache',
        }
        if 'my.usfirst.org/myarea' in url:
            # FIRST is now checking the 'Referer' header for the string 'usfirst.org'.
            # See https://github.com/patfair/frclinks/commit/051bf91d23ca0242dad5b1e471f78468173f597f
            headers['Referer'] = 'usfirst.org'
        if usfirst_session_key is not None:
            headers['Cookie'] = usfirst_session_key

        try:
            result = urlfetch.fetch(url,
                                    headers=headers,
                                    deadline=5)
        except Exception, e:
            logging.error("URLFetch failed for: {}".format(url))
            logging.info(e)
            return [], False

        if result.status_code == 200:
            return parser.parse(result.content)
        else:
            logging.warning('Unable to retreive url: ' + (url))
            return [], False

    def _shorten(self, string):
        MAX_DB_LENGTH = 500
        if string:
            return string[:MAX_DB_LENGTH]
        else:
            return string

########NEW FILE########
__FILENAME__ = datafeed_fms
import logging

from datafeeds.datafeed_base import DatafeedBase
from datafeeds.fms_event_list_parser import FmsEventListParser
from datafeeds.fms_team_list_parser import FmsTeamListParser

from models.event import Event
from models.team import Team


class DatafeedFms(DatafeedBase):

    FMS_EVENT_LIST_URL = "https://my.usfirst.org/frc/scoring/index.lasso?page=eventlist"
    # Raw fast teamlist, no tpids
    FMS_TEAM_LIST_URL = "https://my.usfirst.org/frc/scoring/index.lasso?page=teamlist"

    def __init__(self, *args, **kw):
        super(DatafeedFms, self).__init__(*args, **kw)

    def getFmsEventList(self):
        events = self.parse(self.FMS_EVENT_LIST_URL, FmsEventListParser)

        return [Event(
            id="%s%s" % (event.get("year", None), event.get("event_short", None)),
            end_date=event.get("end_date", None),
            event_short=event.get("event_short", None),
            first_eid=event.get("first_eid", None),
            name=event.get("name", None),
            official=True,
            start_date=event.get("start_date", None),
            venue=event.get("venue", None),
            year=event.get("year", None)
            )
            for event in events]

    def getFmsTeamList(self):
        teams = self.parse(self.FMS_TEAM_LIST_URL, FmsTeamListParser)

        return [Team(
            id="frc%s" % team.get("team_number", None),
            name=self._shorten(team.get("name", None)),
            nickname=self._shorten(team.get("nickname", None)),
            team_number=team.get("team_number", None)
            )
            for team in teams]

########NEW FILE########
__FILENAME__ = datafeed_offseason
import logging

from datafeeds.datafeed_base import DatafeedBase
from datafeeds.offseason_matches_parser import OffseasonMatchesParser

from models.match import Match


class DatafeedOffseason(DatafeedBase):
    def __init__(self, *args, **kw):
        super(DatafeedOffseason, self).__init__(*args, **kw)

    def getMatches(self, event, url):
        matches = self.parse(url, OffseasonMatchesParser)
        logging.info(matches)

        return [Match(
            id=Match.renderKeyName(
                event.key.id(),
                match.get("comp_level", None),
                match.get("set_number", 0),
                match.get("match_number", 0)),
            event=event.key,
            game=Match.FRC_GAMES_BY_YEAR.get(event.year, "frc_unknown"),
            set_number=match.get("set_number", 0),
            match_number=match.get("match_number", 0),
            comp_level=match.get("comp_level", None),
            team_key_names=match.get("team_key_names", None),
            alliances_json=match.get("alliances_json", None)
            )
            for match in matches]

########NEW FILE########
__FILENAME__ = datafeed_tba
from datafeeds.datafeed_base import DatafeedBase

from datafeeds.tba_videos_parser import TbaVideosParser


class DatafeedTba(DatafeedBase):

    TBA_VIDS_DIR_URL_PATTERN = "http://videos.thebluealliance.net/%s/"

    def getVideos(self, event):
        url = self.TBA_VIDS_DIR_URL_PATTERN % (event.key_name)
        videos = self.parse(url, TbaVideosParser)

        return videos

########NEW FILE########
__FILENAME__ = datafeed_twitter
import logging
import json
import oauth2

from datafeeds.datafeed_base import DatafeedBase
from datafeeds.twitter_matches_parser import TwitterMatchesParser
import tba_config

from models.sitevar import Sitevar


class DatafeedTwitter(DatafeedBase):
    def __init__(self, *args, **kw):
        super(DatafeedTwitter, self).__init__(*args, **kw)

    def getMatches(self):
        """
        Doesn't actually return Match objects. Instead, returns dict where
        the key is the event_short and the value is a list of strings
        in the following CSV format:
        match_id, red1, red2, red3, blue1, blue2, blue3, red score, blue score

        """
        toReturn = {}

        max_id = None

        tweets = self.getSomeTweets(max_id)
        while tweets:
            for tweet in tweets:
                event_key, match = TwitterMatchesParser.parse(tweet['text'])
                if event_key in toReturn:
                    toReturn[event_key].append(match)
                else:
                    toReturn[event_key] = [match]
                max_id = tweet['id'] - 1
            tweets = self.getSomeTweets(max_id)
        return toReturn

    def getSomeTweets(self, max_id=None):
        URL = 'https://api.twitter.com/1.1/statuses/user_timeline.json?screen_name=frcfms&count=200'
        if max_id:
            return json.loads(self.oauth_req(URL + '&max_id=' + str(max_id)))
        else:
            return json.loads(self.oauth_req(URL))

    def oauth_req(self, url, http_method="GET", post_body=None,
                  http_headers=None):

        twitter_secrets = Sitevar.get_by_id("twitter.secrets")
        if not twitter_secrets:
            raise Exception("Missing sitevar: twitter.secrets. Cant scrape twitter.")

        TWITTER_CONSUMER_KEY = twitter_secrets.contents['TWITTER_CONSUMER_KEY']
        TWITTER_CONSUMER_SECRET = twitter_secrets.contents['TWITTER_CONSUMER_SECRET']
        TWITTER_ACCESS_TOKEN = twitter_secrets.contents['TWITTER_ACCESS_TOKEN']
        TWITTER_ACCESS_TOKEN_SECRET = twitter_secrets.contents['TWITTER_ACCESS_TOKEN_SECRET']

        consumer = oauth2.Consumer(key=TWITTER_CONSUMER_KEY, secret=TWITTER_CONSUMER_SECRET)
        token = oauth2.Token(key=TWITTER_ACCESS_TOKEN, secret=TWITTER_ACCESS_TOKEN_SECRET)
        client = oauth2.Client(consumer, token)

        resp, content = client.request(
            url,
            method=http_method,
            body=post_body,
            headers=http_headers,
            force_auth_header=True
        )
        return content

########NEW FILE########
__FILENAME__ = datafeed_usfirst
import logging
import json
import re

from google.appengine.api import memcache
from google.appengine.api import urlfetch
from google.appengine.ext import ndb

import tba_config

from consts.event_type import EventType

from helpers.event_helper import EventHelper
from helpers.match_helper import MatchHelper
from helpers.team_helper import TeamTpidHelper

from datafeeds.datafeed_base import DatafeedBase
from datafeeds.usfirst_alliances_parser import UsfirstAlliancesParser
from datafeeds.usfirst_event_details_parser import UsfirstEventDetailsParser
from datafeeds.usfirst_event_list_parser import UsfirstEventListParser
from datafeeds.usfirst_event_rankings_parser import UsfirstEventRankingsParser
from datafeeds.usfirst_event_awards_parser import UsfirstEventAwardsParser
from datafeeds.usfirst_event_awards_parser_02 import UsfirstEventAwardsParser_02
from datafeeds.usfirst_event_awards_parser_03_04 import UsfirstEventAwardsParser_03_04
from datafeeds.usfirst_event_awards_parser_05_06 import UsfirstEventAwardsParser_05_06
from datafeeds.usfirst_event_teams_parser import UsfirstEventTeamsParser
from datafeeds.usfirst_matches_parser import UsfirstMatchesParser
from datafeeds.usfirst_matches_parser_2002 import UsfirstMatchesParser2002
from datafeeds.usfirst_matches_parser_2003 import UsfirstMatchesParser2003
from datafeeds.usfirst_match_schedule_parser import UsfirstMatchScheduleParser
from datafeeds.usfirst_team_details_parser import UsfirstTeamDetailsParser
from datafeeds.usfirst_pre2003_team_events_parser import UsfirstPre2003TeamEventsParser

from models.event import Event
from models.award import Award
from models.match import Match
from models.team import Team


class DatafeedUsfirst(DatafeedBase):
    EVENT_LIST_REGIONALS_URL_PATTERN = "https://my.usfirst.org/myarea/index.lasso?event_type=FRC&season_FRC=%s"  # % (year)

    EVENT_DETAILS_URL_PATTERN = "http://www.usfirst.org/whats-going-on/event/%s?ProgramCode=FRC"  # % (eid)
    EVENT_TEAMS_URL_PATTERN = "http://www.usfirst.org/whats-going-on/event/%s/teams?sort=asc&order=Team%%20Number&ProgramCode=FRC"  # % (eid)
    TEAM_DETAILS_URL_PATTERN = "http://www.usfirst.org/whats-going-on/team/%s?ProgramCode=FRC"  # % (tpid)

    EVENT_AWARDS_URL_PATTERN = "http://www2.usfirst.org/%scomp/events/%s/awards.html"  # % (year, event_short)
    EVENT_RANKINGS_URL_PATTERN = "http://www2.usfirst.org/%scomp/events/%s/rankings.html"  # % (year, event_short)

    YEAR_MATCH_RESULTS_URL_PATTERN = {
        2003: "http://www2.usfirst.org/%scomp/events/%s/matchsum.html",  # % (year, event_short)
    }
    DEFAULT_MATCH_RESULTS_URL_PATTERN = "http://www2.usfirst.org/%scomp/events/%s/matchresults.html"  # % (year, event_short)

    DEFAULT_ALLIANCES_URL_PATTERN = "http://www2.usfirst.org/%scomp/events/%s/matchresults.html"  # % (year, event_short)

    MATCH_SCHEDULE_QUAL_URL_PATTERN = "http://www2.usfirst.org/%scomp/events/%s/schedulequal.html"  # % (year, event_short)
    MATCH_SCHEDULE_ELIMS_URL_PATTERN = "http://www2.usfirst.org/%scomp/events/%s/scheduleelim.html"  # % (year, event_short)
    EVENT_SHORT_EXCEPTIONS = {
        "arc": "Archimedes",
        "cur": "Curie",
        "gal": "Galileo",
        "new": "Newton",
        "ein": "Einstein",  # Only used for 2008ein due to FIRST's inconsistent naming
    }

    ALLIANCES_PARSER = UsfirstAlliancesParser

    YEAR_MATCH_PARSER = {
        2002: UsfirstMatchesParser2002,
        2003: UsfirstMatchesParser2003,
    }
    DEFAULT_MATCH_PARSER = UsfirstMatchesParser

    MATCH_SCHEDULE_PARSER = UsfirstMatchScheduleParser

    YEAR_AWARD_PARSER = {
        2002: UsfirstEventAwardsParser_02,
        2003: UsfirstEventAwardsParser_03_04,
        2004: UsfirstEventAwardsParser_03_04,
        2005: UsfirstEventAwardsParser_05_06,
        2006: UsfirstEventAwardsParser_05_06,
    }
    DEFAULT_AWARD_PARSER = UsfirstEventAwardsParser

    def __init__(self, *args, **kw):
        self._session_key = dict()
        super(DatafeedUsfirst, self).__init__(*args, **kw)

    def getEventAlliances(self, event):
        alliances_url = self.DEFAULT_ALLIANCES_URL_PATTERN % (
                event.year, self.EVENT_SHORT_EXCEPTIONS.get(event.event_short,
                                                            event.event_short))

        alliances, _ = self.parse(alliances_url, self.ALLIANCES_PARSER)
        return alliances

    def getEventDetails(self, first_eid):
        url = self.EVENT_DETAILS_URL_PATTERN % (first_eid)
        event, _ = self.parse(url, UsfirstEventDetailsParser)
        if event is None:
            return None

        return Event(
            id=str(event["year"]) + str.lower(str(event["event_short"])),
            end_date=event.get("end_date", None),
            event_short=event.get("event_short", None),
            event_type_enum=event.get("event_type_enum", None),
            first_eid=first_eid,
            name=event.get("name", None),
            short_name=event.get("short_name", None),
            official=True,
            start_date=event.get("start_date", None),
            venue_address=event.get("venue_address", None),
            venue=event.get("venue", None),
            location=event.get("location", None),
            timezone_id=EventHelper.get_timezone_id(event),
            website=event.get("website", None),
            year=event.get("year", None)
        )

    def getEventList(self, year):
        if type(year) is not int:
            raise TypeError("year must be an integer")
        url = self.EVENT_LIST_REGIONALS_URL_PATTERN % year
        events, _ = self.parse(url, UsfirstEventListParser)

        return [Event(
            event_type_enum=event.get("event_type_enum", None),
            event_district_enum=event.get("event_district_enum", None),
            event_short="???",
            first_eid=event.get("first_eid", None),
            name=event.get("name", None),
            year=int(year)
            )
            for event in events]

    def getEventRankings(self, event):
        url = self.EVENT_RANKINGS_URL_PATTERN % (event.year,
                                                 self.EVENT_SHORT_EXCEPTIONS.get(event.event_short, event.event_short))
        rankings, _ = self.parse(url, UsfirstEventRankingsParser)
        return rankings

    def getEventAwards(self, event):
        """
        Works reliably for regional events from 2002-present and
        championship events from 2007-present
        """
        if event.year < 2002 or (event.event_type_enum in EventType.CMP_EVENT_TYPES and event.year < 2007):
            # award pages malformatted
            logging.warning("Skipping awards parsing for event: {}".format(event.key_name))
            return []

        url = self.EVENT_AWARDS_URL_PATTERN % (event.year,
                                               self.EVENT_SHORT_EXCEPTIONS.get(event.event_short, event.event_short))
        awards, _ = self.parse(url, self.YEAR_AWARD_PARSER.get(event.year, self.DEFAULT_AWARD_PARSER))

        return [Award(
            id=Award.render_key_name(event.key_name, award['award_type_enum']),
            name_str=award['name_str'],
            award_type_enum=award['award_type_enum'],
            year=event.year,
            event=event.key,
            event_type_enum=event.event_type_enum,
            team_list=[ndb.Key(Team, 'frc{}'.format(team_number)) for team_number in award['team_number_list']],
            recipient_json_list=award['recipient_json_list']
            )
            for award in awards]

    def getEventTeams(self, year, first_eid):
        """
        Returns a list of team_numbers attending a particular Event
        """
        if type(year) is not int:
            raise TypeError("year must be an integer")

        teams = []
        seen_teams = set()
        for page in range(8):  # Ensures this won't loop forever. 8 pages should be plenty.
            url = self.EVENT_TEAMS_URL_PATTERN % (first_eid)
            if page != 0:
                url += '&page=%s' % page
            partial_teams, more_pages = self.parse(url, UsfirstEventTeamsParser)
            teams.extend(partial_teams)

            partial_seen_teams = set([team['team_number'] for team in partial_teams])
            new_teams = partial_seen_teams.difference(seen_teams)
            if (not more_pages) or (new_teams == set()):
                break

            seen_teams = seen_teams.union(partial_seen_teams)

        return [Team(
            id="frc%s" % team.get("team_number", None),
            first_tpid=team.get("first_tpid", None),
            first_tpid_year=year,
            team_number=team.get("team_number", None)
            )
            for team in teams]

    def getMatches(self, event):
        matches_url = self.YEAR_MATCH_RESULTS_URL_PATTERN.get(
            event.year, self.DEFAULT_MATCH_RESULTS_URL_PATTERN) % (
                event.year, self.EVENT_SHORT_EXCEPTIONS.get(event.event_short,
                                                            event.event_short))

        match_dicts, _ = self.parse(matches_url, self.YEAR_MATCH_PARSER.get(event.year, self.DEFAULT_MATCH_PARSER))
        if not match_dicts:  # Matches have not been played, but qual match schedule may be out
            # If this is run when there are already matches in the DB, it will overwrite scores!
            # Check to make sure event has no existing matches
            if len(Match.query(Match.event == event.key).fetch(1, keys_only=True)) == 0:
                logging.warning("No matches found for {}. Trying to parse qual match schedule.".format(event.key.id()))

                qual_match_sched_url = self.MATCH_SCHEDULE_QUAL_URL_PATTERN % (
                    event.year, self.EVENT_SHORT_EXCEPTIONS.get(event.event_short,
                                                                event.event_short))
                match_dicts, _ = self.parse(qual_match_sched_url, self.MATCH_SCHEDULE_PARSER)

        for match_dict in match_dicts:
            alliances = json.loads(match_dict['alliances_json'])
            if (alliances['red']['score'] == -1 or alliances['blue']['score'] == -1 or
                match_dict['comp_level'] in Match.ELIM_LEVELS):
                break
        else:  # Only qual matches have been played and they have all been played
            # If this is run when there are already elim matches in the DB, it will overwrite scores!
            # Check to make sure event has no existing elim matches
            if len(Match.query(Match.event == event.key, Match.comp_level.IN(Match.ELIM_LEVELS)).fetch(1, keys_only=True)) == 0:
                logging.warning("No elim matches found for {}. Trying to parse elim match schedule.".format(event.key.id()))

                elim_match_sched_url = self.MATCH_SCHEDULE_ELIMS_URL_PATTERN % (
                    event.year, self.EVENT_SHORT_EXCEPTIONS.get(event.event_short,
                                                                event.event_short))
                elim_match_dicts, _ = self.parse(elim_match_sched_url, self.MATCH_SCHEDULE_PARSER)
                match_dicts += elim_match_dicts

        matches = [Match(
            id=Match.renderKeyName(
                event.key.id(),
                match_dict.get("comp_level", None),
                match_dict.get("set_number", 0),
                match_dict.get("match_number", 0)),
            event=event.key,
            game=Match.FRC_GAMES_BY_YEAR.get(event.year, "frc_unknown"),
            set_number=match_dict.get("set_number", 0),
            match_number=match_dict.get("match_number", 0),
            comp_level=match_dict.get("comp_level", None),
            team_key_names=match_dict.get("team_key_names", None),
            time_string=match_dict.get("time_string", None),
            alliances_json=match_dict.get("alliances_json", None)
            )
            for match_dict in match_dicts]

        MatchHelper.add_match_times(event, matches)
        return matches

    def getTeamDetails(self, team):
        if hasattr(team, 'first_tpid'):
            if team.first_tpid:
                url = self.TEAM_DETAILS_URL_PATTERN % (team.first_tpid)
                team_dict, _ = self.parse(url, UsfirstTeamDetailsParser)

                if team_dict is not None and "team_number" in team_dict:
                    return Team(
                        team_number=team_dict.get("team_number", None),
                        name=self._shorten(team_dict.get("name", None)),
                        address=team_dict.get("address", None),
                        nickname=team_dict.get("nickname", None),
                        website=team_dict.get("website", None)
                    )
                else:
                    logging.warning("No team_number found scraping %s, probably retired team" % team.team_number)
                    return None

        logging.warning('Null TPID for team %s' % team.team_number)
        return None

    def getPre2003TeamEvents(self, team):
        if hasattr(team, 'first_tpid'):
            if team.first_tpid:
                url = self.TEAM_DETAILS_URL_PATTERN % (team.first_tpid)
                first_eids, _ = self.parse(url, UsfirstPre2003TeamEventsParser)
                return first_eids

        logging.warning('Null TPID for team %s' % team.team_number)
        return []

    def getTeamsTpids(self, year, skip=0):
        """
        Calling this function once establishes all of the Team objects in the Datastore.
        It does this by calling up USFIRST to search for Tpids. We have to do this in
        waves to avoid going over the GAE timeout.
        """
        # FIXME: This is not proper Datafeed form. -gregmarra 2012 Aug 26
        # TeamTpidHelper actually creates Team objects.
        TeamTpidHelper.scrapeTpids(skip, year)

########NEW FILE########
__FILENAME__ = datafeed_usfirst_legacy
import logging
import re

from google.appengine.api import memcache
from google.appengine.api import urlfetch

import tba_config

from datafeeds.datafeed_usfirst import DatafeedUsfirst
from datafeeds.usfirst_legacy_event_details_parser import UsfirstLegacyEventDetailsParser
from datafeeds.usfirst_legacy_event_teams_parser import UsfirstLegacyEventTeamsParser
from datafeeds.usfirst_legacy_team_details_parser import UsfirstLegacyTeamDetailsParser

from helpers.event_helper import EventHelper

from models.event import Event
from models.team import Team


class DatafeedUsfirstLegacy(DatafeedUsfirst):
    SESSION_KEY_GENERATING_PATTERN = "https://my.usfirst.org/myarea/index.lasso?event_type=FRC&year=%s"  # % (year)

    EVENT_DETAILS_URL_PATTERN = "https://my.usfirst.org/myarea/index.lasso?page=event_details&eid=%s"
    EVENT_TEAMS_URL_PATTERN = "https://my.usfirst.org/myarea/index.lasso?page=event_teamlist&results_size=250&eid=%s"
    TEAM_DETAILS_URL_PATTERN = "https://my.usfirst.org/myarea/index.lasso?page=team_details&tpid=%s"

    def __init__(self, *args, **kw):
        self._session_key = {}
        super(DatafeedUsfirstLegacy, self).__init__(*args, **kw)

    def getSessionKey(self, year):
        """
        Grab a page from FIRST so we can get a session key out the response header.
        """
        year = int(year)

        if self._session_key.get(year, False):
            return self._session_key.get(year)

        memcache_key = "usfirst_session_key_%s" % year
        session_key = memcache.get(memcache_key)
        if session_key is not None:
            self._session_key[year] = session_key
            return self._session_key.get(year)

        url = self.SESSION_KEY_GENERATING_PATTERN % year
        try:
            result = urlfetch.fetch(url, headers={'Referer': 'usfirst.org'}, deadline=10)
        except Exception, e:
            logging.error("URLFetch failed for: {}".format(url))
            logging.info(e)

            session_key = ''
            if tba_config.CONFIG["memcache"]:
                memcache.set(memcache_key, session_key, 60 * 5)
            self._session_key[year] = session_key
            return session_key

        if result.status_code == 200:
            session_key = result.headers.get('Set-Cookie', None)
            if session_key is not None:
                if tba_config.CONFIG["memcache"]:
                    memcache.set(memcache_key, session_key, 60 * 5)
                self._session_key[year] = session_key
                return self._session_key[year]
            logging.error('Unable to get USFIRST session key for %s.' % year)
            return None
        else:
            logging.error('HTTP code %s. Unable to retreive url: %s' %
                (result.status_code, self.SESSION_KEY_GENERATING_URL))

    def getEventDetails(self, year, first_eid):
        if type(year) is not int:
            raise TypeError("year must be an integer")
        url = self.EVENT_DETAILS_URL_PATTERN % (first_eid)
        event, _ = self.parse(url, UsfirstLegacyEventDetailsParser, self.getSessionKey(year))
        if event is None:
            return None

        return Event(
            id=str(event["year"]) + str.lower(str(event["event_short"])),
            end_date=event.get("end_date", None),
            event_short=event.get("event_short", None),
            event_type_enum=event.get("event_type_enum", None),
            first_eid=first_eid,
            name=event.get("name", None),
            short_name=event.get("short_name", None),
            official=True,
            start_date=event.get("start_date", None),
            venue_address=event.get("venue_address", None),
            location=event.get("location", None),
            timezone_id=EventHelper.get_timezone_id(event),
            website=event.get("website", None),
            year=event.get("year", None)
        )

    def getEventTeams(self, year, first_eid):
        """
        Returns a list of team_numbers attending a particular Event
        """
        if type(year) is not int:
            raise TypeError("year must be an integer")
        url = self.EVENT_TEAMS_URL_PATTERN % (first_eid)
        teams, _ = self.parse(url, UsfirstLegacyEventTeamsParser, self.getSessionKey(year))
        if teams is None:
            return None

        return [Team(
            id="frc%s" % team.get("team_number", None),
            first_tpid=team.get("first_tpid", None),
            first_tpid_year=year,
            team_number=team.get("team_number", None)
            )
            for team in teams]

    def getTeamDetails(self, team):
        if hasattr(team, 'first_tpid'):
            if team.first_tpid:
                url = self.TEAM_DETAILS_URL_PATTERN % (team.first_tpid)
                team_dict, _ = self.parse(url, UsfirstLegacyTeamDetailsParser, self.getSessionKey(team.first_tpid_year))

                if team_dict is not None and "team_number" in team_dict:
                    return Team(
                        id="frc%s" % team_dict.get("team_number", None),
                        team_number=team_dict.get("team_number", None),
                        name=self._shorten(team_dict.get("name", None)),
                        address=team_dict.get("address", None),
                        nickname=team_dict.get("nickname", None),
                        website=team_dict.get("website", None),
                        rookie_year=team_dict.get("rookie_year", None),
                    )
                else:
                    logging.warning("No team_number found scraping %s, probably retired team" % team.team_number)
                    return None

        logging.warning('Null TPID for team %s' % team.team_number)
        return None

########NEW FILE########
__FILENAME__ = datafeed_usfirst_offseason
import logging

from google.appengine.api import memcache
from google.appengine.api import urlfetch

import tba_config

from datafeeds.datafeed_base import DatafeedBase
from datafeeds.usfirst_event_offseason_list_parser import UsfirstEventOffseasonListParser

from models.event import Event

class DatafeedUsfirstOffseason(DatafeedBase):
    EVENT_OFFSEASON_LIST_URL = "http://www.usfirst.org/roboticsprograms/frc/calendar/list"

    def getEventList(self):
        events, _ = self.parse(self.EVENT_OFFSEASON_LIST_URL, UsfirstEventOffseasonListParser)

        return [Event(
            event_type_enum=event.get("event_type_enum", None),
            event_short="???",
            first_eid=event.get("first_eid", None),
            name=event.get("name", None),
            year=2014, #TODO: don't hardcode me -gregmarra 20130921
            start_date=event.get("start_date", None),
            end_date=event.get("end_date", None),
            location=event.get("location", None),
            )
            for event in events]

########NEW FILE########
__FILENAME__ = fms_event_list_parser
import datetime
import logging

from BeautifulSoup import BeautifulSoup

from datafeeds.parser_base import ParserBase


class FmsEventListParser(ParserBase):
    """
    Facilitates getting information about Events from USFIRST.
    Reads from FMS data pages, which are mostly tab delimited files wrapped in some HTML.
    """

    @classmethod
    def parse(self, html):
        return self.parse_2014(html)

    @classmethod
    def parse_2012(self, html):
        """
        Parse the information table on USFIRSTs site to extract event information.
        Return a list of dictionaries of event data.
        Works for data from 2012.
        """
        events = list()
        soup = BeautifulSoup(html,
                             convertEntities=BeautifulSoup.HTML_ENTITIES)

        for title in soup.findAll('title'):
            if "FRC Team/Event List" not in title.string:
                return None

        event_rows = soup.findAll("pre")[0].string.split("\n")

        for line in event_rows[2:]:  # first is blank, second is headers.
            data = line.split("\t")
            if len(data) > 1:
                try:
                    events.append({
                        "first_eid": data[0],
                        "name": data[1].strip(),
                        "venue": data[2],
                        "start_date": self.splitDate(data[6]),
                        "end_date": self.splitDate(data[7]),
                        "year": int(data[8]),
                        "event_short": data[11].strip().lower()
                    })
                except Exception, e:
                    logging.warning("Failed to parse event row: %s" % data)
                    logging.warning(e)

        return events

    @classmethod
    def parse_2014(self, html):
        """
        Parse the information table on USFIRSTs site to extract event information.
        Return a list of dictionaries of event data.
        Works for data from 2014.
        """
        events = list()
        soup = BeautifulSoup(html,
                             convertEntities=BeautifulSoup.HTML_ENTITIES)

        for title in soup.findAll('title'):
            if "FRC Team/Event List" not in title.string:
                return None

        event_rows = soup.findAll("pre")[0].string.split("\n")

        for line in event_rows[2:]:  # first is blank, second is headers.
            data = line.split("\t")
            if len(data) > 1:
                try:
                    events.append({
                        "first_eid": data[0],
                        #"event_subtype": data[1],
                        #"id_event_subtype": data[2],
                        #"district_code": data[3],
                        #"id_district": data[4],
                        "name": data[5].strip(),
                        "venue": data[6],
                        #"city": data[7],
                        #"state": data[8],
                        #"country"": data[9],
                        "start_date": self.splitDate(data[10]),
                        "end_date": self.splitDate(data[11]),
                        "year": int(data[12]),
                        #"num_teams": int(data[13]),
                        #"url_team_list": data[14],
                        "event_short": data[15].strip().lower(),
                        "location": "{}, {}, {}".format(data[7], data[8], data[9])
                    })
                except Exception, e:
                    logging.info("Failed to parse event row: %s" % data)
                    logging.info(e)

        return events

    @classmethod
    def splitDate(self, date):
        try:
            (year, month, day) = date.split("-")
            date = datetime.datetime(int(year), int(month), int(day))
            return date
        except Exception, e:
            return None

########NEW FILE########
__FILENAME__ = fms_team_list_parser
import logging

from BeautifulSoup import BeautifulSoup

from datafeeds.parser_base import ParserBase


class FmsTeamListParser(ParserBase):
    """
    Facilitates getting information about Teams from USFIRST.
    Reads from FMS data pages, which are mostly tab delimited files wrapped in some HTML.
    Note, this doesn't get team websites.
    """

    @classmethod
    def parse(self, html):
        """
        Parse the information table on USFIRSTs site to extract team information.
        Return a list of dictionaries of team data.
        """
        teams = list()
        soup = BeautifulSoup(html,
                             convertEntities=BeautifulSoup.HTML_ENTITIES)

        for title in soup.findAll('title'):
            if "FRC Team/Event List" not in title.string:
                return None

        team_rows = soup.findAll("pre")[0].string.split("\n")

        for line in team_rows[2:]:  # first is blank, second is headers.
            data = line.split("\t")
            if len(data) > 1:
                try:
                    teams.append({
                        "team_number": int(data[1]),
                        "name": data[2],
                        "short_name": data[3],
                        "nickname": data[7]
                    })
                except Exception, e:
                    logging.warning("Failed to parse team row: %s" % data)

        return teams

########NEW FILE########
__FILENAME__ = offseason_matches_parser
import json
import logging
import csv
import StringIO
import re

from datafeeds.parser_base import ParserBase


class OffseasonMatchesParser(ParserBase):
    @classmethod
    def parse(self, data):
        """
        Parse CSV that contains match results.
        Format is as follows:
        match_id, red1, red2, red3, blue1, blue2, blue3, red score, blue score

        Example formats of match_id:
        qm1, sf2m1, f1m1
        """
        matches = list()

        csv_data = list(csv.reader(StringIO.StringIO(data), delimiter=',', skipinitialspace=True))
        for row in csv_data:
            matches.append(self.parseCSVMatch(row))

        return matches

    @classmethod
    def parseCSVMatch(self, row):
        match_id, red_1, red_2, red_3, blue_1, blue_2, blue_3, red_score, blue_score = row
        for i in range(len(row)):
            row[i] = row[i].strip()

        team_key_names = []

        red_teams = [red_1, red_2, red_3]
        red_team_strings = []
        for team in red_teams:
            red_team_strings.append('frc' + team.upper())
            if team.isdigit():
                team_key_names.append('frc' + team.upper())

        blue_teams = [blue_1, blue_2, blue_3]
        blue_team_strings = []
        for team in blue_teams:
            blue_team_strings.append('frc' + team.upper())
            if team.isdigit():
                team_key_names.append('frc' + team.upper())

        if not red_score:
            red_score = -1
        else:
            red_score = int(red_score)

        if not blue_score:
            blue_score = -1
        else:
            blue_score = int(blue_score)

        comp_level, match_number, set_number = self.parseMatchNumberInfo(match_id)

        alliances = {"red": {"teams": red_team_strings,
                             "score": red_score},
                     "blue": {"teams": blue_team_strings,
                              "score": blue_score}}

        match = {"alliances_json": json.dumps(alliances),
                 "comp_level": comp_level,
                 "match_number": match_number,
                 "set_number": set_number,
                 "team_key_names": team_key_names}

        return match

    @classmethod
    def parseMatchNumberInfo(self, string):
        string = string.strip()
        COMP_LEVEL_MAP = {'qm': 'qm',
                          'qfm': 'qf',
                          'sfm': 'sf',
                          'fm': 'f', }

        MATCH_PARSE_STYLE = {'qm': self.parseQualMatchNumberInfo,
                             'qf': self.parseElimMatchNumberInfo,
                             'sf': self.parseElimMatchNumberInfo,
                             'f': self.parseElimMatchNumberInfo, }

        pattern = re.compile('[0-9]')
        comp_level = COMP_LEVEL_MAP[pattern.sub('', string)]

        match_number, set_number = MATCH_PARSE_STYLE[comp_level](string)
        return comp_level, match_number, set_number

    @classmethod
    def parseQualMatchNumberInfo(self, string):
        match_number = int(re.sub('\D', '', string))
        return match_number, 1

    @classmethod
    def parseElimMatchNumberInfo(self, string):
        match_number = int(string[-1:])
        set_number = int(string[-3:-2])
        return match_number, set_number

########NEW FILE########
__FILENAME__ = parser_base
import re
from BeautifulSoup import NavigableString
import HTMLParser


class ParserBase(object):
    """
    Provides a basic structure for parsing pages.
    Parsers are not allowed to return Model objects, only dictionaries.
    """

    @classmethod
    def parse(self, html):
        """
        Given a chunk of HTML, return a (result dictionary, more_pages) tuple
        """
        raise NotImplementedError("No parse method!")

    @classmethod
    def _recurseUntilString(self, node):
        """
        Digs through HTML that Word made worse.
        Written to deal with http://www2.usfirst.org/2011comp/Events/cmp/matchresults.html
        """
        if node.string is not None:
            return re.sub('\s+', ' ', node.string.replace(u'\xa0', ' ')).strip()  # remove multiple whitespaces
        if isinstance(node, NavigableString):
            return node
        if hasattr(node, 'contents'):
            results = []
            for content in node.contents:
                result = self._recurseUntilString(content)
                if result is not None:
                    result = result.strip().replace('\r', '').replace('\n', '').replace('  ', ' ')
                if result is not None and result != "":
                    results.append(result)
            if results != []:
                return ' '.join(results)
        return None

    @classmethod
    def _html_unescape(cls, html):
        h = HTMLParser.HTMLParser()
        return h.unescape(html)

    @classmethod
    def _html_unescape_items(cls, d):
        """
        Unescapes HTML in a dict
        """
        h = HTMLParser.HTMLParser()
        for key, value in d.items():
            try:
                d[key] = h.unescape(value)
            except TypeError:
                continue

########NEW FILE########
__FILENAME__ = tba_videos_parser
import logging
import re

from BeautifulSoup import BeautifulSoup

from datafeeds.parser_base import ParserBase


class TbaVideosParser(ParserBase):
    """
    Facilitates building TBAVideos store from TBA.
    """
    @classmethod
    def parse(self, html):
        """
        Parse the directory listing on TBA to extract relevant TBAVideo
        information. Returns a list of TBAVideos
        """
        soup = BeautifulSoup(html,
                             convertEntities=BeautifulSoup.HTML_ENTITIES)

        videos = dict()

        for a in soup.findAll("a", href=True):
            parts = a["href"].split(".")
            if len(parts) == 2:
                (key, filetype) = parts
                videos.setdefault(key, list())
                videos[key].append(filetype)
            else:
                logging.info("Malformed video filename: " + a["href"])
                continue

        return videos

########NEW FILE########
__FILENAME__ = twitter_matches_parser
import logging

from datafeeds.parser_base import ParserBase

ELIM_MAPPING = {'1': 'qf1m1',
                '2': 'qf2m1',
                '3': 'qf3m1',
                '4': 'qf4m1',
                '5': 'qf1m2',
                '6': 'qf2m2',
                '7': 'qf3m2',
                '8': 'qf4m2',
                '9': 'qf1m3',
                '10': 'qf2m3',
                '11': 'qf3m3',
                '12': 'qf4m3',
                '13': 'sf1m1',
                '14': 'sf2m1',
                '15': 'sf1m2',
                '16': 'sf2m2',
                '17': 'sf1m3',
                '18': 'sf2m3',
                '19': 'f1m1',
                '20': 'f1m2',
                '21': 'f1m3',
                }


class TwitterMatchesParser(ParserBase):
    @classmethod
    def parse(self, tweet):
        """
        Parse a tweet from FRCFMS.
        Returns a tuple in the following form:
        event_short, match result in CSV format

        Match CSV format is as follows:
        match_id, red1, red2, red3, blue1, blue2, blue3, red score, blue score

        Example formats of match_id:
        qm1, sf2m1, f1m1
        """
        i = tweet.split()
        try:
            event_short = str(i[0][4:].lower())
            kind = str(i[2])
            number = str(i[4])
            red_final = str(i[6])
            blue_final = str(i[8])
            red_teams = str(i[10]) + ',' + str(i[11]) + ',' + str(i[12])
            blue_teams = str(i[14]) + ',' + str(i[15]) + ',' + str(i[16])
        except IndexError:
            logging.warning("Failed to parse tweet: {}".format(tweet))
            return None, tweet

        if kind == 'E':
            match_id = ELIM_MAPPING[number]
        elif kind == 'Q':
            match_id = 'qm' + number
        else:
            match_id = '???' + number

        match_csv_row = match_id + ',' + red_teams + ',' + blue_teams + ',' + red_final + ',' + blue_final

        return event_short, match_csv_row

########NEW FILE########
__FILENAME__ = usfirst_alliances_parser
import json
import logging

from BeautifulSoup import BeautifulSoup

from datafeeds.parser_base import ParserBase


class UsfirstAlliancesParser(ParserBase):
    @classmethod
    def parse(self, html):
        """
        Parse the table that contains alliances.
        """
        soup = BeautifulSoup(html, convertEntities=BeautifulSoup.HTML_ENTITIES)

        tables = soup.findAll('table')

        alliances = self.parseAlliances(tables[4])

        return alliances, False

    @classmethod
    def parseAlliances(self, table):
        alliances = []
        for tr in table.findAll('tr'):
            tds = tr.findAll('td')
            if len(tds) < 4:
                continue

            alliance_num_str = self._recurseUntilString(tds[0])
            if not alliance_num_str.isdigit():
                continue

            alliances.append({'picks': ['frc' + self._recurseUntilString(team_td) for team_td in tds[1:]], 'declines': []})

        return alliances if alliances != [] else None

########NEW FILE########
__FILENAME__ = usfirst_event_awards_parser
import json
import logging
import re

from BeautifulSoup import BeautifulSoup

from consts.award_type import AwardType
from datafeeds.parser_base import ParserBase
from helpers.award_helper import AwardHelper


class UsfirstEventAwardsParser(ParserBase):
    """
    Parses USFIRST event award pages for awards
    Works for events 2007-present
    """
    # The format of USFIRST award pages is different for 2007-2011 and 2012-present
    # This dict defines which columns of the USFIRST award table name_str, team_number, and individual are.
    COL_NUM = {'2012-pres': {'name_str': 0,
                             'team_number': 1,
                             'individual': 3},
               '2007-11': {'name_str': 0,
                           'team_number': 1,
                           'individual': 2}}

    @classmethod
    def parse(self, html):
        """
        Parse the awards from USFIRST.
        """
        html = html.decode('utf-8', 'ignore')  # Clean html before feeding itno BeautifulSoup
        soup = BeautifulSoup(html, convertEntities=BeautifulSoup.HTML_ENTITIES)
        table = soup.findAll('table')[2]

        awards_by_type = {}
        for tr in table.findAll('tr')[1:]:
            tds = tr.findAll('td')
            if len(tds) == 5:
                parser_year = '2012-pres'
            else:
                parser_year = '2007-11'

            name_str = unicode(self._recurseUntilString(tds[self.COL_NUM[parser_year]['name_str']]))
            award_type_enum = AwardHelper.parse_award_type(name_str)
            if award_type_enum is None:
                continue

            team_number = None
            try:
                team_number = self._recurseUntilString(tds[self.COL_NUM[parser_year]['team_number']])
            except AttributeError:
                team_number = None
            if team_number and team_number.isdigit():
                team_number = int(team_number)
            else:
                team_number = None

            awardee = None
            if award_type_enum in AwardType.INDIVIDUAL_AWARDS:
                try:
                    awardee_str = self._recurseUntilString(tds[self.COL_NUM[parser_year]['individual']])
                    if awardee_str:
                        awardee = unicode(sanitize(awardee_str))
                except TypeError:
                    awardee = None
                if not awardee:
                    # Turns '' into None
                    awardee = None

            # an award must have either an awardee or a team_number
            if awardee is None and team_number is None:
                continue

            recipient_json = json.dumps({
                'team_number': team_number,
                'awardee': awardee,
            })

            if award_type_enum in awards_by_type:
                if team_number is not None:
                    awards_by_type[award_type_enum]['team_number_list'].append(team_number)
                awards_by_type[award_type_enum]['recipient_json_list'].append(recipient_json)
            else:
                awards_by_type[award_type_enum] = {
                    'name_str': strip_number(name_str),
                    'award_type_enum': award_type_enum,
                    'team_number_list': [team_number] if team_number is not None else [],
                    'recipient_json_list': [recipient_json],
                }

        return awards_by_type.values(), False


def sanitize(text):
    return text.replace('\r\n ', '')


def strip_number(text):
    # Removes things like "#3" from an award name_str
    # Example: "Regional Winners #2" becomes "Regional Winners"
    m = re.match(r'(.*) #\d*(.*)', text)
    if m is not None:
        return m.group(1) + m.group(2)
    else:
        return text

########NEW FILE########
__FILENAME__ = usfirst_event_awards_parser_02
import json
import logging
import re

from BeautifulSoup import BeautifulSoup

from consts.award_type import AwardType
from datafeeds.parser_base import ParserBase
from helpers.award_helper import AwardHelper


class UsfirstEventAwardsParser_02(ParserBase):
    """
    Parses USFIRST event award pages for awards
    """

    @classmethod
    def parse(self, html):
        """
        Parse the awards from USFIRST.
        """
        html = html.decode('utf-8', 'ignore')  # Clean html before feeding itno BeautifulSoup
        soup = BeautifulSoup(html, convertEntities=BeautifulSoup.HTML_ENTITIES)
        table = soup.findAll('table')[6]

        awards_by_type = {}
        for tr in table.findAll('tr')[3:]:
            tds = tr.findAll('td')

            name_str = unicode(self._recurseUntilString(tds[0]))
            award_type_enum = AwardHelper.parse_award_type(name_str)
            if award_type_enum is None:
                continue

            team_number = None
            try:
                team_number = self._recurseUntilString(tds[1])
            except AttributeError:
                team_number = None
            if team_number and team_number.isdigit():
                team_number = int(team_number)
            else:
                team_number = None

            awardee = None
            if award_type_enum in AwardType.INDIVIDUAL_AWARDS:
                try:
                    awardee_str = self._recurseUntilString(tds[2])
                    if awardee_str:
                        awardee = unicode(sanitize(awardee_str))
                except TypeError:
                    awardee = None
                if not awardee:
                    # Turns '' into None
                    awardee = None

            # an award must have either an awardee or a team_number
            if awardee is None and team_number is None:
                continue

            recipient_json = json.dumps({
                'team_number': team_number,
                'awardee': awardee,
            })

            if award_type_enum in awards_by_type:
                if team_number is not None:
                    awards_by_type[award_type_enum]['team_number_list'].append(team_number)
                awards_by_type[award_type_enum]['recipient_json_list'].append(recipient_json)
            else:
                awards_by_type[award_type_enum] = {
                    'name_str': strip_number(name_str),
                    'award_type_enum': award_type_enum,
                    'team_number_list': [team_number] if team_number is not None else [],
                    'recipient_json_list': [recipient_json],
                }

        return awards_by_type.values(), False


def sanitize(text):
    return text.replace('\r\n ', '')


def strip_number(text):
    # Removes things like "3" from the end an award name_str
    # Example: "Regional Winner 3" becomes "Regional Winner"
    m = re.match(r'(.*)\d$', text)
    if m is not None:
        return m.group(1).strip()
    else:
        return text.strip()

########NEW FILE########
__FILENAME__ = usfirst_event_awards_parser_03_04
import json
import logging
import re

from BeautifulSoup import BeautifulSoup

from consts.award_type import AwardType
from datafeeds.parser_base import ParserBase
from helpers.award_helper import AwardHelper


class UsfirstEventAwardsParser_03_04(ParserBase):
    """
    Parses USFIRST event award pages for awards
    """

    @classmethod
    def parse(self, html):
        """
        Parse the awards from USFIRST.
        """
        html = html.decode('utf-8', 'ignore')  # Clean html before feeding itno BeautifulSoup
        soup = BeautifulSoup(html, convertEntities=BeautifulSoup.HTML_ENTITIES)
        table = soup.findAll('table')[0]

        awards_by_type = {}
        for tr in table.findAll('tr')[2:]:
            tds = tr.findAll('td')
            if tds == []:
                continue

            name_str = unicode(self._recurseUntilString(tds[0]))
            award_type_enum = AwardHelper.parse_award_type(name_str)
            if award_type_enum is None:
                continue

            team_number = None
            try:
                team_number = self._recurseUntilString(tds[1])
            except AttributeError:
                team_number = None
            if team_number and team_number.isdigit():
                team_number = int(team_number)
            else:
                team_number = None

            awardee = None
            if award_type_enum in AwardType.INDIVIDUAL_AWARDS:
                try:
                    awardee_str = self._recurseUntilString(tds[2])
                    if awardee_str:
                        awardee = unicode(sanitize(awardee_str))
                except TypeError:
                    awardee = None
                if not awardee:
                    # Turns '' into None
                    awardee = None

            # an award must have either an awardee or a team_number
            if awardee is None and team_number is None:
                continue

            recipient_json = json.dumps({
                'team_number': team_number,
                'awardee': awardee,
            })

            if award_type_enum in awards_by_type:
                if team_number is not None:
                    awards_by_type[award_type_enum]['team_number_list'].append(team_number)
                awards_by_type[award_type_enum]['recipient_json_list'].append(recipient_json)
            else:
                awards_by_type[award_type_enum] = {
                    'name_str': strip_number(name_str),
                    'award_type_enum': award_type_enum,
                    'team_number_list': [team_number] if team_number is not None else [],
                    'recipient_json_list': [recipient_json],
                }

        return awards_by_type.values(), False


def sanitize(text):
    return text.replace('\r\n ', '')


def strip_number(text):
    # Removes things like "3" or "#3" from the end an award name_str
    # Example: "Regional Winner 3" or "Regional Winner #3" becomes "Regional Winner"
    m = re.match(r'(.*) #\d*(.*)', text)
    if m is None:
        m = re.match(r'(.*)\d$', text)
    if m is not None:
        return m.group(1).strip()
    else:
        return text.strip()

########NEW FILE########
__FILENAME__ = usfirst_event_awards_parser_05_06
import json
import logging
import re

from BeautifulSoup import BeautifulSoup

from consts.award_type import AwardType
from datafeeds.parser_base import ParserBase
from helpers.award_helper import AwardHelper


class UsfirstEventAwardsParser_05_06(ParserBase):
    """
    Parses USFIRST event award pages for awards
    """

    @classmethod
    def parse(self, html):
        """
        Parse the awards from USFIRST.
        """
        html = html.decode('utf-8', 'ignore')  # Clean html before feeding itno BeautifulSoup
        soup = BeautifulSoup(html, convertEntities=BeautifulSoup.HTML_ENTITIES)

        # Bad formatting on some pages makes this necessary
        trs1 = soup.findAll('tr', {'style': 'background-color:#D2D2FF;'})
        trs2 = soup.findAll('tr', {'style': 'background-color:#FFFFFF;'})
        trs = trs1 + trs2

        awards_by_type = {}
        for tr in trs:
            tds = tr.findAll('td')

            name_str = unicode(self._recurseUntilString(tds[0]))
            award_type_enum = AwardHelper.parse_award_type(name_str)
            if award_type_enum is None:
                continue

            team_number = None
            try:
                team_number = self._recurseUntilString(tds[1])
            except AttributeError:
                team_number = None
            if team_number and team_number.isdigit():
                team_number = int(team_number)
            else:
                team_number = None

            awardee = None
            if award_type_enum in AwardType.INDIVIDUAL_AWARDS:
                try:
                    awardee_str = self._recurseUntilString(tds[2])
                    if awardee_str:
                        awardee = unicode(sanitize(awardee_str))
                except TypeError:
                    awardee = None
                if not awardee:
                    # Turns '' into None
                    awardee = None

            # an award must have either an awardee or a team_number
            if awardee is None and team_number is None:
                continue

            recipient_json = json.dumps({
                'team_number': team_number,
                'awardee': awardee,
            })

            if award_type_enum in awards_by_type:
                if team_number is not None:
                    awards_by_type[award_type_enum]['team_number_list'].append(team_number)
                awards_by_type[award_type_enum]['recipient_json_list'].append(recipient_json)
            else:
                awards_by_type[award_type_enum] = {
                    'name_str': strip_number(name_str),
                    'award_type_enum': award_type_enum,
                    'team_number_list': [team_number] if team_number is not None else [],
                    'recipient_json_list': [recipient_json],
                }

        return awards_by_type.values(), False


def sanitize(text):
    return text.replace('\r\n ', '')


def strip_number(text):
    # Removes things like "#3" from an award name_str
    # Example: "Regional Winners #2" becomes "Regional Winners"
    m = re.match(r'(.*) #\d*(.*)', text)
    if m is not None:
        return m.group(1) + m.group(2)
    else:
        return text

########NEW FILE########
__FILENAME__ = usfirst_event_details_parser
import datetime
import logging
import re

from BeautifulSoup import BeautifulSoup

from datafeeds.parser_base import ParserBase
from helpers.event_helper import EventHelper


class UsfirstEventDetailsParser(ParserBase):
    @classmethod
    def parse(self, html):
        """
        Parse an event's details page from USFIRST.
        """
        # page_titles look like this:
        # <YEAR> <EVENT_NAME> (<EVENT_TYPE>)
        event_type_re = r'\((.+)\)'

        # locality_regions look like this:
        # <locality>, <region> <random string can have spaces>
        event_locality_region_re = r'(.*?), ([^ ]*)'

        result = dict()
        soup = BeautifulSoup(html, convertEntities=BeautifulSoup.HTML_ENTITIES)

        page_title = soup.find('h1', {'id': 'thepagetitle'}).text
        result['name'] = unicode(re.sub(r'\([^)]*\)', '', page_title[4:]).strip())
        result['short_name'] = EventHelper.getShortName(result['name'])
        result['event_type_enum'] = EventHelper.parseEventType(unicode(re.search(event_type_re, page_title).group(1).strip()))

        try:
            event_dates = soup.find('div', {'class': 'event-dates'}).text
            result['start_date'], result['end_date'] = self._parseEventDates(event_dates)
            result['year'] = int(event_dates[-4:])
        except Exception, detail:
            logging.error('Date Parse Failed: ' + str(detail))

        address = soup.find('div', {'class': 'event-address'})
        if address is not None:
            address_lines_stripped = [re.sub('\s+', ' ', line.replace(u'\xa0', ' ')).strip() for line in address.findAll(text=True)]
            result['venue_address'] = unicode('\r\n'.join(address_lines_stripped)).encode('ascii', 'ignore')

            if len(address_lines_stripped) >= 2:
                match = re.match(event_locality_region_re, address_lines_stripped[-2])
                locality, region = match.group(1), match.group(2)
                country = address_lines_stripped[-1]
                result['location'] = '%s, %s, %s' % (locality, region, country)
            if len(address_lines_stripped) >= 3:
                result['venue'] = address_lines_stripped[0]

        website_tag = soup.find('div', {'class': 'event-info-link'})
        if website_tag is not None:
            result['website'] = unicode(website_tag.find('a')['href'])

        # http://www2.usfirst.org/2010comp/Events/SDC/matchresults.html
        try:
            match_results_url = soup.find('div', {'class': 'event-match-results'}).find('a')['href']
            m = re.match(r"http://www2\.usfirst\.org/%scomp/Events/([a-zA-Z0-9]*)/" % result["year"], match_results_url)
            result['event_short'] = unicode(m.group(1).lower())
        except AttributeError, detail:
            logging.warning("Event short parse failed: {}".format(detail))
            return None, False

        self._html_unescape_items(result)

        return result, False

    @classmethod
    def _parseEventDates(self, datestring):
        """
        Parses the date string provided by USFirst into actual event start and stop DateTimes.
        FIRST date strings look like "01-Apr to 03-Apr-2010" or "09-Mar-2005".
        """
        month_dict = {"Jan": 1, "Feb": 2, "Mar": 3, "Apr": 4, "May": 5, "Jun": 6,
                      "Jul": 7, "Aug": 8, "Sep": 9, "Oct": 10, "Nov": 11, "Dec": 12}

        # "01-Apr to 03-Apr-2010"
        # or "09-Mar-2005"
        if "to" in datestring:
            start_day = int(datestring[0:2])
            start_month = month_dict[datestring[3:6]]
            start_year = int(datestring[-4:])

            stop_day = int(datestring[10:12])
            stop_month = month_dict[datestring[13:16]]
            stop_year = int(datestring[-4:])

            start_date = datetime.datetime(start_year, start_month, start_day)
            stop_date = datetime.datetime(stop_year, stop_month, stop_day)
        else:
            day = int(datestring[0:2])
            month = month_dict[datestring[3:6]]
            year = int(datestring[-4:])

            start_date = datetime.datetime(year, month, day)
            stop_date = datetime.datetime(year, month, day)

        return (start_date, stop_date)

########NEW FILE########
__FILENAME__ = usfirst_event_list_parser
from datetime import datetime
import urlparse
import logging

from BeautifulSoup import BeautifulSoup

from consts.district_type import DistrictType
from consts.event_type import EventType
from datafeeds.parser_base import ParserBase
from helpers.event_helper import EventHelper


class UsfirstEventListParser(ParserBase):
    @classmethod
    def parse(self, html):
        """
        Parse the list of events from USFIRST. This provides us with basic
        information about events and is how we first discover them.
        """
        events = list()
        soup = BeautifulSoup(html,
                             convertEntities=BeautifulSoup.HTML_ENTITIES)

        for tr in soup.findAll('tr'):  # Events are in table rows
            event = dict()
            try:
                tds = tr.findAll('td')
                if tds[0].string is None:
                    # this may happen if this is a district event, in which case we can also extract the district name
                    event_type_str = unicode(tds[0].findAll(text=True)[2].string)

                    district_name_str = unicode(tds[0].findAll('em')[0].string)
                else:
                    event_type_str = unicode(tds[0].string)
                    district_name_str = None
                event["event_type_enum"] = EventHelper.parseEventType(event_type_str)
                event["event_district_enum"] = EventHelper.parseDistrictName(district_name_str)
                url_get_params = urlparse.parse_qs(urlparse.urlparse(tds[1].a["href"]).query)
                event["first_eid"] = url_get_params["eid"][0]

                event["name"] = ''.join(tds[1].a.findAll(text=True)).strip()  # <em>s in event names fix
                #event.venue = unicode(tds[2].string)
                #event.location = unicode(tds[3].string)

                # try:
                #    event_dates = str(tds[4].string).strip()
                #    event.start_date, event.stop_date = self.parseEventDates(event_dates)
                #    event.year = int(event_dates[-4:])
                # except Exception, detail:
                #    logging.error('Date Parse Failed: ' + str(detail))

                if event.get("event_type_enum", None) in EventType.NON_CMP_EVENT_TYPES:
                    events.append(event)

            except Exception, detail:
                logging.info('Event parsing failed: ' + str(detail))

        return events, False

########NEW FILE########
__FILENAME__ = usfirst_event_offseason_list_parser
from datetime import datetime
import urlparse
import logging

from BeautifulSoup import BeautifulSoup

from consts.event_type import EventType
from datafeeds.parser_base import ParserBase
from helpers.event_helper import EventHelper


class UsfirstEventOffseasonListParser(ParserBase):

    @classmethod
    def parse(self, html):
        """
        Parse the list of events from USFIRST. This provides us with basic
        information about events and is how we first discover them.
        """
        events = list()
        soup = BeautifulSoup(html,
                             convertEntities=BeautifulSoup.HTML_ENTITIES)

        for table in soup.findAll('table'):
            if table.find('caption').text.count('Off Season Events') > 0:
                trs = table.find('tbody').findAll('tr')
                for tr in trs:
                    tds = tr.findAll('td')
                    event = dict()
                    for td in tds:
                        if td["class"].count('views-field-title') > 0:
                            event["first_eid"] = td.a["href"].split("/")[-1]
                            event["name"] = " ".join(td.a.text.split(" ")[:-1])
                            event["location"] = str(td.a.text.split(" ")[-1]).translate(None, "()")
                        for span in td.findAll('span'):
                            if span["class"].count("date-display-start") > 0:
                                event["start_date"] = datetime.strptime(span["content"][:10], "%Y-%m-%d")
                            if span["class"].count("date-display-end") > 0:
                                event["end_date"] = datetime.strptime(span["content"][:10], "%Y-%m-%d")
                            if span["class"].count("date-display-single") > 0:
                                event["start_date"] = datetime.strptime(span["content"][:10], "%Y-%m-%d")
                                event["end_date"] = datetime.strptime(span["content"][:10], "%Y-%m-%d")
                    event["event_type_enum"] = EventType.OFFSEASON
                    events.append(event)

        return events, False

########NEW FILE########
__FILENAME__ = usfirst_event_rankings_parser
from BeautifulSoup import BeautifulSoup

from datafeeds.parser_base import ParserBase


class UsfirstEventRankingsParser(ParserBase):
    """
    Works for official events from 2007-2012
    """
    @classmethod
    def parse(self, html):
        """
        Parse the rankings from USFIRST.
        """
        soup = BeautifulSoup(html,
                             convertEntities=BeautifulSoup.HTML_ENTITIES)

        rankings = []
        tables = soup.findAll('table')
        rankings_table = tables[2]

        for tr in rankings_table.findAll('tr'):
            tds = tr.findAll('td')
            if len(tds) > 1:
                row = []
                for td in tds:
                    row.append(str(self._html_unescape(self._recurseUntilString(td))))
                rankings.append(row)

        return rankings, False

########NEW FILE########
__FILENAME__ = usfirst_event_teams_parser
import re

from BeautifulSoup import BeautifulSoup

from datafeeds.parser_base import ParserBase


class UsfirstEventTeamsParser(ParserBase):
    @classmethod
    def parse(self, html):
        """
        Find what Teams are attending an Event, and return their team_numbers.
        """

        teamRe = re.compile(r'whats-going-on\/team\/(\d*)\?ProgramCode=FRC">(\d*)')

        teams = list()
        for first_tpid, team_number in teamRe.findall(html):
            team = dict()
            team["first_tpid"] = int(first_tpid)
            team["team_number"] = int(team_number)
            teams.append(team)

        soup = BeautifulSoup(html, convertEntities=BeautifulSoup.HTML_ENTITIES)
        more_pages = soup.find('a', {'title': 'Go to next page'}) is not None
        return teams, more_pages

########NEW FILE########
__FILENAME__ = usfirst_legacy_event_details_parser
import datetime
import logging
import re

from BeautifulSoup import BeautifulSoup

from datafeeds.parser_base import ParserBase
from helpers.event_helper import EventHelper


class UsfirstLegacyEventDetailsParser(ParserBase):
    @classmethod
    def parse(self, html):
        """
        Parse an event's details page from USFIRST.
        """
        # locality_regions look like this:
        # <locality>, <region> <random string can have spaces>
        event_locality_region_re = r'(.*?), ([^ ]*)'

        result = dict()
        soup = BeautifulSoup(html,
                convertEntities=BeautifulSoup.HTML_ENTITIES)

        for tr in soup.findAll('tr'):
            tds = tr.findAll('td')
            if len(tds) > 1:
                field = str(tds[0].string)
                if field == "Event":
                    result["name"] = unicode(''.join(tds[1].findAll(text=True))).strip()
                    result['short_name'] = EventHelper.getShortName(result['name'])
                if field == "Event Subtype":
                    result["event_type_enum"] = EventHelper.parseEventType(unicode(tds[1].string))
                if field == "When":
                    try:
                        event_dates = str(tds[1].string).strip()
                        result["start_date"], result["end_date"] = self._parseEventDates(event_dates)
                        result["year"] = int(event_dates[-4:])
                    except Exception, detail:
                        logging.error('Date Parse Failed: ' + str(detail))
                if field == "Where":
                    address_lines_stripped = [re.sub('\s+', ' ', line.replace(u'\xa0', ' ')).strip() for line in tds[1].findAll(text=True)]
                    result["venue_address"] = unicode('\r\n'.join(address_lines_stripped)).encode('ascii', 'ignore')

                    match = re.match(event_locality_region_re, address_lines_stripped[-2])
                    locality, region = match.group(1), match.group(2)
                    country = address_lines_stripped[-1]
                    result['location'] = '%s, %s, %s' % (locality, region, country)
                if field == "Event Info":
                    result["website"] = unicode(tds[1].a['href'])
                if field == "Match Results":
                    #http://www2.usfirst.org/2010comp/Events/SDC/matchresults.html
                    m = re.match(r"http://www2\.usfirst\.org/%scomp/Events/([a-zA-Z0-9]*)/matchresults\.html" % result["year"], tds[1].a["href"])
                    if m is None:
                        # Some 2013 events are beautiful-souping tds[1].a["href"] to "http://www2.usfirst.org/2013comp/Events/FLBR" inexplicbly
                        m = re.match(r"http://www2\.usfirst\.org/%scomp/Events/([a-zA-Z0-9]*)" % result["year"], tds[1].a["href"])
                    result["event_short"] = m.group(1).lower()

        self._html_unescape_items(result)

        return result, False

    @classmethod
    def _parseEventDates(self, datestring):
        """
        Parses the date string provided by USFirst into actual event start and stop DateTimes.
        FIRST date strings look like "01-Apr - 03-Apr-2010" or "09-Mar-2005".
        """
        month_dict = {"Jan": 1, "Feb": 2, "Mar": 3, "Apr": 4, "May": 5, "Jun": 6,
                      "Jul": 7, "Aug": 8, "Sep": 9, "Oct": 10, "Nov": 11, "Dec": 12}

        # "01-Apr - 03-Apr-2010"
        # or "09-Mar-2005"
        if " - " in datestring:
            start_day = int(datestring[0:2])
            start_month = month_dict[datestring[3:6]]
            start_year = int(datestring[-4:])

            stop_day = int(datestring[9:11])
            stop_month = month_dict[datestring[12:15]]
            stop_year = int(datestring[-4:])

            start_date = datetime.datetime(start_year, start_month, start_day)
            stop_date = datetime.datetime(stop_year, stop_month, stop_day)
        else:
            day = int(datestring[0:2])
            month = month_dict[datestring[3:6]]
            year = int(datestring[-4:])

            start_date = datetime.datetime(year, month, day)
            stop_date = datetime.datetime(year, month, day)

        return (start_date, stop_date)

########NEW FILE########
__FILENAME__ = usfirst_legacy_event_teams_parser
import re

from datafeeds.parser_base import ParserBase


class UsfirstLegacyEventTeamsParser(ParserBase):
    @classmethod
    def parse(self, html):
        """
        Find what Teams are attending an Event, and return their team_numbers.
        """
        # This code is based on TeamTpidHelper.
        # -gregmarra 5 Dec 2010

        teamRe = re.compile(r'tpid=[A-Za-z0-9=&;\-:]*?">\d+')
        teamNumberRe = re.compile(r'\d+$')
        tpidRe = re.compile(r'\d+')

        teams = []
        for teamResult in teamRe.findall(html):
            team = dict()
            team["team_number"] = int(teamNumberRe.findall(teamResult)[0])
            team["first_tpid"] = int(tpidRe.findall(teamResult)[0])
            teams.append(team)

        return teams, False

########NEW FILE########
__FILENAME__ = usfirst_legacy_team_details_parser
import logging
import re

# for db.link
from google.appengine.ext import db

from BeautifulSoup import BeautifulSoup

from datafeeds.parser_base import ParserBase


class UsfirstLegacyTeamDetailsParser(ParserBase):
    """
    Facilitates building TBAVideos store from TBA.
    """
    @classmethod
    def parse(self, html):
        """
        Parse the information table on USFIRSTs site to extract relevant team
        information. Return a dictionary.
        """
        team = dict()
        soup = BeautifulSoup(html,
                convertEntities=BeautifulSoup.HTML_ENTITIES)

        if soup.find(text='No team found.') is not None:
            logging.error('FIRST lacks team.')
            return None

        for tr in soup.findAll('tr'):
            tds = tr.findAll('td')
            if len(tds) > 1:
                field = str(tds[0].string)
                if field == "Team Number":
                    team["team_number"] = int(tds[1].b.string)
                if field == "Team Name":
                    team["name"] = unicode(tds[1].string)
                if field == "Team Location":
                    #TODO: Filter out &nbsp;'s and stuff -greg 5/21/2010
                    team["address"] = unicode(tds[1].string)
                if field == "Rookie Season":
                   team["rookie_year"] = int(tds[1].string)
                if field == "Team Nickname":
                    team["nickname"] = unicode(tds[1].string)
                if field == "Team Website":
                    try:
                        website_str = re.sub(r'^/|/$', '', unicode(tds[1].a["href"]))  # strip starting and trailing slashes
                        if not website_str.startswith('http://') and not website_str.startswith('https://'):
                            website_str = 'http://%s' % website_str
                        team['website'] = db.Link(website_str)
                    except Exception, details:
                        logging.info("Team website is invalid for team %s." % team['team_number'])
                        logging.info(details)

        self._html_unescape_items(team)

        return team, False

########NEW FILE########
__FILENAME__ = usfirst_matches_parser
import json
import logging

from BeautifulSoup import BeautifulSoup

from datafeeds.parser_base import ParserBase


class UsfirstMatchesParser(ParserBase):
    @classmethod
    def parse(self, html):
        """
        Parse the table that contains match results.
        """
        matches = []
        soup = BeautifulSoup(html, convertEntities=BeautifulSoup.HTML_ENTITIES)

        tables = soup.findAll('table')

        matches.extend(self.parseMatchResultList(tables[2]))
        matches.extend(self.parseMatchResultList(tables[3]))

        return matches, False

    @classmethod
    def parseMatchResultList(self, table):
        matches = list()
        for tr in table.findAll('tr')[2:]:
            tds = tr.findAll('td')
            if len(tds) == 10 or len(tds) == 11:  # qual has 10, elim has 11
                if self._recurseUntilString(tds[1]) is not None:
                    time_string = self._recurseUntilString(tds[0])
                    red_teams = ["frc" + self._recurseUntilString(tds[-8]),
                                 "frc" + self._recurseUntilString(tds[-7]),
                                 "frc" + self._recurseUntilString(tds[-6])]
                    blue_teams = ["frc" + self._recurseUntilString(tds[-5]),
                                  "frc" + self._recurseUntilString(tds[-4]),
                                  "frc" + self._recurseUntilString(tds[-3])]

                    try:
                        if self._recurseUntilString(tds[-2]) is None:
                            red_score = -1
                        else:
                            red_score = int(self._recurseUntilString(tds[-2]))

                        if self._recurseUntilString(tds[-1]) is None:
                            blue_score = -1
                        else:
                            blue_score = int(self._recurseUntilString(tds[-1]))

                        comp_level, match_number, set_number = self.parseMatchNumberInfo(self._recurseUntilString(tds[1]))

                        alliances = {
                            "red": {
                                "teams": red_teams,
                                "score": red_score
                            },
                            "blue": {
                                "teams": blue_teams,
                                "score": blue_score
                            }
                        }

                        matches.append({
                            "alliances_json": json.dumps(alliances),
                            "comp_level": comp_level,
                            "match_number": match_number,
                            "set_number": set_number,
                            "team_key_names": red_teams + blue_teams,
                            "time_string": time_string
                            })

                    except Exception, detail:
                        logging.info('Match Parse Failed: ' + str(detail))

        return matches

    @classmethod
    def parseMatchNumberInfo(self, string):
        """
        Parse out the information about an qual or elim match based on the
        string USFIRST provides.
        They look like "34", "Semi 2-2", or "Final 1-1"
        """
        comp_level_dict = {
            "Qtr": "qf",
            "Semi": "sf",
            "Final": "f",
        }

        # string comes in as unicode.
        string = str(string).strip()
        comp_level = comp_level_dict.get(string[:-4], "qm")
        if comp_level == 'qm':
            match_number = int(string)
            set_number = 1
        else:
            match_number = int(string[-1:])
            set_number = int(string[-3:-2])

        return comp_level, match_number, set_number

########NEW FILE########
__FILENAME__ = usfirst_matches_parser_2002
import json
import re
import logging

from BeautifulSoup import BeautifulSoup

from datafeeds.parser_base import ParserBase


class UsfirstMatchesParser2002(ParserBase):
    @classmethod
    def parse(self, html):
        """
        Parse the table that contains qualification match results.
        Note that 2002 match tables aren't consistently formatted, and this
        parser takes that into account.
        """
        soup = BeautifulSoup(html,
                             convertEntities=BeautifulSoup.HTML_ENTITIES)

        match_table = soup.findAll('table')[5].findAll('table')[0].findAll('table')[1]
        matches = self.parseMatchResultList(match_table)

        return matches, False

    @classmethod
    def parseMatchResultList(self, table):
        """
        Makes use of the assumption that all qual matches are before elim
        matches in the table.
        """
        matches = []
        mid_match = False  # Matches are split across rows. This keeps track of whether or not we are parsing the same match.
        ignore_match = False  # Parsing failed. Ignore this match
        mid_match_comp_level = None
        mid_match_number = None
        mid_match_set_number = None
        mid_match_teams = []  # Teams for the current match, if mid_match. If not mid match, this should be empty.
        mid_match_scores = []  # Scores for the current match, if mid_match. If not mid match, this should be empty.
        elim_match_counter = {}  # Keeps track of the last set number. Keys are like "qf1" and values are like "2"
        for tr in table.findAll('tr')[1:]:  # skip table header
            tds = tr.findAll('td')
            col1 = self._recurseUntilString(tds[0])

            # End of a match or start of a new match. Combine info and reset mid_match info.
            if mid_match and (col1 is None or 'match' in col1.lower()):
                if not ignore_match:
                    if len(mid_match_teams) == len(mid_match_scores):
                        red_teams = mid_match_teams[:len(mid_match_teams) / 2]
                        blue_teams = mid_match_teams[len(mid_match_teams) / 2:]
                        red_score = mid_match_scores[0]
                        blue_score = mid_match_scores[len(mid_match_scores) / 2]
                        alliances = {"red": {
                                        "teams": red_teams,
                                        "score": red_score
                                    },
                                    "blue": {
                                        "teams": blue_teams,
                                        "score": blue_score
                                    }
                        }
                        matches.append({"alliances_json": json.dumps(alliances),
                                        "comp_level": mid_match_comp_level,
                                        "match_number": mid_match_number,
                                        "set_number": mid_match_set_number,
                                        "team_key_names": red_teams + blue_teams,
                        })
                    else:
                        logging.warning("Lengths of mid_match_teams ({}) and mid_match_scores ({}) aren't the same!".format(mid_match_teams, mid_match_scores))

                mid_match = False
                ignore_match = False
                mid_match_comp_level = None
                mid_match_number = None
                mid_match_set_number = None
                mid_match_teams = []
                mid_match_scores = []
                continue

            if not mid_match:
                mid_match = True
                try:
                    match_or_set_number = int(re.findall(r'\d+', col1)[0])
                except:
                    logging.warning("Match/Set number parse for '{}' failed!".format(col1))
                    ignore_match = True
                    continue

                col1_lower = col1.lower()
                if (('final' in col1_lower) or ('quarter' in col1_lower) or
                   ('semi' in col1_lower) or ('champ' in col1_lower)):

                    if 'quarter' in col1_lower:
                        mid_match_comp_level = 'qf'
                    elif 'semi' in col1_lower:
                        mid_match_comp_level = 'sf'
                    else:
                        mid_match_comp_level = 'f'

                    match_counter_key = '{}{}'.format(mid_match_comp_level, match_or_set_number)
                    if match_counter_key in elim_match_counter:
                        elim_match_counter[match_counter_key] += 1
                    else:
                        elim_match_counter[match_counter_key] = 1
                    mid_match_set_number = match_or_set_number
                    mid_match_number = elim_match_counter[match_counter_key]
                else:
                    mid_match_comp_level = 'qm'
                    mid_match_set_number = 1
                    mid_match_number = match_or_set_number

            else:
                try:
                    team_key = 'frc{}'.format(int(re.findall(r'\d+', col1)[0]))
                except:
                    logging.warning("Team number parse for '{}' failed!".format(col1))
                    ignore_match = True
                    continue

                score_col = self._recurseUntilString(tds[1])
                try:
                    match_score = int(re.findall(r'\d+', score_col)[0])
                    if match_score is None:
                        match_score = -1
                except:
                    logging.warning("Score parse for '{}' failed!".format(score_col))
                    ignore_match = True
                    continue

                mid_match_teams.append(team_key)
                mid_match_scores.append(match_score)

        return matches

########NEW FILE########
__FILENAME__ = usfirst_matches_parser_2003
import json
import re
import logging

from BeautifulSoup import BeautifulSoup

from datafeeds.parser_base import ParserBase


class UsfirstMatchesParser2003(ParserBase):
    @classmethod
    def parse(self, html):
        """
        Parse the table that contains qualification match results.
        Note that 2002 match tables aren't consistently formatted, and this
        parser takes that into account.
        """
        soup = BeautifulSoup(html,
                             convertEntities=BeautifulSoup.HTML_ENTITIES)

        match_table = soup.findAll('table')[0]
        matches = self.parseMatchResultList(match_table)

        return matches, False

    @classmethod
    def parseMatchResultList(self, table):
        matches = []
        mid_match = False  # Matches are split across rows. This keeps track of whether or not we are parsing the same match.
        ignore_match = False  # Parsing failed. Ignore this match
        mid_match_comp_level = None
        mid_match_number = None
        mid_match_set_number = None
        mid_match_teams = []  # Teams for the current match, if mid_match. If not mid match, this should be empty.
        mid_match_scores = []  # Scores for the current match, if mid_match. If not mid match, this should be empty.
        for tr in table.findAll('tr')[2:]:  # skip table headers
            tds = tr.findAll('td')
            match_name = self._recurseUntilString(tds[0])

            # Start of a new match. Combine info and reset mid_match info.
            if mid_match and (match_name is not None or len(tds) == 1):
                if not ignore_match:
                    if len(mid_match_teams) == len(mid_match_scores):
                        blue_teams = mid_match_teams[:len(mid_match_teams) / 2]
                        red_teams = mid_match_teams[len(mid_match_teams) / 2:]
                        blue_score = mid_match_scores[0]
                        red_score = mid_match_scores[len(mid_match_scores) / 2]
                        alliances = {"red": {
                                        "teams": red_teams,
                                        "score": red_score
                                    },
                                    "blue": {
                                        "teams": blue_teams,
                                        "score": blue_score
                                    }
                        }
                        matches.append({"alliances_json": json.dumps(alliances),
                                        "comp_level": mid_match_comp_level,
                                        "match_number": mid_match_number,
                                        "set_number": mid_match_set_number,
                                        "team_key_names": red_teams + blue_teams,
                        })
                    else:
                        logging.warning("Lengths of mid_match_teams ({}) and mid_match_scores ({}) aren't the same!".format(mid_match_teams, mid_match_scores))

                mid_match = False
                ignore_match = False
                mid_match_comp_level = None
                mid_match_number = None
                mid_match_set_number = None
                mid_match_teams = []
                mid_match_scores = []
                continue

            if not mid_match:
                mid_match = True
                match_name_lower = match_name.lower()
                if 'elim' in match_name_lower:
                    # looks like: "Elim Finals.1" or or "Elim QF1.2"
                    if 'finals' in match_name_lower:
                        mid_match_comp_level = 'f'
                        mid_match_set_number = 1
                        try:
                            mid_match_number = int(re.findall(r'\d+', match_name)[0])
                        except:
                            logging.warning("Finals match number parse for '%s' failed!" % match_name)
                            ignore_match = True
                            continue
                    else:
                        if 'qf' in match_name_lower:
                            mid_match_comp_level = 'qf'
                        elif 'sf' in match_name_lower:
                            mid_match_comp_level = 'sf'
                        else:
                            logging.warning("Could not extract comp level from: {}".format(match_name))
                            ignore_match = True
                            continue

                        try:
                            prefix, suffix = match_name_lower.split('.')
                            mid_match_set_number = int(prefix[-1])
                            mid_match_number = int(suffix[0])
                        except:
                            logging.warning("Could not extract match set and number from: {}".format(match_name))
                            ignore_match = True
                            continue
                else:
                    mid_match_comp_level = 'qm'
                    mid_match_set_number = 1
                    try:
                        mid_match_number = int(re.findall(r'\d+', match_name)[0])
                    except:
                        logging.warning("Qual match number parse for '%s' failed!" % match_name)
                        ignore_match = True
                        continue
            else:
                team_col = self._recurseUntilString(tds[2])
                try:
                    team_key = 'frc{}'.format(int(re.findall(r'\d+', team_col)[0]))
                except:
                    logging.warning("Team number parse for '%s' failed!" % team_col)
                    ignore_match = True
                    continue

                score_col = self._recurseUntilString(tds[3])
                try:
                    match_score = int(re.findall(r'\d+', score_col)[0])
                    if match_score is None:
                        match_score = -1
                except:
                    logging.warning("Score parse for '%s' failed!" % score_col)
                    ignore_match = True
                    continue

                mid_match_teams.append(team_key)
                mid_match_scores.append(match_score)

        return matches

########NEW FILE########
__FILENAME__ = usfirst_match_schedule_parser
import json
import logging

from BeautifulSoup import BeautifulSoup

from datafeeds.parser_base import ParserBase


class UsfirstMatchScheduleParser(ParserBase):
    @classmethod
    def parse(self, html):
        """
        Parse the table that contains the match schedule
        """
        soup = BeautifulSoup(html,
                             convertEntities=BeautifulSoup.HTML_ENTITIES)

        tables = soup.findAll('table')

        matches = self.parseMatchResultList(tables[2])

        return matches, False

    @classmethod
    def parseMatchResultList(self, table):
        matches = []
        for tr in table.findAll('tr')[2:]:
            tds = tr.findAll('td')
            if len(tds) == 8 or len(tds) == 9:
                if self._recurseUntilString(tds[1]) is not None:
                    time_string = self._recurseUntilString(tds[0])
                    red_teams = ["frc" + self._recurseUntilString(tds[-6]), "frc" + self._recurseUntilString(tds[-5]), "frc" + self._recurseUntilString(tds[-4])]
                    blue_teams = ["frc" + self._recurseUntilString(tds[-3]), "frc" + self._recurseUntilString(tds[-2]), "frc" + self._recurseUntilString(tds[-1])]

                    if 'frc0' in red_teams + blue_teams:  # some matches may have placeholer teams
                        continue

                    try:
                        elim_match_number_info = self.parseElimMatchNumberInfo(self._recurseUntilString(tds[1]))
                        if elim_match_number_info is None:
                            comp_level = "qm"
                            match_number = int(self._recurseUntilString(tds[1]))
                            set_number = 1
                        else:
                            comp_level, match_number, set_number = elim_match_number_info

                        alliances = {
                            "red": {
                                "teams": red_teams,
                                "score":-1,
                            },
                            "blue": {
                                "teams": blue_teams,
                                "score":-1,
                            }
                        }

                        matches.append({
                            "alliances_json": json.dumps(alliances),
                            "comp_level": comp_level,
                            "match_number": match_number,
                            "set_number": set_number,
                            "team_key_names": red_teams + blue_teams,
                            "time_string": time_string
                            })

                    except Exception, detail:
                        logging.info('Match Parse Failed: ' + str(detail))

        return matches

    @classmethod
    def parseElimMatchNumberInfo(self, string):
        """
        Parse out the information about an elimination match based on the
        string USFIRST provides.
        They look like "Semi 2-2"
        """
        comp_level_dict = {
            "Qtr": "qf",
            "Semi": "sf",
            "Final": "f",
        }

        # string comes in as unicode.
        string = str(string).strip()

        comp_level = comp_level_dict.get(string[:-4], None)
        if comp_level is None:
            return None

        match_number = int(string[-1:])
        set_number = int(string[-3:-2])

        return comp_level, match_number, set_number

########NEW FILE########
__FILENAME__ = usfirst_pre2003_team_events_parser
import re

from BeautifulSoup import BeautifulSoup

from datafeeds.parser_base import ParserBase


class UsfirstPre2003TeamEventsParser(ParserBase):
    @classmethod
    def parse(self, html):
        """
        Parse team page on USFIRSTs site to extract FIRST event_ids for events
        that the team has attended. Return a list of event_id.
        """
        team_event_re = r'/whats-going-on/event/([0-9]+)'

        soup = BeautifulSoup(html, convertEntities=BeautifulSoup.HTML_ENTITIES)

        first_eids = []
        history_table = soup.find('div', {'class': 'team-history-wrapper'}).findAll('table')[0]
        for tr in history_table.findAll('tr')[1:]:  # skip table title row
            trs = tr.findAll('td')
            year = int(trs[0].text)
            if year < 2003:
                eid = re.findall(team_event_re, str(trs[1]))[0]
                first_eids.append(eid)

        return first_eids, False

########NEW FILE########
__FILENAME__ = usfirst_team_details_parser
import logging
import re

# for db.link
from google.appengine.ext import db

from BeautifulSoup import BeautifulSoup

from datafeeds.parser_base import ParserBase


class UsfirstTeamDetailsParser(ParserBase):
    """
    Facilitates building TBAVideos store from TBA.
    """
    @classmethod
    def parse(self, html):
        """
        Parse the information table on USFIRSTs site to extract relevant team
        information. Return a dictionary.
        """
        # page_titles look like this:
        # Team Number <NUM> - "<NICK>"
        team_num_re = r'Team Number ([0-9]+) \-'
        team_nick_re = r'"(.*)\"'

        # team addresses look like tihs:
        # <locality>, <region> <random string can have spaces> <country>
        team_address_re = r'(.*?), ([^ ]*) *.* (.*)'

        team = dict()
        soup = BeautifulSoup(html,
                             convertEntities=BeautifulSoup.HTML_ENTITIES)

        page_title = soup.find('h1', {'id': 'thepagetitle'}).text
        try:
            team['team_number'] = int(re.search(team_num_re, page_title).group(1).strip())
        except AttributeError, details:
            logging.warning("Team number could not be parsed: {}".format(details))
            return None, False
        team['nickname'] = unicode(re.search(team_nick_re, page_title).group(1).strip())

        full_address = unicode(soup.find('div', {'class': 'team-address'}).find('div', {'class': 'content'}).text)
        match = re.match(team_address_re, full_address)
        if match:
            locality, region, country = match.group(1), match.group(2), match.group(3)
            team['address'] = '%s, %s, %s' % (locality, region, country)

        team['name'] = unicode(soup.find('div', {'class': 'team-name'}).text)

        try:
            website_str = re.sub(r'^/|/$', '', unicode(soup.find('div', {'class': 'team-website'}).find('a')['href']))  # strip starting and trailing slashes
            if not (website_str.startswith('http://') or website_str.startswith('https://')):
                website_str = 'http://%s' % website_str
            team['website'] = db.Link(website_str)
        except Exception, details:
            logging.info("Team website is invalid for team %s." % team['team_number'])
            logging.info(details)

        self._html_unescape_items(team)

        return team, False

########NEW FILE########
__FILENAME__ = deploy
"""
To use:
1. Clone a production copy of TBA in the same directory as the development copy by running: `git clone git@github.com:the-blue-alliance/the-blue-alliance.git the-blue-alliance-prod`
2. Change the application ID in ../the-blue-alliance-prod/app.yaml from "tbatv-dev-hrd" to "tbatv-prod-hrd"
3. Ensure line 31 of this file points to the correct location

Warning: If you CTRL-c out of this script between lines 18 and 20, make sure the application ID in ../the-blue-alliance-prod/app.yaml is still correct.
"""

import os
import sys


def main(argv):
    skip_tests = '-s' in argv

    os.chdir('../the-blue-alliance-prod')
    os.system('git stash')  # undoes the application ID change to app.yaml
    os.system('git pull origin master')
    os.system('git stash pop')  # restores the application ID change to app.yaml

    test_status = 0
    if skip_tests:
        print "Skipping tests!"
        os.system('paver make')
    else:
        test_status = os.system('paver preflight')
    os.chdir('../')

    if test_status == 0:
        os.system('python ~/Downloads/google_appengine/appcfg.py --oauth2 update the-blue-alliance-prod/')
    else:
        print "Tests failed! Did not deploy."

if __name__ == "__main__":
    main(sys.argv[1:])

########NEW FILE########
__FILENAME__ = do_compress
#!/usr/bin/python
import os.path
import optparse

YUI_COMPRESSOR = 'utils/yuicompressor-2.4.7.jar'

SCRIPTS_MAIN = ['static/jwplayer/jwplayer.js',
                'static/xcharts/d3.v2.min.js',
                'static/xcharts/xcharts.min.js',
                'static/javascript/tba_js/tablesorter.js',
                'static/javascript/tba_js/tba_charts.js',
                'static/javascript/tba_js/tba_countdown.js',
                'static/javascript/tba_js/tba_sidebar.js',
                'static/javascript/tba_js/tba_typeahead.js',
                'static/javascript/tba_js/tba.js',
                ]

SCRIPTS_GAMEDAY = SCRIPTS_MAIN + ['static/javascript/tba_js/gameday.js',
                                  'static/javascript/tba_js/gameday_twitter.js',
                                  'static/javascript/tba_js/gameday_matchbar.js']

STYLESHEETS_MAIN = ['static/css/precompiled_css/jquery.fancybox.css',
                    'static/css/precompiled_css/tablesorter.css',
                    'static/xcharts/xcharts.min.css',
                    'static/css/less_css/tba_style.main.css',
                    ]

STYLESHEETS_GAMEDAY = ['static/css/precompiled_css/jquery.fancybox.css',
                       'static/css/precompiled_css/tablesorter.css',
                       'static/css/less_css/tba_style.gameday.css',
                       ]

SCRIPTS_MAIN_OUT = 'static/javascript/tba_combined_js.main.min.js'
SCRIPTS_GAMEDAY_OUT = 'static/javascript/tba_combined_js.gameday.min.js'
STYLESHEETS_MAIN_OUT = 'static/css/tba_combined_style.main.min.css'
STYLESHEETS_GAMEDAY_OUT = 'static/css/tba_combined_style.gameday.min.css'


def compress(in_files, out_file, in_type='js', verbose=False,
             temp_file='.temp'):
    temp = open(temp_file, 'w')
    for f in in_files:
        fh = open(f)
        data = fh.read() + '\n'
        fh.close()

        temp.write(data)

        print ' + %s' % f
    temp.close()

    options = ['-o "%s"' % out_file,
               '--type %s' % in_type]

    if verbose:
        options.append('-v')

    os.system('java -jar "%s" %s "%s"' % (YUI_COMPRESSOR,
                                          ' '.join(options),
                                          temp_file))

    org_size = os.path.getsize(temp_file)
    new_size = os.path.getsize(out_file)

    print '=> %s' % out_file
    print 'Original: %.2f kB' % (org_size / 1024.0)
    print 'Compressed: %.2f kB' % (new_size / 1024.0)
    print 'Reduction: %.1f%%' % (float(org_size - new_size) / org_size * 100)
    print ''


def main(kind=None):
    if kind == 'js' or kind is None:
        print 'Compressing Main JavaScript...'
        compress(SCRIPTS_MAIN, SCRIPTS_MAIN_OUT, 'js')

        print 'Compressing GameDay JavaScript...'
        compress(SCRIPTS_GAMEDAY, SCRIPTS_GAMEDAY_OUT, 'js')

    if kind == 'css' or kind is None:
        print 'Compressing Main CSS...'
        compress(STYLESHEETS_MAIN, STYLESHEETS_MAIN_OUT, 'css')

        print 'Compressing GameDay CSS...'
        compress(STYLESHEETS_GAMEDAY, STYLESHEETS_GAMEDAY_OUT, 'css')

if __name__ == '__main__':
    parser = optparse.OptionParser()
    options, args = parser.parse_args()
    if len(args) < 1:
        main()
    else:
        main(args[0])

########NEW FILE########
__FILENAME__ = admin_helper
class AdminHelper(object):
    """
    Contains functions helpful for /admin/.
    """

    @classmethod
    def checkPermission(self, user, action):
        """
        Check if a user has permission to undertake some action.
        """

        # TODO: Implement me. -gregmarra 12 Sep 2010

        return True

    @classmethod
    def getCurrentUser(self):
        """
        Get the current user.
        """

########NEW FILE########
__FILENAME__ = api_helper
import json
import logging
from datetime import datetime

from google.appengine.api import memcache

import tba_config
from helpers.event_helper import EventHelper

from models.event import Event
from models.event_team import EventTeam
from models.match import Match
from models.team import Team


class ApiHelper(object):
    """Helper for api_controller."""
    @classmethod
    def getTeamInfo(cls, team_key):
        """
        Return a Team dict with basic information.
        """
        memcache_key = "api_team_info_%s" % team_key
        team_dict = memcache.get(memcache_key)
        if team_dict is None:
            team = Team.get_by_id(team_key)
            if team is not None:
                team_dict = dict()
                team_dict["key"] = team.key_name
                team_dict["team_number"] = team.team_number
                team_dict["name"] = team.name
                team_dict["nickname"] = team.nickname
                team_dict["website"] = team.website
                team_dict["location"] = team.location

                event_teams = EventTeam.query(EventTeam.team == team.key,
                                              EventTeam.year == datetime.now().year)\
                                              .fetch(1000, projection=[EventTeam.event])
                team_dict["events"] = [event_team.event.id() for event_team in event_teams]

                try:
                    team_dict["location"] = team.location
                    team_dict["locality"] = team.locality
                    team_dict["region"] = team.region
                    team_dict["country_name"] = team.country_name
                except Exception, e:
                    logging.warning("Failed to include Address for api_team_info_%s: %s" % (team_key, e))

                # TODO: Reduce caching time before 2013 season. 2592000 is one month -gregmarra
                if tba_config.CONFIG["memcache"]:
                    memcache.set(memcache_key, team_dict, 2592000)
            else:
                raise IndexError

        return team_dict

    @classmethod
    def getEventInfo(cls, event_key):
        """
        Return an Event dict with basic information
        """

        memcache_key = "api_event_info_%s" % event_key
        event_dict = memcache.get(memcache_key)
        if event_dict is None:
            event = Event.get_by_id(event_key)
            if event is not None:
                event_dict = dict()
                event_dict["key"] = event.key_name
                event_dict["year"] = event.year
                event_dict["event_code"] = event.event_short
                event_dict["name"] = event.name
                event_dict["short_name"] = event.short_name
                event_dict["location"] = event.location
                event_dict["official"] = event.official
                event_dict["facebook_eid"] = event.facebook_eid

                if event.start_date:
                    event_dict["start_date"] = event.start_date.isoformat()
                else:
                    event_dict["start_date"] = None
                if event.end_date:
                    event_dict["end_date"] = event.end_date.isoformat()
                else:
                    event_dict["end_date"] = None

                event.prepTeamsMatches()
                event_dict["teams"] = [team.key_name for team in event.teams]
                event_dict["matches"] = [match.key_name for match in event.matches]

                if tba_config.CONFIG["memcache"]:
                    memcache.set(memcache_key, event_dict, 60 * 60)
        return event_dict

    @classmethod
    def addTeamEvents(cls, team_dict, year):
        """
        Consume a Team dict, and return it with a year's Events.
        """
        memcache_key = "api_team_events_%s_%s" % (team_dict["key"], year)
        event_list = memcache.get(memcache_key)

        if event_list is None:
            team = Team.get_by_id(team_dict["key"])
            events = [a.event.get() for a in EventTeam.query(EventTeam.team == team.key, EventTeam.year == int(year)).fetch(1000)]
            events = sorted(events, key=lambda event: event.start_date)
            event_list = [cls.getEventInfo(e.key_name) for e in events]
            for event_dict, event in zip(event_list, events):
                event_dict["team_wlt"] = EventHelper.getTeamWLT(team_dict["key"], event)

            # TODO: Reduce caching time before 2013 season. 2592000 is one month -gregmarra
            if tba_config.CONFIG["memcache"]:
                memcache.set(memcache_key, event_list, 2592000)

        team_dict["events"] = event_list
        return team_dict

    @classmethod
    def addTeamDetails(cls, team_dict, year):
        """
        Consume a Team dict, and return it with a year's Events filtered and Matches added
        """

        # TODO Matches should live under Events - gregmarra 1 feb 2011
        # TODO Filter Events by year - gregmarra 1 feb 2011

        memcache_key = "api_team_details_%s_%s" % (team_dict["key"], year)
        matches_list = memcache.get(memcache_key)
        if matches_list is None:
            matches = list()
            team = Team.get_by_id(team_dict["key"])
            for e in [a.event.get() for a in EventTeam.query(EventTeam.team == team.key).fetch(1000) if a.year == year]:
                match_list = Match.query(Match.event == event.key, Match.team_key_names == team.key_name).fetch(500)
                matches.extend(match_list)
            matches_list = list()
            for match in matches:
                match_dict = dict()
                match_dict["key"] = match.key_name
                match_dict["event"] = match.event
                match_dict["comp_level"] = match.comp_level
                match_dict["set_number"] = match.set_number
                match_dict["match_number"] = match.match_number
                match_dict["team_keys"] = match.team_key_names
                match_dict["alliances"] = json.loads(match.alliances_json)
                matches_list.append(match_dict)

            # TODO: Reduce caching time before 2013 season. 2592000 is one month -gregmarra
            if tba_config.CONFIG["memcache"]:
                memcache.set(memcache_key, matches_list, 2592000)

        team_dict["matches"] = matches_list
        return team_dict

    @classmethod
    def getMatchDetails(cls, match_key):
        """
        Returns match details
        """
        memcache_key = "api_match_details_%s" % match_key
        match_dict = memcache.get(memcache_key)

        if match_dict is None:
            match = Match.get_by_id(match_key)
            if match is None:
                return None

            match_dict = {}
            match_dict["key"] = match.key_name
            match_dict["event"] = match.event.id()
            match_dict["competition_level"] = match.name
            match_dict["set_number"] = match.set_number
            match_dict["match_number"] = match.match_number
            match_dict["team_keys"] = match.team_key_names
            match_dict["alliances"] = json.loads(match.alliances_json)
            match_dict["videos"] = match.videos 
            match_dict["time_string"] = match.time_string
            if match.time is not None:
                match_dict["time"] =  match.time.strftime("%s")
            else:
                match_dict["time"] = None

            if tba_config.CONFIG["memcache"]:
                memcache.set(memcache_key, match_dict, (2 * (60 * 60)))

        return match_dict

########NEW FILE########
__FILENAME__ = award_helper
import logging

from consts.award_type import AwardType


# Prioritized sort order for certain awards
sort_order = {
    AwardType.CHAIRMANS: 0,
    AwardType.FOUNDERS: 1,
    AwardType.ENGINEERING_INSPIRATION: 2,
    AwardType.ROOKIE_ALL_STAR: 3,
    AwardType.WOODIE_FLOWERS: 4,
    AwardType.VOLUNTEER: 5,
    AwardType.DEANS_LIST: 6,
    AwardType.WINNER: 7,
    AwardType.FINALIST: 8,
}


"""
An award matches an AwardType if the award's name_str contains every string in
the the first list of the tuple and does NOT contain any string in the second
list of the tuple.
"""
AWARD_MATCHING_STRINGS = [
    (AwardType.CHAIRMANS, (["chairman"], ["hon", "finalist"])),
    (AwardType.CHAIRMANS_HONORABLE_MENTION, (["chairman", "hon", "mention"], [])),
    (AwardType.CHAIRMANS_HONORABLE_MENTION, (["chairman", "finalist"], [])),
    (AwardType.ENGINEERING_INSPIRATION, (["engineering inspiration"], [])),
    (AwardType.WINNER, (["regional winner"], [])),
    (AwardType.WINNER, (["championship winner"], [])),
    (AwardType.WINNER, (["championship champion"], [])),
    (AwardType.WINNER, (["division champion"], [])),
    (AwardType.WINNER, (["winner", "1"], [])),
    (AwardType.WINNER, (["winner", "2"], [])),
    (AwardType.WINNER, (["winner", "3"], ["3d"])),
    (AwardType.WINNER, (["winner", "4"], [])),
    (AwardType.WINNER, (["division", "champion", "1"], ["finalist"])),
    (AwardType.WINNER, (["division", "champion", "2"], ["finalist"])),
    (AwardType.WINNER, (["division", "champion", "3"], ["finalist", "3d"])),
    (AwardType.WINNER, (["division", "champion", "4"], ["finalist"])),
    (AwardType.WINNER, (["championship", "champion", "1"], ["finalist"])),
    (AwardType.WINNER, (["championship", "champion", "2"], ["finalist"])),
    (AwardType.WINNER, (["championship", "champion", "3"], ["finalist", "3d"])),
    (AwardType.WINNER, (["championship", "champion", "4"], ["finalist"])),
    (AwardType.FINALIST, (["regional finalist"], ["dean"])),
    (AwardType.FINALIST, (["championship finalist"], ["dean"])),
    (AwardType.FINALIST, (["division finalist"], ["dean"])),
    (AwardType.FINALIST, (["finalist", "1"], ["dean"])),
    (AwardType.FINALIST, (["finalist", "2"], ["dean"])),
    (AwardType.FINALIST, (["finalist", "3"], ["dean", "3d"])),
    (AwardType.FINALIST, (["finalist", "4"], ["dean"])),
    (AwardType.COOPERTITION, (["coopertition"], [])),
    (AwardType.SPORTSMANSHIP, (["sportsmanship"], [])),
    (AwardType.CREATIVITY, (["creativity"], [])),
    (AwardType.ENGINEERING_EXCELLENCE, (["engineering", "excellence"], [])),
    (AwardType.ENTREPRENEURSHIP, (["entrepreneurship"], [])),
    (AwardType.ENTREPRENEURSHIP, (["kleiner", "perkins", "caufield", "byers"], [])),
    (AwardType.EXCELLENCE_IN_DESIGN, (["excellence in design"], ["cad", "animation"])),
    (AwardType.EXCELLENCE_IN_DESIGN_CAD, (["excellence in design", "cad"], [])),
    (AwardType.EXCELLENCE_IN_DESIGN_ANIMATION, (["excellence in design", "animation"], [])),
    (AwardType.DEANS_LIST, (["dean", "list"], [])),
    (AwardType.BART_KAMEN_MEMORIAL, (["bart", "kamen", "memorial"], [])),
    (AwardType.DRIVING_TOMORROWS_TECHNOLOGY, (["driving", "tomorrow", "technology"], [])),
    (AwardType.DRIVING_TOMORROWS_TECHNOLOGY, (["delphi", "driv", "tech"], [])),
    (AwardType.GRACIOUS_PROFESSIONALISM, (["gracious professionalism"], [])),
    (AwardType.HIGHEST_ROOKIE_SEED, (["highest rookie seed"], [])),
    (AwardType.IMAGERY, (["imagery"], [])),
    (AwardType.INDUSTRIAL_DEESIGN, (["industrial design"], [])),
    (AwardType.MEDIA_AND_TECHNOLOGY, (["media", "technology"], [])),
    (AwardType.MAKE_IT_LOUD, (["make", "loud"], [])),
    (AwardType.SAFETY, (["safety"], [])),
    (AwardType.INNOVATION_IN_CONTROL, (["innovation in control"], [])),
    (AwardType.QUALITY, (["quality"], [])),
    (AwardType.ROOKIE_ALL_STAR, (["rookie", "all", "star"], [])),
    (AwardType.ROOKIE_INSPIRATION, (["rookie inspiration"], [])),
    (AwardType.SPIRIT, (["spirit"], [])),
    (AwardType.WEBSITE, (["website"], [])),
    (AwardType.WEBSITE, (["web", "site"], [])),
    (AwardType.VISUALIZATION, (["visualization"], ["rising"])),
    (AwardType.VOLUNTEER, (["volunteer"], [])),
    (AwardType.WOODIE_FLOWERS, (["woodie flowers"], [])),
    (AwardType.JUDGES, (["judge"], [])),
    (AwardType.FOUNDERS, (["founder"], [])),
    (AwardType.AUTODESK_INVENTOR, (["autodesk inventor"], [])),
    (AwardType.FUTURE_INNOVATOR, (["future innovator"], [])),
    (AwardType.RECOGNITION_OF_EXTRAORDINARY_SERVICE, (["recognition", "extraordinary", "service"], [])),
    (AwardType.OUTSTANDING_CART, (["outstanding", "cart"], [])),
    (AwardType.WSU_AIM_HIGHER, (["wayne", "state", "university", "aim", "higher"], [])),
    (AwardType.LEADERSHIP_IN_CONTROL, (["leadership", "control"], [])),
    (AwardType.NUM_1_SEED, (["#1", "seed"], [])),
    (AwardType.INCREDIBLE_PLAY, (["incredible", "play"], [])),
    (AwardType.PEOPLES_CHOICE_ANIMATION, (["people", "choice", "animation"], [])),
    (AwardType.VISUALIZATION_RISING_STAR, (["visualization", "rising"], [])),
    (AwardType.BEST_OFFENSIVE_ROUND, (["best", "offensive", "round"], [])),
    (AwardType.BEST_PLAY_OF_THE_DAY, (["best", "play"], [])),
    (AwardType.FEATHERWEIGHT_IN_THE_FINALS, (["featherweight", "finals"], [])),
    (AwardType.MOST_PHOTOGENIC, (["photogenic"], [])),
    (AwardType.OUTSTANDING_DEFENSE, (["outstanding defense"], [])),
    (AwardType.POWER_TO_SIMPLIFY, (["power to simplify"], [])),
    (AwardType.AGAINST_ALL_ODDS, (["against all odds"], [])),
    (AwardType.RISING_STAR, (["autodesk", "rising star"], ["hon", "mention"])),
    (AwardType.CONTENT_COMMUNICATION_HONORABLE_MENTION, (["content communication", "hon", "mention"], [])),
    (AwardType.TECHNICAL_EXECUTION_HONORABLE_MENTION, (["technical execution", "hon", "mention"], [])),
    (AwardType.REALIZATION, (["autodesk", "realization"], ["hon", "mention"])),
    (AwardType.REALIZATION_HONORABLE_MENTION, (["autodesk", "realization", "hon", "mention"], [])),
    (AwardType.DESIGN_YOUR_FUTURE, (["autodesk", "design your future"], ["hon", "mention"])),
    (AwardType.DESIGN_YOUR_FUTURE_HONORABLE_MENTION, (["autodesk", "design your future", "hon", "mention"], [])),
    (AwardType.SPECIAL_RECOGNITION_CHARACTER_ANIMATION, (["autodesk", "special recognition", "character animation"], ["hon", "mention"])),
    (AwardType.HIGH_SCORE, (["high score"], [])),
    (AwardType.TEACHER_PIONEER, (["teacher pioneer"], [])),
    (AwardType.BEST_CRAFTSMANSHIP, (["best craftsmanship"], [])),
    (AwardType.BEST_DEFENSIVE_MATCH, (["best defensive match"], [])),
    (AwardType.PLAY_OF_THE_DAY, (["play of the day"], [])),
]


class AwardHelper(object):
    @classmethod
    def organizeAwards(self, award_list):
        """
        Sorts awards first by sort_order and then alphabetically by name_str
        """
        sorted_awards = sorted(award_list, key=lambda award: sort_order.get(award.award_type_enum, award.name_str))
        return sorted_awards

    @classmethod
    def parse_award_type(self, name_str):
        """
        Returns the AwardType given a name_str, or None if there are no matches.
        """
        name_str_lower = name_str.lower()

        # to match awards without the "#1", "#2", etc suffix
        if name_str_lower == 'winner':
            return AwardType.WINNER
        elif name_str_lower == 'finalist':
            return AwardType.FINALIST

        for type_enum, (yes_strings, no_strings) in AWARD_MATCHING_STRINGS:
            for string in yes_strings:
                if string not in name_str_lower:
                    break
            else:
                for string in no_strings:
                    if string in name_str_lower:
                        break
                else:
                    # found a match
                    return type_enum
        # no matches
        logging.warning("Found an award without an associated type: " + name_str)
        return None

########NEW FILE########
__FILENAME__ = award_manipulator
import json

from helpers.cache_clearer import CacheClearer
from helpers.manipulator_base import ManipulatorBase


class AwardManipulator(ManipulatorBase):
    """
    Handle Award database writes.
    """
    @classmethod
    def clearCache(cls, affected_refs):
        CacheClearer.clear_award_and_references(affected_refs)

    @classmethod
    def updateMerge(self, new_award, old_award, auto_union=True):
        """
        Given an "old" and a "new" Award object, replace the fields in the
        "old" award that are present in the "new" award, but keep fields from
        the "old" award that are null in the "new" award.
        """
        immutable_attrs = [
            'event',
            'award_type_enum',
            'year',
        ]  # These build key_name, and cannot be changed without deleting the model.

        attrs = [
            'name_str',
        ]

        list_attrs = []

        auto_union_attrs = [
            'team_list',
            'recipient_json_list',
        ]

        json_attrs = {
            'recipient_json_list'
        }

        # if not auto_union, treat auto_union_attrs as list_attrs
        if not auto_union:
            list_attrs += auto_union_attrs
            auto_union_attrs = []

        for attr in attrs:
            if getattr(new_award, attr) is not None:
                if getattr(new_award, attr) != getattr(old_award, attr):
                    setattr(old_award, attr, getattr(new_award, attr))
                    old_award.dirty = True
            if getattr(new_award, attr) == "None":
                if getattr(old_award, attr, None) is not None:
                    setattr(old_award, attr, None)
                    old_award.dirty = True

        for attr in list_attrs:
            if len(getattr(new_award, attr)) > 0 or not auto_union:
                if getattr(new_award, attr) != getattr(old_award, attr):
                    setattr(old_award, attr, getattr(new_award, attr))
                    old_award.dirty = True

        for attr in auto_union_attrs:
            # JSON equaltiy comparison is not deterministic
            if attr in json_attrs:
                old_list = [json.loads(j) for j in getattr(old_award, attr)]
                new_list = [json.loads(j) for j in getattr(new_award, attr)]
            else:
                old_list = getattr(old_award, attr)
                new_list = getattr(new_award, attr)

            for item in new_list:
                if item not in old_list:
                    old_list.append(item)
                    old_award.dirty = True

            # Turn dicts back to JSON
            if attr in json_attrs:
                merged_list = [json.dumps(d) for d in old_list]
            else:
                merged_list = old_list

            setattr(old_award, attr, merged_list)

        return old_award

########NEW FILE########
__FILENAME__ = bulkloader_helper
"""
Provides helper functions to assist with bulkloader.yaml
"""


def fix_json(x):
    """
    Replace single quotes in JSON with double quotes.
    """
    if len(x) > 0:
        return str(x).replace("\'", "\"")
    else:
        return None


def fix_list(x):
    """
    Turn a string of a list into a Python list.
    """
    if len(x) > 0:
        y = eval(x)
        if len(y) > 0:
            return y

    return None

########NEW FILE########
__FILENAME__ = cache_clearer
from google.appengine.ext import ndb

from controllers.api.api_team_controller import ApiTeamController
from controllers.api.api_event_controller import ApiEventController, ApiEventTeamsController, \
                                                 ApiEventMatchesController, ApiEventStatsController, \
                                                 ApiEventRankingsController, ApiEventAwardsController, ApiEventListController

from models.event import Event
from models.event_team import EventTeam
from models.team import Team


class CacheClearer(object):
    @classmethod
    def clear_award_and_references(cls, affected_refs):
        """
        Clears cache for controllers that references this award
        """
        event_keys = affected_refs['event']
        team_keys = affected_refs['team_list']
        years = affected_refs['year']

        cls._clear_event_awards_controllers(event_keys)
        cls._clear_teams_controllers(team_keys, years)

    @classmethod
    def clear_event_and_references(cls, affected_refs):
        """
        Clears cache for controllers that references this event
        """
        event_keys = affected_refs['key']
        years = affected_refs['year']

        event_team_keys_future = EventTeam.query(EventTeam.event.IN([event_key for event_key in event_keys])).fetch_async(None, keys_only=True)

        team_keys = set()
        for et_key in event_team_keys_future.get_result():
            team_key_name = et_key.id().split('_')[1]
            team_keys.add(ndb.Key(Team, team_key_name))

        cls._clear_events_controllers(event_keys)
        cls._clear_eventlist_controllers(years)
        cls._clear_teams_controllers(team_keys, years)

    @classmethod
    def clear_eventteam_and_references(cls, affected_refs):
        """
        Clears cache for controllers that references this eventteam
        """
        event_keys = affected_refs['event']
        team_keys = affected_refs['team']
        years = affected_refs['year']

        cls._clear_eventteams_controllers(event_keys)
        cls._clear_teams_controllers(team_keys, years)

    @classmethod
    def clear_match_and_references(cls, affected_refs):
        """
        Clears cache for controllers that references this match
        """
        event_keys = affected_refs['event']
        team_keys = affected_refs['team_keys']
        years = affected_refs['year']

        cls._clear_matches_controllers(event_keys)
        cls._clear_teams_controllers(team_keys, years)

    @classmethod
    def clear_team_and_references(cls, affected_refs):
        """
        Clears cache for controllers that references this team
        """
        team_keys = affected_refs['key']

        event_team_keys_future = EventTeam.query(EventTeam.team.IN([team_key for team_key in team_keys])).fetch_async(None, keys_only=True)

        event_keys = set()
        years = set()
        for et_key in event_team_keys_future.get_result():
            event_key_name = et_key.id().split('_')[0]
            event_keys.add(ndb.Key(Event, event_key_name))
            years.add(int(event_key_name[:4]))

        cls._clear_teams_controllers(team_keys, years)
        cls._clear_eventteams_controllers(event_keys)

    @classmethod
    def _clear_event_awards_controllers(cls, event_keys):
        for event_key in filter(None, event_keys):
            ApiEventAwardsController.clear_cache(event_key.id())

    @classmethod
    def _clear_events_controllers(cls, event_keys):
        for event_key in filter(None, event_keys):
            ApiEventController.clear_cache(event_key.id())
            ApiEventStatsController.clear_cache(event_key.id())
            ApiEventRankingsController.clear_cache(event_key.id())

    @classmethod
    def _clear_eventlist_controllers(cls, years):
        for year in filter(None, years):
            ApiEventListController.clear_cache(year)

    @classmethod
    def _clear_eventteams_controllers(cls, event_keys):
        for event_key in filter(None, event_keys):
            ApiEventTeamsController.clear_cache(event_key.id())

    @classmethod
    def _clear_matches_controllers(cls, event_keys):
        for event_key in filter(None, event_keys):
            ApiEventMatchesController.clear_cache(event_key.id())

    @classmethod
    def _clear_teams_controllers(cls, team_keys, years):
        for team_key in filter(None, team_keys):
            for year in filter(None, years):
                ApiTeamController.clear_cache(team_key.id(), year)

########NEW FILE########
__FILENAME__ = team_details_data_fetcher
import datetime

from google.appengine.ext import ndb

from  models.award import Award
from  models.event_team import EventTeam
from  models.match import Match


class TeamDetailsDataFetcher(object):
    @classmethod
    def fetch(self, team, year, return_valid_years=False):
        """
        returns: events_sorted, matches_by_event_key, awards_by_event_key, valid_years
        of a team for a given year
        """
        @ndb.tasklet
        def get_events_and_matches_async():
            if return_valid_years:
                event_team_keys_query = EventTeam.query(EventTeam.team == team.key)
            else:
                event_team_keys_query = EventTeam.query(EventTeam.team == team.key, EventTeam.year == year)
            event_team_keys = yield event_team_keys_query.fetch_async(1000, keys_only=True)
            event_teams = yield ndb.get_multi_async(event_team_keys)
            event_keys = []
            for event_team in event_teams:
                if return_valid_years:
                    valid_years.add(event_team.year)  # valid_years is a "global" variable (defined below). Doing this removes the complexity of having to propagate the years up through the tasklet call chain.
                if not return_valid_years or event_team.year == year:
                    event_keys.append(event_team.event)
            events, matches = yield ndb.get_multi_async(event_keys), get_matches_async(event_keys)
            raise ndb.Return((events, matches))

        @ndb.tasklet
        def get_matches_async(event_keys):
            if event_keys == []:
                raise ndb.Return([])
            match_keys = yield Match.query(
                Match.event.IN(event_keys), Match.team_key_names == team.key_name).fetch_async(500, keys_only=True)
            matches = yield ndb.get_multi_async(match_keys)
            raise ndb.Return(matches)

        @ndb.tasklet
        def get_awards_async():
            award_keys = yield Award.query(Award.year == year, Award.team_list == team.key).fetch_async(500, keys_only=True)
            awards = yield ndb.get_multi_async(award_keys)
            raise ndb.Return(awards)

        @ndb.toplevel
        def get_events_matches_awards():
            (events, matches), awards = yield get_events_and_matches_async(), get_awards_async()
            raise ndb.Return(events, matches, awards)

        valid_years = set()
        events, matches, awards = get_events_matches_awards()
        valid_years = sorted(valid_years)

        events_sorted = sorted(events, key=lambda e: e.start_date if e.start_date else datetime.datetime(year, 12, 31))  # unknown goes last

        matches_by_event_key = {}
        for match in matches:
            if match.event in matches_by_event_key:
                matches_by_event_key[match.event].append(match)
            else:
                matches_by_event_key[match.event] = [match]
        awards_by_event_key = {}
        for award in awards:
            if award.event in awards_by_event_key:
                awards_by_event_key[award.event].append(award)
            else:
                awards_by_event_key[award.event] = [award]

        return events_sorted, matches_by_event_key, awards_by_event_key, valid_years

########NEW FILE########
__FILENAME__ = event_test_creator
import datetime

from consts.event_type import EventType
from helpers.event_manipulator import EventManipulator
from helpers.event_team.event_team_test_creator import EventTeamTestCreator
from helpers.match.match_test_creator import MatchTestCreator
from models.event import Event


class EventTestCreator(object):
    @classmethod
    def createFutureEvent(self, only_event=False):
        event = Event(
            id="{}testfuture".format(datetime.datetime.now().year),
            end_date=datetime.datetime.today() + datetime.timedelta(days=12),
            event_short="testfuture",
            event_type_enum=EventType.REGIONAL,
            first_eid="5561",
            name="Future Test Event",
            start_date=datetime.datetime.today() + datetime.timedelta(days=8),
            year=datetime.datetime.now().year,
            venue_address="123 Fake Street, California, USA",
            website="http://www.google.com"
        )
        event = EventManipulator.createOrUpdate(event)
        if not only_event:
            EventTeamTestCreator.createEventTeams(event)
        return event

    @classmethod
    def createPresentEvent(self, only_event=False):
        id_string = "{}testpresent".format(datetime.datetime.now().year)
        event = Event(
            id=id_string,
            end_date=datetime.datetime.today() + datetime.timedelta(days=1),
            event_short="testpresent",
            event_type_enum=EventType.REGIONAL,
            first_eid="5561",
            name="Present Test Event",
            start_date=datetime.datetime.today() - datetime.timedelta(days=2),
            year=datetime.datetime.now().year,
            venue_address="123 Fake Street, California, USA",
            website="http://www.google.com",
            webcast_json="""[{"type":"ustream","channel":"6540154"}]"""
        )
        event = EventManipulator.createOrUpdate(event)
        if not only_event:
            EventTeamTestCreator.createEventTeams(event)
            mtc = MatchTestCreator(event)
            mtc.createCompleteQuals()
            mtc.createIncompleteQuals()
        return event

    @classmethod
    def createPastEvent(self, only_event=False):
        event = Event(
            id="{}testpast".format(datetime.datetime.now().year),
            end_date=datetime.datetime.today() - datetime.timedelta(days=8),
            event_short="testpast",
            event_type_enum=EventType.REGIONAL,
            first_eid="5561",
            name="Past Test Event",
            start_date=datetime.datetime.today() - datetime.timedelta(days=12),
            year=datetime.datetime.now().year,
            venue_address="123 Fake Street, California, USA",
            website="http://www.google.com"
        )
        event = EventManipulator.createOrUpdate(event)
        if not only_event:
            EventTeamTestCreator.createEventTeams(event)
            mtc = MatchTestCreator(event)
            mtc.createCompleteQuals()
        return event

########NEW FILE########
__FILENAME__ = event_webcast_adder
import json

from helpers.event_manipulator import EventManipulator
from helpers.memcache.memcache_webcast_flusher import MemcacheWebcastFlusher


class EventWebcastAdder(object):

    @classmethod
    def add_webcast(cls, event, webcast):
        """Takes a webcast dictionary and adds it to an event"""

        if event.webcast:
            webcasts = event.webcast
            if webcast in webcasts:
                return event
            else:
                webcasts.append(webcast)
                event.webcast_json = json.dumps(webcasts)
        else:
            event.webcast_json = json.dumps([webcast])
        event.dirty = True
        EventManipulator.createOrUpdate(event)
        MemcacheWebcastFlusher.flushEvent(event.key_name)

        return event

########NEW FILE########
__FILENAME__ = event_helper
import logging
import collections
import datetime
import json
import re
import urllib

from google.appengine.api import urlfetch
from google.appengine.ext import ndb

from consts.district_type import DistrictType
from consts.event_type import EventType

from models.event import Event
from models.match import Match

CHAMPIONSHIP_EVENTS_LABEL = 'Championship Event'
REGIONAL_EVENTS_LABEL = 'Week {}'
WEEKLESS_EVENTS_LABEL = 'Other Official Events'
OFFSEASON_EVENTS_LABEL = 'Offseason'
PRESEASON_EVENTS_LABEL = 'Preseason'


class EventHelper(object):
    """
    Helper class for Events.
    """
    @classmethod
    def groupByWeek(self, events):
        """
        Events should already be ordered by start_date
        """
        to_return = collections.OrderedDict()  # key: week_label, value: list of events

        current_week = 1
        week_start = None
        weekless_events = []
        offseason_events = []
        preseason_events = []
        for event in events:
            if event.official and event.event_type_enum in {EventType.CMP_DIVISION, EventType.CMP_FINALS}:
                if CHAMPIONSHIP_EVENTS_LABEL in to_return:
                    to_return[CHAMPIONSHIP_EVENTS_LABEL].append(event)
                else:
                    to_return[CHAMPIONSHIP_EVENTS_LABEL] = [event]
            elif event.official and event.event_type_enum in {EventType.REGIONAL, EventType.DISTRICT, EventType.DISTRICT_CMP}:
                if (event.start_date is None or
                   (event.start_date.month == 12 and event.start_date.day == 31)):
                    weekless_events.append(event)
                else:
                    if week_start is None:
                        diff_from_thurs = (event.start_date.weekday() - 3) % 7  # 3 is Thursday
                        week_start = event.start_date - datetime.timedelta(days=diff_from_thurs)

                    if event.start_date >= week_start + datetime.timedelta(days=7):
                        current_week += 1
                        week_start += datetime.timedelta(days=7)

                    label = REGIONAL_EVENTS_LABEL.format(current_week)
                    if label in to_return:
                        to_return[label].append(event)
                    else:
                        to_return[label] = [event]
            elif event.event_type_enum == EventType.PRESEASON:
                preseason_events.append(event)
            else:
                # everything else is an offseason event
                offseason_events.append(event)

        # Add weekless + other events last
        if weekless_events:
            to_return[WEEKLESS_EVENTS_LABEL] = weekless_events
        if preseason_events:
            to_return[PRESEASON_EVENTS_LABEL] = preseason_events
        if offseason_events:
            to_return[OFFSEASON_EVENTS_LABEL] = offseason_events

        return to_return

    @classmethod
    def distantFutureIfNoStartDate(self, event):
        if not event.start_date:
            return datetime.datetime(2177, 1, 1, 1, 1, 1)
        else:
            return event.start_date

    @classmethod
    def distantFutureIfNoEndDate(self, event):
        if not event.end_date:
            return datetime.datetime(2177, 1, 1, 1, 1, 1)
        else:
            return event.end_date

    @classmethod
    def calculateTeamWLTFromMatches(self, team_key, matches):
        """
        Given a team_key and some matches, find the Win Loss Tie.
        """
        wlt = {"win": 0, "loss": 0, "tie": 0}

        for match in matches:
            if match.has_been_played and match.winning_alliance is not None:
                if match.winning_alliance == "":
                    wlt["tie"] += 1
                elif team_key in match.alliances[match.winning_alliance]["teams"]:
                    wlt["win"] += 1
                else:
                    wlt["loss"] += 1
        return wlt

    @classmethod
    def getTeamWLT(self, team_key, event):
        """
        Given a team_key, and an event, find the team's Win Loss Tie.
        """
        match_keys = Match.query(Match.event == event.key, Match.team_key_names == team_key).fetch(500, keys_only=True)
        return self.calculateTeamWLTFromMatches(team_key, ndb.get_multi(match_keys))

    @classmethod
    def getWeekEvents(self):
        """
        Get events this week
        In general, if an event is currently going on, it shows up in this query
        An event shows up in this query iff:
        a) The event is within_a_day
        OR
        b) The event.start_date is on or within 4 days after the closest Thursday
        """
        today = datetime.datetime.today()

        # Make sure all events to be returned are within range
        two_weeks_of_events_keys_future = Event.query().filter(
          Event.start_date >= (today - datetime.timedelta(days=7))).filter(
          Event.start_date <= (today + datetime.timedelta(days=7))).order(
          Event.start_date).fetch_async(50, keys_only=True)

        events = []
        diff_from_thurs = 3 - today.weekday()  # 3 is Thursday. diff_from_thurs ranges from 3 to -3 (Monday thru Sunday)
        closest_thursday = today + datetime.timedelta(days=diff_from_thurs)

        two_weeks_of_event_futures = ndb.get_multi_async(two_weeks_of_events_keys_future.get_result())
        for event_future in two_weeks_of_event_futures:
            event = event_future.get_result()
            if event.within_a_day:
                events.append(event)
            else:
                offset = event.start_date.date() - closest_thursday.date()
                if (offset == datetime.timedelta(0)) or (offset > datetime.timedelta(0) and offset < datetime.timedelta(4)):
                    events.append(event)

        EventHelper.sort_events(events)
        return events

    @classmethod
    def getEventsWithinADay(self):
        week_events = self.getWeekEvents()
        ret = []
        for event in week_events:
            if event.within_a_day:
                ret.append(event)
        return ret

    @classmethod
    def getShortName(self, name_str):
        match = re.match(r'(MAR |PNW )?(FIRST Robotics|FRC)?(.*)(FIRST Robotics|FRC)?(District|Regional|Region|State|Tournament|FRC|Field)( Competition| Event| Championship)?', name_str)
        if match:
            short = match.group(3)
            match = re.match(r'(.*)(FIRST Robotics|FRC)', short)
            if match:
                return match.group(1).strip()
            else:
                return short.strip()

        return name_str.strip()

    @classmethod
    def get_timezone_id(cls, event_dict):
        if event_dict.get('location', None) is None:
            logging.warning('Could not get timezone for event {}{} with no location!'.format(event_dict['year'], event_dict['event_short']))
            return None

        # geocode request
        geocode_params = urllib.urlencode({
            'address': event_dict['location'],
            'sensor': 'false',
        })
        geocode_url = 'https://maps.googleapis.com/maps/api/geocode/json?%s' % geocode_params
        try:
            geocode_result = urlfetch.fetch(geocode_url)
        except Exception, e:
            logging.warning('urlfetch for geocode request failed: {}'.format(geocode_url))
            logging.info(e)
            return None
        if geocode_result.status_code != 200:
            logging.warning('Geocoding for event {}{} failed with url {}'.format(event_dict['year'], event_dict['event_short'], geocode_url))
            return None
        geocode_dict = json.loads(geocode_result.content)
        if not geocode_dict['results']:
            logging.warning('No geocode results for event location: {}'.format(event_dict['location']))
            return None
        lat = geocode_dict['results'][0]['geometry']['location']['lat']
        lng = geocode_dict['results'][0]['geometry']['location']['lng']

        # timezone request
        tz_params = urllib.urlencode({
            'location': '%s,%s' % (lat, lng),
            'timestamp': 0,  # we only care about timeZoneId, which doesn't depend on timestamp
            'sensor': 'false',
        })
        tz_url = 'https://maps.googleapis.com/maps/api/timezone/json?%s' % tz_params
        try:
            tz_result = urlfetch.fetch(tz_url)
        except Exception, e:
            logging.warning('urlfetch for timezone request failed: {}'.format(tz_url))
            logging.info(e)
            return None
        if tz_result.status_code != 200:
            logging.warning('TZ lookup for (lat, lng) failed! ({}, {})'.format(lat, lng))
            return None
        tz_dict = json.loads(tz_result.content)
        if 'timeZoneId' not in tz_dict:
            logging.warning('No timeZoneId for (lat, lng)'.format(lat, lng))
            return None
        return tz_dict['timeZoneId']

    @classmethod
    def parseDistrictName(cls, district_name_str):
        return DistrictType.names.get(district_name_str, DistrictType.NO_DISTRICT)

    @classmethod
    def parseEventType(self, event_type_str):
        """
        Given an event_type_str from USFIRST, return the proper event type
        Examples:
        'Regional' -> EventType.REGIONAL
        'District' -> EventType.DISTRICT
        'District Championship' -> EventType.DISTRICT_CMP
        'MI FRC State Championship' -> EventType.DISTRICT_CMP
        'Championship Finals' -> EventType.CMP_FINALS
        'Championship' -> EventType.CMP_FINALS
        """
        event_type_str = event_type_str.lower()

        # Easy to parse
        if 'regional' in event_type_str:
            return EventType.REGIONAL
        elif 'offseason' in event_type_str:
            return EventType.OFFSEASON
        elif 'preseason' in event_type_str:
            return EventType.PRESEASON

        # Districts have multiple names
        if ('district' in event_type_str) or ('state' in event_type_str)\
           or ('region' in event_type_str) or ('qualif' in event_type_str):
            if 'championship' in event_type_str:
                return EventType.DISTRICT_CMP
            else:
                return EventType.DISTRICT

        # Everything else with 'champ' should be a Championship event
        if 'champ' in event_type_str:
            if 'division' in event_type_str:
                return EventType.CMP_DIVISION
            else:
                return EventType.CMP_FINALS

        # An event slipped through!
        logging.warn("Event type '{}' not recognized!".format(event_type_str))
        return EventType.UNLABLED

    @classmethod
    def sort_events(cls, events):
        """
        Sorts by start date then end date
        Sort is stable
        """
        events.sort(key=EventHelper.distantFutureIfNoStartDate)
        events.sort(key=EventHelper.distantFutureIfNoEndDate)

########NEW FILE########
__FILENAME__ = event_manipulator
from helpers.cache_clearer import CacheClearer
from helpers.manipulator_base import ManipulatorBase


class EventManipulator(ManipulatorBase):
    """
    Handle Event database writes.
    """
    @classmethod
    def clearCache(cls, affected_refs):
        CacheClearer.clear_event_and_references(affected_refs)

    @classmethod
    def updateMerge(self, new_event, old_event, auto_union=True):
        """
        Given an "old" and a "new" Team object, replace the fields in the
        "old" team that are present in the "new" team, but keep fields from
        the "old" team that are null in the "new" team.
        """
        attrs = [
            "alliance_selections_json",
            "end_date",
            "event_short",
            "event_type_enum",
            "event_district_enum",
            "facebook_eid",
            "first_eid",
            "location",
            "timezone_id",
            "name",
            "official",
            "matchstats_json",
            "rankings_json",
            "short_name",
            "start_date",
            "venue",
            "venue_address",
            "webcast_json",
            "website",
            "year"
        ]

        list_attrs = []

        for attr in attrs:
            # Special case for rankings. Don't merge bad data.
            if attr == 'rankings_json':
                if new_event.rankings and len(new_event.rankings) <= 1:
                    continue
            if getattr(new_event, attr) is not None:
                if getattr(new_event, attr) != getattr(old_event, attr):
                    setattr(old_event, attr, getattr(new_event, attr))
                    old_event.dirty = True
            if getattr(new_event, attr) == "None":
                if getattr(old_event, attr, None) != None:
                    setattr(old_event, attr, None)
                    old_event.dirty = True

        for attr in list_attrs:
            if len(getattr(new_event, attr)) > 0:
                if getattr(new_event, attr) != getattr(old_event, attr):
                    setattr(old_event, attr, getattr(new_event, attr))
                    old_event.dirty = True

        return old_event

########NEW FILE########
__FILENAME__ = event_team_test_creator
from helpers.event_team_manipulator import EventTeamManipulator
from models.event_team import EventTeam
from models.team import Team


class EventTeamTestCreator(object):
    @classmethod
    def createEventTeams(self, event):
        teams = Team.query().order(Team.team_number).fetch(60)

        event_teams = [EventTeam(
            id=event.key.id() + "_" + team.key.id(),
            event=event.key,
            team=team.key,
            year=event.year)
            for team in teams]
        return EventTeamManipulator.createOrUpdate(event_teams)

########NEW FILE########
__FILENAME__ = event_team_manipulator
from helpers.cache_clearer import CacheClearer
from helpers.manipulator_base import ManipulatorBase


class EventTeamManipulator(ManipulatorBase):
    """
    Handle EventTeam database writes.
    """
    @classmethod
    def clearCache(cls, affected_refs):
        CacheClearer.clear_eventteam_and_references(affected_refs)

    @classmethod
    def updateMerge(self, new_event_team, old_event_team, auto_union=True):
        """
        Update and return EventTeams.
        """
        immutable_attrs = [
            "event",
            "team",
        ]  # These build key_name, and cannot be changed without deleting the model.

        attrs = [
            "year",  # technically immutable, but corruptable and needs repair. See github issue #409
        ]

        for attr in attrs:
            if getattr(new_event_team, attr) is not None:
                if getattr(new_event_team, attr) != getattr(old_event_team, attr):
                    setattr(old_event_team, attr, getattr(new_event_team, attr))
                    old_event_team.dirty = True

        return old_event_team

########NEW FILE########
__FILENAME__ = event_team_repairer
from models.event_team import EventTeam


class EventTeamRepairer(object):
    """
    Repair corrupt EventTeam objects.
    """

    @classmethod
    def repair(self, event_teams):
        """
        Repair missing year attributes by rebuilding from Event key value.
        """
        new_event_teams = list()

        for event_team in event_teams:
            if event_team.year == None:
                # Note, y10k bug. -gregmarra
                new_event_teams.append(EventTeam(
                    event=event_team.event,
                    team=event_team.team,
                    year=int(event_team.event.id()[:4])))

        return new_event_teams

########NEW FILE########
__FILENAME__ = event_team_updater
import datetime

from google.appengine.ext import ndb

from models.award import Award
from models.event import Event
from models.event_team import EventTeam
from models.match import Match
from models.team import Team


class EventTeamUpdater(object):
    @classmethod
    def update(self, event_key):
        """
        Updates EventTeams for an event.
        Returns a tuple of (teams, event_teams, event_team_keys_to_delete)
        An EventTeam is valid iff the team:
        a) played a match at the event,
        b) the team received an award at the event,
        c) the event has not yet occurred,
        d) or the event is not from the current year. (This is to make sure we don't delete old data we may no longer be able to scrape)
        """
        event = Event.get_by_id(event_key)
        cur_year = datetime.datetime.now().year

        # Add teams from Matches and Awards
        team_ids = set()
        match_key_futures = Match.query(
            Match.event == event.key).fetch_async(1000, keys_only=True)
        award_key_futures = Award.query(
            Award.event == event.key).fetch_async(1000, keys_only=True)
        match_futures = ndb.get_multi_async(match_key_futures.get_result())
        award_futures = ndb.get_multi_async(award_key_futures.get_result())

        for match_future in match_futures:
            match = match_future.get_result()
            for team in match.team_key_names:
                team_ids.add(team)
        for award_future in award_futures:
            award = award_future.get_result()
            for team_key in award.team_list:
                team_ids.add(team_key.id())

        # Create or update EventTeams
        teams = [Team(id=team_id,
                      team_number=int(team_id[3:]))
                      for team_id in team_ids]

        if teams:
            event_teams = [EventTeam(id=event_key + "_" + team.key.id(),
                                     event=event.key,
                                     team=team.key,
                                     year=event.year)
                                     for team in teams]
        else:
            event_teams = None

        # Delete EventTeams for teams who did not participate in the event
        # Only runs if event is over
        existing_event_teams_keys = EventTeam.query(
            EventTeam.event == event.key).fetch(1000, keys_only=True)
        existing_event_teams = ndb.get_multi(existing_event_teams_keys)
        existing_team_ids = set()
        for et in existing_event_teams:
            existing_team_ids.add(et.team.id())

        et_keys_to_delete = set()
        if event.year == cur_year and event.end_date is not None and event.end_date < datetime.datetime.now():
            for team_id in existing_team_ids.difference(team_ids):
                et_key_name = "{}_{}".format(event.key_name, team_id)
                et_keys_to_delete.add(ndb.Key(EventTeam, et_key_name))

        return teams, event_teams, et_keys_to_delete

########NEW FILE########
__FILENAME__ = firebase_pusher
import json
import time

from google.appengine.api import taskqueue


class FirebasePusher(object):
    @classmethod
    def updated_event(self, event_key_name):
        """
        Pushes the timestamp at which the event was updated to Firebase.
        """
        taskqueue.add(url='/tasks/posts/firebase_push',
                      method='POST',
                      queue_name='firebase',
                      payload=json.dumps({'key': 'events/{}'.format(event_key_name),
                                          'data': int(time.time())}))

########NEW FILE########
__FILENAME__ = insights_helper
import json
import math

from collections import defaultdict

from google.appengine.ext import ndb

from consts.award_type import AwardType
from consts.event_type import EventType

from models.insight import Insight
from models.event import Event
from models.award import Award
from models.match import Match

from helpers.event_helper import EventHelper
from helpers.event_helper import OFFSEASON_EVENTS_LABEL


class InsightsHelper(object):
    """
    Helper for calculating insights and generating Insight objects
    """

    @classmethod
    def doMatchInsights(self, year):
        """
        Calculate match insights for a given year. Returns a list of Insights.
        """
        # Only fetch from DB once
        official_events = Event.query(Event.year == year).order(Event.start_date).fetch(1000)
        events_by_week = EventHelper.groupByWeek(official_events)
        week_event_matches = []  # Tuples of: (week, events) where events are tuples of (event, matches)
        for week, events in events_by_week.items():
            if week == OFFSEASON_EVENTS_LABEL:
                continue
            week_events = []
            for event in events:
                if not event.official:
                    continue
                matches = event.matches
                week_events.append((event, matches))
            week_event_matches.append((week, week_events))

        insights = []
        insights += self._calculateHighscoreMatchesByWeek(week_event_matches, year)
        insights += self._calculateHighscoreMatches(week_event_matches, year)
        insights += self._calculateMatchAveragesByWeek(week_event_matches, year)
        insights += self._calculateScoreDistribution(week_event_matches, year)
        insights += self._calculateNumMatches(week_event_matches, year)
        return insights

    @classmethod
    def doAwardInsights(self, year):
        """
        Calculate award insights for a given year. Returns a list of Insights.
        """
        # Get all Blue Banner, Division Finalist, and Championship Finalist awards
        blue_banner_award_keys_future = Award.query(
            Award.year == year,
            Award.award_type_enum.IN(AwardType.BLUE_BANNER_AWARDS)
        ).fetch_async(10000, keys_only=True)
        cmp_finalist_award_keys_future = Award.query(
            Award.year == year,
            Award.award_type_enum == AwardType.FINALIST,
            Award.event_type_enum.IN({EventType.CMP_DIVISION, EventType.CMP_FINALS})
        ).fetch_async(10000, keys_only=True)

        award_futures = ndb.get_multi_async(
            set(blue_banner_award_keys_future.get_result()).union(
            set(cmp_finalist_award_keys_future.get_result()))
        )

        insights = []
        insights += self._calculateBlueBanners(award_futures, year)
        insights += self._calculateChampionshipStats(award_futures, year)
        insights += self._calculateRegionalStats(award_futures, year)

        return insights

    @classmethod
    def _createInsight(self, data, name, year):
        """
        Create Insight object given data, name, and year
        """
        return Insight(id=Insight.renderKeyName(year, name),
                       name=name,
                       year=year,
                       data_json=json.dumps(data))

    @classmethod
    def _generateMatchData(self, match, event):
        """
        A dict of any data needed for front-end rendering
        """
        return {'key_name': match.key_name,
                'verbose_name': match.verbose_name,
                'event_name': event.name,
                'alliances': match.alliances,
                'winning_alliance': match.winning_alliance
                }

    @classmethod
    def _sortTeamWinsDict(self, wins_dict):
        """
        Sorts dicts with key: number of wins, value: list of teams
        by number of wins and by team number
        """
        wins_dict = sorted(wins_dict.items(), key=lambda pair: int(pair[0][3:]))  # Sort by team number
        temp = defaultdict(list)
        for team, numWins in wins_dict:
            temp[numWins].append(team)
        return sorted(temp.items(), key=lambda pair: int(pair[0]), reverse=True)  # Sort by win number

    @classmethod
    def _sortTeamList(self, team_list):
        """
        Sorts list of teams
        """
        return sorted(team_list, key=lambda team: int(team[3:]))  # Sort by team number

    @classmethod
    def _calculateHighscoreMatchesByWeek(self, week_event_matches, year):
        """
        Returns an Insight where the data is a list of tuples:
        (week string, list of highest scoring matches)
        """
        highscore_matches_by_week = []  # tuples: week, list of matches (if there are ties)
        for week, week_events in week_event_matches:
            week_highscore_matches = []
            highscore = 0
            for event, matches in week_events:
                for match in matches:
                    redScore = int(match.alliances['red']['score'])
                    blueScore = int(match.alliances['blue']['score'])
                    maxScore = max(redScore, blueScore)
                    if maxScore >= highscore:
                        if maxScore > highscore:
                            week_highscore_matches = []
                        week_highscore_matches.append(self._generateMatchData(match, event))
                        highscore = maxScore
            highscore_matches_by_week.append((week, week_highscore_matches))

        insight = None
        if highscore_matches_by_week != []:
            insight = self._createInsight(highscore_matches_by_week, Insight.INSIGHT_NAMES[Insight.MATCH_HIGHSCORE_BY_WEEK], year)
        if insight is not None:
            return [insight]
        else:
            return []

    @classmethod
    def _calculateHighscoreMatches(self, week_event_matches, year):
        """
        Returns an Insight where the data is list of highest scoring matches
        """
        highscore_matches = []  # list of matches (if there are ties)
        highscore = 0
        for _, week_events in week_event_matches:
            for event, matches in week_events:
                for match in matches:
                    redScore = int(match.alliances['red']['score'])
                    blueScore = int(match.alliances['blue']['score'])
                    maxScore = max(redScore, blueScore)
                    if maxScore >= highscore:
                        if maxScore > highscore:
                            highscore_matches = []
                        highscore_matches.append(self._generateMatchData(match, event))
                        highscore = maxScore

        insight = None
        if highscore_matches != []:
            insight = self._createInsight(highscore_matches, Insight.INSIGHT_NAMES[Insight.MATCH_HIGHSCORE], year)
        if insight is not None:
            return [insight]
        else:
            return []

    @classmethod
    def _calculateMatchAveragesByWeek(self, week_event_matches, year):
        """
        Returns a list of Insights, one for all data and one for elim data
        The data for each Insight is a list of tuples:
        (week string, match averages)
        """
        match_averages_by_week = []  # tuples: week, average score
        elim_match_averages_by_week = []  # tuples: week, average score
        for week, week_events in week_event_matches:
            week_match_sum = 0
            num_matches_by_week = 0
            elim_week_match_sum = 0
            elim_num_matches_by_week = 0
            for _, matches in week_events:
                for match in matches:
                    redScore = int(match.alliances['red']['score'])
                    blueScore = int(match.alliances['blue']['score'])
                    week_match_sum += redScore + blueScore
                    num_matches_by_week += 1
                    if match.comp_level in Match.ELIM_LEVELS:
                        elim_week_match_sum += redScore + blueScore
                        elim_num_matches_by_week += 1

            if num_matches_by_week != 0:
                week_average = float(week_match_sum) / num_matches_by_week / 2
                match_averages_by_week.append((week, week_average))

            if elim_num_matches_by_week != 0:
                elim_week_average = float(elim_week_match_sum) / elim_num_matches_by_week / 2
                elim_match_averages_by_week.append((week, elim_week_average))

        insights = []
        if match_averages_by_week != []:
            insights.append(self._createInsight(match_averages_by_week, Insight.INSIGHT_NAMES[Insight.MATCH_AVERAGES_BY_WEEK], year))
        if elim_match_averages_by_week != []:
            insights.append(self._createInsight(elim_match_averages_by_week, Insight.INSIGHT_NAMES[Insight.ELIM_MATCH_AVERAGES_BY_WEEK], year))
        return insights

    @classmethod
    def _calculateScoreDistribution(self, week_event_matches, year):
        """
        Returns a list of Insights, one for all data and one for elim data
        The data for each Insight is a dict:
        Key: Middle score of a bucketed range of scores, Value: % occurrence
        """
        score_distribution = defaultdict(int)
        elim_score_distribution = defaultdict(int)
        overall_highscore = 0
        for _, week_events in week_event_matches:
            for _, matches in week_events:
                for match in matches:
                    if not match.has_been_played: continue
                    redScore = int(match.alliances['red']['score'])
                    blueScore = int(match.alliances['blue']['score'])

                    overall_highscore = max(overall_highscore, redScore, blueScore)

                    score_distribution[redScore] += 1
                    score_distribution[blueScore] += 1

                    if match.comp_level in Match.ELIM_LEVELS:
                        elim_score_distribution[redScore] += 1
                        elim_score_distribution[blueScore] += 1

        insights = []
        if score_distribution != {}:
            binAmount = math.ceil(float(overall_highscore) / 20)
            totalCount = float(sum(score_distribution.values()))
            score_distribution_normalized = {}
            for score, amount in score_distribution.items():
                roundedScore = score - int(score % binAmount) + binAmount / 2 #Round off and then center in the bin
                contribution = float(amount) * 100 / totalCount
                if roundedScore in score_distribution_normalized:
                    score_distribution_normalized[roundedScore] += contribution
                else:
                    score_distribution_normalized[roundedScore] = contribution
            insights.append(self._createInsight(score_distribution_normalized, Insight.INSIGHT_NAMES[Insight.SCORE_DISTRIBUTION], year))
        if elim_score_distribution != {}:
            if binAmount is None:  # Use same binAmount from above if possible
                binAmount = math.ceil(float(overall_highscore) / 20)
            totalCount = float(sum(elim_score_distribution.values()))
            elim_score_distribution_normalized = {}
            for score, amount in elim_score_distribution.items():
                roundedScore = score - int(score % binAmount) + binAmount / 2
                contribution = float(amount) * 100 / totalCount
                if roundedScore in elim_score_distribution_normalized:
                    elim_score_distribution_normalized[roundedScore] += contribution
                else:
                    elim_score_distribution_normalized[roundedScore] = contribution
            insights.append(self._createInsight(elim_score_distribution_normalized, Insight.INSIGHT_NAMES[Insight.ELIM_SCORE_DISTRIBUTION], year))

        return insights

    @classmethod
    def _calculateNumMatches(self, week_event_matches, year):
        """
        Returns an Insight where the data is the number of matches
        """
        numMatches = 0
        for _, week_events in week_event_matches:
            for _, matches in week_events:
                numMatches += len(matches)

        insight = None
        if numMatches != 0:
            insight = self._createInsight(numMatches, Insight.INSIGHT_NAMES[Insight.NUM_MATCHES], year)
        if insight is not None:
            return [insight]
        else:
            return []

    @classmethod
    def _calculateBlueBanners(self, award_futures, year):
        """
        Returns an Insight where the data is a dict:
        Key: number of blue banners, Value: list of teams with that number of blue banners
        """
        blue_banner_winners = defaultdict(int)
        for award_future in award_futures:
            award = award_future.get_result()
            if award.award_type_enum in AwardType.BLUE_BANNER_AWARDS:
                for team_key in award.team_list:
                    team_key_name = team_key.id()
                    blue_banner_winners[team_key_name] += 1
        blue_banner_winners = self._sortTeamWinsDict(blue_banner_winners)

        insight = None
        if blue_banner_winners != []:
            insight = self._createInsight(blue_banner_winners, Insight.INSIGHT_NAMES[Insight.BLUE_BANNERS], year)
        if insight is not None:
            return [insight]
        else:
            return []

    @classmethod
    def _calculateChampionshipStats(self, award_futures, year):
        """
        Returns a list of Insights where, depending on the Insight, the data
        is either a team or a list of teams
        """
        ca_winner = None
        world_champions = []
        world_finalists = []
        division_winners = []
        division_finalists = []
        for award_future in award_futures:
            award = award_future.get_result()
            for team_key in award.team_list:
                team_key_name = team_key.id()
                if award.event_type_enum == EventType.CMP_FINALS:
                    if award.award_type_enum == AwardType.CHAIRMANS:
                        ca_winner = team_key_name
                    elif award.award_type_enum == AwardType.WINNER:
                        world_champions.append(team_key_name)
                    elif award.award_type_enum == AwardType.FINALIST:
                        world_finalists.append(team_key_name)
                elif award.event_type_enum == EventType.CMP_DIVISION:
                    if award.award_type_enum == AwardType.WINNER:
                        division_winners.append(team_key_name)
                    elif award.award_type_enum == AwardType.FINALIST:
                        division_finalists.append(team_key_name)

        world_champions = self._sortTeamList(world_champions)
        world_finalists = self._sortTeamList(world_finalists)
        division_winners = self._sortTeamList(division_winners)
        division_finalists = self._sortTeamList(division_finalists)

        insights = []
        if ca_winner is not None:
            insights += [self._createInsight(ca_winner, Insight.INSIGHT_NAMES[Insight.CA_WINNER], year)]
        if world_champions != []:
            insights += [self._createInsight(world_champions, Insight.INSIGHT_NAMES[Insight.WORLD_CHAMPIONS], year)]
        if world_finalists != []:
            insights += [self._createInsight(world_finalists, Insight.INSIGHT_NAMES[Insight.WORLD_FINALISTS], year)]
        if division_winners != []:
            insights += [self._createInsight(division_winners, Insight.INSIGHT_NAMES[Insight.DIVISION_WINNERS], year)]
        if division_finalists != []:
            insights += [self._createInsight(division_finalists, Insight.INSIGHT_NAMES[Insight.DIVISION_FINALISTS], year)]
        return insights

    @classmethod
    def _calculateRegionalStats(self, award_futures, year):
        """
        Returns a list of Insights where, depending on the Insight, the data
        is either a list of teams or a dict:
        Key: number of wins, Value: list of teams with that number of wins
        """
        rca_winners = []
        regional_winners = defaultdict(int)
        for award_future in award_futures:
            award = award_future.get_result()
            if award.event_type_enum in EventType.CMP_EVENT_TYPES:
                continue
            for team_key in award.team_list:
                team_key_name = team_key.id()
                if award.award_type_enum == AwardType.CHAIRMANS and award.event_type_enum in {EventType.REGIONAL, EventType.DISTRICT_CMP}:
                    # Only count Chairman's at regionals and district championships
                    rca_winners.append(team_key_name)
                elif award.award_type_enum == AwardType.WINNER:
                    regional_winners[team_key_name] += 1

        rca_winners = self._sortTeamList(rca_winners)
        regional_winners = self._sortTeamWinsDict(regional_winners)

        insights = []
        if rca_winners != []:
            insights += [self._createInsight(rca_winners, Insight.INSIGHT_NAMES[Insight.RCA_WINNERS], year)]
        if regional_winners != []:
            insights += [self._createInsight(regional_winners, Insight.INSIGHT_NAMES[Insight.REGIONAL_DISTRICT_WINNERS], year)]
        return insights

    @classmethod
    def doOverallMatchInsights(self):
        """
        Calculate match insights across all years. Returns a list of Insights.
        """
        insights = []

        year_num_matches = Insight.query(Insight.name == Insight.INSIGHT_NAMES[Insight.NUM_MATCHES], Insight.year != 0).fetch(1000)
        num_matches = []
        for insight in year_num_matches:
            num_matches.append((insight.year, insight.data))

        # Creating Insights
        if num_matches:
            insights.append(self._createInsight(num_matches, Insight.INSIGHT_NAMES[Insight.NUM_MATCHES], 0))

        return insights

    @classmethod
    def doOverallAwardInsights(self):
        """
        Calculate award insights across all years. Returns a list of Insights.
        """
        insights = []

        year_regional_winners = Insight.query(Insight.name == Insight.INSIGHT_NAMES[Insight.REGIONAL_DISTRICT_WINNERS], Insight.year != 0).fetch(1000)
        regional_winners = defaultdict(int)
        for insight in year_regional_winners:
            for number, team_list in insight.data:
                for team in team_list:
                    regional_winners[team] += number

        year_blue_banners = Insight.query(Insight.name == Insight.INSIGHT_NAMES[Insight.BLUE_BANNERS], Insight.year != 0).fetch(1000)
        blue_banners = defaultdict(int)
        for insight in year_blue_banners:
            for number, team_list in insight.data:
                for team in team_list:
                    blue_banners[team] += number

        year_rca_winners = Insight.query(Insight.name == Insight.INSIGHT_NAMES[Insight.RCA_WINNERS], Insight.year != 0).fetch(1000)
        rca_winners = defaultdict(int)
        for insight in year_rca_winners:
            for team in insight.data:
                rca_winners[team] += 1

        year_world_champions = Insight.query(Insight.name == Insight.INSIGHT_NAMES[Insight.WORLD_CHAMPIONS], Insight.year != 0).fetch(1000)
        world_champions = defaultdict(int)
        for insight in year_world_champions:
            for team in insight.data:
                world_champions[team] += 1

        # Sorting
        regional_winners = self._sortTeamWinsDict(regional_winners)
        blue_banners = self._sortTeamWinsDict(blue_banners)
        rca_winners = self._sortTeamWinsDict(rca_winners)
        world_champions = self._sortTeamWinsDict(world_champions)

        # Creating Insights
        if regional_winners:
            insights.append(self._createInsight(regional_winners, Insight.INSIGHT_NAMES[Insight.REGIONAL_DISTRICT_WINNERS], 0))

        if blue_banners:
            insights.append(self._createInsight(blue_banners, Insight.INSIGHT_NAMES[Insight.BLUE_BANNERS], 0))

        if rca_winners:
            insights.append(self._createInsight(rca_winners, Insight.INSIGHT_NAMES[Insight.RCA_WINNERS], 0))

        if world_champions:
            insights.append(self._createInsight(world_champions, Insight.INSIGHT_NAMES[Insight.WORLD_CHAMPIONS], 0))

        return insights

########NEW FILE########
__FILENAME__ = insight_manipulator
from helpers.manipulator_base import ManipulatorBase


class InsightManipulator(ManipulatorBase):
    """
    Handle Insight database writes.
    """

    @classmethod
    def updateMerge(self, new_insight, old_insight, auto_union=True):
        """
        Given an "old" and a "new" Insight object, replace the fields in the
        "old" Insight that are present in the "new" Insight, but keep fields from
        the "old" Insight that are null in the "new" insight.
        """
        attrs = [
            'name',
            'year',
            'data_json',
        ]

        for attr in attrs:
            if getattr(new_insight, attr) is not None:
                if getattr(new_insight, attr) != getattr(old_insight, attr):
                    setattr(old_insight, attr, getattr(new_insight, attr))
                    old_insight.dirty = True
            if getattr(new_insight, attr) == "None":
                if getattr(old_insight, attr, None) != None:
                    setattr(old_insight, attr, None)
                    old_insight.dirty = True

        return old_insight

########NEW FILE########
__FILENAME__ = manipulator_base
from google.appengine.ext import ndb
from helpers.cache_clearer import CacheClearer


class ManipulatorBase(object):
    """
    Provides a basic framework for manipulating db models.
    Ideally, all db read/writes would go through manipulators.
    """

    BATCH_SIZE = 500

    @classmethod
    def delete_keys(cls, model_keys):
        models = [model_key.get() for model_key in model_keys]
        cls.delete(models)

    @classmethod
    def delete(self, models):
        keys = [model.key for model in self.listify(models)]
        ndb.delete_multi(keys)
        for model in self.listify(models):
            if hasattr(model, '_affected_references'):
                self._computeAndSaveAffectedReferences(model)
                self.clearCache(model._affected_references)

    @classmethod
    def clearCache(cls, affected_refs):
        """
        Child classes should replace method with appropriate call to CacheClearer.
        """
        return

    @classmethod
    def listify(self, thing):
        if not isinstance(thing, list):
            return [thing]
        else:
            return thing

    @classmethod
    def delistify(self, things):
        if len(things) == 0:
            return None
        if len(things) == 1:
            return things.pop()
        else:
            return things

    @classmethod
    def _computeAndSaveAffectedReferences(cls, old_model, new_model=None):
        """
        This method is called whenever a model may potentially be created or updated.
        Stores the affected references in the original instance of the model.
        """
        if hasattr(old_model, '_affected_references'):
            for attr in old_model._affected_references.keys():
                for a in [old_model, new_model] if new_model is not None else [old_model]:
                    val = cls.listify(getattr(a, attr))
                    old_model._affected_references[attr] = old_model._affected_references[attr].union(val)

    @classmethod
    def createOrUpdate(self, new_models, auto_union=True):
        """
        Given a model or list of models, either insert them into the database, or update
        existing models with the same key.
        """
        models = self.listify(self.findOrSpawn(self.listify(new_models), auto_union=auto_union))
        models_to_put = [model for model in models if getattr(model, "dirty", False)]
        ndb.put_multi(models_to_put)
        for model in models:
            if hasattr(model, '_affected_references') and getattr(model, 'dirty', False):
                self.clearCache(model._affected_references)
        return self.delistify(models)

    @classmethod
    def findOrSpawn(self, new_models, auto_union=True):
        """"
        Check if a model or models currently exists in the database based on
        key_name. Doesn't put models.
        If it does, update it and give it back. If it does not, give it back.
        """
        new_models = self.listify(new_models)
        old_models = ndb.get_multi([ndb.Key(type(model).__name__, model.key_name) for model in new_models])
        new_models = [self.updateMergeBase(new_model, old_model, auto_union=auto_union) for (new_model, old_model) in zip(new_models, old_models)]
        return self.delistify(new_models)

    @classmethod
    def updateMergeBase(self, new_model, old_model, auto_union=True):
        """
        Given an "old" and a "new" model object, replace the fields in the
        "old" one that are present in the "new" one, but keep fields from
        the "old" one that are null or the empty list in the "new" one.
        """
        if old_model is None:
            new_model.dirty = True
            self._computeAndSaveAffectedReferences(new_model)
            return new_model

        self._computeAndSaveAffectedReferences(old_model, new_model)
        return self.updateMerge(new_model, old_model, auto_union=auto_union)

    @classmethod
    def updateMerge(self, new_model, old_model, auto_union=True):
        """
        Child classes should replace with method with specific merging logic
        """
        raise NotImplementedError("No updateMerge method!")

########NEW FILE########
__FILENAME__ = match_test_creator
import json
import random

from helpers.match_manipulator import MatchManipulator
from models.match import Match


class MatchTestCreator(object):

    def __init__(self, event):
        self.event = event
        self.event.prepTeams()

    def buildTestMatch(self, comp_level, set_number, match_number, complete):
        teams = random.sample(self.event.teams, 6)
        youtube_videos = []
        tba_videos = []
        if complete:
            red_score = random.randint(0, 100)
            blue_score = random.randint(0, 100)
            if random.choice([True, False]):
                youtube_videos.append("P3C2BOtL7e8")
            if random.choice([True, False]):
                youtube_videos.append("TqY324xLU4s")
            if random.choice([True, False]):
                tba_videos.append('mp4')
        else:
            red_score = -1
            blue_score = -1

        alliances = {
            "red": {
                "teams": [team.key_name for team in teams[:3]],
                "score": red_score
            },
            "blue": {
                "teams": [team.key_name for team in teams[3:]],
                "score": blue_score
            }
        }

        if comp_level == "qm":
            id_string = "{}_{}{}".format(
                self.event.key_name,
                comp_level,
                match_number)
        else:
            id_string = "{}_{}{}m{}".format(
                self.event.key_name,
                comp_level,
                set_number,
                match_number)

        return Match(
            id=id_string,
            alliances_json=json.dumps(alliances),
            comp_level=comp_level,
            event=self.event.key,
            game="frc_2012_rebr",
            set_number=set_number,
            match_number=match_number,
            team_key_names=[team.key_name for team in teams],
            youtube_videos=youtube_videos,
            tba_videos=tba_videos,
        )

    def createCompleteQuals(self):
        comp_level = "qm"
        set_number = 1
        complete = True
        matches = [self.buildTestMatch(comp_level, set_number, match_number, complete) for match_number in range(1, 11)]
        MatchManipulator.createOrUpdate(matches)

    def createIncompleteQuals(self):
        comp_level = "qm"
        set_number = 1
        complete = False
        matches = [self.buildTestMatch(comp_level, set_number, match_number, complete) for match_number in range(11, 21)]
        MatchManipulator.createOrUpdate(matches)

########NEW FILE########
__FILENAME__ = matchstats_helper
# Calculates OPR/DPR/CCWM
# For implementation details, see
# http://www.chiefdelphi.com/forums/showpost.php?p=484220&postcount=19

# M is n x n where n is # of teams
# s is n x 1 where n is # of teams
# solve [M][x]=[s] for [x]

# x is OPR and should be n x 1

import numpy as np


class MatchstatsHelper(object):
    @classmethod
    def calculate_matchstats(cls, matches):
        parsed_matches_by_type, team_list, team_id_map = cls._parse_matches(matches)

        oprs_dict = cls._calculate_stat(parsed_matches_by_type['opr'], team_list, team_id_map)
        dprs_dict = cls._calculate_stat(parsed_matches_by_type['dpr'], team_list, team_id_map)
        ccwms_dict = cls._calculate_stat(parsed_matches_by_type['ccwm'], team_list, team_id_map)

        return {'oprs': oprs_dict, 'dprs': dprs_dict, 'ccwms': ccwms_dict}

    @classmethod
    def _calculate_stat(cls, parsed_matches, team_list, team_id_map):
        """
        Returns: a dict where
        key: a string representing a team number (Example: "254", "254B", "1114")
        value: a float representing the stat (OPR/DPR/CCWM) for that team
        """
        n = len(team_list)
        M = np.zeros([n, n])
        s = np.zeros([n, 1])

        # Constructing M and s
        for teams, score in parsed_matches:
            for team1 in teams:
                team1_id = team_id_map[team1]
                for team2 in teams:
                    M[team1_id, team_id_map[team2]] += 1
                s[team1_id] += score

        # Solving M*x = s for x
        try:
            x = np.linalg.solve(M, s)
        except (np.linalg.LinAlgError, ValueError):
            return {}

        stat_dict = {}
        for team, stat in zip(team_list, x):
            stat_dict[team] = stat[0]

        return stat_dict

    @classmethod
    def _parse_matches(cls, matches):
        """
        Returns:
        parsed_matches_by_type: list of matches as the tuple ([team, team, team], <score/opposingscore/scoremargin>) for each key ('opr', 'dpr', 'ccwm')
        team_list: list of strings representing team numbers. Example: "254", "254B", "1114"
        team_id_map: dict that maps a team to a unique integer from 0 to n-1
        """
        team_list = set()
        parsed_matches_by_type = {
            'opr': [],
            'dpr': [],
            'ccwm': [],
        }
        for match in matches:
            if not match.has_been_played or match.comp_level != 'qm':  # only calculate OPRs for played quals matches
                continue
            alliances = match.alliances
            for alliance_color, opposing_color in zip(['red', 'blue'], ['blue', 'red']):
                match_team_list = []
                for team in alliances[alliance_color]['teams']:
                    team = team[3:]  # turns "frc254B" into "254B"
                    team_list.add(team)
                    match_team_list.append(team)

                alliance_score = int(alliances[alliance_color]['score'])
                opposing_score = int(alliances[opposing_color]['score'])
                parsed_matches_by_type['opr'].append((match_team_list, alliance_score))
                parsed_matches_by_type['dpr'].append((match_team_list, opposing_score))
                parsed_matches_by_type['ccwm'].append((match_team_list, alliance_score - opposing_score))

        team_list = list(team_list)

        team_id_map = {}
        for i, team in enumerate(team_list):
            team_id_map[team] = i

        return parsed_matches_by_type, team_list, team_id_map

########NEW FILE########
__FILENAME__ = match_helper
import logging
import datetime
import pytz
import re

from collections import defaultdict

from helpers.match_manipulator import MatchManipulator

from models.match import Match


class MatchHelper(object):
    @classmethod
    def add_match_times(cls, event, matches):
        """
        Calculates and adds match times given an event and match time strings (from USFIRST)
        Assumes the last match is played on the last day of comeptition and
        works backwards from there.
        """
        if event.timezone_id is None:  # Can only calculate match times if event timezone is known
            logging.warning('Cannot compute match time for event with no timezone_id: {}'.format(event.key_name))
            return

        matches_reversed = cls.play_order_sort_matches(matches, reverse=True)
        tz = pytz.timezone(event.timezone_id)

        last_match_time = None
        cur_date = event.end_date + datetime.timedelta(hours=23, minutes=59, seconds=59)  # end_date is specified at midnight of the last day
        for match in matches_reversed:
            r = re.match(r'(\d+):(\d+) (am|pm)', match.time_string.lower())
            hour = int(r.group(1))
            minute = int(r.group(2))
            if hour == 12:
                hour = 0
            if r.group(3) == 'pm':
                hour += 12

            match_time = datetime.datetime(cur_date.year, cur_date.month, cur_date.day, hour, minute)
            if last_match_time is not None and last_match_time + datetime.timedelta(hours=6) < match_time:
                cur_date = cur_date - datetime.timedelta(days=1)
                match_time = datetime.datetime(cur_date.year, cur_date.month, cur_date.day, hour, minute)
            last_match_time = match_time

            match.time = match_time - tz.utcoffset(match_time)

    """
    Helper to put matches into sub-dictionaries for the way we render match tables
    """
    # Allows us to sort matches by key name.
    # Note: Matches within a comp_level (qual, qf, sf, f, etc.) will be in order,
    # but the comp levels themselves may not be in order. Doesn't matter because
    # XXX_match_table.html checks for comp_level when rendering the page
    @classmethod
    def natural_sort_matches(self, matches):
        import re
        convert = lambda text: int(text) if text.isdigit() else text.lower()
        alphanum_key = lambda match: [convert(c) for c in re.split('([0-9]+)', str(match.key_name))]
        return sorted(matches, key=alphanum_key)

    @classmethod
    def play_order_sort_matches(self, matches, reverse=False):
        sort_key = lambda match: match.play_order
        return sorted(matches, key=sort_key, reverse=reverse)

    @classmethod
    def organizeMatches(self, match_list):
        match_list = MatchHelper.natural_sort_matches(match_list)
        matches = dict([(comp_level, list()) for comp_level in Match.COMP_LEVELS])
        matches["num"] = len(match_list)
        while len(match_list) > 0:
            match = match_list.pop(0)
            matches[match.comp_level].append(match)

        return matches

    @classmethod
    def recentMatches(self, matches, num=3):
        def cmp_matches(x, y):
            if x.updated is None:
                return False
            if y.updated is None:
                return True
            else:
                cmp(x.updated, y.updated)

        matches = filter(lambda x: x.has_been_played, matches)
        matches = MatchHelper.organizeMatches(matches)

        all_matches = []
        for comp_level in Match.COMP_LEVELS:
            if comp_level in matches:
                play_order_sorted = self.play_order_sort_matches(matches[comp_level])
                all_matches += play_order_sorted
        return all_matches[-num:]

    @classmethod
    def upcomingMatches(self, matches, num=3):
        matches = filter(lambda x: not x.has_been_played, matches)
        matches = MatchHelper.organizeMatches(matches)

        unplayed_matches = []
        for comp_level in Match.COMP_LEVELS:
            if comp_level in matches:
                play_order_sorted = self.play_order_sort_matches(matches[comp_level])
                for match in play_order_sorted:
                    unplayed_matches.append(match)
        return unplayed_matches[:num]

    @classmethod
    def deleteInvalidMatches(self, match_list):
        """
        A match is invalid iff it is an elim match where the match number is 3
        and the same alliance won in match numbers 1 and 2 of the same set.
        """
        matches_by_key = {}
        for match in match_list:
            matches_by_key[match.key_name] = match

        return_list = []
        for match in match_list:
            if match.comp_level in Match.ELIM_LEVELS and match.match_number == 3 and (not match.has_been_played):
                match_1 = matches_by_key.get(Match.renderKeyName(match.event.id(), match.comp_level, match.set_number, 1))
                match_2 = matches_by_key.get(Match.renderKeyName(match.event.id(), match.comp_level, match.set_number, 2))
                if match_1 != None and match_2 != None and\
                    match_1.has_been_played and match_2.has_been_played and\
                    match_1.winning_alliance == match_2.winning_alliance:
                        try:
                            MatchManipulator.delete(match)
                            logging.warning("Deleting invalid match: %s" % match.key_name)
                        except:
                            logging.warning("Tried to delete invalid match, but failed: %s" % match.key_name)
                        continue
            return_list.append(match)
        return return_list

    @classmethod
    def generateBracket(cls, matches, alliance_selections=None):
        complete_alliances = []
        bracket_table = defaultdict(dict)
        for comp_level in ['qf', 'sf', 'f']:
            for match in matches[comp_level]:
                set_number = str(match.set_number)  # for template to work
                if set_number not in bracket_table[comp_level]:
                    bracket_table[comp_level][set_number] = {
                        'red_alliance': [],
                        'blue_alliance': [],
                        'winning_alliance': None,
                        'red_wins': 0,
                        'blue_wins': 0,
                    }
                for color in ['red', 'blue']:
                    alliance = match.alliances[color]['teams']
                    for i, complete_alliance in enumerate(complete_alliances):  # search for alliance. could be more efficient
                        if len(set(alliance).intersection(set(complete_alliance))) >= 2:  # if >= 2 teams are the same, then the alliance is the same
                            backups = list(set(alliance).difference(set(complete_alliance)))
                            complete_alliances[i] += backups  # ensures that backup robots are listed last

                            for team_num in cls.getOrderedAlliance(complete_alliances[i], alliance_selections):
                                if team_num not in bracket_table[comp_level][set_number]['{}_alliance'.format(color)]:
                                    bracket_table[comp_level][set_number]['{}_alliance'.format(color)].append(team_num)

                            break
                    else:
                        complete_alliances.append(alliance)

                winner = match.winning_alliance
                if not winner or winner == '':
                    # if the match is a tie
                    continue

                bracket_table[comp_level][set_number]['{}_wins'.format(winner)] = \
                    bracket_table[comp_level][set_number]['{}_wins'.format(winner)] + 1
                if bracket_table[comp_level][set_number]['red_wins'] == 2:
                    bracket_table[comp_level][set_number]['winning_alliance'] = 'red'
                if bracket_table[comp_level][set_number]['blue_wins'] == 2:
                    bracket_table[comp_level][set_number]['winning_alliance'] = 'blue'


        return bracket_table

    @classmethod
    def getOrderedAlliance(cls, team_keys, alliance_selections):
        if alliance_selections:
            for alliance_selection in alliance_selections:  # search for alliance. could be more efficient
                picks = alliance_selection['picks']
                if len(set(picks).intersection(set(team_keys))) >= 2:  # if >= 2 teams are the same, then the alliance is the same
                    backups = list(set(team_keys).difference(set(picks)))
                    team_keys = picks + backups
                    break

        team_nums = []
        for team in team_keys:
            # Strip the "frc" prefix
            team_nums.append(team[3:])
        return team_nums

########NEW FILE########
__FILENAME__ = match_manipulator
import json

from google.appengine.ext import ndb

from helpers.cache_clearer import CacheClearer
from helpers.manipulator_base import ManipulatorBase


class MatchManipulator(ManipulatorBase):
    """
    Handle Match database writes.
    """
    @classmethod
    def clearCache(cls, affected_refs):
        CacheClearer.clear_match_and_references(affected_refs)

    @classmethod
    def updateMerge(self, new_match, old_match, auto_union=True):
        """
        Given an "old" and a "new" Match object, replace the fields in the
        "old" team that are present in the "new" team, but keep fields from
        the "old" team that are null in the "new" team.
        """
        immutable_attrs = [
            "comp_level",
            "event",
            "set_number",
            "match_number",
        ]  # These build key_name, and cannot be changed without deleting the model.

        attrs = [
            "game",
            "no_auto_update",
            "time",
            "time_string",
        ]

        json_attrs = [
            "alliances_json",
        ]

        list_attrs = [
            "team_key_names"
        ]

        auto_union_attrs = [
            "tba_videos",
            "youtube_videos"
        ]

        # if not auto_union, treat auto_union_attrs as list_attrs
        if not auto_union:
            list_attrs += auto_union_attrs
            auto_union_attrs = []

        for attr in attrs:
            if getattr(new_match, attr) is not None:
                if getattr(new_match, attr) != getattr(old_match, attr):
                    setattr(old_match, attr, getattr(new_match, attr))
                    old_match.dirty = True

        for attr in json_attrs:
            if getattr(new_match, attr) is not None:
                if json.loads(getattr(new_match, attr)) != json.loads(getattr(old_match, attr)):
                    setattr(old_match, attr, getattr(new_match, attr))
                    # changinging 'attr_json' doesn't clear lazy-loaded '_attr'
                    setattr(old_match, '_{}'.format(attr.replace('_json', '')), None)
                    old_match.dirty = True

        for attr in list_attrs:
            if len(getattr(new_match, attr)) > 0:
                if getattr(new_match, attr) != getattr(old_match, attr):
                    setattr(old_match, attr, getattr(new_match, attr))
                    old_match.dirty = True

        for attr in auto_union_attrs:
            old_set = set(getattr(old_match, attr))
            new_set = set(getattr(new_match, attr))
            unioned = old_set.union(new_set)
            if unioned != old_set:
                setattr(old_match, attr, list(unioned))
                old_match.dirty = True

        return old_match

########NEW FILE########
__FILENAME__ = media_helper
import json
import logging
import re

from google.appengine.api import urlfetch

from BeautifulSoup import BeautifulSoup
from consts.media_type import MediaType


class MediaHelper(object):
    @classmethod
    def group_by_slugname(cls, medias):
        medias_by_slugname = {}
        for media in medias:
            slugname = media.slug_name
            if slugname in medias_by_slugname:
                medias_by_slugname[slugname].append(media)
            else:
                medias_by_slugname[slugname] = [media]
        return medias_by_slugname


class MediaParser(object):
    CD_PHOTO_THREAD_URL_PATTERNS = ['chiefdelphi.com/media/photos/']
    YOUTUBE_URL_PATTERNS = ['youtube.com', 'youtu.be']

    @classmethod
    def partial_media_dict_from_url(cls, url):
        """
        Takes a url, and turns it into a partial Media object dict
        """
        if any(s in url for s in cls.CD_PHOTO_THREAD_URL_PATTERNS):
            return cls._partial_media_dict_from_cd_photo_thread(url)
        elif any(s in url for s in cls.YOUTUBE_URL_PATTERNS):
            return cls._partial_media_dict_from_youtube(url)
        else:
            logging.warning("Failed to determine media type from url: {}".format(url))
            return None

    @classmethod
    def _partial_media_dict_from_cd_photo_thread(cls, url):
        media_dict = {}
        media_dict['media_type_enum'] = MediaType.CD_PHOTO_THREAD
        foreign_key = cls._parse_cdphotothread_foreign_key(url)
        if foreign_key is None:
            logging.warning("Failed to determine foreign_key from url: {}".format(url))
            return None
        media_dict['foreign_key'] = foreign_key

        urlfetch_result = urlfetch.fetch(url, deadline=10)
        if urlfetch_result.status_code != 200:
            logging.warning('Unable to retrieve url: {}'.format(url))
            return None

        image_partial = cls._parse_cdphotothread_image_partial(urlfetch_result.content)
        if image_partial is None:
            logging.warning("Failed to determine image_partial from the page: {}".format(url))
            return None
        media_dict['details_json'] = json.dumps({'image_partial': image_partial})

        return media_dict

    @classmethod
    def _partial_media_dict_from_youtube(cls, url):
        media_dict = {}
        media_dict['media_type_enum'] = MediaType.YOUTUBE
        foreign_key = cls._parse_youtube_foreign_key(url)
        if foreign_key is None:
            logging.warning("Failed to determine foreign_key from url: {}".format(url))
            return None
        media_dict['foreign_key'] = foreign_key

        return media_dict

    @classmethod
    def _parse_cdphotothread_foreign_key(cls, url):
        regex1 = re.match(r'.*chiefdelphi.com\/media\/photos\/(\d+)', url)
        if regex1 is not None:
            return regex1.group(1)
        else:
            return None

    @classmethod
    def _parse_cdphotothread_image_partial(cls, html):
        """
        Input: the HTML from the thread page
        ex: http://www.chiefdelphi.com/media/photos/38464,

        returns the url of the image in the thread
        ex: http://www.chiefdelphi.com/media/img/3f5/3f5db241521ae5f2636ff8460f277997_l.jpg
        """
        # parse html for the image url
        soup = BeautifulSoup(html,
                             convertEntities=BeautifulSoup.HTML_ENTITIES)
        element = soup.find('a', {'target': 'cdmLargePic'})
        if element is not None:
            partial_url = element['href']
        else:
            return None

        # partial_url looks something like: "/media/img/774/774d98c80dcf656f2431b2e9186f161a_l.jpg"
        # we want "774/774d98c80dcf656f2431b2e9186f161a_l.jpg"
        image_partial = re.match(r'\/media\/img\/(.*)', partial_url)
        if image_partial is not None:
            return image_partial.group(1)
        else:
            return None

    @classmethod
    def _parse_youtube_foreign_key(cls, url):
        youtube_id = None
        regex1 = re.match(r".*youtu\.be\/(.*)", url)
        if regex1 is not None:
            youtube_id = regex1.group(1)
        else:
            regex2 = re.match(r".*v=([a-zA-Z0-9_-]*)", url)
            if regex2 is not None:
                youtube_id = regex2.group(1)

        if youtube_id is None:
            return None
        else:
            return youtube_id

########NEW FILE########
__FILENAME__ = media_manipulator
from helpers.manipulator_base import ManipulatorBase


class MediaManipulator(ManipulatorBase):
    """
    Handle Media database writes.
    """

    @classmethod
    def updateMerge(self, new_media, old_media, auto_union=True):
        """
        Given an "old" and a "new" Media object, replace the fields in the
        "old" object that are present in the "new" object, but keep fields from
        the "old" object that are null in the "new" object.
        Special case: References (list of Keys) are merged, not overwritten
        """
        attrs = [
            'media_type_enum',
            'foreign_key',
            'details_json',
            'year',
        ]

        list_attrs = []

        auto_union_attrs = [
            'references',
        ]

        # if not auto_union, treat auto_union_attrs as list_attrs
        if not auto_union:
            list_attrs += auto_union_attrs
            auto_union_attrs = []

        for attr in attrs:
            if getattr(new_media, attr) is not None:
                if getattr(new_media, attr) != getattr(old_media, attr):
                    setattr(old_media, attr, getattr(new_media, attr))
                    old_media.dirty = True

        for attr in list_attrs:
            if len(getattr(new_media, attr)) > 0:
                if getattr(new_media, attr) != getattr(old_media, attr):
                    setattr(old_media, attr, getattr(new_media, attr))
                    old_media.dirty = True

        for attr in auto_union_attrs:
            old_set = set(getattr(old_media, attr))
            new_set = set(getattr(new_media, attr))
            unioned = old_set.union(new_set)
            if unioned != old_set:
                setattr(old_media, attr, list(unioned))
                old_media.dirty = True

        return old_media

########NEW FILE########
__FILENAME__ = memcache_webcast_flusher
from controllers.ajax_controller import WebcastHandler
from controllers.event_controller import EventList
from controllers.main_controller import MainChampsHandler, MainCompetitionseasonHandler, MainOffseasonHandler, MainInsightsHandler, GamedayHandler, WebcastsHandler


class MemcacheWebcastFlusher(object):
    @classmethod
    def flush(self):
        flushed = []

        flushed.append(MainChampsHandler().memcacheFlush())
        flushed.append(MainCompetitionseasonHandler().memcacheFlush())
        flushed.append(MainOffseasonHandler().memcacheFlush())
        flushed.append(MainInsightsHandler().memcacheFlush())
        flushed.append(GamedayHandler().memcacheFlush())
        flushed.append(WebcastsHandler().memcacheFlush())
        flushed.append(EventList().memcacheFlush())

        return flushed

    @classmethod
    def flushEvent(self, event_key):
        flushed = self.flush()
        flushed.append(WebcastHandler().memcacheFlush(event_key))
        return flushed

########NEW FILE########
__FILENAME__ = model_to_dict
import logging


class ModelToDict(object):

    @classmethod
    def teamConverter(self, team):
        """
        return top level team dictionary
        """
        team_dict = dict()
        team_dict["key"] = team.key_name
        team_dict["team_number"] = team.team_number
        team_dict["name"] = team.name
        team_dict["nickname"] = team.nickname
        team_dict["website"] = team.website
        team_dict["location"] = team.location

        try:
            team_dict["location"] = team.location
            team_dict["locality"] = team.locality
            team_dict["region"] = team.region
            team_dict["country_name"] = team.country_name
        except Exception, e:
            logging.warning("Failed to include Address for api_team_info_%s: %s" % (team.key.id(), e))

        return team_dict

    @classmethod
    def eventConverter(self, event):
        """
        return top level event dictionary
        """
        event_dict = dict()
        event_dict["key"] = event.key_name
        event_dict["name"] = event.name
        event_dict["short_name"] = event.short_name
        event_dict["event_code"] = event.event_short
        event_dict["event_type_string"] = event.event_type_str
        event_dict["event_type"] = event.event_type_enum
        event_dict["year"] = event.year
        event_dict["location"] = event.location
        event_dict["official"] = event.official
        event_dict["facebook_eid"] = event.facebook_eid

        if event.start_date:
            event_dict["start_date"] = event.start_date.date().isoformat()
        else:
            event_dict["start_date"] = None
        if event.end_date:
            event_dict["end_date"] = event.end_date.date().isoformat()
        else:
            event_dict["end_date"] = None

        return event_dict

    @classmethod
    def matchConverter(self, match):
        """
        return top level match dictionary
        """
        match_dict = dict()
        match_dict["key"] = match.key_name
        match_dict["event_key"] = match.event.id()
        match_dict["alliances"] = match.alliances
        match_dict["comp_level"] = match.comp_level
        match_dict["match_number"] = match.match_number
        match_dict["set_number"] = match.set_number
        match_dict["videos"] = match.videos
        match_dict["time_string"] = match.time_string
        if match.time is not None:
            match_dict["time"] = match.time.strftime("%s")
        else:
            match_dict["time"] = None

        return match_dict

    @classmethod
    def awardConverter(self, award):
        """
        return top level award dictionary
        """
        award_dict = dict()
        award_dict["name"] = award.name_str
        award_dict["year"] = award.year
        award_dict["event_key"] = award.event.id()
        award_dict["recipient_list"] = award.recipient_list

        return award_dict

########NEW FILE########
__FILENAME__ = match_suggestion_accepter
from helpers.match_manipulator import MatchManipulator
from models.match import Match


class MatchSuggestionAccepter(object):
    """
    Handle accepting Match suggestions.
    """

    @classmethod
    def accept_suggestions(self, suggestions):
        if (len(suggestions) < 1):
            return None

        matches = map(lambda match_future: match_future.get_result(),
                      [Match.get_by_id_async(suggestion.target_key) for suggestion in suggestions])

        pairs = zip(matches, suggestions)

        for match, suggestion in pairs:
            self._accept_suggestion(match, suggestion)

        matches, suggestions = zip(*pairs)

        matches = MatchManipulator.createOrUpdate(list(matches))

        return matches

    @classmethod
    def _accept_suggestion(self, match, suggestion):
        if "youtube_videos" in suggestion.contents:
            match = self._merge_youtube_videos(match, suggestion.contents["youtube_videos"])

        return match

    @classmethod
    def _merge_youtube_videos(self, match, youtube_videos):
        for youtube_video in youtube_videos:
            if youtube_video not in match.youtube_videos:
                match.youtube_videos.append(youtube_video)
                match.dirty = True  # This is so hacky. -gregmarra 20130601

        return match

########NEW FILE########
__FILENAME__ = tbavideo_helper
class TBAVideoHelper(object):
    """
    Same interface as the retired TBAVideo class.
    """
    TBA_NET_VID_PATTERN = "http://videos.thebluealliance.net/%s/%s.%s"

    THUMBNAIL_FILETYPES = ["jpg", "jpeg"]
    STREAMABLE_FILETYPES = ["mp4", "flv"]
    DOWNLOADABLE_FILETYPES = ["mp4", "mov", "avi", "wmv", "flv"]

    def __init__(self, match):
        self.match = match

    @property
    def thumbnail_path(self):
        return self._best_path_of(self.THUMBNAIL_FILETYPES)

    @property
    def streamable_path(self):
        return self._best_path_of(self.STREAMABLE_FILETYPES)

    @property
    def downloadable_path(self):
        return self._best_path_of(self.DOWNLOADABLE_FILETYPES)

    def _best_path_of(self, consider_filetypes):
        for filetype in consider_filetypes:
            if filetype in self.match.tba_videos:
                return self.TBA_NET_VID_PATTERN % (self.match.event_key_name, self.match.key_name, filetype)
        return None

########NEW FILE########
__FILENAME__ = team_helper
import re
import logging

from google.appengine.api import urlfetch

from helpers.team_manipulator import TeamManipulator
from models.team import Team


class TeamHelper(object):
    """
    Helper to sort teams and stuff
    """
    @classmethod
    def sortTeams(self, team_list):
        """
        Takes a list of Teams (not a Query object).
        """
        # Sometimes there are None objects in the list.
        team_list = filter(None, team_list)
        team_list = sorted(team_list, key=lambda team: team.team_number)

        return team_list


class TeamTpidHelper(object):

    # Separates tpids on the FIRST list of all teams.
    teamRe = re.compile(r'tpid=[A-Za-z0-9=&;\-:]*?"><b>\d+')
    # Extracts the team number from the team result.
    teamNumberRe = re.compile(r'\d+$')
    # Extracts the tpid from the team result.
    tpidRe = re.compile(r'\d+')
    # Extracts the link to the next page of results on the FIRST list of all teams.
    lastPageRe = re.compile(r'Next ->')

    TPID_URL_PATTERN = "https://my.usfirst.org/myarea/index.lasso?page=searchresults&programs=FRC&reports=teams&sort_teams=number&results_size=250&omit_searchform=1&season_FRC=%s&skip_teams=%s"

    @classmethod
    def scrapeTpids(self, skip, year):
      """
      Searches the FIRST list of all teams for tpids, writing in the datastore.
      Also creates new Team objects.

      This code is modified from Pat Fairbank's frclinks source and modified
      to fit in the TBA framework. He has given us permission to borrow
      his code.
      """
      while 1:
        logging.info("Fetching 250 teams based on %s data, skipping %s" % (year, skip))

        tpids_dict = dict()

        # FIRST is now checking the 'Referer' header for the string 'usfirst.org'.
        # See https://github.com/patfair/frclinks/commit/051bf91d23ca0242dad5b1e471f78468173f597f
        teamList = urlfetch.fetch(self.TPID_URL_PATTERN % (year, skip), headers={'Referrer': 'usfirst.org'}, deadline=10)
        teamResults = self.teamRe.findall(teamList.content)

        for teamResult in teamResults:
          teamNumber = self.teamNumberRe.findall(teamResult)[0]
          teamTpid = self.tpidRe.findall(teamResult)[0]

          logging.info("Team %s TPID was %s in year %s." % (teamNumber, teamTpid, year))
          tpids_dict[teamNumber] = teamTpid

        teams = [Team(
              team_number=int(team_number),
              first_tpid=int(tpids_dict[team_number]),
              first_tpid_year=int(year),
              id="frc" + str(team_number)
            )
        for team_number in tpids_dict]

        TeamManipulator.createOrUpdate(teams)
        skip = int(skip) + 250

        # Handle degenerate cases.
        if skip > 10000:
          return None

        if len(self.lastPageRe.findall(teamList.content)) == 0:
          return None

########NEW FILE########
__FILENAME__ = team_manipulator
from helpers.cache_clearer import CacheClearer
from helpers.manipulator_base import ManipulatorBase


class TeamManipulator(ManipulatorBase):
    """
    Handle Team database writes.
    """
    @classmethod
    def clearCache(cls, affected_refs):
        CacheClearer.clear_team_and_references(affected_refs)

    @classmethod
    def updateMerge(self, new_team, old_team, auto_union=True):
        """
        Given an "old" and a "new" Team object, replace the fields in the
        "old" team that are present in the "new" team, but keep fields from
        the "old" team that are null in the "new" team.
        """
        attrs = [
            "address",
            "name",
            "nickname",
            "website",
            "rookie_year",
        ]

        for attr in attrs:
            if getattr(new_team, attr) is not None:
                if getattr(new_team, attr) != getattr(old_team, attr):
                    setattr(old_team, attr, getattr(new_team, attr))
                    old_team.dirty = True

        # Take the new tpid and tpid_year iff the year is newer than the old one
        if (new_team.first_tpid_year > old_team.first_tpid_year):
            old_team.first_tpid_year = new_team.first_tpid_year
            old_team.first_tpid = new_team.first_tpid
            old_team.dirty = True

        return old_team

########NEW FILE########
__FILENAME__ = typeahead_helper
import unicodedata
import re


class TypeaheadHelper(object):
    @classmethod
    def get_search_keys(self, name):
        """
        search keys for a name are generally the first letter of each word
        in the name, with a few exceptions. Also, if the a letter is a
        unicode character like \xfc, it gets changed to "u"
        """
        keys = set()
        name = unicodedata.normalize('NFKD', unicode(name)).strip().lower()
        for word in name.split(' '):
            for letter in word:
                if letter != '':
                    keys.add(letter)
                if re.match('^[\w-]+$', letter) is not None:
                    break  # continue until first alphanumeric character
        return keys

########NEW FILE########
__FILENAME__ = user_bundle
from google.appengine.api import users

from models.account import Account


class UserBundle(object):
    """
    UserBundle encapsulates a bunch of Google AppEngine user management stuff
    to make it easier for templates.
    """
    def __init__(self):
        self._account = None

    @property
    def account(self):
        if self._account is None:
            self._account = Account.get_or_insert(
                self.user.user_id(),
                email = self.user.email(),
                nickname = self.user.nickname(),
                registered = False,
                display_name = self.user.nickname())
        return self._account

    @property
    def user(self):
        return users.get_current_user()

    @property
    def is_current_user_admin(self):
        return users.is_current_user_admin()

    @property
    def login_url(self):
        return users.create_login_url("/")

    @property
    def logout_url(self):
        return users.create_logout_url("/")

    def create_login_url(self, target_url="/"):
        return users.create_login_url(target_url)

########NEW FILE########
__FILENAME__ = validation_helper
from models.event import Event
from models.match import Match
from models.team import Team


class ValidationHelper(object):
    """
    A collection of methods to validate model ids and return standard
    error messages if they are invalid.
    """

    @classmethod
    def validate(cls, validators):
        """
        Takes a list of tuples that defines a call to a validator
        (ie team_id_validator) and it's corresponding value to validate.
        Returns a dictionary of error messages if invalid.
        Example: ValidationHelper.validate([('team_id_validator', 'frc101')])
        """

        error_dict = { "Errors": list() }
        valid = True
        for v in validators:
            results = getattr(ValidationHelper, v[0])(v[1])
            if results:
                error_dict["Errors"].append(results)
                valid = False

        if valid is False: 
            return error_dict

    @classmethod
    def team_id_validator(cls, value):
        error_message = "{} is not a valid team id".format(value)
        team_key_error = { "team_id": error_message}
        if Team.validate_key_name(value) is False:
            return team_key_error

    @classmethod
    def event_id_validator(cls, value):
        error_message = "{} is not a valid event id".format(value)
        event_key_error = { "event_id": error_message}
        if Event.validate_key_name(value) is False:
            return event_key_error

    @classmethod
    def match_id_validator(cls, value):
        error_message = "{} is not a valid match id".format(value)
        match_key_error = { "match_id": error_message}
        if Match.validate_key_name(value) is False:
            return match_key_error

########NEW FILE########
__FILENAME__ = iri2uri
"""
iri2uri

Converts an IRI to a URI.

"""
__author__ = "Joe Gregorio (joe@bitworking.org)"
__copyright__ = "Copyright 2006, Joe Gregorio"
__contributors__ = []
__version__ = "1.0.0"
__license__ = "MIT"
__history__ = """
"""

import urlparse


# Convert an IRI to a URI following the rules in RFC 3987
#
# The characters we need to enocde and escape are defined in the spec:
#
# iprivate =  %xE000-F8FF / %xF0000-FFFFD / %x100000-10FFFD
# ucschar = %xA0-D7FF / %xF900-FDCF / %xFDF0-FFEF
#         / %x10000-1FFFD / %x20000-2FFFD / %x30000-3FFFD
#         / %x40000-4FFFD / %x50000-5FFFD / %x60000-6FFFD
#         / %x70000-7FFFD / %x80000-8FFFD / %x90000-9FFFD
#         / %xA0000-AFFFD / %xB0000-BFFFD / %xC0000-CFFFD
#         / %xD0000-DFFFD / %xE1000-EFFFD

escape_range = [
   (0xA0, 0xD7FF ),
   (0xE000, 0xF8FF ),
   (0xF900, 0xFDCF ),
   (0xFDF0, 0xFFEF),
   (0x10000, 0x1FFFD ),
   (0x20000, 0x2FFFD ),
   (0x30000, 0x3FFFD),
   (0x40000, 0x4FFFD ),
   (0x50000, 0x5FFFD ),
   (0x60000, 0x6FFFD),
   (0x70000, 0x7FFFD ),
   (0x80000, 0x8FFFD ),
   (0x90000, 0x9FFFD),
   (0xA0000, 0xAFFFD ),
   (0xB0000, 0xBFFFD ),
   (0xC0000, 0xCFFFD),
   (0xD0000, 0xDFFFD ),
   (0xE1000, 0xEFFFD),
   (0xF0000, 0xFFFFD ),
   (0x100000, 0x10FFFD)
]

def encode(c):
    retval = c
    i = ord(c)
    for low, high in escape_range:
        if i < low:
            break
        if i >= low and i <= high:
            retval = "".join(["%%%2X" % ord(o) for o in c.encode('utf-8')])
            break
    return retval


def iri2uri(uri):
    """Convert an IRI to a URI. Note that IRIs must be
    passed in a unicode strings. That is, do not utf-8 encode
    the IRI before passing it into the function."""
    if isinstance(uri ,unicode):
        (scheme, authority, path, query, fragment) = urlparse.urlsplit(uri)
        authority = authority.encode('idna')
        # For each character in 'ucschar' or 'iprivate'
        #  1. encode as utf-8
        #  2. then %-encode each octet of that utf-8
        uri = urlparse.urlunsplit((scheme, authority, path, query, fragment))
        uri = "".join([encode(c) for c in uri])
    return uri

if __name__ == "__main__":
    import unittest

    class Test(unittest.TestCase):

        def test_uris(self):
            """Test that URIs are invariant under the transformation."""
            invariant = [
                u"ftp://ftp.is.co.za/rfc/rfc1808.txt",
                u"http://www.ietf.org/rfc/rfc2396.txt",
                u"ldap://[2001:db8::7]/c=GB?objectClass?one",
                u"mailto:John.Doe@example.com",
                u"news:comp.infosystems.www.servers.unix",
                u"tel:+1-816-555-1212",
                u"telnet://192.0.2.16:80/",
                u"urn:oasis:names:specification:docbook:dtd:xml:4.1.2" ]
            for uri in invariant:
                self.assertEqual(uri, iri2uri(uri))

        def test_iri(self):
            """ Test that the right type of escaping is done for each part of the URI."""
            self.assertEqual("http://xn--o3h.com/%E2%98%84", iri2uri(u"http://\N{COMET}.com/\N{COMET}"))
            self.assertEqual("http://bitworking.org/?fred=%E2%98%84", iri2uri(u"http://bitworking.org/?fred=\N{COMET}"))
            self.assertEqual("http://bitworking.org/#%E2%98%84", iri2uri(u"http://bitworking.org/#\N{COMET}"))
            self.assertEqual("#%E2%98%84", iri2uri(u"#\N{COMET}"))
            self.assertEqual("/fred?bar=%E2%98%9A#%E2%98%84", iri2uri(u"/fred?bar=\N{BLACK LEFT POINTING INDEX}#\N{COMET}"))
            self.assertEqual("/fred?bar=%E2%98%9A#%E2%98%84", iri2uri(iri2uri(u"/fred?bar=\N{BLACK LEFT POINTING INDEX}#\N{COMET}")))
            self.assertNotEqual("/fred?bar=%E2%98%9A#%E2%98%84", iri2uri(u"/fred?bar=\N{BLACK LEFT POINTING INDEX}#\N{COMET}".encode('utf-8')))

    unittest.main()



########NEW FILE########
__FILENAME__ = socks
"""SocksiPy - Python SOCKS module.
Version 1.00

Copyright 2006 Dan-Haim. All rights reserved.

Redistribution and use in source and binary forms, with or without modification,
are permitted provided that the following conditions are met:
1. Redistributions of source code must retain the above copyright notice, this
   list of conditions and the following disclaimer.
2. Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions and the following disclaimer in the documentation
   and/or other materials provided with the distribution.
3. Neither the name of Dan Haim nor the names of his contributors may be used
   to endorse or promote products derived from this software without specific
   prior written permission.

THIS SOFTWARE IS PROVIDED BY DAN HAIM "AS IS" AND ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
EVENT SHALL DAN HAIM OR HIS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA
OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMANGE.


This module provides a standard socket-like interface for Python
for tunneling connections through SOCKS proxies.

"""

"""

Minor modifications made by Christopher Gilbert (http://motomastyle.com/)
for use in PyLoris (http://pyloris.sourceforge.net/)

Minor modifications made by Mario Vilas (http://breakingcode.wordpress.com/)
mainly to merge bug fixes found in Sourceforge

"""

import base64
import socket
import struct
import sys

if getattr(socket, 'socket', None) is None:
    raise ImportError('socket.socket missing, proxy support unusable')

PROXY_TYPE_SOCKS4 = 1
PROXY_TYPE_SOCKS5 = 2
PROXY_TYPE_HTTP = 3
PROXY_TYPE_HTTP_NO_TUNNEL = 4

_defaultproxy = None
_orgsocket = socket.socket

class ProxyError(Exception): pass
class GeneralProxyError(ProxyError): pass
class Socks5AuthError(ProxyError): pass
class Socks5Error(ProxyError): pass
class Socks4Error(ProxyError): pass
class HTTPError(ProxyError): pass

_generalerrors = ("success",
    "invalid data",
    "not connected",
    "not available",
    "bad proxy type",
    "bad input")

_socks5errors = ("succeeded",
    "general SOCKS server failure",
    "connection not allowed by ruleset",
    "Network unreachable",
    "Host unreachable",
    "Connection refused",
    "TTL expired",
    "Command not supported",
    "Address type not supported",
    "Unknown error")

_socks5autherrors = ("succeeded",
    "authentication is required",
    "all offered authentication methods were rejected",
    "unknown username or invalid password",
    "unknown error")

_socks4errors = ("request granted",
    "request rejected or failed",
    "request rejected because SOCKS server cannot connect to identd on the client",
    "request rejected because the client program and identd report different user-ids",
    "unknown error")

def setdefaultproxy(proxytype=None, addr=None, port=None, rdns=True, username=None, password=None):
    """setdefaultproxy(proxytype, addr[, port[, rdns[, username[, password]]]])
    Sets a default proxy which all further socksocket objects will use,
    unless explicitly changed.
    """
    global _defaultproxy
    _defaultproxy = (proxytype, addr, port, rdns, username, password)

def wrapmodule(module):
    """wrapmodule(module)
    Attempts to replace a module's socket library with a SOCKS socket. Must set
    a default proxy using setdefaultproxy(...) first.
    This will only work on modules that import socket directly into the namespace;
    most of the Python Standard Library falls into this category.
    """
    if _defaultproxy != None:
        module.socket.socket = socksocket
    else:
        raise GeneralProxyError((4, "no proxy specified"))

class socksocket(socket.socket):
    """socksocket([family[, type[, proto]]]) -> socket object
    Open a SOCKS enabled socket. The parameters are the same as
    those of the standard socket init. In order for SOCKS to work,
    you must specify family=AF_INET, type=SOCK_STREAM and proto=0.
    """

    def __init__(self, family=socket.AF_INET, type=socket.SOCK_STREAM, proto=0, _sock=None):
        _orgsocket.__init__(self, family, type, proto, _sock)
        if _defaultproxy != None:
            self.__proxy = _defaultproxy
        else:
            self.__proxy = (None, None, None, None, None, None)
        self.__proxysockname = None
        self.__proxypeername = None
        self.__httptunnel = True

    def __recvall(self, count):
        """__recvall(count) -> data
        Receive EXACTLY the number of bytes requested from the socket.
        Blocks until the required number of bytes have been received.
        """
        data = self.recv(count)
        while len(data) < count:
            d = self.recv(count-len(data))
            if not d: raise GeneralProxyError((0, "connection closed unexpectedly"))
            data = data + d
        return data

    def sendall(self, content, *args):
        """ override socket.socket.sendall method to rewrite the header
        for non-tunneling proxies if needed
        """
        if not self.__httptunnel:
            content = self.__rewriteproxy(content)
        return super(socksocket, self).sendall(content, *args)

    def __rewriteproxy(self, header):
        """ rewrite HTTP request headers to support non-tunneling proxies
        (i.e. those which do not support the CONNECT method).
        This only works for HTTP (not HTTPS) since HTTPS requires tunneling.
        """
        host, endpt = None, None
        hdrs = header.split("\r\n")
        for hdr in hdrs:
            if hdr.lower().startswith("host:"):
                host = hdr
            elif hdr.lower().startswith("get") or hdr.lower().startswith("post"):
                endpt = hdr
        if host and endpt:
            hdrs.remove(host)
            hdrs.remove(endpt)
            host = host.split(" ")[1]
            endpt = endpt.split(" ")
            if (self.__proxy[4] != None and self.__proxy[5] != None):
                hdrs.insert(0, self.__getauthheader())
            hdrs.insert(0, "Host: %s" % host)
            hdrs.insert(0, "%s http://%s%s %s" % (endpt[0], host, endpt[1], endpt[2]))
        return "\r\n".join(hdrs)

    def __getauthheader(self):
        auth = self.__proxy[4] + ":" + self.__proxy[5]
        return "Proxy-Authorization: Basic " + base64.b64encode(auth)

    def setproxy(self, proxytype=None, addr=None, port=None, rdns=True, username=None, password=None):
        """setproxy(proxytype, addr[, port[, rdns[, username[, password]]]])
        Sets the proxy to be used.
        proxytype -    The type of the proxy to be used. Three types
                are supported: PROXY_TYPE_SOCKS4 (including socks4a),
                PROXY_TYPE_SOCKS5 and PROXY_TYPE_HTTP
        addr -        The address of the server (IP or DNS).
        port -        The port of the server. Defaults to 1080 for SOCKS
                servers and 8080 for HTTP proxy servers.
        rdns -        Should DNS queries be preformed on the remote side
                (rather than the local side). The default is True.
                Note: This has no effect with SOCKS4 servers.
        username -    Username to authenticate with to the server.
                The default is no authentication.
        password -    Password to authenticate with to the server.
                Only relevant when username is also provided.
        """
        self.__proxy = (proxytype, addr, port, rdns, username, password)

    def __negotiatesocks5(self, destaddr, destport):
        """__negotiatesocks5(self,destaddr,destport)
        Negotiates a connection through a SOCKS5 server.
        """
        # First we'll send the authentication packages we support.
        if (self.__proxy[4]!=None) and (self.__proxy[5]!=None):
            # The username/password details were supplied to the
            # setproxy method so we support the USERNAME/PASSWORD
            # authentication (in addition to the standard none).
            self.sendall(struct.pack('BBBB', 0x05, 0x02, 0x00, 0x02))
        else:
            # No username/password were entered, therefore we
            # only support connections with no authentication.
            self.sendall(struct.pack('BBB', 0x05, 0x01, 0x00))
        # We'll receive the server's response to determine which
        # method was selected
        chosenauth = self.__recvall(2)
        if chosenauth[0:1] != chr(0x05).encode():
            self.close()
            raise GeneralProxyError((1, _generalerrors[1]))
        # Check the chosen authentication method
        if chosenauth[1:2] == chr(0x00).encode():
            # No authentication is required
            pass
        elif chosenauth[1:2] == chr(0x02).encode():
            # Okay, we need to perform a basic username/password
            # authentication.
            self.sendall(chr(0x01).encode() + chr(len(self.__proxy[4])) + self.__proxy[4] + chr(len(self.__proxy[5])) + self.__proxy[5])
            authstat = self.__recvall(2)
            if authstat[0:1] != chr(0x01).encode():
                # Bad response
                self.close()
                raise GeneralProxyError((1, _generalerrors[1]))
            if authstat[1:2] != chr(0x00).encode():
                # Authentication failed
                self.close()
                raise Socks5AuthError((3, _socks5autherrors[3]))
            # Authentication succeeded
        else:
            # Reaching here is always bad
            self.close()
            if chosenauth[1] == chr(0xFF).encode():
                raise Socks5AuthError((2, _socks5autherrors[2]))
            else:
                raise GeneralProxyError((1, _generalerrors[1]))
        # Now we can request the actual connection
        req = struct.pack('BBB', 0x05, 0x01, 0x00)
        # If the given destination address is an IP address, we'll
        # use the IPv4 address request even if remote resolving was specified.
        try:
            ipaddr = socket.inet_aton(destaddr)
            req = req + chr(0x01).encode() + ipaddr
        except socket.error:
            # Well it's not an IP number,  so it's probably a DNS name.
            if self.__proxy[3]:
                # Resolve remotely
                ipaddr = None
                req = req + chr(0x03).encode() + chr(len(destaddr)).encode() + destaddr
            else:
                # Resolve locally
                ipaddr = socket.inet_aton(socket.gethostbyname(destaddr))
                req = req + chr(0x01).encode() + ipaddr
        req = req + struct.pack(">H", destport)
        self.sendall(req)
        # Get the response
        resp = self.__recvall(4)
        if resp[0:1] != chr(0x05).encode():
            self.close()
            raise GeneralProxyError((1, _generalerrors[1]))
        elif resp[1:2] != chr(0x00).encode():
            # Connection failed
            self.close()
            if ord(resp[1:2])<=8:
                raise Socks5Error((ord(resp[1:2]), _socks5errors[ord(resp[1:2])]))
            else:
                raise Socks5Error((9, _socks5errors[9]))
        # Get the bound address/port
        elif resp[3:4] == chr(0x01).encode():
            boundaddr = self.__recvall(4)
        elif resp[3:4] == chr(0x03).encode():
            resp = resp + self.recv(1)
            boundaddr = self.__recvall(ord(resp[4:5]))
        else:
            self.close()
            raise GeneralProxyError((1,_generalerrors[1]))
        boundport = struct.unpack(">H", self.__recvall(2))[0]
        self.__proxysockname = (boundaddr, boundport)
        if ipaddr != None:
            self.__proxypeername = (socket.inet_ntoa(ipaddr), destport)
        else:
            self.__proxypeername = (destaddr, destport)

    def getproxysockname(self):
        """getsockname() -> address info
        Returns the bound IP address and port number at the proxy.
        """
        return self.__proxysockname

    def getproxypeername(self):
        """getproxypeername() -> address info
        Returns the IP and port number of the proxy.
        """
        return _orgsocket.getpeername(self)

    def getpeername(self):
        """getpeername() -> address info
        Returns the IP address and port number of the destination
        machine (note: getproxypeername returns the proxy)
        """
        return self.__proxypeername

    def __negotiatesocks4(self,destaddr,destport):
        """__negotiatesocks4(self,destaddr,destport)
        Negotiates a connection through a SOCKS4 server.
        """
        # Check if the destination address provided is an IP address
        rmtrslv = False
        try:
            ipaddr = socket.inet_aton(destaddr)
        except socket.error:
            # It's a DNS name. Check where it should be resolved.
            if self.__proxy[3]:
                ipaddr = struct.pack("BBBB", 0x00, 0x00, 0x00, 0x01)
                rmtrslv = True
            else:
                ipaddr = socket.inet_aton(socket.gethostbyname(destaddr))
        # Construct the request packet
        req = struct.pack(">BBH", 0x04, 0x01, destport) + ipaddr
        # The username parameter is considered userid for SOCKS4
        if self.__proxy[4] != None:
            req = req + self.__proxy[4]
        req = req + chr(0x00).encode()
        # DNS name if remote resolving is required
        # NOTE: This is actually an extension to the SOCKS4 protocol
        # called SOCKS4A and may not be supported in all cases.
        if rmtrslv:
            req = req + destaddr + chr(0x00).encode()
        self.sendall(req)
        # Get the response from the server
        resp = self.__recvall(8)
        if resp[0:1] != chr(0x00).encode():
            # Bad data
            self.close()
            raise GeneralProxyError((1,_generalerrors[1]))
        if resp[1:2] != chr(0x5A).encode():
            # Server returned an error
            self.close()
            if ord(resp[1:2]) in (91, 92, 93):
                self.close()
                raise Socks4Error((ord(resp[1:2]), _socks4errors[ord(resp[1:2]) - 90]))
            else:
                raise Socks4Error((94, _socks4errors[4]))
        # Get the bound address/port
        self.__proxysockname = (socket.inet_ntoa(resp[4:]), struct.unpack(">H", resp[2:4])[0])
        if rmtrslv != None:
            self.__proxypeername = (socket.inet_ntoa(ipaddr), destport)
        else:
            self.__proxypeername = (destaddr, destport)

    def __negotiatehttp(self, destaddr, destport):
        """__negotiatehttp(self,destaddr,destport)
        Negotiates a connection through an HTTP server.
        """
        # If we need to resolve locally, we do this now
        if not self.__proxy[3]:
            addr = socket.gethostbyname(destaddr)
        else:
            addr = destaddr
        headers =  ["CONNECT ", addr, ":", str(destport), " HTTP/1.1\r\n"]
        headers += ["Host: ", destaddr, "\r\n"]
        if (self.__proxy[4] != None and self.__proxy[5] != None):
                headers += [self.__getauthheader(), "\r\n"]
        headers.append("\r\n")
        self.sendall("".join(headers).encode())
        # We read the response until we get the string "\r\n\r\n"
        resp = self.recv(1)
        while resp.find("\r\n\r\n".encode()) == -1:
            resp = resp + self.recv(1)
        # We just need the first line to check if the connection
        # was successful
        statusline = resp.splitlines()[0].split(" ".encode(), 2)
        if statusline[0] not in ("HTTP/1.0".encode(), "HTTP/1.1".encode()):
            self.close()
            raise GeneralProxyError((1, _generalerrors[1]))
        try:
            statuscode = int(statusline[1])
        except ValueError:
            self.close()
            raise GeneralProxyError((1, _generalerrors[1]))
        if statuscode != 200:
            self.close()
            raise HTTPError((statuscode, statusline[2]))
        self.__proxysockname = ("0.0.0.0", 0)
        self.__proxypeername = (addr, destport)

    def connect(self, destpair):
        """connect(self, despair)
        Connects to the specified destination through a proxy.
        destpar - A tuple of the IP/DNS address and the port number.
        (identical to socket's connect).
        To select the proxy server use setproxy().
        """
        # Do a minimal input check first
        if (not type(destpair) in (list,tuple)) or (len(destpair) < 2) or (not isinstance(destpair[0], basestring)) or (type(destpair[1]) != int):
            raise GeneralProxyError((5, _generalerrors[5]))
        if self.__proxy[0] == PROXY_TYPE_SOCKS5:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 1080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatesocks5(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_SOCKS4:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 1080
            _orgsocket.connect(self,(self.__proxy[1], portnum))
            self.__negotiatesocks4(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_HTTP:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 8080
            _orgsocket.connect(self,(self.__proxy[1], portnum))
            self.__negotiatehttp(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_HTTP_NO_TUNNEL:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 8080
            _orgsocket.connect(self,(self.__proxy[1],portnum))
            if destpair[1] == 443:
                self.__negotiatehttp(destpair[0],destpair[1])
            else:
                self.__httptunnel = False
        elif self.__proxy[0] == None:
            _orgsocket.connect(self, (destpair[0], destpair[1]))
        else:
            raise GeneralProxyError((4, _generalerrors[4]))

########NEW FILE########
__FILENAME__ = socket
from realsocket import gaierror, error, getaddrinfo, SOCK_STREAM

########NEW FILE########
__FILENAME__ = test_proxies
import unittest
import errno
import os
import signal
import subprocess
import tempfile

import nose

import httplib2
from httplib2 import socks
from httplib2.test import miniserver

tinyproxy_cfg = """
User "%(user)s"
Port %(port)s
Listen 127.0.0.1
PidFile "%(pidfile)s"
LogFile "%(logfile)s"
MaxClients 2
StartServers 1
LogLevel Info
"""


class FunctionalProxyHttpTest(unittest.TestCase):
    def setUp(self):
        if not socks:
            raise nose.SkipTest('socks module unavailable')
        if not subprocess:
            raise nose.SkipTest('subprocess module unavailable')

        # start a short-lived miniserver so we can get a likely port
        # for the proxy
        self.httpd, self.proxyport = miniserver.start_server(
            miniserver.ThisDirHandler)
        self.httpd.shutdown()
        self.httpd, self.port = miniserver.start_server(
            miniserver.ThisDirHandler)

        self.pidfile = tempfile.mktemp()
        self.logfile = tempfile.mktemp()
        fd, self.conffile = tempfile.mkstemp()
        f = os.fdopen(fd, 'w')
        our_cfg = tinyproxy_cfg % {'user': os.getlogin(),
                                   'pidfile': self.pidfile,
                                   'port': self.proxyport,
                                   'logfile': self.logfile}
        f.write(our_cfg)
        f.close()
        try:
            # TODO use subprocess.check_call when 2.4 is dropped
            ret = subprocess.call(['tinyproxy', '-c', self.conffile])
            self.assertEqual(0, ret)
        except OSError, e:
            if e.errno == errno.ENOENT:
                raise nose.SkipTest('tinyproxy not available')
            raise

    def tearDown(self):
        self.httpd.shutdown()
        try:
            pid = int(open(self.pidfile).read())
            os.kill(pid, signal.SIGTERM)
        except OSError, e:
            if e.errno == errno.ESRCH:
                print '\n\n\nTinyProxy Failed to start, log follows:'
                print open(self.logfile).read()
                print 'end tinyproxy log\n\n\n'
            raise
        map(os.unlink, (self.pidfile,
                        self.logfile,
                        self.conffile))

    def testSimpleProxy(self):
        proxy_info = httplib2.ProxyInfo(socks.PROXY_TYPE_HTTP,
                                        'localhost', self.proxyport)
        client = httplib2.Http(proxy_info=proxy_info)
        src = 'miniserver.py'
        response, body = client.request('http://localhost:%d/%s' %
                                        (self.port, src))
        self.assertEqual(response.status, 200)
        self.assertEqual(body, open(os.path.join(miniserver.HERE, src)).read())
        lf = open(self.logfile).read()
        expect = ('Established connection to host "127.0.0.1" '
                  'using file descriptor')
        self.assertTrue(expect in lf,
                        'tinyproxy did not proxy a request for miniserver')

########NEW FILE########
__FILENAME__ = miniserver
import logging
import os
import select
import SimpleHTTPServer
import SocketServer
import threading

HERE = os.path.dirname(__file__)
logger = logging.getLogger(__name__)


class ThisDirHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):
    def translate_path(self, path):
        path = path.split('?', 1)[0].split('#', 1)[0]
        return os.path.join(HERE, *filter(None, path.split('/')))

    def log_message(self, s, *args):
        # output via logging so nose can catch it
        logger.info(s, *args)


class ShutdownServer(SocketServer.TCPServer):
    """Mixin that allows serve_forever to be shut down.

    The methods in this mixin are backported from SocketServer.py in the Python
    2.6.4 standard library. The mixin is unnecessary in 2.6 and later, when
    BaseServer supports the shutdown method directly.
    """

    def __init__(self, *args, **kwargs):
        SocketServer.TCPServer.__init__(self, *args, **kwargs)
        self.__is_shut_down = threading.Event()
        self.__serving = False

    def serve_forever(self, poll_interval=0.1):
        """Handle one request at a time until shutdown.

        Polls for shutdown every poll_interval seconds. Ignores
        self.timeout. If you need to do periodic tasks, do them in
        another thread.
        """
        self.__serving = True
        self.__is_shut_down.clear()
        while self.__serving:
            r, w, e = select.select([self.socket], [], [], poll_interval)
            if r:
                self._handle_request_noblock()
        self.__is_shut_down.set()

    def shutdown(self):
        """Stops the serve_forever loop.

        Blocks until the loop has finished. This must be called while
        serve_forever() is running in another thread, or it will deadlock.
        """
        self.__serving = False
        self.__is_shut_down.wait()

    def handle_request(self):
        """Handle one request, possibly blocking.

        Respects self.timeout.
        """
        # Support people who used socket.settimeout() to escape
        # handle_request before self.timeout was available.
        timeout = self.socket.gettimeout()
        if timeout is None:
            timeout = self.timeout
        elif self.timeout is not None:
            timeout = min(timeout, self.timeout)
        fd_sets = select.select([self], [], [], timeout)
        if not fd_sets[0]:
            self.handle_timeout()
            return
        self._handle_request_noblock()

    def _handle_request_noblock(self):
        """Handle one request, without blocking.

        I assume that select.select has returned that the socket is
        readable before this function was called, so there should be
        no risk of blocking in get_request().
        """
        try:
            request, client_address = self.get_request()
        except socket.error:
            return
        if self.verify_request(request, client_address):
            try:
                self.process_request(request, client_address)
            except:
                self.handle_error(request, client_address)
                self.close_request(request)


def start_server(handler):
    httpd = ShutdownServer(("", 0), handler)
    threading.Thread(target=httpd.serve_forever).start()
    _, port = httpd.socket.getsockname()
    return httpd, port

########NEW FILE########
__FILENAME__ = smoke_test
import os
import unittest

import httplib2

from httplib2.test import miniserver


class HttpSmokeTest(unittest.TestCase):
    def setUp(self):
        self.httpd, self.port = miniserver.start_server(
            miniserver.ThisDirHandler)

    def tearDown(self):
        self.httpd.shutdown()

    def testGetFile(self):
        client = httplib2.Http()
        src = 'miniserver.py'
        response, body = client.request('http://localhost:%d/%s' %
                                        (self.port, src))
        self.assertEqual(response.status, 200)
        self.assertEqual(body, open(os.path.join(miniserver.HERE, src)).read())

########NEW FILE########
__FILENAME__ = test_no_socket
"""Tests for httplib2 when the socket module is missing.

This helps ensure compatibility with environments such as AppEngine.
"""
import os
import sys
import unittest

import httplib2


class MissingSocketTest(unittest.TestCase):
    def setUp(self):
        self._oldsocks = httplib2.socks
        httplib2.socks = None

    def tearDown(self):
        httplib2.socks = self._oldsocks

    def testProxyDisabled(self):
        proxy_info = httplib2.ProxyInfo('blah',
                                        'localhost', 0)
        client = httplib2.Http(proxy_info=proxy_info)
        self.assertRaises(httplib2.ProxiesUnavailableError,
                          client.request, 'http://localhost:-1/')

########NEW FILE########
__FILENAME__ = linter
#!/usr/bin/python

"""
Based on https://github.com/cbrueffer/pep8-git-hook
Forked from https://gist.github.com/810399
"""
from __future__ import with_statement
import os
import re
import shutil
import subprocess
import sys
import tempfile

# don't fill in both of these
select_codes = ["E111", "E125", "E203", "E261", "E262", "E301", "E302", "E303",
                "E502", "E701", "E711", "W291", "W293"]
ignore_codes = []


def system(*args, **kwargs):
    kwargs.setdefault('stdout', subprocess.PIPE)
    proc = subprocess.Popen(args, **kwargs)
    out, err = proc.communicate()
    return out


def main():
    modified = re.compile('^[AM]+\s+(?P<name>.*\.py)', re.MULTILINE)
    files = system('git', 'status', '--porcelain')
    files = modified.findall(files)

    tempdir = tempfile.mkdtemp()
    for name in files:
        filename = os.path.join(tempdir, name)
        filepath = os.path.dirname(filename)

        if not os.path.exists(filepath):
            os.makedirs(filepath)
        with file(filename, 'w') as f:
            system('git', 'show', ':' + name, stdout=f)

    try:
        if select_codes and ignore_codes:
            print "Error: select and ignore codes are mutually exclusive"
            sys.exit(1)
        elif select_codes:
            output = system('pep8', '--select', ','.join(select_codes), '.', cwd=tempdir)
        elif ignore_codes:
            output = system('pep8', '--ignore', ','.join(ignore_codes), '.', cwd=tempdir)
        else:
            output = system('pep8', '.', cwd=tempdir)
    except OSError:
        print "ERROR: PEP8 needs to be installed!"
        print "You can install it by running: easy_install pep8"
        sys.exit(1)

    shutil.rmtree(tempdir)
    if output:
        print 'PEP8 style violations have been detected.  Please fix them\n' \
        'or force the commit with "git commit --no-verify".\n'
        print output,
        sys.exit(1)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = main
#!/usr/bin/env python
import webapp2
from webapp2_extras.routes import RedirectRoute

import tba_config

from controllers.account_controller import AccountEdit, AccountLogout, AccountOverview, AccountRegister
from controllers.ajax_controller import LiveEventHandler, TypeaheadHandler, WebcastHandler
from controllers.event_controller import EventList, EventDetail, EventRss
from controllers.gameday2_controller import Gameday2Controller
from controllers.insights_controller import InsightsOverview, InsightsDetail
from controllers.main_controller import ContactHandler, HashtagsHandler, \
      MainKickoffHandler, MainBuildseasonHandler, MainChampsHandler, MainCompetitionseasonHandler, \
      MainInsightsHandler, MainOffseasonHandler, OprHandler, SearchHandler, \
      AboutHandler, ThanksHandler, PageNotFoundHandler, \
      GamedayHandler, WebcastsHandler, RecordHandler, ApiDocumentationHandler
from controllers.match_controller import MatchDetail
from controllers.suggestions.suggest_match_video_controller import SuggestMatchVideoController
from controllers.suggestions.suggest_event_webcast_controller import SuggestEventWebcastController
from controllers.suggestions.suggest_team_media_controller import SuggestTeamMediaController
from controllers.team_controller import TeamList, TeamCanonical, TeamDetail, TeamHistory

from google.appengine.ext.webapp import template
template.register_template_library('common.my_filters')

landing_handler = {tba_config.KICKOFF: MainKickoffHandler,
                   tba_config.BUILDSEASON: MainBuildseasonHandler,
                   tba_config.CHAMPS: MainChampsHandler,
                   tba_config.COMPETITIONSEASON: MainCompetitionseasonHandler,
                   tba_config.OFFSEASON: MainOffseasonHandler,
                   tba_config.INSIGHTS: MainInsightsHandler,
                   }


class Webapp2HandlerAdapter(webapp2.BaseHandlerAdapter):
    def __call__(self, request, response, exception):
        request.route_args = {}
        request.route_args['exception'] = exception
        handler = self.handler(request, response)
        return handler.get()

app = webapp2.WSGIApplication([
      RedirectRoute(r'/', landing_handler[tba_config.CONFIG['landing_handler']], 'landing', strict_slash=True),
      RedirectRoute(r'/about', AboutHandler, 'about', strict_slash=True),
      RedirectRoute(r'/account', AccountOverview, 'account-overview', strict_slash=True),
      RedirectRoute(r'/account/edit', AccountEdit, 'account-edit', strict_slash=True),
      RedirectRoute(r'/account/register', AccountRegister, 'account-register', strict_slash=True),
      RedirectRoute(r'/apidocs', ApiDocumentationHandler, 'api-documentation', strict_slash=True),
      RedirectRoute(r'/contact', ContactHandler, 'contact', strict_slash=True),
      RedirectRoute(r'/event/<event_key>', EventDetail, 'event-detail', strict_slash=True),
      RedirectRoute(r'/event/<event_key>/feed', EventRss, 'event-rss', strict_slash=True),
      RedirectRoute(r'/events/<year:[0-9]+>', EventList, 'event-list-year', strict_slash=True),
      RedirectRoute(r'/events', EventList, 'event-list', strict_slash=True),
      RedirectRoute(r'/gameday', GamedayHandler, 'gameday', strict_slash=True),
      RedirectRoute(r'/gameday2', Gameday2Controller, 'gameday2', strict_slash=True),
      RedirectRoute(r'/hashtags', HashtagsHandler, 'hashtags', strict_slash=True),
      RedirectRoute(r'/insights/<year:[0-9]+>', InsightsDetail, 'insights-detail', strict_slash=True),
      RedirectRoute(r'/insights', InsightsOverview, 'insights', strict_slash=True),
      RedirectRoute(r'/logout', AccountLogout, 'account-logout', strict_slash=True),
      RedirectRoute(r'/match/<match_key>', MatchDetail, 'match-detail', strict_slash=True),
      RedirectRoute(r'/opr', OprHandler, 'opr', strict_slash=True),
      RedirectRoute(r'/record', RecordHandler, 'record', strict_slash=True),
      RedirectRoute(r'/search', SearchHandler, 'search', strict_slash=True),
      RedirectRoute(r'/suggest/event/webcast', SuggestEventWebcastController, 'suggest-event-webcast', strict_slash=True),
      RedirectRoute(r'/suggest/match/video', SuggestMatchVideoController, 'suggest-match-video', strict_slash=True),
      RedirectRoute(r'/suggest/team/media', SuggestTeamMediaController, 'suggest-team-media', strict_slash=True),
      RedirectRoute(r'/team/<team_number:[0-9]+>', TeamCanonical, 'team-canonical', strict_slash=True),
      RedirectRoute(r'/team/<team_number:[0-9]+>/<year:[0-9]+>', TeamDetail, 'team-detail', strict_slash=True),
      RedirectRoute(r'/team/<team_number:[0-9]+>/history', TeamHistory, 'team-history', strict_slash=True),
      RedirectRoute(r'/teams/<page:[0-9]+>', TeamList, 'team-list-year', strict_slash=True),
      RedirectRoute(r'/teams', TeamList, 'team-list', strict_slash=True),
      RedirectRoute(r'/thanks', ThanksHandler, 'thanks', strict_slash=True),
      RedirectRoute(r'/webcasts', WebcastsHandler, 'webcasts', strict_slash=True),
      RedirectRoute(r'/_/live-event/<event_key>/<timestamp:[0-9]+>', LiveEventHandler, 'ajax-live-event', strict_slash=True),
      RedirectRoute(r'/_/typeahead/<search_key>', TypeaheadHandler, 'ajax-typeahead', strict_slash=True),
      RedirectRoute(r'/_/webcast/<event_key>/<webcast_number>', WebcastHandler, 'ajax-webcast', strict_slash=True),
      RedirectRoute(r'/<:.*>', PageNotFoundHandler, 'page-not-found', strict_slash=True),
      ],
      debug=tba_config.DEBUG)
app.error_handlers[404] = Webapp2HandlerAdapter(PageNotFoundHandler)

########NEW FILE########
__FILENAME__ = account
from google.appengine.ext import ndb


class Account(ndb.Model):
    """
    Accounts represent accounts people use on TBA.
    """
    # Set by login/registration
    # Not editable by the user
    email = ndb.StringProperty()
    nickname = ndb.StringProperty()
    registered = ndb.BooleanProperty()

    created = ndb.DateTimeProperty(auto_now_add=True)
    updated = ndb.DateTimeProperty(auto_now=True, indexed=False)

    # These optional properties are editable by the user
    display_name = ndb.StringProperty()

########NEW FILE########
__FILENAME__ = award
import json

from google.appengine.ext import ndb

from models.event import Event
from models.team import Team


class Award(ndb.Model):
    """
    Awards represent FIRST Robotics Competition awards given out at an event.
    key_name is formatted as: <event_key_name>_<award_type_enum>
    If multiple recipients win the same award at the same event (such as
    Winner or Dean's List), they show up under the repeated properties.
    """

    name_str = ndb.StringProperty(required=True, indexed=False)  # award name that shows up on USFIRST Pages. May vary for the same award type.
    award_type_enum = ndb.IntegerProperty(required=True)
    year = ndb.IntegerProperty(required=True)  # year the award was awarded
    event = ndb.KeyProperty(kind=Event, required=True)  # event at which the award was awarded
    event_type_enum = ndb.IntegerProperty(required=True)  # needed to query for awards from events of a certain event type

    team_list = ndb.KeyProperty(kind=Team, repeated=True)  # key of team(s) that won the award (if applicable)
    recipient_json_list = ndb.StringProperty(repeated=True)  # JSON dict(s) with team_number and/or awardee

    created = ndb.DateTimeProperty(auto_now_add=True)
    updated = ndb.DateTimeProperty(auto_now=True)

    def __init__(self, *args, **kw):
        # store set of affected references referenced keys for cache clearing
        # keys must be model properties
        self._affected_references = {
            'event': set(),
            'team_list': set(),
            'year': set(),
        }
        self._recipient_list = None
        self._recipient_dict = None
        self._recipient_list_json = None
        super(Award, self).__init__(*args, **kw)

    @property
    def recipient_dict(self):
        """
        Uses recipient_list to add a recipient_dict property,
        where the key is the team_number and the value is a list of awardees.
        """
        if self._recipient_dict is None:
            self._recipient_dict = {}
            for recipient in self.recipient_list:
                team_number = recipient['team_number']
                awardee = recipient['awardee']
                if team_number in self._recipient_dict:
                    self._recipient_dict[team_number].append(awardee)
                else:
                    self._recipient_dict[team_number] = [awardee]
        return self._recipient_dict

    @property
    def recipient_list(self):
        if self._recipient_list is None:
            self._recipient_list = []
            for recipient_json in self.recipient_json_list:
                self._recipient_list.append(json.loads(recipient_json))
        return self._recipient_list

    @property
    def recipient_list_json(self):
        """
        A JSON version of the recipient_list
        """
        if self._recipient_list_json is None:
            self._recipient_list_json = json.dumps(self.recipient_list)

        return self._recipient_list_json

    @property
    def key_name(self):
        return self.render_key_name(self.event.id(), self.award_type_enum)

    @classmethod
    def render_key_name(self, event_key_name, award_type_enum):
        return '{}_{}'.format(event_key_name, award_type_enum)

########NEW FILE########
__FILENAME__ = event
from google.appengine.ext import ndb
import datetime
import json
import re

from consts.district_type import DistrictType
from consts.event_type import EventType


class Event(ndb.Model):
    """
    Events represent FIRST Robotics Competition events, both official and unofficial.
    key_name is like '2010ct'
    """
    name = ndb.StringProperty()
    event_type_enum = ndb.IntegerProperty(required=True)
    short_name = ndb.StringProperty(indexed=False)  # Should not contain "Regional" or "Division", like "Hartford"
    event_short = ndb.StringProperty(required=True, indexed=False)  # Smaller abbreviation like "CT"
    year = ndb.IntegerProperty(required=True)
    event_district_enum = ndb.IntegerProperty()
    start_date = ndb.DateTimeProperty()
    end_date = ndb.DateTimeProperty()
    venue = ndb.StringProperty(indexed=False)
    venue_address = ndb.StringProperty(indexed=False)  # We can scrape this.
    location = ndb.StringProperty(indexed=False)  # in the format "locality, region, country". similar to Team.address
    timezone_id = ndb.StringProperty()  # such as 'America/Los_Angeles' or 'Asia/Jerusalem'
    official = ndb.BooleanProperty(default=False)  # Is the event FIRST-official?
    first_eid = ndb.StringProperty()  # from USFIRST
    facebook_eid = ndb.StringProperty(indexed=False)  # from Facebook
    website = ndb.StringProperty(indexed=False)
    webcast_json = ndb.TextProperty(indexed=False)  # list of dicts, valid keys include 'type' and 'channel'
    matchstats_json = ndb.TextProperty(indexed=False)  # for OPR, DPR, CCWM, etc.
    rankings_json = ndb.TextProperty(indexed=False)
    alliance_selections_json = ndb.TextProperty(indexed=False)  # Formatted as: [{'picks': [captain, pick1, pick2, 'frc123', ...], 'declines':[decline1, decline2, ...] }, {'picks': [], 'declines': []}, ... ]

    created = ndb.DateTimeProperty(auto_now_add=True, indexed=False)
    updated = ndb.DateTimeProperty(auto_now=True, indexed=False)

    def __init__(self, *args, **kw):
        # store set of affected references referenced keys for cache clearing
        # keys must be model properties
        self._affected_references = {
            'key': set(),
            'year': set(),
        }
        self._alliance_selections = None
        self._awards = None
        self._matches = None
        self._matchstats = None
        self._rankings = None
        self._teams = None
        self._webcast = None
        super(Event, self).__init__(*args, **kw)

    @ndb.tasklet
    def get_awards_async(self):
        from models.award import Award
        award_keys = yield Award.query(Award.event == self.key).fetch_async(500, keys_only=True)
        self._awards = yield ndb.get_multi_async(award_keys)

    @property
    def alliance_selections(self):
        """
        Lazy load alliance_selections JSON
        """
        if self._alliance_selections is None:
            try:
                self._alliance_selections = json.loads(self.alliance_selections_json)
            except Exception, e:
                self._alliance_selections = None
        return self._alliance_selections

    @property
    def awards(self):
        # This import is ugly, and maybe all the models should be in one file again -gregmarra 20121006
        from models.award import Award
        if self._awards is None:
            self.get_awards_async().wait()
        return self._awards

    @ndb.tasklet
    def get_matches_async(self):
        from models.match import Match
        match_keys = yield Match.query(Match.event == self.key).fetch_async(500, keys_only=True)
        self._matches = yield ndb.get_multi_async(match_keys)

    @property
    def matches(self):
        # This import is ugly, and maybe all the models should be in one file again -gregmarra 20121006
        from models.match import Match
        if self._matches is None:
            if self._matches is None:
                self.get_matches_async().wait()
        return self._matches

    def withinDays(self, negative_days_before, days_after):
        if not self.start_date or not self.end_date:
            return False
        today = datetime.datetime.today()
        after_start = self.start_date.date() + datetime.timedelta(days=negative_days_before) <= today.date()
        before_end = self.end_date.date() + datetime.timedelta(days=days_after) >= today.date()

        return (after_start and before_end)

    @property
    def now(self):
        return self.withinDays(0, 0)

    @property
    def within_a_day(self):
        return self.withinDays(-1, 1)

    @property
    def past(self):
        return self.end_date.date() < datetime.date.today() and not self.within_a_day

    @property
    def future(self):
        return self.start_date.date() > datetime.date.today() and not self.within_a_day

    @ndb.tasklet
    def get_teams_async(self):
        from models.event_team import EventTeam
        event_team_keys = yield EventTeam.query(EventTeam.event == self.key).fetch_async(500, keys_only=True)
        event_teams = yield ndb.get_multi_async(event_team_keys)
        team_keys = map(lambda event_team: event_team.team, event_teams)
        self._teams = yield ndb.get_multi_async(team_keys)

    @property
    def teams(self):
        if self._teams is None:
            self.get_teams_async().wait()
        return self._teams

    @ndb.toplevel
    def prepAwardsMatchesTeams(self):
        yield self.get_awards_async(), self.get_matches_async(), self.get_teams_async()

    @ndb.toplevel
    def prepTeams(self):
        yield self.get_teams_async()

    @ndb.toplevel
    def prepTeamsMatches(self):
        yield self.get_matches_async(), self.get_teams_async()

    @property
    def matchstats(self):
        """
        Lazy load parsing matchstats JSON
        """
        if self._matchstats is None:
            try:
                self._matchstats = json.loads(self.matchstats_json)
            except Exception, e:
                self._matchstats = None
        return self._matchstats

    @property
    def rankings(self):
        """
        Lazy load parsing rankings JSON
        """
        if self._rankings is None:
            try:
                self._rankings = json.loads(self.rankings_json)
            except Exception, e:
                self._rankings = None
        return self._rankings

    @property
    def webcast(self):
        """
        Lazy load parsing webcast JSON
        """
        if self._webcast is None:
            try:
                self._webcast = json.loads(self.webcast_json)
            except Exception, e:
                self._webcast = None
        return self._webcast

    @property
    def key_name(self):
        """
        Returns the string of the key_name of the Event object before writing it.
        """
        return str(self.year) + self.event_short

    @property
    def facebook_event_url(self):
        """
        Return a string of the Facebook Event URL.
        """
        return "http://www.facebook.com/event.php?eid=%s" % self.facebook_eid

    @property
    def details_url(self):
        """
        Returns the URL pattern for the link to this Event on TBA
        """
        return "/event/%s" % self.key_name

    @property
    def gameday_url(self):
        """
        Returns the URL pattern for the link to watch webcasts in Gameday
        """
        if self.webcast:
            gameday_link = '/gameday'
            view_num = 0
            for webcast in self.webcast:
                if view_num == 0:
                    gameday_link += '#'
                else:
                    gameday_link += '&'
                if 'type' in webcast and 'channel' in webcast:
                    gameday_link += 'view_' + str(view_num) + '=' + self.key_name + '-' + str(view_num + 1)
                view_num += 1
            return gameday_link
        else:
            return None

    # Depreciated, still here to keep GAE clean.
    webcast_url = ndb.StringProperty(indexed=False)

    @classmethod
    def validate_key_name(self, event_key):
        key_name_regex = re.compile(r'^[1-9]\d{3}[a-z]+[1-9]?$')
        match = re.match(key_name_regex, event_key)
        return True if match else False

    @property
    def event_district_str(self):
        return DistrictType.type_names.get(self.event_district_enum, None)

    @property
    def event_type_str(self):
        return EventType.type_names[self.event_type_enum]

########NEW FILE########
__FILENAME__ = event_team
from google.appengine.ext import ndb

from models.event import Event
from models.team import Team


class EventTeam(ndb.Model):
    """
    EventTeam serves as a join model between Events and Teams, indicating that
    a team will or has competed in an Event.
    key_name is like 2010cmp_frc177 or 2007ct_frc195
    """
    event = ndb.KeyProperty(kind=Event)
    team = ndb.KeyProperty(kind=Team)
    year = ndb.IntegerProperty()

    created = ndb.DateTimeProperty(auto_now_add=True, indexed=False)
    updated = ndb.DateTimeProperty(auto_now=True, indexed=False)

    def __init__(self, *args, **kw):
        # store set of affected references referenced keys for cache clearing
        # keys must be model properties
        self._affected_references = {
            'event': set(),
            'team': set(),
            'year': set(),
        }
        super(EventTeam, self).__init__(*args, **kw)

    @property
    def key_name(self):
        return self.event.id() + "_" + self.team.id()

########NEW FILE########
__FILENAME__ = insight
import json
from google.appengine.ext import ndb


class Insight(ndb.Model):
    """
    Insights are the end result of analyzing a batch of data, such as the
    average score for all matches in a year.
    key_name is like '2012insights_matchavg'
    """

    MATCH_HIGHSCORE = 0
    MATCH_HIGHSCORE_BY_WEEK = 1
    MATCH_AVERAGES_BY_WEEK = 2
    ELIM_MATCH_AVERAGES_BY_WEEK = 3
    SCORE_DISTRIBUTION = 4
    ELIM_SCORE_DISTRIBUTION = 5
    NUM_MATCHES = 6
    BLUE_BANNERS = 7
    CA_WINNER = 8
    RCA_WINNERS = 9
    WORLD_CHAMPIONS = 10
    WORLD_FINALISTS = 11
    DIVISION_WINNERS = 12
    DIVISION_FINALISTS = 13
    REGIONAL_DISTRICT_WINNERS = 14

    # Used for datastore keys! Don't change unless you know what you're doing.
    INSIGHT_NAMES = {MATCH_HIGHSCORE: 'match_highscore',
                     MATCH_HIGHSCORE_BY_WEEK: 'match_highscore_by_week',
                     MATCH_AVERAGES_BY_WEEK: 'match_averages_by_week',
                     ELIM_MATCH_AVERAGES_BY_WEEK: 'elim_match_averages_by_week',
                     SCORE_DISTRIBUTION: 'score_distribution',
                     ELIM_SCORE_DISTRIBUTION: 'elim_score_distribution',
                     NUM_MATCHES: 'num_matches',
                     BLUE_BANNERS: 'blue_banners',
                     CA_WINNER: 'ca_winner',
                     RCA_WINNERS: 'rca_winners',
                     WORLD_CHAMPIONS: 'world_champions',
                     WORLD_FINALISTS: 'world_finalists',
                     DIVISION_WINNERS: 'division_winners',
                     DIVISION_FINALISTS: 'division_finalists',
                     REGIONAL_DISTRICT_WINNERS: 'regional_district_winners',
                     }

    name = ndb.StringProperty(required=True)  # general name used for sorting
    year = ndb.IntegerProperty(required=True)  # year this insight pertains to. year = 0 for overall insights
    data_json = ndb.StringProperty(required=True, indexed=False)  # JSON dictionary of the data of the insight

    created = ndb.DateTimeProperty(auto_now_add=True, indexed=False)
    updated = ndb.DateTimeProperty(auto_now=True, indexed=False)

    def __init__(self, *args, **kw):
        self._data = None
        super(Insight, self).__init__(*args, **kw)

    @property
    def data(self):
        """
        Lazy load data_json as an OrderedDict
        """
        if self._data is None:
            self._data = json.loads(self.data_json)
        return self._data

    @property
    def key_name(self):
        """
        Returns the string of the key_name of the Insight object before writing it.
        """
        return self.renderKeyName(self.year, self.name)

    @classmethod
    def renderKeyName(cls, year, name):
        if year == 0:
            return 'insights' + '_' + str(name)
        else:
            return str(year) + 'insights' + '_' + str(name)

########NEW FILE########
__FILENAME__ = match
import json
import re

from google.appengine.ext import ndb

from helpers.tbavideo_helper import TBAVideoHelper
from models.event import Event
from models.team import Team


class Match(ndb.Model):
    """
    Matches represent individual matches at Events.
    Matches have many Videos.
    Matches have many Alliances.
    key_name is like 2010ct_qm10 or 2010ct_sf1m2
    """

    COMP_LEVELS = ["qm", "ef", "qf", "sf", "f"]
    ELIM_LEVELS = ["ef", "qf", "sf", "f"]
    COMP_LEVELS_VERBOSE = {
        "qm": "Quals",
        "ef": "Eighths",
        "qf": "Quarters",
        "sf": "Semis",
        "f": "Finals",
    }
    COMP_LEVELS_PLAY_ORDER = {
        'qm': 1,
        'ef': 2,
        'qf': 3,
        'sf': 4,
        'f': 5,
    }

    FRC_GAMES = [
        "frc_2012_rebr",
        "frc_2011_logo",
        "frc_2010_bkwy",
        "frc_2009_lncy",
        "frc_2008_ovdr",
        "frc_2007_rkrl",
        "frc_2006_amhi",
        "frc_2005_trpl",
        "frc_2004_frnz",
        "frc_2003_stck",
        "frc_2002_znzl",
        "frc_2001_dbdy",
        "frc_2000_coop",
        "frc_1999_trbl",
        "frc_1998_lddr",
        "frc_1997_trdt",
        "frc_1996_hxgn",
        "frc_1995_rmpr",
        "frc_1994_tpwr",
        "frc_1993_rgrg",
        "frc_1992_maiz",
        "frc_unknown",
    ]

    FRC_GAMES_BY_YEAR = {
        2012: "frc_2012_rebr",
        2011: "frc_2011_logo",
        2010: "frc_2010_bkwy",
        2009: "frc_2009_lncy",
        2008: "frc_2008_ovdr",
        2007: "frc_2007_rkrl",
        2006: "frc_2006_amhi",
        2005: "frc_2005_trpl",
        2004: "frc_2004_frnz",
        2003: "frc_2003_stck",
        2002: "frc_2002_znzl",
        2001: "frc_2001_dbdy",
        2000: "frc_2000_coop",
        1999: "frc_1999_trbl",
        1998: "frc_1998_lddr",
        1997: "frc_1997_trdt",
        1996: "frc_1996_hxgn",
        1995: "frc_1995_rmpr",
        1994: "frc_1994_tpwr",
        1993: "frc_1993_rgrg",
        1992: "frc_1992_maiz",
    }

    alliances_json = ndb.StringProperty(required=True, indexed=False)  # JSON dictionary with alliances and scores.

    # {
    # "red": {
    # "teams": ["frc177", "frc195", "frc125"], # These are Team keys
    #    "score": 25
    # },
    # "blue": {
    #    "teams": ["frc433", "frc254", "frc222"],
    #    "score": 12
    # }
    # }

    comp_level = ndb.StringProperty(required=True, choices=set(COMP_LEVELS))
    event = ndb.KeyProperty(kind=Event, required=True)
    game = ndb.StringProperty(required=True, choices=set(FRC_GAMES), indexed=False)
    match_number = ndb.IntegerProperty(required=True, indexed=False)
    no_auto_update = ndb.BooleanProperty(default=False, indexed=False)  # Set to True after manual update
    set_number = ndb.IntegerProperty(required=True, indexed=False)
    team_key_names = ndb.StringProperty(repeated=True)  # list of teams in Match, for indexing.
    time = ndb.DateTimeProperty()  # UTC
    time_string = ndb.StringProperty(indexed=False)  # the time as displayed on FIRST's site (event's local time)
    youtube_videos = ndb.StringProperty(repeated=True)  # list of Youtube IDs
    tba_videos = ndb.StringProperty(repeated=True)  # list of filetypes a TBA video exists for

    created = ndb.DateTimeProperty(auto_now_add=True, indexed=False)
    updated = ndb.DateTimeProperty(auto_now=True)

    def __init__(self, *args, **kw):
        # store set of affected references referenced keys for cache clearing
        # keys must be model properties
        self._affected_references = {
            'event': set(),
            'team_keys': set(),
            'year': set(),
        }
        self._alliances = None
        self._tba_video = None
        self._winning_alliance = None
        self._youtube_videos = None
        super(Match, self).__init__(*args, **kw)

    @property
    def alliances(self):
        """
        Lazy load alliances_json
        """
        if self._alliances is None:
            self._alliances = json.loads(self.alliances_json)

            # score types are inconsistent in the db. convert everything to ints for now.
            for color in ['red', 'blue']:
                score = self._alliances[color]['score']
                if score is None:
                    self._alliances[color]['score'] = -1
                else:
                    self._alliances[color]['score'] = int(score)

        return self._alliances

    @property
    def winning_alliance(self):
        if self._winning_alliance is None:
            highest_score = 0
            for alliance in self.alliances:
                if int(self.alliances[alliance]["score"]) > highest_score:
                    highest_score = int(self.alliances[alliance]["score"])
                    self._winning_alliance = alliance
                elif int(self.alliances[alliance]["score"]) == highest_score:
                    self._winning_alliance = ""
        return self._winning_alliance

    @property
    def event_key_name(self):
        return self.event.id()

    @property
    def team_keys(self):
        return [ndb.Key(Team, team_key_name) for team_key_name in self.team_key_names]

    @property
    def year(self):
        return self.event.id()[:4]

    @property
    def key_name(self):
        return self.renderKeyName(self.event_key_name, self.comp_level, self.set_number, self.match_number)

    @property
    def has_been_played(self):
        """If there are scores, it's been played"""
        for alliance in self.alliances:
            if (self.alliances[alliance]["score"] == None) or \
            (self.alliances[alliance]["score"] == -1):
                return False
        return True

    @property
    def verbose_name(self):
        if self.comp_level == "qm" or self.comp_level == "f":
            return "%s %s" % (self.COMP_LEVELS_VERBOSE[self.comp_level], self.match_number)
        else:
            return "%s %s Match %s" % (self.COMP_LEVELS_VERBOSE[self.comp_level], self.set_number, self.match_number)

    @property
    def short_name(self):
        if self.comp_level == "qm":
            return "Q%s" % self.match_number
        elif self.comp_level == "f":
            return "F%s" % self.match_number
        else:
            return "%s%s-%s" % (self.comp_level.upper(), self.set_number, self.match_number)

    @property
    def has_video(self):
        return (len(self.youtube_videos) + len(self.tba_videos)) > 0

    @property
    def details_url(self):
        return "/match/%s" % self.key_name

    @property
    def tba_video(self):
        if len(self.tba_videos) > 0:
            if self._tba_video is None:
                self._tba_video = TBAVideoHelper(self)
        return self._tba_video

    @property
    def play_order(self):
        return self.COMP_LEVELS_PLAY_ORDER[self.comp_level] * 1000000 + self.match_number * 1000 + self.set_number

    @property
    def name(self):
        return "%s" % (self.COMP_LEVELS_VERBOSE[self.comp_level])

    @property
    def youtube_videos_formatted(self):
        """
        Get youtube video ids formatted for embedding
        """
        if self._youtube_videos is None:
            self._youtube_videos = []
            for video in self.youtube_videos:
                if '#t=' in video:  # Old style-timetamp, convert it!
                    sp = video.split('#t=')
                    video_id = sp[0]
                    old_ts = sp[1]
                    match = re.match('((?P<hour>\d*?)h)?((?P<min>\d*?)m)?((?P<sec>\d*)s?)?', old_ts).groupdict()
                    hours = match['hour'] or 0
                    minutes = match['min'] or 0
                    seconds = match['sec'] or 0
                    total_seconds = (int(hours) * 3600) + (int(minutes) * 60) + int(seconds)
                    video = '%s?start=%i' % (video_id, total_seconds)
                self._youtube_videos.append(video)
        return self._youtube_videos

    @property
    def videos(self):
        videos = []
        for v in self.youtube_videos_formatted:
            videos.append({"type": "youtube", "key": v})
        if self.tba_video is not None:
            tba_path = self.tba_video.streamable_path
            if tba_path is not None:
                videos.append({"type": "tba", "key": tba_path})
        return videos

    @classmethod
    def renderKeyName(self, event_key_name, comp_level, set_number, match_number):
        if comp_level == "qm":
            return "%s_qm%s" % (event_key_name, match_number)
        else:
            return "%s_%s%sm%s" % (event_key_name, comp_level, set_number, match_number)

    @classmethod
    def validate_key_name(self, match_key):
        key_name_regex = re.compile(r'^[1-9]\d{3}[a-z]+\_(?:qm|ef|qf\dm|sf\dm|f\dm)\d+$')
        match = re.match(key_name_regex, match_key)
        return True if match else False

########NEW FILE########
__FILENAME__ = media
import json

from google.appengine.ext import ndb

from consts.media_type import MediaType
from models.team import Team


class Media(ndb.Model):
    """
    The Media model represents different forms of media, such as YouTube Videos
    or ChiefDelphi photos, that are associated with other models, such as Teams.
    """

    # Do not change! key_names are generated based on this
    SLUG_NAMES = {
        MediaType.YOUTUBE: 'youtube',
        MediaType.CD_PHOTO_THREAD: 'cdphotothread',
    }

    REFERENCE_MAP = {
        'team': Team
    }

    # media_type and foreign_key make up the key_name
    media_type_enum = ndb.IntegerProperty(required=True)
    foreign_key = ndb.StringProperty(required=True)  # Unique id for the particular media type. Ex: the Youtube Video key at the end of a YouTube url

    details_json = ndb.StringProperty()  # Additional details required for rendering
    year = ndb.IntegerProperty()  # None if year is not relevant
    references = ndb.KeyProperty(repeated=True)  # Other models that are linked to this object

    created = ndb.DateTimeProperty(auto_now_add=True, indexed=False)
    updated = ndb.DateTimeProperty(auto_now=True, indexed=False)

    def __init__(self, *args, **kw):
        self._details = None
        super(Media, self).__init__(*args, **kw)

    @property
    def details(self):
        if self._details is None:
            self._details = json.loads(self.details_json)
        return self._details

    @classmethod
    def create_reference(self, reference_type, reference_key):
        return ndb.Key(self.REFERENCE_MAP[reference_type], reference_key)

    @property
    def key_name(self):
        return self.render_key_name(self.media_type_enum, self.foreign_key)

    @property
    def slug_name(self):
        return self.SLUG_NAMES[self.media_type_enum]

    @classmethod
    def render_key_name(self, media_type_enum, foreign_key):
        return "{}_{}".format(self.SLUG_NAMES[media_type_enum], foreign_key)

    # URL renderers
    @property
    def cdphotothread_image_url(self):
        return 'http://www.chiefdelphi.com/media/img/{}'.format(self.details['image_partial'])

    @property
    def cdphotothread_image_url_med(self):
        return self.cdphotothread_image_url.replace('_l', '_m')

    @property
    def cdphotothread_thread_url(self):
        return 'http://www.chiefdelphi.com/media/photos/{}'.format(self.foreign_key)

    @property
    def youtube_url(self):
        return 'http://www.youtube.com/embed/{}'.format(self.foreign_key)

########NEW FILE########
__FILENAME__ = sitevar
import json

from google.appengine.ext import ndb


class Sitevar(ndb.Model):
    """
    Sitevars represent site configuration parameters that should be adjustable
    without requiring a code push. They may be used to store secret information
    such as API keys and secrets since only app admins can read them.

    Code should assume sitevars may not come back from the datastore, in which
    case their value should be treated as dict(). Otherwise, sitevar should
    contain a json blob that contain one or more keys with values. They are
    manually edited by site administrators in the admin console.
    """
    description = ndb.StringProperty(indexed=False)
    values_json = ndb.StringProperty(indexed=False)  # a json blob

    created = ndb.DateTimeProperty(auto_now_add=True, indexed=False)
    updated = ndb.DateTimeProperty(auto_now=True, indexed=False)

    def __init__(self, *args, **kw):
        self._contents = None
        super(Sitevar, self).__init__(*args, **kw)

    @property
    def contents(self):
        """
        Lazy load values_json
        """
        if self._contents is None:
            self._contents = json.loads(self.values_json)
        return self._contents

    @contents.setter
    def contents(self, contents):
        self._contents = contents
        self.values_json = json.puts(self._contents)

########NEW FILE########
__FILENAME__ = suggestion
import json

from google.appengine.ext import ndb

from models.account import Account


class Suggestion(ndb.Model):
    """
    Suggestions are generic containers for user-submitted data corrections to
    the site. The generally store a model, a key, and then a json blob of
    fields to append or ammend in the model.
    """
    MODELS = set(["event", "match", "media"])
    REVIEW_ACCEPTED = 1
    REVIEW_PENDING = 0
    REVIEW_REJECTED = -1

    review_state = ndb.IntegerProperty(default=0)
    reviewed_at = ndb.DateTimeProperty()
    reviewer = ndb.KeyProperty(kind=Account)
    author = ndb.KeyProperty(kind=Account, required=True)
    contents_json = ndb.StringProperty(indexed=False)  # a json blob
    target_key = ndb.StringProperty()  # "2012cmp"
    target_model = ndb.StringProperty(choices=MODELS, required=True)  # "event"

    created = ndb.DateTimeProperty(auto_now_add=True, indexed=False)
    updated = ndb.DateTimeProperty(auto_now=True, indexed=False)

    def __init__(self, *args, **kw):
        self._contents = None
        super(Suggestion, self).__init__(*args, **kw)

    @property
    def contents(self):
        """
        Lazy load contents_json
        """
        if self._contents is None:
            self._contents = json.loads(self.contents_json)
        return self._contents

    @contents.setter
    def contents(self, contents):
        self._contents = contents
        self.contents_json = json.dumps(self._contents)

    @property
    def youtube_video(self):
        if "youtube_videos" in self.contents:
            return self.contents["youtube_videos"][0]

########NEW FILE########
__FILENAME__ = team
import logging
import re

from google.appengine.ext import ndb


class Team(ndb.Model):
    """
    Teams represent FIRST Robotics Competition teams.
    key_name is like 'frc177'
    """
    team_number = ndb.IntegerProperty(required=True)
    name = ndb.StringProperty(indexed=False)
    nickname = ndb.StringProperty(indexed=False)
    address = ndb.StringProperty(indexed=False)  # in the format "locality, region, country". similar to Event.location
    website = ndb.StringProperty(indexed=False)
    first_tpid = ndb.IntegerProperty()  # from USFIRST. FIRST team ID number. -greg 5/20/2010
    first_tpid_year = ndb.IntegerProperty()  # from USFIRST. Year tpid is applicable for. -greg 9 Jan 2011
    rookie_year = ndb.IntegerProperty()

    created = ndb.DateTimeProperty(auto_now_add=True, indexed=False)
    updated = ndb.DateTimeProperty(auto_now=True, indexed=False)

    def __init__(self, *args, **kw):
        # store set of affected references referenced keys for cache clearing
        # keys must be model properties
        self._affected_references = {
            'key': set(),
        }
        self._country_name = None
        self._locality = None
        self._location = None
        self._region = None
        super(Team, self).__init__(*args, **kw)

    @property
    def country_name(self):
        if not self._country_name:
            self.split_address()
        return self._country_name

    @property
    def locality(self):
        if not self._locality:
            self.split_address()
        return self._locality

    @property
    def region(self):
        if not self._region:
            self.split_address()
        return self._region

    def split_address(self):
        """
        Return the various parts of a team address.
        Start like, 'South Windsor, CT USA'
        """
        try:
            if self.address is not None:
                address_parts = self.address.split(",")
                if len(address_parts) == 3:
                    self._country_name = address_parts.pop().strip()
                    self._region = address_parts.pop().strip()
                    self._locality = address_parts.pop().strip()
                if len(address_parts) == 2:
                    region_country = address_parts.pop().strip().split(" ")
                    if len(region_country) == 2:
                        self._country_name = region_country.pop().strip()
                    self._region = region_country.pop().strip()
                    self._locality = address_parts.pop().strip()
        except Exception, e:
            logging.warning("Error on team.split_address: %s", e)

    @property
    def location(self):
        if not self._location:
            location_parts = list()
            if self.locality:
                location_parts.append(self.locality)
            if self.region:
                location_parts.append(self.region)
            if self.country_name:
                location_parts.append(self.country_name)
            if len(location_parts) > 0:
                self._location = ", ".join(location_parts)
            else:
                self._location = None
        return self._location

    @property
    def details_url(self):
        return "/team/%s" % self.team_number

    @property
    def key_name(self):
        return "frc%s" % self.team_number

    @classmethod
    def validate_key_name(self, team_key):
        key_name_regex = re.compile(r'^frc[1-9]\d*$')
        match = re.match(key_name_regex, team_key)
        return True if match else False

########NEW FILE########
__FILENAME__ = typeahead_entry
from google.appengine.ext import ndb


class TypeaheadEntry(ndb.Model):
    """
    Model for storing precomputed typeahead entries as keys and values, where
    TypeaheadEntry.id (set in cron_controller.TypeaheadCalcDo) is the key and
    TypeaheadEntry.data_json is the value.
    """
    ALL_TEAMS_KEY = 'teams-all'
    ALL_EVENTS_KEY = 'events-all'
    YEAR_EVENTS_KEY = 'events-{}'

    data_json = ndb.StringProperty(required=True, indexed=False)

    created = ndb.DateTimeProperty(auto_now_add=True, indexed=False)
    updated = ndb.DateTimeProperty(auto_now=True, indexed=False)

########NEW FILE########
__FILENAME__ = user
from google.appengine.ext import db


class User(db.Model):
    # TBA ID
    id = db.StringProperty(required=True)
    # Created date and updated date
    created = db.DateTimeProperty(auto_now_add=True)
    updated = db.DateTimeProperty(auto_now=True)
    # User's full name
    name = db.StringProperty(required=True)
    # FB access token
    access_token = db.StringProperty(required=True)

########NEW FILE########
__FILENAME__ = pavement
# Easy paver commands for less command typing and more coding.
# Visit http://paver.github.com/paver/ to get started. - @brandondean Sept. 30
import subprocess
import json
import time
from paver.easy import *

path = path("./")

@task
def deploy():
  sh("python deploy.py")

@task
def javascript():
    """Combine Compress Javascript"""
    print("Combining and Compressing Javascript")
    sh("python do_compress.py js")


@task
def less():
  """Build and Combine CSS"""
  print("Building and Combining CSS")
  sh("lessc static/css/less_css/tba_style.main.less static/css/less_css/tba_style.main.css")
  sh("lessc static/css/less_css/tba_style.gameday.less static/css/less_css/tba_style.gameday.css")
  sh("python do_compress.py css")


@task
def lint():
  sh("python linter.py")


@task
def make():
  javascript()
  less()

  git_branch_name = subprocess.check_output(["git", "rev-parse", "--abbrev-ref", "HEAD"])
  git_last_commit = subprocess.check_output(["git", "log", "-1"])
  build_time = time.ctime()
  data = {'git_branch_name': git_branch_name,
          'git_last_commit': git_last_commit,
          'build_time': build_time}
  with open('version_info.json', 'w') as f:
      f.write(json.dumps(data))


@task
def preflight():
  """Prep a prod push"""
  test_function([])
  make()


@task
def setup():
  """Set up for development environments."""
  setup_function()


@task
@consume_args
def test(args):
  """Run tests. Accepts an argument to match subnames of tests"""
  test_function(args)


@task
def test_fast():
  """Run tests that don't require HTTP"""
  print("Running Fast Tests")
  sh("python run_tests.py --test_pattern=test_math_*.py")
  sh("python run_tests.py --test_pattern=test_*parser*.py")
  sh("python run_tests.py --test_pattern=test_*manipulator.py")
  sh("python run_tests.py --test_pattern=test_*api.py")
  sh("python run_tests.py --test_pattern=test_event.py")
  sh("python run_tests.py --test_pattern=test_match_cleanup.py")
  sh("python run_tests.py --test_pattern=test_event_group_by_week.py")
  sh("python run_tests.py --test_pattern=test_event_team_repairer.py")
  sh("python run_tests.py --test_pattern=test_event_team_updater.py")
  sh("python run_tests.py --test_pattern=test_event_get_short_name.py")


def setup_function():
  make()

  print("Set up test data at http://localhost:8088/admin/")
  print("1/ Click 'Get Teams' and 'Create Test Events'")
  print("2/ Click 'Create Test Events'")


def test_function(args):
  print("Running Tests")

  test_pattern = ""
  if len(args) > 0:
    test_pattern = " --test_pattern=*%s*" % args[0]

  sh("python run_tests.py%s 2> test_failures.temp" % test_pattern)

########NEW FILE########
__FILENAME__ = exceptions
'''
Custom exceptions raised by pytz.
'''

__all__ = [
    'UnknownTimeZoneError', 'InvalidTimeError', 'AmbiguousTimeError',
    'NonExistentTimeError',
    ]


class UnknownTimeZoneError(KeyError):
    '''Exception raised when pytz is passed an unknown timezone.

    >>> isinstance(UnknownTimeZoneError(), LookupError)
    True

    This class is actually a subclass of KeyError to provide backwards
    compatibility with code relying on the undocumented behavior of earlier
    pytz releases.

    >>> isinstance(UnknownTimeZoneError(), KeyError)
    True
    '''
    pass


class InvalidTimeError(Exception):
    '''Base class for invalid time exceptions.'''


class AmbiguousTimeError(InvalidTimeError):
    '''Exception raised when attempting to create an ambiguous wallclock time.

    At the end of a DST transition period, a particular wallclock time will
    occur twice (once before the clocks are set back, once after). Both
    possibilities may be correct, unless further information is supplied.

    See DstTzInfo.normalize() for more info
    '''


class NonExistentTimeError(InvalidTimeError):
    '''Exception raised when attempting to create a wallclock time that
    cannot exist.

    At the start of a DST transition period, the wallclock time jumps forward.
    The instants jumped over never occur.
    '''

########NEW FILE########
__FILENAME__ = lazy
from threading import RLock
try:
    from UserDict import DictMixin
except ImportError:
    from collections import Mapping as DictMixin


# With lazy loading, we might end up with multiple threads triggering
# it at the same time. We need a lock.
_fill_lock = RLock()


class LazyDict(DictMixin):
    """Dictionary populated on first use."""
    data = None
    def __getitem__(self, key):
        if self.data is None:
            _fill_lock.acquire()
            try:
                if self.data is None:
                    self._fill()
            finally:
                _fill_lock.release()
        return self.data[key.upper()]

    def __contains__(self, key):
        if self.data is None:
            _fill_lock.acquire()
            try:
                if self.data is None:
                    self._fill()
            finally:
                _fill_lock.release()
        return key in self.data

    def __iter__(self):
        if self.data is None:
            _fill_lock.acquire()
            try:
                if self.data is None:
                    self._fill()
            finally:
                _fill_lock.release()
        return iter(self.data)

    def __len__(self):
        if self.data is None:
            _fill_lock.acquire()
            try:
                if self.data is None:
                    self._fill()
            finally:
                _fill_lock.release()
        return len(self.data)

    def keys(self):
        if self.data is None:
            _fill_lock.acquire()
            try:
                if self.data is None:
                    self._fill()
            finally:
                _fill_lock.release()
        return self.data.keys()


class LazyList(list):
    """List populated on first use."""

    _props = [
        '__str__', '__repr__', '__unicode__',
        '__hash__', '__sizeof__', '__cmp__',
        '__lt__', '__le__', '__eq__', '__ne__', '__gt__', '__ge__',
        'append', 'count', 'index', 'extend', 'insert', 'pop', 'remove',
        'reverse', 'sort', '__add__', '__radd__', '__iadd__', '__mul__',
        '__rmul__', '__imul__', '__contains__', '__len__', '__nonzero__',
        '__getitem__', '__setitem__', '__delitem__', '__iter__',
        '__reversed__', '__getslice__', '__setslice__', '__delslice__']

    def __new__(cls, fill_iter=None):

        if fill_iter is None:
            return list()

        # We need a new class as we will be dynamically messing with its
        # methods.
        class LazyList(list):
            pass

        fill_iter = [fill_iter]

        def lazy(name):
            def _lazy(self, *args, **kw):
                _fill_lock.acquire()
                try:
                    if len(fill_iter) > 0:
                        list.extend(self, fill_iter.pop())
                        for method_name in cls._props:
                            delattr(LazyList, method_name)
                finally:
                    _fill_lock.release()
                return getattr(list, name)(self, *args, **kw)
            return _lazy

        for name in cls._props:
            setattr(LazyList, name, lazy(name))

        new_list = LazyList()
        return new_list

# Not all versions of Python declare the same magic methods.
# Filter out properties that don't exist in this version of Python
# from the list.
LazyList._props = [prop for prop in LazyList._props if hasattr(list, prop)]


class LazySet(set):
    """Set populated on first use."""

    _props = (
        '__str__', '__repr__', '__unicode__',
        '__hash__', '__sizeof__', '__cmp__',
        '__lt__', '__le__', '__eq__', '__ne__', '__gt__', '__ge__',
        '__contains__', '__len__', '__nonzero__',
        '__getitem__', '__setitem__', '__delitem__', '__iter__',
        '__sub__', '__and__', '__xor__', '__or__',
        '__rsub__', '__rand__', '__rxor__', '__ror__',
        '__isub__', '__iand__', '__ixor__', '__ior__',
        'add', 'clear', 'copy', 'difference', 'difference_update',
        'discard', 'intersection', 'intersection_update', 'isdisjoint',
        'issubset', 'issuperset', 'pop', 'remove',
        'symmetric_difference', 'symmetric_difference_update',
        'union', 'update')

    def __new__(cls, fill_iter=None):

        if fill_iter is None:
            return set()

        class LazySet(set):
            pass

        fill_iter = [fill_iter]

        def lazy(name):
            def _lazy(self, *args, **kw):
                _fill_lock.acquire()
                try:
                    if len(fill_iter) > 0:
                        for i in fill_iter.pop():
                            set.add(self, i)
                        for method_name in cls._props:
                            delattr(LazySet, method_name)
                finally:
                    _fill_lock.release()
                return getattr(set, name)(self, *args, **kw)
            return _lazy

        for name in cls._props:
            setattr(LazySet, name, lazy(name))

        new_set = LazySet()
        return new_set

# Not all versions of Python declare the same magic methods.
# Filter out properties that don't exist in this version of Python
# from the list.
LazySet._props = [prop for prop in LazySet._props if hasattr(set, prop)]

########NEW FILE########
__FILENAME__ = reference
'''
Reference tzinfo implementations from the Python docs.
Used for testing against as they are only correct for the years
1987 to 2006. Do not use these for real code.
'''

from datetime import tzinfo, timedelta, datetime
from pytz import utc, UTC, HOUR, ZERO

# A class building tzinfo objects for fixed-offset time zones.
# Note that FixedOffset(0, "UTC") is a different way to build a
# UTC tzinfo object.

class FixedOffset(tzinfo):
    """Fixed offset in minutes east from UTC."""

    def __init__(self, offset, name):
        self.__offset = timedelta(minutes = offset)
        self.__name = name

    def utcoffset(self, dt):
        return self.__offset

    def tzname(self, dt):
        return self.__name

    def dst(self, dt):
        return ZERO

# A class capturing the platform's idea of local time.

import time as _time

STDOFFSET = timedelta(seconds = -_time.timezone)
if _time.daylight:
    DSTOFFSET = timedelta(seconds = -_time.altzone)
else:
    DSTOFFSET = STDOFFSET

DSTDIFF = DSTOFFSET - STDOFFSET

class LocalTimezone(tzinfo):

    def utcoffset(self, dt):
        if self._isdst(dt):
            return DSTOFFSET
        else:
            return STDOFFSET

    def dst(self, dt):
        if self._isdst(dt):
            return DSTDIFF
        else:
            return ZERO

    def tzname(self, dt):
        return _time.tzname[self._isdst(dt)]

    def _isdst(self, dt):
        tt = (dt.year, dt.month, dt.day,
              dt.hour, dt.minute, dt.second,
              dt.weekday(), 0, -1)
        stamp = _time.mktime(tt)
        tt = _time.localtime(stamp)
        return tt.tm_isdst > 0

Local = LocalTimezone()

# A complete implementation of current DST rules for major US time zones.

def first_sunday_on_or_after(dt):
    days_to_go = 6 - dt.weekday()
    if days_to_go:
        dt += timedelta(days_to_go)
    return dt

# In the US, DST starts at 2am (standard time) on the first Sunday in April.
DSTSTART = datetime(1, 4, 1, 2)
# and ends at 2am (DST time; 1am standard time) on the last Sunday of Oct.
# which is the first Sunday on or after Oct 25.
DSTEND = datetime(1, 10, 25, 1)

class USTimeZone(tzinfo):

    def __init__(self, hours, reprname, stdname, dstname):
        self.stdoffset = timedelta(hours=hours)
        self.reprname = reprname
        self.stdname = stdname
        self.dstname = dstname

    def __repr__(self):
        return self.reprname

    def tzname(self, dt):
        if self.dst(dt):
            return self.dstname
        else:
            return self.stdname

    def utcoffset(self, dt):
        return self.stdoffset + self.dst(dt)

    def dst(self, dt):
        if dt is None or dt.tzinfo is None:
            # An exception may be sensible here, in one or both cases.
            # It depends on how you want to treat them.  The default
            # fromutc() implementation (called by the default astimezone()
            # implementation) passes a datetime with dt.tzinfo is self.
            return ZERO
        assert dt.tzinfo is self

        # Find first Sunday in April & the last in October.
        start = first_sunday_on_or_after(DSTSTART.replace(year=dt.year))
        end = first_sunday_on_or_after(DSTEND.replace(year=dt.year))

        # Can't compare naive to aware objects, so strip the timezone from
        # dt first.
        if start <= dt.replace(tzinfo=None) < end:
            return HOUR
        else:
            return ZERO

Eastern  = USTimeZone(-5, "Eastern",  "EST", "EDT")
Central  = USTimeZone(-6, "Central",  "CST", "CDT")
Mountain = USTimeZone(-7, "Mountain", "MST", "MDT")
Pacific  = USTimeZone(-8, "Pacific",  "PST", "PDT")


########NEW FILE########
__FILENAME__ = test_docs
# -*- coding: ascii -*-

from doctest import DocTestSuite
import unittest, os, os.path, sys
import warnings

# We test the documentation this way instead of using DocFileSuite so
# we can run the tests under Python 2.3
def test_README():
    pass

this_dir = os.path.dirname(__file__)
locs = [
    os.path.join(this_dir, os.pardir, 'README.txt'),
    os.path.join(this_dir, os.pardir, os.pardir, 'README.txt'),
    ]
for loc in locs:
    if os.path.exists(loc):
        test_README.__doc__ = open(loc).read()
        break
if test_README.__doc__ is None:
    raise RuntimeError('README.txt not found')


def test_suite():
    "For the Z3 test runner"
    return DocTestSuite()


if __name__ == '__main__':
    sys.path.insert(0, os.path.abspath(os.path.join(
        this_dir, os.pardir, os.pardir
        )))
    unittest.main(defaultTest='test_suite')



########NEW FILE########
__FILENAME__ = test_lazy
from operator import *
import os.path
import sys
import unittest
import warnings


if __name__ == '__main__':
    # Only munge path if invoked as a script. Testrunners should have setup
    # the paths already
    sys.path.insert(0, os.path.abspath(os.path.join(os.pardir, os.pardir)))


from pytz.lazy import LazyList, LazySet


class LazyListTestCase(unittest.TestCase):
    initial_data = [3,2,1]

    def setUp(self):
        self.base = [3, 2, 1]
        self.lesser = [2, 1, 0]
        self.greater = [4, 3, 2]

        self.lazy = LazyList(iter(list(self.base)))

    def test_unary_ops(self):
        unary_ops = [str, repr, len, bool, not_]
        try:
            unary_ops.append(unicode)
        except NameError:
            pass  # unicode no longer exists in Python 3.

        for op in unary_ops:
            self.assertEqual(
                op(self.lazy),
                op(self.base), str(op))

    def test_binary_ops(self):
        binary_ops = [eq, ge, gt, le, lt, ne, add, concat]
        try:
            binary_ops.append(cmp)
        except NameError:
            pass  # cmp no longer exists in Python 3.

        for op in binary_ops:
            self.assertEqual(
                op(self.lazy, self.lazy),
                op(self.base, self.base), str(op))
            for other in [self.base, self.lesser, self.greater]:
                self.assertEqual(
                    op(self.lazy, other),
                    op(self.base, other), '%s %s' % (op, other))
                self.assertEqual(
                    op(other, self.lazy),
                    op(other, self.base), '%s %s' % (op, other))

        # Multiplication
        self.assertEqual(self.lazy * 3, self.base * 3)
        self.assertEqual(3 * self.lazy, 3 * self.base)

        # Contains
        self.assertTrue(2 in self.lazy)
        self.assertFalse(42 in self.lazy)

    def test_iadd(self):
        self.lazy += [1]
        self.base += [1]
        self.assertEqual(self.lazy, self.base)

    def test_bool(self):
        self.assertTrue(bool(self.lazy))
        self.assertFalse(bool(LazyList()))
        self.assertFalse(bool(LazyList(iter([]))))

    def test_hash(self):
        self.assertRaises(TypeError, hash, self.lazy)

    def test_isinstance(self):
        self.assertTrue(isinstance(self.lazy, list))
        self.assertFalse(isinstance(self.lazy, tuple))

    def test_callable(self):
        try:
            callable
        except NameError:
            return  # No longer exists with Python 3.
        self.assertFalse(callable(self.lazy))

    def test_append(self):
        self.base.append('extra')
        self.lazy.append('extra')
        self.assertEqual(self.lazy, self.base)

    def test_count(self):
        self.assertEqual(self.lazy.count(2), 1)

    def test_index(self):
        self.assertEqual(self.lazy.index(2), 1)

    def test_extend(self):
        self.base.extend([6, 7])
        self.lazy.extend([6, 7])
        self.assertEqual(self.lazy, self.base)

    def test_insert(self):
        self.base.insert(0, 'ping')
        self.lazy.insert(0, 'ping')
        self.assertEqual(self.lazy, self.base)

    def test_pop(self):
        self.assertEqual(self.lazy.pop(), self.base.pop())
        self.assertEqual(self.lazy, self.base)

    def test_remove(self):
        self.base.remove(2)
        self.lazy.remove(2)
        self.assertEqual(self.lazy, self.base)

    def test_reverse(self):
        self.base.reverse()
        self.lazy.reverse()
        self.assertEqual(self.lazy, self.base)

    def test_reversed(self):
        self.assertEqual(list(reversed(self.lazy)), list(reversed(self.base)))

    def test_sort(self):
        self.base.sort()
        self.assertNotEqual(self.lazy, self.base, 'Test data already sorted')
        self.lazy.sort()
        self.assertEqual(self.lazy, self.base)

    def test_sorted(self):
        self.assertEqual(sorted(self.lazy), sorted(self.base))

    def test_getitem(self):
        for idx in range(-len(self.base), len(self.base)):
            self.assertEqual(self.lazy[idx], self.base[idx])

    def test_setitem(self):
        for idx in range(-len(self.base), len(self.base)):
            self.base[idx] = idx + 1000
            self.assertNotEqual(self.lazy, self.base)
            self.lazy[idx] = idx + 1000
            self.assertEqual(self.lazy, self.base)

    def test_delitem(self):
        del self.base[0]
        self.assertNotEqual(self.lazy, self.base)
        del self.lazy[0]
        self.assertEqual(self.lazy, self.base)

        del self.base[-2]
        self.assertNotEqual(self.lazy, self.base)
        del self.lazy[-2]
        self.assertEqual(self.lazy, self.base)

    def test_iter(self):
        self.assertEqual(list(iter(self.lazy)), list(iter(self.base)))

    def test_getslice(self):
        for i in range(-len(self.base), len(self.base)):
            for j in range(-len(self.base), len(self.base)):
                for step in [-1, 1]:
                    self.assertEqual(self.lazy[i:j:step], self.base[i:j:step])

    def test_setslice(self):
        for i in range(-len(self.base), len(self.base)):
            for j in range(-len(self.base), len(self.base)):
                for step in [-1, 1]:
                    replacement = range(0, len(self.base[i:j:step]))
                    self.base[i:j:step] = replacement
                    self.lazy[i:j:step] = replacement
                    self.assertEqual(self.lazy, self.base)

    def test_delslice(self):
        del self.base[0:1]
        del self.lazy[0:1]
        self.assertEqual(self.lazy, self.base)

        del self.base[-1:1:-1]
        del self.lazy[-1:1:-1]
        self.assertEqual(self.lazy, self.base)


class LazySetTestCase(unittest.TestCase):
    initial_data = set([3,2,1])

    def setUp(self):
        self.base = set([3, 2, 1])
        self.lazy = LazySet(iter(set(self.base)))

    def test_unary_ops(self):
        # These ops just need to work.
        unary_ops = [str, repr]
        try:
            unary_ops.append(unicode)
        except NameError:
            pass  # unicode no longer exists in Python 3.

        for op in unary_ops:
            op(self.lazy)  # These ops just need to work.

        # These ops should return identical values as a real set.
        unary_ops = [len, bool, not_]

        for op in unary_ops:
            self.assertEqual(
                op(self.lazy),
                op(self.base), '%s(lazy) == %r' % (op, op(self.lazy)))

    def test_binary_ops(self):
        binary_ops = [eq, ge, gt, le, lt, ne, sub, and_, or_, xor]
        try:
            binary_ops.append(cmp)
        except NameError:
            pass  # cmp no longer exists in Python 3.

        for op in binary_ops:
            self.assertEqual(
                op(self.lazy, self.lazy),
                op(self.base, self.base), str(op))
            self.assertEqual(
                op(self.lazy, self.base),
                op(self.base, self.base), str(op))
            self.assertEqual(
                op(self.base, self.lazy),
                op(self.base, self.base), str(op))

        # Contains
        self.assertTrue(2 in self.lazy)
        self.assertFalse(42 in self.lazy)

    def test_iops(self):
        try:
            iops = [isub, iand, ior, ixor]
        except NameError:
            return  # Don't exist in older Python versions.
        for op in iops:
            # Mutating operators, so make fresh copies.
            lazy = LazySet(self.base)
            base = self.base.copy()
            op(lazy, set([1]))
            op(base, set([1]))
            self.assertEqual(lazy, base, str(op))

    def test_bool(self):
        self.assertTrue(bool(self.lazy))
        self.assertFalse(bool(LazySet()))
        self.assertFalse(bool(LazySet(iter([]))))

    def test_hash(self):
        self.assertRaises(TypeError, hash, self.lazy)

    def test_isinstance(self):
        self.assertTrue(isinstance(self.lazy, set))

    def test_callable(self):
        try:
            callable
        except NameError:
            return  # No longer exists with Python 3.
        self.assertFalse(callable(self.lazy))

    def test_add(self):
        self.base.add('extra')
        self.lazy.add('extra')
        self.assertEqual(self.lazy, self.base)

    def test_copy(self):
        self.assertEqual(self.lazy.copy(), self.base)

    def test_method_ops(self):
        ops = [
            'difference', 'intersection', 'isdisjoint',
            'issubset', 'issuperset', 'symmetric_difference', 'union',
            'difference_update', 'intersection_update',
            'symmetric_difference_update', 'update']
        for op in ops:
            if not hasattr(set, op):
                continue  # Not in this version of Python.
            # Make a copy, as some of the ops are mutating.
            lazy = LazySet(set(self.base))
            base = set(self.base)
            self.assertEqual(
                getattr(self.lazy, op)(set([1])),
                getattr(self.base, op)(set([1])), op)
            self.assertEqual(self.lazy, self.base, op)

    def test_discard(self):
        self.base.discard(1)
        self.assertNotEqual(self.lazy, self.base)
        self.lazy.discard(1)
        self.assertEqual(self.lazy, self.base)

    def test_pop(self):
        self.assertEqual(self.lazy.pop(), self.base.pop())
        self.assertEqual(self.lazy, self.base)

    def test_remove(self):
        self.base.remove(2)
        self.lazy.remove(2)
        self.assertEqual(self.lazy, self.base)

    def test_clear(self):
        self.lazy.clear()
        self.assertEqual(self.lazy, set())


if __name__ == '__main__':
    warnings.simplefilter("error") # Warnings should be fatal in tests.
    unittest.main()

########NEW FILE########
__FILENAME__ = test_tzinfo
# -*- coding: ascii -*-

import sys, os, os.path
import unittest, doctest
try:
    import cPickle as pickle
except ImportError:
    import pickle
from datetime import datetime, time, timedelta, tzinfo
import warnings

if __name__ == '__main__':
    # Only munge path if invoked as a script. Testrunners should have setup
    # the paths already
    sys.path.insert(0, os.path.abspath(os.path.join(os.pardir, os.pardir)))

import pytz
from pytz import reference
from pytz.tzfile import _byte_string
from pytz.tzinfo import DstTzInfo, StaticTzInfo

# I test for expected version to ensure the correct version of pytz is
# actually being tested.
EXPECTED_VERSION='2013.9'
EXPECTED_OLSON_VERSION='2013i'

fmt = '%Y-%m-%d %H:%M:%S %Z%z'

NOTIME = timedelta(0)

# GMT is a tzinfo.StaticTzInfo--the class we primarily want to test--while
# UTC is reference implementation.  They both have the same timezone meaning.
UTC = pytz.timezone('UTC')
GMT = pytz.timezone('GMT')
assert isinstance(GMT, StaticTzInfo), 'GMT is no longer a StaticTzInfo'

def prettydt(dt):
    """datetime as a string using a known format.

    We don't use strftime as it doesn't handle years earlier than 1900
    per http://bugs.python.org/issue1777412
    """
    if dt.utcoffset() >= timedelta(0):
        offset = '+%s' % (dt.utcoffset(),)
    else:
        offset = '-%s' % (-1 * dt.utcoffset(),)
    return '%04d-%02d-%02d %02d:%02d:%02d %s %s' % (
        dt.year, dt.month, dt.day,
        dt.hour, dt.minute, dt.second,
        dt.tzname(), offset)


try:
    unicode
except NameError:
    # Python 3.x doesn't have unicode(), making writing code
    # for Python 2.3 and Python 3.x a pain.
    unicode = str


class BasicTest(unittest.TestCase):

    def testVersion(self):
        # Ensuring the correct version of pytz has been loaded
        self.assertEqual(EXPECTED_VERSION, pytz.__version__,
                'Incorrect pytz version loaded. Import path is stuffed '
                'or this test needs updating. (Wanted %s, got %s)'
                % (EXPECTED_VERSION, pytz.__version__))

        self.assertEqual(EXPECTED_OLSON_VERSION, pytz.OLSON_VERSION,
                'Incorrect pytz version loaded. Import path is stuffed '
                'or this test needs updating. (Wanted %s, got %s)'
                % (EXPECTED_VERSION, pytz.__version__))

    def testGMT(self):
        now = datetime.now(tz=GMT)
        self.assertTrue(now.utcoffset() == NOTIME)
        self.assertTrue(now.dst() == NOTIME)
        self.assertTrue(now.timetuple() == now.utctimetuple())
        self.assertTrue(now==now.replace(tzinfo=UTC))

    def testReferenceUTC(self):
        now = datetime.now(tz=UTC)
        self.assertTrue(now.utcoffset() == NOTIME)
        self.assertTrue(now.dst() == NOTIME)
        self.assertTrue(now.timetuple() == now.utctimetuple())

    def testUnknownOffsets(self):
        # This tzinfo behavior is required to make
        # datetime.time.{utcoffset, dst, tzname} work as documented.

        dst_tz = pytz.timezone('US/Eastern')

        # This information is not known when we don't have a date,
        # so return None per API.
        self.assertTrue(dst_tz.utcoffset(None) is None)
        self.assertTrue(dst_tz.dst(None) is None)
        # We don't know the abbreviation, but this is still a valid
        # tzname per the Python documentation.
        self.assertEqual(dst_tz.tzname(None), 'US/Eastern')

    def clearCache(self):
        pytz._tzinfo_cache.clear()

    def testUnicodeTimezone(self):
        # We need to ensure that cold lookups work for both Unicode
        # and traditional strings, and that the desired singleton is
        # returned.
        self.clearCache()
        eastern = pytz.timezone(unicode('US/Eastern'))
        self.assertTrue(eastern is pytz.timezone('US/Eastern'))

        self.clearCache()
        eastern = pytz.timezone('US/Eastern')
        self.assertTrue(eastern is pytz.timezone(unicode('US/Eastern')))


class PicklingTest(unittest.TestCase):

    def _roundtrip_tzinfo(self, tz):
        p = pickle.dumps(tz)
        unpickled_tz = pickle.loads(p)
        self.assertTrue(tz is unpickled_tz, '%s did not roundtrip' % tz.zone)

    def _roundtrip_datetime(self, dt):
        # Ensure that the tzinfo attached to a datetime instance
        # is identical to the one returned. This is important for
        # DST timezones, as some state is stored in the tzinfo.
        tz = dt.tzinfo
        p = pickle.dumps(dt)
        unpickled_dt = pickle.loads(p)
        unpickled_tz = unpickled_dt.tzinfo
        self.assertTrue(tz is unpickled_tz, '%s did not roundtrip' % tz.zone)

    def testDst(self):
        tz = pytz.timezone('Europe/Amsterdam')
        dt = datetime(2004, 2, 1, 0, 0, 0)

        for localized_tz in tz._tzinfos.values():
            self._roundtrip_tzinfo(localized_tz)
            self._roundtrip_datetime(dt.replace(tzinfo=localized_tz))

    def testRoundtrip(self):
        dt = datetime(2004, 2, 1, 0, 0, 0)
        for zone in pytz.all_timezones:
            tz = pytz.timezone(zone)
            self._roundtrip_tzinfo(tz)

    def testDatabaseFixes(self):
        # Hack the pickle to make it refer to a timezone abbreviation
        # that does not match anything. The unpickler should be able
        # to repair this case
        tz = pytz.timezone('Australia/Melbourne')
        p = pickle.dumps(tz)
        tzname = tz._tzname
        hacked_p = p.replace(_byte_string(tzname), _byte_string('???'))
        self.assertNotEqual(p, hacked_p)
        unpickled_tz = pickle.loads(hacked_p)
        self.assertTrue(tz is unpickled_tz)

        # Simulate a database correction. In this case, the incorrect
        # data will continue to be used.
        p = pickle.dumps(tz)
        new_utcoffset = tz._utcoffset.seconds + 42

        # Python 3 introduced a new pickle protocol where numbers are stored in
        # hexadecimal representation. Here we extract the pickle
        # representation of the number for the current Python version.
        old_pickle_pattern = pickle.dumps(tz._utcoffset.seconds)[3:-1]
        new_pickle_pattern = pickle.dumps(new_utcoffset)[3:-1]
        hacked_p = p.replace(old_pickle_pattern, new_pickle_pattern)

        self.assertNotEqual(p, hacked_p)
        unpickled_tz = pickle.loads(hacked_p)
        self.assertEqual(unpickled_tz._utcoffset.seconds, new_utcoffset)
        self.assertTrue(tz is not unpickled_tz)

    def testOldPickles(self):
        # Ensure that applications serializing pytz instances as pickles
        # have no troubles upgrading to a new pytz release. These pickles
        # where created with pytz2006j
        east1 = pickle.loads(_byte_string(
            "cpytz\n_p\np1\n(S'US/Eastern'\np2\nI-18000\n"
            "I0\nS'EST'\np3\ntRp4\n."
            ))
        east2 = pytz.timezone('US/Eastern')
        self.assertTrue(east1 is east2)

        # Confirm changes in name munging between 2006j and 2007c cause
        # no problems.
        pap1 = pickle.loads(_byte_string(
            "cpytz\n_p\np1\n(S'America/Port_minus_au_minus_Prince'"
            "\np2\nI-17340\nI0\nS'PPMT'\np3\ntRp4\n."))
        pap2 = pytz.timezone('America/Port-au-Prince')
        self.assertTrue(pap1 is pap2)

        gmt1 = pickle.loads(_byte_string(
            "cpytz\n_p\np1\n(S'Etc/GMT_plus_10'\np2\ntRp3\n."))
        gmt2 = pytz.timezone('Etc/GMT+10')
        self.assertTrue(gmt1 is gmt2)


class USEasternDSTStartTestCase(unittest.TestCase):
    tzinfo = pytz.timezone('US/Eastern')

    # 24 hours before DST changeover
    transition_time = datetime(2002, 4, 7, 7, 0, 0, tzinfo=UTC)

    # Increase for 'flexible' DST transitions due to 1 minute granularity
    # of Python's datetime library
    instant = timedelta(seconds=1)

    # before transition
    before = {
        'tzname': 'EST',
        'utcoffset': timedelta(hours = -5),
        'dst': timedelta(hours = 0),
        }

    # after transition
    after = {
        'tzname': 'EDT',
        'utcoffset': timedelta(hours = -4),
        'dst': timedelta(hours = 1),
        }

    def _test_tzname(self, utc_dt, wanted):
        tzname = wanted['tzname']
        dt = utc_dt.astimezone(self.tzinfo)
        self.assertEqual(dt.tzname(), tzname,
            'Expected %s as tzname for %s. Got %s' % (
                tzname, str(utc_dt), dt.tzname()
                )
            )

    def _test_utcoffset(self, utc_dt, wanted):
        utcoffset = wanted['utcoffset']
        dt = utc_dt.astimezone(self.tzinfo)
        self.assertEqual(
                dt.utcoffset(), wanted['utcoffset'],
                'Expected %s as utcoffset for %s. Got %s' % (
                    utcoffset, utc_dt, dt.utcoffset()
                    )
                )

    def _test_dst(self, utc_dt, wanted):
        dst = wanted['dst']
        dt = utc_dt.astimezone(self.tzinfo)
        self.assertEqual(dt.dst(),dst,
            'Expected %s as dst for %s. Got %s' % (
                dst, utc_dt, dt.dst()
                )
            )

    def test_arithmetic(self):
        utc_dt = self.transition_time

        for days in range(-420, 720, 20):
            delta = timedelta(days=days)

            # Make sure we can get back where we started
            dt = utc_dt.astimezone(self.tzinfo)
            dt2 = dt + delta
            dt2 = dt2 - delta
            self.assertEqual(dt, dt2)

            # Make sure arithmetic crossing DST boundaries ends
            # up in the correct timezone after normalization
            utc_plus_delta = (utc_dt + delta).astimezone(self.tzinfo)
            local_plus_delta = self.tzinfo.normalize(dt + delta)
            self.assertEqual(
                    prettydt(utc_plus_delta),
                    prettydt(local_plus_delta),
                    'Incorrect result for delta==%d days.  Wanted %r. Got %r'%(
                        days,
                        prettydt(utc_plus_delta),
                        prettydt(local_plus_delta),
                        )
                    )

    def _test_all(self, utc_dt, wanted):
        self._test_utcoffset(utc_dt, wanted)
        self._test_tzname(utc_dt, wanted)
        self._test_dst(utc_dt, wanted)

    def testDayBefore(self):
        self._test_all(
                self.transition_time - timedelta(days=1), self.before
                )

    def testTwoHoursBefore(self):
        self._test_all(
                self.transition_time - timedelta(hours=2), self.before
                )

    def testHourBefore(self):
        self._test_all(
                self.transition_time - timedelta(hours=1), self.before
                )

    def testInstantBefore(self):
        self._test_all(
                self.transition_time - self.instant, self.before
                )

    def testTransition(self):
        self._test_all(
                self.transition_time, self.after
                )

    def testInstantAfter(self):
        self._test_all(
                self.transition_time + self.instant, self.after
                )

    def testHourAfter(self):
        self._test_all(
                self.transition_time + timedelta(hours=1), self.after
                )

    def testTwoHoursAfter(self):
        self._test_all(
                self.transition_time + timedelta(hours=1), self.after
                )

    def testDayAfter(self):
        self._test_all(
                self.transition_time + timedelta(days=1), self.after
                )


class USEasternDSTEndTestCase(USEasternDSTStartTestCase):
    tzinfo = pytz.timezone('US/Eastern')
    transition_time = datetime(2002, 10, 27, 6, 0, 0, tzinfo=UTC)
    before = {
        'tzname': 'EDT',
        'utcoffset': timedelta(hours = -4),
        'dst': timedelta(hours = 1),
        }
    after = {
        'tzname': 'EST',
        'utcoffset': timedelta(hours = -5),
        'dst': timedelta(hours = 0),
        }


class USEasternEPTStartTestCase(USEasternDSTStartTestCase):
    transition_time = datetime(1945, 8, 14, 23, 0, 0, tzinfo=UTC)
    before = {
        'tzname': 'EWT',
        'utcoffset': timedelta(hours = -4),
        'dst': timedelta(hours = 1),
        }
    after = {
        'tzname': 'EPT',
        'utcoffset': timedelta(hours = -4),
        'dst': timedelta(hours = 1),
        }


class USEasternEPTEndTestCase(USEasternDSTStartTestCase):
    transition_time = datetime(1945, 9, 30, 6, 0, 0, tzinfo=UTC)
    before = {
        'tzname': 'EPT',
        'utcoffset': timedelta(hours = -4),
        'dst': timedelta(hours = 1),
        }
    after = {
        'tzname': 'EST',
        'utcoffset': timedelta(hours = -5),
        'dst': timedelta(hours = 0),
        }


class WarsawWMTEndTestCase(USEasternDSTStartTestCase):
    # In 1915, Warsaw changed from Warsaw to Central European time.
    # This involved the clocks being set backwards, causing a end-of-DST
    # like situation without DST being involved.
    tzinfo = pytz.timezone('Europe/Warsaw')
    transition_time = datetime(1915, 8, 4, 22, 36, 0, tzinfo=UTC)
    before = {
        'tzname': 'WMT',
        'utcoffset': timedelta(hours=1, minutes=24),
        'dst': timedelta(0),
        }
    after = {
        'tzname': 'CET',
        'utcoffset': timedelta(hours=1),
        'dst': timedelta(0),
        }


class VilniusWMTEndTestCase(USEasternDSTStartTestCase):
    # At the end of 1916, Vilnius changed timezones putting its clock
    # forward by 11 minutes 35 seconds. Neither timezone was in DST mode.
    tzinfo = pytz.timezone('Europe/Vilnius')
    instant = timedelta(seconds=31)
    transition_time = datetime(1916, 12, 31, 22, 36, 00, tzinfo=UTC)
    before = {
        'tzname': 'WMT',
        'utcoffset': timedelta(hours=1, minutes=24),
        'dst': timedelta(0),
        }
    after = {
        'tzname': 'KMT',
        'utcoffset': timedelta(hours=1, minutes=36), # Really 1:35:36
        'dst': timedelta(0),
        }


class VilniusCESTStartTestCase(USEasternDSTStartTestCase):
    # In 1941, Vilnius changed from MSG to CEST, switching to summer
    # time while simultaneously reducing its UTC offset by two hours,
    # causing the clocks to go backwards for this summer time
    # switchover.
    tzinfo = pytz.timezone('Europe/Vilnius')
    transition_time = datetime(1941, 6, 23, 21, 00, 00, tzinfo=UTC)
    before = {
        'tzname': 'MSK',
        'utcoffset': timedelta(hours=3),
        'dst': timedelta(0),
        }
    after = {
        'tzname': 'CEST',
        'utcoffset': timedelta(hours=2),
        'dst': timedelta(hours=1),
        }


class LondonHistoryStartTestCase(USEasternDSTStartTestCase):
    # The first known timezone transition in London was in 1847 when
    # clocks where synchronized to GMT. However, we currently only
    # understand v1 format tzfile(5) files which does handle years
    # this far in the past, so our earliest known transition is in
    # 1916.
    tzinfo = pytz.timezone('Europe/London')
    # transition_time = datetime(1847, 12, 1, 1, 15, 00, tzinfo=UTC)
    # before = {
    #     'tzname': 'LMT',
    #     'utcoffset': timedelta(minutes=-75),
    #     'dst': timedelta(0),
    #     }
    # after = {
    #     'tzname': 'GMT',
    #     'utcoffset': timedelta(0),
    #     'dst': timedelta(0),
    #     }
    transition_time = datetime(1916, 5, 21, 2, 00, 00, tzinfo=UTC)
    before = {
        'tzname': 'GMT',
        'utcoffset': timedelta(0),
        'dst': timedelta(0),
        }
    after = {
        'tzname': 'BST',
        'utcoffset': timedelta(hours=1),
        'dst': timedelta(hours=1),
        }


class LondonHistoryEndTestCase(USEasternDSTStartTestCase):
    # Timezone switchovers are projected into the future, even
    # though no official statements exist or could be believed even
    # if they did exist. We currently only check the last known
    # transition in 2037, as we are still using v1 format tzfile(5)
    # files.
    tzinfo = pytz.timezone('Europe/London')
    # transition_time = datetime(2499, 10, 25, 1, 0, 0, tzinfo=UTC)
    transition_time = datetime(2037, 10, 25, 1, 0, 0, tzinfo=UTC)
    before = {
        'tzname': 'BST',
        'utcoffset': timedelta(hours=1),
        'dst': timedelta(hours=1),
        }
    after = {
        'tzname': 'GMT',
        'utcoffset': timedelta(0),
        'dst': timedelta(0),
        }


class NoumeaHistoryStartTestCase(USEasternDSTStartTestCase):
    # Noumea adopted a whole hour offset in 1912. Previously
    # it was 11 hours, 5 minutes and 48 seconds off UTC. However,
    # due to limitations of the Python datetime library, we need
    # to round that to 11 hours 6 minutes.
    tzinfo = pytz.timezone('Pacific/Noumea')
    transition_time = datetime(1912, 1, 12, 12, 54, 12, tzinfo=UTC)
    before = {
        'tzname': 'LMT',
        'utcoffset': timedelta(hours=11, minutes=6),
        'dst': timedelta(0),
        }
    after = {
        'tzname': 'NCT',
        'utcoffset': timedelta(hours=11),
        'dst': timedelta(0),
        }


class NoumeaDSTEndTestCase(USEasternDSTStartTestCase):
    # Noumea dropped DST in 1997.
    tzinfo = pytz.timezone('Pacific/Noumea')
    transition_time = datetime(1997, 3, 1, 15, 00, 00, tzinfo=UTC)
    before = {
        'tzname': 'NCST',
        'utcoffset': timedelta(hours=12),
        'dst': timedelta(hours=1),
        }
    after = {
        'tzname': 'NCT',
        'utcoffset': timedelta(hours=11),
        'dst': timedelta(0),
        }


class NoumeaNoMoreDSTTestCase(NoumeaDSTEndTestCase):
    # Noumea dropped DST in 1997. Here we test that it stops occuring.
    transition_time = (
        NoumeaDSTEndTestCase.transition_time + timedelta(days=365*10))
    before = NoumeaDSTEndTestCase.after
    after = NoumeaDSTEndTestCase.after


class TahitiTestCase(USEasternDSTStartTestCase):
    # Tahiti has had a single transition in its history.
    tzinfo = pytz.timezone('Pacific/Tahiti')
    transition_time = datetime(1912, 10, 1, 9, 58, 16, tzinfo=UTC)
    before = {
        'tzname': 'LMT',
        'utcoffset': timedelta(hours=-9, minutes=-58),
        'dst': timedelta(0),
        }
    after = {
        'tzname': 'TAHT',
        'utcoffset': timedelta(hours=-10),
        'dst': timedelta(0),
        }


class SamoaInternationalDateLineChange(USEasternDSTStartTestCase):
    # At the end of 2011, Samoa will switch from being east of the
    # international dateline to the west. There will be no Dec 30th
    # 2011 and it will switch from UTC-10 to UTC+14.
    tzinfo = pytz.timezone('Pacific/Apia')
    transition_time = datetime(2011, 12, 30, 10, 0, 0, tzinfo=UTC)
    before = {
        'tzname': 'WSDT',
        'utcoffset': timedelta(hours=-10),
        'dst': timedelta(hours=1),
        }
    after = {
        'tzname': 'WSDT',
        'utcoffset': timedelta(hours=14),
        'dst': timedelta(hours=1),
        }


class ReferenceUSEasternDSTStartTestCase(USEasternDSTStartTestCase):
    tzinfo = reference.Eastern
    def test_arithmetic(self):
        # Reference implementation cannot handle this
        pass


class ReferenceUSEasternDSTEndTestCase(USEasternDSTEndTestCase):
    tzinfo = reference.Eastern

    def testHourBefore(self):
        # Python's datetime library has a bug, where the hour before
        # a daylight savings transition is one hour out. For example,
        # at the end of US/Eastern daylight savings time, 01:00 EST
        # occurs twice (once at 05:00 UTC and once at 06:00 UTC),
        # whereas the first should actually be 01:00 EDT.
        # Note that this bug is by design - by accepting this ambiguity
        # for one hour one hour per year, an is_dst flag on datetime.time
        # became unnecessary.
        self._test_all(
                self.transition_time - timedelta(hours=1), self.after
                )

    def testInstantBefore(self):
        self._test_all(
                self.transition_time - timedelta(seconds=1), self.after
                )

    def test_arithmetic(self):
        # Reference implementation cannot handle this
        pass


class LocalTestCase(unittest.TestCase):
    def testLocalize(self):
        loc_tz = pytz.timezone('Europe/Amsterdam')

        loc_time = loc_tz.localize(datetime(1930, 5, 10, 0, 0, 0))
        # Actually +00:19:32, but Python datetime rounds this
        self.assertEqual(loc_time.strftime('%Z%z'), 'AMT+0020')

        loc_time = loc_tz.localize(datetime(1930, 5, 20, 0, 0, 0))
        # Actually +00:19:32, but Python datetime rounds this
        self.assertEqual(loc_time.strftime('%Z%z'), 'NST+0120')

        loc_time = loc_tz.localize(datetime(1940, 5, 10, 0, 0, 0))
        self.assertEqual(loc_time.strftime('%Z%z'), 'NET+0020')

        loc_time = loc_tz.localize(datetime(1940, 5, 20, 0, 0, 0))
        self.assertEqual(loc_time.strftime('%Z%z'), 'CEST+0200')

        loc_time = loc_tz.localize(datetime(2004, 2, 1, 0, 0, 0))
        self.assertEqual(loc_time.strftime('%Z%z'), 'CET+0100')

        loc_time = loc_tz.localize(datetime(2004, 4, 1, 0, 0, 0))
        self.assertEqual(loc_time.strftime('%Z%z'), 'CEST+0200')

        tz = pytz.timezone('Europe/Amsterdam')
        loc_time = loc_tz.localize(datetime(1943, 3, 29, 1, 59, 59))
        self.assertEqual(loc_time.strftime('%Z%z'), 'CET+0100')


        # Switch to US
        loc_tz = pytz.timezone('US/Eastern')

        # End of DST ambiguity check
        loc_time = loc_tz.localize(datetime(1918, 10, 27, 1, 59, 59), is_dst=1)
        self.assertEqual(loc_time.strftime('%Z%z'), 'EDT-0400')

        loc_time = loc_tz.localize(datetime(1918, 10, 27, 1, 59, 59), is_dst=0)
        self.assertEqual(loc_time.strftime('%Z%z'), 'EST-0500')

        self.assertRaises(pytz.AmbiguousTimeError,
            loc_tz.localize, datetime(1918, 10, 27, 1, 59, 59), is_dst=None
            )

        # Start of DST non-existent times
        loc_time = loc_tz.localize(datetime(1918, 3, 31, 2, 0, 0), is_dst=0)
        self.assertEqual(loc_time.strftime('%Z%z'), 'EST-0500')

        loc_time = loc_tz.localize(datetime(1918, 3, 31, 2, 0, 0), is_dst=1)
        self.assertEqual(loc_time.strftime('%Z%z'), 'EDT-0400')

        self.assertRaises(pytz.NonExistentTimeError,
            loc_tz.localize, datetime(1918, 3, 31, 2, 0, 0), is_dst=None
            )

        # Weird changes - war time and peace time both is_dst==True

        loc_time = loc_tz.localize(datetime(1942, 2, 9, 3, 0, 0))
        self.assertEqual(loc_time.strftime('%Z%z'), 'EWT-0400')

        loc_time = loc_tz.localize(datetime(1945, 8, 14, 19, 0, 0))
        self.assertEqual(loc_time.strftime('%Z%z'), 'EPT-0400')

        loc_time = loc_tz.localize(datetime(1945, 9, 30, 1, 0, 0), is_dst=1)
        self.assertEqual(loc_time.strftime('%Z%z'), 'EPT-0400')

        loc_time = loc_tz.localize(datetime(1945, 9, 30, 1, 0, 0), is_dst=0)
        self.assertEqual(loc_time.strftime('%Z%z'), 'EST-0500')

    def testNormalize(self):
        tz = pytz.timezone('US/Eastern')
        dt = datetime(2004, 4, 4, 7, 0, 0, tzinfo=UTC).astimezone(tz)
        dt2 = dt - timedelta(minutes=10)
        self.assertEqual(
                dt2.strftime('%Y-%m-%d %H:%M:%S %Z%z'),
                '2004-04-04 02:50:00 EDT-0400'
                )

        dt2 = tz.normalize(dt2)
        self.assertEqual(
                dt2.strftime('%Y-%m-%d %H:%M:%S %Z%z'),
                '2004-04-04 01:50:00 EST-0500'
                )

    def testPartialMinuteOffsets(self):
        # utcoffset in Amsterdam was not a whole minute until 1937
        # However, we fudge this by rounding them, as the Python
        # datetime library 
        tz = pytz.timezone('Europe/Amsterdam')
        utc_dt = datetime(1914, 1, 1, 13, 40, 28, tzinfo=UTC) # correct
        utc_dt = utc_dt.replace(second=0) # But we need to fudge it
        loc_dt = utc_dt.astimezone(tz)
        self.assertEqual(
                loc_dt.strftime('%Y-%m-%d %H:%M:%S %Z%z'),
                '1914-01-01 14:00:00 AMT+0020'
                )

        # And get back...
        utc_dt = loc_dt.astimezone(UTC)
        self.assertEqual(
                utc_dt.strftime('%Y-%m-%d %H:%M:%S %Z%z'),
                '1914-01-01 13:40:00 UTC+0000'
                )

    def no_testCreateLocaltime(self):
        # It would be nice if this worked, but it doesn't.
        tz = pytz.timezone('Europe/Amsterdam')
        dt = datetime(2004, 10, 31, 2, 0, 0, tzinfo=tz)
        self.assertEqual(
                dt.strftime(fmt),
                '2004-10-31 02:00:00 CET+0100'
                )


class CommonTimezonesTestCase(unittest.TestCase):
    def test_bratislava(self):
        # Bratislava is the default timezone for Slovakia, but our
        # heuristics where not adding it to common_timezones. Ideally,
        # common_timezones should be populated from zone.tab at runtime,
        # but I'm hesitant to pay the startup cost as loading the list
        # on demand whilst remaining backwards compatible seems
        # difficult.
        self.assertTrue('Europe/Bratislava' in pytz.common_timezones)
        self.assertTrue('Europe/Bratislava' in pytz.common_timezones_set)

    def test_us_eastern(self):
        self.assertTrue('US/Eastern' in pytz.common_timezones)
        self.assertTrue('US/Eastern' in pytz.common_timezones_set)

    def test_belfast(self):
        # Belfast uses London time.
        self.assertTrue('Europe/Belfast' in pytz.all_timezones_set)
        self.assertFalse('Europe/Belfast' in pytz.common_timezones)
        self.assertFalse('Europe/Belfast' in pytz.common_timezones_set)


class BaseTzInfoTestCase:
    '''Ensure UTC, StaticTzInfo and DstTzInfo work consistently.

    These tests are run for each type of tzinfo.
    '''
    tz = None  # override
    tz_class = None  # override

    def test_expectedclass(self):
        self.assertTrue(isinstance(self.tz, self.tz_class))

    def test_fromutc(self):
        # naive datetime.
        dt1 = datetime(2011, 10, 31)

        # localized datetime, same timezone.
        dt2 = self.tz.localize(dt1)

        # Both should give the same results. Note that the standard
        # Python tzinfo.fromutc() only supports the second.
        for dt in [dt1, dt2]:
            loc_dt = self.tz.fromutc(dt)
            loc_dt2 = pytz.utc.localize(dt1).astimezone(self.tz)
            self.assertEqual(loc_dt, loc_dt2)

        # localized datetime, different timezone.
        new_tz = pytz.timezone('Europe/Paris')
        self.assertTrue(self.tz is not new_tz)
        dt3 = new_tz.localize(dt1)
        self.assertRaises(ValueError, self.tz.fromutc, dt3)

    def test_normalize(self):
        other_tz = pytz.timezone('Europe/Paris')
        self.assertTrue(self.tz is not other_tz)

        dt = datetime(2012, 3, 26, 12, 0)
        other_dt = other_tz.localize(dt)

        local_dt = self.tz.normalize(other_dt)

        self.assertTrue(local_dt.tzinfo is not other_dt.tzinfo)
        self.assertNotEqual(
            local_dt.replace(tzinfo=None), other_dt.replace(tzinfo=None))

    def test_astimezone(self):
        other_tz = pytz.timezone('Europe/Paris')
        self.assertTrue(self.tz is not other_tz)

        dt = datetime(2012, 3, 26, 12, 0)
        other_dt = other_tz.localize(dt)

        local_dt = other_dt.astimezone(self.tz)

        self.assertTrue(local_dt.tzinfo is not other_dt.tzinfo)
        self.assertNotEqual(
            local_dt.replace(tzinfo=None), other_dt.replace(tzinfo=None))


class OptimizedUTCTestCase(unittest.TestCase, BaseTzInfoTestCase):
    tz = pytz.utc
    tz_class = tz.__class__


class LegacyUTCTestCase(unittest.TestCase, BaseTzInfoTestCase):
    # Deprecated timezone, but useful for comparison tests.
    tz = pytz.timezone('Etc/UTC')
    tz_class = StaticTzInfo


class StaticTzInfoTestCase(unittest.TestCase, BaseTzInfoTestCase):
    tz = pytz.timezone('GMT')
    tz_class = StaticTzInfo


class DstTzInfoTestCase(unittest.TestCase, BaseTzInfoTestCase):
    tz = pytz.timezone('Australia/Melbourne')
    tz_class = DstTzInfo


def test_suite():
    suite = unittest.TestSuite()
    suite.addTest(doctest.DocTestSuite('pytz'))
    suite.addTest(doctest.DocTestSuite('pytz.tzinfo'))
    import test_tzinfo
    suite.addTest(unittest.defaultTestLoader.loadTestsFromModule(test_tzinfo))
    return suite


if __name__ == '__main__':
    warnings.simplefilter("error") # Warnings should be fatal in tests.
    unittest.main(defaultTest='test_suite')


########NEW FILE########
__FILENAME__ = tzfile
#!/usr/bin/env python
'''
$Id: tzfile.py,v 1.8 2004/06/03 00:15:24 zenzen Exp $
'''

try:
    from cStringIO import StringIO
except ImportError:
    from io import StringIO
from datetime import datetime, timedelta
from struct import unpack, calcsize

from pytz.tzinfo import StaticTzInfo, DstTzInfo, memorized_ttinfo
from pytz.tzinfo import memorized_datetime, memorized_timedelta

def _byte_string(s):
    """Cast a string or byte string to an ASCII byte string."""
    return s.encode('US-ASCII')

_NULL = _byte_string('\0')

def _std_string(s):
    """Cast a string or byte string to an ASCII string."""
    return str(s.decode('US-ASCII'))

def build_tzinfo(zone, fp):
    head_fmt = '>4s c 15x 6l'
    head_size = calcsize(head_fmt)
    (magic, format, ttisgmtcnt, ttisstdcnt,leapcnt, timecnt,
        typecnt, charcnt) =  unpack(head_fmt, fp.read(head_size))

    # Make sure it is a tzfile(5) file
    assert magic == _byte_string('TZif'), 'Got magic %s' % repr(magic)

    # Read out the transition times, localtime indices and ttinfo structures.
    data_fmt = '>%(timecnt)dl %(timecnt)dB %(ttinfo)s %(charcnt)ds' % dict(
        timecnt=timecnt, ttinfo='lBB'*typecnt, charcnt=charcnt)
    data_size = calcsize(data_fmt)
    data = unpack(data_fmt, fp.read(data_size))

    # make sure we unpacked the right number of values
    assert len(data) == 2 * timecnt + 3 * typecnt + 1
    transitions = [memorized_datetime(trans)
                   for trans in data[:timecnt]]
    lindexes = list(data[timecnt:2 * timecnt])
    ttinfo_raw = data[2 * timecnt:-1]
    tznames_raw = data[-1]
    del data

    # Process ttinfo into separate structs
    ttinfo = []
    tznames = {}
    i = 0
    while i < len(ttinfo_raw):
        # have we looked up this timezone name yet?
        tzname_offset = ttinfo_raw[i+2]
        if tzname_offset not in tznames:
            nul = tznames_raw.find(_NULL, tzname_offset)
            if nul < 0:
                nul = len(tznames_raw)
            tznames[tzname_offset] = _std_string(
                tznames_raw[tzname_offset:nul])
        ttinfo.append((ttinfo_raw[i],
                       bool(ttinfo_raw[i+1]),
                       tznames[tzname_offset]))
        i += 3

    # Now build the timezone object
    if len(transitions) == 0:
        ttinfo[0][0], ttinfo[0][2]
        cls = type(zone, (StaticTzInfo,), dict(
            zone=zone,
            _utcoffset=memorized_timedelta(ttinfo[0][0]),
            _tzname=ttinfo[0][2]))
    else:
        # Early dates use the first standard time ttinfo
        i = 0
        while ttinfo[i][1]:
            i += 1
        if ttinfo[i] == ttinfo[lindexes[0]]:
            transitions[0] = datetime.min
        else:
            transitions.insert(0, datetime.min)
            lindexes.insert(0, i)

        # calculate transition info
        transition_info = []
        for i in range(len(transitions)):
            inf = ttinfo[lindexes[i]]
            utcoffset = inf[0]
            if not inf[1]:
                dst = 0
            else:
                for j in range(i-1, -1, -1):
                    prev_inf = ttinfo[lindexes[j]]
                    if not prev_inf[1]:
                        break
                dst = inf[0] - prev_inf[0] # dst offset

                # Bad dst? Look further. DST > 24 hours happens when
                # a timzone has moved across the international dateline.
                if dst <= 0 or dst > 3600*3:
                    for j in range(i+1, len(transitions)):
                        stdinf = ttinfo[lindexes[j]]
                        if not stdinf[1]:
                            dst = inf[0] - stdinf[0]
                            if dst > 0:
                                break # Found a useful std time.

            tzname = inf[2]

            # Round utcoffset and dst to the nearest minute or the
            # datetime library will complain. Conversions to these timezones
            # might be up to plus or minus 30 seconds out, but it is
            # the best we can do.
            utcoffset = int((utcoffset + 30) // 60) * 60
            dst = int((dst + 30) // 60) * 60
            transition_info.append(memorized_ttinfo(utcoffset, dst, tzname))

        cls = type(zone, (DstTzInfo,), dict(
            zone=zone,
            _utc_transition_times=transitions,
            _transition_info=transition_info))

    return cls()

if __name__ == '__main__':
    import os.path
    from pprint import pprint
    base = os.path.join(os.path.dirname(__file__), 'zoneinfo')
    tz = build_tzinfo('Australia/Melbourne',
                      open(os.path.join(base,'Australia','Melbourne'), 'rb'))
    tz = build_tzinfo('US/Eastern',
                      open(os.path.join(base,'US','Eastern'), 'rb'))
    pprint(tz._utc_transition_times)
    #print tz.asPython(4)
    #print tz.transitions_mapping

########NEW FILE########
__FILENAME__ = tzinfo
'''Base classes and helpers for building zone specific tzinfo classes'''

from datetime import datetime, timedelta, tzinfo
from bisect import bisect_right
try:
    set
except NameError:
    from sets import Set as set

import pytz
from pytz.exceptions import AmbiguousTimeError, NonExistentTimeError

__all__ = []

_timedelta_cache = {}
def memorized_timedelta(seconds):
    '''Create only one instance of each distinct timedelta'''
    try:
        return _timedelta_cache[seconds]
    except KeyError:
        delta = timedelta(seconds=seconds)
        _timedelta_cache[seconds] = delta
        return delta

_epoch = datetime.utcfromtimestamp(0)
_datetime_cache = {0: _epoch}
def memorized_datetime(seconds):
    '''Create only one instance of each distinct datetime'''
    try:
        return _datetime_cache[seconds]
    except KeyError:
        # NB. We can't just do datetime.utcfromtimestamp(seconds) as this
        # fails with negative values under Windows (Bug #90096)
        dt = _epoch + timedelta(seconds=seconds)
        _datetime_cache[seconds] = dt
        return dt

_ttinfo_cache = {}
def memorized_ttinfo(*args):
    '''Create only one instance of each distinct tuple'''
    try:
        return _ttinfo_cache[args]
    except KeyError:
        ttinfo = (
                memorized_timedelta(args[0]),
                memorized_timedelta(args[1]),
                args[2]
                )
        _ttinfo_cache[args] = ttinfo
        return ttinfo

_notime = memorized_timedelta(0)

def _to_seconds(td):
    '''Convert a timedelta to seconds'''
    return td.seconds + td.days * 24 * 60 * 60


class BaseTzInfo(tzinfo):
    # Overridden in subclass
    _utcoffset = None
    _tzname = None
    zone = None

    def __str__(self):
        return self.zone


class StaticTzInfo(BaseTzInfo):
    '''A timezone that has a constant offset from UTC

    These timezones are rare, as most locations have changed their
    offset at some point in their history
    '''
    def fromutc(self, dt):
        '''See datetime.tzinfo.fromutc'''
        if dt.tzinfo is not None and dt.tzinfo is not self:
            raise ValueError('fromutc: dt.tzinfo is not self')
        return (dt + self._utcoffset).replace(tzinfo=self)

    def utcoffset(self, dt, is_dst=None):
        '''See datetime.tzinfo.utcoffset

        is_dst is ignored for StaticTzInfo, and exists only to
        retain compatibility with DstTzInfo.
        '''
        return self._utcoffset

    def dst(self, dt, is_dst=None):
        '''See datetime.tzinfo.dst

        is_dst is ignored for StaticTzInfo, and exists only to
        retain compatibility with DstTzInfo.
        '''
        return _notime

    def tzname(self, dt, is_dst=None):
        '''See datetime.tzinfo.tzname

        is_dst is ignored for StaticTzInfo, and exists only to
        retain compatibility with DstTzInfo.
        '''
        return self._tzname

    def localize(self, dt, is_dst=False):
        '''Convert naive time to local time'''
        if dt.tzinfo is not None:
            raise ValueError('Not naive datetime (tzinfo is already set)')
        return dt.replace(tzinfo=self)

    def normalize(self, dt, is_dst=False):
        '''Correct the timezone information on the given datetime.

        This is normally a no-op, as StaticTzInfo timezones never have
        ambiguous cases to correct:

        >>> from pytz import timezone
        >>> gmt = timezone('GMT')
        >>> isinstance(gmt, StaticTzInfo)
        True
        >>> dt = datetime(2011, 5, 8, 1, 2, 3, tzinfo=gmt)
        >>> gmt.normalize(dt) is dt
        True

        The supported method of converting between timezones is to use
        datetime.astimezone(). Currently normalize() also works:

        >>> la = timezone('America/Los_Angeles')
        >>> dt = la.localize(datetime(2011, 5, 7, 1, 2, 3))
        >>> fmt = '%Y-%m-%d %H:%M:%S %Z (%z)'
        >>> gmt.normalize(dt).strftime(fmt)
        '2011-05-07 08:02:03 GMT (+0000)'
        '''
        if dt.tzinfo is self:
            return dt
        if dt.tzinfo is None:
            raise ValueError('Naive time - no tzinfo set')
        return dt.astimezone(self)

    def __repr__(self):
        return '<StaticTzInfo %r>' % (self.zone,)

    def __reduce__(self):
        # Special pickle to zone remains a singleton and to cope with
        # database changes. 
        return pytz._p, (self.zone,)


class DstTzInfo(BaseTzInfo):
    '''A timezone that has a variable offset from UTC

    The offset might change if daylight savings time comes into effect,
    or at a point in history when the region decides to change their
    timezone definition.
    '''
    # Overridden in subclass
    _utc_transition_times = None # Sorted list of DST transition times in UTC
    _transition_info = None # [(utcoffset, dstoffset, tzname)] corresponding
                            # to _utc_transition_times entries
    zone = None

    # Set in __init__
    _tzinfos = None
    _dst = None # DST offset

    def __init__(self, _inf=None, _tzinfos=None):
        if _inf:
            self._tzinfos = _tzinfos
            self._utcoffset, self._dst, self._tzname = _inf
        else:
            _tzinfos = {}
            self._tzinfos = _tzinfos
            self._utcoffset, self._dst, self._tzname = self._transition_info[0]
            _tzinfos[self._transition_info[0]] = self
            for inf in self._transition_info[1:]:
                if inf not in _tzinfos:
                    _tzinfos[inf] = self.__class__(inf, _tzinfos)

    def fromutc(self, dt):
        '''See datetime.tzinfo.fromutc'''
        if (dt.tzinfo is not None
            and getattr(dt.tzinfo, '_tzinfos', None) is not self._tzinfos):
            raise ValueError('fromutc: dt.tzinfo is not self')
        dt = dt.replace(tzinfo=None)
        idx = max(0, bisect_right(self._utc_transition_times, dt) - 1)
        inf = self._transition_info[idx]
        return (dt + inf[0]).replace(tzinfo=self._tzinfos[inf])

    def normalize(self, dt):
        '''Correct the timezone information on the given datetime

        If date arithmetic crosses DST boundaries, the tzinfo
        is not magically adjusted. This method normalizes the
        tzinfo to the correct one.

        To test, first we need to do some setup

        >>> from pytz import timezone
        >>> utc = timezone('UTC')
        >>> eastern = timezone('US/Eastern')
        >>> fmt = '%Y-%m-%d %H:%M:%S %Z (%z)'

        We next create a datetime right on an end-of-DST transition point,
        the instant when the wallclocks are wound back one hour.

        >>> utc_dt = datetime(2002, 10, 27, 6, 0, 0, tzinfo=utc)
        >>> loc_dt = utc_dt.astimezone(eastern)
        >>> loc_dt.strftime(fmt)
        '2002-10-27 01:00:00 EST (-0500)'

        Now, if we subtract a few minutes from it, note that the timezone
        information has not changed.

        >>> before = loc_dt - timedelta(minutes=10)
        >>> before.strftime(fmt)
        '2002-10-27 00:50:00 EST (-0500)'

        But we can fix that by calling the normalize method

        >>> before = eastern.normalize(before)
        >>> before.strftime(fmt)
        '2002-10-27 01:50:00 EDT (-0400)'

        The supported method of converting between timezones is to use
        datetime.astimezone(). Currently, normalize() also works:

        >>> th = timezone('Asia/Bangkok')
        >>> am = timezone('Europe/Amsterdam')
        >>> dt = th.localize(datetime(2011, 5, 7, 1, 2, 3))
        >>> fmt = '%Y-%m-%d %H:%M:%S %Z (%z)'
        >>> am.normalize(dt).strftime(fmt)
        '2011-05-06 20:02:03 CEST (+0200)'
        '''
        if dt.tzinfo is None:
            raise ValueError('Naive time - no tzinfo set')

        # Convert dt in localtime to UTC
        offset = dt.tzinfo._utcoffset
        dt = dt.replace(tzinfo=None)
        dt = dt - offset
        # convert it back, and return it
        return self.fromutc(dt)

    def localize(self, dt, is_dst=False):
        '''Convert naive time to local time.

        This method should be used to construct localtimes, rather
        than passing a tzinfo argument to a datetime constructor.

        is_dst is used to determine the correct timezone in the ambigous
        period at the end of daylight savings time.

        >>> from pytz import timezone
        >>> fmt = '%Y-%m-%d %H:%M:%S %Z (%z)'
        >>> amdam = timezone('Europe/Amsterdam')
        >>> dt  = datetime(2004, 10, 31, 2, 0, 0)
        >>> loc_dt1 = amdam.localize(dt, is_dst=True)
        >>> loc_dt2 = amdam.localize(dt, is_dst=False)
        >>> loc_dt1.strftime(fmt)
        '2004-10-31 02:00:00 CEST (+0200)'
        >>> loc_dt2.strftime(fmt)
        '2004-10-31 02:00:00 CET (+0100)'
        >>> str(loc_dt2 - loc_dt1)
        '1:00:00'

        Use is_dst=None to raise an AmbiguousTimeError for ambiguous
        times at the end of daylight savings

        >>> try:
        ...     loc_dt1 = amdam.localize(dt, is_dst=None)
        ... except AmbiguousTimeError:
        ...     print('Ambiguous')
        Ambiguous

        is_dst defaults to False

        >>> amdam.localize(dt) == amdam.localize(dt, False)
        True

        is_dst is also used to determine the correct timezone in the
        wallclock times jumped over at the start of daylight savings time.

        >>> pacific = timezone('US/Pacific')
        >>> dt = datetime(2008, 3, 9, 2, 0, 0)
        >>> ploc_dt1 = pacific.localize(dt, is_dst=True)
        >>> ploc_dt2 = pacific.localize(dt, is_dst=False)
        >>> ploc_dt1.strftime(fmt)
        '2008-03-09 02:00:00 PDT (-0700)'
        >>> ploc_dt2.strftime(fmt)
        '2008-03-09 02:00:00 PST (-0800)'
        >>> str(ploc_dt2 - ploc_dt1)
        '1:00:00'

        Use is_dst=None to raise a NonExistentTimeError for these skipped
        times.

        >>> try:
        ...     loc_dt1 = pacific.localize(dt, is_dst=None)
        ... except NonExistentTimeError:
        ...     print('Non-existent')
        Non-existent
        '''
        if dt.tzinfo is not None:
            raise ValueError('Not naive datetime (tzinfo is already set)')

        # Find the two best possibilities.
        possible_loc_dt = set()
        for delta in [timedelta(days=-1), timedelta(days=1)]:
            loc_dt = dt + delta
            idx = max(0, bisect_right(
                self._utc_transition_times, loc_dt) - 1)
            inf = self._transition_info[idx]
            tzinfo = self._tzinfos[inf]
            loc_dt = tzinfo.normalize(dt.replace(tzinfo=tzinfo))
            if loc_dt.replace(tzinfo=None) == dt:
                possible_loc_dt.add(loc_dt)

        if len(possible_loc_dt) == 1:
            return possible_loc_dt.pop()

        # If there are no possibly correct timezones, we are attempting
        # to convert a time that never happened - the time period jumped
        # during the start-of-DST transition period.
        if len(possible_loc_dt) == 0:
            # If we refuse to guess, raise an exception.
            if is_dst is None:
                raise NonExistentTimeError(dt)

            # If we are forcing the pre-DST side of the DST transition, we
            # obtain the correct timezone by winding the clock forward a few
            # hours.
            elif is_dst:
                return self.localize(
                    dt + timedelta(hours=6), is_dst=True) - timedelta(hours=6)

            # If we are forcing the post-DST side of the DST transition, we
            # obtain the correct timezone by winding the clock back.
            else:
                return self.localize(
                    dt - timedelta(hours=6), is_dst=False) + timedelta(hours=6)


        # If we get this far, we have multiple possible timezones - this
        # is an ambiguous case occuring during the end-of-DST transition.

        # If told to be strict, raise an exception since we have an
        # ambiguous case
        if is_dst is None:
            raise AmbiguousTimeError(dt)

        # Filter out the possiblilities that don't match the requested
        # is_dst
        filtered_possible_loc_dt = [
            p for p in possible_loc_dt
                if bool(p.tzinfo._dst) == is_dst
            ]

        # Hopefully we only have one possibility left. Return it.
        if len(filtered_possible_loc_dt) == 1:
            return filtered_possible_loc_dt[0]

        if len(filtered_possible_loc_dt) == 0:
            filtered_possible_loc_dt = list(possible_loc_dt)

        # If we get this far, we have in a wierd timezone transition
        # where the clocks have been wound back but is_dst is the same
        # in both (eg. Europe/Warsaw 1915 when they switched to CET).
        # At this point, we just have to guess unless we allow more
        # hints to be passed in (such as the UTC offset or abbreviation),
        # but that is just getting silly.
        #
        # Choose the earliest (by UTC) applicable timezone.
        sorting_keys = {}
        for local_dt in filtered_possible_loc_dt:
            key = local_dt.replace(tzinfo=None) - local_dt.tzinfo._utcoffset
            sorting_keys[key] = local_dt
        first_key = sorted(sorting_keys)[0]
        return sorting_keys[first_key]

    def utcoffset(self, dt, is_dst=None):
        '''See datetime.tzinfo.utcoffset

        The is_dst parameter may be used to remove ambiguity during DST
        transitions.

        >>> from pytz import timezone
        >>> tz = timezone('America/St_Johns')
        >>> ambiguous = datetime(2009, 10, 31, 23, 30)

        >>> tz.utcoffset(ambiguous, is_dst=False)
        datetime.timedelta(-1, 73800)

        >>> tz.utcoffset(ambiguous, is_dst=True)
        datetime.timedelta(-1, 77400)

        >>> try:
        ...     tz.utcoffset(ambiguous)
        ... except AmbiguousTimeError:
        ...     print('Ambiguous')
        Ambiguous

        '''
        if dt is None:
            return None
        elif dt.tzinfo is not self:
            dt = self.localize(dt, is_dst)
            return dt.tzinfo._utcoffset
        else:
            return self._utcoffset

    def dst(self, dt, is_dst=None):
        '''See datetime.tzinfo.dst

        The is_dst parameter may be used to remove ambiguity during DST
        transitions.

        >>> from pytz import timezone
        >>> tz = timezone('America/St_Johns')

        >>> normal = datetime(2009, 9, 1)

        >>> tz.dst(normal)
        datetime.timedelta(0, 3600)
        >>> tz.dst(normal, is_dst=False)
        datetime.timedelta(0, 3600)
        >>> tz.dst(normal, is_dst=True)
        datetime.timedelta(0, 3600)

        >>> ambiguous = datetime(2009, 10, 31, 23, 30)

        >>> tz.dst(ambiguous, is_dst=False)
        datetime.timedelta(0)
        >>> tz.dst(ambiguous, is_dst=True)
        datetime.timedelta(0, 3600)
        >>> try:
        ...     tz.dst(ambiguous)
        ... except AmbiguousTimeError:
        ...     print('Ambiguous')
        Ambiguous

        '''
        if dt is None:
            return None
        elif dt.tzinfo is not self:
            dt = self.localize(dt, is_dst)
            return dt.tzinfo._dst
        else:
            return self._dst

    def tzname(self, dt, is_dst=None):
        '''See datetime.tzinfo.tzname

        The is_dst parameter may be used to remove ambiguity during DST
        transitions.

        >>> from pytz import timezone
        >>> tz = timezone('America/St_Johns')

        >>> normal = datetime(2009, 9, 1)

        >>> tz.tzname(normal)
        'NDT'
        >>> tz.tzname(normal, is_dst=False)
        'NDT'
        >>> tz.tzname(normal, is_dst=True)
        'NDT'

        >>> ambiguous = datetime(2009, 10, 31, 23, 30)

        >>> tz.tzname(ambiguous, is_dst=False)
        'NST'
        >>> tz.tzname(ambiguous, is_dst=True)
        'NDT'
        >>> try:
        ...     tz.tzname(ambiguous)
        ... except AmbiguousTimeError:
        ...     print('Ambiguous')
        Ambiguous
        '''
        if dt is None:
            return self.zone
        elif dt.tzinfo is not self:
            dt = self.localize(dt, is_dst)
            return dt.tzinfo._tzname
        else:
            return self._tzname

    def __repr__(self):
        if self._dst:
            dst = 'DST'
        else:
            dst = 'STD'
        if self._utcoffset > _notime:
            return '<DstTzInfo %r %s+%s %s>' % (
                    self.zone, self._tzname, self._utcoffset, dst
                )
        else:
            return '<DstTzInfo %r %s%s %s>' % (
                    self.zone, self._tzname, self._utcoffset, dst
                )

    def __reduce__(self):
        # Special pickle to zone remains a singleton and to cope with
        # database changes.
        return pytz._p, (
                self.zone,
                _to_seconds(self._utcoffset),
                _to_seconds(self._dst),
                self._tzname
                )



def unpickler(zone, utcoffset=None, dstoffset=None, tzname=None):
    """Factory function for unpickling pytz tzinfo instances.

    This is shared for both StaticTzInfo and DstTzInfo instances, because
    database changes could cause a zones implementation to switch between
    these two base classes and we can't break pickles on a pytz version
    upgrade.
    """
    # Raises a KeyError if zone no longer exists, which should never happen
    # and would be a bug.
    tz = pytz.timezone(zone)

    # A StaticTzInfo - just return it
    if utcoffset is None:
        return tz

    # This pickle was created from a DstTzInfo. We need to
    # determine which of the list of tzinfo instances for this zone
    # to use in order to restore the state of any datetime instances using
    # it correctly.
    utcoffset = memorized_timedelta(utcoffset)
    dstoffset = memorized_timedelta(dstoffset)
    try:
        return tz._tzinfos[(utcoffset, dstoffset, tzname)]
    except KeyError:
        # The particular state requested in this timezone no longer exists.
        # This indicates a corrupt pickle, or the timezone database has been
        # corrected violently enough to make this particular
        # (utcoffset,dstoffset) no longer exist in the zone, or the
        # abbreviation has been changed.
        pass

    # See if we can find an entry differing only by tzname. Abbreviations
    # get changed from the initial guess by the database maintainers to
    # match reality when this information is discovered.
    for localized_tz in tz._tzinfos.values():
        if (localized_tz._utcoffset == utcoffset
                and localized_tz._dst == dstoffset):
            return localized_tz

    # This (utcoffset, dstoffset) information has been removed from the
    # zone. Add it back. This might occur when the database maintainers have
    # corrected incorrect information. datetime instances using this
    # incorrect information will continue to do so, exactly as they were
    # before being pickled. This is purely an overly paranoid safety net - I
    # doubt this will ever been needed in real life.
    inf = (utcoffset, dstoffset, tzname)
    tz._tzinfos[inf] = tz.__class__(inf, tz._tzinfos)
    return tz._tzinfos[inf]


########NEW FILE########
__FILENAME__ = team_renderer
import datetime
import os

from google.appengine.ext import ndb
from google.appengine.ext.webapp import template

from helpers.data_fetchers.team_details_data_fetcher import TeamDetailsDataFetcher

from helpers.award_helper import AwardHelper
from helpers.event_helper import EventHelper
from helpers.match_helper import MatchHelper
from helpers.media_helper import MediaHelper

from models.award import Award
from models.event_team import EventTeam
from models.match import Match
from models.media import Media


class TeamRenderer(object):
    @classmethod
    def render_team_details(cls, handler, team, year, is_canonical):
        media_key_futures = Media.query(Media.references == team.key, Media.year == year).fetch_async(500, keys_only=True)
        events_sorted, matches_by_event_key, awards_by_event_key, valid_years = TeamDetailsDataFetcher.fetch(team, year, return_valid_years=True)
        if not events_sorted:
            return None

        media_futures = ndb.get_multi_async(media_key_futures.get_result())

        participation = []
        year_wlt_list = []

        current_event = None
        matches_upcoming = None
        short_cache = False
        for event in events_sorted:
            event_matches = matches_by_event_key.get(event.key, [])
            event_awards = AwardHelper.organizeAwards(awards_by_event_key.get(event.key, []))
            matches_organized = MatchHelper.organizeMatches(event_matches)

            if event.now:
                current_event = event
                matches_upcoming = MatchHelper.upcomingMatches(event_matches)

            if event.within_a_day:
                short_cache = True

            wlt = EventHelper.calculateTeamWLTFromMatches(team.key_name, event_matches)
            year_wlt_list.append(wlt)
            if wlt["win"] + wlt["loss"] + wlt["tie"] == 0:
                display_wlt = None
            else:
                display_wlt = wlt

            team_rank = None
            if event.rankings:
                for element in event.rankings:
                    if element[1] == str(team.team_number):
                        team_rank = element[0]
                        break

            participation.append({'event': event,
                                   'matches': matches_organized,
                                   'wlt': display_wlt,
                                   'rank': team_rank,
                                   'awards': event_awards})

        year_wlt = {"win": 0, "loss": 0, "tie": 0}
        for wlt in year_wlt_list:
            year_wlt["win"] += wlt["win"]
            year_wlt["loss"] += wlt["loss"]
            year_wlt["tie"] += wlt["tie"]
        if year_wlt["win"] + year_wlt["loss"] + year_wlt["tie"] == 0:
            year_wlt = None

        medias_by_slugname = MediaHelper.group_by_slugname([media_future.get_result() for media_future in media_futures])

        template_values = {"is_canonical": is_canonical,
                           "team": team,
                           "participation": participation,
                           "year": year,
                           "years": valid_years,
                           "year_wlt": year_wlt,
                           "current_event": current_event,
                           "matches_upcoming": matches_upcoming,
                           "medias_by_slugname": medias_by_slugname}

        if short_cache:
            handler._cache_expiration = handler.SHORT_CACHE_EXPIRATION

        path = os.path.join(os.path.dirname(__file__), '../templates/team_details.html')
        return template.render(path, template_values)

    @classmethod
    def render_team_history(cls, handler, team, is_canonical):
        event_team_keys_future = EventTeam.query(EventTeam.team == team.key).fetch_async(1000, keys_only=True)
        award_keys_future = Award.query(Award.team_list == team.key).fetch_async(1000, keys_only=True)

        event_teams_futures = ndb.get_multi_async(event_team_keys_future.get_result())
        awards_futures = ndb.get_multi_async(award_keys_future.get_result())

        event_keys = [event_team_future.get_result().event for event_team_future in event_teams_futures]
        events_futures = ndb.get_multi_async(event_keys)

        awards_by_event = {}
        for award_future in awards_futures:
            award = award_future.get_result()
            if award.event.id() not in awards_by_event:
                awards_by_event[award.event.id()] = [award]
            else:
                awards_by_event[award.event.id()].append(award)

        event_awards = []
        current_event = None
        matches_upcoming = None
        short_cache = False
        for event_future in events_futures:
            event = event_future.get_result()
            if event.now:
                current_event = event

                team_matches_future = Match.query(Match.event == event.key, Match.team_key_names == team.key_name)\
                  .fetch_async(500, keys_only=True)
                matches = ndb.get_multi(team_matches_future.get_result())
                matches_upcoming = MatchHelper.upcomingMatches(matches)

            if event.within_a_day:
                short_cache = True

            if event.key_name in awards_by_event:
                sorted_awards = AwardHelper.organizeAwards(awards_by_event[event.key_name])
            else:
                sorted_awards = []
            event_awards.append((event, sorted_awards))
        event_awards = sorted(event_awards, key=lambda (e, _): e.start_date if e.start_date else datetime.datetime(e.year, 12, 31))

        years = sorted(set([et.get_result().year for et in event_teams_futures if et.get_result().year is not None]))

        template_values = {'is_canonical': is_canonical,
                           'team': team,
                           'event_awards': event_awards,
                           'years': years,
                           'current_event': current_event,
                           'matches_upcoming': matches_upcoming}

        if short_cache:
            handler._cache_expiration = handler.SHORT_CACHE_EXPIRATION

        path = os.path.join(os.path.dirname(__file__), '../templates/team_history.html')
        return template.render(path, template_values)

########NEW FILE########
__FILENAME__ = run_tests
#!/usr/bin/python
import multiprocessing
import optparse
import StringIO
import sys
import time
import warnings

# Install the Python unittest2 package before you run this script.
import unittest2

USAGE = """%prog SDK_PATH
Run unit tests for App Engine apps.
The SDK Path is probably /usr/local/google_appengine on Mac OS

SDK_PATH    Path to the SDK installation"""

RESULT_QUEUE = multiprocessing.Queue()


def start_suite(suite):
    sio = StringIO.StringIO()
    testresult = unittest2.TextTestRunner(sio, verbosity=2).run(suite)
    RESULT_QUEUE.put((sio.getvalue(), testresult.testsRun, testresult.wasSuccessful()))


def main(sdk_path, test_pattern):
    start_time = time.time()

    sys.path.insert(0, sdk_path)
    import dev_appserver
    dev_appserver.fix_sys_path()

    suites = unittest2.loader.TestLoader().discover("tests", test_pattern)

    processes = []
    for suite in suites:
        process = multiprocessing.Process(target=start_suite, args=[suite])
        process.start()
        processes.append(process)

    for process in processes:
        process.join()

    fail = False
    total_tests_run = 0
    while not RESULT_QUEUE.empty():
        test_output, tests_run, was_successful = RESULT_QUEUE.get()
        total_tests_run += tests_run
        print '-----------------------'
        print test_output
        if not was_successful:
            fail = True

    print "================================"
    print "Completed {} tests in: {} seconds".format(total_tests_run, time.time() - start_time)
    if fail:
        print "TESTS FAILED!"
    else:
        print "TESTS PASSED!"
    print "================================"
    if fail:
        sys.exit(1)
    else:
        sys.exit(0)


if __name__ == '__main__':
    parser = optparse.OptionParser(USAGE)

    parser.add_option("-s", "--sdk_path", type="string", default="/usr/local/google_appengine",
                      help="path to load Google Appengine SDK from")
    parser.add_option("-t", "--test_pattern", type="string", default="test*.py",
                      help="pattern for tests to run")
    options, args = parser.parse_args()

    main(options.sdk_path, options.test_pattern)

########NEW FILE########
__FILENAME__ = s3_cache
#!/usr/bin/env python2.7
from __future__ import absolute_import, unicode_literals, print_function, division

from sys import argv
from os import environ, stat, remove as _delete_file
from os.path import isfile, dirname, basename, abspath
from hashlib import sha256
from subprocess import check_call as run

from boto.s3.connection import S3Connection
from boto.s3.key import Key
from boto.exception import S3ResponseError


NEED_TO_UPLOAD_MARKER = '.need-to-upload'
BYTES_PER_MB = 1024 * 1024
try:
    BUCKET_NAME = environ['TWBS_S3_BUCKET']
except KeyError:
    raise SystemExit("TWBS_S3_BUCKET environment variable not set!")


def _sha256_of_file(filename):
    hasher = sha256()
    with open(filename, 'rb') as input_file:
        hasher.update(input_file.read())
    file_hash = hasher.hexdigest()
    print('sha256({}) = {}'.format(filename, file_hash))
    return file_hash


def _delete_file_quietly(filename):
    try:
        _delete_file(filename)
    except (OSError, IOError):
        pass


def _tarball_size(directory):
    kib = stat(_tarball_filename_for(directory)).st_size // BYTES_PER_MB
    return "{} MiB".format(kib)


def _tarball_filename_for(directory):
    return abspath('./{}.tar.gz'.format(basename(directory)))


def _create_tarball(directory):
    print("Creating tarball of {}...".format(directory))
    run(['tar', '-czf', _tarball_filename_for(directory), '-C', dirname(directory), basename(directory)])


def _extract_tarball(directory):
    print("Extracting tarball of {}...".format(directory))
    run(['tar', '-xzf', _tarball_filename_for(directory), '-C', dirname(directory)])


def download(directory):
    _delete_file_quietly(NEED_TO_UPLOAD_MARKER)
    try:
        print("Downloading {} tarball from S3...".format(friendly_name))
        key.get_contents_to_filename(_tarball_filename_for(directory))
    except S3ResponseError as err:
        open(NEED_TO_UPLOAD_MARKER, 'a').close()
        print(err)
        raise SystemExit("Cached {} download failed!".format(friendly_name))
    print("Downloaded {}.".format(_tarball_size(directory)))
    _extract_tarball(directory)
    print("{} successfully installed from cache.".format(friendly_name))


def upload(directory):
    _create_tarball(directory)
    print("Uploading {} tarball to S3... ({})".format(friendly_name, _tarball_size(directory)))
    key.set_contents_from_filename(_tarball_filename_for(directory))
    print("{} cache successfully updated.".format(friendly_name))
    _delete_file_quietly(NEED_TO_UPLOAD_MARKER)


if __name__ == '__main__':
    # Uses environment variables:
    #   AWS_ACCESS_KEY_ID -- AWS Access Key ID
    #   AWS_SECRET_ACCESS_KEY -- AWS Secret Access Key
    argv.pop(0)
    if len(argv) != 4:
        raise SystemExit("USAGE: s3_cache.py <download | upload> <friendly name> <dependencies file> <directory>")
    mode, friendly_name, dependencies_file, directory = argv

    conn = S3Connection()
    bucket = conn.lookup(BUCKET_NAME)
    if bucket is None:
        raise SystemExit("Could not access bucket!")

    dependencies_file_hash = _sha256_of_file(dependencies_file)

    key = Key(bucket, dependencies_file_hash)
    key.storage_class = 'REDUCED_REDUNDANCY'

    if mode == 'download':
        download(directory)
    elif mode == 'upload':
        if isfile(NEED_TO_UPLOAD_MARKER):  # FIXME
            upload(directory)
        else:
            print("No need to upload anything.")
    else:
        raise SystemExit("Unrecognized mode {!r}".format(mode))

########NEW FILE########
__FILENAME__ = tba_config
import os

DEBUG = os.environ.get('SERVER_SOFTWARE', '').startswith('Dev')

# For choosing what the main landing page displays
KICKOFF = 1
BUILDSEASON = 2
COMPETITIONSEASON = 3
OFFSEASON = 4
INSIGHTS = 5
CHAMPS = 6

# The CONFIG variables should have exactly the same structure between environments
# Eventually a test environment should be added. -gregmarra 17 Jul 2012
if DEBUG:
    CONFIG = {
        "env": "dev",
        "memcache": False,
        "firebase-url": "https://thebluealliance-dev.firebaseio.com/{}.json?auth={}"
    }
else:
    CONFIG = {
        "env": "prod",
        "memcache": True,
        "firebase-url": "https://thebluealliance.firebaseio.com/{}.json?auth={}"
    }

CONFIG['landing_handler'] = INSIGHTS
CONFIG["static_resource_version"] = 7

########NEW FILE########
__FILENAME__ = test_add_match_times
import datetime
import unittest2

from google.appengine.ext import testbed

from consts.event_type import EventType
from datafeeds.usfirst_matches_parser import UsfirstMatchesParser
from helpers.match_helper import MatchHelper
from models.event import Event
from models.match import Match


class TestAddMatchTimes(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_memcache_stub()

        self.event = Event(
            id="2014casj",
            event_short="casj",
            event_type_enum=EventType.REGIONAL,
            name="Silicon Valley Regional",
            start_date=datetime.datetime(2014, 2, 27, 0, 0),
            end_date=datetime.datetime(2014, 3, 1, 0, 0),
            year=2014,
            timezone_id="America/New_York",
        )

        self.event_dst = Event(
            id="2014casj",
            event_short="casj",
            event_type_enum=EventType.REGIONAL,
            name="Silicon Valley Regional",
            start_date=datetime.datetime(2014, 3, 8, 0, 0),
            end_date=datetime.datetime(2014, 3, 9, 0, 0),  # chosen to span DST change
            year=2014,
            timezone_id="America/Los_Angeles",
        )

    def match_dict_to_matches(self, match_dicts):
        return [Match(
            id=Match.renderKeyName(
                self.event.key.id(),
                match_dict.get("comp_level", None),
                match_dict.get("set_number", 0),
                match_dict.get("match_number", 0)),
            event=self.event.key,
            game=Match.FRC_GAMES_BY_YEAR.get(self.event.year, "frc_unknown"),
            set_number=match_dict.get("set_number", 0),
            match_number=match_dict.get("match_number", 0),
            comp_level=match_dict.get("comp_level", None),
            team_key_names=match_dict.get("team_key_names", None),
            time_string=match_dict.get("time_string", None),
            alliances_json=match_dict.get("alliances_json", None)
            )
            for match_dict in match_dicts]

    def test_match_times(self):
        with open('test_data/usfirst_html/usfirst_event_matches_2013cama.html', 'r') as f:  # using matches from a random event as data
            match_dicts, _ = UsfirstMatchesParser.parse(f.read())

        matches = self.match_dict_to_matches(match_dicts)
        MatchHelper.add_match_times(self.event, matches)

        self.assertEqual(len(matches), 92)

        PST_OFFSET = -5
        self.assertEqual(matches[0].time, datetime.datetime(2014, 2, 28, 9, 0) - datetime.timedelta(hours=PST_OFFSET))
        self.assertEqual(matches[75].time, datetime.datetime(2014, 3, 1, 11, 50) - datetime.timedelta(hours=PST_OFFSET))

    def test_match_times_dst(self):
        with open('test_data/usfirst_html/usfirst_event_matches_2012ct.html', 'r') as f:  # using matches from a random event as data
            match_dicts, _ = UsfirstMatchesParser.parse(f.read())

        matches = self.match_dict_to_matches(match_dicts)
        MatchHelper.add_match_times(self.event_dst, matches)

        self.assertEqual(len(matches), 125)

        PST_OFFSET = -8
        PDT_OFFSET = -7
        self.assertEqual(matches[0].time, datetime.datetime(2014, 3, 8, 9, 0) - datetime.timedelta(hours=PST_OFFSET))
        self.assertEqual(matches[-1].time, datetime.datetime(2014, 3, 9, 16, 5) - datetime.timedelta(hours=PDT_OFFSET))


########NEW FILE########
__FILENAME__ = test_apiv2_event_controller
import unittest2
import webtest
import json
import webapp2

from datetime import datetime

from google.appengine.ext import ndb
from google.appengine.ext import testbed

from consts.event_type import EventType

from controllers.api.api_event_controller import ApiEventController
from controllers.api.api_event_controller import ApiEventTeamsController
from controllers.api.api_event_controller import ApiEventMatchesController
from controllers.api.api_event_controller import ApiEventStatsController
from controllers.api.api_event_controller import ApiEventListController
from controllers.api.api_event_controller import ApiEventRankingsController

from models.event import Event
from models.event_team import EventTeam
from models.match import Match
from models.team import Team

class TestEventApiController(unittest2.TestCase):

    def setUp(self):
        app = webapp2.WSGIApplication([webapp2.Route(r'/<event_key:>', ApiEventController, methods=['GET'])], debug=True)
        self.testapp = webtest.TestApp(app)

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_memcache_stub()
        self.testbed.init_taskqueue_stub()

        self.event = Event(
                id="2010sc",
                name="Palmetto Regional",
                event_type_enum=EventType.REGIONAL,
                short_name="Palmetto",
                event_short="sc",
                year=2010,
                end_date=datetime(2010, 03, 27),
                official=True,
                location='Clemson, SC',
                start_date=datetime(2010, 03, 24),
        )

        self.event.put()

    def tearDown(self):
        self.testbed.deactivate()

    def assertEventJson(self, event):
        self.assertEqual(event["key"], self.event.key_name)
        self.assertEqual(event["name"], self.event.name)
        self.assertEqual(event["short_name"], self.event.short_name)
        self.assertEqual(event["official"], self.event.official)
        self.assertEqual(event["start_date"], self.event.start_date.date().isoformat())
        self.assertEqual(event["end_date"], self.event.end_date.date().isoformat())

    def testEventApi(self):
        response = self.testapp.get('/2010sc', headers={"X-TBA-App-Id": "tba-tests:event-controller-test:v01"})

        event_dict = json.loads(response.body)
        self.assertEventJson(event_dict)


class TestEventTeamsApiController(unittest2.TestCase):

    def setUp(self):
        app = webapp2.WSGIApplication([webapp2.Route(r'/<event_key:>', ApiEventTeamsController, methods=['GET'])], debug=True)
        self.testapp = webtest.TestApp(app)

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_memcache_stub()
        self.testbed.init_taskqueue_stub()

        self.event = Event(
                id="2010sc",
                name="Palmetto Regional",
                event_type_enum=EventType.REGIONAL,
                short_name="Palmetto",
                event_short="sc",
                year=2010,
                end_date=datetime(2010, 03, 27),
                official=True,
                location='Clemson, SC',
                start_date=datetime(2010, 03, 24),
        )
        self.event.put()

        self.team = Team(
                id="frc281",
                name="Michelin / Caterpillar / Greenville Technical College /\
                jcpenney / Baldor / ASME / Gastroenterology Associates /\
                Laserflex South & Greenville County Schools & Greenville\
                Technical Charter High School",
                team_number=281,
                nickname="EnTech GreenVillians",
                address="Greenville, SC, USA",
                website="www.entech.org",
        )
        self.team.put()

        self.event_team = EventTeam(
                team=self.team.key,
                event=self.event.key,
                year=datetime.now().year
        )
        self.event_team.put()

    def tearDown(self):
        self.testbed.deactivate()

    def assertTeamJson(self, team):
        team = team[0]
        self.assertEqual(team["key"], self.team.key_name)
        self.assertEqual(team["team_number"], self.team.team_number)
        self.assertEqual(team["nickname"], self.team.nickname)
        self.assertEqual(team["location"], self.team.location)
        self.assertEqual(team["locality"], "Greenville")
        self.assertEqual(team["country_name"], "USA")
        self.assertEqual(team["region"], "SC")
        self.assertEqual(team["website"], self.team.website)

    def testEventTeamsApi(self):
        response = self.testapp.get('/2010sc', headers={"X-TBA-App-Id": "tba-tests:event-controller-test:v01"})

        team_dict = json.loads(response.body)
        self.assertTeamJson(team_dict)


class TestEventMatchApiController(unittest2.TestCase):

    def setUp(self):
        app = webapp2.WSGIApplication([webapp2.Route(r'/<event_key:>', ApiEventMatchesController, methods=['GET'])], debug=True)
        self.testapp = webtest.TestApp(app)

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_memcache_stub()
        self.testbed.init_taskqueue_stub()

        self.event = Event(
                id="2010sc",
                name="Palmetto Regional",
                event_type_enum=EventType.REGIONAL,
                short_name="Palmetto",
                event_short="sc",
                year=2010,
                end_date=datetime(2010, 03, 27),
                official=True,
                location='Clemson, SC',
                start_date=datetime(2010, 03, 24),
        )
        self.event.put()

        self.match = Match(
            id="2010sc_qm1",
            alliances_json="""{"blue": {"score": 57, "teams": ["frc3464", "frc20", "frc1073"]}, "red": {"score": 74, "teams": ["frc281", "frc571", "frc176"]}}""",
            comp_level="qm",
            event=self.event.key,
            game="frc_2012_rebr",
            set_number=1,
            match_number=1,
            team_key_names=[u'frc281', u'frc571', u'frc176', u'frc3464', u'frc20', u'frc1073'],
            youtube_videos=["94UGXIq6jUA"],
            tba_videos=[".mp4"]
        )
        self.match.put()

    def tearDown(self):
        self.testbed.deactivate()

    def assertMatchJson(self, matches):
        match = matches[0]
        self.assertEqual(str(match["key"]), self.match.key.string_id())
        self.assertEqual(match["comp_level"], self.match.comp_level)
        self.assertEqual(match["event_key"], self.match.event.string_id())
        self.assertEqual(match["set_number"], self.match.set_number)
        self.assertEqual(match["match_number"], self.match.match_number)
        self.assertEqual(match["videos"], self.match.videos)
        self.assertEqual(match["time_string"], self.match.time_string)
        if self.match.time is None:
            self.assertEqual(match["time"], None)
        else:
            self.assertEqual(match["time"], self.match.time.strftime("%s"))
    
    def testEventMatchApi(self):
        response = self.testapp.get('/2010sc', headers={"X-TBA-App-Id": "tba-tests:event-controller-test:v01"})

        match_json = json.loads(response.body)
        self.assertMatchJson(match_json)


class TestEventStatsApiController(unittest2.TestCase):

    def setUp(self):
        app = webapp2.WSGIApplication([webapp2.Route(r'/<event_key:>', ApiEventStatsController, methods=['GET'])], debug=True)
        self.testapp = webtest.TestApp(app)

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_memcache_stub()
        self.testbed.init_taskqueue_stub()

        self.matchstats = {
            "dprs": {"971": 10.52178695299036, "114": 23.7313645955704, "115": 29.559784481082044},
            "oprs": {"971": 91.42946669932006, "114": 59.27751047482864, "115": 13.285278757495144},
            "ccwms": {"971": 80.90767974632955, "114": 35.54614587925829, "115": -16.27450572358693},
        }

        self.event = Event(
                id="2010sc",
                name="Palmetto Regional",
                event_type_enum=EventType.REGIONAL,
                short_name="Palmetto",
                event_short="sc",
                year=2010,
                end_date=datetime(2010, 03, 27),
                official=True,
                location='Clemson, SC',
                start_date=datetime(2010, 03, 24),
                matchstats_json=json.dumps(self.matchstats)
        )
        self.event.put()

    def tearDown(self):
        self.testbed.deactivate()

    def testEventStatsApi(self):
        response = self.testapp.get('/2010sc', headers={"X-TBA-App-Id": "tba-tests:event-controller-test:v01"})

        matchstats = json.loads(response.body)
        self.assertEqual(self.matchstats, matchstats)

class TestEventRankingsApiController(unittest2.TestCase):

    def setUp(self):
        app = webapp2.WSGIApplication([webapp2.Route(r'/<event_key:>', ApiEventRankingsController, methods=['GET'])], debug=True)
        self.testapp = webtest.TestApp(app)

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_memcache_stub()
        self.testbed.init_taskqueue_stub()

        self.rankings = [
            ["Rank", "Team", "QS", "ASSIST", "AUTO", "T&C", "TELEOP", "Record (W-L-T)", "DQ", "PLAYED"], 
            ["1", "1126", "20.00", "240.00", "480.00", "230.00", "478.00", "10-2-0", "0", "12"], 
            ["2", "5030", "20.00", "200.00", "290.00", "220.00", "592.00", "10-2-0", "0", "12"], 
            ["3", "250", "20.00", "70.00", "415.00", "220.00", "352.00", "10-2-0", "0", "12"]
            ]
        

        self.event = Event(
                id="2010sc",
                name="Palmetto Regional",
                event_type_enum=EventType.REGIONAL,
                short_name="Palmetto",
                event_short="sc",
                year=2010,
                end_date=datetime(2010, 03, 27),
                official=True,
                location='Clemson, SC',
                start_date=datetime(2010, 03, 24),
                rankings_json=json.dumps(self.rankings)
        )
        self.event.put()

        self.eventNoRanks = Event(
                id="2010ct",
                name="Palmetto Regional",
                event_type_enum=EventType.REGIONAL,
                short_name="Palmetto",
                event_short="ct",
                year=2010,
                end_date=datetime(2010, 03, 27),
                official=True,
                location='Clemson, SC',
                start_date=datetime(2010, 03, 24),
        )
        self.eventNoRanks.put()

    def tearDown(self):
        self.testbed.deactivate()

    def testEventRankingsApi(self):
        response = self.testapp.get('/2010sc', headers={"X-TBA-App-Id": "tba-tests:event-controller-test:v01"})

        rankings = json.loads(response.body)
        self.assertEqual(self.rankings, rankings)

    def testEventNoRankingsApi(self):
        response = self.testapp.get('/2010ct', headers={"X-TBA-App-Id": "tba-tests:event-controller-test:v01"})

        self.assertEqual("[]", response.body)

class TestEventListApiController(unittest2.TestCase):

    def setUp(self):
        app = webapp2.WSGIApplication([webapp2.Route(r'/<year:>', ApiEventListController, methods=['GET'])], debug=True)
        self.testapp = webtest.TestApp(app)

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_memcache_stub()
        self.testbed.init_taskqueue_stub()

        self.event = Event(
                id="2010sc",
                name="Palmetto Regional",
                event_type_enum=EventType.REGIONAL,
                short_name="Palmetto",
                event_short="sc",
                year=2010,
                end_date=datetime(2010, 03, 27),
                official=True,
                location='Clemson, SC',
                start_date=datetime(2010, 03, 24),
        )

        self.event.put()

    def tearDown(self):
        self.testbed.deactivate()

    def assertEventJson(self, event):
        self.assertEqual(event["key"], self.event.key_name)
        self.assertEqual(event["name"], self.event.name)
        self.assertEqual(event["short_name"], self.event.short_name)
        self.assertEqual(event["official"], self.event.official)
        self.assertEqual(event["start_date"], self.event.start_date.date().isoformat())
        self.assertEqual(event["end_date"], self.event.end_date.date().isoformat())

    def testEventListApi(self):
        response = self.testapp.get('/2010', headers={"X-TBA-App-Id": "tba-tests:event-controller-test:v01"})
        event_dict = json.loads(response.body)
        self.assertEventJson(event_dict[0])

########NEW FILE########
__FILENAME__ = test_apiv2_team_controller
import unittest2
import webtest
import json
import webapp2

from datetime import datetime

from google.appengine.ext import ndb
from google.appengine.ext import testbed

from consts.event_type import EventType

from controllers.api.api_team_controller import ApiTeamController

from consts.award_type import AwardType
from consts.event_type import EventType

from models.award import Award
from models.event import Event
from models.event_team import EventTeam
from models.match import Match
from models.team import Team


class TestTeamApiController(unittest2.TestCase):

    def setUp(self):
        app = webapp2.WSGIApplication([webapp2.Route(r'/<team_key:>', ApiTeamController, methods=['GET'])], debug=True)
        self.testapp = webtest.TestApp(app)

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_memcache_stub()
        self.testbed.init_taskqueue_stub()

        self.team = Team(
                id="frc281",
                name="Michelin / Caterpillar / Greenville Technical College /\
                jcpenney / Baldor / ASME / Gastroenterology Associates /\
                Laserflex South & Greenville County Schools & Greenville\
                Technical Charter High School",
                team_number=281,
                nickname="EnTech GreenVillians",
                address="Greenville, SC, USA",
                website="www.entech.org",
        )

        self.team.put()

        self.event = Event(
                id="2010sc",
                name="Palmetto Regional",
                event_type_enum=EventType.REGIONAL,
                short_name="Palmetto",
                event_short="sc",
                year=datetime.now().year,
                end_date=datetime(2010, 03, 27),
                official=True,
                location='Clemson, SC',
                start_date=datetime(2010, 03, 24),
        )

        self.event.put()

        self.event_team = EventTeam(
                team=self.team.key,
                event=self.event.key,
                year=datetime.now().year
        )
        self.event_team.put()

        self.match = Match(
            id="2010sc_qm1",
            alliances_json="""{"blue": {"score": 57, "teams": ["frc3464", "frc20", "frc1073"]}, "red": {"score": 74, "teams": ["frc281", "frc571", "frc176"]}}""",
            comp_level="qm",
            event=self.event.key,
            game="frc_2012_rebr",
            set_number=1,
            match_number=1,
            team_key_names=[u'frc281', u'frc571', u'frc176', u'frc3464', u'frc20', u'frc1073'],

        )
        self.match.put()

        self.award = Award(
            name_str = "Engineering Inspiration",
            award_type_enum = AwardType.ENGINEERING_INSPIRATION,
            year=datetime.now().year,
            event = self.event.key,
            event_type_enum = EventType.REGIONAL,
            team_list = [self.team.key],
            recipient_json_list=[json.dumps({'team_number': 281, 'awardee': None})],
        )
        self.award.put()

    def tearDown(self):
        self.testbed.deactivate()

    def assertTeamJson(self, team):
        self.assertEqual(team["key"], self.team.key_name)
        self.assertEqual(team["team_number"], self.team.team_number)
        self.assertEqual(team["nickname"], self.team.nickname)
        self.assertEqual(team["location"], self.team.location)
        self.assertEqual(team["locality"], "Greenville")
        self.assertEqual(team["country_name"], "USA")
        self.assertEqual(team["region"], "SC")
        self.assertEqual(team["website"], self.team.website)

    def assertEventJson(self, event):
        self.assertEqual(event["key"], self.event.key_name)
        self.assertEqual(event["name"], self.event.name)
        self.assertEqual(event["short_name"], self.event.short_name)
        self.assertEqual(event["official"], self.event.official)
        self.assertEqual(event["start_date"], self.event.start_date.date().isoformat())
        self.assertEqual(event["end_date"], self.event.end_date.date().isoformat())
        self.assertEqual(event["event_type_string"], self.event.event_type_str)
        self.assertEqual(event["event_type"], self.event.event_type_enum)

    def assertMatchJson(self, match):
        self.assertEqual(str(match["key"]), self.match.key.string_id())
        self.assertEqual(match["comp_level"], self.match.comp_level)
        self.assertEqual(match["event_key"], self.match.event.string_id())
        self.assertEqual(match["set_number"], self.match.set_number)
        self.assertEqual(match["match_number"], self.match.match_number)

    def assertAwardJson(self, award):
        self.assertEqual(award["name"], self.award.name_str)
        self.assertEqual(award["event_key"], self.award.event.string_id())
        self.assertEqual(award["recipient_list"], self.award.recipient_list)
        self.assertEqual(award["year"], self.award.year)

    def testTeamApi(self):
        response = self.testapp.get('/frc281', headers={"X-TBA-App-Id": "tba-tests:team-controller-test:v01"})

        team_dict = json.loads(response.body)
        self.assertTeamJson(team_dict)
        self.assertEventJson(team_dict["events"][0])
        self.assertMatchJson(team_dict["events"][0]["matches"][0])
        self.assertAwardJson(team_dict["events"][0]["awards"][0])

########NEW FILE########
__FILENAME__ = test_api_cache_clearer
import json
import unittest2
import webtest
import webapp2

from datetime import datetime

from google.appengine.api import memcache
from google.appengine.ext import testbed

from consts.award_type import AwardType
from consts.event_type import EventType

from controllers.api.api_event_controller import ApiEventController
from controllers.api.api_event_controller import ApiEventListController
from controllers.api.api_event_controller import ApiEventTeamsController
from controllers.api.api_event_controller import ApiEventMatchesController
from controllers.api.api_event_controller import ApiEventStatsController
from controllers.api.api_event_controller import ApiEventRankingsController
from controllers.api.api_event_controller import ApiEventAwardsController
from controllers.api.api_team_controller import ApiTeamController

from helpers.award_manipulator import AwardManipulator
from helpers.event_manipulator import EventManipulator
from helpers.event_team_manipulator import EventTeamManipulator
from helpers.match_manipulator import MatchManipulator
from helpers.team_manipulator import TeamManipulator

from models.award import Award
from models.event import Event
from models.event_team import EventTeam
from models.match import Match
from models.team import Team


class TestApiCacheClearer(unittest2.TestCase):
    def setUp(self):
        app = webapp2.WSGIApplication([
           webapp2.Route(r'/api/v2/team/<team_key:>',
                         ApiTeamController,
                         methods=['GET']),
           webapp2.Route(r'/api/v2/team/<team_key:>/<year:([0-9]*)>',
                         ApiTeamController,
                         methods=['GET']),
           webapp2.Route(r'/api/v2/event/<event_key:>',
                         ApiEventController,
                         methods=['GET']),
           webapp2.Route(r'/api/v2/event/<event_key:>/teams',
                         ApiEventTeamsController,
                         methods=['GET']),
           webapp2.Route(r'/api/v2/event/<event_key:>/matches',
                         ApiEventMatchesController,
                         methods=['GET']),
           webapp2.Route(r'/api/v2/event/<event_key:>/stats',
                         ApiEventStatsController,
                         methods=['GET']),
           webapp2.Route(r'/api/v2/event/<event_key:>/rankings',
                         ApiEventRankingsController,
                         methods=['GET']),
           webapp2.Route(r'/api/v2/event/<event_key:>/awards',
                        ApiEventAwardsController,
                        methods=['GET']),
           webapp2.Route(r'/api/v2/events/<year:([0-9]*)>',
                         ApiEventListController,
                         methods=['GET']),
           ], debug=True)

        self.testapp = webtest.TestApp(app)

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_memcache_stub()
        self.testbed.init_taskqueue_stub()

        # populate mini db
        self.event_2010sc_1 = Event(
            id='2010sc',
            name='Palmetto Regional',
            event_type_enum=EventType.REGIONAL,
            short_name='Palmetto',
            event_short='sc',
            year=2010,
            end_date=datetime(2010, 03, 27),
            official=True,
            location='Clemson, SC',
            start_date=datetime(2010, 03, 24),
        )

        self.event_2010sc_2 = Event(
            id='2010sc',
            name='New Regional',
            event_type_enum=EventType.REGIONAL,
            short_name='Palmetto',
            event_short='sc',
            year=2010,
            end_date=datetime(2010, 03, 27),
            official=True,
            location='Clemson, SC',
            start_date=datetime(2010, 03, 24),
        )

        self.team_frc1_1 = Team(
            id='frc1',
            name='This is a name',
            team_number=1,
            nickname='NICKNAME',
            address='San Jose, CA, USA',
            website='www.usfirst.org',
        )

        self.team_frc1_2 = Team(
            id='frc1',
            name='This is a name',
            team_number=1,
            nickname='NICKNAME',
            address='San Jose, CA, USA',
            website='www.thebluealliance.com',
        )

        self.team_frc2_1 = Team(
            id='frc2',
            name='This is a name',
            team_number=2,
            nickname='NICKNAME',
            address='San Jose, CA, USA',
            website='www.usfirst.org',
        )

        self.team_frc2_2 = Team(
            id='frc2',
            name='This is a name',
            team_number=2,
            nickname='nickname',
            address='San Jose, CA, USA',
            website='www.usfirst.org',
        )

        self.eventteam_2010sc_frc1 = EventTeam(
            id='2010sc_frc1',
            event=self.event_2010sc_1.key,
            team=self.team_frc1_1.key,
            year=2010,
        )

        self.eventteam_2010sc_frc2 = EventTeam(
            id='2010sc_frc2',
            event=self.event_2010sc_1.key,
            team=self.team_frc2_1.key,
            year=2010,
        )

        self.match1_1 = Match(
            id='2010sc_qm1',
            alliances_json=json.dumps({'blue': {'score': -1, 'teams': ['frc1', 'frc2', 'frc3']}, 'red': {'score': -1, 'teams': ['frc4', 'frc5', 'frc6']}}),
            comp_level='qm',
            event=self.event_2010sc_1.key,
            set_number=1,
            match_number=1,
            game='frc_unknown',
            team_key_names=[u'frc1', u'frc2', u'frc3', u'frc4', u'frc5', u'frc6'],
        )

        self.match1_2 = Match(
            id='2010sc_qm1',
            alliances_json=json.dumps({'blue': {'score': -1, 'teams': ['frc1', 'frc999', 'frc3']}, 'red': {'score': -1, 'teams': ['frc4', 'frc5', 'frc6']}}),
            comp_level='qm',
            event=self.event_2010sc_1.key,
            set_number=1,
            match_number=1,
            game='frc_unknown',
            team_key_names=[u'frc1', u'frc999', u'frc3', u'frc4', u'frc5', u'frc6'],
        )

        self.award1_1 = Award(
            id="2010sc_1",
            name_str="Regional Champion",
            award_type_enum=AwardType.WINNER,
            year=2010,
            event=self.event_2010sc_1.key,
            event_type_enum=EventType.REGIONAL,
            team_list=[self.team_frc1_1.key],
            recipient_json_list=[json.dumps({'team_number': 1, 'awardee': None})],
        )

        self.award1_2 = Award(
            id="2010sc_1",
            name_str="Regional Champion",
            award_type_enum=AwardType.WINNER,
            year=2010,
            event=self.event_2010sc_1.key,
            event_type_enum=EventType.REGIONAL,
            team_list=[self.team_frc2_1.key],
            recipient_json_list=[json.dumps({'team_number': 2, 'awardee': None})],
        )

        self.eventlist_2010_cache_key = ApiEventListController._get_full_cache_key(ApiEventListController.CACHE_KEY_FORMAT.format('2010'))
        self.event_2010sc_cache_key = ApiEventController._get_full_cache_key(ApiEventController.CACHE_KEY_FORMAT.format('2010sc'))
        self.eventteams_2010sc_cache_key = ApiEventTeamsController._get_full_cache_key(ApiEventTeamsController.CACHE_KEY_FORMAT.format('2010sc'))
        self.eventmatches_2010sc_cache_key = ApiEventMatchesController._get_full_cache_key(ApiEventMatchesController.CACHE_KEY_FORMAT.format('2010sc'))
        self.eventstats_2010sc_cache_key = ApiEventStatsController._get_full_cache_key(ApiEventStatsController.CACHE_KEY_FORMAT.format('2010sc'))
        self.eventrankings_2010sc_cache_key = ApiEventRankingsController._get_full_cache_key(ApiEventRankingsController.CACHE_KEY_FORMAT.format('2010sc'))
        self.eventawards_2010sc_cache_key = ApiEventAwardsController._get_full_cache_key(ApiEventAwardsController.CACHE_KEY_FORMAT.format('2010sc'))
        self.team_frc1_cache_key = ApiTeamController._get_full_cache_key(ApiTeamController.CACHE_KEY_FORMAT.format('frc1', 2010))
        self.team_frc2_cache_key = ApiTeamController._get_full_cache_key(ApiTeamController.CACHE_KEY_FORMAT.format('frc2', 2010))

    def tearDown(self):
        self.testbed.deactivate()

    def resetAll(self, flushed=False):
        response = self.testapp.get('/api/v2/events/2010', headers={'X-TBA-App-Id': 'tba-tests:api-cache-clear-test:v01'})
        self.assertNotEqual(memcache.get(self.eventlist_2010_cache_key), None)

        EventManipulator.createOrUpdate(self.event_2010sc_1)
        if flushed:
            self.assertEqual(memcache.get(self.eventlist_2010_cache_key), None)
        TeamManipulator.createOrUpdate(self.team_frc1_1)
        TeamManipulator.createOrUpdate(self.team_frc2_1)
        EventTeamManipulator.createOrUpdate(self.eventteam_2010sc_frc1)
        EventTeamManipulator.createOrUpdate(self.eventteam_2010sc_frc2)
        MatchManipulator.createOrUpdate(self.match1_1)
        AwardManipulator.createOrUpdate(self.award1_1)

        response = self.testapp.get('/api/v2/events/2010', headers={'X-TBA-App-Id': 'tba-tests:api-cache-clear-test:v01'})
        self.assertNotEqual(memcache.get(self.eventlist_2010_cache_key), None)
        if flushed:
            self.assertEqual(memcache.get(self.event_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.team_frc1_cache_key), None)
            self.assertEqual(memcache.get(self.team_frc2_cache_key), None)

        response = self.testapp.get('/api/v2/event/2010sc', headers={'X-TBA-App-Id': 'tba-tests:api-cache-clear-test:v01'})
        self.assertNotEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertNotEqual(memcache.get(self.event_2010sc_cache_key), None)
        if flushed:
            self.assertEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.team_frc1_cache_key), None)
            self.assertEqual(memcache.get(self.team_frc2_cache_key), None)

        response = self.testapp.get('/api/v2/event/2010sc/teams', headers={'X-TBA-App-Id': 'tba-tests:api-cache-clear-test:v01'})
        self.assertNotEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertNotEqual(memcache.get(self.event_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
        if flushed:
            self.assertEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.team_frc1_cache_key), None)
            self.assertEqual(memcache.get(self.team_frc2_cache_key), None)

        response = self.testapp.get('/api/v2/event/2010sc/matches', headers={'X-TBA-App-Id': 'tba-tests:api-cache-clear-test:v01'})
        self.assertNotEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertNotEqual(memcache.get(self.event_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
        if flushed:
            self.assertEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.team_frc1_cache_key), None)
            self.assertEqual(memcache.get(self.team_frc2_cache_key), None)

        response = self.testapp.get('/api/v2/event/2010sc/stats', headers={'X-TBA-App-Id': 'tba-tests:api-cache-clear-test:v01'})
        self.assertNotEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertNotEqual(memcache.get(self.event_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
        if flushed:
            self.assertEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.team_frc1_cache_key), None)
            self.assertEqual(memcache.get(self.team_frc2_cache_key), None)

        response = self.testapp.get('/api/v2/event/2010sc/rankings', headers={'X-TBA-App-Id': 'tba-tests:api-cache-clear-test:v01'})
        self.assertNotEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertNotEqual(memcache.get(self.event_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
        if flushed:
            self.assertEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
            self.assertEqual(memcache.get(self.team_frc1_cache_key), None)
            self.assertEqual(memcache.get(self.team_frc2_cache_key), None)

        response = self.testapp.get('/api/v2/event/2010sc/awards', headers={'X-TBA-App-Id': 'tba-tests:api-cache-clear-test:v01'})
        self.assertNotEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertNotEqual(memcache.get(self.event_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
        if flushed:
            self.assertEqual(memcache.get(self.team_frc1_cache_key), None)
            self.assertEqual(memcache.get(self.team_frc2_cache_key), None)

        response = self.testapp.get('/api/v2/team/frc1/2010', headers={'X-TBA-App-Id': 'tba-tests:api-cache-clear-test:v01'})
        self.assertNotEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertNotEqual(memcache.get(self.event_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.team_frc1_cache_key), None)
        if flushed:
            self.assertEqual(memcache.get(self.team_frc2_cache_key), None)

        response = self.testapp.get('/api/v2/team/frc2/2010', headers={'X-TBA-App-Id': 'tba-tests:api-cache-clear-test:v01'})
        self.assertNotEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertNotEqual(memcache.get(self.event_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.team_frc1_cache_key), None)
        self.assertNotEqual(memcache.get(self.team_frc2_cache_key), None)


    def testApiCacheClear(self):
        self.assertEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertEqual(memcache.get(self.event_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.team_frc1_cache_key), None)
        self.assertEqual(memcache.get(self.team_frc2_cache_key), None)

        self.resetAll(flushed=True)

        # this shouldn't evict any caches
        EventManipulator.createOrUpdate(self.event_2010sc_1)
        EventTeamManipulator.createOrUpdate(self.eventteam_2010sc_frc1)
        EventTeamManipulator.createOrUpdate(self.eventteam_2010sc_frc2)
        AwardManipulator.createOrUpdate(self.award1_1)
        MatchManipulator.createOrUpdate(self.match1_1)
        TeamManipulator.createOrUpdate(self.team_frc1_1)
        TeamManipulator.createOrUpdate(self.team_frc2_1)
        self.assertNotEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertNotEqual(memcache.get(self.event_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.team_frc1_cache_key), None)
        self.assertNotEqual(memcache.get(self.team_frc2_cache_key), None)

        # updating an event
        EventManipulator.createOrUpdate(self.event_2010sc_2)
        self.assertEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertEqual(memcache.get(self.event_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.team_frc1_cache_key), None)
        self.assertEqual(memcache.get(self.team_frc2_cache_key), None)

        self.resetAll()

        # updating a team
        TeamManipulator.createOrUpdate(self.team_frc1_2)
        self.assertNotEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertNotEqual(memcache.get(self.event_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.team_frc1_cache_key), None)
        self.assertNotEqual(memcache.get(self.team_frc2_cache_key), None)

        self.resetAll()

        # updating a match
        MatchManipulator.createOrUpdate(self.match1_2)
        self.assertNotEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertNotEqual(memcache.get(self.event_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.team_frc1_cache_key), None)
        self.assertEqual(memcache.get(self.team_frc2_cache_key), None)

        self.resetAll()

        # updating an award
        AwardManipulator.createOrUpdate(self.award1_2)
        self.assertNotEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertNotEqual(memcache.get(self.event_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.team_frc1_cache_key), None)
        self.assertEqual(memcache.get(self.team_frc2_cache_key), None)

        self.resetAll()

        # deleting an award
        AwardManipulator.delete(self.award1_2)
        self.assertNotEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertNotEqual(memcache.get(self.event_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.team_frc1_cache_key), None)
        self.assertEqual(memcache.get(self.team_frc2_cache_key), None)

        self.resetAll()

        # deleting a match
        MatchManipulator.delete(self.match1_2)
        self.assertNotEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertNotEqual(memcache.get(self.event_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.team_frc1_cache_key), None)
        self.assertNotEqual(memcache.get(self.team_frc2_cache_key), None)

        self.resetAll()

        # deleting a team
        TeamManipulator.delete(self.team_frc2_2)
        self.assertNotEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertNotEqual(memcache.get(self.event_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.team_frc1_cache_key), None)
        self.assertEqual(memcache.get(self.team_frc2_cache_key), None)

        self.resetAll()

        # deleting an event
        EventManipulator.delete(self.event_2010sc_2)
        self.assertEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertEqual(memcache.get(self.event_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.team_frc1_cache_key), None)
        self.assertEqual(memcache.get(self.team_frc2_cache_key), None)

        self.resetAll()

        # deleting an eventteam
        EventTeamManipulator.delete(self.eventteam_2010sc_frc1)
        self.assertNotEqual(memcache.get(self.eventlist_2010_cache_key), None)
        self.assertNotEqual(memcache.get(self.event_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.eventteams_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventmatches_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventstats_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventrankings_2010sc_cache_key), None)
        self.assertNotEqual(memcache.get(self.eventawards_2010sc_cache_key), None)
        self.assertEqual(memcache.get(self.team_frc1_cache_key), None)
        self.assertNotEqual(memcache.get(self.team_frc2_cache_key), None)

########NEW FILE########
__FILENAME__ = test_award_manipulator
import json
import unittest2

from google.appengine.ext import ndb
from google.appengine.ext import testbed

from consts.award_type import AwardType
from consts.event_type import EventType
from helpers.award_manipulator import AwardManipulator
from models.award import Award
from models.event import Event
from models.team import Team


class TestAwardManipulator(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_memcache_stub()
        self.testbed.init_taskqueue_stub()

        self.event = Event(
          id="2013casj",
          event_short="casj",
          year=2013,
          event_type_enum=EventType.REGIONAL,
        )

        self.old_award = Award(
            id=Award.render_key_name(self.event.key_name, AwardType.WINNER),
            name_str="Regional Winner",
            award_type_enum=AwardType.WINNER,
            year=2013,
            event=self.event.key,
            event_type_enum=EventType.REGIONAL,
            team_list=[ndb.Key(Team, 'frc111'), ndb.Key(Team, 'frc234')],
            recipient_json_list=[json.dumps({'team_number': 111, 'awardee': None}),
                                 json.dumps({'team_number': 234, 'awardee': None})],
        )

        self.new_award = Award(
            id="2013casj_1",
            name_str="Regional Champion",
            award_type_enum=AwardType.WINNER,
            year=2013,
            event=self.event.key,
            event_type_enum=EventType.REGIONAL,
            team_list=[ndb.Key(Team, 'frc359')],
            recipient_json_list=[json.dumps({'team_number': 359, 'awardee': None})],
        )

    def tearDown(self):
        self.testbed.deactivate()

    def assertMergedAward(self, award, is_auto_union):
        self.assertEqual(award.name_str, "Regional Champion")
        self.assertEqual(award.award_type_enum, AwardType.WINNER)
        self.assertEqual(award.year, 2013)
        self.assertEqual(award.event, self.event.key)
        self.assertEqual(award.event_type_enum, EventType.REGIONAL)
        if is_auto_union:
            self.assertEqual(set(award.team_list), {ndb.Key(Team, 'frc111'), ndb.Key(Team, 'frc234'), ndb.Key(Team, 'frc359')})
            self.assertEqual(len(award.recipient_json_list), 3)
            for r in award.recipient_json_list:
                self.assertTrue(json.loads(r) in [{'team_number': 111, 'awardee': None}, {'team_number': 234, 'awardee': None}, {'team_number': 359, 'awardee': None}])
        else:
            self.assertEqual(set(award.team_list), {ndb.Key(Team, 'frc359')})
            self.assertEqual(len(award.recipient_json_list), 1)
            for r in award.recipient_json_list:
                self.assertTrue(json.loads(r) in [{'team_number': 359, 'awardee': None}])

    def assertOldAward(self, award):
        self.assertEqual(award.name_str, "Regional Winner")
        self.assertEqual(award.award_type_enum, AwardType.WINNER)
        self.assertEqual(award.year, 2013)
        self.assertEqual(award.event, self.event.key)
        self.assertEqual(award.event_type_enum, EventType.REGIONAL)
        self.assertEqual(set(award.team_list), {ndb.Key(Team, 'frc111'), ndb.Key(Team, 'frc234')})
        self.assertEqual(len(award.recipient_json_list), 2)
        for r in award.recipient_json_list:
            self.assertTrue(json.loads(r) in [{'team_number': 111, 'awardee': None}, {'team_number': 234, 'awardee': None}])

    def test_createOrUpdate(self):
        AwardManipulator.createOrUpdate(self.old_award)
        self.assertOldAward(Award.get_by_id("2013casj_1"))

        AwardManipulator.createOrUpdate(self.new_award)
        self.assertMergedAward(Award.get_by_id("2013casj_1"), True)

    def test_findOrSpawn(self):
        self.old_award.put()
        self.assertMergedAward(AwardManipulator.findOrSpawn(self.new_award), True)

    def test_updateMerge(self):
        self.assertMergedAward(AwardManipulator.updateMerge(self.new_award, self.old_award), True)

    def test_createOrUpdate_no_auto_union(self):
        AwardManipulator.createOrUpdate(self.old_award)
        self.assertOldAward(Award.get_by_id("2013casj_1"))

        AwardManipulator.createOrUpdate(self.new_award, auto_union=False)
        self.assertMergedAward(Award.get_by_id("2013casj_1"), False)

    def test_findOrSpawn_no_auto_union(self):
        self.old_award.put()
        self.assertMergedAward(AwardManipulator.findOrSpawn(self.new_award, auto_union=False), False)

    def test_updateMerge_no_auto_union(self):
        self.assertMergedAward(AwardManipulator.updateMerge(self.new_award, self.old_award, auto_union=False), False)

########NEW FILE########
__FILENAME__ = test_award_type_parser
import unittest2
import csv
import StringIO

from consts.award_type import AwardType
from helpers.award_helper import AwardHelper


class TestUsfirstEventTypeParser(unittest2.TestCase):
    def test_parse(self):
        """
        Tests for a select subset of award types. Add more if desired.
        """
        self.assertEqual(AwardHelper.parse_award_type("Chairman's"), AwardType.CHAIRMANS)
        self.assertEqual(AwardHelper.parse_award_type("Chairman"), AwardType.CHAIRMANS)

        self.assertEqual(AwardHelper.parse_award_type("Winner #1"), AwardType.WINNER)
        self.assertEqual(AwardHelper.parse_award_type("Division Winner #2"), AwardType.WINNER)
        self.assertEqual(AwardHelper.parse_award_type("Newton - Division Champion #3"), AwardType.WINNER)
        self.assertEqual(AwardHelper.parse_award_type("Championship Winner #3"), AwardType.WINNER)
        self.assertEqual(AwardHelper.parse_award_type("Championship Champion #4"), AwardType.WINNER)
        self.assertEqual(AwardHelper.parse_award_type("Championship Champion"), AwardType.WINNER)
        self.assertEqual(AwardHelper.parse_award_type("Championship Winner"), AwardType.WINNER)
        self.assertEqual(AwardHelper.parse_award_type("Winner"), AwardType.WINNER)

        self.assertEqual(AwardHelper.parse_award_type("Finalist #1"), AwardType.FINALIST)
        self.assertEqual(AwardHelper.parse_award_type("Division Finalist #2"), AwardType.FINALIST)
        self.assertEqual(AwardHelper.parse_award_type("Championship Finalist #3"), AwardType.FINALIST)
        self.assertEqual(AwardHelper.parse_award_type("Championship Finalist #4"), AwardType.FINALIST)
        self.assertEqual(AwardHelper.parse_award_type("Championship Finalist"), AwardType.FINALIST)
        self.assertEqual(AwardHelper.parse_award_type("Finalist"), AwardType.FINALIST)

        self.assertEqual(AwardHelper.parse_award_type("Dean's List Finalist #1"), AwardType.DEANS_LIST)
        self.assertEqual(AwardHelper.parse_award_type("Dean's List Finalist"), AwardType.DEANS_LIST)
        self.assertEqual(AwardHelper.parse_award_type("Dean's List Winner #9"), AwardType.DEANS_LIST)
        self.assertEqual(AwardHelper.parse_award_type("Dean's List Winner"), AwardType.DEANS_LIST)
        self.assertEqual(AwardHelper.parse_award_type("Dean's List"), AwardType.DEANS_LIST)

        self.assertEqual(AwardHelper.parse_award_type("Excellence in Design Award sponsored by Autodesk (3D CAD)"), AwardType.EXCELLENCE_IN_DESIGN_CAD)
        self.assertEqual(AwardHelper.parse_award_type("Excellence in Design Award sponsored by Autodesk (Animation)"), AwardType.EXCELLENCE_IN_DESIGN_ANIMATION)
        self.assertEqual(AwardHelper.parse_award_type("Excellence in Design Award"), AwardType.EXCELLENCE_IN_DESIGN)

        self.assertEqual(AwardHelper.parse_award_type("Dr. Bart Kamen Memorial Scholarship #1"), AwardType.BART_KAMEN_MEMORIAL)
        self.assertEqual(AwardHelper.parse_award_type("Media and Technology Award sponsored by Comcast"), AwardType.MEDIA_AND_TECHNOLOGY)
        self.assertEqual(AwardHelper.parse_award_type("Make It Loud Award"), AwardType.MAKE_IT_LOUD)
        self.assertEqual(AwardHelper.parse_award_type("Founder's Award"), AwardType.FOUNDERS)
        self.assertEqual(AwardHelper.parse_award_type("Championship - Web Site Award"), AwardType.WEBSITE)
        self.assertEqual(AwardHelper.parse_award_type("Recognition of Extraordinary Service"), AwardType.RECOGNITION_OF_EXTRAORDINARY_SERVICE)
        self.assertEqual(AwardHelper.parse_award_type("Outstanding Cart Award"), AwardType.OUTSTANDING_CART)
        self.assertEqual(AwardHelper.parse_award_type("Wayne State University Aim Higher Award"), AwardType.WSU_AIM_HIGHER)
        self.assertEqual(AwardHelper.parse_award_type("Delphi \"Driving Tommorow's Technology\" Award"), AwardType.DRIVING_TOMORROWS_TECHNOLOGY)
        self.assertEqual(AwardHelper.parse_award_type("Delphi Drive Tommorows Technology"), AwardType.DRIVING_TOMORROWS_TECHNOLOGY)
        self.assertEqual(AwardHelper.parse_award_type("Kleiner, Perkins, Caufield and Byers"), AwardType.ENTREPRENEURSHIP)
        self.assertEqual(AwardHelper.parse_award_type("Leadership in Control Award"), AwardType.LEADERSHIP_IN_CONTROL)
        self.assertEqual(AwardHelper.parse_award_type("#1 Seed"), AwardType.NUM_1_SEED)
        self.assertEqual(AwardHelper.parse_award_type("Incredible Play Award"), AwardType.INCREDIBLE_PLAY)
        self.assertEqual(AwardHelper.parse_award_type("People's Choice Animation Award"), AwardType.PEOPLES_CHOICE_ANIMATION)
        self.assertEqual(AwardHelper.parse_award_type("Autodesk Award for Visualization - Grand Prize"), AwardType.VISUALIZATION)
        self.assertEqual(AwardHelper.parse_award_type("Autodesk Award for Visualization - Rising Star"), AwardType.VISUALIZATION_RISING_STAR)

        self.assertEqual(AwardHelper.parse_award_type("Some Random Award Winner"), None)
        self.assertEqual(AwardHelper.parse_award_type("Random Champion"), None)
        self.assertEqual(AwardHelper.parse_award_type("An Award"), None)

        # Make sure all old regional awards have matching types
        with open('test_data/pre_2002_regional_awards.csv', 'r') as f:
            csv_data = list(csv.reader(StringIO.StringIO(f.read()), delimiter=',', skipinitialspace=True))
            for award in csv_data:
                self.assertNotEqual(AwardHelper.parse_award_type(award[2]), None)

        # Make sure all old regional awards have matching types
        with open('test_data/pre_2007_cmp_awards.csv', 'r') as f:
            csv_data = list(csv.reader(StringIO.StringIO(f.read()), delimiter=',', skipinitialspace=True))
            for award in csv_data:
                self.assertNotEqual(AwardHelper.parse_award_type(award[2]), None)

########NEW FILE########
__FILENAME__ = test_base_apiv2_controller
import json
import unittest2
import webapp2
import webtest

from google.appengine.ext import testbed

from controllers.api.api_base_controller import ApiBaseController


class TestTeamApi(unittest2.TestCase):

    def setUp(self):
        app = webapp2.WSGIApplication([webapp2.Route(r'/', ApiBaseController, methods=['GET'])], debug=True)
        self.testapp = webtest.TestApp(app)

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_urlfetch_stub()

    def tearDown(self):
        self.testbed.deactivate()

    def test_validate_app_id(self):
        response = self.testapp.get('/', expect_errors=True)  # By default get() doesn't send a X-TBA-App-ID
        self.assertEqual(response.status, "400 Bad Request")
        self.assertTrue('Error' in json.loads(response.body).keys())

        response = self.testapp.get('/', headers={"X-TBA-App-Id": "this:is:a:bad:id"}, expect_errors=True)
        self.assertEqual(response.status, "400 Bad Request")

########NEW FILE########
__FILENAME__ = test_csv_alliance_selections_parser
import unittest2

from datafeeds.csv_alliance_selections_parser import CSVAllianceSelectionsParser


class TestCSVAllianceSelectionsParser(unittest2.TestCase):
    def test_parse(self):
        with open('test_data/2014casj_alliances.csv', 'r') as f:
            alliances = CSVAllianceSelectionsParser.parse(f.read())

        self.assertEqual(
            alliances,
            [
                {'picks': ['frc971', 'frc254', 'frc1662'], 'declines':[] },
                {'picks': ['frc1678', 'frc368', 'frc4171'], 'declines':[] },
                {'picks': ['frc2035', 'frc192', 'frc4990'], 'declines':[] },
                {'picks': ['frc1323', 'frc846', 'frc2135'], 'declines':[] },
                {'picks': ['frc2144', 'frc1388', 'frc668'], 'declines':[] },
                {'picks': ['frc1280', 'frc604', 'frc100'], 'declines':[] },
                {'picks': ['frc114', 'frc852', 'frc841'], 'declines':[] },
                {'picks': ['frc2473', 'frc3256', 'frc1868'], 'declines':[] },
            ]
        )

########NEW FILE########
__FILENAME__ = test_csv_awards_parser
import unittest2
import json

from consts.award_type import AwardType
from datafeeds.csv_awards_parser import CSVAwardsParser


def convert_to_comparable(data):
    """
    Converts jsons to dicts so that elements can be more easily compared
    """
    if type(data) == list:
        return [convert_to_comparable(e) for e in data]
    elif type(data) == dict:
        to_return = {}
        for key, value in data.items():
            to_return[key] = convert_to_comparable(value)
        return to_return
    elif type(data) == str or type(data) == unicode:
        try:
            return json.loads(data)
        except ValueError:
            return data
    else:
        return data


class TestCSVAwardssParser(unittest2.TestCase):
    def test_parse_regional_awards(self):
        with open('test_data/pre_2002_regional_awards.csv', 'r') as f:
            awards = CSVAwardsParser.parse(f.read())

        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 885)
        self.assertEqual(len(awards), 601)

        awards = convert_to_comparable(awards)

        award = {
            'year': 1995,
            'event_short': 'nh',
            'name_str': "Team Spirit Award",
            'award_type_enum': AwardType.SPIRIT,
            'team_number_list': [213],
            'recipient_json_list': [{'team_number': 213, 'awardee': None}],
        }
        self.assertTrue(award in awards)

        award = {
            'year': 2000,
            'event_short': 'mi',
            'name_str': "Best Offensive Round",
            'award_type_enum': AwardType.BEST_OFFENSIVE_ROUND,
            'team_number_list': [1, 240],
            'recipient_json_list': [{'team_number': 1, 'awardee': None},
                                    {'team_number': 240, 'awardee': None}],
        }
        self.assertTrue(award in awards)

    def test_parse_cmp_awards(self):
        with open('test_data/pre_2007_cmp_awards.csv', 'r') as f:
            awards = CSVAwardsParser.parse(f.read())

        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 560)
        self.assertEqual(len(awards), 330)

        awards = convert_to_comparable(awards)

        award = {
            'year': 2003,
            'event_short': 'cmp',
            'name_str': "Championship Finalist",
            'award_type_enum': AwardType.FINALIST,
            'team_number_list': [25, 343, 494],
            'recipient_json_list': [{'team_number': 25, 'awardee': None},
                                    {'team_number': 343, 'awardee': None},
                                    {'team_number': 494, 'awardee': None}],
        }
        self.assertTrue(award in awards)

        award = {
            'year': 2002,
            'event_short': 'arc',
            'name_str': "#1 Seed",
            'award_type_enum': AwardType.NUM_1_SEED,
            'team_number_list': [121],
            'recipient_json_list': [{'team_number': 121, 'awardee': None}],
        }
        self.assertTrue(award in awards)

        award = {
            'year': 1992,
            'event_short': 'cmp',
            'name_str': "Play of the Day",
            'award_type_enum': AwardType.PLAY_OF_THE_DAY,
            'team_number_list': [131],
            'recipient_json_list': [{'team_number': 131, 'awardee': None}],
        }
        self.assertTrue(award in awards)

########NEW FILE########
__FILENAME__ = test_csv_teams_parser
import unittest2

from datafeeds.csv_teams_parser import CSVTeamsParser


class TestCSVTeamsParser(unittest2.TestCase):
    def test_parse(self):
        with open('test_data/teams.csv', 'r') as f:
            teams = CSVTeamsParser.parse(f.read())

        correct_teams = {1, 2, 3, 5, 9, 10, 15, 16, 18, 22, 100, 101, 102, 103, 999}
        self.assertEqual(teams.difference(correct_teams), set())

########NEW FILE########
__FILENAME__ = test_datafeed_controller
import unittest2
import datetime

from google.appengine.api import urlfetch
from google.appengine.ext import db
from google.appengine.ext import testbed
from google.appengine.ext.webapp import Response

from controllers.datafeed_controller import UsfirstEventDetailsGet

from models.event import Event
from models.team import Team


class TestUsfirstEventDetailsGet(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_memcache_stub()

    def tearDown(self):
        self.testbed.deactivate()

    # def test_get(self):
    # test with 2011ct
    #    usfirsteventget = UsfirstEventDetailsGet()
    #    usfirsteventget.response = Response()
    #    usfirsteventget.get(2011, "5561")
    #
    # check event object got created
    #    event = Event.get_by_id("2011ct")
    #    self.assertEqual(event.name, "Northeast Utilities FIRST Connecticut Regional")
    #    self.assertEqual(event.event_type, "Regional")
    #    self.assertEqual(event.start_date, datetime.datetime(2011, 3, 31, 0, 0))
    #    self.assertEqual(event.end_date, datetime.datetime(2011, 4, 2, 0, 0))
    #    self.assertEqual(event.year, 2011)
    #    self.assertEqual(event.venue_address, "Connecticut Convention Center\r\n100 Columbus Blvd\r\nHartford, CT 06103\r\nUSA")
    #    self.assertEqual(event.website, "http://www.ctfirst.org/ctr")
    #    self.assertEqual(event.event_short, "ct")
    #
    # check team objects get created for missing teams
    #    frc177 = Team.get_by_id("frc177")
    #    self.assertEqual(frc177.team_number, 177)
    #    self.assertEqual(frc177.first_tpid, 41633)

########NEW FILE########
__FILENAME__ = test_datafeed_fms_events
import unittest2
import datetime

from google.appengine.ext import testbed

from datafeeds.datafeed_fms import DatafeedFms


class TestDatafeedFmsEvents(unittest2.TestCase):

    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_urlfetch_stub()

        self.datafeed = DatafeedFms()

    def tearDown(self):
        self.testbed.deactivate()

    def test_getFmsEventList(self):
        events = self.datafeed.getFmsEventList()
        self.assertTrue(self.find2014CT(events))

    def find2012CT(self, events):
        found_ct = False
        for event in events:
            if event.event_short == "ct":
                found_ct = True
                self.assertEqual(event.key.id(), "2012ct")
                self.assertEqual(event.end_date, datetime.datetime(2012, 03, 31))
                self.assertEqual(event.event_short, "ct")
                self.assertEqual(event.first_eid, "7627")
                #self.assertEqual(event.location, "Hartford, CT")
                self.assertEqual(event.name, "Northeast Utilities FIRST Connecticut Regional")
                self.assertEqual(event.official, True)
                self.assertEqual(event.start_date, datetime.datetime(2012, 03, 29))
                self.assertEqual(event.venue, "Connecticut Convention Center")
                self.assertEqual(event.year, 2012)

        return found_ct

    def find2013CT(self, events):
        found_ct = False
        for event in events:
            if event.event_short == "ctha":
                found_ct = True
                self.assertEqual(event.key.id(), "2013ctha")
                self.assertEqual(event.end_date, datetime.datetime(2013, 03, 30))
                self.assertEqual(event.event_short, "ctha")
                self.assertEqual(event.first_eid, "8985")
                #self.assertEqual(event.location, "Hartford, CT")
                self.assertEqual(event.name, "Connecticut Regional sponsored by UTC")
                self.assertEqual(event.official, True)
                self.assertEqual(event.start_date, datetime.datetime(2013, 03, 28))
                self.assertEqual(event.venue, "Connecticut Convention Center")
                self.assertEqual(event.year, 2013)

        return found_ct

    def find2014CT(self, events):
        found_ct = False
        for event in events:
            if event.event_short == "cthar":
                found_ct = True
                self.assertEqual(event.key.id(), "2014cthar")
                self.assertEqual(event.end_date, datetime.datetime(2014, 03, 30))
                self.assertEqual(event.event_short, "cthar")
                self.assertEqual(event.first_eid, "10893")
                #self.assertEqual(event.location, "Hartford, CT")
                self.assertEqual(event.name, "Hartford District Event")
                self.assertEqual(event.official, True)
                self.assertEqual(event.start_date, datetime.datetime(2014, 03, 29))
                self.assertEqual(event.venue, "Hartford Public High School")
                self.assertEqual(event.year, 2014)

        return found_ct

########NEW FILE########
__FILENAME__ = test_datafeed_fms_teams
import unittest2
import datetime

from google.appengine.ext import testbed

from datafeeds.datafeed_fms import DatafeedFms


class TestDatafeedFmsTeams(unittest2.TestCase):

    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_memcache_stub()

        self.datafeed = DatafeedFms()

    def tearDown(self):
        self.testbed.deactivate()

    def test_getFmsTeamList(self):
        teams = self.datafeed.getFmsTeamList()
        self.find177(teams)

    def find177(self, teams):
        found_177 = False
        for team in teams:
            if team.team_number == 177:
                found_177 = True
                self.assertEqual(team.name, "United Technologies / ClearEdge Power / Gain Talent / EBA&D & South Windsor High School")
                #self.assertEqual(team.address, u"South Windsor, CT, USA")
                self.assertEqual(team.nickname, "Bobcat Robotics")

                break

        self.assertTrue(found_177)
        self.assertTrue(len(teams) > 0)

########NEW FILE########
__FILENAME__ = test_datafeed_usfirst_events
import unittest2
import datetime

from google.appengine.ext import testbed
from google.appengine.api import urlfetch

from consts.event_type import EventType

from datafeeds.datafeed_usfirst import DatafeedUsfirst

from models.event import Event
from models.team import Team


class TestDatafeedUsfirstEvents(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_memcache_stub()

        self.datafeed = DatafeedUsfirst()

    def tearDown(self):
        self.testbed.deactivate()

    def test_getEvent(self):
        # test with 2011ct
        event = self.datafeed.getEventDetails("5561")

        self.assertEqual(event.key.id(), "2011ct")
        self.assertEqual(event.name, "Northeast Utilities FIRST Connecticut Regional")
        self.assertEqual(event.event_type_enum, EventType.REGIONAL)
        self.assertEqual(event.start_date, datetime.datetime(2011, 3, 31, 0, 0))
        self.assertEqual(event.end_date, datetime.datetime(2011, 4, 2, 0, 0))
        self.assertEqual(event.year, 2011)
        self.assertEqual(event.venue_address, "Connecticut Convention Center\r\n100 Columbus Blvd\r\nHartford, CT 06103\r\nUSA")
        self.assertEqual(event.website, "http://www.ctfirst.org/ctr")
        self.assertEqual(event.event_short, "ct")

    def test_getEventAlliances(self):
        event = Event(
            event_short='cur',
            year=2014,
        )
        alliances = self.datafeed.getEventAlliances(event)
        self.assertEqual(
            alliances,
            [
                {'picks': ['frc254', 'frc469', 'frc2848', 'frc74'], 'declines':[] },
                {'picks': ['frc1718', 'frc2451', 'frc573', 'frc2016'], 'declines':[] },
                {'picks': ['frc2928', 'frc2013', 'frc1311', 'frc842'], 'declines':[] },
                {'picks': ['frc180', 'frc125', 'frc1323', 'frc2468'], 'declines':[] },
                {'picks': ['frc118', 'frc359', 'frc4334', 'frc865'], 'declines':[] },
                {'picks': ['frc135', 'frc1241', 'frc11', 'frc68'], 'declines':[] },
                {'picks': ['frc3478', 'frc177', 'frc294', 'frc230'], 'declines':[] },
                {'picks': ['frc624', 'frc987', 'frc3476', 'frc3015'], 'declines':[] },
            ]
        )

    def test_getEventTeams(self):
        # test with 2011ct
        teams = self.datafeed.getEventTeams(2011, "5561")

        sort_key = lambda t: t[1]
        self.assertEqual(
            sorted([(team.team_number, team.first_tpid) for team in teams], key=sort_key),
            sorted([(383, 41829), (1124, 42285), (155, 41609), (3634, 51637), (999, 42215), (1699, 42751), (173, 41625), (175, 41629), (716, 42049), (178, 41635), (2170, 43331), (3146, 44577), (2168, 43335), (2067, 43175), (181, 41641), (1991, 43133), (3125, 44539), (2785, 44073), (1740, 42765), (1784, 42895), (3654, 51609), (3718, 49891), (558, 41939), (3719, 52081), (230, 41681), (3464, 49827), (177, 41633), (2064, 43159), (195, 41651), (3104, 44463), (3555, 49069), (3141, 44487), (3461, 47483), (3525, 48801), (237, 41691), (3182, 44547), (571, 41947), (176, 41631), (1071, 42251), (2836, 43965), (126, 41585), (157, 41611), (69, 41519), (1027, 42235), (663, 42007), (3585, 50743), (1073, 42255), (501, 41899), (869, 42131), (714, 42047), (1923, 42947), (743, 42051), (20, 41475), (3204, 44731), (1601, 42659), (2791, 43935), (533, 41919), (694, 42027)], key=sort_key)
        )

    def test_getEventList(self):
        events = self.datafeed.getEventList(2011)

        self.assertEqual(len(events), 58)  # 58 events expected

        self.assertEqual(events[0].first_eid, "5519")
        self.assertEqual(events[0].event_type_enum, EventType.REGIONAL)
        self.assertEqual(events[0].name, "BAE Systems/Granite State Regional")

        self.assertEqual(events[1].first_eid, "5523")
        self.assertEqual(events[1].event_type_enum, EventType.REGIONAL)
        self.assertEqual(events[1].name, "New Jersey Regional")

########NEW FILE########
__FILENAME__ = test_datafeed_usfirst_legacy_events
import unittest2
import datetime

from google.appengine.ext import testbed

from consts.event_type import EventType

from datafeeds.datafeed_usfirst_legacy import DatafeedUsfirstLegacy


class TestDatafeedUsfirstLegacyEvents(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_memcache_stub()
        self.datafeed = DatafeedUsfirstLegacy()

    def tearDown(self):
        self.testbed.deactivate()

    def test_getEvent(self):
        # test with 2011ct
        event = self.datafeed.getEventDetails(2011, "5561")

        self.assertEqual(event.key.id(), "2011ct")
        self.assertEqual(event.name, "Northeast Utilities FIRST Connecticut Regional")
        self.assertEqual(event.event_type_enum, EventType.REGIONAL)
        self.assertEqual(event.start_date, datetime.datetime(2011, 3, 31, 0, 0))
        self.assertEqual(event.end_date, datetime.datetime(2011, 4, 2, 0, 0))
        self.assertEqual(event.year, 2011)
        self.assertEqual(event.venue_address, "Connecticut Convention Center\r\n100 Columbus Blvd\r\nHartford, CT 06103\r\nUSA")
        self.assertEqual(event.website, "http://www.ctfirst.org/ctr")
        self.assertEqual(event.event_short, "ct")

    def test_getEventTeams(self):
        # test with 2011ct
        teams = self.datafeed.getEventTeams(2011, "5561")

        self.assertEqual(
            [(team.team_number, team.first_tpid) for team in teams],
            [(383, 41829), (1124, 42285), (155, 41609), (3634, 51637), (999, 42215), (1699, 42751), (173, 41625), (175, 41629), (716, 42049), (178, 41635), (2170, 43331), (3146, 44577), (2168, 43335), (2067, 43175), (181, 41641), (1991, 43133), (3125, 44539), (2785, 44073), (1740, 42765), (1784, 42895), (3654, 51609), (3718, 49891), (558, 41939), (3719, 52081), (230, 41681), (3464, 49827), (177, 41633), (2064, 43159), (195, 41651), (3104, 44463), (3555, 49069), (3141, 44487), (3461, 47483), (3525, 48801), (237, 41691), (3182, 44547), (571, 41947), (176, 41631), (1071, 42251), (2836, 43965), (126, 41585), (157, 41611), (69, 41519), (1027, 42235), (663, 42007), (3585, 50743), (1073, 42255), (501, 41899), (869, 42131), (714, 42047), (1923, 42947), (743, 42051), (20, 41475), (3204, 44731), (1601, 42659), (2791, 43935), (533, 41919), (694, 42027)]
        )

    def test_getEventList(self):
        events = self.datafeed.getEventList(2011)

        self.assertEqual(len(events), 58)  # 58 events expected

        self.assertEqual(events[0].first_eid, "5519")
        self.assertEqual(events[0].event_type_enum, EventType.REGIONAL)
        self.assertEqual(events[0].name, "BAE Systems/Granite State Regional")

        self.assertEqual(events[1].first_eid, "5523")
        self.assertEqual(events[1].event_type_enum, EventType.REGIONAL)
        self.assertEqual(events[1].name, "New Jersey Regional")

########NEW FILE########
__FILENAME__ = test_datafeed_usfirst_legacy_teams
import unittest2

from google.appengine.ext import testbed

from datafeeds.datafeed_usfirst_legacy import DatafeedUsfirstLegacy
from models.team import Team


class TestDatafeedUsfirstLegacyTeams(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_memcache_stub()

        self.datafeed = DatafeedUsfirstLegacy()

        self.team177 = Team(
            id="frc177",
            team_number=177,
            first_tpid=211521,
            first_tpid_year=2014,
        )
        self.team177.put()

    def tearDown(self):
        self.testbed.deactivate()

    def test_getTeamDetails(self):
        team = self.datafeed.getTeamDetails(self.team177)

        self.assertEqual(team.key.id(), 'frc177')
        self.assertEqual(team.name, "United Technologies / ClearEdge Power / Gain Talent / EBA&D & South Windsor High School")
        self.assertEqual(team.address, u"South Windsor, CT\xa0 USA")
        self.assertEqual(team.nickname, "Bobcat Robotics")
        self.assertEqual(team.rookie_year, 1995)
        self.assertEqual(team.website, "http://www.bobcatrobotics.org")

    def test_getTeamsTpids(self):
        Team(
          id="frc4409",
          team_number=4409,
          first_tpid=0,  # should be 74735
          first_tpid_year=2011
        ).put()

        # We can skip 2000 records, paginate, and still get frc4409 and frc4410 in 2012
        self.datafeed.getTeamsTpids(2012, skip=2000)

        # Check new team insertion
        frc4410 = Team.get_by_id("frc4410")
        self.assertEqual(frc4410.team_number, 4410)
        self.assertEqual(frc4410.first_tpid, 74193)
        self.assertEqual(frc4410.first_tpid_year, 2012)

        # Check old team updating
        frc4409 = Team.get_by_id("frc4409")
        self.assertEqual(frc4409.team_number, 4409)
        self.assertEqual(frc4409.first_tpid, 74735)
        self.assertEqual(frc4409.first_tpid_year, 2012)

########NEW FILE########
__FILENAME__ = test_datafeed_usfirst_matches
import unittest2
import datetime

from google.appengine.ext import db
from google.appengine.ext import testbed

from datafeeds.datafeed_usfirst import DatafeedUsfirst
from models.event import Event


class TestDatafeedUsfirstMatches(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_memcache_stub()

        self.datafeed = DatafeedUsfirst()

    def tearDown(self):
        self.testbed.deactivate()

    def test_getMatchResultsList(self):
        event = Event(
          id="2011ct",
          event_short="ct",
          year=2011
        )

        matches = self.datafeed.getMatches(event)

        # Test 2011ct_qm1
        match = matches[0]
        self.assertEqual(match.comp_level, "qm")
        self.assertEqual(match.set_number, 1)
        self.assertEqual(match.match_number, 1)
        self.assertEqual(match.team_key_names, [u'frc69', u'frc571', u'frc176', u'frc3464', u'frc20', u'frc1073'])
        self.assertEqual(match.alliances_json, """{"blue": {"score": 57, "teams": ["frc3464", "frc20", "frc1073"]}, "red": {"score": 74, "teams": ["frc69", "frc571", "frc176"]}}""")
        self.assertEqual(match.time_string, "9:29 AM")

        # Test 2011ct_qf2m3
        match = matches[-7]
        self.assertEqual(match.comp_level, "qf")
        self.assertEqual(match.set_number, 2)
        self.assertEqual(match.match_number, 3)
        self.assertEqual(match.team_key_names, [u'frc716', u'frc3125', u'frc181', u'frc1699', u'frc1124', u'frc714'])
        self.assertEqual(match.alliances_json, """{"blue": {"score": 74, "teams": ["frc1699", "frc1124", "frc714"]}, "red": {"score": 90, "teams": ["frc716", "frc3125", "frc181"]}}""")
        self.assertEqual(match.time_string, "2:05 PM")

        # Test 2011ct_f1m2
        match = matches[-1]
        self.assertEqual(match.comp_level, "f")
        self.assertEqual(match.set_number, 1)
        self.assertEqual(match.match_number, 2)
        self.assertEqual(match.team_key_names, [u'frc195', u'frc1923', u'frc155', u'frc177', u'frc175', u'frc1073'])
        self.assertEqual(match.alliances_json, """{"blue": {"score": 65, "teams": ["frc177", "frc175", "frc1073"]}, "red": {"score": 97, "teams": ["frc195", "frc1923", "frc155"]}}""")
        self.assertEqual(match.time_string, "3:23 PM")

########NEW FILE########
__FILENAME__ = test_datafeed_usfirst_teams
import unittest2
import datetime

from google.appengine.ext import db
from google.appengine.ext import testbed

from datafeeds.datafeed_usfirst import DatafeedUsfirst
from models.team import Team


class TestDatafeedUsfirstTeams(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_memcache_stub()

        self.datafeed = DatafeedUsfirst()

        self.team177 = Team(
            id="frc177",
            team_number=177,
            first_tpid=61771,
            first_tpid_year=2012
        )
        self.team177.put()

    def tearDown(self):
        self.testbed.deactivate()

    def test_getTeamDetails(self):
        team = self.datafeed.getTeamDetails(self.team177)

        self.assertEqual(team.name, "UTC Power/Ensign Bickford Aerospace & Defense & South Windsor High School")
        self.assertEqual(team.address, u"South Windsor, CT, USA")
        self.assertEqual(team.nickname, "Bobcat Robotics")
        self.assertEqual(team.website, "http://www.bobcatrobotics.org")

    def test_getTeamsTpids(self):
        Team(
          id="frc4409",
          team_number=4409,
          first_tpid=0,  # should be 74735
          first_tpid_year=2011
        ).put()

        # We can skip 2000 records, paginate, and still get frc4409 and frc4410 in 2012
        self.datafeed.getTeamsTpids(2012, skip=2000)

        # Check new team insertion
        frc4410 = Team.get_by_id("frc4410")
        self.assertEqual(frc4410.team_number, 4410)
        self.assertEqual(frc4410.first_tpid, 74193)
        self.assertEqual(frc4410.first_tpid_year, 2012)

        # Check old team updating
        frc4409 = Team.get_by_id("frc4409")
        self.assertEqual(frc4409.team_number, 4409)
        self.assertEqual(frc4409.first_tpid, 74735)
        self.assertEqual(frc4409.first_tpid_year, 2012)

########NEW FILE########
__FILENAME__ = test_event
import datetime
import unittest2
import json

from google.appengine.ext import ndb
from google.appengine.ext import testbed

from helpers.event.event_test_creator import EventTestCreator


class TestEventManipulator(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_memcache_stub()

        self.future_event = EventTestCreator.createFutureEvent(only_event=True)
        self.present_event = EventTestCreator.createPresentEvent(only_event=True)
        self.past_event = EventTestCreator.createPastEvent(only_event=True)

    def tearDown(self):
        self.future_event.key.delete()
        self.present_event.key.delete()
        self.past_event.key.delete()

        self.testbed.deactivate()

    def test_datesFuture(self):
        self.assertFalse(self.future_event.now)
        self.assertTrue(self.future_event.future)
        self.assertFalse(self.future_event.past)
        self.assertFalse(self.future_event.withinDays(0, 8))
        self.assertTrue(self.future_event.withinDays(-8, 0))

    def test_datesPast(self):
        self.assertFalse(self.past_event.now)
        self.assertFalse(self.past_event.future)
        self.assertTrue(self.past_event.past)
        self.assertTrue(self.past_event.withinDays(0, 8))
        self.assertFalse(self.past_event.withinDays(-8, 0))

    def test_datesPresent(self):
        self.assertTrue(self.present_event.now)
        self.assertTrue(self.present_event.withinDays(-1, 1))
        self.assertFalse(self.present_event.future)
        self.assertFalse(self.present_event.past)

########NEW FILE########
__FILENAME__ = test_event_api
import json
import os
import unittest2
import webtest
import webapp2

from datetime import datetime

from google.appengine.ext import ndb
from google.appengine.ext import testbed

from consts.event_type import EventType

from controllers.api_controller import ApiEventsShow, ApiEventList, ApiMatchDetails

from models.event import Event
from models.event_team import EventTeam
from models.match import Match
from models.team import Team


class TestApiEventList(unittest2.TestCase):

    def setUp(self):
        app = webapp2.WSGIApplication([(r'/', ApiEventList)], debug=True)
        self.testapp = webtest.TestApp(app)

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_memcache_stub()
        self.testbed.init_taskqueue_stub()

        self.event = Event(
            id="2010sc",
            name="Palmetto Regional",
            event_type_enum=EventType.REGIONAL,
            short_name="Palmetto",
            event_short="sc",
            year=2010,
            end_date=datetime(2010, 03, 27),
            official=True,
            location='Clemson, SC',
            start_date=datetime(2010, 03, 24)
        )
        self.event.put()

    def tearDown(self):
        self.testbed.deactivate()

    def assertEventJson(self, event_dict):
        self.assertEqual(event_dict["key"], self.event.key_name)
        self.assertEqual(event_dict["name"], self.event.name)
        self.assertEqual(event_dict["short_name"], self.event.short_name)
        self.assertEqual(event_dict["official"], self.event.official)
        self.assertEqual(event_dict["start_date"], self.event.start_date.isoformat())
        self.assertEqual(event_dict["end_date"], self.event.end_date.isoformat())

    def test_event_show(self):
        response = self.testapp.get('/?year=2010', headers={"X-TBA-App-Id": "tba-tests:event-api-test:v01"})

        event_dict = json.loads(response.body)
        self.assertEventJson(event_dict[0])

    def test_validate_tba_app_id(self):
        response = self.testapp.get('/?year=2010', expect_errors=True)  # By default get() doesn't send a user agent
        self.assertEqual(response.status, "400 Bad Request")
        self.assertTrue('Error' in json.loads(response.body).keys())


class TestApiMatchDetails(unittest2.TestCase):

    def setUp(self):
        app = webapp2.WSGIApplication([(r'/', ApiMatchDetails)], debug=True)
        self.testapp = webtest.TestApp(app)

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_memcache_stub()
        self.testbed.init_taskqueue_stub()

        events = {
            '2010cmp': Event(
                id="2010cmp",
                name="Einstein Field",
                event_type_enum=EventType.CMP_FINALS,
                short_name="Einstein",
                event_short="cmp",
                year=2010,
                start_date=datetime(2010, 04, 17),
                end_date=datetime(2010, 04, 17),
                official=True,
                location='Atlanta, GA'
            ),
            '2011cmp': Event(
                id="2011cmp",
                name="Einstein Field",
                event_type_enum=EventType.CMP_FINALS,
                short_name="Einstein",
                event_short="cmp",
                year=2011,
                start_date=datetime(2011, 04, 30),
                end_date=datetime(2011, 04, 30),
                official=True,
                location='St. Louis, MO'
            ),
            '2012cmp': Event(
                id="2012cmp",
                name="Einstein Field",
                event_type_enum=EventType.CMP_FINALS,
                short_name="Einstein",
                event_short="cmp",
                year=2012,
                start_date=datetime(2012, 04, 26),
                end_date=datetime(2012, 04, 28),
                official=True,
                location='St. Louis, MO'
            )
        }

        event_keys = {}

        for event_id, event in events.items():
            event_keys[event_id] = event.put()

        self.matches = {
            '2010cmp_f1m1': Match(
                id='2010cmp_f1m1',
                comp_level='f',
                match_number=1,
                team_key_names=["frc177", "frc67", "frc294", "frc469", "frc1114", "frc2041"],
                alliances_json='{"blue": {"score": 14, "teams": ["frc469", "frc1114", "frc2041"]}, "red": {"score": 16, "teams": ["frc177", "frc67", "frc294"]}}',
                set_number=1,
                game='frc_2010_bkwy',
                event=event_keys['2010cmp']
            ),
            '2011cmp_f1m1': Match(
                id='2011cmp_f1m1',
                comp_level='f',
                match_number=1,
                team_key_names=["frc973", "frc254", "frc111", "frc177", "frc2016", "frc781"],
                alliances_json='{"blue": {"score": 79, "teams": ["frc177", "frc2016", "frc781"]}, "red": {"score": 147, "teams": ["frc973", "frc254", "frc111"]}}',
                set_number=1,
                game='frc_2011_logo',
                event=event_keys['2011cmp']
            ),
            '2012cmp_f1m1': Match(
                id='2012cmp_f1m1',
                comp_level='f',
                match_number=1,
                team_key_names=["frc25", "frc180", "frc16", "frc207", "frc233", "frc987"],
                alliances_json='{"blue": {"score": 45, "teams": ["frc207", "frc233", "frc987"]}, "red": {"score": 89, "teams": ["frc25", "frc180", "frc16"]}}',
                set_number=1,
                game='frc_2012_rebr',
                event=event_keys['2012cmp']
            )
        }

        for match in self.matches.values():
            match.put()

    def tearDown(self):
        self.testbed.deactivate()

    def assertMatch(self, match):
        match_id = match["key"]

        self.assertEqual(match["event"], self.matches[match_id].event.get().key_name)
        self.assertEqual(match["competition_level"], self.matches[match_id].name)
        self.assertEqual(match["set_number"], self.matches[match_id].set_number)
        self.assertEqual(match["match_number"], self.matches[match_id].match_number)
        self.assertEqual(match["team_keys"], self.matches[match_id].team_key_names)
        self.assertEqual(match["alliances"], self.matches[match_id].alliances)

    def assertMatchNames(self, match_list):
        match_names = ",".join(match_list)
        api_names = ["match", "matches"]

        for api_name in api_names:
            response = self.testapp.get("/?" + api_name + "=" + match_names, headers={"X-TBA-App-Id": "tba-tests:event-api-test:v01"})

            matches = json.loads(response.body)
            for match in matches:
                self.assertIn(match["key"], self.matches)
                self.assertMatch(match)

    def test_match_details(self):
        self.assertMatchNames(["2010cmp_f1m1"])
        self.assertMatchNames(["2010cmp_f1m1", "2011cmp_f1m1", "2012cmp_f1m1"])

########NEW FILE########
__FILENAME__ = test_event_district_name_parser
import unittest2

from consts.district_type import DistrictType
from helpers.event_helper import EventHelper


class TestEventDistrictNameParser(unittest2.TestCase):
    def test_event_parse_district_name(self):
        """
        A bunch of tests from various years
        """
        self.assertEqual(EventHelper.parseDistrictName('FIRST in Michigan'), DistrictType.MICHIGAN)
        self.assertEqual(EventHelper.parseDistrictName('Mid-Atlantic Robotics'), DistrictType.MID_ATLANTIC)
        self.assertEqual(EventHelper.parseDistrictName('New England'), DistrictType.NEW_ENGLAND)
        self.assertEqual(EventHelper.parseDistrictName('Pacific Northwest'), DistrictType.PACIFIC_NORTHWEST)
        self.assertEqual(EventHelper.parseDistrictName('Not a valid district'), DistrictType.NO_DISTRICT)
        self.assertEqual(EventHelper.parseDistrictName('California'), DistrictType.NO_DISTRICT)
        self.assertEqual(EventHelper.parseDistrictName(None), DistrictType.NO_DISTRICT)
        self.assertEqual(EventHelper.parseDistrictName(''), DistrictType.NO_DISTRICT)

########NEW FILE########
__FILENAME__ = test_event_get_short_name
import unittest2

from helpers.event_helper import EventHelper


class TestEventGetShortName(unittest2.TestCase):
    def test_event_get_short_name(self):
        """
        A bunch of tests from various years
        """
        self.assertEqual(EventHelper.getShortName("FIRST Robotics Competition"), "FIRST Robotics Competition")
        self.assertEqual(EventHelper.getShortName("National Championship"), "National Championship")
        self.assertEqual(EventHelper.getShortName("New England Tournament"), "New England")
        self.assertEqual(EventHelper.getShortName("FIRST National Championship"), "FIRST National Championship")
        self.assertEqual(EventHelper.getShortName("Motorola Midwest Regional"), "Motorola Midwest")
        self.assertEqual(EventHelper.getShortName("DEKA New England Regional"), "DEKA New England")
        self.assertEqual(EventHelper.getShortName("Johnson & Johnson Mid-Atlantic Regional"), "Johnson & Johnson Mid-Atlantic")
        self.assertEqual(EventHelper.getShortName("Great Lakes Regional"), "Great Lakes")
        self.assertEqual(EventHelper.getShortName("New England Regional"), "New England")
        self.assertEqual(EventHelper.getShortName("Southwest Regional"), "Southwest")
        self.assertEqual(EventHelper.getShortName("NASA Ames Regional"), "NASA Ames")
        self.assertEqual(EventHelper.getShortName("Kennedy Space Center Regional"), "Kennedy Space Center")
        self.assertEqual(EventHelper.getShortName("UTC New England Regional"), "UTC New England")
        self.assertEqual(EventHelper.getShortName("Philadelphia Alliance Regional"), "Philadelphia Alliance")
        self.assertEqual(EventHelper.getShortName("Kennedy Space Center Southeast Regional"), "Kennedy Space Center Southeast")
        self.assertEqual(EventHelper.getShortName("Long Island Regional"), "Long Island")
        self.assertEqual(EventHelper.getShortName("Lone Star Regional"), "Lone Star")
        self.assertEqual(EventHelper.getShortName("NASA Langley/VCU Regional"), "NASA Langley/VCU")
        self.assertEqual(EventHelper.getShortName("Archimedes Field"), "Archimedes")
        self.assertEqual(EventHelper.getShortName("Southern California Regional"), "Southern California")
        self.assertEqual(EventHelper.getShortName("Silicon Valley Regional"), "Silicon Valley")
        self.assertEqual(EventHelper.getShortName("UTC/New England Regional"), "UTC/New England")
        self.assertEqual(EventHelper.getShortName("Curie Field"), "Curie")
        self.assertEqual(EventHelper.getShortName("NASA KSC Southeast Regional"), "NASA KSC Southeast")
        self.assertEqual(EventHelper.getShortName("Galileo Field"), "Galileo")
        self.assertEqual(EventHelper.getShortName("West Michigan Regional"), "West Michigan")
        self.assertEqual(EventHelper.getShortName("Newton Field"), "Newton")
        self.assertEqual(EventHelper.getShortName("J&J Mid-Atlantic Regional"), "J&J Mid-Atlantic")
        self.assertEqual(EventHelper.getShortName("New York City Regional"), "New York City")
        self.assertEqual(EventHelper.getShortName("NASA Langley Regional"), "NASA Langley")
        self.assertEqual(EventHelper.getShortName("SBPLI Long Island Regional"), "SBPLI Long Island")
        self.assertEqual(EventHelper.getShortName("Western Michigan Regional"), "Western Michigan")
        self.assertEqual(EventHelper.getShortName("St. Louis Regional"), "St. Louis")
        self.assertEqual(EventHelper.getShortName("J&J Mid Atlantic Regional"), "J&J Mid Atlantic")
        self.assertEqual(EventHelper.getShortName("Buckeye Regional"), "Buckeye")
        self.assertEqual(EventHelper.getShortName("Canadian Regional"), "Canadian")
        self.assertEqual(EventHelper.getShortName("NASA Langley / VCU Regional"), "NASA Langley / VCU")
        self.assertEqual(EventHelper.getShortName("Pacific Northwest Regional"), "Pacific Northwest")
        self.assertEqual(EventHelper.getShortName("Arizona Regional"), "Arizona")
        self.assertEqual(EventHelper.getShortName("Einstein Field"), "Einstein")
        self.assertEqual(EventHelper.getShortName("Central Florida Regional"), "Central Florida")
        self.assertEqual(EventHelper.getShortName("Peachtree Regional"), "Peachtree")
        self.assertEqual(EventHelper.getShortName("Midwest Regional"), "Midwest")
        self.assertEqual(EventHelper.getShortName("Chesapeake Regional"), "Chesapeake")
        self.assertEqual(EventHelper.getShortName("BAE SYSTEMS Granite State Regional"), "BAE SYSTEMS Granite State")
        self.assertEqual(EventHelper.getShortName("Philadelphia Regional"), "Philadelphia")
        self.assertEqual(EventHelper.getShortName("Pittsburgh Regional"), "Pittsburgh")
        self.assertEqual(EventHelper.getShortName("Sacramento Regional"), "Sacramento")
        self.assertEqual(EventHelper.getShortName("NASA / VCU Regional"), "NASA / VCU")
        self.assertEqual(EventHelper.getShortName("Colorado Regional"), "Colorado")
        self.assertEqual(EventHelper.getShortName("Detroit Regional"), "Detroit")
        self.assertEqual(EventHelper.getShortName("Florida Regional"), "Florida")
        self.assertEqual(EventHelper.getShortName("New Jersey Regional"), "New Jersey")
        self.assertEqual(EventHelper.getShortName("Greater Toronto Regional"), "Greater Toronto")
        self.assertEqual(EventHelper.getShortName("Palmetto Regional"), "Palmetto")
        self.assertEqual(EventHelper.getShortName("Boilermaker Regional"), "Boilermaker")
        self.assertEqual(EventHelper.getShortName("GM/Technion University Israel Pilot Regional"), "GM/Technion University Israel Pilot")
        self.assertEqual(EventHelper.getShortName("Las Vegas Regional"), "Las Vegas")
        self.assertEqual(EventHelper.getShortName("Finger Lakes Regional"), "Finger Lakes")
        self.assertEqual(EventHelper.getShortName("Waterloo Regional"), "Waterloo")
        self.assertEqual(EventHelper.getShortName("GM/Technion Israel Regional"), "GM/Technion Israel")
        self.assertEqual(EventHelper.getShortName("Boston Regional"), "Boston")
        self.assertEqual(EventHelper.getShortName("Davis Sacramento Regional"), "Davis Sacramento")
        self.assertEqual(EventHelper.getShortName("Wisconsin Regional"), "Wisconsin")
        self.assertEqual(EventHelper.getShortName("Brazil Pilot"), "Brazil Pilot")
        self.assertEqual(EventHelper.getShortName("Los Angeles Regional"), "Los Angeles")
        self.assertEqual(EventHelper.getShortName("UTC Connecticut Regional"), "UTC Connecticut")
        self.assertEqual(EventHelper.getShortName("Greater Kansas City Regional"), "Greater Kansas City")
        self.assertEqual(EventHelper.getShortName("Bayou Regional"), "Bayou")
        self.assertEqual(EventHelper.getShortName("San Diego Regional"), "San Diego")
        self.assertEqual(EventHelper.getShortName("Brazil Regional"), "Brazil")
        self.assertEqual(EventHelper.getShortName("Connecticut Regional"), "Connecticut")
        self.assertEqual(EventHelper.getShortName("Hawaii Regional"), "Hawaii")
        self.assertEqual(EventHelper.getShortName("Israel Regional"), "Israel")
        self.assertEqual(EventHelper.getShortName("Minnesota Regional"), "Minnesota")
        self.assertEqual(EventHelper.getShortName("BAE Systems Granite State Regional"), "BAE Systems Granite State")
        self.assertEqual(EventHelper.getShortName("Oklahoma City Regional"), "Oklahoma City")
        self.assertEqual(EventHelper.getShortName("Oregon Regional"), "Oregon")
        self.assertEqual(EventHelper.getShortName("UC Davis Sacramento Regional"), "UC Davis Sacramento")
        self.assertEqual(EventHelper.getShortName("Microsoft Seattle Regional"), "Microsoft Seattle")
        self.assertEqual(EventHelper.getShortName("Dallas Regional, Sponsored by JCPenney and the JCPenney Afterschool Fund"), "Dallas")
        self.assertEqual(EventHelper.getShortName("Washington DC  Regional"), "Washington DC")
        self.assertEqual(EventHelper.getShortName("Detroit FIRST Robotics District Competition"), "Detroit")
        self.assertEqual(EventHelper.getShortName("Cass Tech FIRST Robotics District Competition"), "Cass Tech")
        self.assertEqual(EventHelper.getShortName("Kettering University FIRST Robotics District Competition"), "Kettering University")
        self.assertEqual(EventHelper.getShortName("Michigan FIRST Robotics Competition State Championship"), "Michigan")
        self.assertEqual(EventHelper.getShortName("Lansing FIRST Robotics District Competition"), "Lansing")
        self.assertEqual(EventHelper.getShortName("Traverse City FIRST Robotics District Competition"), "Traverse City")
        self.assertEqual(EventHelper.getShortName("West Michigan FIRST Robotics District Competition"), "West Michigan")
        self.assertEqual(EventHelper.getShortName("Minnesota 10000 Lakes Regional"), "Minnesota 10000 Lakes")
        self.assertEqual(EventHelper.getShortName("Minnesota North Star Regional"), "Minnesota North Star")
        self.assertEqual(EventHelper.getShortName("BAE Granite State Regional"), "BAE Granite State")
        self.assertEqual(EventHelper.getShortName("Troy FIRST Robotics District Competition"), "Troy")
        self.assertEqual(EventHelper.getShortName("NASA VCU Regional"), "NASA VCU")
        self.assertEqual(EventHelper.getShortName("Northeast Utilities FIRST Connecticut Regional"), "Northeast Utilities FIRST Connecticut")
        self.assertEqual(EventHelper.getShortName("Dallas Regional sponsored by JCPenney and the JCPenney Afterschool Fund"), "Dallas")
        self.assertEqual(EventHelper.getShortName("Hawaii Regional sponsored by BAE Systems"), "Hawaii")
        self.assertEqual(EventHelper.getShortName("North Carolina Regional"), "North Carolina")
        self.assertEqual(EventHelper.getShortName("Oklahoma Regional"), "Oklahoma")
        self.assertEqual(EventHelper.getShortName("Autodesk Oregon Regional"), "Autodesk Oregon")
        self.assertEqual(EventHelper.getShortName("Silicon Valley Regional sponsored by Google and BAE Systems"), "Silicon Valley")
        self.assertEqual(EventHelper.getShortName("Utah Regional sponsored by NASA & Platt"), "Utah")
        self.assertEqual(EventHelper.getShortName("Virginia Regional"), "Virginia")
        self.assertEqual(EventHelper.getShortName("Ann Arbor FIRST Robotics District Competition"), "Ann Arbor")
        self.assertEqual(EventHelper.getShortName("WPI Regional"), "WPI")
        self.assertEqual(EventHelper.getShortName("Dallas Regional sponsored by jcpenney"), "Dallas")
        self.assertEqual(EventHelper.getShortName("Lake Superior Regional"), "Lake Superior")
        self.assertEqual(EventHelper.getShortName("Michigan FIRST Robotics District Competition State Championship"), "Michigan")
        self.assertEqual(EventHelper.getShortName("BAE Systems/Granite State Regional"), "BAE Systems/Granite State")
        self.assertEqual(EventHelper.getShortName("Waterford FIRST Robotics District Competition"), "Waterford")
        self.assertEqual(EventHelper.getShortName("Greater Toronto East Regional"), "Greater Toronto East")
        self.assertEqual(EventHelper.getShortName("Greater Toronto West Regional"), "Greater Toronto West")
        self.assertEqual(EventHelper.getShortName("Alamo Regional"), "Alamo")
        self.assertEqual(EventHelper.getShortName("Niles FIRST Robotics District Competition"), "Niles")
        self.assertEqual(EventHelper.getShortName("Smoky Mountain Regional"), "Smoky Mountain")
        self.assertEqual(EventHelper.getShortName("Utah Regional co-sponsored by NASA and Platt"), "Utah")
        self.assertEqual(EventHelper.getShortName("Seattle Olympic Regional"), "Seattle Olympic")
        self.assertEqual(EventHelper.getShortName("Seattle Cascade Regional"), "Seattle Cascade")
        self.assertEqual(EventHelper.getShortName("Livonia FIRST Robotics District Competition"), "Livonia")
        self.assertEqual(EventHelper.getShortName("Central Valley Regional"), "Central Valley")
        self.assertEqual(EventHelper.getShortName("Dallas East Regional sponsored by jcpenney"), "Dallas East")
        self.assertEqual(EventHelper.getShortName("Dallas West Regional sponsored by jcpenney"), "Dallas West")
        self.assertEqual(EventHelper.getShortName("Orlando Regional"), "Orlando")
        self.assertEqual(EventHelper.getShortName("Michigan FRC State Championship"), "Michigan")
        self.assertEqual(EventHelper.getShortName("Gull Lake FIRST Robotics District Competition"), "Gull Lake")
        self.assertEqual(EventHelper.getShortName("Rutgers University FIRST Robotics District Competition"), "Rutgers University")
        self.assertEqual(EventHelper.getShortName("Mount Olive FIRST Robotics District Competition"), "Mount Olive")
        self.assertEqual(EventHelper.getShortName("Lenape FIRST Robotics District Competition"), "Lenape")
        self.assertEqual(EventHelper.getShortName("Queen City Regional"), "Queen City")
        self.assertEqual(EventHelper.getShortName("Mid-Atlantic Robotics FRC Region Championship"), "Mid-Atlantic Robotics")
        self.assertEqual(EventHelper.getShortName("Hatboro-Horsham FIRST Robotics District Competition"), "Hatboro-Horsham")
        self.assertEqual(EventHelper.getShortName("Chestnut Hill FIRST Robotics District Competition"), "Chestnut Hill")
        self.assertEqual(EventHelper.getShortName("Festival de Robotique FRC a Montreal Regional"), "Festival de Robotique")
        self.assertEqual(EventHelper.getShortName("South Florida Regional"), "South Florida")
        self.assertEqual(EventHelper.getShortName("Smoky Mountains Regional"), "Smoky Mountains")
        self.assertEqual(EventHelper.getShortName("Spokane Regional"), "Spokane")
        self.assertEqual(EventHelper.getShortName("Northville FIRST Robotics District Competition"), "Northville")
        self.assertEqual(EventHelper.getShortName("Western Canadian FRC Regional"), "Western Canadian")
        self.assertEqual(EventHelper.getShortName("Razorback Regional"), "Razorback")
        self.assertEqual(EventHelper.getShortName("Phoenix Regional"), "Phoenix")
        self.assertEqual(EventHelper.getShortName("Los Angeles Regional sponsored by The Roddenberry Foundation"), "Los Angeles")
        self.assertEqual(EventHelper.getShortName("Inland Empire Regional"), "Inland Empire")
        self.assertEqual(EventHelper.getShortName("Connecticut Regional sponsored by UTC"), "Connecticut")
        self.assertEqual(EventHelper.getShortName("Crossroads Regional"), "Crossroads")
        self.assertEqual(EventHelper.getShortName("Pine Tree Regional"), "Pine Tree")
        self.assertEqual(EventHelper.getShortName("Bedford FIRST Robotics District Competition"), "Bedford")
        self.assertEqual(EventHelper.getShortName("Grand Blanc FIRST Robotics District Competition"), "Grand Blanc")
        self.assertEqual(EventHelper.getShortName("St Joseph FIRST Robotics District Competition"), "St Joseph")
        self.assertEqual(EventHelper.getShortName("Northern Lights Regional"), "Northern Lights")
        self.assertEqual(EventHelper.getShortName("Bridgewater-Raritan FIRST Robotics District Competition"), "Bridgewater-Raritan")
        self.assertEqual(EventHelper.getShortName("TCNJ FIRST Robotics District Competition"), "TCNJ")
        self.assertEqual(EventHelper.getShortName("Lenape Seneca FIRST Robotics District Competition"), "Lenape Seneca")
        self.assertEqual(EventHelper.getShortName("Springside - Chestnut Hill FIRST Robotics District Competition"), "Springside - Chestnut Hill")
        self.assertEqual(EventHelper.getShortName("Festival de Robotique FRC de Montreal Regional"), "Festival de Robotique")
        self.assertEqual(EventHelper.getShortName("Dallas Regional"), "Dallas")
        self.assertEqual(EventHelper.getShortName("Hub City Regional"), "Hub City")
        self.assertEqual(EventHelper.getShortName("Alamo Regional sponsored by Rackspace Hosting"), "Alamo")
        self.assertEqual(EventHelper.getShortName("Utah Regional co-sponsored by the Larry H. Miller Group & Platt"), "Utah")
        self.assertEqual(EventHelper.getShortName("Seattle Regional"), "Seattle")
        self.assertEqual(EventHelper.getShortName("Central Washington Regional"), "Central Washington")
        self.assertEqual(EventHelper.getShortName("Western Canada Regional"), "Western Canada")
        self.assertEqual(EventHelper.getShortName("Arkansas Regional"), "Arkansas")
        self.assertEqual(EventHelper.getShortName("Groton District Event"), "Groton")
        self.assertEqual(EventHelper.getShortName("Hartford District Event"), "Hartford")
        self.assertEqual(EventHelper.getShortName("Southington District Event"), "Southington")
        self.assertEqual(EventHelper.getShortName("Greater DC Regional"), "Greater DC")
        self.assertEqual(EventHelper.getShortName("Central Illinois Regional"), "Central Illinois")
        self.assertEqual(EventHelper.getShortName("Northeastern University District Event"), "Northeastern University")
        self.assertEqual(EventHelper.getShortName("WPI District Event"), "WPI")
        self.assertEqual(EventHelper.getShortName("Pine Tree District Event"), "Pine Tree")
        self.assertEqual(EventHelper.getShortName("Center Line FIRST Robotics District Competition"), "Center Line")
        self.assertEqual(EventHelper.getShortName("Escanaba FIRST Robotics District Competition"), "Escanaba")
        self.assertEqual(EventHelper.getShortName("Howell FIRST Robotics District Competition"), "Howell")
        self.assertEqual(EventHelper.getShortName("St. Joseph FIRST Robotics District Competition"), "St. Joseph")
        self.assertEqual(EventHelper.getShortName("Southfield FIRST Robotics District Competition"), "Southfield")
        self.assertEqual(EventHelper.getShortName("Mexico City Regional"), "Mexico City")
        self.assertEqual(EventHelper.getShortName("New England FRC Region Championship"), "New England")
        self.assertEqual(EventHelper.getShortName("UNH District Event"), "UNH")
        self.assertEqual(EventHelper.getShortName("Granite State District Event"), "Granite State")
        self.assertEqual(EventHelper.getShortName("MAR FIRST Robotics Bridgewater-Raritan District Competition"), "Bridgewater-Raritan")
        self.assertEqual(EventHelper.getShortName("MAR FIRST Robotics Clifton District Competition"), "Clifton")
        self.assertEqual(EventHelper.getShortName("MAR FIRST Robotics Mt. Olive District Competition"), "Mt. Olive")
        self.assertEqual(EventHelper.getShortName("MAR FIRST Robotics Lenape-Seneca District Competition"), "Lenape-Seneca")
        self.assertEqual(EventHelper.getShortName("New York Tech Valley Regional"), "New York Tech Valley")
        self.assertEqual(EventHelper.getShortName("North Bay Regional"), "North Bay")
        self.assertEqual(EventHelper.getShortName("Windsor Essex Great Lakes Regional"), "Windsor Essex Great Lakes")
        self.assertEqual(EventHelper.getShortName("PNW FIRST Robotics Oregon City District Event"), "Oregon City")
        self.assertEqual(EventHelper.getShortName("PNW FIRST Robotics Oregon State University District Event"), "Oregon State University")
        self.assertEqual(EventHelper.getShortName("PNW FIRST Robotics Wilsonville District Event"), "Wilsonville")
        self.assertEqual(EventHelper.getShortName("MAR FIRST Robotics Hatboro-Horsham District Competition"), "Hatboro-Horsham")
        self.assertEqual(EventHelper.getShortName("MAR FIRST Robotics Springside Chestnut Hill District Competition"), "Springside Chestnut Hill")
        self.assertEqual(EventHelper.getShortName("Greater Pittsburgh Regional"), "Greater Pittsburgh")
        self.assertEqual(EventHelper.getShortName("Autodesk PNW FRC Championship"), "Autodesk PNW")
        self.assertEqual(EventHelper.getShortName("Rhode Island District Event"), "Rhode Island")
        self.assertEqual(EventHelper.getShortName("Utah Regional"), "Utah")
        self.assertEqual(EventHelper.getShortName("PNW FIRST Robotics Auburn District Event"), "Auburn")
        self.assertEqual(EventHelper.getShortName("PNW FIRST Robotics Auburn Mountainview District Event"), "Auburn Mountainview")
        self.assertEqual(EventHelper.getShortName("PNW FIRST Robotics Eastern Washington University District Event"), "Eastern Washington University")
        self.assertEqual(EventHelper.getShortName("PNW FIRST Robotics Central Washington University District Event"), "Central Washington University")
        self.assertEqual(EventHelper.getShortName("PNW FIRST Robotics Mt. Vernon District Event"), "Mt. Vernon")
        self.assertEqual(EventHelper.getShortName("PNW FIRST Robotics Shorewood District Event"), "Shorewood")
        self.assertEqual(EventHelper.getShortName("PNW FIRST Robotics Glacier Peak District Event"), "Glacier Peak")

########NEW FILE########
__FILENAME__ = test_event_get_timezone_id
import unittest2

from google.appengine.ext import testbed

from datafeeds.usfirst_event_details_parser import UsfirstEventDetailsParser
from helpers.event_helper import EventHelper


class TestEventGetTimezoneId(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_urlfetch_stub()

    def test_2012ct_no_location(self):
        with open('test_data/usfirst_html/usfirst_event_details_2012ct.html', 'r') as f:
            event, _ = UsfirstEventDetailsParser.parse(f.read())
        event['location'] = None
        self.assertEqual(EventHelper.get_timezone_id(event), None)

    def test_2012ct_bad_location(self):
        with open('test_data/usfirst_html/usfirst_event_details_2012ct.html', 'r') as f:
            event, _ = UsfirstEventDetailsParser.parse(f.read())
        event['location'] = "somewhere on mars"
        self.assertEqual(EventHelper.get_timezone_id(event), None)

    def test_2012ct(self):
        with open('test_data/usfirst_html/usfirst_event_details_2012ct.html', 'r') as f:
            event, _ = UsfirstEventDetailsParser.parse(f.read())

        self.assertEqual(EventHelper.get_timezone_id(event), 'America/New_York')

    def test_2013flbr(self):
        with open('test_data/usfirst_html/usfirst_event_details_2013flbr.html', 'r') as f:
            event, _ = UsfirstEventDetailsParser.parse(f.read())

        self.assertEqual(EventHelper.get_timezone_id(event), 'America/New_York')

    def test_2013casj(self):
        with open('test_data/usfirst_html/usfirst_event_details_2013casj.html', 'r') as f:
            event, _ = UsfirstEventDetailsParser.parse(f.read())

        self.assertEqual(EventHelper.get_timezone_id(event), 'America/Los_Angeles')

    def test_2001sj(self):
        with open('test_data/usfirst_html/usfirst_event_details_2001ca2.html', 'r') as f:
            event, _ = UsfirstEventDetailsParser.parse(f.read())

        self.assertEqual(EventHelper.get_timezone_id(event), 'America/Los_Angeles')

    def test_2005is(self):
        with open('test_data/usfirst_html/usfirst_event_details_2005is.html', 'r') as f:
            event, _ = UsfirstEventDetailsParser.parse(f.read())

        self.assertEqual(EventHelper.get_timezone_id(event), 'Asia/Jerusalem')

    def test_2005or(self):
        with open('test_data/usfirst_html/usfirst_event_details_2005or.html', 'r') as f:
            event, _ = UsfirstEventDetailsParser.parse(f.read())

        self.assertEqual(EventHelper.get_timezone_id(event), 'America/Los_Angeles')

    def test_1997il(self):
        with open('test_data/usfirst_html/usfirst_event_details_1997il.html', 'r') as f:
            event, _ = UsfirstEventDetailsParser.parse(f.read())

        self.assertEqual(EventHelper.get_timezone_id(event), 'America/Chicago')

    def test_2002sj(self):
        with open('test_data/usfirst_html/usfirst_event_details_2002sj.html', 'r') as f:
            event, _ = UsfirstEventDetailsParser.parse(f.read())

        self.assertEqual(EventHelper.get_timezone_id(event), 'America/Los_Angeles')

########NEW FILE########
__FILENAME__ = test_event_group_by_week
import unittest2
import time
import datetime
import random
import logging

from consts.event_type import EventType
from helpers.event_helper import EventHelper, REGIONAL_EVENTS_LABEL, CHAMPIONSHIP_EVENTS_LABEL,\
    WEEKLESS_EVENTS_LABEL, OFFSEASON_EVENTS_LABEL, PRESEASON_EVENTS_LABEL
from models.event import Event


class TestEventGroupByWeek(unittest2.TestCase):
    def test_group_by_week(self):
        """
        All events that start in the same span of Thursday-Wednesday
        should be considered as being in the same week.
        """
        events_by_week = {}  # we will use this as the "answer key"

        # Generate random regional events
        seed = int(time.time())
        state = random.Random()
        state.seed(seed)

        event_id_counter = 0
        week_start = datetime.datetime(2013, 2, 28)
        for i in range(1, 7):  # test for 6 weeks
            for _ in range(state.randint(1, 15)):  # random number of events per week
                week_label = REGIONAL_EVENTS_LABEL.format(i)

                start_date = week_start + datetime.timedelta(days=state.randint(0, 6))
                end_date = start_date + datetime.timedelta(days=state.randint(0, 3))

                event = Event(
                    id='2013tst{}'.format(event_id_counter),
                    event_short='tst{}'.format(event_id_counter),
                    start_date=start_date,
                    end_date=end_date,
                    official=True,
                    event_type_enum=state.choice([EventType.REGIONAL, EventType.DISTRICT, EventType.DISTRICT_CMP])
                )

                if week_label in events_by_week:
                    events_by_week[week_label].append(event)
                else:
                    events_by_week[week_label] = [event]

                event_id_counter += 1
            week_start = week_start + datetime.timedelta(days=7)

        # Generate Championship events
        week_start += datetime.timedelta(days=7)
        events_by_week[CHAMPIONSHIP_EVENTS_LABEL] = [
            Event(
                id='2013arc'.format(event_id_counter),
                event_short='arc',
                start_date=week_start,
                end_date=week_start + datetime.timedelta(days=2),
                official=True,
                event_type_enum=EventType.CMP_DIVISION
            ),
            Event(
                id='2013gal'.format(event_id_counter),
                event_short='gal',
                start_date=week_start,
                end_date=week_start + datetime.timedelta(days=2),
                official=True,
                event_type_enum=EventType.CMP_DIVISION
            ),
            Event(
                id='2013cmp'.format(event_id_counter),
                event_short='cmp',
                start_date=week_start + datetime.timedelta(days=2),
                end_date=week_start + datetime.timedelta(days=2),
                official=True,
                event_type_enum=EventType.CMP_FINALS
            )
        ]

        # Generate official events with no dates
        events_by_week[WEEKLESS_EVENTS_LABEL] = [
            Event(
                id='2013weekless1'.format(event_id_counter),
                event_short='weekless1',
                official=True,
                event_type_enum=state.choice([EventType.REGIONAL, EventType.DISTRICT, EventType.DISTRICT_CMP])
            ),
            Event(
                id='2013weekless2'.format(event_id_counter),
                event_short='weekless2',
                official=True,
                event_type_enum=state.choice([EventType.REGIONAL, EventType.DISTRICT, EventType.DISTRICT_CMP])
            ),
            Event(
                id='2013weekless3'.format(event_id_counter),
                event_short='weekless3',
                official=True,
                event_type_enum=state.choice([EventType.REGIONAL, EventType.DISTRICT, EventType.DISTRICT_CMP])
            ),
            Event(
                id='2013weekless4'.format(event_id_counter),
                event_short='weekless4',
                start_date=datetime.datetime(2013, 12, 31),
                end_date=datetime.datetime(2013, 12, 31),
                official=True,
                event_type_enum=state.choice([EventType.REGIONAL, EventType.DISTRICT, EventType.DISTRICT_CMP])
            )
        ]

        # Generate preseason events
        events_by_week[PRESEASON_EVENTS_LABEL] = [
            Event(
                id='2013preseason1'.format(event_id_counter),
                event_short='preseason1',
                official=False,
                event_type_enum=EventType.PRESEASON
            ),
            Event(
                id='2013preseason2'.format(event_id_counter),
                event_short='preseason2',
                start_date=datetime.datetime(2013, 1, 18),
                end_date=datetime.datetime(2013, 1, 20),
                official=False,
                event_type_enum=EventType.PRESEASON
            ),
            Event(
                id='2013preseason3'.format(event_id_counter),
                event_short='preseason3',
                start_date=datetime.datetime(2013, 7, 11),
                end_date=datetime.datetime(2013, 7, 12),
                official=False,
                event_type_enum=EventType.PRESEASON
            )
        ]

        # Generate offseason events. Offseason events are any event that doesn't fall under one of the above categories.
        events_by_week[OFFSEASON_EVENTS_LABEL] = [
            Event(
                id='2013offseason1'.format(event_id_counter),
                event_short='offseason1',
                official=False,
                event_type_enum=EventType.OFFSEASON
            ),
            Event(
                id='2013offseason2'.format(event_id_counter),
                event_short='offseason2',
                start_date=datetime.datetime(2013, 8, 18),
                end_date=datetime.datetime(2013, 8, 20),
                official=False,
                event_type_enum=EventType.OFFSEASON
            ),
            Event(
                id='2013offseason3'.format(event_id_counter),
                event_short='offseason3',
                start_date=datetime.datetime(2013, 12, 30),
                end_date=datetime.datetime(2013, 12, 31),
                official=False,
                event_type_enum=EventType.OFFSEASON
            ),
            Event(
                id='2013offseason4'.format(event_id_counter),
                event_short='offseason4',
                start_date=datetime.datetime(2013, 11, 13),
                end_date=datetime.datetime(2013, 11, 14),
                official=False,
                event_type_enum=EventType.REGIONAL
            )
        ]

        # Combine all events and shufle randomly
        events = []
        for week_events in events_by_week.values():
            events.extend(week_events)
        state.shuffle(events)

        # Begin testing
        events.sort(key=EventHelper.distantFutureIfNoStartDate)
        week_events = EventHelper.groupByWeek(events)

        for key in events_by_week.keys():
            try:
                self.assertEqual(set([e.key.id() for e in events_by_week[key]]),
                                 set([e.key.id() for e in week_events[key]]))
            except AssertionError, e:
                logging.warning("\n\nseed: {}".format(seed))
                logging.warning("\n\nkey: {}".format(key))
                logging.warning("\n\nevents_by_week: {}".format(events_by_week[key]))
                logging.warning("\n\nweek_events: {}".format(week_events[key]))
                raise e

########NEW FILE########
__FILENAME__ = test_event_manipulator
import datetime
import json
import os
import unittest2

from google.appengine.ext import ndb
from google.appengine.ext import testbed

from consts.district_type import DistrictType
from consts.event_type import EventType
from datafeeds.usfirst_event_rankings_parser import UsfirstEventRankingsParser
from helpers.event_manipulator import EventManipulator
from models.event import Event


class TestEventManipulator(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_memcache_stub()
        self.maxDiff = None

        with open('test_data/usfirst_html/usfirst_event_rankings_2012ct.html', 'r') as f:
            good_rankings, _ = UsfirstEventRankingsParser.parse(f.read())

        with open('test_data/usfirst_html/usfirst_event_rankings_2012ct_bad.html', 'r') as f:
            bad_rankings, _ = UsfirstEventRankingsParser.parse(f.read())

        self.old_alliance_selections = {
            '1': {'picks': ['frc254', 'frc469', 'frc2848', 'frc74'], 'declines':[] },
            '2': {'picks': ['frc1718', 'frc2451', 'frc573', 'frc2016'], 'declines':[] },
            '3': {'picks': ['frc2928', 'frc2013', 'frc1311', 'frc842'], 'declines':[] },
            '4': {'picks': ['frc180', 'frc125', 'frc1323', 'frc2468'], 'declines':[] },
            '5': {'picks': ['frc118', 'frc359', 'frc4334', 'frc865'], 'declines':[] },
            '6': {'picks': ['frc135', 'frc1241', 'frc11', 'frc68'], 'declines':[] },
            '7': {'picks': ['frc3478', 'frc177', 'frc294', 'frc230'], 'declines':[] },
            '8': {'picks': ['frc624', 'frc987', 'frc3476', 'frc123'], 'declines':[] },
        }

        self.new_alliance_selections = {
            '1': {'picks': ['frc254', 'frc469', 'frc2848', 'frc74'], 'declines':[] },
            '2': {'picks': ['frc1718', 'frc2451', 'frc573', 'frc2016'], 'declines':[] },
            '3': {'picks': ['frc2928', 'frc2013', 'frc1311', 'frc842'], 'declines':[] },
            '4': {'picks': ['frc180', 'frc125', 'frc1323', 'frc2468'], 'declines':[] },
            '5': {'picks': ['frc118', 'frc359', 'frc4334', 'frc865'], 'declines':[] },
            '6': {'picks': ['frc135', 'frc1241', 'frc11', 'frc68'], 'declines':[] },
            '7': {'picks': ['frc3478', 'frc177', 'frc294', 'frc230'], 'declines':[] },
            '8': {'picks': ['frc624', 'frc987', 'frc3476', 'frc3015'], 'declines':[] },
        }

        self.old_event = Event(
            id="2011ct",
            end_date=datetime.datetime(2011, 4, 2, 0, 0),
            event_short="ct",
            event_type_enum=EventType.REGIONAL,
            event_district_enum=DistrictType.NO_DISTRICT,
            first_eid="5561",
            name="Northeast Utilities FIRST Connecticut Regional",
            start_date=datetime.datetime(2011, 3, 31, 0, 0),
            year=2011,
            venue_address="Connecticut Convention Center\r\n100 Columbus Blvd\r\nHartford, CT 06103\r\nUSA",
            website="http://www.ctfirst.org/ctr",
            rankings_json=json.dumps(good_rankings),
            alliance_selections_json=json.dumps(self.old_alliance_selections),
        )

        self.new_event = Event(
            id="2011ct",
            end_date=datetime.datetime(2011, 4, 2, 0, 0),
            event_short="ct",
            event_type_enum=EventType.REGIONAL,
            event_district_enum=DistrictType.NO_DISTRICT,
            first_eid="5561",
            name="Northeast Utilities FIRST Connecticut Regional",
            start_date=datetime.datetime(2011, 3, 31, 0, 0),
            year=2011,
            venue_address="Connecticut Convention Center\r\n100 Columbus Blvd\r\nHartford, CT 06103\r\nUSA",
            website="http://www.ctfirst.org/ctr",

            matchstats_json=json.dumps({'oprs': {'4255': 7.4877151786460301, '2643': 27.285682906835952, '852': 10.452538750544525, '4159': 25.820137009871139, '581': 18.513816255143144}}),
            facebook_eid="7",
            webcast_json=json.dumps([{'type': 'ustream', 'channel': 'foo'}]),
            rankings_json=json.dumps(bad_rankings),
            alliance_selections_json=json.dumps(self.new_alliance_selections),
        )

    def tearDown(self):
        self.testbed.deactivate()

    def assertMergedEvent(self, event):
        self.assertOldEvent(event)
        self.assertEqual(event.matchstats, {'oprs': {'4255': 7.4877151786460301, '2643': 27.285682906835952, '852': 10.452538750544525, '4159': 25.820137009871139, '581': 18.513816255143144}})
        self.assertEqual(event.facebook_eid, "7")
        self.assertEqual(event.webcast[0]['type'], 'ustream')
        self.assertEqual(event.webcast[0]['channel'], 'foo')
        self.assertEqual(event.rankings, [['Rank', 'Team', 'QS', 'HP', 'BP', 'TP', 'CP', 'Record (W-L-T)', 'DQ', 'Played'], ['1', '2168', '32.00', '147.00', '60.00', '208.00', '14', '9-1-0', '0', '10'], ['2', '118', '31.00', '168.00', '90.00', '231.00', '17', '7-3-0', '0', '10'], ['3', '177', '30.00', '177.00', '120.00', '151.00', '14', '8-2-0', '0', '10'], ['4', '195', '29.00', '116.00', '70.00', '190.00', '16', '6-3-1', '0', '10'], ['5', '237', '28.00', '120.00', '60.00', '123.00', '14', '7-3-0', '0', '10'], ['6', '1071', '28.00', '115.00', '120.00', '142.00', '10', '9-1-0', '0', '10'], ['7', '173', '28.00', '114.00', '110.00', '108.00', '14', '7-3-0', '0', '10'], ['8', '1073', '28.00', '110.00', '100.00', '152.00', '11', '8-1-1', '0', '10'], ['9', '694', '28.00', '78.00', '100.00', '140.00', '14', '7-3-0', '0', '10'], ['10', '558', '27.00', '152.00', '100.00', '145.00', '13', '7-3-0', '0', '10'], ['11', '175', '27.00', '141.00', '160.00', '117.00', '13', '7-3-0', '0', '10'], ['12', '181', '26.00', '151.00', '70.00', '95.00', '14', '6-4-0', '0', '10'], ['13', '176', '26.00', '120.00', '60.00', '90.00', '18', '4-6-0', '0', '10'], ['14', '1511', '26.00', '111.00', '80.00', '164.00', '14', '6-4-0', '0', '10'], ['15', '126', '26.00', '108.00', '70.00', '165.00', '14', '6-4-0', '0', '10'], ['16', '4122', '26.00', '92.00', '100.00', '78.00', '14', '6-4-0', '0', '10'], ['17', '869', '25.00', '68.00', '130.00', '75.00', '12', '6-3-1', '0', '10'], ['18', '3464', '24.00', '135.00', '80.00', '109.00', '14', '5-5-0', '0', '10'], ['19', '3467', '24.00', '101.00', '80.00', '123.00', '10', '7-3-0', '0', '10'], ['20', '3718', '24.00', '100.00', '60.00', '106.00', '12', '6-4-0', '0', '10'], ['21', '3461', '24.00', '79.00', '30.00', '94.00', '14', '5-5-0', '0', '10'], ['22', '4055', '24.00', '78.00', '80.00', '79.00', '16', '4-6-0', '0', '10'], ['23', '1922', '23.00', '114.00', '110.00', '151.00', '10', '6-3-1', '0', '10'], ['24', '95', '22.00', '120.00', '70.00', '123.00', '14', '4-6-0', '0', '10'], ['25', '1991', '22.00', '113.00', '100.00', '58.00', '12', '5-5-0', '0', '10'], ['26', '839', '22.00', '96.00', '110.00', '136.00', '10', '6-4-0', '0', '10'], ['27', '1099', '21.00', '126.00', '110.00', '97.00', '8', '6-3-1', '0', '10'], ['28', '230', '20.00', '143.00', '80.00', '104.00', '8', '6-4-0', '0', '10'], ['29', '3017', '20.00', '134.00', '50.00', '88.00', '12', '4-6-0', '0', '10'], ['30', '2067', '20.00', '128.00', '80.00', '122.00', '10', '5-5-0', '0', '10'], ['31', '250', '20.00', '118.00', '40.00', '99.00', '10', '5-5-0', '0', '10'], ['32', '155', '20.00', '100.00', '50.00', '74.00', '12', '4-6-0', '0', '10'], ['33', '236', '20.00', '99.00', '20.00', '126.00', '10', '5-5-0', '0', '10'], ['34', '1124', '20.00', '92.00', '80.00', '109.00', '8', '6-4-0', '0', '10'], ['35', '3146', '20.00', '81.00', '110.00', '81.00', '6', '7-3-0', '0', '10'], ['36', '663', '20.00', '71.00', '90.00', '90.00', '12', '4-6-0', '0', '10'], ['37', '1699', '20.00', '70.00', '80.00', '139.00', '12', '4-6-0', '0', '10'], ['38', '1027', '20.00', '53.00', '70.00', '97.00', '12', '4-6-0', '0', '10'], ['39', '20', '19.00', '79.00', '70.00', '106.00', '9', '5-5-0', '0', '10'], ['40', '3182', '18.00', '108.00', '60.00', '147.00', '8', '5-5-0', '0', '10'], ['41', '229', '18.00', '97.00', '40.00', '153.00', '10', '4-6-0', '0', '10'], ['42', '1665', '18.00', '95.00', '120.00', '106.00', '10', '4-6-0', '0', '10'], ['43', '228', '18.00', '81.00', '60.00', '163.00', '10', '4-6-0', '0', '10'], ['44', '178', '18.00', '81.00', '50.00', '58.00', '12', '3-7-0', '0', '10'], ['45', '1740', '18.00', '62.00', '20.00', '99.00', '8', '5-5-0', '0', '10'], ['46', '3634', '18.00', '54.00', '30.00', '105.00', '10', '4-6-0', '0', '10'], ['47', '2791', '18.00', '53.00', '100.00', '108.00', '10', '4-6-0', '0', '10'], ['48', '571', '18.00', '53.00', '70.00', '109.00', '10', '4-6-0', '0', '10'], ['49', '2170', '17.00', '89.00', '60.00', '103.00', '9', '4-5-0', '1', '10'], ['50', '1493', '16.00', '150.00', '60.00', '132.00', '6', '5-5-0', '0', '10'], ['51', '549', '16.00', '129.00', '100.00', '91.00', '6', '5-5-0', '0', '10'], ['52', '743', '16.00', '70.00', '30.00', '67.00', '10', '3-7-0', '0', '10'], ['53', '2836', '16.00', '64.00', '80.00', '126.00', '8', '4-6-0', '0', '10'], ['54', '999', '14.00', '114.00', '20.00', '79.00', '10', '2-8-0', '0', '10'], ['55', '3525', '14.00', '109.00', '40.00', '66.00', '6', '4-6-0', '0', '10'], ['56', '3104', '14.00', '92.00', '20.00', '80.00', '6', '4-6-0', '0', '10'], ['57', '3555', '14.00', '68.00', '60.00', '68.00', '8', '3-7-0', '0', '10'], ['58', '4134', '13.00', '96.00', '30.00', '80.00', '6', '3-6-1', '0', '10'], ['59', '1559', '12.00', '110.00', '10.00', '94.00', '8', '2-8-0', '0', '10'], ['60', '3719', '12.00', '97.00', '60.00', '95.00', '6', '3-7-0', '0', '10'], ['61', '3654', '12.00', '59.00', '20.00', '57.00', '8', '2-8-0', '0', '10'], ['62', '2785', '12.00', '41.00', '70.00', '96.00', '8', '2-8-0', '0', '10'], ['63', '1880', '10.00', '57.00', '40.00', '86.00', '6', '2-8-0', '0', '10'], ['64', '1784', '10.00', '44.00', '40.00', '60.00', '6', '2-7-0', '1', '10']])
        self.assertEqual(event.alliance_selections, self.new_alliance_selections)

    def assertOldEvent(self, event):
        self.assertEqual(event.key.id(), "2011ct")
        self.assertEqual(event.name, "Northeast Utilities FIRST Connecticut Regional")
        self.assertEqual(event.event_type_enum, EventType.REGIONAL)
        self.assertEqual(event.event_district_enum, DistrictType.NO_DISTRICT)
        self.assertEqual(event.start_date, datetime.datetime(2011, 3, 31, 0, 0))
        self.assertEqual(event.end_date, datetime.datetime(2011, 4, 2, 0, 0))
        self.assertEqual(event.year, 2011)
        self.assertEqual(event.venue_address, "Connecticut Convention Center\r\n100 Columbus Blvd\r\nHartford, CT 06103\r\nUSA")
        self.assertEqual(event.website, "http://www.ctfirst.org/ctr")
        self.assertEqual(event.event_short, "ct")

    def test_createOrUpdate(self):
        EventManipulator.createOrUpdate(self.old_event)
        self.assertOldEvent(Event.get_by_id("2011ct"))
        EventManipulator.createOrUpdate(self.new_event)
        self.assertMergedEvent(Event.get_by_id("2011ct"))

    def test_findOrSpawn(self):
        self.old_event.put()
        self.assertMergedEvent(EventManipulator.findOrSpawn(self.new_event))

    def test_updateMerge(self):
        self.assertMergedEvent(EventManipulator.updateMerge(self.new_event, self.old_event))

########NEW FILE########
__FILENAME__ = test_event_team_repairer
import datetime
import unittest2

from consts.event_type import EventType

from google.appengine.ext import ndb
from google.appengine.ext import testbed

from helpers.event_team_manipulator import EventTeamManipulator
from helpers.event_team_repairer import EventTeamRepairer

from models.event import Event
from models.event_team import EventTeam
from models.team import Team


class TestEventTeamRepairer(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_memcache_stub()

        event = Event(
            id="2011ct",
            end_date=datetime.datetime(2011, 4, 2, 0, 0),
            event_short="ct",
            event_type_enum=EventType.REGIONAL,
            first_eid="5561",
            name="Northeast Utilities FIRST Connecticut Regional",
            start_date=datetime.datetime(2011, 3, 31, 0, 0),
            year=2011,
            venue_address="Connecticut Convention Center\r\n100 Columbus Blvd\r\nHartford, CT 06103\r\nUSA",
            website="http://www.ctfirst.org/ctr"
        )
        event.put()

        team = Team(
            id="frc177",
            team_number=177,
            website="http://www.bobcatrobotics.org"
        )
        team.put()

        event_team = EventTeam(
            id="%s_%s" % (event.key.id(), team.key.id()),
            event=event.key,
            team=team.key,
            year=None)
        event_team.put()

    def tearDown(self):
        self.testbed.deactivate()

    def test_repair(self):
        event_team = EventTeam.get_by_id("2011ct_frc177")
        self.assertEqual(event_team.year, None)

        broken_event_teams = EventTeam.query(EventTeam.year == None).fetch()
        self.assertGreater(len(broken_event_teams), 0)

        fixed_event_teams = EventTeamRepairer.repair(broken_event_teams)
        fixed_event_teams = EventTeamManipulator.createOrUpdate(fixed_event_teams)

        event_team = EventTeam.get_by_id("2011ct_frc177")
        self.assertEqual(event_team.year, 2011)

########NEW FILE########
__FILENAME__ = test_event_team_updater
import datetime
import os
import unittest2

from google.appengine.ext import ndb
from google.appengine.ext import testbed

from consts.event_type import EventType
from datafeeds.usfirst_matches_parser import UsfirstMatchesParser
from helpers.event_team_updater import EventTeamUpdater
from models.event import Event
from models.event_team import EventTeam
from models.match import Match
from models.team import Team


CUR_YEAR = datetime.datetime.now().year


def set_up_matches(html, event):
    with open(html, 'r') as f:
        parsed_matches, _ = UsfirstMatchesParser.parse(f.read())
        matches = [Match(
            id=Match.renderKeyName(
                event.key.id(),
                match.get("comp_level", None),
                match.get("set_number", 0),
                match.get("match_number", 0)),
            event=event.key,
            game=Match.FRC_GAMES_BY_YEAR.get(event.year, "frc_unknown"),
            set_number=match.get("set_number", 0),
            match_number=match.get("match_number", 0),
            comp_level=match.get("comp_level", None),
            team_key_names=match.get("team_key_names", None),
            alliances_json=match.get("alliances_json", None)
            )
            for match in parsed_matches]
        return matches


class TestEventTeamUpdater(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_memcache_stub()

        past_event = Event(
            id="{}tstupdaterpast".format(CUR_YEAR),
            end_date=datetime.datetime.now() - datetime.timedelta(days=1),
            event_short="tstupdaterpast",
            event_type_enum=EventType.REGIONAL,
            start_date=datetime.datetime.now() - datetime.timedelta(days=2),
            year=CUR_YEAR,
        )
        past_event.put()
        past_event_matches = set_up_matches(
            'test_data/usfirst_html/usfirst_event_matches_2012ct.html',
            past_event)
        ndb.put_multi(past_event_matches)

        future_event = Event(
            id="{}tstupdaterfuture".format(CUR_YEAR),
            end_date=datetime.datetime.now() + datetime.timedelta(days=2),
            event_short="tstupdaterfuture",
            event_type_enum=EventType.REGIONAL,
            start_date=datetime.datetime.now() + datetime.timedelta(days=1),
            year=CUR_YEAR,
        )
        future_event.put()
        future_event_matches = set_up_matches(
            'test_data/usfirst_html/usfirst_event_matches_2012ct.html',
            future_event)
        ndb.put_multi(future_event_matches)

        past_year_event = Event(
            id="{}tstupdaterpastyear".format(CUR_YEAR - 1),
            end_date=datetime.datetime.now() - datetime.timedelta(days=1),
            event_short="tstupdaterpastyear",
            event_type_enum=EventType.REGIONAL,
            start_date=datetime.datetime.now() - datetime.timedelta(days=2),
            year=CUR_YEAR - 1,
        )
        past_year_event.put()
        past_year_event_matches = set_up_matches(
            'test_data/usfirst_html/usfirst_event_matches_2012ct.html',
            past_year_event)
        ndb.put_multi(past_year_event_matches)

    def tearDown(self):
        self.testbed.deactivate()

    def test_update_future(self):
        teams, event_teams, et_keys_to_delete = EventTeamUpdater.update('{}tstupdaterfuture'.format(CUR_YEAR))
        self.assertEqual(set([team.team_number for team in teams]).symmetric_difference({95, 178, 176, 1922, 173, 2785, 228, 177, 1099, 175, 1027, 3017, 1493, 118, 229, 2791, 155, 549, 195, 4134, 20, 2836, 869, 1665, 4055, 3555, 126, 1699, 1559, 3464, 2168, 3461, 1991, 3467, 2067, 230, 1124, 3104, 236, 237, 1511, 250, 1880, 558, 694, 571, 3634, 3525, 999, 181, 1073, 3146, 1071, 1740, 3719, 3718, 2170, 663, 4122, 3182, 839, 1784, 3654, 743}),
                         set())
        self.assertEqual(set([et.key_name for et in event_teams]).symmetric_difference({'{}tstupdaterfuture_frc95'.format(CUR_YEAR), '{}tstupdaterfuture_frc178'.format(CUR_YEAR), '{}tstupdaterfuture_frc176'.format(CUR_YEAR), '{}tstupdaterfuture_frc1922'.format(CUR_YEAR), '{}tstupdaterfuture_frc173'.format(CUR_YEAR), '{}tstupdaterfuture_frc2785'.format(CUR_YEAR), '{}tstupdaterfuture_frc228'.format(CUR_YEAR), '{}tstupdaterfuture_frc177'.format(CUR_YEAR), '{}tstupdaterfuture_frc1099'.format(CUR_YEAR), '{}tstupdaterfuture_frc175'.format(CUR_YEAR), '{}tstupdaterfuture_frc1027'.format(CUR_YEAR), '{}tstupdaterfuture_frc3017'.format(CUR_YEAR), '{}tstupdaterfuture_frc1493'.format(CUR_YEAR), '{}tstupdaterfuture_frc118'.format(CUR_YEAR), '{}tstupdaterfuture_frc229'.format(CUR_YEAR), '{}tstupdaterfuture_frc2791'.format(CUR_YEAR), '{}tstupdaterfuture_frc155'.format(CUR_YEAR), '{}tstupdaterfuture_frc549'.format(CUR_YEAR), '{}tstupdaterfuture_frc195'.format(CUR_YEAR), '{}tstupdaterfuture_frc4134'.format(CUR_YEAR), '{}tstupdaterfuture_frc20'.format(CUR_YEAR), '{}tstupdaterfuture_frc2836'.format(CUR_YEAR), '{}tstupdaterfuture_frc869'.format(CUR_YEAR), '{}tstupdaterfuture_frc1665'.format(CUR_YEAR), '{}tstupdaterfuture_frc4055'.format(CUR_YEAR), '{}tstupdaterfuture_frc3555'.format(CUR_YEAR), '{}tstupdaterfuture_frc126'.format(CUR_YEAR), '{}tstupdaterfuture_frc1699'.format(CUR_YEAR), '{}tstupdaterfuture_frc1559'.format(CUR_YEAR), '{}tstupdaterfuture_frc3464'.format(CUR_YEAR), '{}tstupdaterfuture_frc2168'.format(CUR_YEAR), '{}tstupdaterfuture_frc3461'.format(CUR_YEAR), '{}tstupdaterfuture_frc1991'.format(CUR_YEAR), '{}tstupdaterfuture_frc3467'.format(CUR_YEAR), '{}tstupdaterfuture_frc2067'.format(CUR_YEAR), '{}tstupdaterfuture_frc230'.format(CUR_YEAR), '{}tstupdaterfuture_frc1124'.format(CUR_YEAR), '{}tstupdaterfuture_frc3104'.format(CUR_YEAR), '{}tstupdaterfuture_frc236'.format(CUR_YEAR), '{}tstupdaterfuture_frc237'.format(CUR_YEAR), '{}tstupdaterfuture_frc1511'.format(CUR_YEAR), '{}tstupdaterfuture_frc250'.format(CUR_YEAR), '{}tstupdaterfuture_frc1880'.format(CUR_YEAR), '{}tstupdaterfuture_frc558'.format(CUR_YEAR), '{}tstupdaterfuture_frc694'.format(CUR_YEAR), '{}tstupdaterfuture_frc571'.format(CUR_YEAR), '{}tstupdaterfuture_frc3634'.format(CUR_YEAR), '{}tstupdaterfuture_frc3525'.format(CUR_YEAR), '{}tstupdaterfuture_frc999'.format(CUR_YEAR), '{}tstupdaterfuture_frc181'.format(CUR_YEAR), '{}tstupdaterfuture_frc1073'.format(CUR_YEAR), '{}tstupdaterfuture_frc3146'.format(CUR_YEAR), '{}tstupdaterfuture_frc1071'.format(CUR_YEAR), '{}tstupdaterfuture_frc1740'.format(CUR_YEAR), '{}tstupdaterfuture_frc3719'.format(CUR_YEAR), '{}tstupdaterfuture_frc3718'.format(CUR_YEAR), '{}tstupdaterfuture_frc2170'.format(CUR_YEAR), '{}tstupdaterfuture_frc663'.format(CUR_YEAR), '{}tstupdaterfuture_frc4122'.format(CUR_YEAR), '{}tstupdaterfuture_frc3182'.format(CUR_YEAR), '{}tstupdaterfuture_frc839'.format(CUR_YEAR), '{}tstupdaterfuture_frc1784'.format(CUR_YEAR), '{}tstupdaterfuture_frc3654'.format(CUR_YEAR), '{}tstupdaterfuture_frc743'.format(CUR_YEAR)}),
                         set())
        self.assertEqual(et_keys_to_delete, set())

        event_team = EventTeam(
            id="%s_%s" % ('{}tstupdaterfuture'.format(CUR_YEAR), 'frc9999'),
            event=ndb.Key(Event, '{}tstupdaterfuture'.format(CUR_YEAR)),
            team=ndb.Key(Team, 'frc9999'),
            year=CUR_YEAR)
        event_team.put()

        teams, event_teams, et_keys_to_delete = EventTeamUpdater.update('{}tstupdaterfuture'.format(CUR_YEAR))
        self.assertEqual(set([team.team_number for team in teams]).symmetric_difference({95, 178, 176, 1922, 173, 2785, 228, 177, 1099, 175, 1027, 3017, 1493, 118, 229, 2791, 155, 549, 195, 4134, 20, 2836, 869, 1665, 4055, 3555, 126, 1699, 1559, 3464, 2168, 3461, 1991, 3467, 2067, 230, 1124, 3104, 236, 237, 1511, 250, 1880, 558, 694, 571, 3634, 3525, 999, 181, 1073, 3146, 1071, 1740, 3719, 3718, 2170, 663, 4122, 3182, 839, 1784, 3654, 743}),
                         set())
        self.assertEqual(set([et.key_name for et in event_teams]).symmetric_difference({'{}tstupdaterfuture_frc95'.format(CUR_YEAR), '{}tstupdaterfuture_frc178'.format(CUR_YEAR), '{}tstupdaterfuture_frc176'.format(CUR_YEAR), '{}tstupdaterfuture_frc1922'.format(CUR_YEAR), '{}tstupdaterfuture_frc173'.format(CUR_YEAR), '{}tstupdaterfuture_frc2785'.format(CUR_YEAR), '{}tstupdaterfuture_frc228'.format(CUR_YEAR), '{}tstupdaterfuture_frc177'.format(CUR_YEAR), '{}tstupdaterfuture_frc1099'.format(CUR_YEAR), '{}tstupdaterfuture_frc175'.format(CUR_YEAR), '{}tstupdaterfuture_frc1027'.format(CUR_YEAR), '{}tstupdaterfuture_frc3017'.format(CUR_YEAR), '{}tstupdaterfuture_frc1493'.format(CUR_YEAR), '{}tstupdaterfuture_frc118'.format(CUR_YEAR), '{}tstupdaterfuture_frc229'.format(CUR_YEAR), '{}tstupdaterfuture_frc2791'.format(CUR_YEAR), '{}tstupdaterfuture_frc155'.format(CUR_YEAR), '{}tstupdaterfuture_frc549'.format(CUR_YEAR), '{}tstupdaterfuture_frc195'.format(CUR_YEAR), '{}tstupdaterfuture_frc4134'.format(CUR_YEAR), '{}tstupdaterfuture_frc20'.format(CUR_YEAR), '{}tstupdaterfuture_frc2836'.format(CUR_YEAR), '{}tstupdaterfuture_frc869'.format(CUR_YEAR), '{}tstupdaterfuture_frc1665'.format(CUR_YEAR), '{}tstupdaterfuture_frc4055'.format(CUR_YEAR), '{}tstupdaterfuture_frc3555'.format(CUR_YEAR), '{}tstupdaterfuture_frc126'.format(CUR_YEAR), '{}tstupdaterfuture_frc1699'.format(CUR_YEAR), '{}tstupdaterfuture_frc1559'.format(CUR_YEAR), '{}tstupdaterfuture_frc3464'.format(CUR_YEAR), '{}tstupdaterfuture_frc2168'.format(CUR_YEAR), '{}tstupdaterfuture_frc3461'.format(CUR_YEAR), '{}tstupdaterfuture_frc1991'.format(CUR_YEAR), '{}tstupdaterfuture_frc3467'.format(CUR_YEAR), '{}tstupdaterfuture_frc2067'.format(CUR_YEAR), '{}tstupdaterfuture_frc230'.format(CUR_YEAR), '{}tstupdaterfuture_frc1124'.format(CUR_YEAR), '{}tstupdaterfuture_frc3104'.format(CUR_YEAR), '{}tstupdaterfuture_frc236'.format(CUR_YEAR), '{}tstupdaterfuture_frc237'.format(CUR_YEAR), '{}tstupdaterfuture_frc1511'.format(CUR_YEAR), '{}tstupdaterfuture_frc250'.format(CUR_YEAR), '{}tstupdaterfuture_frc1880'.format(CUR_YEAR), '{}tstupdaterfuture_frc558'.format(CUR_YEAR), '{}tstupdaterfuture_frc694'.format(CUR_YEAR), '{}tstupdaterfuture_frc571'.format(CUR_YEAR), '{}tstupdaterfuture_frc3634'.format(CUR_YEAR), '{}tstupdaterfuture_frc3525'.format(CUR_YEAR), '{}tstupdaterfuture_frc999'.format(CUR_YEAR), '{}tstupdaterfuture_frc181'.format(CUR_YEAR), '{}tstupdaterfuture_frc1073'.format(CUR_YEAR), '{}tstupdaterfuture_frc3146'.format(CUR_YEAR), '{}tstupdaterfuture_frc1071'.format(CUR_YEAR), '{}tstupdaterfuture_frc1740'.format(CUR_YEAR), '{}tstupdaterfuture_frc3719'.format(CUR_YEAR), '{}tstupdaterfuture_frc3718'.format(CUR_YEAR), '{}tstupdaterfuture_frc2170'.format(CUR_YEAR), '{}tstupdaterfuture_frc663'.format(CUR_YEAR), '{}tstupdaterfuture_frc4122'.format(CUR_YEAR), '{}tstupdaterfuture_frc3182'.format(CUR_YEAR), '{}tstupdaterfuture_frc839'.format(CUR_YEAR), '{}tstupdaterfuture_frc1784'.format(CUR_YEAR), '{}tstupdaterfuture_frc3654'.format(CUR_YEAR), '{}tstupdaterfuture_frc743'.format(CUR_YEAR)}),
                         set())
        self.assertEqual(set([et_key.id() for et_key in et_keys_to_delete]),
                         set())

    def test_update_past(self):
        teams, event_teams, et_keys_to_delete = EventTeamUpdater.update('{}tstupdaterpast'.format(CUR_YEAR))
        self.assertEqual(set([team.team_number for team in teams]).symmetric_difference({95, 178, 176, 1922, 173, 2785, 228, 177, 1099, 175, 1027, 3017, 1493, 118, 229, 2791, 155, 549, 195, 4134, 20, 2836, 869, 1665, 4055, 3555, 126, 1699, 1559, 3464, 2168, 3461, 1991, 3467, 2067, 230, 1124, 3104, 236, 237, 1511, 250, 1880, 558, 694, 571, 3634, 3525, 999, 181, 1073, 3146, 1071, 1740, 3719, 3718, 2170, 663, 4122, 3182, 839, 1784, 3654, 743}),
                         set())
        self.assertEqual(set([et.key_name for et in event_teams]).symmetric_difference({'{}tstupdaterpast_frc95'.format(CUR_YEAR), '{}tstupdaterpast_frc178'.format(CUR_YEAR), '{}tstupdaterpast_frc176'.format(CUR_YEAR), '{}tstupdaterpast_frc1922'.format(CUR_YEAR), '{}tstupdaterpast_frc173'.format(CUR_YEAR), '{}tstupdaterpast_frc2785'.format(CUR_YEAR), '{}tstupdaterpast_frc228'.format(CUR_YEAR), '{}tstupdaterpast_frc177'.format(CUR_YEAR), '{}tstupdaterpast_frc1099'.format(CUR_YEAR), '{}tstupdaterpast_frc175'.format(CUR_YEAR), '{}tstupdaterpast_frc1027'.format(CUR_YEAR), '{}tstupdaterpast_frc3017'.format(CUR_YEAR), '{}tstupdaterpast_frc1493'.format(CUR_YEAR), '{}tstupdaterpast_frc118'.format(CUR_YEAR), '{}tstupdaterpast_frc229'.format(CUR_YEAR), '{}tstupdaterpast_frc2791'.format(CUR_YEAR), '{}tstupdaterpast_frc155'.format(CUR_YEAR), '{}tstupdaterpast_frc549'.format(CUR_YEAR), '{}tstupdaterpast_frc195'.format(CUR_YEAR), '{}tstupdaterpast_frc4134'.format(CUR_YEAR), '{}tstupdaterpast_frc20'.format(CUR_YEAR), '{}tstupdaterpast_frc2836'.format(CUR_YEAR), '{}tstupdaterpast_frc869'.format(CUR_YEAR), '{}tstupdaterpast_frc1665'.format(CUR_YEAR), '{}tstupdaterpast_frc4055'.format(CUR_YEAR), '{}tstupdaterpast_frc3555'.format(CUR_YEAR), '{}tstupdaterpast_frc126'.format(CUR_YEAR), '{}tstupdaterpast_frc1699'.format(CUR_YEAR), '{}tstupdaterpast_frc1559'.format(CUR_YEAR), '{}tstupdaterpast_frc3464'.format(CUR_YEAR), '{}tstupdaterpast_frc2168'.format(CUR_YEAR), '{}tstupdaterpast_frc3461'.format(CUR_YEAR), '{}tstupdaterpast_frc1991'.format(CUR_YEAR), '{}tstupdaterpast_frc3467'.format(CUR_YEAR), '{}tstupdaterpast_frc2067'.format(CUR_YEAR), '{}tstupdaterpast_frc230'.format(CUR_YEAR), '{}tstupdaterpast_frc1124'.format(CUR_YEAR), '{}tstupdaterpast_frc3104'.format(CUR_YEAR), '{}tstupdaterpast_frc236'.format(CUR_YEAR), '{}tstupdaterpast_frc237'.format(CUR_YEAR), '{}tstupdaterpast_frc1511'.format(CUR_YEAR), '{}tstupdaterpast_frc250'.format(CUR_YEAR), '{}tstupdaterpast_frc1880'.format(CUR_YEAR), '{}tstupdaterpast_frc558'.format(CUR_YEAR), '{}tstupdaterpast_frc694'.format(CUR_YEAR), '{}tstupdaterpast_frc571'.format(CUR_YEAR), '{}tstupdaterpast_frc3634'.format(CUR_YEAR), '{}tstupdaterpast_frc3525'.format(CUR_YEAR), '{}tstupdaterpast_frc999'.format(CUR_YEAR), '{}tstupdaterpast_frc181'.format(CUR_YEAR), '{}tstupdaterpast_frc1073'.format(CUR_YEAR), '{}tstupdaterpast_frc3146'.format(CUR_YEAR), '{}tstupdaterpast_frc1071'.format(CUR_YEAR), '{}tstupdaterpast_frc1740'.format(CUR_YEAR), '{}tstupdaterpast_frc3719'.format(CUR_YEAR), '{}tstupdaterpast_frc3718'.format(CUR_YEAR), '{}tstupdaterpast_frc2170'.format(CUR_YEAR), '{}tstupdaterpast_frc663'.format(CUR_YEAR), '{}tstupdaterpast_frc4122'.format(CUR_YEAR), '{}tstupdaterpast_frc3182'.format(CUR_YEAR), '{}tstupdaterpast_frc839'.format(CUR_YEAR), '{}tstupdaterpast_frc1784'.format(CUR_YEAR), '{}tstupdaterpast_frc3654'.format(CUR_YEAR), '{}tstupdaterpast_frc743'.format(CUR_YEAR)}),
                         set())
        self.assertEqual(et_keys_to_delete, set())

        event_team = EventTeam(
            id="%s_%s" % ('{}tstupdaterpast'.format(CUR_YEAR), 'frc9999'),
            event=ndb.Key(Event, '{}tstupdaterpast'.format(CUR_YEAR)),
            team=ndb.Key(Team, 'frc9999'),
            year=CUR_YEAR)
        event_team.put()

        teams, event_teams, et_keys_to_delete = EventTeamUpdater.update('{}tstupdaterpast'.format(CUR_YEAR))
        self.assertEqual(set([team.team_number for team in teams]).symmetric_difference({95, 178, 176, 1922, 173, 2785, 228, 177, 1099, 175, 1027, 3017, 1493, 118, 229, 2791, 155, 549, 195, 4134, 20, 2836, 869, 1665, 4055, 3555, 126, 1699, 1559, 3464, 2168, 3461, 1991, 3467, 2067, 230, 1124, 3104, 236, 237, 1511, 250, 1880, 558, 694, 571, 3634, 3525, 999, 181, 1073, 3146, 1071, 1740, 3719, 3718, 2170, 663, 4122, 3182, 839, 1784, 3654, 743}),
                         set())
        self.assertEqual(set([et.key_name for et in event_teams]).symmetric_difference({'{}tstupdaterpast_frc95'.format(CUR_YEAR), '{}tstupdaterpast_frc178'.format(CUR_YEAR), '{}tstupdaterpast_frc176'.format(CUR_YEAR), '{}tstupdaterpast_frc1922'.format(CUR_YEAR), '{}tstupdaterpast_frc173'.format(CUR_YEAR), '{}tstupdaterpast_frc2785'.format(CUR_YEAR), '{}tstupdaterpast_frc228'.format(CUR_YEAR), '{}tstupdaterpast_frc177'.format(CUR_YEAR), '{}tstupdaterpast_frc1099'.format(CUR_YEAR), '{}tstupdaterpast_frc175'.format(CUR_YEAR), '{}tstupdaterpast_frc1027'.format(CUR_YEAR), '{}tstupdaterpast_frc3017'.format(CUR_YEAR), '{}tstupdaterpast_frc1493'.format(CUR_YEAR), '{}tstupdaterpast_frc118'.format(CUR_YEAR), '{}tstupdaterpast_frc229'.format(CUR_YEAR), '{}tstupdaterpast_frc2791'.format(CUR_YEAR), '{}tstupdaterpast_frc155'.format(CUR_YEAR), '{}tstupdaterpast_frc549'.format(CUR_YEAR), '{}tstupdaterpast_frc195'.format(CUR_YEAR), '{}tstupdaterpast_frc4134'.format(CUR_YEAR), '{}tstupdaterpast_frc20'.format(CUR_YEAR), '{}tstupdaterpast_frc2836'.format(CUR_YEAR), '{}tstupdaterpast_frc869'.format(CUR_YEAR), '{}tstupdaterpast_frc1665'.format(CUR_YEAR), '{}tstupdaterpast_frc4055'.format(CUR_YEAR), '{}tstupdaterpast_frc3555'.format(CUR_YEAR), '{}tstupdaterpast_frc126'.format(CUR_YEAR), '{}tstupdaterpast_frc1699'.format(CUR_YEAR), '{}tstupdaterpast_frc1559'.format(CUR_YEAR), '{}tstupdaterpast_frc3464'.format(CUR_YEAR), '{}tstupdaterpast_frc2168'.format(CUR_YEAR), '{}tstupdaterpast_frc3461'.format(CUR_YEAR), '{}tstupdaterpast_frc1991'.format(CUR_YEAR), '{}tstupdaterpast_frc3467'.format(CUR_YEAR), '{}tstupdaterpast_frc2067'.format(CUR_YEAR), '{}tstupdaterpast_frc230'.format(CUR_YEAR), '{}tstupdaterpast_frc1124'.format(CUR_YEAR), '{}tstupdaterpast_frc3104'.format(CUR_YEAR), '{}tstupdaterpast_frc236'.format(CUR_YEAR), '{}tstupdaterpast_frc237'.format(CUR_YEAR), '{}tstupdaterpast_frc1511'.format(CUR_YEAR), '{}tstupdaterpast_frc250'.format(CUR_YEAR), '{}tstupdaterpast_frc1880'.format(CUR_YEAR), '{}tstupdaterpast_frc558'.format(CUR_YEAR), '{}tstupdaterpast_frc694'.format(CUR_YEAR), '{}tstupdaterpast_frc571'.format(CUR_YEAR), '{}tstupdaterpast_frc3634'.format(CUR_YEAR), '{}tstupdaterpast_frc3525'.format(CUR_YEAR), '{}tstupdaterpast_frc999'.format(CUR_YEAR), '{}tstupdaterpast_frc181'.format(CUR_YEAR), '{}tstupdaterpast_frc1073'.format(CUR_YEAR), '{}tstupdaterpast_frc3146'.format(CUR_YEAR), '{}tstupdaterpast_frc1071'.format(CUR_YEAR), '{}tstupdaterpast_frc1740'.format(CUR_YEAR), '{}tstupdaterpast_frc3719'.format(CUR_YEAR), '{}tstupdaterpast_frc3718'.format(CUR_YEAR), '{}tstupdaterpast_frc2170'.format(CUR_YEAR), '{}tstupdaterpast_frc663'.format(CUR_YEAR), '{}tstupdaterpast_frc4122'.format(CUR_YEAR), '{}tstupdaterpast_frc3182'.format(CUR_YEAR), '{}tstupdaterpast_frc839'.format(CUR_YEAR), '{}tstupdaterpast_frc1784'.format(CUR_YEAR), '{}tstupdaterpast_frc3654'.format(CUR_YEAR), '{}tstupdaterpast_frc743'.format(CUR_YEAR)}),
                         set())
        self.assertEqual(set([et_key.id() for et_key in et_keys_to_delete]).symmetric_difference({'{}tstupdaterpast_frc9999'.format(CUR_YEAR)}),
                         set())

    def test_update_pastyear(self):
        teams, event_teams, et_keys_to_delete = EventTeamUpdater.update('{}tstupdaterpastyear'.format(CUR_YEAR - 1))
        self.assertEqual(set([team.team_number for team in teams]).symmetric_difference({95, 178, 176, 1922, 173, 2785, 228, 177, 1099, 175, 1027, 3017, 1493, 118, 229, 2791, 155, 549, 195, 4134, 20, 2836, 869, 1665, 4055, 3555, 126, 1699, 1559, 3464, 2168, 3461, 1991, 3467, 2067, 230, 1124, 3104, 236, 237, 1511, 250, 1880, 558, 694, 571, 3634, 3525, 999, 181, 1073, 3146, 1071, 1740, 3719, 3718, 2170, 663, 4122, 3182, 839, 1784, 3654, 743}),
                         set())
        self.assertEqual(set([et.key_name for et in event_teams]).symmetric_difference({'{}tstupdaterpastyear_frc95'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc178'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc176'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1922'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc173'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc2785'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc228'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc177'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1099'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc175'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1027'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3017'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1493'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc118'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc229'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc2791'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc155'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc549'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc195'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc4134'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc20'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc2836'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc869'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1665'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc4055'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3555'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc126'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1699'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1559'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3464'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc2168'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3461'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1991'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3467'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc2067'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc230'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1124'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3104'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc236'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc237'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1511'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc250'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1880'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc558'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc694'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc571'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3634'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3525'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc999'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc181'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1073'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3146'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1071'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1740'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3719'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3718'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc2170'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc663'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc4122'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3182'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc839'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1784'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3654'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc743'.format(CUR_YEAR - 1)}),
                         set())
        self.assertEqual(et_keys_to_delete, set())

        event_team = EventTeam(
            id="%s_%s" % ('{}tstupdaterpastyear'.format(CUR_YEAR - 1), 'frc9999'),
            event=ndb.Key(Event, '{}tstupdaterpastyear'.format(CUR_YEAR - 1)),
            team=ndb.Key(Team, 'frc9999'),
            year=CUR_YEAR - 1)
        event_team.put()

        teams, event_teams, et_keys_to_delete = EventTeamUpdater.update('{}tstupdaterpastyear'.format(CUR_YEAR - 1))
        self.assertEqual(set([team.team_number for team in teams]).symmetric_difference({95, 178, 176, 1922, 173, 2785, 228, 177, 1099, 175, 1027, 3017, 1493, 118, 229, 2791, 155, 549, 195, 4134, 20, 2836, 869, 1665, 4055, 3555, 126, 1699, 1559, 3464, 2168, 3461, 1991, 3467, 2067, 230, 1124, 3104, 236, 237, 1511, 250, 1880, 558, 694, 571, 3634, 3525, 999, 181, 1073, 3146, 1071, 1740, 3719, 3718, 2170, 663, 4122, 3182, 839, 1784, 3654, 743}),
                         set())
        self.assertEqual(set([et.key_name for et in event_teams]).symmetric_difference({'{}tstupdaterpastyear_frc95'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc178'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc176'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1922'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc173'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc2785'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc228'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc177'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1099'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc175'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1027'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3017'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1493'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc118'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc229'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc2791'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc155'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc549'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc195'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc4134'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc20'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc2836'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc869'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1665'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc4055'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3555'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc126'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1699'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1559'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3464'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc2168'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3461'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1991'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3467'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc2067'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc230'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1124'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3104'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc236'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc237'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1511'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc250'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1880'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc558'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc694'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc571'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3634'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3525'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc999'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc181'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1073'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3146'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1071'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1740'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3719'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3718'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc2170'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc663'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc4122'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3182'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc839'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc1784'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc3654'.format(CUR_YEAR - 1), '{}tstupdaterpastyear_frc743'.format(CUR_YEAR - 1)}),
                         set())
        self.assertEqual(et_keys_to_delete, set())

########NEW FILE########
__FILENAME__ = test_event_test_creator
import datetime
import unittest2

from google.appengine.ext import testbed

from helpers.event.event_test_creator import EventTestCreator

from models.event import Event
from models.team import Team


class TestEventTeamCreator(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_memcache_stub()

        for team_number in range(7):
            Team(id="frc%s" % team_number,
                 team_number=team_number).put()

        self.events = []

    def tearDown(self):
        for event in self.events:
            event.key.delete()

        self.testbed.deactivate()

    def test_creates(self):
        self.events.append(EventTestCreator.createPastEvent())
        self.events.append(EventTestCreator.createFutureEvent())
        self.events.append(EventTestCreator.createPresentEvent())

        # TODO: assert the events got created properly -gregmarra 20130416

########NEW FILE########
__FILENAME__ = test_fms_event_list_parser
import unittest2

from datafeeds.fms_event_list_parser import FmsEventListParser


class TestFmsEventListParser(unittest2.TestCase):
    def test_parse_2012(self):
        with open('test_data/usfirst_html/fms_event_list_2012.html', 'r') as f:
            events = FmsEventListParser.parse_2012(f.read())

        self.assertEqual(len(events), 74)

        self.assertEqual(events[0]["first_eid"], "7689")
        self.assertEqual(events[0]["name"], "Alamo Regional")

        self.assertEqual(events[1]["first_eid"], "7585")
        self.assertEqual(events[1]["name"], "BAE Systems Granite State Regional")

    def test_parse_2014(self):
    	with open('test_data/usfirst_html/fms_event_list_2014.html', 'r') as f:
            events = FmsEventListParser.parse_2014(f.read())

        self.assertEqual(len(events), 103)

        self.assertEqual(events[0]["first_eid"], "10851")
        self.assertEqual(events[0]["name"], "Alamo Regional sponsored by Rackspace Hosting")

        self.assertEqual(events[1]["first_eid"], "10759")
        self.assertEqual(events[1]["name"], "Arizona Regional")

########NEW FILE########
__FILENAME__ = test_fms_team_list_parser
import unittest2

from datafeeds.fms_team_list_parser import FmsTeamListParser


class TestFmsTeamListParser(unittest2.TestCase):
    def test_parse(self):
        with open('test_data/usfirst_html/fms_team_list_2012.html', 'r') as f:
            teams = FmsTeamListParser.parse(f.read())

        # Test frc1
        team = teams[0]
        #self.assertEqual(team["address"], u'Pontiac, MI, USA')
        self.assertEqual(team["name"], u'BAE Systems/The Chrysler Fondation & Oakland Schools Technical Campus Northeast High School')
        self.assertEqual(team["nickname"], u'The Juggernauts')
        self.assertEqual(team["short_name"], u'ChryslerOSTCNE')
        self.assertEqual(team["team_number"], 1)

        # Test frc4403
        team = teams[-7]
        #self.assertEqual(team["address"], u'Torreon, CU, Mexico')
        self.assertEqual(team["name"], u'MET MEX PE\xd1OLES, S.A. DE C.V. & Tec de Monterrey Campus Laguna')
        self.assertEqual(team["nickname"], u'Pe\xf1oles-ITESM')
        self.assertEqual(team["short_name"], u'')
        self.assertEqual(team["team_number"], 4403)

########NEW FILE########
__FILENAME__ = test_key_name_validators
import unittest2

from models.event import Event
from models.match import Match
from models.team import Team


class TestKeyNameValidators(unittest2.TestCase):

    def setUp(self):
        self.valid_team_key = "frc177"
        self.valid_team_key2 = "frc1"
        self.invalid_team_key = "bcr077"
        self.invalid_team_key2 = "frc 011"
        self.invalid_team_key3 = "frc711\\"

        self.valid_event_key = "2010ct"
        self.valid_event_key2 = "2014onto2"
        self.invalid_event_key = "210c1"
        self.invalid_event_key2 = "frc2010ct"
        self.invalid_event_key3 = "2010 ct"

        self.valid_match_key = "2010ct_sf1m2"
        self.invalid_match_key = "0010c1_0m2"
        self.invalid_match_key2 = "2010c1_1f1m1"
        self.invalid_match_key3 = "2010c1_ef10m1"

    def test_valid_team_key(self):
        self.assertEqual(Team.validate_key_name(self.valid_team_key), True)
        self.assertEqual(Team.validate_key_name(self.valid_team_key2), True)

    def test_invalid_team_key(self):
        self.assertEqual(Team.validate_key_name(self.invalid_team_key), False)
        self.assertEqual(Team.validate_key_name(self.invalid_team_key2), False)
        self.assertEqual(Team.validate_key_name(self.invalid_team_key3), False)

    def test_valid_event_key(self):
        self.assertEqual(Event.validate_key_name(self.valid_event_key), True)
        self.assertEqual(Event.validate_key_name(self.valid_event_key2), True)

    def test_invalid_event_key(self):
        self.assertEqual(Event.validate_key_name(self.invalid_event_key), False)
        self.assertEqual(Event.validate_key_name(self.invalid_event_key2), False)
        self.assertEqual(Event.validate_key_name(self.invalid_event_key3), False)

    def test_valid_match_key(self):
        self.assertEqual(Match.validate_key_name(self.valid_match_key), True)

    def test_invalid_match_key(self):
        self.assertEqual(Match.validate_key_name(self.invalid_match_key), False)
        self.assertEqual(Match.validate_key_name(self.invalid_match_key2), False)
        self.assertEqual(Match.validate_key_name(self.invalid_match_key3), False)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_match
import unittest2

from google.appengine.ext import testbed

from models.match import Match


class TestMatch(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()

    def test_youtube_videos_formatted(self):
        # Test timestamp conversion
        data = {
            '5m6s': '306',
            '1m02s': '62',
            '10s': '10',
            '2m': '120',
            '12345': '12345',
            '5h': '18000',
            '1h2m3s': '3723',
        }
        for old_ts, seconds in data.items():
            match = Match(
                youtube_videos=['TqY324xLU4s#t=' + old_ts]
            )

            self.assertListEqual(match.youtube_videos_formatted, ['TqY324xLU4s?start=' + seconds])

        # Test that nothing is changed if there is no timestamp
        match = Match(
            youtube_videos=['TqY324xLU4s']
        )
        self.assertListEqual(match.youtube_videos_formatted, ['TqY324xLU4s'])

    def tearDown(self):
        self.testbed.deactivate()

########NEW FILE########
__FILENAME__ = test_match_cleanup
import unittest2

from google.appengine.ext import testbed

from helpers.match_helper import MatchHelper
from datafeeds.offseason_matches_parser import OffseasonMatchesParser
from models.event import Event
from models.match import Match


class TestMatchCleanup(unittest2.TestCase):
    def setUp(self):
        self.event = Event(
          id="2013test",
          event_short="test",
          year=2013
        )

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_memcache_stub()

    def tearDown(self):
        self.testbed.deactivate()

    def setupMatches(self, csv):
        with open(csv, 'r') as f:
            parsed_matches = OffseasonMatchesParser.parse(f.read())
            matches = [Match(
                id=Match.renderKeyName(
                    self.event.key.id(),
                    match.get("comp_level", None),
                    match.get("set_number", 0),
                    match.get("match_number", 0)),
                event=self.event.key,
                game=Match.FRC_GAMES_BY_YEAR.get(self.event.year, "frc_unknown"),
                set_number=match.get("set_number", 0),
                match_number=match.get("match_number", 0),
                comp_level=match.get("comp_level", None),
                team_key_names=match.get("team_key_names", None),
                alliances_json=match.get("alliances_json", None)
                )
                for match in parsed_matches]
            return matches

    def test_cleanup(self):
        matches = self.setupMatches('test_data/cleanup_matches.csv')
        cleaned_matches = MatchHelper.deleteInvalidMatches(matches)
        indices = {9, 12, 26}
        correct_matches = []
        for i, match in enumerate(matches):
            if i not in indices:
                correct_matches.append(match)
        self.assertEqual(correct_matches, cleaned_matches)

########NEW FILE########
__FILENAME__ = test_match_manipulator
import unittest2

from google.appengine.ext import ndb
from google.appengine.ext import testbed

from helpers.match_manipulator import MatchManipulator
from models.event import Event
from models.match import Match


class TestMatchManipulator(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_memcache_stub()
        self.testbed.init_taskqueue_stub()

        self.event = Event(
          id="2012ct",
          event_short="ct",
          year=2012
        )

        self.old_match = Match(
            id="2012ct_qm1",
            alliances_json="""{"blue": {"score": -1, "teams": ["frc3464", "frc20", "frc1073"]}, "red": {"score": -1, "teams": ["frc69", "frc571", "frc176"]}}""",
            comp_level="qm",
            event=self.event.key,
            game="frc_2012_rebr",
            set_number=1,
            match_number=1,
            team_key_names=[u'frc69', u'frc571', u'frc176', u'frc3464', u'frc20', u'frc1073'],
            youtube_videos=[u'P3C2BOtL7e8', u'tst1', u'tst2', u'tst3']
        )

        self.new_match = Match(
            id="2012ct_qm1",
            alliances_json="""{"blue": {"score": 57, "teams": ["frc3464", "frc20", "frc1073"]}, "red": {"score": 74, "teams": ["frc69", "frc571", "frc176"]}}""",
            comp_level="qm",
            event=self.event.key,
            game="frc_2012_rebr",
            set_number=1,
            match_number=1,
            team_key_names=[u'frc69', u'frc571', u'frc176', u'frc3464', u'frc20', u'frc1073'],
            youtube_videos=[u'TqY324xLU4s', u'tst1', u'tst3', u'tst4']
        )

    def tearDown(self):
        self.testbed.deactivate()

    def assertMergedMatch(self, match, is_auto_union):
        self.assertOldMatch(match)
        self.assertEqual(match.alliances_json, """{"blue": {"score": 57, "teams": ["frc3464", "frc20", "frc1073"]}, "red": {"score": 74, "teams": ["frc69", "frc571", "frc176"]}}""")
        if is_auto_union:
            self.assertEqual(set(match.youtube_videos), {u'P3C2BOtL7e8', u'TqY324xLU4s', u'tst1', u'tst2', u'tst3', u'tst4'})
        else:
            self.assertEqual(match.youtube_videos, [u'TqY324xLU4s', u'tst1', u'tst3', u'tst4'])

    def assertOldMatch(self, match):
        self.assertEqual(match.comp_level, "qm")
        self.assertEqual(match.set_number, 1)
        self.assertEqual(match.match_number, 1)
        self.assertEqual(match.team_key_names, [u'frc69', u'frc571', u'frc176', u'frc3464', u'frc20', u'frc1073'])

    def test_createOrUpdate(self):
        MatchManipulator.createOrUpdate(self.old_match)

        self.assertOldMatch(Match.get_by_id("2012ct_qm1"))
        self.assertEqual(Match.get_by_id("2012ct_qm1").alliances_json, """{"blue": {"score": -1, "teams": ["frc3464", "frc20", "frc1073"]}, "red": {"score": -1, "teams": ["frc69", "frc571", "frc176"]}}""")

        MatchManipulator.createOrUpdate(self.new_match)
        self.assertMergedMatch(Match.get_by_id("2012ct_qm1"), True)

    def test_findOrSpawn(self):
        self.old_match.put()
        self.assertMergedMatch(MatchManipulator.findOrSpawn(self.new_match), True)

    def test_updateMerge(self):
        self.assertMergedMatch(MatchManipulator.updateMerge(self.new_match, self.old_match), True)

    def test_createOrUpdate_no_auto_union(self):
        MatchManipulator.createOrUpdate(self.old_match)

        self.assertOldMatch(Match.get_by_id("2012ct_qm1"))
        self.assertEqual(Match.get_by_id("2012ct_qm1").alliances_json, """{"blue": {"score": -1, "teams": ["frc3464", "frc20", "frc1073"]}, "red": {"score": -1, "teams": ["frc69", "frc571", "frc176"]}}""")

        MatchManipulator.createOrUpdate(self.new_match, auto_union=False)
        self.assertMergedMatch(Match.get_by_id("2012ct_qm1"), False)

    def test_findOrSpawn_no_auto_union(self):
        self.old_match.put()
        self.assertMergedMatch(MatchManipulator.findOrSpawn(self.new_match, auto_union=False), False)

    def test_updateMerge_no_auto_union(self):
        self.assertMergedMatch(MatchManipulator.updateMerge(self.new_match, self.old_match, auto_union=False), False)

########NEW FILE########
__FILENAME__ = test_match_suggestion_accepter
import unittest2

from google.appengine.ext import ndb
from google.appengine.ext import testbed

from consts.event_type import EventType
from helpers.suggestions.match_suggestion_accepter import MatchSuggestionAccepter
from models.account import Account
from models.event import Event
from models.match import Match
from models.suggestion import Suggestion


class TestMatchSuggestionAccepter(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_memcache_stub()

        self.account = Account(
            email="tba@thebluealliance.com",
        )
        self.account.put()

        self.suggestion = Suggestion(
            author=self.account.key,
            contents_json="{\"youtube_videos\":[\"123456\"]}",
            target_key="2012ct_qm1",
            target_model="match"
        )
        self.suggestion.put()

        self.event = Event(
          id="2012ct",
          event_short="ct",
          year=2012,
          event_type_enum=EventType.REGIONAL,
        )
        self.event.put()

        self.match = Match(
            id="2012ct_qm1",
            alliances_json="""{"blue": {"score": -1, "teams": ["frc3464", "frc20", "frc1073"]}, "red": {"score": -1, "teams": ["frc69", "frc571", "frc176"]}}""",
            comp_level="qm",
            event=self.event.key,
            game="frc_2012_rebr",
            set_number=1,
            match_number=1,
            team_key_names=[u'frc69', u'frc571', u'frc176', u'frc3464', u'frc20', u'frc1073'],
            youtube_videos=["abcdef"]
        )
        self.match.put()

    def tearDown(self):
        self.testbed.deactivate()

    def test_accept_suggestions(self):
        MatchSuggestionAccepter.accept_suggestions([self.suggestion])

        match = Match.get_by_id("2012ct_qm1")
        self.assertTrue("abcdef" in match.youtube_videos)
        self.assertTrue("123456" in match.youtube_videos)

########NEW FILE########
__FILENAME__ = test_math_event_team_update
import unittest2
import datetime

from google.appengine.ext import ndb
from google.appengine.ext import testbed
from google.appengine.ext.webapp import Response

from consts.event_type import EventType
from controllers.cron_controller import EventTeamUpdate
from datafeeds.datafeed_usfirst import DatafeedUsfirst
from models.event import Event
from models.event_team import EventTeam
from models.match import Match
from models.team import Team


class TestDatafeedUsfirstTeams(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_memcache_stub()

        self.event = Event(
                id="2010sc",
                name="Palmetto Regional",
                event_type_enum=EventType.REGIONAL,
                short_name="Palmetto",
                event_short="sc",
                year=2010,
                end_date=datetime.datetime(2010, 03, 27),
                official=True,
                location='Clemson, SC',
                start_date=datetime.datetime(2010, 03, 24),
        )
        self.event.put()

        self.match = Match(
            id="2010sc_qm1",
            alliances_json="""{"blue": {"score": -1, "teams": ["frc3464", "frc20", "frc1073"]}, "red": {"score": -1, "teams": ["frc69", "frc571", "frc176"]}}""",
            comp_level="qm",
            event=self.event.key,
            game="frc_2010_bkwy",
            set_number=1,
            match_number=1,
            team_key_names=[u'frc69', u'frc571', u'frc176', u'frc3464', u'frc20', u'frc1073']
        )
        self.match.put()

    def tearDown(self):
        self.testbed.deactivate()

    # def test_doEventTeamUpdate(self):
    # call EventTeamUpdate with 2010sc
    #    eventteamupdate = EventTeamUpdate()
    #    eventteamupdate.response = Response()
    #    eventteamupdate.get("2010sc")
    #
    # Teams were generated by EventTeamUpdate, make sure EventTeams
    # exist and feature Team Keys
    #
    #    event_team_from_match_one = EventTeam.get_by_id("2010sc_frc69")
    #    self.assertEqual(event_team_from_match_one.event, self.event.key)
    #    self.assertEqual(event_team_from_match_one.team, ndb.Key(Team, "frc69"))
    #
    #    event_team_from_match_two = EventTeam.get_by_id("2010sc_frc20")
    #    self.assertEqual(event_team_from_match_two.event, self.event.key)
    #    self.assertEqual(event_team_from_match_two.team, ndb.Key(Team, "frc20"))

########NEW FILE########
__FILENAME__ = test_offseason_matches_parser
import unittest2

from datafeeds.offseason_matches_parser import OffseasonMatchesParser


class TestUsfirstMatchesParser(unittest2.TestCase):
    def test_parse(self):
        with open('test_data/offseason_matches.csv', 'r') as f:
            matches = OffseasonMatchesParser.parse(f.read())

        match = matches[0]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 1)
        self.assertEqual(match["team_key_names"], ['frc1', 'frc2', 'frc3', 'frc4', 'frc5', 'frc6'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": 8, "teams": ["frc4", "frc5", "frc6"]}, "red": {"score": 7, "teams": ["frc1", "frc2", "frc3"]}}""")

        match = matches[1]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 22)
        self.assertEqual(match["team_key_names"], ['frc1', 'frc2', 'frc3', 'frc4', 'frc5', 'frc6'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": -1, "teams": ["frc4", "frc5", "frc6"]}, "red": {"score": -1, "teams": ["frc1", "frc2", "frc3"]}}""")

        match = matches[2]
        self.assertEqual(match["comp_level"], "qf")
        self.assertEqual(match["set_number"], 2)
        self.assertEqual(match["match_number"], 1)
        self.assertEqual(match["team_key_names"], ['frc1', 'frc2', 'frc3', 'frc4', 'frc5', 'frc6'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": 8, "teams": ["frc4", "frc5", "frc6"]}, "red": {"score": 7, "teams": ["frc1", "frc2", "frc3"]}}""")

        match = matches[3]
        self.assertEqual(match["comp_level"], "sf")
        self.assertEqual(match["set_number"], 2)
        self.assertEqual(match["match_number"], 3)
        self.assertEqual(match["team_key_names"], ['frc1', 'frc2', 'frc3', 'frc4', 'frc5', 'frc6'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": 8, "teams": ["frc4", "frc5", "frc6"]}, "red": {"score": 7, "teams": ["frc1", "frc2", "frc3"]}}""")

        match = matches[4]
        self.assertEqual(match["comp_level"], "f")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 2)
        self.assertEqual(match["team_key_names"], ['frc1', 'frc2', 'frc3', 'frc4', 'frc5', 'frc6'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": 8, "teams": ["frc4", "frc5", "frc6"]}, "red": {"score": 7, "teams": ["frc1", "frc2", "frc3"]}}""")

        match = matches[5]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 99)
        self.assertEqual(match["team_key_names"], ['frc3', 'frc4', 'frc5'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": 8, "teams": ["frc4", "frc5", "frc6A"]}, "red": {"score": 7, "teams": ["frc1A", "frc2B", "frc3"]}}""")

        match = matches[6]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 12)
        self.assertEqual(match["team_key_names"], ['frc3', 'frc1', 'frc2', 'frc6'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": 8, "teams": ["frc1", "frc2", "frc6"]}, "red": {"score": 7, "teams": ["frc1B", "frc2B", "frc3"]}}""")

########NEW FILE########
__FILENAME__ = test_tba_videos_parser
import unittest2

from datafeeds.tba_videos_parser import TbaVideosParser


class TestTbaVideosParser(unittest2.TestCase):
    def test_parse(self):
        with open('test_data/usfirst_html/tba_videos_2006gal.html', 'r') as f:
            videos = TbaVideosParser.parse(f.read())

        self.assertEqual(videos, {u'2006gal_qm34': [u'flv', u'jpg'], u'2006gal_qm35': [u'flv', u'jpg'], u'2006gal_qm36': [u'flv', u'jpg'], u'2006gal_qm37': [u'flv', u'jpg'], u'2006gal_qm30': [u'flv', u'jpg'], u'2006gal_qm31': [u'flv', u'jpg'], u'2006gal_qm32': [u'flv', u'jpg'], u'2006gal_qm33': [u'flv', u'jpg'], u'2006gal_qm38': [u'flv', u'jpg'], u'2006gal_qm39': [u'flv', u'jpg'], u'2006gal_f1m1': [u'flv', u'jpg'], u'2006gal_qm41': [u'flv', u'jpg'], u'2006gal_qm40': [u'flv', u'jpg'], u'2006gal_qm43': [u'flv', u'jpg'], u'2006gal_qm42': [u'flv', u'jpg'], u'2006gal_qm45': [u'flv', u'jpg'], u'2006gal_qm44': [u'flv', u'jpg'], u'2006gal_qm47': [u'flv', u'jpg'], u'2006gal_qm46': [u'flv', u'jpg'], u'2006gal_qm49': [u'flv', u'jpg'], u'2006gal_qm48': [u'flv', u'jpg'], u'2006gal_qm27': [u'flv', u'jpg'], u'2006gal_qm26': [u'flv', u'jpg'], u'2006gal_qm24': [u'flv', u'jpg'], u'2006gal_qm23': [u'flv', u'jpg'], u'2006gal_qm22': [u'flv', u'jpg'], u'2006gal_qm21': [u'flv', u'jpg'], u'2006gal_qm20': [u'flv', u'jpg'], u'2006gal_qm29': [u'flv', u'jpg'], u'2006gal_qm28': [u'flv', u'jpg'], u'2006gal_sf2m1': [u'flv', u'jpg'], u'2006gal_sf2m2': [u'flv', u'jpg'], u'2006gal_qm12': [u'flv', u'jpg'], u'2006gal_qm13': [u'flv', u'jpg'], u'2006gal_qm10': [u'flv', u'jpg'], u'2006gal_qm11': [u'flv', u'jpg'], u'2006gal_qm16': [u'flv', u'jpg'], u'2006gal_qm17': [u'flv', u'jpg'], u'2006gal_qm14': [u'flv', u'jpg'], u'2006gal_qm15': [u'flv', u'jpg'], u'2006gal_qm18': [u'flv', u'jpg'], u'2006gal_qm19': [u'flv', u'jpg'], u'2006gal_qf4m1': [u'flv', u'jpg'], u'2006gal_qf4m2': [u'flv', u'jpg'], u'2006gal_qm98': [u'flv', u'jpg'], u'2006gal_qm99': [u'flv', u'jpg'], u'2006gal_qm92': [u'flv', u'jpg'], u'2006gal_qm93': [u'flv', u'jpg'], u'2006gal_qm90': [u'flv', u'jpg'], u'2006gal_qm91': [u'flv', u'jpg'], u'2006gal_qm96': [u'flv', u'jpg'], u'2006gal_qm97': [u'flv', u'jpg'], u'2006gal_qm94': [u'flv', u'jpg'], u'2006gal_qm95': [u'flv', u'jpg'], u'2006gal_f2m2': [u'flv', u'jpg'], u'2006gal_qm85': [u'flv', u'jpg'], u'2006gal_qm84': [u'flv', u'jpg'], u'2006gal_qm87': [u'flv', u'jpg'], u'2006gal_qm86': [u'flv', u'jpg'], u'2006gal_qm81': [u'flv', u'jpg'], u'2006gal_qm80': [u'flv', u'jpg'], u'2006gal_qm83': [u'flv', u'jpg'], u'2006gal_qm82': [u'flv', u'jpg'], u'2006gal_qm89': [u'flv', u'jpg'], u'2006gal_qm88': [u'flv', u'jpg'], u'2006gal_sf1m2': [u'flv', u'jpg'], u'2006gal_sf1m1': [u'flv', u'jpg'], u'2006gal_qm101': [u'flv', u'jpg'], u'2006gal_qm100': [u'flv', u'jpg'], u'2006gal_qf3m2': [u'flv', u'jpg'], u'2006gal_qf3m1': [u'flv', u'jpg'], u'2006gal_qm1': [u'flv', u'jpg'], u'2006gal_qm2': [u'flv', u'jpg'], u'2006gal_qm3': [u'flv', u'jpg'], u'2006gal_qm4': [u'flv', u'jpg'], u'2006gal_qm5': [u'flv', u'jpg'], u'2006gal_qm6': [u'flv', u'jpg'], u'2006gal_qm7': [u'flv', u'jpg'], u'2006gal_qm8': [u'flv', u'jpg'], u'2006gal_qm9': [u'flv', u'jpg'], u'2006gal_qm78': [u'flv', u'jpg'], u'2006gal_qm79': [u'flv', u'jpg'], u'2006gal_qm70': [u'flv', u'jpg'], u'2006gal_qm71': [u'flv', u'jpg'], u'2006gal_qm72': [u'flv', u'jpg'], u'2006gal_qm73': [u'flv', u'jpg'], u'2006gal_qm74': [u'flv', u'jpg'], u'2006gal_qm75': [u'flv', u'jpg'], u'2006gal_qm76': [u'flv', u'jpg'], u'2006gal_qm77': [u'flv', u'jpg'], u'2006gal_qf1m1': [u'flv', u'jpg'], u'2006gal_qf1m2': [u'flv', u'jpg'], u'2006gal_qf2m2': [u'flv', u'jpg'], u'2006gal_qf2m1': [u'flv', u'jpg'], u'2006gal_qm69': [u'flv', u'jpg'], u'2006gal_qm68': [u'flv', u'jpg'], u'2006gal_qm63': [u'flv', u'jpg'], u'2006gal_qm62': [u'flv', u'jpg'], u'2006gal_qm61': [u'flv', u'jpg'], u'2006gal_qm60': [u'flv', u'jpg'], u'2006gal_qm67': [u'flv', u'jpg'], u'2006gal_qm66': [u'flv', u'jpg'], u'2006gal_qm65': [u'flv', u'jpg'], u'2006gal_qm64': [u'flv', u'jpg'], u'2006gal_qm56': [u'flv', u'jpg'], u'2006gal_qm57': [u'flv', u'jpg'], u'2006gal_qm54': [u'flv', u'jpg'], u'2006gal_qm55': [u'flv', u'jpg'], u'2006gal_qm52': [u'flv', u'jpg'], u'2006gal_qm53': [u'flv', u'jpg'], u'2006gal_qm50': [u'flv', u'jpg'], u'2006gal_qm51': [u'flv', u'jpg'], u'2006gal_qm58': [u'flv', u'jpg'], u'2006gal_qm59': [u'flv', u'jpg']})

########NEW FILE########
__FILENAME__ = test_team_api
import unittest2
import webtest
import json
import webapp2

from datetime import datetime

from google.appengine.ext import ndb
from google.appengine.ext import testbed

from consts.event_type import EventType

from controllers.api_controller import ApiTeamDetails, ApiTeamsShow

from models.team import Team
from models.event import Event
from models.event_team import EventTeam


class TestApiTeamShow(unittest2.TestCase):

    # TODO: Add event_keys testing. -brandondean 10/21/2012
    def setUp(self):
        app = webapp2.WSGIApplication([(r'/', ApiTeamsShow)], debug=True)
        self.testapp = webtest.TestApp(app)

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_memcache_stub()
        self.testbed.init_taskqueue_stub()

        self.team = Team(
                id="frc281",
                name="Michelin / Caterpillar / Greenville Technical College /\
                jcpenney / Baldor / ASME / Gastroenterology Associates /\
                Laserflex South & Greenville County Schools & Greenville\
                Technical Charter High School",
                team_number=281,
                nickname="EnTech GreenVillians",
                address="Greenville, SC, USA",
                website="www.entech.org",
        )

        self.team.put()

        self.event = Event(
                id="2010sc",
                name="Palmetto Regional",
                event_type_enum=EventType.REGIONAL,
                short_name="Palmetto",
                event_short="sc",
                year=2010,
                end_date=datetime(2010, 03, 27),
                official=True,
                location='Clemson, SC',
                start_date=datetime(2010, 03, 24),
        )

        self.event.put()

        self.event_team = EventTeam(
                team=self.team.key,
                event=self.event.key,
                year=datetime.now().year
        )

        self.event_team.put()

    def tearDown(self):
        self.testbed.deactivate()

    def assertTeamJson(self, team_dict):
        self.assertEqual(team_dict["key"], self.team.key_name)
        self.assertEqual(team_dict["team_number"], self.team.team_number)
        self.assertEqual(team_dict["nickname"], self.team.nickname)
        self.assertEqual(team_dict["location"], self.team.location)
        self.assertEqual(team_dict["locality"], "Greenville")
        self.assertEqual(team_dict["country_name"], "USA")
        self.assertEqual(team_dict["region"], "SC")
        self.assertEqual(team_dict["website"], self.team.website)
        self.assertTrue(self.event.key.id() in team_dict["events"])

    def testTeamShow(self):
        response = self.testapp.get('/?teams=frc281', headers={"X-TBA-App-Id": "tba-tests:team-api-test:v01"})

        team_dict = json.loads(response.body)
        self.assertTeamJson(team_dict[0])

    def testNonexistentTeam(self):
        response = self.testapp.get('/?teams=frc3141579265', headers={"X-TBA-App-Id": "tba-tests:team-api-test:v01"}, status=404)

        self.assertEqual(response.status_int, 404)

    def test_validate_tba_app_id(self):
        response = self.testapp.get('/?teams=frc254', expect_errors=True)  # By default get() doesn't send a user agent
        self.assertEqual(response.status, "400 Bad Request")
        self.assertTrue('Error' in json.loads(response.body).keys())


class TestApiTeamDetails(unittest2.TestCase):

    def setUp(self):
        app = webapp2.WSGIApplication([(r'/', ApiTeamDetails)], debug=True)
        self.testapp = webtest.TestApp(app)

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_urlfetch_stub()
        self.testbed.init_memcache_stub()

    def tearDown(self):
        self.testbed.deactivate()

    def testNonexistentTeam(self):
        response = self.testapp.get('/?team=frc3141579265', headers={"X-TBA-App-Id": "tba-tests:team-api-test:v01"}, status=404)

        self.assertEqual(response.status_int, 404)

    def test_validate_tba_app_id(self):
        response = self.testapp.get('/?team=frc3141579265', expect_errors=True)  # By default get() doesn't send a user agent
        self.assertEqual(response.status, "400 Bad Request")
        self.assertTrue('Error' in json.loads(response.body).keys())

########NEW FILE########
__FILENAME__ = test_team_manipulator
import unittest2

from google.appengine.ext import ndb
from google.appengine.ext import testbed

from helpers.team_manipulator import TeamManipulator
from models.team import Team


class TestTeamManipulator(unittest2.TestCase):
    def setUp(self):
        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_datastore_v3_stub()
        self.testbed.init_memcache_stub()

        self.old_team = Team(
            id="frc177",
            team_number=177,
            rookie_year=1996,
            first_tpid=61771,
            first_tpid_year=2012,
        )

        self.new_team = Team(
            id="frc177",
            team_number=177,
            rookie_year=1995,
            website="http://www.bobcatrobotics.org",
        )

    def tearDown(self):
        self.testbed.deactivate()

    def assertMergedTeam(self, team):
        self.assertOldTeam(team)
        self.assertEqual(team.website, "http://www.bobcatrobotics.org")
        self.assertEqual(team.rookie_year, 1995)

    def assertOldTeam(self, team):
        self.assertEqual(team.first_tpid, 61771)
        self.assertEqual(team.first_tpid_year, 2012)
        self.assertEqual(team.key_name, "frc177")
        self.assertEqual(team.team_number, 177)

    def test_createOrUpdate(self):
        TeamManipulator.createOrUpdate(self.old_team)
        self.assertOldTeam(Team.get_by_id("frc177"))
        TeamManipulator.createOrUpdate(self.new_team)
        self.assertMergedTeam(Team.get_by_id("frc177"))

    def test_findOrSpawn(self):
        self.old_team.put()
        self.assertMergedTeam(TeamManipulator.findOrSpawn(self.new_team))

    def test_updateMerge(self):
        self.assertMergedTeam(TeamManipulator.updateMerge(self.new_team, self.old_team))

    def test_create_lots_of_teams(self):
        number = 500
        teams = [Team(
            id="frc%s" % team_number,
            team_number=team_number)
            for team_number in range(number)]
        TeamManipulator.createOrUpdate(teams)

        team = Team.get_by_id("frc177")
        self.assertEqual(team.key_name, "frc177")
        self.assertEqual(team.team_number, 177)

        team = Team.get_by_id("frc%s" % (number - 1))
        self.assertEqual(team.key_name, "frc%s" % (number - 1))
        self.assertEqual(team.team_number, number - 1)

########NEW FILE########
__FILENAME__ = test_usfirst_alliances_parser
import unittest2

from datafeeds.usfirst_alliances_parser import UsfirstAlliancesParser


class TestUsfirstAlliancesParser(unittest2.TestCase):
    def test_parse_2014test(self):
        with open('test_data/usfirst_html/usfirst_event_matches_2014test.html', 'r') as f:
            alliances, _ = UsfirstAlliancesParser.parse(f.read())

        self.assertEqual(alliances, None)

    def test_parse_2014curie(self):
        with open('test_data/usfirst_html/usfirst_event_matches_2014curie.html', 'r') as f:
            alliances, _ = UsfirstAlliancesParser.parse(f.read())

        self.assertEqual(
            alliances,
            [
                {'picks': ['frc254', 'frc469', 'frc2848', 'frc74'], 'declines':[] },
                {'picks': ['frc1718', 'frc2451', 'frc573', 'frc2016'], 'declines':[] },
                {'picks': ['frc2928', 'frc2013', 'frc1311', 'frc842'], 'declines':[] },
                {'picks': ['frc180', 'frc125', 'frc1323', 'frc2468'], 'declines':[] },
                {'picks': ['frc118', 'frc359', 'frc4334', 'frc865'], 'declines':[] },
                {'picks': ['frc135', 'frc1241', 'frc11', 'frc68'], 'declines':[] },
                {'picks': ['frc3478', 'frc177', 'frc294', 'frc230'], 'declines':[] },
                {'picks': ['frc624', 'frc987', 'frc3476', 'frc3015'], 'declines':[] },
            ]
        )

########NEW FILE########
__FILENAME__ = test_usfirst_event_awards_parser
import unittest2
import json

from consts.award_type import AwardType
from datafeeds.usfirst_event_awards_parser import UsfirstEventAwardsParser


def convert_to_comparable(data):
    """
    Converts jsons to dicts so that elements can be more easily compared
    """
    if type(data) == list:
        return [convert_to_comparable(e) for e in data]
    elif type(data) == dict:
        to_return = {}
        for key, value in data.items():
            to_return[key] = convert_to_comparable(value)
        return to_return
    elif type(data) == str or type(data) == unicode:
        try:
            return json.loads(data)
        except ValueError:
            return data
    else:
        return data


class TestUsfirstEventAwardsParser(unittest2.TestCase):
    def test_parse_regional_2007(self):
        with open('test_data/usfirst_html/usfirst_event_awards_2007sj.html', 'r') as f:
            awards, _ = UsfirstEventAwardsParser.parse(f.read())
 
        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 26)
        self.assertEqual(len(awards), 21)
 
        awards = convert_to_comparable(awards)
 
        # Test Team Award
        team_award = {
            'name_str': "Regional Chairman's Award",
            'award_type_enum': AwardType.CHAIRMANS,
            'team_number_list': [604],
            'recipient_json_list': [{'team_number': 604, 'awardee': None}],
        }
        self.assertTrue(team_award in awards)
 
        # Test Multi Team Award
        multi_team_award = {
            'name_str': "Regional Winner",
            'award_type_enum': AwardType.WINNER,
            'team_number_list': [1280, 1516, 190],
            'recipient_json_list': [{'team_number': 1280, 'awardee': None},
                                    {'team_number': 1516, 'awardee': None},
                                    {'team_number': 190, 'awardee': None}],
        }
        self.assertTrue(multi_team_award in awards)
 
        # Test Individual Award
        individual_award = {
            'name_str': "Woodie Flowers Award",
            'award_type_enum': AwardType.WOODIE_FLOWERS,
            'team_number_list': [],
            'recipient_json_list': [{'team_number': None, 'awardee': u"Yang Xie \u2013 Team 846"}],
        }
        self.assertTrue(individual_award in awards)
 
    def test_parse_regional_2010(self):
        with open('test_data/usfirst_html/usfirst_event_awards_2010sac.html', 'r') as f:
            awards, _ = UsfirstEventAwardsParser.parse(f.read())
 
        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 29)
        self.assertEqual(len(awards), 24)
 
        awards = convert_to_comparable(awards)
 
        # Test Team Award
        team_award = {
            'name_str': "Regional Chairman's Award",
            'award_type_enum': AwardType.CHAIRMANS,
            'team_number_list': [604],
            'recipient_json_list': [{'team_number': 604, 'awardee': None}],
        }
        self.assertTrue(team_award in awards)
 
        # Test Individual Award
        individual_award = {
            'name_str': "Outstanding Volunteer of the Year",
            'award_type_enum': AwardType.VOLUNTEER,
            'team_number_list': [],
            'recipient_json_list': [{'team_number': None, 'awardee': u"Gary Blakesley"}],
        }
        self.assertTrue(individual_award in awards)
 
        # Test Team and Individual Award
        team_and_individual_award = {
            'name_str': "Woodie Flowers Award",
            'award_type_enum': AwardType.WOODIE_FLOWERS,
            'team_number_list': [604],
            'recipient_json_list': [{'team_number': 604, 'awardee': u"Helen Arrington"}],
        }
        self.assertTrue(team_and_individual_award in awards)
 
    def test_parse_regional_2012(self):
        with open('test_data/usfirst_html/usfirst_event_awards_2012sj.html', 'r') as f:
            awards, _ = UsfirstEventAwardsParser.parse(f.read())
 
        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 28)
        self.assertEqual(len(awards), 23)
 
        awards = convert_to_comparable(awards)
 
        # Test Team Award
        team_award = {
            'name_str': "Regional Chairman's Award",
            'award_type_enum': AwardType.CHAIRMANS,
            'team_number_list': [604],
            'recipient_json_list': [{'team_number': 604, 'awardee': None}],
        }
        self.assertTrue(team_award in awards)
 
        # Test Individual Award
        individual_award = {
            'name_str': "Volunteer of the Year",
            'award_type_enum': AwardType.VOLUNTEER,
            'team_number_list': [],
            'recipient_json_list': [{'team_number': None, 'awardee': u"Joanne Heberer"}],
        }
        self.assertTrue(individual_award in awards)
 
        # Test Team and Individual Award
        team_and_individual_award = {
            'name_str': "Woodie Flowers Finalist Award",
            'award_type_enum': AwardType.WOODIE_FLOWERS,
            'team_number_list': [604],
            'recipient_json_list': [{'team_number': 604, 'awardee': u"Jim Mori"}],
        }
        self.assertTrue(team_and_individual_award in awards)
 
    def test_parse_district_championship_2009(self):
        with open('test_data/usfirst_html/usfirst_event_awards_2009gl.html', 'r') as f:
            awards, _ = UsfirstEventAwardsParser.parse(f.read())
 
        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 30)
        self.assertEqual(len(awards), 21)
 
        awards = convert_to_comparable(awards)
 
        # Test Multi Team Award
        multi_team_award = {
            'name_str': "State Championship Chairman's Award",
            'award_type_enum': AwardType.CHAIRMANS,
            'team_number_list': [27, 33, 217],
            'recipient_json_list': [{'team_number': 27, 'awardee': None},
                                    {'team_number': 33, 'awardee': None},
                                    {'team_number': 217, 'awardee': None}],
        }
        self.assertTrue(multi_team_award in awards)
 
        # Test Individual Award
        individual_award = {
            'name_str': "Woodie Flowers Award",
            'award_type_enum': AwardType.WOODIE_FLOWERS,
            'team_number_list': [],
            'recipient_json_list': [{'team_number': None, 'awardee': u"Jennifer Harvey of Team 503"}],
        }
        self.assertTrue(individual_award in awards)
 
    def test_parse_district_championship_2012(self):
        with open('test_data/usfirst_html/usfirst_event_awards_2012gl.html', 'r') as f:
            awards, _ = UsfirstEventAwardsParser.parse(f.read())
 
        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 34)
        self.assertEqual(len(awards), 22)
 
        awards = convert_to_comparable(awards)
 
        # Test Muti Team Award
        multi_team_award = {
            'name_str': "Regional Chairman's Award",
            'award_type_enum': AwardType.CHAIRMANS,
            'team_number_list': [33, 503, 27],
            'recipient_json_list': [{'team_number': 33, 'awardee': None},
                                    {'team_number': 503, 'awardee': None},
                                    {'team_number': 27, 'awardee': None}],
        }
        self.assertTrue(multi_team_award in awards)
 
        # Test Multi Team and Individual Award
        multi_team_and_individual_award = {
            'name_str': "FIRST Dean's List Finalist Award",
            'award_type_enum': AwardType.DEANS_LIST,
            'team_number_list': [3538, 548, 2834, 33, 862, 1684],
            'recipient_json_list': [{'team_number': 3538, 'awardee': u"Jaris Dingman"},
                                    {'team_number': 548, 'awardee': u"Claire Goolsby"},
                                    {'team_number': 2834, 'awardee': u"Ryan Hoyt"},
                                    {'team_number': 33, 'awardee': u"Andrew Palardy"},
                                    {'team_number': 862, 'awardee': u"Ian Pudney"},
                                    {'team_number': 1684, 'awardee': u"Matthew Wagner"}],
        }
        self.assertTrue(multi_team_and_individual_award in awards)
 
    def test_parse_championship_divison_2007(self):
        with open('test_data/usfirst_html/usfirst_event_awards_2007galileo.html', 'r') as f:
            awards, _ = UsfirstEventAwardsParser.parse(f.read())
 
        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 7)
        self.assertEqual(len(awards), 3)
 
        awards = convert_to_comparable(awards)
 
        # Test Team Award
        team_award = {
            'name_str': "Galileo - Highest Rookie Seed",
            'award_type_enum': AwardType.HIGHEST_ROOKIE_SEED,
            'team_number_list': [2272],
            'recipient_json_list': [{'team_number': 2272, 'awardee': None}],
        }
        self.assertTrue(team_award in awards)
 
        # Test Multi Team Award
        multi_team_award = {
            'name_str': "Galileo - Division Winner",
            'award_type_enum': AwardType.WINNER,
            'team_number_list': [173, 1319, 1902],
            'recipient_json_list': [{'team_number': 173, 'awardee': None},
                                    {'team_number': 1319, 'awardee': None},
                                    {'team_number': 1902, 'awardee': None}],
        }
        self.assertTrue(multi_team_award in awards)
 
    def test_parse_championship_2007(self):
        with open('test_data/usfirst_html/usfirst_event_awards_2007cmp.html', 'r') as f:
            awards, _ = UsfirstEventAwardsParser.parse(f.read())
 
        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 29)
        self.assertEqual(len(awards), 23)
 
        awards = convert_to_comparable(awards)
 
        # Test Team Award
        team_award = {
            'name_str': "Championship - Chairman's Award",
            'award_type_enum': AwardType.CHAIRMANS,
            'team_number_list': [365],
            'recipient_json_list': [{'team_number': 365, 'awardee': None}],
        }
        self.assertTrue(team_award in awards)
 
        # Test Individual Award
        individual_award = {
            'name_str': "Championship - FRC Outstanding Volunteer Award",
            'award_type_enum': AwardType.VOLUNTEER,
            'team_number_list': [],
            'recipient_json_list': [{'team_number': None, 'awardee': u"Mark Koors"}],
        }
        self.assertTrue(individual_award in awards)
 
    def test_parse_championship_divison_2012(self):
        with open('test_data/usfirst_html/usfirst_event_awards_2012galileo.html', 'r') as f:
            awards, _ = UsfirstEventAwardsParser.parse(f.read())
 
        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 7)
        self.assertEqual(len(awards), 3)
 
        awards = convert_to_comparable(awards)
 
        # Test Team Award
        team_award = {
            'name_str': "Highest Rookie Seed - Galileo",
            'award_type_enum': AwardType.HIGHEST_ROOKIE_SEED,
            'team_number_list': [4394],
            'recipient_json_list': [{'team_number': 4394, 'awardee': None}],
        }
        self.assertTrue(team_award in awards)
 
        # Test Multi Team Award
        multi_team_award = {
            'name_str': "Championship Division Winners - Galileo",
            'award_type_enum': AwardType.WINNER,
            'team_number_list': [25, 180, 16],
            'recipient_json_list': [{'team_number': 25, 'awardee': None},
                                    {'team_number': 180, 'awardee': None},
                                    {'team_number': 16, 'awardee': None}],
        }
        self.assertTrue(multi_team_award in awards)

    def test_parse_championship_2011(self):
        with open('test_data/usfirst_html/usfirst_event_awards_2011cmp.html', 'r') as f:
            awards, _ = UsfirstEventAwardsParser.parse(f.read())

        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 38)
        self.assertEqual(len(awards), 24)

        awards = convert_to_comparable(awards)

        # Test Team Award
        team_award = {
            'name_str': "Championship - Excellence in Design Award sponsored by Autodesk (3D CAD)",
            'award_type_enum': AwardType.EXCELLENCE_IN_DESIGN_CAD,
            'team_number_list': [75],
            'recipient_json_list': [{'team_number': 75, 'awardee': None}],
        }
        self.assertTrue(team_award in awards)

    def test_parse_championship_2012(self):
        with open('test_data/usfirst_html/usfirst_event_awards_2012cmp.html', 'r') as f:
            awards, _ = UsfirstEventAwardsParser.parse(f.read())
 
        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 39)
        self.assertEqual(len(awards), 25)
 
        awards = convert_to_comparable(awards)
 
        # Test Team Award
        team_award = {
            'name_str': "Excellence in Design Award sponsored by Autodesk (3D CAD)",
            'award_type_enum': AwardType.EXCELLENCE_IN_DESIGN_CAD,
            'team_number_list': [862],
            'recipient_json_list': [{'team_number': 862, 'awardee': None}],
        }
        self.assertTrue(team_award in awards)
 
        team_award = {
            'name_str': "Excellence in Design Award sponsored by Autodesk (Animation)",
            'award_type_enum': AwardType.EXCELLENCE_IN_DESIGN_ANIMATION,
            'team_number_list': [192],
            'recipient_json_list': [{'team_number': 192, 'awardee': None}],
        }
        self.assertTrue(team_award in awards)
 
        team_award = {
            'name_str': "Entrepreneurship Award sponsored by Kleiner Perkins Caufield and Byers",
            'award_type_enum': AwardType.ENTREPRENEURSHIP,
            'team_number_list': [3132],
            'recipient_json_list': [{'team_number': 3132, 'awardee': None}],
        }
        self.assertTrue(team_award in awards)
 
        # Test Individual Award
        individual_award = {
            'name_str': "Founder's Award",
            'award_type_enum': AwardType.FOUNDERS,
            'team_number_list': [],
            'recipient_json_list': [{'team_number': None, 'awardee': u"Google"}],
        }
        self.assertTrue(individual_award in awards)
 
        # Test Multi Team and Individual Award
        multi_team_and_individual_award = {
            'name_str': "FIRST Dean's List Award",
            'award_type_enum': AwardType.DEANS_LIST,
            'team_number_list': [3059, 1540, 128, 1058, 3138, 3196, 1912, 2996, 842, 704],
            'recipient_json_list': [{'team_number': 3059, 'awardee': u"Ikechukwa Chima"},
                                    {'team_number': 1540, 'awardee': u"Marina Dimitrov"},
                                    {'team_number': 128, 'awardee': u"Chase Douglas"},
                                    {'team_number': 1058, 'awardee': u"Tristan Evarts"},
                                    {'team_number': 3138, 'awardee': u"Danielle Gehron"},
                                    {'team_number': 3196, 'awardee': u"David Gomez"},
                                    {'team_number': 1912, 'awardee': u"Rachel Holladay"},
                                    {'team_number': 2996, 'awardee': u"Jasmine Kemper"},
                                    {'team_number': 842, 'awardee': u"John Rangel"},
                                    {'team_number': 704, 'awardee': u"Matthew Ricks"}],
        }
        self.assertTrue(multi_team_and_individual_award in awards)
 
        # Test Team and Individual Award
        team_and_individual_award = {
            'name_str': "Woodie Flowers Award",
            'award_type_enum': AwardType.WOODIE_FLOWERS,
            'team_number_list': [2614],
            'recipient_json_list': [{'team_number': 2614, 'awardee': u"Earl Scime"}],
        }
        self.assertTrue(team_and_individual_award in awards)
 
    def test_parse_championship_2013(self):
        with open('test_data/usfirst_html/usfirst_event_awards_2013cmp.html', 'r') as f:
            awards, _ = UsfirstEventAwardsParser.parse(f.read())
 
        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 37)
        self.assertEqual(len(awards), 22)
 
        awards = convert_to_comparable(awards)
 
        # Test New Awards
        new_award = {
            'name_str': "Make It Loud Award",
            'award_type_enum': AwardType.MAKE_IT_LOUD,
            'team_number_list': [],
            'recipient_json_list': [{'team_number': None, 'awardee': "will.i.am"}],
        }
        self.assertTrue(new_award in awards)
 
        new_award = {
            'name_str': "Media and Technology Award sponsored by Comcast",
            'award_type_enum': AwardType.MEDIA_AND_TECHNOLOGY,
            'team_number_list': [2283],
            'recipient_json_list': [{'team_number': 2283, 'awardee': None}],
        }
        self.assertTrue(new_award in awards)
 
        new_award = {
            'name_str': "Dr. Bart Kamen Memorial Scholarship",
            'award_type_enum': AwardType.BART_KAMEN_MEMORIAL,
            'team_number_list': [],
            'recipient_json_list': [{'team_number': None, 'awardee': "Sofia Dhanani"},
                                    {'team_number': None, 'awardee': "Sarah Rudasill"},
                                    {'team_number': None, 'awardee': "Pascale Wallace Patterson"}],
        }
        self.assertTrue(new_award in awards)

########NEW FILE########
__FILENAME__ = test_usfirst_event_awards_parser_02
import unittest2
import json

from consts.award_type import AwardType
from datafeeds.usfirst_event_awards_parser_02 import UsfirstEventAwardsParser_02


def convert_to_comparable(data):
    """
    Converts jsons to dicts so that elements can be more easily compared
    """
    if type(data) == list:
        return [convert_to_comparable(e) for e in data]
    elif type(data) == dict:
        to_return = {}
        for key, value in data.items():
            to_return[key] = convert_to_comparable(value)
        return to_return
    elif type(data) == str or type(data) == unicode:
        try:
            return json.loads(data)
        except ValueError:
            return data
    else:
        return data


class TestUsfirstEventAwardsParser_02(unittest2.TestCase):
    def test_parse_regional_2002(self):
        with open('test_data/usfirst_html/usfirst_event_awards_2002sj.html', 'r') as f:
            awards, _ = UsfirstEventAwardsParser_02.parse(f.read())

        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 26)
        self.assertEqual(len(awards), 20)

        awards = convert_to_comparable(awards)

        # Test Team Award
        team_award = {
            'name_str': u"Regional Chairmans Award",
            'award_type_enum': AwardType.CHAIRMANS,
            'team_number_list': [192],
            'recipient_json_list': [{'team_number': 192, 'awardee': None}],
        }
        self.assertTrue(team_award in awards)

        individual_award = {
            'name_str': "#1 Seed",
            'award_type_enum': AwardType.NUM_1_SEED,
            'team_number_list': [254],
            'recipient_json_list': [{'team_number': 254, 'awardee': None}],
        }
        self.assertTrue(individual_award in awards)

        # Test Multi Team Award
        multi_team_award = {
            'name_str': "Regional Winner",
            'award_type_enum': AwardType.WINNER,
            'team_number_list': [254, 60, 359],
            'recipient_json_list': [{'team_number': 254, 'awardee': None},
                                    {'team_number': 60, 'awardee': None},
                                    {'team_number': 359, 'awardee': None}],
        }
        self.assertTrue(multi_team_award in awards)

########NEW FILE########
__FILENAME__ = test_usfirst_event_awards_parser_03_04
import unittest2
import json

from consts.award_type import AwardType
from datafeeds.usfirst_event_awards_parser_03_04 import UsfirstEventAwardsParser_03_04


def convert_to_comparable(data):
    """
    Converts jsons to dicts so that elements can be more easily compared
    """
    if type(data) == list:
        return [convert_to_comparable(e) for e in data]
    elif type(data) == dict:
        to_return = {}
        for key, value in data.items():
            to_return[key] = convert_to_comparable(value)
        return to_return
    elif type(data) == str or type(data) == unicode:
        try:
            return json.loads(data)
        except ValueError:
            return data
    else:
        return data


class TestUsfirstEventAwardsParser_03_04(unittest2.TestCase):
    def test_parse_regional_2004(self):
        with open('test_data/usfirst_html/usfirst_event_awards_2004sj.html', 'r') as f:
            awards, _ = UsfirstEventAwardsParser_03_04.parse(f.read())

        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 26)
        self.assertEqual(len(awards), 21)

        awards = convert_to_comparable(awards)

        # Test Team Award
        team_award = {
            'name_str': u"Regional Chairman\u2019s Award",
            'award_type_enum': AwardType.CHAIRMANS,
            'team_number_list': [254],
            'recipient_json_list': [{'team_number': 254, 'awardee': None}],
        }
        self.assertTrue(team_award in awards)

        # Test Multi Team Award
        multi_team_award = {
            'name_str': "Regional Winner",
            'award_type_enum': AwardType.WINNER,
            'team_number_list': [971, 254, 852],
            'recipient_json_list': [{'team_number': 971, 'awardee': None},
                                    {'team_number': 254, 'awardee': None},
                                    {'team_number': 852, 'awardee': None}],
        }
        self.assertTrue(multi_team_award in awards)

        # Test Individual Award
        individual_award = {
            'name_str': "Regional Woodie Flowers Award",
            'award_type_enum': AwardType.WOODIE_FLOWERS,
            'team_number_list': [115],
            'recipient_json_list': [{'team_number': 115, 'awardee': u"Ted Shinta"}],
        }
        self.assertTrue(individual_award in awards)

    def test_parse_regional_2003(self):
        with open('test_data/usfirst_html/usfirst_event_awards_2003sj.html', 'r') as f:
            awards, _ = UsfirstEventAwardsParser_03_04.parse(f.read())

        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 25)
        self.assertEqual(len(awards), 18)

        awards = convert_to_comparable(awards)

        # Test Team Award
        team_award = {
            'name_str': u"Regional Chairman\u2019s Award",
            'award_type_enum': AwardType.CHAIRMANS,
            'team_number_list': [359],
            'recipient_json_list': [{'team_number': 359, 'awardee': None}],
        }
        self.assertTrue(team_award in awards)

        # Test Multi Team Award
        multi_team_award = {
            'name_str': "Regional Winner",
            'award_type_enum': AwardType.WINNER,
            'team_number_list': [115, 254, 852],
            'recipient_json_list': [{'team_number': 115, 'awardee': None},
                                    {'team_number': 254, 'awardee': None},
                                    {'team_number': 852, 'awardee': None}],
        }
        self.assertTrue(multi_team_award in awards)

        # Test Individual Award
        individual_award = {
            'name_str': "Silicon Valley Regional Volunteer of the Year",
            'award_type_enum': AwardType.VOLUNTEER,
            'team_number_list': [],
            'recipient_json_list': [{'team_number': None, 'awardee': u"Ken Krieger"},
                                    {'team_number': None, 'awardee': u"Ken Leung"},],
        }
        self.assertTrue(individual_award in awards)

    def test_parse_cmp_2003(self):
        with open('test_data/usfirst_html/usfirst_event_awards_2003cmp.html', 'r') as f:
            awards, _ = UsfirstEventAwardsParser_03_04.parse(f.read())

        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 26)
        self.assertEqual(len(awards), 20)

        awards = convert_to_comparable(awards)

        team_award = {
            'name_str': u"Rookie All Star Award",
            'award_type_enum': AwardType.ROOKIE_ALL_STAR,
            'team_number_list': [1108, 1023],
            'recipient_json_list': [{'team_number': 1108, 'awardee': None},
                                    {'team_number': 1023, 'awardee': None}],
        }
        self.assertTrue(team_award in awards)

########NEW FILE########
__FILENAME__ = test_usfirst_event_awards_parser_05_06
import unittest2
import json

from consts.award_type import AwardType
from datafeeds.usfirst_event_awards_parser_05_06 import UsfirstEventAwardsParser_05_06


def convert_to_comparable(data):
    """
    Converts jsons to dicts so that elements can be more easily compared
    """
    if type(data) == list:
        return [convert_to_comparable(e) for e in data]
    elif type(data) == dict:
        to_return = {}
        for key, value in data.items():
            to_return[key] = convert_to_comparable(value)
        return to_return
    elif type(data) == str or type(data) == unicode:
        try:
            return json.loads(data)
        except ValueError:
            return data
    else:
        return data


class TestUsfirstEventAwardsParser_05_06(unittest2.TestCase):
    def test_parse_regional_2006sj(self):
        with open('test_data/usfirst_html/usfirst_event_awards_2006sj.html', 'r') as f:
            awards, _ = UsfirstEventAwardsParser_05_06.parse(f.read())

        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 27)
        self.assertEqual(len(awards), 22)

        awards = convert_to_comparable(awards)

        # Test Team Award
        team_award = {
            'name_str': "Regional Chairman's Award",
            'award_type_enum': AwardType.CHAIRMANS,
            'team_number_list': [192],
            'recipient_json_list': [{'team_number': 192, 'awardee': None}],
        }
        self.assertTrue(team_award in awards)

        # Test Multi Team Award
        multi_team_award = {
            'name_str': "Regional Winner",
            'award_type_enum': AwardType.WINNER,
            'team_number_list': [254, 581, 766],
            'recipient_json_list': [{'team_number': 254, 'awardee': None},
                                    {'team_number': 581, 'awardee': None},
                                    {'team_number': 766, 'awardee': None}],
        }
        self.assertTrue(multi_team_award in awards)

        # Test Individual Award
        individual_award = {
            'name_str': "Regional Woodie Flowers Award",
            'award_type_enum': AwardType.WOODIE_FLOWERS,
            'team_number_list': [],
            'recipient_json_list': [{'team_number': None, 'awardee': u"William Dunbar"}],
        }
        self.assertTrue(individual_award in awards)

    def test_parse_regional_2006or(self):
        with open('test_data/usfirst_html/usfirst_event_awards_2006or.html', 'r') as f:
            awards, _ = UsfirstEventAwardsParser_05_06.parse(f.read())

        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 28)
        self.assertEqual(len(awards), 22)

        awards = convert_to_comparable(awards)

        # Test Team Award
        team_award = {
            'name_str': "Regional Chairman's Award",
            'award_type_enum': AwardType.CHAIRMANS,
            'team_number_list': [492],
            'recipient_json_list': [{'team_number': 492, 'awardee': None}],
        }
        self.assertTrue(team_award in awards)

    def test_parse_regional_2005sj(self):
        with open('test_data/usfirst_html/usfirst_event_awards_2005sj.html', 'r') as f:
            awards, _ = UsfirstEventAwardsParser_05_06.parse(f.read())

        # Check number of parsed awards
        num_awards = 0
        for award in awards:
            num_awards += len(award['recipient_json_list'])
        self.assertEqual(num_awards, 26)
        self.assertEqual(len(awards), 21)

        awards = convert_to_comparable(awards)

        # Test Team Award
        team_award = {
            'name_str': "Regional Chairmans Winner",
            'award_type_enum': AwardType.CHAIRMANS,
            'team_number_list': [368],
            'recipient_json_list': [{'team_number': 368, 'awardee': None}],
        }
        self.assertTrue(team_award in awards)

        # Test Multi Team Award
        multi_team_award = {
            'name_str': "Regional Winner",
            'award_type_enum': AwardType.WINNER,
            'team_number_list': [980, 254, 22],
            'recipient_json_list': [{'team_number': 980, 'awardee': None},
                                    {'team_number': 254, 'awardee': None},
                                    {'team_number': 22, 'awardee': None}],
        }
        self.assertTrue(multi_team_award in awards)

        # Test Individual Award
        individual_award = {
            'name_str': "Regional Woodie Flowers Award",
            'award_type_enum': AwardType.WOODIE_FLOWERS,
            'team_number_list': [568],
            'recipient_json_list': [{'team_number': 568, 'awardee': u"AREA/BP/CIRI & Dimond High"}],
        }
        self.assertTrue(individual_award in awards)

########NEW FILE########
__FILENAME__ = test_usfirst_event_details_parser
import unittest2
import datetime

from consts.event_type import EventType
from datafeeds.usfirst_event_details_parser import UsfirstEventDetailsParser


class TestUsfirstEventDetailsParser(unittest2.TestCase):
    def test_parse2012ct(self):
        with open('test_data/usfirst_html/usfirst_event_details_2012ct.html', 'r') as f:
            event, _ = UsfirstEventDetailsParser.parse(f.read())

        self.assertEqual(event["name"], "Northeast Utilities FIRST Connecticut Regional")
        self.assertEqual(event["short_name"], "Northeast Utilities FIRST Connecticut")
        self.assertEqual(event["event_type_enum"], EventType.REGIONAL)
        self.assertEqual(event["start_date"], datetime.datetime(2012, 3, 29, 0, 0))
        self.assertEqual(event["end_date"], datetime.datetime(2012, 3, 31, 0, 0))
        self.assertEqual(event["year"], 2012)
        self.assertEqual(event["venue_address"], "Connecticut Convention Center\r\n100 Columbus Blvd\r\nHartford, CT 06103\r\nUSA")
        self.assertEqual(event["venue"], "Connecticut Convention Center")
        self.assertEqual(event["location"], "Hartford, CT, USA")
        self.assertEqual(event["website"], "http://www.ctfirst.org/ctr")
        self.assertEqual(event["event_short"], "ct")

    def test_parse2013flbr(self):
        with open('test_data/usfirst_html/usfirst_event_details_2013flbr.html', 'r') as f:
            event, _ = UsfirstEventDetailsParser.parse(f.read())

        self.assertEqual(event["name"], "South Florida Regional")
        self.assertEqual(event["short_name"], "South Florida")
        self.assertEqual(event["event_type_enum"], EventType.REGIONAL)
        self.assertEqual(event["start_date"], datetime.datetime(2013, 3, 28, 0, 0))
        self.assertEqual(event["end_date"], datetime.datetime(2013, 3, 30, 0, 0))
        self.assertEqual(event["year"], 2013)
        self.assertEqual(event["venue_address"], "Great Fort Lauderdale & Broward County Convention Center\r\n1950 Eisenhower Boulevard\r\nFort Lauderdale, FL 33316\r\nUSA")
        self.assertEqual(event["venue"], "Great Fort Lauderdale & Broward County Convention Center")
        self.assertEqual(event["location"], "Fort Lauderdale, FL, USA")
        self.assertEqual(event["website"], "http://firstinflorida.org")
        self.assertEqual(event["event_short"], "flbr")

    def test_parse2013casj(self):
        with open('test_data/usfirst_html/usfirst_event_details_2013casj.html', 'r') as f:
            event, _ = UsfirstEventDetailsParser.parse(f.read())

        self.assertEqual(event["name"], "Silicon Valley Regional")
        self.assertEqual(event["short_name"], "Silicon Valley")
        self.assertEqual(event["event_type_enum"], EventType.REGIONAL)
        self.assertEqual(event["start_date"], datetime.datetime(2013, 4, 4, 0, 0))
        self.assertEqual(event["end_date"], datetime.datetime(2013, 4, 6, 0, 0))
        self.assertEqual(event["year"], 2013)
        self.assertEqual(event["venue_address"], "San Jose State University\r\nThe Event Center\r\nSan Jose, CA 95192\r\nUSA")
        self.assertEqual(event["venue"], "San Jose State University")
        self.assertEqual(event["location"], "San Jose, CA, USA")
        self.assertEqual(event["website"], "http://www.firstsv.org")
        self.assertEqual(event["event_short"], "casj")

    def test_parse2001sj(self):
        with open('test_data/usfirst_html/usfirst_event_details_2001ca2.html', 'r') as f:
            event, _ = UsfirstEventDetailsParser.parse(f.read())

        self.assertEqual(event["name"], "Silicon Valley Regional")
        self.assertEqual(event["short_name"], "Silicon Valley")
        self.assertEqual(event["event_type_enum"], EventType.REGIONAL)
        self.assertEqual(event["start_date"], datetime.datetime(2001, 3, 22, 0, 0))
        self.assertEqual(event["end_date"], datetime.datetime(2001, 3, 24, 0, 0))
        self.assertEqual(event["year"], 2001)
        self.assertEqual(event["venue_address"], "San Jose, CA\r\nUSA")
        self.assertEqual(event["location"], "San Jose, CA, USA")
        self.assertEqual(event["event_short"], "ca2")

    def test_parse2005is(self):
        with open('test_data/usfirst_html/usfirst_event_details_2005is.html', 'r') as f:
            event, _ = UsfirstEventDetailsParser.parse(f.read())

        self.assertEqual(event["name"], "GM/Technion University Israel Pilot Regional")
        self.assertEqual(event["short_name"], "GM/Technion University Israel Pilot")
        self.assertEqual(event["event_type_enum"], EventType.REGIONAL)
        self.assertEqual(event["start_date"], datetime.datetime(2005, 3, 9, 0, 0))
        self.assertEqual(event["end_date"], datetime.datetime(2005, 3, 9, 0, 0))
        self.assertEqual(event["year"], 2005)
        self.assertEqual(event["venue_address"], "Haifa Sports Coliseum\r\nHaifa, Haifa\r\nIsrael")
        self.assertEqual(event["venue"], "Haifa Sports Coliseum")
        self.assertEqual(event["location"], "Haifa, Haifa, Israel")
        self.assertEqual(event["event_short"], "is")

    def test_parse2005or(self):
        with open('test_data/usfirst_html/usfirst_event_details_2005or.html', 'r') as f:
            event, _ = UsfirstEventDetailsParser.parse(f.read())

        self.assertEqual(event["name"], "Pacific Northwest Regional")
        self.assertEqual(event["short_name"], "Pacific Northwest")
        self.assertEqual(event["event_type_enum"], EventType.REGIONAL)
        self.assertEqual(event["start_date"], datetime.datetime(2005, 3, 10, 0, 0))
        self.assertEqual(event["end_date"], datetime.datetime(2005, 3, 12, 0, 0))
        self.assertEqual(event["year"], 2005)
        self.assertEqual(event["venue_address"], "Memorial Coliseum\r\nPortland, OR 97201\r\nUSA")
        self.assertEqual(event["venue"], "Memorial Coliseum")
        self.assertEqual(event["location"], "Portland, OR, USA")
        self.assertEqual(event["event_short"], "or")

    def test_parse_1997il(self):
        with open('test_data/usfirst_html/usfirst_event_details_1997il.html', 'r') as f:
            event, _ = UsfirstEventDetailsParser.parse(f.read())

        self.assertEqual(event["name"], "Motorola Midwest Regional")
        self.assertEqual(event["short_name"], "Motorola Midwest")
        self.assertEqual(event["event_type_enum"], EventType.REGIONAL)
        self.assertEqual(event["start_date"], datetime.datetime(1997, 3, 6, 0, 0))
        self.assertEqual(event["end_date"], datetime.datetime(1997, 3, 8, 0, 0))
        self.assertEqual(event["year"], 1997)
        self.assertEqual(event["venue_address"], "William Rainey Harper College\r\nChicago, IL\r\nUSA")
        self.assertEqual(event["venue"], "William Rainey Harper College")
        self.assertEqual(event["location"], "Chicago, IL, USA")
        self.assertEqual(event["event_short"], "il")

    def test_parse_2002sj(self):
        with open('test_data/usfirst_html/usfirst_event_details_2002sj.html', 'r') as f:
            event, _ = UsfirstEventDetailsParser.parse(f.read())

        self.assertEqual(event["name"], "Silicon Valley Regional")
        self.assertEqual(event["short_name"], "Silicon Valley")
        self.assertEqual(event["event_type_enum"], EventType.REGIONAL)
        self.assertEqual(event["start_date"], datetime.datetime(2002, 3, 28, 0, 0))
        self.assertEqual(event["end_date"], datetime.datetime(2002, 3, 30, 0, 0))
        self.assertEqual(event["year"], 2002)
        self.assertEqual(event["venue_address"], "San Jose, CA\r\nUSA")
        self.assertEqual(event["location"], "San Jose, CA, USA")
        self.assertEqual(event["event_short"], "sj")

########NEW FILE########
__FILENAME__ = test_usfirst_event_list_parser
import unittest2
import datetime

from consts.district_type import DistrictType
from consts.event_type import EventType
from datafeeds.usfirst_event_list_parser import UsfirstEventListParser


class TestUsfirstEventListParser(unittest2.TestCase):
    def test_parse_2012(self):
        with open('test_data/usfirst_html/usfirst_event_list_2012.html', 'r') as f:
            events, _ = UsfirstEventListParser.parse(f.read())

        self.assertEqual(len(events), 69)

        self.assertEqual(events[0]["first_eid"], "7617")
        self.assertEqual(events[0]["event_type_enum"], EventType.REGIONAL)
        self.assertEqual(events[0]["name"], "Greater Kansas City Regional")

        self.assertEqual(events[1]["first_eid"], "7585")
        self.assertEqual(events[1]["event_type_enum"], EventType.REGIONAL)
        self.assertEqual(events[1]["name"], "BAE Systems Granite State Regional")

        self.assertEqual(events[51]["event_type_enum"], EventType.REGIONAL)
        self.assertEqual(events[52]["event_type_enum"], EventType.DISTRICT_CMP)
        self.assertEqual(events[54]["event_type_enum"], EventType.DISTRICT)

    def test_parse_2014(self):
        with open('test_data/usfirst_html/usfirst_event_list_2014.html', 'r') as f:
            events, _ = UsfirstEventListParser.parse(f.read())

        self.assertEqual(len(events), 98)

        self.assertEqual(events[2]["first_eid"], "10851")
        self.assertEqual(events[2]["event_type_enum"], EventType.REGIONAL)
        self.assertEqual(events[2]["name"], "Alamo Regional sponsored by Rackspace Hosting")
        self.assertEqual(events[2]["event_district_enum"], DistrictType.NO_DISTRICT)

        self.assertEqual(events[5]["first_eid"], "10807")
        self.assertEqual(events[5]["event_type_enum"], EventType.REGIONAL)
        self.assertEqual(events[5]["name"], "Israel Regional")
        self.assertEqual(events[5]["event_district_enum"], DistrictType.NO_DISTRICT)

        self.assertEqual(events[56]["event_type_enum"], EventType.DISTRICT_CMP)
        self.assertEqual(events[56]["event_district_enum"], DistrictType.NEW_ENGLAND)
        self.assertEqual(events[93]["event_type_enum"], EventType.DISTRICT)
        self.assertEqual(events[93]["event_district_enum"], DistrictType.PACIFIC_NORTHWEST)

########NEW FILE########
__FILENAME__ = test_usfirst_event_offseason_list_parser
import unittest2
import datetime

from consts.event_type import EventType
from datafeeds.usfirst_event_offseason_list_parser import UsfirstEventOffseasonListParser


class TestUsfirstEventOffseasonListParser(unittest2.TestCase):
    def test_parse_2013(self):
        with open('test_data/usfirst_html/usfirst_event_offseason_list_2013.html', 'r') as f:
            events, _ = UsfirstEventOffseasonListParser.parse(f.read())

        self.assertEqual(len(events), 35)

        self.assertEqual(events[0]["first_eid"], "off-season-ozark-mountain-2013")
        self.assertEqual(events[0]["event_type_enum"], EventType.OFFSEASON)
        self.assertEqual(events[0]["name"], "Ozark Mountain")
        self.assertEqual(events[0]["start_date"], datetime.datetime(2013, 9, 13))
        self.assertEqual(events[0]["end_date"], datetime.datetime(2013, 9, 14))
        self.assertEqual(events[0]["location"], "AR")

        self.assertEqual(events[1]["first_eid"], "off-season-powerhouse-pwnage")
        self.assertEqual(events[1]["event_type_enum"], EventType.OFFSEASON)
        self.assertEqual(events[1]["name"], "Powerhouse Pwnage")
        self.assertEqual(events[1]["start_date"], datetime.datetime(2013, 9, 13))
        self.assertEqual(events[1]["end_date"], datetime.datetime(2013, 9, 14))
        self.assertEqual(events[1]["location"], "CA")

        self.assertEqual(events[4]["start_date"], datetime.datetime(2013, 9, 21))
        self.assertEqual(events[4]["end_date"], datetime.datetime(2013, 9, 21))

########NEW FILE########
__FILENAME__ = test_usfirst_event_rankings_parser
import unittest2

from datafeeds.usfirst_event_rankings_parser import UsfirstEventRankingsParser


class TestUsfirstEventRankingsParser(unittest2.TestCase):
    def test_parse_2012ct(self):
        with open('test_data/usfirst_html/usfirst_event_rankings_2012ct.html', 'r') as f:
            rankings, _ = UsfirstEventRankingsParser.parse(f.read())

        self.assertEqual(rankings, [['Rank', 'Team', 'QS', 'HP', 'BP', 'TP', 'CP', 'Record (W-L-T)', 'DQ', 'Played'], ['1', '2168', '32.00', '147.00', '60.00', '208.00', '14', '9-1-0', '0', '10'], ['2', '118', '31.00', '168.00', '90.00', '231.00', '17', '7-3-0', '0', '10'], ['3', '177', '30.00', '177.00', '120.00', '151.00', '14', '8-2-0', '0', '10'], ['4', '195', '29.00', '116.00', '70.00', '190.00', '16', '6-3-1', '0', '10'], ['5', '237', '28.00', '120.00', '60.00', '123.00', '14', '7-3-0', '0', '10'], ['6', '1071', '28.00', '115.00', '120.00', '142.00', '10', '9-1-0', '0', '10'], ['7', '173', '28.00', '114.00', '110.00', '108.00', '14', '7-3-0', '0', '10'], ['8', '1073', '28.00', '110.00', '100.00', '152.00', '11', '8-1-1', '0', '10'], ['9', '694', '28.00', '78.00', '100.00', '140.00', '14', '7-3-0', '0', '10'], ['10', '558', '27.00', '152.00', '100.00', '145.00', '13', '7-3-0', '0', '10'], ['11', '175', '27.00', '141.00', '160.00', '117.00', '13', '7-3-0', '0', '10'], ['12', '181', '26.00', '151.00', '70.00', '95.00', '14', '6-4-0', '0', '10'], ['13', '176', '26.00', '120.00', '60.00', '90.00', '18', '4-6-0', '0', '10'], ['14', '1511', '26.00', '111.00', '80.00', '164.00', '14', '6-4-0', '0', '10'], ['15', '126', '26.00', '108.00', '70.00', '165.00', '14', '6-4-0', '0', '10'], ['16', '4122', '26.00', '92.00', '100.00', '78.00', '14', '6-4-0', '0', '10'], ['17', '869', '25.00', '68.00', '130.00', '75.00', '12', '6-3-1', '0', '10'], ['18', '3464', '24.00', '135.00', '80.00', '109.00', '14', '5-5-0', '0', '10'], ['19', '3467', '24.00', '101.00', '80.00', '123.00', '10', '7-3-0', '0', '10'], ['20', '3718', '24.00', '100.00', '60.00', '106.00', '12', '6-4-0', '0', '10'], ['21', '3461', '24.00', '79.00', '30.00', '94.00', '14', '5-5-0', '0', '10'], ['22', '4055', '24.00', '78.00', '80.00', '79.00', '16', '4-6-0', '0', '10'], ['23', '1922', '23.00', '114.00', '110.00', '151.00', '10', '6-3-1', '0', '10'], ['24', '95', '22.00', '120.00', '70.00', '123.00', '14', '4-6-0', '0', '10'], ['25', '1991', '22.00', '113.00', '100.00', '58.00', '12', '5-5-0', '0', '10'], ['26', '839', '22.00', '96.00', '110.00', '136.00', '10', '6-4-0', '0', '10'], ['27', '1099', '21.00', '126.00', '110.00', '97.00', '8', '6-3-1', '0', '10'], ['28', '230', '20.00', '143.00', '80.00', '104.00', '8', '6-4-0', '0', '10'], ['29', '3017', '20.00', '134.00', '50.00', '88.00', '12', '4-6-0', '0', '10'], ['30', '2067', '20.00', '128.00', '80.00', '122.00', '10', '5-5-0', '0', '10'], ['31', '250', '20.00', '118.00', '40.00', '99.00', '10', '5-5-0', '0', '10'], ['32', '155', '20.00', '100.00', '50.00', '74.00', '12', '4-6-0', '0', '10'], ['33', '236', '20.00', '99.00', '20.00', '126.00', '10', '5-5-0', '0', '10'], ['34', '1124', '20.00', '92.00', '80.00', '109.00', '8', '6-4-0', '0', '10'], ['35', '3146', '20.00', '81.00', '110.00', '81.00', '6', '7-3-0', '0', '10'], ['36', '663', '20.00', '71.00', '90.00', '90.00', '12', '4-6-0', '0', '10'], ['37', '1699', '20.00', '70.00', '80.00', '139.00', '12', '4-6-0', '0', '10'], ['38', '1027', '20.00', '53.00', '70.00', '97.00', '12', '4-6-0', '0', '10'], ['39', '20', '19.00', '79.00', '70.00', '106.00', '9', '5-5-0', '0', '10'], ['40', '3182', '18.00', '108.00', '60.00', '147.00', '8', '5-5-0', '0', '10'], ['41', '229', '18.00', '97.00', '40.00', '153.00', '10', '4-6-0', '0', '10'], ['42', '1665', '18.00', '95.00', '120.00', '106.00', '10', '4-6-0', '0', '10'], ['43', '228', '18.00', '81.00', '60.00', '163.00', '10', '4-6-0', '0', '10'], ['44', '178', '18.00', '81.00', '50.00', '58.00', '12', '3-7-0', '0', '10'], ['45', '1740', '18.00', '62.00', '20.00', '99.00', '8', '5-5-0', '0', '10'], ['46', '3634', '18.00', '54.00', '30.00', '105.00', '10', '4-6-0', '0', '10'], ['47', '2791', '18.00', '53.00', '100.00', '108.00', '10', '4-6-0', '0', '10'], ['48', '571', '18.00', '53.00', '70.00', '109.00', '10', '4-6-0', '0', '10'], ['49', '2170', '17.00', '89.00', '60.00', '103.00', '9', '4-5-0', '1', '10'], ['50', '1493', '16.00', '150.00', '60.00', '132.00', '6', '5-5-0', '0', '10'], ['51', '549', '16.00', '129.00', '100.00', '91.00', '6', '5-5-0', '0', '10'], ['52', '743', '16.00', '70.00', '30.00', '67.00', '10', '3-7-0', '0', '10'], ['53', '2836', '16.00', '64.00', '80.00', '126.00', '8', '4-6-0', '0', '10'], ['54', '999', '14.00', '114.00', '20.00', '79.00', '10', '2-8-0', '0', '10'], ['55', '3525', '14.00', '109.00', '40.00', '66.00', '6', '4-6-0', '0', '10'], ['56', '3104', '14.00', '92.00', '20.00', '80.00', '6', '4-6-0', '0', '10'], ['57', '3555', '14.00', '68.00', '60.00', '68.00', '8', '3-7-0', '0', '10'], ['58', '4134', '13.00', '96.00', '30.00', '80.00', '6', '3-6-1', '0', '10'], ['59', '1559', '12.00', '110.00', '10.00', '94.00', '8', '2-8-0', '0', '10'], ['60', '3719', '12.00', '97.00', '60.00', '95.00', '6', '3-7-0', '0', '10'], ['61', '3654', '12.00', '59.00', '20.00', '57.00', '8', '2-8-0', '0', '10'], ['62', '2785', '12.00', '41.00', '70.00', '96.00', '8', '2-8-0', '0', '10'], ['63', '1880', '10.00', '57.00', '40.00', '86.00', '6', '2-8-0', '0', '10'], ['64', '1784', '10.00', '44.00', '40.00', '60.00', '6', '2-7-0', '1', '10']])

    def test_parse_2014casj(self):
        with open('test_data/usfirst_html/usfirst_event_rankings_2014casj.html', 'r') as f:
            rankings, _ = UsfirstEventRankingsParser.parse(f.read())

        self.assertEqual(rankings, [['Rank', 'Team', 'QS', 'ASSIST', 'AUTO', 'T&C', 'TELEOP', 'Record (W-L-T)', 'DQ', 'PLAYED'], ['1', '971', '22.00', '670.00', '650.00', '80.00', '477.00', '11-0-0', '0', '11'], ['2', '1678', '20.00', '1020.00', '541.00', '260.00', '227.00', '10-1-0', '0', '11'], ['3', '254', '20.00', '830.00', '520.00', '180.00', '505.00', '10-1-0', '0', '11'], ['4', '2035', '18.00', '750.00', '426.00', '280.00', '253.00', '9-2-0', '0', '11'], ['5', '1323', '18.00', '640.00', '435.00', '320.00', '335.00', '9-2-0', '0', '11'], ['6', '2144', '18.00', '600.00', '436.00', '180.00', '400.00', '9-2-0', '0', '11'], ['7', '192', '16.00', '610.00', '485.00', '140.00', '302.00', '8-3-0', '0', '11'], ['8', '1280', '16.00', '460.00', '387.00', '160.00', '182.00', '8-3-0', '0', '11'], ['9', '846', '16.00', '450.00', '555.00', '180.00', '381.00', '8-3-0', '0', '11'], ['10', '114', '14.00', '610.00', '401.00', '210.00', '265.00', '7-4-0', '0', '11'], ['11', '2473', '14.00', '460.00', '375.00', '190.00', '328.00', '7-4-0', '0', '11'], ['12', '4990', '14.00', '460.00', '351.00', '140.00', '378.00', '7-4-0', '0', '11'], ['13', '670', '14.00', '360.00', '470.00', '190.00', '294.00', '7-4-0', '0', '11'], ['14', '604', '14.00', '360.00', '386.00', '230.00', '301.00', '7-4-0', '0', '11'], ['15', '668', '14.00', '350.00', '500.00', '110.00', '323.00', '7-4-0', '0', '11'], ['16', '368', '12.00', '530.00', '555.00', '180.00', '468.00', '6-5-0', '0', '11'], ['17', '2489', '12.00', '470.00', '335.00', '130.00', '263.00', '6-5-0', '0', '11'], ['18', '1351', '12.00', '420.00', '330.00', '220.00', '361.00', '6-5-0', '0', '11'], ['19', '4543', '12.00', '390.00', '495.00', '190.00', '284.00', '6-5-0', '0', '11'], ['20', '2813', '12.00', '390.00', '272.00', '150.00', '199.00', '6-5-0', '0', '11'], ['21', '1388', '12.00', '380.00', '347.00', '160.00', '282.00', '6-5-0', '0', '11'], ['22', '8', '12.00', '370.00', '383.00', '180.00', '122.00', '6-5-0', '0', '11'], ['23', '3482', '12.00', '370.00', '328.00', '130.00', '259.00', '6-5-0', '0', '11'], ['24', '751', '12.00', '360.00', '307.00', '170.00', '276.00', '6-5-0', '0', '11'], ['25', '256', '12.00', '310.00', '385.00', '120.00', '263.00', '6-5-0', '0', '11'], ['26', '1700', '12.00', '280.00', '441.00', '100.00', '238.00', '6-5-0', '0', '11'], ['27', '840', '10.00', '500.00', '344.00', '50.00', '376.00', '5-6-0', '0', '11'], ['28', '3256', '10.00', '400.00', '361.00', '150.00', '388.00', '5-6-0', '0', '11'], ['29', '1662', '10.00', '370.00', '490.00', '180.00', '200.00', '5-6-0', '0', '11'], ['30', '2135', '10.00', '360.00', '307.00', '130.00', '159.00', '5-6-0', '0', '11'], ['31', '692', '10.00', '350.00', '301.00', '100.00', '218.00', '5-6-0', '0', '11'], ['32', '852', '10.00', '340.00', '361.00', '150.00', '312.00', '5-6-0', '0', '11'], ['33', '4047', '10.00', '340.00', '275.00', '150.00', '173.00', '5-6-0', '0', '11'], ['34', '1868', '10.00', '300.00', '431.00', '140.00', '247.00', '5-6-0', '0', '11'], ['35', '1072', '10.00', '300.00', '373.00', '180.00', '202.00', '5-6-0', '0', '11'], ['36', '5171', '10.00', '290.00', '376.00', '130.00', '377.00', '5-6-0', '0', '11'], ['37', '4765', '10.00', '290.00', '346.00', '110.00', '319.00', '5-6-0', '0', '11'], ['38', '766', '10.00', '280.00', '302.00', '110.00', '229.00', '5-6-0', '0', '11'], ['39', '2367', '10.00', '270.00', '366.00', '180.00', '244.00', '5-6-0', '0', '11'], ['40', '3669', '10.00', '250.00', '401.00', '120.00', '279.00', '5-6-0', '0', '11'], ['41', '4255', '10.00', '230.00', '310.00', '140.00', '226.00', '5-5-0', '1', '11'], ['42', '5311', '10.00', '120.00', '365.00', '110.00', '180.00', '5-6-0', '0', '11'], ['43', '295', '8.00', '450.00', '340.00', '90.00', '353.00', '4-7-0', '0', '11'], ['44', '100', '8.00', '310.00', '278.00', '210.00', '187.00', '4-7-0', '0', '11'], ['45', '841', '8.00', '290.00', '396.00', '120.00', '217.00', '4-7-0', '0', '11'], ['46', '5023', '8.00', '260.00', '397.00', '200.00', '224.00', '4-7-0', '0', '11'], ['47', '4904', '8.00', '260.00', '302.00', '150.00', '247.00', '4-7-0', '0', '11'], ['48', '2141', '8.00', '210.00', '220.00', '90.00', '156.00', '4-7-0', '0', '11'], ['49', '4171', '6.00', '310.00', '290.00', '70.00', '257.00', '3-8-0', '0', '11'], ['50', '5104', '6.00', '290.00', '375.00', '110.00', '289.00', '3-8-0', '0', '11'], ['51', '2854', '6.00', '280.00', '281.00', '110.00', '223.00', '3-8-0', '0', '11'], ['52', '581', '6.00', '260.00', '286.00', '120.00', '362.00', '3-8-0', '0', '11'], ['53', '115', '6.00', '210.00', '386.00', '160.00', '122.00', '3-8-0', '0', '11'], ['54', '5027', '4.00', '350.00', '361.00', '130.00', '165.00', '2-9-0', '0', '11'], ['55', '5026', '4.00', '260.00', '305.00', '120.00', '136.00', '2-9-0', '0', '11'], ['56', '3045', '4.00', '240.00', '261.00', '200.00', '184.00', '2-9-0', '0', '11'], ['57', '1967', '4.00', '220.00', '476.00', '170.00', '234.00', '2-9-0', '0', '11'], ['58', '4186', '4.00', '130.00', '251.00', '130.00', '170.00', '2-9-0', '0', '11']])

########NEW FILE########
__FILENAME__ = test_usfirst_event_teams_parser
import unittest2

from datafeeds.usfirst_event_teams_parser import UsfirstEventTeamsParser


class TestUsfirstEventTeamsParser(unittest2.TestCase):
    def test_parse(self):
        teams = []
        with open('test_data/usfirst_html/usfirst_event_teams_2012ct-1.html', 'r') as f:
            partial_teams, more_pages1 = UsfirstEventTeamsParser.parse(f.read())
            teams.extend(partial_teams)
        with open('test_data/usfirst_html/usfirst_event_teams_2012ct-2.html', 'r') as f:
            partial_teams, more_pages2 = UsfirstEventTeamsParser.parse(f.read())
            teams.extend(partial_teams)
        with open('test_data/usfirst_html/usfirst_event_teams_2012ct-3.html', 'r') as f:
            partial_teams, more_pages3 = UsfirstEventTeamsParser.parse(f.read())
            teams.extend(partial_teams)

        sort_key = lambda t: t['team_number']
        self.assertEqual(sorted(teams, key=sort_key),
                         sorted([{'first_tpid': 62407, 'team_number': 1124}, {'first_tpid': 61747, 'team_number': 155}, {'first_tpid': 65461, 'team_number': 3634}, {'first_tpid': 62391, 'team_number': 1099}, {'first_tpid': 62339, 'team_number': 999}, {'first_tpid': 62827, 'team_number': 1699}, {'first_tpid': 61763, 'team_number': 173}, {'first_tpid': 61767, 'team_number': 175}, {'first_tpid': 61773, 'team_number': 178}, {'first_tpid': 63343, 'team_number': 2170}, {'first_tpid': 64443, 'team_number': 3146}, {'first_tpid': 63347, 'team_number': 2168}, {'first_tpid': 63209, 'team_number': 2067}, {'first_tpid': 61779, 'team_number': 181}, {'first_tpid': 63169, 'team_number': 1991}, {'first_tpid': 64005, 'team_number': 2785}, {'first_tpid': 62841, 'team_number': 1740}, {'first_tpid': 62963, 'team_number': 1784}, {'first_tpid': 61815, 'team_number': 228}, {'first_tpid': 65459, 'team_number': 3654}, {'first_tpid': 65159, 'team_number': 3718}, {'first_tpid': 62069, 'team_number': 558}, {'first_tpid': 65535, 'team_number': 3719}, {'first_tpid': 61827, 'team_number': 236}, {'first_tpid': 61819, 'team_number': 230}, {'first_tpid': 65145, 'team_number': 3464}, {'first_tpid': 61771, 'team_number': 177}, {'first_tpid': 61789, 'team_number': 195}, {'first_tpid': 64351, 'team_number': 3104}, {'first_tpid': 65045, 'team_number': 3555}, {'first_tpid': 64881, 'team_number': 3461}, {'first_tpid': 65017, 'team_number': 3525}, {'first_tpid': 61829, 'team_number': 237}, {'first_tpid': 64419, 'team_number': 3182}, {'first_tpid': 62077, 'team_number': 571}, {'first_tpid': 61769, 'team_number': 176}, {'first_tpid': 71713, 'team_number': 4055}, {'first_tpid': 62373, 'team_number': 1071}, {'first_tpid': 63913, 'team_number': 2836}, {'first_tpid': 62229, 'team_number': 839}, {'first_tpid': 61723, 'team_number': 126}, {'first_tpid': 64859, 'team_number': 549}, {'first_tpid': 62359, 'team_number': 1027}, {'first_tpid': 62137, 'team_number': 663}, {'first_tpid': 63077, 'team_number': 1922}, {'first_tpid': 62377, 'team_number': 1073}, {'first_tpid': 61685, 'team_number': 95}, {'first_tpid': 64947, 'team_number': 3467}, {'first_tpid': 62257, 'team_number': 869}, {'first_tpid': 62635, 'team_number': 1493}, {'first_tpid': 72789, 'team_number': 4134}, {'first_tpid': 62181, 'team_number': 743}, {'first_tpid': 61615, 'team_number': 20}, {'first_tpid': 61843, 'team_number': 250}, {'first_tpid': 63903, 'team_number': 3017}, {'first_tpid': 62799, 'team_number': 1665}, {'first_tpid': 63885, 'team_number': 2791}, {'first_tpid': 62157, 'team_number': 694}, {'first_tpid': 63063, 'team_number': 1880}, {'first_tpid': 68707, 'team_number': 4122}, {'first_tpid': 62653, 'team_number': 1511}, {'first_tpid': 61817, 'team_number': 229}, {'first_tpid': 62707, 'team_number': 1559}, {'first_tpid': 61711, 'team_number': 118}], key=sort_key))
        self.assertEqual(more_pages1, True)
        self.assertEqual(more_pages2, True)
        self.assertEqual(more_pages3, False)

########NEW FILE########
__FILENAME__ = test_usfirst_event_type_parser
import unittest2

from consts.event_type import EventType
from helpers.event_helper import EventHelper


class TestUsfirstEventTypeParser(unittest2.TestCase):
    def test_parse(self):
        self.assertEqual(EventHelper.parseEventType("Regional"), EventType.REGIONAL)
        self.assertEqual(EventHelper.parseEventType("regional"), EventType.REGIONAL)

        self.assertEqual(EventHelper.parseEventType("District"), EventType.DISTRICT)
        self.assertEqual(EventHelper.parseEventType("district"), EventType.DISTRICT)
        self.assertEqual(EventHelper.parseEventType("MI District"), EventType.DISTRICT)
        self.assertEqual(EventHelper.parseEventType("District Event"), EventType.DISTRICT)
        self.assertEqual(EventHelper.parseEventType("Qualifying Event"), EventType.DISTRICT)
        self.assertEqual(EventHelper.parseEventType("Qualifier"), EventType.DISTRICT)

        self.assertEqual(EventHelper.parseEventType("District Championship"), EventType.DISTRICT_CMP)
        self.assertEqual(EventHelper.parseEventType("MI FRC State Championship"), EventType.DISTRICT_CMP)
        self.assertEqual(EventHelper.parseEventType("Qualifying Championship"), EventType.DISTRICT_CMP)

        self.assertEqual(EventHelper.parseEventType("Championship Division"), EventType.CMP_DIVISION)

        self.assertEqual(EventHelper.parseEventType("Championship Finals"), EventType.CMP_FINALS)
        self.assertEqual(EventHelper.parseEventType("Championship"), EventType.CMP_FINALS)

        self.assertEqual(EventHelper.parseEventType("Offseason"), EventType.OFFSEASON)
        self.assertEqual(EventHelper.parseEventType("Preseason"), EventType.PRESEASON)

        self.assertEqual(EventHelper.parseEventType("Division"), EventType.UNLABLED)

########NEW FILE########
__FILENAME__ = test_usfirst_legacy_event_details_parser
import unittest2
import datetime

from consts.event_type import EventType
from datafeeds.usfirst_legacy_event_details_parser import UsfirstLegacyEventDetailsParser


class TestUsfirstLegacyEventDetailsParser(unittest2.TestCase):
    def test_parse2012ct(self):
        with open('test_data/usfirst_legacy_html/usfirst_event_details_2012ct.html', 'r') as f:
            event, _ = UsfirstLegacyEventDetailsParser.parse(f.read())

        self.assertEqual(event["name"], "Northeast Utilities FIRST Connecticut Regional")
        self.assertEqual(event["short_name"], "Northeast Utilities FIRST Connecticut")
        self.assertEqual(event["event_type_enum"], EventType.REGIONAL)
        self.assertEqual(event["start_date"], datetime.datetime(2012, 3, 29, 0, 0))
        self.assertEqual(event["end_date"], datetime.datetime(2012, 3, 31, 0, 0))
        self.assertEqual(event["year"], 2012)
        self.assertEqual(event["venue_address"], "Connecticut Convention Center\r\n100 Columbus Blvd\r\nHartford, CT 06103\r\nUSA")
        self.assertEqual(event["location"], "Hartford, CT, USA")
        self.assertEqual(event["website"], "http://www.ctfirst.org/ctr")
        self.assertEqual(event["event_short"], "ct")

    def test_parse2013flbr(self):
        with open('test_data/usfirst_legacy_html/usfirst_event_details_2013flbr.html', 'r') as f:
            event, _ = UsfirstLegacyEventDetailsParser.parse(f.read())

        self.assertEqual(event["name"], "South Florida Regional")
        self.assertEqual(event["short_name"], "South Florida")
        self.assertEqual(event["event_type_enum"], EventType.REGIONAL)
        self.assertEqual(event["start_date"], datetime.datetime(2013, 3, 28, 0, 0))
        self.assertEqual(event["end_date"], datetime.datetime(2013, 3, 30, 0, 0))
        self.assertEqual(event["year"], 2013)
        self.assertEqual(event["venue_address"], "Great Fort Lauderdale & Broward County Convention Center\r\n1950 Eisenhower Boulevard\r\nFort Lauderdale, FL 33316\r\nUSA")
        self.assertEqual(event["location"], "Fort Lauderdale, FL, USA")
        self.assertEqual(event["website"], "http://firstinflorida.org")
        self.assertEqual(event["event_short"], "flbr")

    def test_parse2014casj(self):
        with open('test_data/usfirst_legacy_html/usfirst_event_details_2014casj.html', 'r') as f:
            event, _ = UsfirstLegacyEventDetailsParser.parse(f.read())

        self.assertEqual(event["name"], "Silicon Valley Regional")
        self.assertEqual(event["short_name"], "Silicon Valley")
        self.assertEqual(event["event_type_enum"], EventType.REGIONAL)
        self.assertEqual(event["start_date"], datetime.datetime(2014, 4, 3, 0, 0))
        self.assertEqual(event["end_date"], datetime.datetime(2014, 4, 5, 0, 0))
        self.assertEqual(event["year"], 2014)
        self.assertEqual(event["venue_address"], "San Jose State University\r\nThe Event Center\r\nOne Washington Square\r\nSan Jose, CA 95112\r\nUSA")
        self.assertEqual(event["location"], "San Jose, CA, USA")
        self.assertEqual(event["website"], "http://www.firstsv.org")
        self.assertEqual(event["event_short"], "casj")

    def test_parse2014lake(self):
        with open('test_data/usfirst_legacy_html/usfirst_event_details_2014lake.html', 'r') as f:
            event, _ = UsfirstLegacyEventDetailsParser.parse(f.read())

        self.assertEqual(event["name"], "Bayou Regional")
        self.assertEqual(event["short_name"], "Bayou")
        self.assertEqual(event["event_type_enum"], EventType.REGIONAL)
        self.assertEqual(event["start_date"], datetime.datetime(2014, 4, 3, 0, 0))
        self.assertEqual(event["end_date"], datetime.datetime(2014, 4, 5, 0, 0))
        self.assertEqual(event["year"], 2014)
        self.assertEqual(event["website"], "http://www.frcbayouregional.com")
        self.assertEqual(event["event_short"], "lake")

    def test_parse2014nvlv_preliminary(self):
        with open('test_data/usfirst_legacy_html/usfirst_event_details_2014nvlv_preliminary.html', 'r') as f:
            event, _ = UsfirstLegacyEventDetailsParser.parse(f.read())

        self.assertEqual(event["name"], "Las Vegas Regional - Preliminary")
        self.assertEqual(event["short_name"], "Las Vegas")
        self.assertEqual(event["event_type_enum"], EventType.REGIONAL)
        self.assertEqual(event["start_date"], datetime.datetime(2014, 12, 31, 0, 0))
        self.assertEqual(event["end_date"], datetime.datetime(2014, 12, 31, 0, 0))
        self.assertEqual(event["year"], 2014)
        self.assertEqual(event["website"], "http://www.firstnv.org")
        self.assertEqual(event["event_short"], "nvlv")

########NEW FILE########
__FILENAME__ = test_usfirst_legacy_event_teams_parser
import unittest2

from datafeeds.usfirst_legacy_event_teams_parser import UsfirstLegacyEventTeamsParser


class TestUsfirstLegacyEventTeamsParser(unittest2.TestCase):
    def test_parse(self):
        with open('test_data/usfirst_legacy_html/usfirst_event_teams_2012ct.html', 'r') as f:
            teams, _ = UsfirstLegacyEventTeamsParser.parse(f.read())

        self.assertEqual(teams, [{'first_tpid': 62407, 'team_number': 1124}, {'first_tpid': 61747, 'team_number': 155}, {'first_tpid': 65461, 'team_number': 3634}, {'first_tpid': 62391, 'team_number': 1099}, {'first_tpid': 62339, 'team_number': 999}, {'first_tpid': 62827, 'team_number': 1699}, {'first_tpid': 61763, 'team_number': 173}, {'first_tpid': 61767, 'team_number': 175}, {'first_tpid': 61773, 'team_number': 178}, {'first_tpid': 63343, 'team_number': 2170}, {'first_tpid': 64443, 'team_number': 3146}, {'first_tpid': 63347, 'team_number': 2168}, {'first_tpid': 63209, 'team_number': 2067}, {'first_tpid': 61779, 'team_number': 181}, {'first_tpid': 63169, 'team_number': 1991}, {'first_tpid': 64005, 'team_number': 2785}, {'first_tpid': 62841, 'team_number': 1740}, {'first_tpid': 62963, 'team_number': 1784}, {'first_tpid': 61815, 'team_number': 228}, {'first_tpid': 65459, 'team_number': 3654}, {'first_tpid': 65159, 'team_number': 3718}, {'first_tpid': 62069, 'team_number': 558}, {'first_tpid': 65535, 'team_number': 3719}, {'first_tpid': 61827, 'team_number': 236}, {'first_tpid': 61819, 'team_number': 230}, {'first_tpid': 65145, 'team_number': 3464}, {'first_tpid': 61771, 'team_number': 177}, {'first_tpid': 61789, 'team_number': 195}, {'first_tpid': 64351, 'team_number': 3104}, {'first_tpid': 65045, 'team_number': 3555}, {'first_tpid': 64881, 'team_number': 3461}, {'first_tpid': 65017, 'team_number': 3525}, {'first_tpid': 61829, 'team_number': 237}, {'first_tpid': 64419, 'team_number': 3182}, {'first_tpid': 62077, 'team_number': 571}, {'first_tpid': 61769, 'team_number': 176}, {'first_tpid': 71713, 'team_number': 4055}, {'first_tpid': 62373, 'team_number': 1071}, {'first_tpid': 63913, 'team_number': 2836}, {'first_tpid': 62229, 'team_number': 839}, {'first_tpid': 61723, 'team_number': 126}, {'first_tpid': 64859, 'team_number': 549}, {'first_tpid': 62359, 'team_number': 1027}, {'first_tpid': 62137, 'team_number': 663}, {'first_tpid': 63077, 'team_number': 1922}, {'first_tpid': 62377, 'team_number': 1073}, {'first_tpid': 61685, 'team_number': 95}, {'first_tpid': 64947, 'team_number': 3467}, {'first_tpid': 62257, 'team_number': 869}, {'first_tpid': 62635, 'team_number': 1493}, {'first_tpid': 72789, 'team_number': 4134}, {'first_tpid': 62181, 'team_number': 743}, {'first_tpid': 61615, 'team_number': 20}, {'first_tpid': 61843, 'team_number': 250}, {'first_tpid': 63903, 'team_number': 3017}, {'first_tpid': 62799, 'team_number': 1665}, {'first_tpid': 63885, 'team_number': 2791}, {'first_tpid': 62157, 'team_number': 694}, {'first_tpid': 63063, 'team_number': 1880}, {'first_tpid': 68707, 'team_number': 4122}, {'first_tpid': 62653, 'team_number': 1511}, {'first_tpid': 61817, 'team_number': 229}, {'first_tpid': 62707, 'team_number': 1559}, {'first_tpid': 61711, 'team_number': 118}])

########NEW FILE########
__FILENAME__ = test_usfirst_legacy_team_details_parser
import unittest2

from datafeeds.usfirst_legacy_team_details_parser import UsfirstLegacyTeamDetailsParser


class TestUsfirstLegacyTeamDetailsParser(unittest2.TestCase):
    def test_parse(self):
        with open('test_data/usfirst_legacy_html/usfirst_team_details_frc177_2012.html', 'r') as f:
            team, _ = UsfirstLegacyTeamDetailsParser.parse(f.read())

        self.assertEqual(team["address"], u"South Windsor, CT\xa0 USA")
        self.assertEqual(team["name"], "UTC Power/Ensign Bickford Aerospace & Defense & South Windsor High School")
        self.assertEqual(team["nickname"], "Bobcat Robotics")
        self.assertEqual(team["team_number"], 177)
        self.assertEqual(team["rookie_year"], 1995)
        self.assertEqual(team["website"], "http://www.bobcatrobotics.org")

########NEW FILE########
__FILENAME__ = test_usfirst_matches_parser
import unittest2

from datafeeds.usfirst_matches_parser import UsfirstMatchesParser


class TestUsfirstMatchesParser(unittest2.TestCase):
    def test_parse_2012ct(self):
        with open('test_data/usfirst_html/usfirst_event_matches_2012ct.html', 'r') as f:
            matches, _ = UsfirstMatchesParser.parse(f.read())

        # Test 2012ct_qm1
        match = matches[0]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 1)
        self.assertEqual(match["team_key_names"], [u'frc1073', u'frc549', u'frc1991', u'frc999', u'frc3464', u'frc126'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": 31, "teams": ["frc999", "frc3464", "frc126"]}, "red": {"score": 37, "teams": ["frc1073", "frc549", "frc1991"]}}""")
        self.assertEqual(match["time_string"], "9:00 AM")

        # Test 2012ct_sf2m1
        match = matches[-7]
        self.assertEqual(match["comp_level"], "sf")
        self.assertEqual(match["set_number"], 2)
        self.assertEqual(match["match_number"], 1)
        self.assertEqual(match["team_key_names"], [u'frc177', u'frc228', u'frc236', u'frc20', u'frc181', u'frc195'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": 76, "teams": ["frc20", "frc181", "frc195"]}, "red": {"score": 41, "teams": ["frc177", "frc228", "frc236"]}}""")
        self.assertEqual(match["time_string"], "2:51 PM")

        # Test 2012ct_f1m3
        match = matches[-1]
        self.assertEqual(match["comp_level"], "f")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 3)
        self.assertEqual(match["team_key_names"], [u'frc1071', u'frc558', u'frc2067', u'frc195', u'frc181', u'frc20'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": 62, "teams": ["frc195", "frc181", "frc20"]}, "red": {"score": 39, "teams": ["frc1071", "frc558", "frc2067"]}}""")
        self.assertEqual(match["time_string"], "4:05 PM")

    def test_parse_2013cama(self):
        with open('test_data/usfirst_html/usfirst_event_matches_2013cama.html', 'r') as f:
            matches, _ = UsfirstMatchesParser.parse(f.read())

        # Test 2013cama_qm1
        match = matches[0]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 1)
        self.assertEqual(match["team_key_names"], [u'frc3256', u'frc2135', u'frc1323', u'frc1678', u'frc4135', u'frc3501'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": 58, "teams": ["frc1678", "frc4135", "frc3501"]}, "red": {"score": 5, "teams": ["frc3256", "frc2135", "frc1323"]}}""")
        self.assertEqual(match["time_string"], "9:00 AM")

        # Test 2013cama_f1m3
        match = matches[-1]
        self.assertEqual(match["comp_level"], "f")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 3)
        self.assertEqual(match["team_key_names"], [u'frc2643', u'frc3501', u'frc3970', u'frc295', u'frc840', u'frc1678'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": 110, "teams": ["frc295", "frc840", "frc1678"]}, "red": {"score": 74, "teams": ["frc2643", "frc3501", "frc3970"]}}""")
        self.assertEqual(match["time_string"], "4:04 PM")

    def test_parse_2013casd(self):
        with open('test_data/usfirst_html/usfirst_event_matches_2013casd.html', 'r') as f:
            matches, _ = UsfirstMatchesParser.parse(f.read())

        # Test 2013casd_qm1
        match = matches[0]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 1)
        self.assertEqual(match["team_key_names"], [u'frc3480', u'frc1159', u'frc2496', u'frc3453', u'frc702', u'frc4161'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": 0, "teams": ["frc3453", "frc702", "frc4161"]}, "red": {"score": 32, "teams": ["frc3480", "frc1159", "frc2496"]}}""")
        self.assertEqual(match["time_string"], "9:00 AM")

    def test_parse_2013pahat_incomplete(self):
        with open('test_data/usfirst_html/usfirst_event_matches_2013pahat_incomplete.html', 'r') as f:
            matches, _ = UsfirstMatchesParser.parse(f.read())

        # Test 2013pahat_qm1, played match
        match = matches[0]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 1)
        self.assertEqual(match["team_key_names"], [u'frc4342', u'frc3151', u'frc1647', u'frc816', u'frc3974', u'frc3123'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": 66, "teams": ["frc816", "frc3974", "frc3123"]}, "red": {"score": 35, "teams": ["frc4342", "frc3151", "frc1647"]}}""")
        self.assertEqual(match["time_string"], "11:30 AM")

        # Test 2013pahat_qm37, unplayed match
        match = matches[36]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 37)
        self.assertEqual(match["team_key_names"], [u'frc1143', u'frc2729', u'frc3123', u'frc87', u'frc304', u'frc1495'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": -1, "teams": ["frc87", "frc304", "frc1495"]}, "red": {"score": -1, "teams": ["frc1143", "frc2729", "frc3123"]}}""")
        self.assertEqual(match["time_string"], "5:32 PM")

    def test_parse_2014test(self):
        with open('test_data/usfirst_html/usfirst_event_matches_2014test.html', 'r') as f:
            matches, _ = UsfirstMatchesParser.parse(f.read())

        # Test 2014test_qm1, played match
        match = matches[0]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 1)
        self.assertEqual(match["team_key_names"], [u'frc7', u'frc9', u'frc12', u'frc3', u'frc2', u'frc11'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": 225, "teams": ["frc3", "frc2", "frc11"]}, "red": {"score": 45, "teams": ["frc7", "frc9", "frc12"]}}""")
        self.assertEqual(match["time_string"], "4:00 PM")

        # Test 2014test_qm16, unplayed match
        match = matches[15]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 16)
        self.assertEqual(match["team_key_names"], [u'frc9', u'frc2', u'frc1', u'frc13', u'frc3', u'frc10'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": -1, "teams": ["frc13", "frc3", "frc10"]}, "red": {"score": -1, "teams": ["frc9", "frc2", "frc1"]}}""")
        self.assertEqual(match["time_string"], "8:45 PM")

########NEW FILE########
__FILENAME__ = test_usfirst_matches_parser_2002
import unittest2
import json

from datafeeds.usfirst_matches_parser_2002 import UsfirstMatchesParser2002


class TestUsfirstMatchesParser2002(unittest2.TestCase):
    def test_parse_2002sj(self):
        with open('test_data/usfirst_html/usfirst_event_matches_2002sj.html', 'r') as f:
            matches, _ = UsfirstMatchesParser2002.parse(f.read())

        self.assertEqual(len(matches), 128)

        # Test 2002sj_qm1
        match = matches[0]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 1)
        self.assertEqual(match["team_key_names"], [u'frc814', u'frc604', u'frc295', u'frc254'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"blue": {"score": 28, "teams": ["frc295", "frc254"]}, "red": {"score": 10, "teams": ["frc814", "frc604"]}}"""))

        # Test 2002sj_qm5
        match = matches[4]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 5)
        self.assertEqual(match["team_key_names"], [u'frc609', u'frc852', u'frc359', u'frc258'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"blue": {"score": 27, "teams": ["frc359", "frc258"]}, "red": {"score": 40, "teams": ["frc609", "frc852"]}}"""))

        # Test 2002sj_qm111
        match = matches[110]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 111)
        self.assertEqual(match["team_key_names"], [u'frc841', u'frc100', u'frc254', u'frc524'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"blue": {"score": 40, "teams": ["frc254", "frc524"]}, "red": {"score": 37, "teams": ["frc841", "frc100"]}}"""))

        # Test 2002sj_qf1m1
        match = matches[111]
        self.assertEqual(match["comp_level"], "qf")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 1)
        self.assertEqual(match["team_key_names"], [u'frc254', u'frc60', u'frc359', u'frc298', u'frc368', u'frc409'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"blue": {"score": 10, "teams": ["frc298", "frc368", "frc409"]}, "red": {"score": 49, "teams": ["frc254", "frc60", "frc359"]}}"""))

        # Test 2002sj_sf2m2
        match = matches[-3]
        self.assertEqual(match["comp_level"], "sf")
        self.assertEqual(match["set_number"], 2)
        self.assertEqual(match["match_number"], 2)
        self.assertEqual(match["team_key_names"], [u'frc8', u'frc609', u'frc100', u'frc701', u'frc376', u'frc115'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"blue": {"score": 20, "teams": ["frc701", "frc376", "frc115"]}, "red": {"score": 13, "teams": ["frc8", "frc609", "frc100"]}}"""))

        # Test 2002sj_f1m2
        match = matches[-1]
        self.assertEqual(match["comp_level"], "f")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 2)
        self.assertEqual(match["team_key_names"], [u'frc254', u'frc60', u'frc359', u'frc701', u'frc376', u'frc115'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"blue": {"score": 20, "teams": ["frc701", "frc376", "frc115"]}, "red": {"score": 41, "teams": ["frc254", "frc60", "frc359"]}}"""))

    def test_parse_2002tx(self):
        with open('test_data/usfirst_html/usfirst_event_matches_2002tx.html', 'r') as f:
            matches, _ = UsfirstMatchesParser2002.parse(f.read())

        self.assertEqual(len(matches), 111)

        # Test 2002tx_qm11
        match = matches[10]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 11)
        self.assertEqual(match["team_key_names"], [u'frc317', u'frc442', u'frc624', u'frc922'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"blue": {"score": 31, "teams": ["frc624", "frc922"]}, "red": {"score": 32, "teams": ["frc317", "frc442"]}}"""))

        # Test 2002tx_qm52
        match = matches[51]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 52)
        self.assertEqual(match["team_key_names"], [u'frc231', u'frc356', u'frc499', u'frc908'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"blue": {"score": 39, "teams": ["frc499", "frc908"]}, "red": {"score": 32, "teams": ["frc231", "frc356"]}}"""))

        # Test 2002sj_qf2m1
        match = matches[-12]
        self.assertEqual(match["comp_level"], "qf")
        self.assertEqual(match["set_number"], 2)
        self.assertEqual(match["match_number"], 1)
        self.assertEqual(match["team_key_names"], [u'frc57', u'frc357', u'frc481', u'frc317', u'frc624', u'frc704'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"blue": {"score": 20, "teams": ["frc317", "frc624", "frc704"]}, "red": {"score": 35, "teams": ["frc57", "frc357", "frc481"]}}"""))

        # Test 2002sj_sf2m2
        match = matches[-3]
        self.assertEqual(match["comp_level"], "sf")
        self.assertEqual(match["set_number"], 2)
        self.assertEqual(match["match_number"], 2)
        self.assertEqual(match["team_key_names"], [u'frc635', u'frc476', u'frc437', u'frc34', u'frc457', u'frc192'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"blue": {"score": 35, "teams": ["frc34", "frc457", "frc192"]}, "red": {"score": 20, "teams": ["frc635", "frc476", "frc437"]}}"""))

        # Test 2002sj_f1m2
        match = matches[-1]
        self.assertEqual(match["comp_level"], "f")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 2)
        self.assertEqual(match["team_key_names"], [u'frc16', u'frc118', u'frc609', u'frc34', u'frc457', u'frc192'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"blue": {"score": 25, "teams": ["frc34", "frc457", "frc192"]}, "red": {"score": 11, "teams": ["frc16", "frc118", "frc609"]}}"""))

    def test_parse_2002va(self):
        with open('test_data/usfirst_html/usfirst_event_matches_2002va.html', 'r') as f:
            matches, _ = UsfirstMatchesParser2002.parse(f.read())

        self.assertEqual(len(matches), 132)

        # Test 2002va_qm2
        match = matches[1]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 2)
        self.assertEqual(match["team_key_names"], [u'frc602', u'frc900', u'frc975', u'frc769'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"blue": {"teams": ["frc975", "frc769"], "score": 31}, "red": {"teams": ["frc602", "frc900"], "score": 21}}"""))

        # Test 2002va_qm16
        match = matches[15]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 16)
        self.assertEqual(match["team_key_names"], [u'frc837', u'frc587', u'frc825', u'frc345'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"blue": {"teams": ["frc825", "frc345"], "score": 35}, "red": {"teams": ["frc837", "frc587"], "score": 35}}"""))

        # Test 2002va_qm132
        match = matches[-1]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 132)
        self.assertEqual(match["team_key_names"], [u'frc414', u'frc837', u'frc400', u'frc53'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"blue": {"teams": ["frc400", "frc53"], "score": 40}, "red": {"teams": ["frc414", "frc837"], "score": 11}}"""))

########NEW FILE########
__FILENAME__ = test_usfirst_matches_parser_2003
import unittest2
import json

from datafeeds.usfirst_matches_parser_2003 import UsfirstMatchesParser2003


class TestUsfirstMatchesParser2003(unittest2.TestCase):
    def test_parse_2003sj(self):
        with open('test_data/usfirst_html/usfirst_event_matches_2003sj.html', 'r') as f:
            matches, _ = UsfirstMatchesParser2003.parse(f.read())

        self.assertEqual(len(matches), 118)

        # Test 2003sj_qm1
        match = matches[14]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 1)
        self.assertEqual(match["team_key_names"], [u'frc973', u'frc359', u'frc632', u'frc480'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"red": {"teams": ["frc973", "frc359"], "score": 14}, "blue": {"teams": ["frc632", "frc480"], "score": 63}}"""))

        # Test 2003sj_qm5
        match = matches[18]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 5)
        self.assertEqual(match["team_key_names"], [u'frc159', u'frc376', u'frc1043', u'frc814'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"red": {"teams": ["frc159", "frc376"], "score": 31}, "blue": {"teams": ["frc1043", "frc814"], "score": 122}}"""))

        # Test 2003sj_qm104
        match = matches[-1]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 104)
        self.assertEqual(match["team_key_names"], [u'frc605', u'frc159', u'frc192', u'frc766'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"red": {"teams": ["frc605", "frc159"], "score": 42}, "blue": {"teams": ["frc192", "frc766"], "score": 70}}"""))

        # Test 2003sj_qf1m1
        match = matches[2]
        self.assertEqual(match["comp_level"], "qf")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 1)
        self.assertEqual(match["team_key_names"], [u'frc368', u'frc359', u'frc691', u'frc114'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"red": {"teams": ["frc368", "frc359"], "score": 33}, "blue": {"teams": ["frc691", "frc114"], "score": 7}}"""))

        # Test 2003sj_sf2m2
        match = matches[13]
        self.assertEqual(match["comp_level"], "sf")
        self.assertEqual(match["set_number"], 2)
        self.assertEqual(match["match_number"], 2)
        self.assertEqual(match["team_key_names"], [u'frc8', u'frc492', u'frc115', u'frc254'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"red": {"teams": ["frc8", "frc492"], "score": 26}, "blue": {"teams": ["frc115", "frc254"], "score": 53}}"""))

        # Test 2003sj_f1m2
        match = matches[1]
        self.assertEqual(match["comp_level"], "f")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 2)
        self.assertEqual(match["team_key_names"], [u'frc368', u'frc359', u'frc115', u'frc254'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"red": {"teams": ["frc368", "frc359"], "score": 50}, "blue": {"teams": ["frc115", "frc254"], "score": 6}}"""))

    def test_parse_2003cmp(self):
        with open('test_data/usfirst_html/usfirst_event_matches_2003cmp.html', 'r') as f:
            matches, _ = UsfirstMatchesParser2003.parse(f.read())

        self.assertEqual(len(matches), 6)

        # Test 2003cmp_sf2m2
        match = matches[-1]
        self.assertEqual(match["comp_level"], "sf")
        self.assertEqual(match["set_number"], 2)
        self.assertEqual(match["match_number"], 2)
        self.assertEqual(match["team_key_names"], [u'frc343', u'frc25', u'frc292', u'frc302'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"blue": {"teams": ["frc292", "frc302"], "score": 1}, "red": {"teams": ["frc343", "frc25"], "score": 43}}"""))

        # Test 2003cmp_f1m2
        match = matches[1]
        self.assertEqual(match["comp_level"], "f")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 2)
        self.assertEqual(match["team_key_names"], [u'frc111', u'frc65', u'frc343', u'frc25'])
        self.assertEqual(json.loads(match["alliances_json"]), json.loads("""{"blue": {"teams": ["frc343", "frc25"], "score": 58}, "red": {"teams": ["frc111", "frc65"], "score": 10}}"""))

########NEW FILE########
__FILENAME__ = test_usfirst_match_schedule_parser
import unittest2

from datafeeds.usfirst_match_schedule_parser import UsfirstMatchScheduleParser


class TestUsfirstMatchScheduleParser(unittest2.TestCase):
    def test_parse_2013casj_qual(self):
        with open('test_data/usfirst_html/usfirst_event_match_schedule_2013casj_qual.html', 'r') as f:
            matches, _ = UsfirstMatchScheduleParser.parse(f.read())

        self.assertEqual(len(matches), 99)

        # Test 2013casj_qm1
        match = matches[0]
        self.assertEqual(match["comp_level"], "qm")
        self.assertEqual(match["set_number"], 1)
        self.assertEqual(match["match_number"], 1)
        self.assertEqual(match["team_key_names"], [u'frc254', u'frc295', u'frc2489', u'frc1388', u'frc3482', u'frc649'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": -1, "teams": ["frc1388", "frc3482", "frc649"]}, "red": {"score": -1, "teams": ["frc254", "frc295", "frc2489"]}}""")
        self.assertEqual(match["time_string"], "9:00 AM")

    def test_parse_2013casj_elim(self):
        with open('test_data/usfirst_html/usfirst_event_match_schedule_2013casj_elim.html', 'r') as f:
            matches, _ = UsfirstMatchScheduleParser.parse(f.read())

        self.assertEqual(len(matches), 12)

        # Test 2013casj_qf4-3
        match = matches[-1]
        self.assertEqual(match["comp_level"], "qf")
        self.assertEqual(match["set_number"], 4)
        self.assertEqual(match["match_number"], 3)
        self.assertEqual(match["team_key_names"], [u'frc192', u'frc604', u'frc971', u'frc649', u'frc114', u'frc1388'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": -1, "teams": ["frc649", "frc114", "frc1388"]}, "red": {"score": -1, "teams": ["frc192", "frc604", "frc971"]}}""")
        self.assertEqual(match["time_string"], "2:58 PM")

    def test_parse_2014cmp_elim(self):
        with open('test_data/usfirst_html/usfirst_event_match_schedule_2014cmp_elim.html', 'r') as f:
            matches, _ = UsfirstMatchScheduleParser.parse(f.read())

        self.assertEqual(len(matches), 7)

        # Test 2014cmp_sf2-2
        match = matches[3]
        self.assertEqual(match["comp_level"], "sf")
        self.assertEqual(match["set_number"], 2)
        self.assertEqual(match["match_number"], 2)
        self.assertEqual(match["team_key_names"], [u'frc16', u'frc5', u'frc9', u'frc2', u'frc40', u'frc1'])
        self.assertEqual(match["alliances_json"], """{"blue": {"score": -1, "teams": ["frc2", "frc40", "frc1"]}, "red": {"score": -1, "teams": ["frc16", "frc5", "frc9"]}}""")
        self.assertEqual(match["time_string"], "5:29 PM")

########NEW FILE########
__FILENAME__ = test_usfirst_team_details_parser
import unittest2

from datafeeds.usfirst_team_details_parser import UsfirstTeamDetailsParser


class TestUsfirstTeamDetailsParser(unittest2.TestCase):
    def test_parse_team_not_found(self):
        with open('test_data/usfirst_html/usfirst_team_details_team_not_found.html', 'r') as f:
            team, _ = UsfirstTeamDetailsParser.parse(f.read())

        self.assertEqual(team, None)

    def test_parse_frc254_2014(self):
        with open('test_data/usfirst_html/usfirst_team_details_frc254_2014.html', 'r') as f:
           team, _ = UsfirstTeamDetailsParser.parse(f.read())

        self.assertEqual(team["address"], u"San Jose, CA, USA")
        self.assertEqual(team["name"], "NASA Ames Research Center / Lockheed Martin / The Mercadante Family / Ooyala / TR Manufacturing / Qualcomm / HP / West Coast Products / The Magarelli Family / The Yun Family / Google / Modern Machine / The Gebhart Family / Aditazz / Cisco Meraki / Vivid-Hosting / Nvidia / BAE Systems / Gilbert Spray Coat / Pacific Coast Metal / S&S Welding / Good Plastics / Team Whyachi / Hy-Tech Plating / Applied Welding / World Metal Finishing / The Jimenez Family & Bellarmine College Preparatory")
        self.assertEqual(team["nickname"], "The Cheesy Poofs")
        self.assertEqual(team["team_number"], 254)
        self.assertEqual(team["website"], "http://www.team254.com")

    def test_parse_frc842_2014(self):
        with open('test_data/usfirst_html/usfirst_team_details_frc842_2014.html', 'r') as f:
           team, _ = UsfirstTeamDetailsParser.parse(f.read())

        self.assertEqual(team["address"], u"Phoenix, AZ, USA")
        self.assertEqual(team["name"], "The Boeing Company/DLR Group/Fast Signs/Southwest Fasteners & Carl Hayden High School")
        self.assertEqual(team["nickname"], "Falcon Robotics")
        self.assertEqual(team["team_number"], 842)
        self.assertEqual(team["website"], "https://sites.google.com/site/falconroboticsfrcteam842/frc-robots/2014-dream")

    def test_parse_frc177_2013(self):
        with open('test_data/usfirst_html/usfirst_team_details_frc177_2013.html', 'r') as f:
            team, _ = UsfirstTeamDetailsParser.parse(f.read())

        self.assertEqual(team["address"], u"South Windsor, CT, USA")
        self.assertEqual(team["name"], "UTC Power/Ensign Bickford Aerospace & Defense & South Windsor High School")
        self.assertEqual(team["nickname"], "Bobcat Robotics")
        self.assertEqual(team["team_number"], 177)
        self.assertEqual(team["website"], "http://www.bobcatrobotics.org")

    def test_parse_frc1114_2013(self):
        with open('test_data/usfirst_html/usfirst_team_details_frc1114_2013.html', 'r') as f:
            team, _ = UsfirstTeamDetailsParser.parse(f.read())

        self.assertEqual(team["address"], u"St. Catharines, ON, Canada")
        self.assertEqual(team["name"], "Innovation First International/General Motors St. Catharines Powertrain & Simbotics")
        self.assertEqual(team["nickname"], "Simbotics")
        self.assertEqual(team["team_number"], 1114)
        self.assertEqual(team["website"], "http://www.simbotics.org")

    def test_parse_frc4590_2013(self):
        with open('test_data/usfirst_html/usfirst_team_details_frc4590_2013.html', 'r') as f:
            team, _ = UsfirstTeamDetailsParser.parse(f.read())

        self.assertEqual(team["address"], u"Kfar Hayarok, TA, Israel")
        self.assertEqual(team["name"], "Hakfar Hayarok")
        self.assertEqual(team["nickname"], "Greenblitz")
        self.assertEqual(team["team_number"], 4590)

    def test_parse_frc4756_2013(self):
        with open('test_data/usfirst_html/usfirst_team_details_frc4756_2013.html', 'r') as f:
            team, _ = UsfirstTeamDetailsParser.parse(f.read())

        self.assertEqual(team["name"], "aaaaaa")
        self.assertEqual(team["nickname"], "wgogfom3")
        self.assertEqual(team["team_number"], 4756)

    def test_parse_frc1309_2004(self):
        with open('test_data/usfirst_html/usfirst_team_details_frc1309_2004.html', 'r') as f:
            team, _ = UsfirstTeamDetailsParser.parse(f.read())

        self.assertEqual(team["address"], u"Toronto, ON, Canada")
        self.assertEqual(team["name"], "Toronto District School Board & Emery Collegiate Institute")
        self.assertEqual(team["nickname"], "Diamond Eagles")
        self.assertEqual(team["team_number"], 1309)

########NEW FILE########
__FILENAME__ = test_validation_helper
import unittest2

from helpers.validation_helper import ValidationHelper

class TestValidationHelper(unittest2.TestCase):

    def testTeamValidation(self):
        errors = ValidationHelper.validate([("team_id_validator", "frc01")])
        self.assertEqual(errors, {"Errors": [{"team_id": "frc01 is not a valid team id"}]})

    def testEventValidation(self):
        errors = ValidationHelper.validate([("event_id_validator", "1cmp")])
        self.assertEqual(errors, {"Errors": [{"event_id": "1cmp is not a valid event id"}]})

    def testMatchValidation(self):
        errors = ValidationHelper.validate([("match_id_validator", "0010c1_0m2")])
        self.assertEqual(errors, {"Errors": [{"match_id": "0010c1_0m2 is not a valid match id"}]})

    def testComboValidation(self):
        errors = ValidationHelper.validate([("match_id_validator", "0010c1_0m2"),
            ("team_id_validator", "frc01"),
            ("event_id_validator", "1cmp")])
        self.assertEqual(errors, {"Errors": [{"match_id": "0010c1_0m2 is not a valid match id"}, {"team_id": "frc01 is not a valid team id"},{"event_id": "1cmp is not a valid event id"}]})

    def testValidValidation(self):
        errors = ValidationHelper.validate([("team_id_validator", "frc101")])
        self.assertEqual(None, errors)






########NEW FILE########
__FILENAME__ = three2four_match
# three2four_match.py
#
# Converts a CSV export of TBAv3 SQL Matches into the read CSV format for
# TBAv4 on Google App Engine. Sets no_auto_update to true.
#
# Incoming CSV format:
# event.year, event.eventshort, complevel, matchnumber, red1, red2, red3, blue1, blue2, blue3, redscore, bluescore
#
# python three2four_match.py -i old_matches.csv -o new_matches.csv
#
# -gregmarra 6 Sept 2010

import csv
import logging
import math
from optparse import OptionParser

COMPLEVELS = {
    10: "qm",
    20: "ef",
    30: "qf",
    40: "sf",
    50: "f"
}

FRC_GAMES_BY_YEAR = {
    2010: "frc_2010_bkwy",
    2009: "frc_2009_lncy",
    2008: "frc_2008_ovdr",
    2007: "frc_2007_rkrl",
    2006: "frc_2006_amhi",
    2005: "frc_2005_trpl",
    2004: "frc_2004_frnz",
    2003: "frc_2003_stck",
    2002: "frc_2002_znzl",
    2001: "frc_2001_dbdy",
    2000: "frc_2000_coop",
    1999: "frc_1999_trbl",
    1998: "frc_1998_lddr",
    1997: "frc_1997_trdt",
    1996: "frc_1996_hxgn",
    1995: "frc_1995_rmpr",
    1994: "frc_1994_tpwr",
    1993: "frc_1993_rgrg",
    1992: "frc_1992_maiz"
}


def parse_row(row):
    """Parse a row into a nice dictionary."""
    old_match = dict()
    old_match["year"] = row[0]
    old_match["eventshort"] = row[1]
    old_match["complevel"] = row[2]
    old_match["matchnumber"] = row[3]
    old_match["red1"] = row[4]
    old_match["red2"] = row[5]
    old_match["red3"] = row[6]
    old_match["blue1"] = row[7]
    old_match["blue2"] = row[8]
    old_match["blue3"] = row[9]
    old_match["redscore"] = row[10]
    old_match["bluescore"] = row[11]
    return old_match


def legal_teams(teams):
    """Return a list of teams that are possible."""
    good_teams = list()
    for team in teams:
        try:
            team_number = int(team)
            if team_number > 0:
                good_teams.append(team_number)
        except ValueError:
            logging.warning(str(team) + " is not a valid team number.")

    good_teams = ["frc" + str(team) for team in good_teams]
    return good_teams


def build_new_match(old_match):
    """Build a new match."""
    match = dict()
    match["event"] = old_match["year"] + old_match["eventshort"]
    try:
        match["comp_level"] = COMPLEVELS[int(old_match["complevel"])]
    except Exception as e:
        logging.error("Bad comp_level: " + str(old_match["complevel"]))
        return None
    match["game"] = FRC_GAMES_BY_YEAR[int(old_match["year"])]

    red_teams = legal_teams([old_match["red1"], old_match["red2"], old_match["red3"]])
    blue_teams = legal_teams([old_match["blue1"], old_match["blue2"], old_match["blue3"]])

    match["alliances_json"] = {
        "red": {
            "teams": red_teams,
            "score": old_match["redscore"]
        },
        "blue": {
            "teams": blue_teams,
            "score": old_match["bluescore"]
        }
    }
    match["team_key_names"] = red_teams + blue_teams

    if match["comp_level"] == "qm":
        match["match_number"] = old_match["matchnumber"]
        match["set_number"] = 1
        match["key"] = match["event"] + "_qm" + match["match_number"]
    else:
        match["match_number"] = str(int(old_match["matchnumber"]) % 10)
        match["set_number"] = str(int(math.floor(int(old_match["matchnumber"]) / 10.0)))
        match["key"] = match["event"] + "_" + match["comp_level"] + match["set_number"] + "m" + match["match_number"]

    match["no_auto_update"] = "TRUE"

    return match


def main():
    parser = OptionParser()
    parser.add_option("-i", "--input_file", type="string",
                      help="Input TBA SQL export CSV")
    parser.add_option("-o", "--output_file", type="string",
                      help="Output CSV")
    options, args = parser.parse_args()

    matches = list()

    matchReader = csv.reader(open(options.input_file, 'rb'), delimiter=',')
    for row in matchReader:
        old_match = parse_row(row)
        new_match = build_new_match(old_match)
        if new_match:
            matches.append(new_match)

    matchWriter = csv.writer(open(options.output_file, 'wb'), delimiter=',',
                             quotechar='"', quoting=csv.QUOTE_MINIMAL)
    matchWriter.writerow(["key", "event", "game", "comp_level", "set_number", "match_number", "team_key_names", "alliances_json", "no_auto_update"])
    for match in matches:
        matchWriter.writerow([match["key"], match["event"], match["game"], match["comp_level"], match["set_number"], match["match_number"], match["team_key_names"], match["alliances_json"], match["no_auto_update"]])

if __name__ == "__main__":
    main()

########NEW FILE########
