__FILENAME__ = bootstrap
# Initialise a newly-created db with required tables, users,
# categories and tags.
from r2.lib.db.thing import NotFound
from r2.models.account import Account, AccountExists, register
from r2.models.link import Tag, TagExists
from r2.models.subreddit import Subreddit

try:
    register('admin', 'swordfish', '')
except AccountExists:
    pass

admin = Account._by_name('admin')
admin.email_validated = True
admin._commit()

try:
    Subreddit._by_name('lesswrong')
except NotFound:
    Subreddit._create_and_subscribe('lesswrong', admin,
                                    { 'title': 'Less Wrong',
                                      'type': 'restricted',
                                      'default_listing': 'blessed' })

try:
    Subreddit._by_name('discussion')
except NotFound:
    s = Subreddit._create_and_subscribe('discussion', admin,
                                        { 'title': 'Less Wrong Discussion',
                                          'type': 'public',
                                          'default_listing': 'new' })
    s.header = "/static/logo-discussion.png"
    s.stylesheet = "/static/discussion.css"
    s.infotext = u"""You're looking at Less Wrong's discussion board. This includes
                     all posts, including those that haven't been promoted to the front
                     page yet. For more information, see [About Less Wrong](/about-less-wrong)."""

    s.posts_per_page_multiplier = 4
    s.post_karma_multiplier = 1
    s._commit()

try:
    Subreddit._by_name('meetups')
except NotFound:
    s = Subreddit._create_and_subscribe('meetups', admin,
                                        { 'title': 'Less Wrong Meetups',
                                          'type': 'restricted',
                                          'default_listing': 'new' })

    s.posts_per_page_multiplier = 4
    s.post_karma_multiplier = 1
    s._commit()


tags = ['group_rationality_diary', 'open_thread', 'quotes']
for tag in tags:
    try:
        Tag._new(tag)
    except TagExists:
        pass

########NEW FILE########
__FILENAME__ = commands
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
import paste.deploy.config
import paste.fixture
from paste.registry import RegistryManager
from paste.script import command
from paste.deploy import appconfig        
from r2.config.environment import load_environment
from paste.script.pluginlib import find_egg_info_dir
from pylons.wsgiapp import PylonsApp

#from pylons.commands import ShellCommand, ControllerCommand, \
#     RestControllerCommand

import os, sys
#
# commands that will be available by running paste with this app
# 

class RunCommand(command.Command):
    max_args = 2
    min_args = 1

    usage = "CONFIGFILE CMDFILE.py"
    summary = "Executed CMDFILE with pylons support"
    group_name = "Reddit"


    parser = command.Command.standard_parser(verbose=True)
    parser.add_option('-c', '--command',
                      dest='command',
                      help="execute command in module")

    def command(self):
        config_name = 'config:%s' % self.args[0]
        here_dir = os.getcwd()

        conf = appconfig(config_name, relative_to=here_dir)
        conf.update(dict(app_conf=conf.local_conf,
                         global_conf=conf.global_conf))
        paste.deploy.config.CONFIG.push_thread_config(conf)

        load_environment(conf.global_conf, conf.local_conf)

        # Load locals and populate with objects for use in shell
        sys.path.insert(0, here_dir)

        # Load the wsgi app first so that everything is initialized right
        wsgiapp = RegistryManager(PylonsApp())
        test_app = paste.fixture.TestApp(wsgiapp)
                        
        # Query the test app to setup the environment
        tresponse = test_app.get('/_test_vars')
        request_id = int(tresponse.body)

        # Disable restoration during test_app requests
        test_app.pre_request_hook = lambda self: \
            paste.registry.restorer.restoration_end()
        test_app.post_request_hook = lambda self: \
            paste.registry.restorer.restoration_begin(request_id)

        # Restore the state of the Pylons special objects
        # (StackedObjectProxies)
        paste.registry.restorer.restoration_begin(request_id)

        loaded_namespace = {}

        if self.args[1:]:
            cmd = self.args[1]
            f = open(cmd);
            data = f.read()
            f.close()
            
            exec data in loaded_namespace
            
        if self.options.command:
            exec self.options.command in loaded_namespace

########NEW FILE########
__FILENAME__ = admin_routes
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
def add(mc):
    mc('/admin/', controller='i18n', action='list')

try:
    from r2admin.config.routing import *
except ImportError:
    pass
    
    

########NEW FILE########
__FILENAME__ = databases
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.lib.manager import db_manager
from pylons import g
try:
    #TODO: move?
    from r2admin.config.databases import *
except:
    pass

tz = g.tz

dbm = db_manager.db_manager()

main_engine = db_manager.get_engine(g.main_db_name,
                                    db_host = g.main_db_host,
                                    db_user = g.main_db_user,
                                    db_pass = g.main_db_pass)

comment_engine = db_manager.get_engine(g.comment_db_name,
                                    db_host = g.comment_db_host,
                                    db_user = g.comment_db_user,
                                    db_pass = g.comment_db_pass)

vote_engine = db_manager.get_engine(g.vote_db_name,
                                    db_host = g.vote_db_host,
                                    db_user = g.vote_db_user,
                                    db_pass = g.vote_db_pass)

change_engine = db_manager.get_engine(g.change_db_name,
                                      db_host = g.change_db_host,
                                      db_user = g.change_db_user,
                                      db_pass = g.change_db_pass,
                                      pool_size = 2,
                                      max_overflow = 2)

email_engine = db_manager.get_engine(g.email_db_name,
                                      db_host = g.email_db_host,
                                      db_user = g.email_db_user,
                                      db_pass = g.email_db_pass,
                                      pool_size = 2,
                                      max_overflow = 2)

query_queue_engine = db_manager.get_engine(g.query_queue_db_name,
                                           db_host = g.query_queue_db_host,
                                           db_user = g.query_queue_db_user,
                                           db_pass = g.query_queue_db_pass,
                                           pool_size = 2,
                                           max_overflow = 2)

dbm.type_db = main_engine
dbm.relation_type_db = main_engine

dbm.thing('link', main_engine, main_engine)
dbm.thing('account', main_engine, main_engine)
dbm.thing('message', main_engine, main_engine)
dbm.thing('tag', main_engine, main_engine)
dbm.thing('edit', main_engine, main_engine)
dbm.thing('meetup', main_engine, main_engine)
dbm.thing('award', main_engine, main_engine)
dbm.thing('karmaadjustment', main_engine, main_engine)
dbm.thing('pendingjob', main_engine, main_engine)

dbm.relation('savehide', 'account', 'link', main_engine)
dbm.relation('click', 'account', 'link', main_engine)
dbm.relation('subscription', 'account', 'link', main_engine)
dbm.relation('commentsubscription', 'account', 'comment', main_engine)
dbm.relation('subscriptionstorage', 'account', 'comment', main_engine)
dbm.relation('linktag', 'link', 'tag', main_engine)

dbm.thing('comment', comment_engine, comment_engine)

dbm.thing('subreddit', comment_engine, comment_engine)
dbm.relation('srmember', 'subreddit', 'account', comment_engine)

dbm.relation('friend', 'account', 'account', comment_engine)

dbm.relation('vote_account_link', 'account', 'link', vote_engine)
dbm.relation('vote_account_comment', 'account', 'comment', vote_engine)

dbm.relation('inbox_account_comment', 'account', 'comment', comment_engine)
dbm.relation('inbox_account_message', 'account', 'message', main_engine)

dbm.relation('report_account_link', 'account', 'link', main_engine)
dbm.relation('report_account_comment', 'account', 'comment', comment_engine)
dbm.relation('report_account_message', 'account', 'message', main_engine)
dbm.relation('report_account_subreddit', 'account', 'subreddit', main_engine)

dbm.thing('poll', main_engine, main_engine)
dbm.relation('ballot', 'account', 'poll', main_engine)


########NEW FILE########
__FILENAME__ = environment
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
import os

#import pylons.config
from pylons import config

import webhelpers

from   r2.config.routing import make_map
import r2.lib.app_globals as app_globals
from   r2.lib import  rpc
import r2.lib.helpers
import r2.config as reddit_config

from r2.templates import tmpl_dirs

def load_environment(global_conf={}, app_conf={}):
    map = make_map(global_conf, app_conf)
    # Setup our paths
    root_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

    paths = {'root': root_path,
             'controllers': os.path.join(root_path, 'controllers'),
             'templates': tmpl_dirs,
             'static_files': os.path.join(root_path, 'public')
             }

    config.init_app(global_conf, app_conf, package='r2',
                    template_engine='mako', paths=paths)

    config['pylons.g'] = app_globals.Globals(global_conf, app_conf, paths)
    config['pylons.h'] = r2.lib.helpers
    config['routes.map'] = map

    #override the default response options
    config['pylons.response_options']['headers'] = {}

    # The following template options are passed to your template engines
    #tmpl_options = {}
    #tmpl_options['myghty.log_errors'] = True
    #tmpl_options['myghty.escapes'] = dict(l=webhelpers.auto_link, s=webhelpers.simple_format)

    tmpl_options = config['buffet.template_options']
    tmpl_options['mako.default_filters'] = ["websafe"]
    tmpl_options['mako.imports'] = \
                                 ["from r2.lib.filters import websafe, unsafe",
                                  "from pylons import c, g, request",
                                  "from pylons.i18n import _, ungettext"]
    
    # Add your own template options config options here,
    # note that all config options will override
    # any Pylons config options
    g = config['pylons.g']
    reddit_config.cache = g.cache

    # Return our loaded config object
    #return config.Config(tmpl_options, map, paths)

########NEW FILE########
__FILENAME__ = middleware
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is Reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
#
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
"""Pylons middleware initialization"""
from paste.cascade import Cascade
from paste.registry import RegistryManager
from paste.urlparser import StaticURLParser
from paste.deploy.converters import asbool
from paste.gzipper import make_gzip_middleware
from paste.request import resolve_relative_url
from paste.response import header_value, replace_header

from pylons import config, request, Response
from pylons.error import error_template
from pylons.middleware import ErrorDocuments, ErrorHandler, StaticJavascripts
from pylons.wsgiapp import PylonsApp, PylonsBaseWSGIApp

from r2.config.environment import load_environment
from r2.config.rewrites import rewrites
from r2.lib.utils import rstrips
from r2.lib.jsontemplates import api_type

#middleware stuff
from r2.lib.html_source import HTMLValidationParser
from cStringIO import StringIO
import sys, tempfile, urllib, re, os, hashlib


#from pylons.middleware import error_mapper
def error_mapper(code, message, environ, global_conf=None, **kw):

    if environ.get('pylons.error_call'):
        return None
    else:
        environ['pylons.error_call'] = True

    if global_conf is None:
        global_conf = {}
    codes = [401, 403, 404, 503]
    if not asbool(global_conf.get('debug')):
        codes.append(500)
    if code in codes:
        # StatusBasedForward expects a relative URL (no SCRIPT_NAME)
        d = dict(code = code, message = message)
        if environ.get('REDDIT_CNAME'):
            d['cnameframe'] = 1
        if environ.get('REDDIT_NAME'):
            d['srname'] = environ.get('REDDIT_NAME')
        url = '/error/document/?%s' % (urllib.urlencode(d))
        return url

class DebugMiddleware(object):
    def __init__(self, app, keyword):
        self.app = app
        self.keyword = keyword

    def __call__(self, environ, start_response):
        def foo(*a, **kw):
            self.res = self.app(environ, start_response)
            return self.res
        debug = config['global_conf']['debug'].lower() == 'true'
        args = {}
        for x in environ['QUERY_STRING'].split('&'):
            x = x.split('=')
            args[x[0]] = x[1] if x[1:] else None
        if debug and self.keyword in args.keys():
            prof_arg = args.get(self.keyword)
            prof_arg = urllib.unquote(prof_arg) if prof_arg else None
            return self.filter(foo, prof_arg = prof_arg)
        return self.app(environ, start_response)

    def filter(self, execution_func, prof_arg = None):
        pass

class ProfilingMiddleware(DebugMiddleware):
    def __init__(self, app):
        DebugMiddleware.__init__(self, app, 'profile')

    def filter(self, execution_func, prof_arg = None):
        import cProfile as profile
        from pstats import Stats

        tmpfile = tempfile.NamedTemporaryFile()
        try:
            file, line = prof_arg.split(':')
            line, func = line.split('(')
            func = func.strip(')')
        except:
            file = line = func = None

        try:
            profile.runctx('execution_func()',
                           globals(), locals(), tmpfile.name)
            out = StringIO()
            stats = Stats(tmpfile.name, stream=out)
            stats.sort_stats('time', 'calls')

            def parse_table(t, ncol):
                table = []
                for s in t:
                    t = [x for x in s.split(' ') if x]
                    if len(t) > 1:
                        table += [t[:ncol-1] + [' '.join(t[ncol-1:])]]
                return table

            def cmp(n):
                def _cmp(x, y):
                    return 0 if x[n] == y[n] else 1 if x[n] < y[n] else -1
                return _cmp

            if not file:
                stats.print_stats()
                stats_str = out.getvalue()
                statdata = stats_str.split('\n')
                headers = '\n'.join(statdata[:6])
                table = parse_table(statdata[6:], 6)
                from r2.lib.pages import Profiling
                res = Profiling(header = headers, table = table,
                                path = request.path).render()
                return [unicode(res)]
            else:
                query = "%s:%s" % (file, line)
                stats.print_callees(query)
                stats.print_callers(query)
                statdata = out.getvalue()

                data =  statdata.split(query)
                callee = data[2].split('->')[1].split('Ordered by')[0]
                callee = parse_table(callee.split('\n'), 4)
                callee.sort(cmp(1))
                callee = [['ncalls', 'tottime', 'cputime']] + callee
                i = 4
                while '<-' not in data[i] and i < len(data): i+= 1
                caller = data[i].split('<-')[1]
                caller = parse_table(caller.split('\n'), 4)
                caller.sort(cmp(1))
                caller = [['ncalls', 'tottime', 'cputime']] + caller
                from r2.lib.pages import Profiling
                res = Profiling(header = prof_arg,
                                caller = caller, callee = callee,
                                path = request.path).render()
                return [unicode(res)]
        finally:
            tmpfile.close()

class SourceViewMiddleware(DebugMiddleware):
    def __init__(self, app):
        DebugMiddleware.__init__(self, app, 'chk_source')

    def filter(self, execution_func, prof_arg = None):
        output = execution_func()
        output = [x for x in output]
        parser = HTMLValidationParser()
        res = parser.feed(output[-1])
        return [res]

class DomainMiddleware(object):
    lang_re = re.compile(r"^\w\w(-\w\w)?$")

    def __init__(self, app):
        self.app = app
        auth_cnames = config['global_conf'].get('authorized_cnames', '')
        auth_cnames = [x.strip() for x in auth_cnames.split(',')]
        # we are going to be matching with endswith, so make sure there
        # are no empty strings that have snuck in
        self.auth_cnames = filter(None, auth_cnames)

    def is_auth_cname(self, domain):
        return any((domain == cname or domain.endswith('.' + cname))
                   for cname in self.auth_cnames)

    def __call__(self, environ, start_response):
        # get base domain as defined in INI file
        base_domain = config['global_conf']['domain']
        try:
            sub_domains, request_port  = environ['HTTP_HOST'].split(':')
            environ['request_port'] = int(request_port)
        except ValueError:
            sub_domains = environ['HTTP_HOST'].split(':')[0]
        except KeyError:
            sub_domains = "localhost"

        #If the domain doesn't end with base_domain, assume
        #this is a cname, and redirect to the frame controller.
        #Ignore localhost so paster shell still works.
        #If this is an error, don't redirect

        if (not sub_domains.endswith(base_domain)
            and (not sub_domains == 'localhost')):
            environ['sub_domain'] = sub_domains
            if not environ.get('extension'):
                if environ['PATH_INFO'].startswith('/frame'):
                    return self.app(environ, start_response)
                elif self.is_auth_cname(sub_domains):
                    environ['frameless_cname'] = True
                    environ['authorized_cname'] = True
                elif ("redditSession" in environ.get('HTTP_COOKIE', '')
                      and environ['REQUEST_METHOD'] != 'POST'
                      and not environ['PATH_INFO'].startswith('/error')):
                    environ['original_path'] = environ['PATH_INFO']
                    environ['PATH_INFO'] = '/frame'
                else:
                    environ['frameless_cname'] = True
            return self.app(environ, start_response)

        sub_domains = sub_domains[:-len(base_domain)].strip('.')
        sub_domains = sub_domains.split('.')

        sr_redirect = None
        for sd in list(sub_domains):
            # subdomains to disregard completely
            if sd in ('www', 'origin', 'beta'):
                continue
            # subdomains which change the extension
            elif sd == 'm':
                environ['reddit-domain-extension'] = 'mobile'
            elif sd in ('api', 'rss', 'xml', 'json'):
                environ['reddit-domain-extension'] = sd
            elif (len(sd) == 2 or (len(sd) == 5 and sd[2] == '-')) and self.lang_re.match(sd):
                environ['reddit-prefer-lang'] = sd
            else:
                sr_redirect = sd
                sub_domains.remove(sd)

        if sr_redirect and environ.get("FULLPATH"):
            r = Response()
            sub_domains.append(base_domain)
            redir = "%s/r/%s/%s" % ('.'.join(sub_domains),
                                    sr_redirect, environ['FULLPATH'])
            redir = "http://" + redir.replace('//', '/')
            r.status_code = 301
            r.headers['location'] = redir
            r.content = ""
            return r(environ, start_response)

        return self.app(environ, start_response)


class SubredditMiddleware(object):
    sr_pattern = re.compile(r'^/r/([^/]{2,})')

    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        path = environ['PATH_INFO']
        sr = self.sr_pattern.match(path)
        if sr:
            environ['subreddit'] = sr.groups()[0]
            environ['PATH_INFO'] = self.sr_pattern.sub('', path) or '/'
        elif path.startswith("/categories"):
            environ['subreddit'] = 'r'
        return self.app(environ, start_response)

class DomainListingMiddleware(object):
    domain_pattern = re.compile(r'^/domain/(([-\w]+\.)+[\w]+)')

    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        if not environ.has_key('subreddit'):
            path = environ['PATH_INFO']
            domain = self.domain_pattern.match(path)
            if domain:
                environ['domain'] = domain.groups()[0]
                environ['PATH_INFO'] = self.domain_pattern.sub('', path) or '/'
        return self.app(environ, start_response)

class ExtensionMiddleware(object):
    ext_pattern = re.compile(r'\.([^/]+)$')

    extensions = {'rss' : ('xml', 'text/xml; charset=UTF-8'),
                  'xml' : ('xml', 'text/xml; charset=UTF-8'),
                  'js' : ('js', 'text/javascript; charset=UTF-8'),
                  #'png' : ('png', 'image/png'),
                  #'css' : ('css', 'text/css'),
                  'api' : (api_type(), 'application/json; charset=UTF-8'),
                  'json' : (api_type(), 'application/json; charset=UTF-8'),
                  'json-html' : (api_type('html'), 'application/json; charset=UTF-8')}

    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        path = environ['PATH_INFO']
        domain_ext = environ.get('reddit-domain-extension')
        for ext, val in self.extensions.iteritems():
            if ext == domain_ext or path.endswith('.' + ext):
                environ['extension'] = ext
                environ['render_style'] = val[0]
                environ['content_type'] = val[1]
                #strip off the extension
                if path.endswith('.' + ext):
                    environ['PATH_INFO'] = path[:-(len(ext) + 1)]
                break
        else:
            environ['render_style'] = 'html'
            environ['content_type'] = 'text/html; charset=UTF-8'

        return self.app(environ, start_response)

class RewriteMiddleware(object):
    def __init__(self, app):
        self.app = app

    def rewrite(self, regex, out_template, input):
        m = regex.match(input)
        out = out_template
        if m:
            for num, group in enumerate(m.groups('')):
                out = out.replace('$%s' % (num + 1), group)
            return out

    def __call__(self, environ, start_response):
        path = environ['PATH_INFO']
        for r in rewrites:
            newpath = self.rewrite(r[0], r[1], path)
            if newpath:
                environ['PATH_INFO'] = newpath
                break

        environ['FULLPATH'] = environ.get('PATH_INFO')
        qs = environ.get('QUERY_STRING')
        if qs:
            environ['FULLPATH'] += '?' + qs

        return self.app(environ, start_response)

class RequestLogMiddleware(object):
    def __init__(self, log_path, process_iden, app):
        self.log_path = log_path
        self.app = app
        self.process_iden = str(process_iden)

    def __call__(self, environ, start_response):
        request = '\n'.join('%s: %s' % (k,v) for k,v in environ.iteritems()
                           if k.isupper())
        iden = self.process_iden + '-' + hashlib.sha1(request).hexdigest()

        fname = os.path.join(self.log_path, iden)
        f = open(fname, 'w')
        f.write(request)
        f.close()

        r = self.app(environ, start_response)

        if os.path.exists(fname):
            try:
                os.remove(fname)
            except OSError:
                pass
        return r

class LimitUploadSize(object):
    """
    Middleware for restricting the size of uploaded files (such as
    image files for the CSS editing capability).
    """
    def __init__(self, app, max_size=1024*500):
        self.app = app
        self.max_size = max_size

    def __call__(self, environ, start_response):
        cl_key = 'CONTENT_LENGTH'
        if environ['REQUEST_METHOD'] == 'POST':
            if ((cl_key not in environ)
                or int(environ[cl_key]) > self.max_size):
                r = Response()
                r.status_code = 500
                r.content = '<html><head></head><body><script type="text/javascript">parent.too_big();</script>request too big</body></html>'
                return r(environ, start_response)

        return self.app(environ, start_response)

class AbsoluteRedirectMiddleware(object):
    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):

        def start_response_wrapper(status, headers, exc_info=None):
            location_header = 'location'
            status_code = int(status.split(None,1)[0])
            if (status_code >= 301 and status_code <= 303) or status_code == 307:
                location = header_value(headers, location_header)
                if location:
                    replace_header(headers, location_header, resolve_relative_url(location, environ))
            return start_response(status, headers, exc_info)

        return self.app(environ, start_response_wrapper)

class CleanupMiddleware(object):
    """
    Put anything here that should be called after every other bit of
    middleware. This currently includes the code for removing
    duplicate headers (except multiple cookie setting).  The behavior
    here is to disregard all but the last record.
    """
    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        def custom_start_response(status, headers, exc_info = None):
            fixed = []
            seen = set()
            for head, val in reversed(headers):
                head = head.title()
                if head == 'Set-Cookie' or head not in seen:
                    fixed.insert(0, (head, val))
                    seen.add(head)
            return start_response(status, fixed, exc_info)
        return self.app(environ, custom_start_response)

#god this shit is disorganized and confusing
class RedditApp(PylonsBaseWSGIApp):
    def find_controller(self, controller):
        if controller in self.controller_classes:
            return self.controller_classes[controller]

        full_module_name = self.package_name + '.controllers'
        class_name = controller.capitalize() + 'Controller'

        __import__(self.package_name + '.controllers')
        mycontroller = getattr(sys.modules[full_module_name], class_name)
        self.controller_classes[controller] = mycontroller
        return mycontroller

def make_app(global_conf, full_stack=True, **app_conf):
    """Create a Pylons WSGI application and return it

    `global_conf`
        The inherited configuration for this application. Normally from the
        [DEFAULT] section of the Paste ini file.

    `full_stack`
        Whether or not this application provides a full WSGI stack (by default,
        meaning it handles its own exceptions and errors). Disable full_stack
        when this application is "managed" by another WSGI middleware.

    `app_conf`
        The application's local configuration. Normally specified in the
        [app:<name>] section of the Paste ini file (where <name> defaults to
        main).
    """

    # Configure the Pylons environment
    load_environment(global_conf, app_conf)

    # The Pylons WSGI app
    app = PylonsApp(base_wsgi_app=RedditApp)

    # CUSTOM MIDDLEWARE HERE (filtered by the error handling middlewares)

    app = LimitUploadSize(app)
    app = ProfilingMiddleware(app)
    app = SourceViewMiddleware(app)

    app = DomainListingMiddleware(app)
    app = SubredditMiddleware(app)
    app = ExtensionMiddleware(app)
    app = DomainMiddleware(app)

    log_path = global_conf.get('log_path')
    if log_path:
        process_iden = global_conf.get('scgi_port', 'default')
        app = RequestLogMiddleware(log_path, process_iden, app)

    #TODO: breaks on 404
    #app = make_gzip_middleware(app, app_conf)

    if asbool(full_stack):
        # Handle Python exceptions
        app = ErrorHandler(app, global_conf, error_template=error_template,
                           **config['pylons.errorware'])

        # Display error documents for 401, 403, 404 status codes (and 500 when
        # debug is disabled)
        app = ErrorDocuments(app, global_conf, mapper=error_mapper, **app_conf)

    # Establish the Registry for this application
    app = RegistryManager(app)

    # Static files
    javascripts_app = StaticJavascripts()
    # Set cache headers indicating the client should cache for 7 days
    static_app = StaticURLParser(config['pylons.paths']['static_files'], cache_max_age=604800)
    app = Cascade([static_app, javascripts_app, app])

    app = AbsoluteRedirectMiddleware(app)

    #add the rewrite rules
    app = RewriteMiddleware(app)

    app = CleanupMiddleware(app)

    return app

########NEW FILE########
__FILENAME__ = rewrites
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
import re

rewrites = (#these first two rules prevent the .embed rewrite from
            #breaking other js that should work
            ("^/_(.*)", "/_$1"),
            ("^/static/(.*\.js)", "/static/$1"),
            #This next rewrite makes it so that all the embed stuff works.
            ("^(.*)(?<!button)(?<!buttonlite)(\.js)$", "$1.embed"))

rewrites = tuple((re.compile(r[0]), r[1]) for r in rewrites)

########NEW FILE########
__FILENAME__ = routing
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is Reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
#
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
"""
Setup your Routes options here
"""
import os
from routes import Mapper
import admin_routes
from wiki_pages_embed import allWikiPagesCached

def make_map(global_conf={}, app_conf={}):
    map = Mapper()
    mc = map.connect

    admin_routes.add(mc)

    mc('/login',    controller='front', action='login')
    mc('/logout',   controller='front', action='logout')
    mc('/adminon',  controller='front', action='adminon')
    mc('/adminoff', controller='front', action='adminoff')
    mc('/submit',   controller='front', action='submit')
    mc('/verifyemail', controller='front', action='verifyemail')

    mc('/imagebrowser', controller='front', action='imagebrowser')
    mc('/imagebrowser/:article', controller='front', action='imagebrowser')

    mc('/validuser',   controller='front', action='validuser')

    mc('/over18',   controller='post', action='over18')

    mc('/search/results', controller='front', action='search_results')

    mc('/about/:location', controller='front',
       action='editreddit', location = 'about')

    mc('/categories/create', controller='front', action='newreddit')
    mc('/categories/:where', controller='reddits', action='listing',
       where = 'popular',
       requirements=dict(where="popular|new|banned"))

    mc('/categories/mine/:where', controller='myreddits', action='listing',
       where='subscriber',
       requirements=dict(where='subscriber|contributor|moderator'))

    #mc('/stats', controller='front', action='stats')

    mc('/user/:username/:where', controller='user', action='listing',
       where='profile')

    mc('/prefs/:location', controller='front',
       action='prefs', location='options')

    mc('/related/:article/:title', controller='front',
       action = 'related', title=None)
    mc('/lw/:article/:title/:comment', controller='front',
       action= 'comments', title=None, comment = None)
    mc('/edit/:article', controller='front', action="editarticle")


    mc('/stylesheet', controller = 'front', action = 'stylesheet')

    mc('/', controller='promoted', action='listing')
    
    for name,page in allWikiPagesCached.items():
        if page.has_key('route'):
            mc("/wiki/"+page['route'], controller='wikipage', action='wikipage', name=name)
        
    mc('/invalidate_cache', controller='wikipage', action='invalidate_cache')

    listing_controllers = "hot|saved|toplinks|topcomments|new|recommended|randomrising|comments|blessed|recentposts|edits|promoted"

    mc('/:controller', action='listing',
       requirements=dict(controller=listing_controllers))

    mc('/dashboard/comments', action='listing', controller='interestingcomments')
    mc('/dashboard/subscribed', action='listing', controller='interestingsubscribed')
    mc('/dashboard/posts', action='listing', controller='interestingposts')

    # Can't use map.resource because the version of the Routing module we're
    # using doesn't support the controller_action kw arg
    #map.resource('meetup', 'meetups', collection_actions=['create', 'new'])
    mc('/meetups/create', action='create', controller='meetups')
    mc('/meetups', action='listing', controller='meetupslisting')
    mc('/meetups/new', action='new', controller='meetups')
    mc('/meetups/:id/edit', action='edit', controller='meetups')
    mc('/meetups/:id/update', action='update', controller='meetups')
    mc('/meetups/:id', action='show', controller='meetups')

    mc('/tag/:tag', controller='tag', action='listing', where='tag')

    mc('/by_id/:names', controller='byId', action='listing')

    mc('/:sort', controller='browse', sort='top', action = 'listing',
       requirements = dict(sort = 'top|controversial'))

    mc('/message/compose', controller='message', action='compose')
    mc('/message/:where', controller='message', action='listing')

    mc('/karma/award', controller='karmaaward', action='award')
    mc('/karma', controller='karmaaward', action='listing')

    mc('/:action', controller='front',
       requirements=dict(action="password|random|framebuster"))
    mc('/:action', controller='embed',
       requirements=dict(action="help|blog"))

    mc('/:action', controller='toolbar',
       requirements=dict(action="goto|toolbar"))

    mc('/resetpassword/:key', controller='front',
       action='resetpassword')
    mc('/resetpassword', controller='front',
       action='resetpassword')

    mc('/post/:action', controller='post',
       requirements=dict(action="options|over18|optout|optin|login|reg"))

    mc('/api/:action', controller='api')

    mc('/captcha/:iden', controller='captcha', action='captchaimg')

    mc('/doquery', controller='query', action='doquery')

    mc('/code', controller='redirect', action='redirect',
       dest='http://code.google.com/p/lesswrong/')

    mc('/about-less-wrong', controller='front', action='about')
    mc('/issues', controller='front', action='issues')

    # Google webmaster tools verification page
    mc('/googlea26ba8329f727095.html', controller='front', action='blank')

    # This route handles displaying the error page and
    # graphics used in the 404/500
    # error pages. It should likely stay at the top
    # to ensure that the error page is
    # displayed properly.
    mc('/error/document/:id', controller='error', action="document")

    mc("/*url", controller='front', action='catchall')

    return map


########NEW FILE########
__FILENAME__ = templates
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.lib.manager import tp_manager
from r2.lib.jsontemplates import *

tpm = tp_manager.tp_manager()

def api(type, cls):
    tpm.add_handler(type, 'api', cls())
    tpm.add_handler(type, 'api-html', cls())

# blanket fallback rule
api('wrapped', NullJsonTemplate)

# class specific overrides
api('link',          LinkJsonTemplate)
api('comment',       CommentJsonTemplate)
api('message',       MessageJsonTemplate)
api('subreddit',     SubredditJsonTemplate)
api('morerecursion', MoreCommentJsonTemplate)
api('morechildren',  MoreCommentJsonTemplate)
api('reddit',        RedditJsonTemplate)
api('panestack',     PanestackJsonTemplate)
api('listing',       ListingJsonTemplate)

api('organiclisting',       OrganicListingJsonTemplate)

########NEW FILE########
__FILENAME__ = admin
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.controllers.reddit_base import RedditController
from r2.controllers.reddit_base import base_listing

from r2.controllers.validator import *
from r2.lib.pages import *
from r2.models import *

from pylons.i18n import _

def admin_profile_query(vuser, location, db_sort):
    return None 

class AdminController(RedditController): pass

try:
    from r2admin.controllers.admin import *
except ImportError:
    pass



########NEW FILE########
__FILENAME__ = api
# -*- coding: utf-8 -*-
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is Reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
#
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from reddit_base import RedditController

from pylons.i18n import _
from pylons import c, request, response
from pylons.controllers.util import etag_cache

import hashlib
import httplib
import urllib2
from validator import *

from r2.models import *
from r2.models.subreddit import Default as DefaultSR
from r2.models.subreddit import Subreddit
import r2.models.thing_changes as tc

from r2.controllers import ListingController

from r2.lib.utils import get_title, sanitize_url, timeuntil, \
    set_last_modified, remote_addr
from r2.lib.utils import query_string, to36, timefromnow
from r2.lib.wrapped import Wrapped
from r2.lib.rancode import random_key
from r2.lib.pages import FriendList, ContributorList, ModList, EditorList, \
    BannedList, BoringPage, FormPage, NewLink, CssError, UploadedImage, \
    RecentArticles, RecentComments, TagCloud, TopContributors, TopMonthlyContributors, WikiPageList, \
    ArticleNavigation, UpcomingMeetups, RecentPromotedArticles, \
    MeetupsMap, RecentTagged


from r2.lib.menus import CommentSortMenu
from r2.lib.translation import Translator
from r2.lib.normalized_hot import expire_hot
from r2.lib.captcha import get_iden
from r2.lib import emailer
from r2.lib.strings import strings
from r2.lib.memoize import clear_memo
from r2.lib.filters import _force_unicode
from r2.lib.db import queries
from r2.config import cache
from r2.lib.jsonresponse import JsonResponse, Json
from r2.lib.jsontemplates import api_type
from r2.lib import cssfilter
from r2.lib import tracking
from r2.lib.media import force_thumbnail, thumbnail_url
from r2.lib.comment_tree import add_comment, delete_comment
from r2.lib import wiki_account

from datetime import datetime, timedelta
from simplejson import dumps
from md5 import md5
from lxml import etree

from r2.lib.promote import promote, unpromote, get_promoted

def link_listing_by_url(url, count = None):
    try:
        links = list(tup(Link._by_url(url, sr = c.site)))
        links.sort(key = lambda x: -x._score)
        if count is not None:
            links = links[:count]
    except NotFound:
        links = ()

    names = [l._fullname for l in links]
    builder = IDBuilder(names, num = 25)
    listing = LinkListing(builder).listing()
    return listing


class ApiController(RedditController):
    def response_func(self, **kw):
        return self.sendstring(dumps(kw))

    @Json
    def ajax_login_redirect(self, res, dest):
        res._redirect("/login" + query_string(dict(dest=dest)))

    def link_exists(self, url, sr, message = False):
        try:
            l = Link._by_url(url, sr)
            if message:
                return l.already_submitted_link()
            else:
                return l.make_permalink_slow()
        except NotFound:
            pass


    @validate(url = nop("url"),
              sr = VSubredditName,
              count = VLimit('limit'))
    def GET_info(self, url, sr, count):
        listing = link_listing_by_url(url, count = count)
        res = BoringPage(_("API"),
                         content = listing).render()
        return res

    @Json
    @validate(VCaptcha(),
              name=VRequired('name', errors.NO_NAME),
              email=VRequired('email', errors.NO_EMAIL),
              replyto = nop('replyto'),
              reason = nop('reason'),
              message=VRequired('message', errors.NO_MESSAGE))
    def POST_feedback(self, res, name, email, replyto, reason, message):
        res._update('status', innerHTML = '')
        if res._chk_error(errors.NO_NAME):
            res._focus("name")
        elif res._chk_error(errors.NO_EMAIL):
            res._focus("email")
        elif res._chk_error(errors.NO_MESSAGE):
            res._focus("personal")
        elif res._chk_captcha(errors.BAD_CAPTCHA):
            pass

        if not res.error:
            if reason != 'ad_inq':
                emailer.feedback_email(email, message, name = name or '',
                                       reply_to = replyto or '')
            else:
                emailer.ad_inq_email(email, message, name = name or '',
                                       reply_to = replyto or '')
            res._update('success',
                        innerHTML=_("Thanks for your message! you should hear back from us shortly."))
            res._update("personal", value='')
            res._update("captcha", value='')
            res._hide("wtf")
    POST_ad_inq = POST_feedback

    @Json
    @validate(VCaptcha(),
              VUser(),
              VModhash(),
              ip = ValidIP(),
              to = VExistingUname('to'),
              subject = VRequired('subject', errors.NO_SUBJECT),
              body = VMessage('message'))
    def POST_compose(self, res, to, subject, body, ip):
        res._update('status', innerHTML='')
        if (res._chk_error(errors.NO_USER) or
            res._chk_error(errors.USER_DOESNT_EXIST)):
            res._focus('to')
        elif res._chk_error(errors.NO_SUBJECT):
            res._focus('subject')
        elif (res._chk_error(errors.NO_MSG_BODY) or
              res._chk_error(errors.COMMENT_TOO_LONG)):
            res._focus('message')
        elif res._chk_captcha(errors.BAD_CAPTCHA):
            pass

        if not res.error:
            spam = (c.user._spam or
                    errors.BANNED_IP in c.errors or
                    errors.BANNED_DOMAIN in c.errors)

            m, inbox_rel = Message._new(c.user, to, subject, body, ip, spam)
            res._update('success',
                        innerHTML=_("Your message has been delivered"))
            res._update('to', value='')
            res._update('subject', value='')
            res._update('message', value='')

            if g.write_query_queue:
                queries.new_message(m, inbox_rel)
        else:
            res._update('success', innerHTML='')

    @Json
    @validate(VUser(),
              code = VEmailVerify('code'))
    def POST_verifyemail(self, res, code):
        res._update('status', innerHTML = '')
        if res._chk_error(errors.NO_CODE):
            res._focus('code')
        elif res._chk_error(errors.WRONG_CODE):
            res._focus('code')
        else:
            c.user.email_validated = True
            c.user._commit()
            res._success()

    @Json
    @validate(VCaptcha(),
              VUser(),
              VModhash(),
              ip = ValidIP(),
              to = VExistingUname('to'),
              subject = VAwardAmount('amount', errors.NO_AMOUNT),
              body = VMessage('reason'))
    def POST_award(self, res, to, subject, body, ip):
        res._update('status', innerHTML='')
        if (res._chk_error(errors.NO_USER) or
            res._chk_error(errors.USER_DOESNT_EXIST)):
            res._focus('to')
        elif (res._chk_error(errors.NO_AMOUNT) or
              res._chk_error(errors.AMOUNT_NOT_NUM) or
              res._chk_error(errors.AMOUNT_NEGATIVE)):
            res._focus('amount')
        elif (res._chk_error(errors.NO_MSG_BODY) or
              res._chk_error(errors.COMMENT_TOO_LONG)):
            res._focus('reason')
        elif res._chk_captcha(errors.BAD_CAPTCHA):
            pass

        if not res.error:
            spam = (c.user._spam or
                    errors.BANNED_IP in c.errors or
                    errors.BANNED_DOMAIN in c.errors)

            to.incr_karma('adjustment', Subreddit._by_name('discussion'), int(subject), 0)

            res._update('success',
                        innerHTML=_("Karma Awarded"))
            res._update('to', value='')
            res._update('amount', value='')
            res._update('reason', value='')
            Award._new(c.user, body, subject, to, ip)

            messagebody = 'You have been awarded ' + subject + ' karma for ' + body

            m, inbox_rel = Message._new(c.user, to, 'Karma Award', messagebody, ip, spam)

        else:
            res._update('success', innerHTML='')



    @Json
    @validate(VAdmin(),
              link = VByName('id'))
    def POST_bless(self, res, link):
        link.set_blessed(True)

    @Json
    @validate(VUser(),
              link = VByName('id'))
    def POST_linksubscribe(self, res, link):
        link.add_subscriber(c.user)

    @Json
    @validate(VUser(),
              link = VByName('id'))
    def POST_linkunsubscribe(self, res, link):
        link.remove_subscriber(c.user)

    @Json
    @validate(VUser(),
              comment = VByName('id'))
    def POST_commentsubscribe(self, res, comment):
        comment.add_subscriber(c.user)

    @Json
    @validate(VUser(),
              comment = VByName('id'))
    def POST_commentunsubscribe(self, res, comment):
        comment.remove_subscriber(c.user)

    @Json
    @validate(VAdmin(),
              link = VByName('id'))
    def POST_unbless(self, res, link):
        link.set_blessed(False)

    @Json
    @validate(VUser(),
              VCaptcha(),
              VRatelimit(rate_user = True, rate_ip = True, prefix='rate_submit_'),
              VModhash(),
              ip = ValidIP(),
              sr = VSubmitSR('sr'),
              title = VTitle('title'),
              l = VLink('article_id'),
              new_content = nop('article'),
              save = nop('save'),
              continue_editing = VBoolean('keep_editing'),
              notify_on_comment = VBoolean('notify_on_comment'),
              cc_licensed = VBoolean('cc_licensed'),
              tags = VTags('tags'))
    def POST_submit(self, res, l, new_content, title, save, continue_editing, sr, ip, tags, notify_on_comment, cc_licensed):
        res._update('status', innerHTML = '')
        should_ratelimit = sr.should_ratelimit(c.user, 'link') if sr else True

        #remove the ratelimit error if the user's karma is high
        if not should_ratelimit:
            c.errors.remove(errors.RATELIMIT)

        #ratelimiter
        if res._chk_error(errors.RATELIMIT):
            pass
        # check for title, otherwise look it up and return it
        elif res._chk_error(errors.NO_TITLE):
            # clear out this error
            res._chk_error(errors.TITLE_TOO_LONG)
            res._focus('title')
        elif res._chk_error(errors.TITLE_TOO_LONG):
            res._focus('title')
        elif res._chk_captcha(errors.BAD_CAPTCHA):
            pass
        elif res._chk_error(errors.SUBREDDIT_FORBIDDEN):
            pass


        if res.error or not title: return

        # check whether this is spam:
        spam = (c.user._spam or
                errors.BANNED_IP in c.errors or
                errors.BANNED_DOMAIN in c.errors)

        if not new_content:
            new_content = ''

        # well, nothing left to do but submit it
        # TODO: include article body in arguments to Link model
        # print "\n".join(request.post.va)
        if not l:
          l = Link._submit(request.post.title, new_content, c.user, sr, ip, tags, spam,
                           notify_on_comment=notify_on_comment, cc_licensed=cc_licensed)
          if save == 'on':
              r = l._save(c.user)
              if g.write_query_queue:
                  queries.new_savehide(r)

          #set the ratelimiter
          if should_ratelimit:
              VRatelimit.ratelimit(rate_user=True, rate_ip = True, prefix='rate_submit_')

          #update the queries
          if g.write_query_queue:
              queries.new_link(l)
        else:
          edit = None
          if c.user._id != l.author_id:
            edit = Edit._new(l,c.user,new_content)
          old_url = l.url
          l.title = request.post.title
          l.set_article(new_content)
          l.notify_on_comment = notify_on_comment
          l.cc_licensed = cc_licensed
          l.change_subreddit(sr._id)
          l._commit()
          l.set_tags(tags)
          l.update_url_cache(old_url)
          if edit:
            edit._commit()

        #update the modified flags
        set_last_modified(c.user, 'overview')
        set_last_modified(c.user, 'submitted')

        # flag search indexer that something has changed
        tc.changed(l)

        if continue_editing:
          path = "/edit/%s" % l._id36
        else:
          # make_permalink is designed for links that can be set to _top
          # here, we need to generate an ajax redirect as if we were not on a
          # cname.
          cname = c.cname
          c.cname = False
          #path = l.make_permalink_slow()
          path = l.make_permalink(sr, sr_path = not sr.name == g.default_sr)
          c.cname = cname

        res._redirect(path)


    def _login(self, res, user, dest='', rem = None):
        self.login(user, rem = rem)
        dest = dest or request.referer or '/'
        res._redirect(dest)

    @Json
    @validate(user = VLogin(['user_login', 'passwd_login']),
              op = VOneOf('op', options = ("login-main", "reg", "login"),
                          default = 'login'),
              dest = nop('dest'),
              rem = nop('rem'),
              reason = VReason('reason'))
    def POST_login(self, res, user, op, dest, rem, reason):
        if reason and reason[0] == 'redirect':
            dest = reason[1]

        res._update('status_' + op, innerHTML='')
        if res._chk_error(errors.WRONG_PASSWORD, op):
            res._focus('passwd_' + op)
        else:
            self._login(res, user, dest, rem == 'on')


    @Json
    @validate(VCaptcha(),
              VRatelimit(rate_ip = True, prefix='rate_register_'),
              name = VUname(['user_reg']),
              email = ValidEmail('email_reg'),
              password = VPassword(['passwd_reg', 'passwd2_reg']),
              op = VOneOf('op', options = ("login-main", "reg", "login"),
                          default = 'login'),
              dest = nop('dest'),
              rem = nop('rem'),
              reason = VReason('reason'))
    def POST_register(self, res, name, email, password, op, dest, rem, reason):
        res._update('status_' + op, innerHTML='')
        if res._chk_error(errors.BAD_USERNAME, op):
            res._focus('user_reg')
        elif res._chk_error(errors.BAD_USERNAME_SHORT, op):
            res._focus('user_reg')
        elif res._chk_error(errors.BAD_USERNAME_LONG, op):
            res._focus('user_reg')
        elif res._chk_error(errors.BAD_USERNAME_CHARS, op):
            res._focus('user_reg')
        elif res._chk_error(errors.USERNAME_TAKEN, op):
            res._focus('user_reg')
        elif res._chk_error(errors.BAD_EMAIL, op):
            res._focus('email_reg')
        elif res._chk_error(errors.NO_EMAIL, op):
            res._focus('email_reg')
        elif res._chk_error(errors.BAD_PASSWORD, op):
            res._focus('passwd_reg')
        elif res._chk_error(errors.BAD_PASSWORD_MATCH, op):
            res._focus('passwd2_reg')
        elif res._chk_error(errors.DRACONIAN, op):
            res._focus('legal_reg')
        elif res._chk_captcha(errors.BAD_CAPTCHA):
            pass
        elif res._chk_error(errors.RATELIMIT, op):
            pass

        if res.error:
            return

        user = register(name, password, email)
        VRatelimit.ratelimit(rate_ip = True, prefix='rate_register_')

        user.pref_lang = c.lang
        if c.content_langs == 'all':
            user.pref_content_langs = 'all'
        else:
            langs = list(c.content_langs)
            langs.sort()
            user.pref_content_langs = tuple(langs)

        d = c.user._dirties.copy()
        user._commit()

        c.user = user

        Subreddit.subscribe_defaults(user)

        # Create a drafts subredit for this user
        sr = Subreddit._create_and_subscribe(
            user.draft_sr_name, user, {
                'title': "Drafts for " + user.name,
                'type': "private",
                'default_listing': 'new',
            }
        )

        if reason:
            if reason[0] == 'redirect':
                dest = reason[1]
            elif reason[0] == 'subscribe':
                for sr, sub in reason[1].iteritems():
                    self._subscribe(sr, sub)

        self._login(res, user, dest, rem)


    @Json
    @validate(VUser(),
              VModhash(),
              container = VByName('id'),
              type = VOneOf('location', ('moderator',  'contributor')))
    def POST_leave(self, res, container, type):
        if container and c.user:
            res._hide("pre_" + container._fullname)
            res._hide("thingrow_" + container._fullname)
            fn = getattr(container, 'remove_' + type)
            fn(c.user)

    @Json
    @validate(VUser(),
              VModhash(),
              ip = ValidIP(),
              action = VOneOf('action', ('add', 'remove')),
              redirect = nop('redirect'),
              friend = VExistingUname('name'),
              container = VByName('container'),
              type = VOneOf('type', ('friend', 'moderator', 'editor', 'contributor', 'banned')))
    def POST_friend(self, res, ip, friend, action, redirect, container, type):
        res._update('status', innerHTML='')

        fn = getattr(container, action + '_' + type)

        if (not c.user_is_admin
            and (type in ('moderator','contributer','banned')
                 and not c.site.is_moderator(c.user))):

            abort(403,'forbidden')
        elif type == 'editor' and not c.user_is_admin:
            abort(403,'forbidden')
        elif action == 'add':
            if res._chk_errors((errors.USER_DOESNT_EXIST,
                                errors.NO_USER)):
                res._focus('name')
            else:
                new = fn(friend)
                cls = dict(friend=FriendList,
                           moderator=ModList,
                           editor=EditorList,
                           contributor=ContributorList,
                           banned=BannedList).get(type)
                res._update('name', value = '')

                #subscribing doesn't need a response
                if new and cls:
                    res.object = cls().ajax_user(friend).for_ajax('add')

                    if type != 'friend':
                        msg = strings.msg_add_friend.get(type)
                        subj = strings.subj_add_friend.get(type)
                        if msg and subj and friend.name != c.user.name:
                            # fullpath with domain needed or the markdown link
                            # will break
                            d = dict(url = container.path,
                                     title = container.title)
                            msg = msg % d
                            subj = subj % d
                            Message._new(c.user, friend, subj, msg, ip,
                                         c.user._spam)
        elif action == 'remove' and friend:
            fn(friend)


    def send_confirmation(self):
        """Send the conformation code to validate an email address."""
        c.user.email_validated = False
        c.user.confirmation_code = random_key(6)
        c.user._commit()
        emailer.confirmation_email(c.user)

    @Json
    @validate(VUser(),
              VModhash())
    def POST_resendconfirmation(self, res):
        if c.user.email is None or c.user.email_validated: return
        self.send_confirmation()
        res._update('resend-confirmation-form', innerHTML='')
        res._success()

    @Json
    @validate(VUser('curpass', default = ''),
              VModhash(),
              curpass = nop('curpass'),
              email = ValidEmail("email"),
              newpass = nop("newpass"),
              verpass = nop("verpass"),
              password = VPassword(['newpass', 'verpass']))
    def POST_update(self, res, email, curpass, password, newpass, verpass):
        res._update('status', innerHTML='')
        if res._chk_error(errors.WRONG_PASSWORD):
            res._focus('curpass')
            res._update('curpass', value='')
            return
        updated = False
        if res._chk_error(errors.BAD_EMAIL):
            res._focus('email')
        elif not hasattr(c.user,'email') and res._chk_error(errors.NO_EMAIL):
            res._focus('email')
        elif email and (not hasattr(c.user,'email')
                        or c.user.email != email):
            c.user.email = email
            self.send_confirmation()
            res._update('status',
                        innerHTML=_('Your email has been updated.  You will have to confirm before commenting or posting.'))
            updated = True

        if newpass or verpass:
            if res._chk_error(errors.BAD_PASSWORD):
                res._focus('newpass')
            elif res._chk_error(errors.BAD_PASSWORD_MATCH):
                res._focus('verpass')
                res._update('verpass', value='')
            else:
                change_password(c.user, password)
                if updated:
                    res._update('status',
                                innerHTML=_('Your email and password have been updated'))
                else:
                    res._update('status',
                                innerHTML=_('Your password has been updated'))
                self.login(c.user)

    @Json
    @validate(VUser('password', default = ''),
              VModhash(),
              password = nop('password'))
    def POST_wikiaccount(self, res, password):
        res._update('status', innerHTML='')
        if res._chk_error(errors.WRONG_PASSWORD):
            res._focus('wiki-password')
            res._update('wiki-password', value='')
            return

        if (not c.user.email or
            not c.user.email_validated or
            c.user.wiki_account is not None):
            # The form isn't rendered but in case someone sends a request directly
            return

        c.user.wiki_account = '__error__'

        def on_request_error():
            c.errors.add(errors.WIKI_DOWN)
            res._chk_error(errors.WIKI_DOWN)
        def on_wiki_error():
            c.errors.add(errors.WIKI_ACCOUNT_CREATION_FAILED)
            res._chk_error(errors.WIKI_ACCOUNT_CREATION_FAILED)
            res._update('wiki-create-form', innerHTML='')
            c.user._commit()
        if c.user.create_associated_wiki_account(password,
                                                 on_request_error=on_request_error,
                                                 on_wiki_error=on_wiki_error):
            res._success()
            res._update('wiki-create-form', innerHTML='')
            c.user._commit()

    def _reload(self, res):
        res._redirect(request.referer)

    @Json
    @validate(VUser(),
              VModhash(),
              areyousure1 = nop('areyousure1'),
              areyousure2 = nop('areyousure2'),
              areyousure3 = nop('areyousure3'))
    def POST_delete_user(self, res, areyousure1, areyousure2, areyousure3):
        if areyousure1 == areyousure2 == areyousure3 == 'Yes':
            c.user.delete()
            res._redirect('/?deleted=true')
        else:
            res._update('status',
                        innerHTML = _("See? you don't really want to leave"))

    @Json
    @validate(VUser(),
              VModhash(),
              thing = VByNameIfAuthor('id'))
    def POST_del(self, res, thing):
        '''for deleting all sorts of things'''

        # Special check if comment can be deleted
        if isinstance(thing, Comment) and (not thing.can_delete()):
            res._set_error(errors.CANNOT_DELETE)
            return

        thing._deleted = True
        thing._commit()

        # flag search indexer that something has changed
        tc.changed(thing)

        #expire the item from the sr cache
        if isinstance(thing, Link):
            sr = thing.subreddit_slow
            expire_hot(sr)
            if g.use_query_cache:
                queries.new_link(thing)

        #comments have special delete tasks
        elif isinstance(thing, Comment):
            thing._delete()
            delete_comment(thing)
            if g.use_query_cache:
                queries.new_comment(thing, None)

    @Json
    @validate(VUser(),
              VModhash(),
              thing = VByNameIfAuthor('id'))
    def POST_retract(self, res, thing):
        '''for retracting comments'''

        if isinstance(thing, Comment):
            thing.retracted = True
            thing._commit()
            if g.use_query_cache:
                queries.new_comment(thing, None)


    @Json
    @validate(VUser(), VModhash(),
              thing = VByName('id'))
    def POST_report(self, res, thing):
        '''for reporting...'''
        Report.new(c.user, thing)


    def _validate_comment_text(self, res, error_thing, text):
        # This needs access to `res` to set the displayed message. A Validator isn't flexible enough
        if text:
            try:
                parsepolls(text, None, dry_run = True)
            except PollError as ex:
                res._set_error(errors.BAD_POLL_SYNTAX, error_thing._fullname, ex.message)


    @Json
    @validate(VUser(), VModhash(),
              comment = VByNameIfAuthor('id'),
              body = VComment('comment'))
    def POST_editcomment(self, res, comment, body):
        self._validate_comment_text(res, comment, body)

        res._update('status_' + comment._fullname, innerHTML = '')

        if not res._chk_errors((errors.BAD_COMMENT, errors.COMMENT_TOO_LONG, errors.NOT_AUTHOR,
                                errors.BAD_POLL_SYNTAX),
                               comment._fullname):
            if not c.user_is_admin: comment.editted = True
            comment.set_body(body)
            res._send_things(comment)

            # flag search indexer that something has changed
            tc.changed(comment)


    @Json
    @validate(VUser(),
              VModhash(),
              VRatelimit(rate_user = True, rate_ip = True, prefix = "rate_comment_"),
              ip = ValidIP(),
              parent = VSubmitParent('id'),
              comment = VComment('comment'))
    def POST_comment(self, res, parent, comment, ip):
        #wipe out the status message
        res._update('status_' + parent._fullname, innerHTML = '')

        should_ratelimit = True
        adjust_karma = 0

        #check the parent type here cause we need that for the
        #ratelimit checks
        parent_comment = None
        if isinstance(parent, Message):
            is_message = True
            should_ratelimit = False
            link = None
        else:
            is_message = False
            is_comment = True
            if isinstance(parent, Link):
                link = parent
            else:
                link = Link._byID(parent.link_id, data = True)
                parent_comment = parent
            sr = parent.subreddit_slow
            if not sr.should_ratelimit(c.user, 'comment'):
                should_ratelimit = False

        if link and not link.comments_enabled:
            return abort(403,'forbidden')

        #remove the ratelimit error if the user's karma is high
        if not should_ratelimit:
            c.errors.remove(errors.RATELIMIT)

        # assess a tax on replies to downvoted comments
        if parent_comment and parent_comment.reply_costs_karma:
            if c.user.safe_karma < g.downvoted_reply_karma_cost:
                c.errors.add(errors.NOT_ENOUGH_KARMA)
            else:
                adjust_karma = -g.downvoted_reply_karma_cost

        # make sure there are no errors in poll syntax
        self._validate_comment_text(res, parent, comment)

        if res._chk_errors((errors.BAD_COMMENT, errors.COMMENT_TOO_LONG, errors.RATELIMIT,
                            errors.NOT_ENOUGH_KARMA, errors.BAD_POLL_SYNTAX),
                           parent._fullname):
            res._focus("comment_reply_" + parent._fullname)
            return
        res._show('reply_' + parent._fullname)
        res._update("comment_reply_" + parent._fullname, rows = '2')

        # now that all inputs are verified, we can begin performing destructive updates:

        if adjust_karma:
            assert adjust_karma < 0  # If not, need to fix the call to incr_karma
            KarmaAdjustment.store(c.user, sr, adjust_karma)
            c.user.incr_karma('adjustment', sr, 0, -adjust_karma)

        spam = (c.user._spam or
                errors.BANNED_IP in c.errors)

        if is_message:
            to = Account._byID(parent.author_id)
            subject = parent.subject
            re = "Re: "
            if not subject.startswith(re):
                subject = re + subject
            item, inbox_rel = Message._new(c.user, to, subject, comment, ip, spam)
            item.parent_id = parent._id
            res._send_things(item)
        else:
            item, inbox_rel =  Comment._new(c.user, link, parent_comment, comment,
                                            ip, spam)
            item.set_body(comment)  # some markup requires the comment to exist and have an ID before it can be parsed
            res._update("comment_reply_" + parent._fullname,
                        innerHTML='', value='')
            res._send_things(item)
            res._hide('noresults')

        #update the queries
        if g.write_query_queue:
            if is_message:
                queries.new_message(item, inbox_rel)
            else:
                queries.new_comment(item, inbox_rel)

        #set the ratelimiter
        if should_ratelimit:
            VRatelimit.ratelimit(rate_user=True, rate_ip = True, prefix = "rate_comment_")


    @Json
    @validate(VUser(),
              VModhash(),
              VCaptcha(),
              VRatelimit(rate_user = True, rate_ip = True,
                         prefix = "rate_share_"),
              share_from = VLength('share_from', length = 100),
              emails = ValidEmails("share_to"),
              reply_to = ValidEmails("replyto", num = 1),
              message = VLength("message", length = 1000),
              thing = VByName('id'))
    def POST_share(self, res, emails, thing, share_from, reply_to,
                   message):

        # remove the ratelimit error if the user's karma is high
        sr = thing.subreddit_slow
        should_ratelimit = sr.should_ratelimit(c.user, 'link')
        if not should_ratelimit:
            c.errors.remove(errors.RATELIMIT)

        res._hide("status_" + thing._fullname)

        if res._chk_captcha(errors.BAD_CAPTCHA, thing._fullname):
            pass
        elif res._chk_error(errors.RATELIMIT, thing._fullname):
            pass
        elif (share_from is None and
              res._chk_error(errors.COMMENT_TOO_LONG,
                             'share_from_' + thing._fullname)):
            res._focus('share_from_' + thing._fullname)
        elif (message is None and
              res._chk_error(errors.COMMENT_TOO_LONG,
                             'message_' + thing._fullname)):
            res._focus('message_' + thing._fullname)
        elif not emails and res._chk_errors((errors.BAD_EMAILS,
                                             errors.NO_EMAILS,
                                             errors.TOO_MANY_EMAILS),
                                            "emails_" + thing._fullname):
            res._focus("emails_" + thing._fullname)
        elif not reply_to and res._chk_error(errors.BAD_EMAILS,
                                             "replyto_" + thing._fullname):
            res._focus("replyto_" + thing._fullname)
        else:
            c.user.add_share_emails(emails)
            c.user._commit()

            res._update("share_li_" + thing._fullname,
                        innerHTML=_('Shared'))

            res._update("sharelink_" + thing._fullname,
                        innerHTML=("<div class='clearleft'></div><p class='error'>%s</p>" %
                                   _("Your link has been shared.")))

            emailer.share(thing, emails, from_name = share_from or "",
                          body = message or "", reply_to = reply_to or "")

            #set the ratelimiter
            if should_ratelimit:
                VRatelimit.ratelimit(rate_user=True, rate_ip = True, prefix = "rate_share_")



    @Json
    @validate(VUser(),
              VModhash(),
              vote_type = VVotehash(('vh', 'id')),
              ip = ValidIP(),
              dir = VInt('dir', min=-1, max=1),
              thing = VByName('id'))
    def POST_vote(self, res, dir, thing, ip, vote_type):
        ip = request.ip
        user = c.user
        spam = (c.user._spam or
                errors.BANNED_IP in c.errors or
                errors.CHEATER in c.errors)
        dir = (True if dir > 0
               else False if dir < 0
               else None)

        isRetracted = thing.retracted if thing and isinstance(thing,Comment) else False

        # Ensure authors can't vote on their own posts / comments
        # Cannot vote on retracted comments
        if thing and thing.author_id != c.user._id and not isRetracted:
            try:
                organic = vote_type == 'organic'
                v = Vote.vote(user, thing, dir, ip, spam, organic)

                #update relevant caches
                if isinstance(thing, Link):
                    sr = thing.subreddit_slow
                    set_last_modified(c.user, 'liked')
                    set_last_modified(c.user, 'disliked')

                    if v.valid_thing:
                        expire_hot(sr)

                    if g.write_query_queue:
                        queries.new_vote(v)

                # flag search indexer that something has changed
                tc.changed(thing)

            except NotEnoughKarma, e:
                # User is downvoting and does not have enough karma.
                res._update('status_'+thing._fullname, innerHTML = e.message)
                res._show('status_'+thing._fullname)

    @Json
    @validate(VUser(), VModhash(),
              comment = VCommentFullName('owner_thing'),
              anonymous = VBoolean('anonymous'))
    def POST_submitballot(self, res, comment, anonymous):
        ip = request.ip
        user = c.user
        spam = (c.user._spam or
                errors.BANNED_IP in c.errors or
                errors.CHEATER in c.errors)

        if not comment:
            return

        if c.user.safe_karma < g.karma_to_vote:
            res._set_error(errors.BAD_POLL_BALLOT, comment._fullname,
                           'You do not have the required {0} karma to vote'.format(g.karma_to_vote))
            return

        any_submitted = False

        #Save a ballot for each poll answered (corresponding to POST parameters named poll_[id36])
        for param in request.POST:
            ballotparam = re.match("poll_([a-z0-9]+)", param)
            if(ballotparam and request.POST[param]):
                pollid = int(ballotparam.group(1), 36)
                pollobj = Poll._byID(pollid, data = True)
                try:
                    response = pollobj.validate_response(request.POST[param])
                    ballot = Ballot.submitballot(user, comment, pollobj, response, anonymous, ip, spam)
                except PollError as ex:
                    res._set_error(errors.BAD_POLL_BALLOT, comment._fullname, ex.message)
                else:
                    any_submitted = True
                    if g.write_query_queue:
                        queries.new_ballot(ballot)

        if any_submitted:
            res._send_things(comment)
        else:
            res._set_error(errors.BAD_POLL_BALLOT, comment._fullname, 'No valid votes found')

        # A comment might have multiple polls, with only some of them voted on, and others
        # with errors. The above call to res._send_things causes JS to erase the error
        # message. So if there were errors, make sure they're visible.

        if res.error:
            res._update(res.error.name + '_' + comment._fullname, textContent = res.error.message)

    @Json
    @validate(thing = VCommentFullName('thing'))
    def GET_rawdata(self, response, thing):
        pollids = getpolls(thing.body)
        csv = exportvotes(pollids)
        c.response_content_type = 'text/plain'
        c.response.content = csv
        c.response.headers['Content-Disposition'] = 'attachment; filename="poll.csv"'
        return c.response

    @Json
    @validate(VUser(),
              VModhash(),
              stylesheet_contents = nop('stylesheet_contents'),
              op = VOneOf('op',['save','preview']))
    def POST_subreddit_stylesheet(self, res, stylesheet_contents = '', op='save'):
        if not c.site.can_change_stylesheet(c.user):
            return self.abort(403,'forbidden')

        if g.css_killswitch:
            return self.abort(403,'forbidden')

        parsed, report = cssfilter.validate_css(stylesheet_contents)

        if report.errors:
            error_items = [ CssError(x).render(style='html')
                            for x in sorted(report.errors) ]

            res._update('status', innerHTML = _('Validation errors'))
            res._update('validation-errors', innerHTML = ''.join(error_items))
            res._show('error-header')
        else:
            res._hide('error-header')
            res._update('status', innerHTML = '')
            res._update('validation-errors', innerHTML = '')

        stylesheet_contents_parsed = parsed.cssText if parsed else ''
        # if the css parsed, we're going to apply it (both preview & save)
        if not report.errors:
            res._call('applyStylesheet("%s"); '  %
                      stylesheet_contents_parsed.replace('"', r"\"").replace("\n", r"\n").replace("\r", r"\r"))
        if not report.errors and op == 'save':
            stylesheet_contents_user   = stylesheet_contents

            c.site.stylesheet_contents      = stylesheet_contents_parsed
            c.site.stylesheet_contents_user = stylesheet_contents_user

            c.site.stylesheet_hash = md5(stylesheet_contents_parsed).hexdigest()

            set_last_modified(c.site,'stylesheet_contents')
            tc.changed(c.site)
            c.site._commit()

            res._update('status', innerHTML = 'saved')
            res._update('validation-errors', innerHTML = '')

        elif op == 'preview':
            # try to find a link to use, otherwise give up and
            # return
            links = cssfilter.find_preview_links(c.site)
            if not links:
                # we're probably not going to be able to find any
                # comments, either; screw it
                return

            res._show('preview-table')

            # do a regular link
            cssfilter.rendered_link('preview_link_normal',
                                    res, links,
                                    media = 'off', compress=False)
            # now do one with media
            cssfilter.rendered_link('preview_link_media',
                                    res, links,
                                    media = 'on', compress=False)
            # do a compressed link
            cssfilter.rendered_link('preview_link_compressed',
                                    res, links,
                                    media = 'off', compress=True)
            # and do a comment
            comments = cssfilter.find_preview_comments(c.site)
            if not comments:
                return
            cssfilter.rendered_comment('preview_comment',res,comments)

    @Json
    @validate(VSrModerator(),
              VModhash(),
              name = VCssName('img_name'))
    def POST_delete_sr_img(self, res, name):
        """
        Called called upon requested delete on /about/stylesheet.
        Updates the site's image list, and causes the <li> which wraps
        the image to be hidden.
        """
        # just in case we need to kill this feature from XSS
        if g.css_killswitch:
            return self.abort(403,'forbidden')
        c.site.del_image(name)
        c.site._commit()
        # hide the image and it's container
        res._hide("img-li_%s" % name)
        # reset the status
        res._update('img-status', innerHTML = _("Deleted"))


    @Json
    @validate(VModhash(),
              link = VLink('article_id'),
              name = VCssName('img_name'))
    def POST_delete_link_img(self, res, link, name):
        """
        Updates the link's image list, and causes the <li> which wraps
        the image to be hidden.
        """
        # just in case we need to kill this feature from XSS
        if g.css_killswitch:
            return self.abort(403,'forbidden')
        link.del_image(name)
        link._commit()
        # hide the image and it's container
        res._hide("img-li_%s" % name)
        # reset the status
        res._update('img-status', innerHTML = _("Deleted"))

    @Json
    @validate(VSrModerator(),
              VModhash())
    def POST_delete_sr_header(self, res):
        """
        Called when the user request that the header on a sr be reset.
        """
        # just in case we need to kill this feature from XSS
        if g.css_killswitch:
            return self.abort(403,'forbidden')
        if c.site.header:
            c.site.header = None
            c.site._commit()
        # reset the header image on the page
        res._update('header-img', src = DefaultSR.header)
        # hide the button which started this
        res._hide  ('delete-img')
        # hide the preview box
        res._hide  ('img-preview-container')
        # reset the status boxes
        res._update('img-status', innerHTML = _("Deleted"))
        res._update('status', innerHTML = "")

    def render_cached(self, cache_key, render_cls, max_age, cache_time=0, *args, **kwargs):
        """Render content using client caching and server caching."""

        # Default the cache to be the same as our max age if not
        # supplied.
        cache_time = cache_time or max_age

        # Postfix the cache key with the subreddit name
        # This scopes all the caches by subreddit
        cache_key = cache_key + '-' + c.site.name

        # Get the etag and content from the cache.
        hit = g.rendercache.get(cache_key)
        if hit:
            etag, content = hit
        else:
            # Generate and cache the content along with an etag.
            content = render_cls(*args, **kwargs).render()
            etag = '"%s"' % datetime.utcnow().isoformat()
            g.rendercache.set(cache_key, (etag, content), time=cache_time)

        # Check if the client already has the correct content and
        # throw 304 if so. Note that we want to set the max age in the
        # 304 response, we can only do this by using the
        # pylons.response object just like the etag_cache fn does
        # within pylons (it sets the etag header).  Setting it on the
        # c.response won't work as c.response isn't used when an
        # exception is thrown.  Note also that setting it on the
        # pylons.response will send the max age in the 200 response
        # (just like the etag header is sent in the response).
        response.headers['Cache-Control'] = 'max-age=%d' % max_age
        etag_cache(etag)

        # Return full response using our cached info.
        c.response.content = content
        return c.response

    TWELVE_HOURS = 3600 * 12

    def GET_side_posts(self, *a, **kw):
        """Return HTML snippet of the recent posts for the side bar."""
        # Server side cache is also invalidated when new article is posted
        return self.render_cached('side-posts', RecentArticles, g.side_posts_max_age)

    def GET_side_comments(self, *a, **kw):
        """Return HTML snippet of the recent comments for the side bar."""
        # Server side cache is also invalidated when new comment is posted
        return self.render_cached('side-comments', RecentComments, g.side_comments_max_age, self.TWELVE_HOURS)

    def GET_side_open(self, *a, **kw):
        """Return HTML snippet of the most recent comment in an open thread for the side bar."""
        # Server side cache is also invalidated when new comment is posted
        return self.render_cached('side-open', RecentTagged, g.side_comments_max_age, tagtype = 'open_thread', title = 'Open Thread')

    def GET_side_quote(self, *a, **kw):
        """Return HTML snippet of the most recent comment in a rationality quote thread for the side bar."""
        # Server side cache is also invalidated when new comment is posted
        return self.render_cached('side-quote', RecentTagged, g.side_comments_max_age, tagtype = 'quotes', title = 'Rationality Quote')

    def GET_side_diary(self, *a, **kw):
        """Return HTML snippet of the most recent comment in a rationality diary thread for the side bar."""
        # Server side cache is also invalidated when new comment is posted
        return self.render_cached('side-diary', RecentTagged, g.side_comments_max_age, tagtype = 'group_rationality_diary', title = 'Rationality Diary')

    def GET_side_tags(self, *a, **kw):
        """Return HTML snippet of the tags for the side bar."""
        return self.render_cached('side-tags', TagCloud, g.side_tags_max_age)

    def GET_side_contributors(self, *a, **kw):
        """Return HTML snippet of the top contributors for the side bar."""
        return self.render_cached('side-contributors', TopContributors, g.side_contributors_max_age)

    def GET_side_meetups(self, *a, **kw):
        """Return HTML snippet of the upcoming meetups for the side bar."""
        ip = remote_addr(c.environ)
        location = Meetup.geoLocateIp(ip)
        # Key to group cached meetup pages with
        invalidating_key = g.rendercache.get_key_group_value(Meetup.group_cache_key())
        cache_key = "%s-side-meetups-%s" % (invalidating_key,ip)
        return self.render_cached(cache_key, UpcomingMeetups, g.side_meetups_max_age,
                                  cache_time=self.TWELVE_HOURS, location=location,
                                  max_distance=g.meetups_radius)

    def GET_side_monthly_contributors(self, *a, **kw):
        """Return HTML snippet of the top monthly contributors for the side bar."""
        return self.render_cached('side-monthly-contributors', TopMonthlyContributors, g.side_contributors_max_age)

    def GET_upload_sr_img(self, *a, **kw):
        """
        Completely unnecessary method which exists because safari can
        be dumb too.  On page reload after an image has been posted in
        safari, the iframe to which the request posted preserves the
        URL of the POST, and safari attempts to execute a GET against
        it.  The iframe is hidden, so what it returns is completely
        irrelevant.
        """
        return "nothing to see here."

    def GET_upload_link_img(self, *a, **kw):
        """
        As above
        """
        return "nothing to see here."

    def GET_front_recent_posts(self, *a, **kw):
        """Return HTML snippet of the recent promoted posts for the front page."""
        # Server side cache is also invalidated when new article is posted
        return self.render_cached('recent-promoted', RecentPromotedArticles, g.side_posts_max_age)

    def GET_front_meetups_map(self, *a, **kw):
        ip = remote_addr(c.environ)
        location = Meetup.geoLocateIp(ip)
        invalidating_key = g.rendercache.get_key_group_value(Meetup.group_cache_key())
        cache_key = "%s-front-meetups-%s" % (invalidating_key,ip)
        return self.render_cached(cache_key, MeetupsMap, g.side_meetups_max_age,
                                  cache_time=self.TWELVE_HOURS, location=location,
                                  max_distance=g.meetups_radius)

    @validate(link = VLink('article_id', redirect=False))
    def GET_article_navigation(self, link, *a, **kw):
      """Returns the article navigation fragment for the article specified"""
      author = Account._byID(link.author_id, data=True) if link else None
      return self.render_cached(
        'article_navigation_%s' % (link._id36 if link else None),
        ArticleNavigation, g.article_navigation_max_age,
        link=link, author=author
      )

    @validate(VModhash(),
              file = VLength('file', length=1024*500),
              name = VCssName("name"),
              header = nop('header'))
    def POST_upload_sr_img(self, file, header, name):
        """
        Called on /about/stylesheet when an image needs to be replaced
        or uploaded, as well as on /about/edit for updating the
        header.  Unlike every other POST in this controller, this
        method does not get called with Ajax but rather is from the
        original form POSTing to a hidden iFrame.  Unfortunately, this
        means the response needs to generate an page with a script tag
        to fire the requisite updates to the parent document, and,
        more importantly, that we can't use our normal toolkit for
        passing those responses back.

        The result of this function is a rendered UploadedImage()
        object in charge of firing the completedUploadImage() call in
        JS.
        """

        # default error list (default values will reset the errors in
        # the response if no error is raised)
        errors = dict(BAD_CSS_NAME = "", IMAGE_ERROR = "")
        try:
            cleaned = cssfilter.clean_image(file,'PNG')
            if header:
                num = None # there is one and only header, and it is unnumbered
            elif not name:
                # error if the name wasn't specified or didn't satisfy
                # the validator
                errors['BAD_CSS_NAME'] = _("Bad image name")
            else:
                num = c.site.add_image(name, max_num = g.max_sr_images)
                c.site._commit()

        except cssfilter.BadImage:
            # if the image doesn't clean up nicely, abort
            errors["IMAGE_ERROR"] = _("Bad image")
        except ValueError:
            # the add_image method will raise only on too many images
            errors['IMAGE_ERROR'] = (
                _("Too many images (you only get %d)") % g.max_sr_images)

        if any(errors.values()):
            return  UploadedImage("", "", "", errors = errors).render()
        else:
            # with the image num, save the image an upload to s3.  the
            # header image will be of the form "${c.site._fullname}.png"
            # while any other image will be ${c.site._fullname}_${num}.png
            new_url = cssfilter.save_sr_image(c.site, cleaned, num = num)
            if header:
                c.site.header = new_url
                c.site._commit()

            return UploadedImage(_('Saved'), new_url, name,
                                 errors = errors).render()


    @validate(VModhash(),
              link = VLink('article_id'),
              file = VLength('file', length=1024*500),
              name = VCssName("name"))
    def POST_upload_link_img(self, link, file, name):
        """
        Upload an image to a link

        The result of this function is a rendered UploadedImage()
        object in charge of firing the completedUploadImage() call in
        JS.
        """

        # default error list (default values will reset the errors in
        # the response if no error is raised)
        errors = dict(BAD_CSS_NAME = "", IMAGE_ERROR = "")
        try:
            cleaned = cssfilter.clean_image(file,'PNG')
            if not name:
                # error if the name wasn't specified or didn't satisfy
                # the validator
                errors['BAD_CSS_NAME'] = _("Bad image name")
            else:
                num = link.add_image(name, max_num = g.max_sr_images)
                link._commit()

        except cssfilter.BadImage:
            # if the image doesn't clean up nicely, abort
            errors["IMAGE_ERROR"] = _("Bad image")
        except ValueError:
            # the add_image method will raise only on too many images
            errors['IMAGE_ERROR'] = (
                _("Too many images (you only get %d)") % g.max_sr_images)

        if any(errors.values()):
            return  UploadedImage("", "", "", errors = errors).render()
        else:
            # with the image num, save the image an upload to s3.  the
            #  image will be of the form ${link._fullname}_${num}.png
            # Note save_sr_image expects the first argument to be a
            # subreddit, however that argument just needs to respond to
            # fullname, which links do.
            new_url = cssfilter.save_sr_image(link, cleaned, num = num)

            return UploadedImage(_('Saved'), new_url, name,
                                 errors = errors).render()


    @Json
    @validate(VAdmin(),
              VModhash(),
              VRatelimit(rate_user = True,
                         rate_ip = True,
                         prefix = 'create_reddit_'),
              sr = VByName('sr'),
              name = VSubredditName("name"),
              title = VSubredditTitle("title"),
              domain = VCnameDomain("domain"),
              description = VSubredditDesc("description"),
              lang = VLang("lang"),
              over_18 = VBoolean('over_18'),
              show_media = VBoolean('show_media'),
              type = VOneOf('type', ('public', 'private', 'restricted')),
              default_listing = VOneOf('default_listing', ListingController.listing_names())
              )
    def POST_site_admin(self, res, name ='', sr = None, **kw):
        redir = False
        kw = dict((k, v) for k, v in kw.iteritems()
                  if k in ('name', 'title', 'domain', 'description', 'over_18',
                           'show_media', 'type', 'lang', 'default_listing',))

        #if a user is banned, return rate-limit errors
        if c.user._spam:
            time = timeuntil(datetime.now(g.tz) + timedelta(seconds=600))
            c.errors.add(errors.RATELIMIT, {'time': time})

        domain = kw['domain']
        cname_sr = domain and Subreddit._by_domain(domain)
        if cname_sr and (not sr or sr != cname_sr):
                c.errors.add(errors.USED_CNAME)

        if not sr and res._chk_error(errors.RATELIMIT):
            pass
        elif not sr and res._chk_errors((errors.SUBREDDIT_EXISTS,
                                         errors.BAD_SR_NAME)):
            res._hide('example_name')
            res._focus('name')
        elif res._chk_errors((errors.NO_TITLE, errors.TITLE_TOO_LONG)):
            res._hide('example_title')
            res._focus('title')
        elif res._chk_error(errors.INVALID_OPTION):
            pass
        elif res._chk_errors((errors.BAD_CNAME, errors.USED_CNAME)):
            res._hide('example_domain')
            res._focus('domain')
        elif res._chk_error(errors.DESC_TOO_LONG):
            res._focus('description')

        res._update('status', innerHTML = '')

        if res.error:
            pass

        #creating a new reddit
        elif not sr:
            sr = Subreddit._create_and_subscribe(name, c.user, kw)

        #editting an existing reddit
        elif sr.is_moderator(c.user) or c.user_is_admin:
            #assume sr existed, or was just built
            clear_memo('subreddit._by_domain',
                       Subreddit, _force_unicode(sr.domain))
            for k, v in kw.iteritems():
                setattr(sr, k, v)
            sr._commit()
            clear_memo('subreddit._by_domain',
                       Subreddit, _force_unicode(sr.domain))

            # flag search indexer that something has changed
            tc.changed(sr)

            res._update('status', innerHTML = _('Saved'))


        if redir:
            res._redirect(redir)

    @Json
    @validate(VModhash(),
              VSrCanBan('id'),
              thing = VByName('id'))
    def POST_ban(self, res, thing):
        thing.moderator_banned = not c.user_is_admin
        thing.banner = c.user.name
        thing._commit()
        # NB: change table updated by reporting
        unreport(thing, correct=True, auto=False)

    @Json
    @validate(VModhash(),
              VSrCanBan('id'),
              thing = VByName('id'))
    def POST_unban(self, res, thing):
        # NB: change table updated by reporting
        unreport(thing, correct=False)

    @Json
    @validate(VModhash(),
              VSrCanBan('id'),
              thing = VByName('id'))
    def POST_ignore(self, res, thing):
        # NB: change table updated by reporting
        unreport(thing, correct=False)

    @Json
    @validate(VUser(),
              VModhash(),
              thing = VByName('id'))
    def POST_save(self, res, thing):
        r = thing._save(c.user)
        if g.write_query_queue:
            queries.new_savehide(r)

    @Json
    @validate(VUser(),
              VModhash(),
              thing = VByName('id'))
    def POST_unsave(self, res, thing):
        r = thing._unsave(c.user)
        if g.write_query_queue and r:
            queries.new_savehide(r)

    @Json
    @validate(VUser(),
              VModhash(),
              thing = VByName('id'))
    def POST_hide(self, res, thing):
        r = thing._hide(c.user)
        if g.write_query_queue:
            queries.new_savehide(r)

    @Json
    @validate(VUser(),
              VModhash(),
              thing = VByName('id'))
    def POST_unhide(self, res, thing):
        r = thing._unhide(c.user)
        if g.write_query_queue and r:
            queries.new_savehide(r)


    @Json
    @validate(link = VByName('link_id'),
              sort = VMenu('where', CommentSortMenu),
              children = VCommentIDs('children'),
              depth = VInt('depth', min = 0, max = 8),
              mc_id = nop('id'))
    def POST_morechildren(self, res, link, sort, children, depth, mc_id):
        if children:
            builder = CommentBuilder(link, CommentSortMenu.operator(sort), children)
            items = builder.get_items(starting_depth = depth, num = 20)
            def _children(cur_items):
                items = []
                for cm in cur_items:
                    items.append(cm)
                    if hasattr(cm, 'child'):
                        if hasattr(cm.child, 'things'):
                            items.extend(_children(cm.child.things))
                            cm.child = None
                        else:
                            items.append(cm.child)

                return items
            # assumes there is at least one child
#            a = _children(items[0].child.things)
            a = []
            for item in items:
                a.append(item)
                if hasattr(item, 'child'):
                    a.extend(_children(item.child.things))
                    item.child = None

            # the result is not always sufficient to replace the
            # morechildren link
            if mc_id not in [x._fullname for x in a]:
                res._hide('thingrow_' + str(mc_id))
            res._send_things(a)


    @validate(uh = nop('uh'),
              action = VOneOf('what', ('like', 'dislike', 'save')),
              links = VUrl(['u']))
    def GET_bookmarklet(self, action, uh, links):
        '''Controller for the functionality of the bookmarklets (not the distribution page)'''

        # the redirect handler will clobber the extension if not told otherwise
        c.extension = "png"

        if not c.user_is_loggedin:
            return self.redirect("/static/css_login.png")
        # check the modhash (or force them to get new bookmarlets)
        elif not c.user.valid_hash(uh) or not action:
            return self.redirect("/static/css_update.png")
        # unlike most cases, if not already submitted, error.
        elif errors.ALREADY_SUB in c.errors:
            # preserve the subreddit if not Default
            sr = c.site if not isinstance(c.site, FakeSubreddit) else None

            # check permissions on those links to make sure votes will count
            Subreddit.load_subreddits(links, return_dict = False)
            user = c.user if c.user_is_loggedin else None
            links = [l for l in links if l.subreddit_slow.can_view(user)]

            if links:
                if action in ['like', 'dislike']:
                    #vote up all of the links
                    for link in links:
                        v = Vote.vote(c.user, link, action == 'like', request.ip)
                        if g.write_query_queue:
                            queries.new_vote(v)
                elif action == 'save':
                    link = max(links, key = lambda x: x._score)
                    r = link._save(c.user)
                    if g.write_query_queue:
                        queries.new_savehide(r)
                return self.redirect("/static/css_%sd.png" % action)
        return self.redirect("/static/css_submit.png")


    @Json
    @validate(user = VUserWithEmail('name'))
    def POST_password(self, res, user):
        res._update('status', innerHTML = '')
        if res._chk_error(errors.USER_DOESNT_EXIST):
            res._focus('name')
        elif res._chk_error(errors.NO_EMAIL_FOR_USER):
            res._focus('name')
        else:
            emailer.password_email(user)
            res._success()

    @Json
    @validate(user = VCacheKey('reset', ('key', 'name')),
              key= nop('key'),
              password = VPassword(['passwd', 'passwd2']))
    def POST_resetpassword(self, res, user, key, password):
        res._update('status', innerHTML = '')
        if res._chk_error(errors.BAD_PASSWORD):
            res._focus('passwd')
        elif res._chk_error(errors.BAD_PASSWORD_MATCH):
            res._focus('passwd2')
        elif errors.BAD_USERNAME in c.errors:
            cache.delete(str('reset_%s' % key))
            return res._redirect('/password')
        elif user:
            cache.delete(str('reset_%s' % key))
            change_password(user, password)
            self._login(res, user, '/resetpassword')


    @Json
    @validate(VUser())
    def POST_frame(self, res):
        c.user.pref_frame = True
        c.user._commit()


    @Json
    @validate(VUser())
    def POST_noframe(self, res):
        c.user.pref_frame = False
        c.user._commit()


    @Json
    @validate(VUser(),
              where=nop('where'),
              sort = nop('sort'))
    def POST_sort(self, res, where, sort):
        if where.startswith('sort_'):
            setattr(c.user, where, sort)
        c.user._commit()

    @Json
    def POST_new_captcha(self, res, *a, **kw):
        res.captcha = dict(iden = get_iden(), refresh = True)

    @Json
    @validate(VAdmin(),
              l = nop('id'))
    def POST_deltranslator(self, res, l):
        lang, a = l.split('_')
        if a and Translator.exists(lang):
            tr = Translator(locale = lang)
            tr.author.remove(a)
            tr.save()


    @Json
    @validate(VUser(),
              VModhash(),
              action = VOneOf('action', ('sub', 'unsub')),
              sr = VByName('sr'))
    def POST_subscribe(self, res, action, sr):
        self._subscribe(sr, action == 'sub')

    def _subscribe(self, sr, sub):
        Subreddit.subscribe_defaults(c.user)

        if sub:
            if sr.add_subscriber(c.user):
                sr._incr('_ups', 1)
        else:
            if sr.remove_subscriber(c.user):
                sr._incr('_ups', -1)
        tc.changed(sr)


    @Json
    @validate(VAdmin(),
              lang = nop("id"))
    def POST_disable_lang(self, res, lang):
        if lang and Translator.exists(lang):
            tr = Translator(locale = lang)
            tr._is_enabled = False


    @Json
    @validate(VAdmin(),
              lang = nop("id"))
    def POST_enable_lang(self, res, lang):
        if lang and Translator.exists(lang):
            tr = Translator(locale = lang)
            tr._is_enabled = True

    def action_cookie(action):
        s = action + request.ip + request.user_agent
        return hashlib.sha1(s).hexdigest()


    @Json
    @validate(num_margin = VCssMeasure('num_margin'),
              mid_margin = VCssMeasure('mid_margin'),
              links = VFullNames('links'))
    def POST_fetch_links(self, res, num_margin, mid_margin, links):
        b = IDBuilder([l._fullname for l in links],
                      wrap = ListingController.builder_wrapper)
        l = OrganicListing(b)
        l.num_margin = num_margin
        l.mid_margin = mid_margin
        res.object = res._thing(l.listing(), action = 'populate')

    @Json
    @validate(VUser(),
              ui_elem = VOneOf('id', ('organic',)))
    def POST_disable_ui(self, res, ui_elem):
        if ui_elem:
            pref = "pref_%s" % ui_elem
            if getattr(c.user, pref):
                setattr(c.user, "pref_" + ui_elem, False)
                c.user._commit()

    @Json
    @validate(VSponsor(),
              thing = VByName('id'))
    def POST_promote(self, res, thing):
        promote(thing)

    @Json
    @validate(VSponsor(),
              thing = VByName('id'))
    def POST_unpromote(self, res, thing):
        unpromote(thing)

    @Json
    @validate(VSponsor(),
              ValidDomain('url'),
              ip               = ValidIP(),
              l                = VLink('link_id'),
              title            = VTitle('title'),
              url              = VUrl(['url', 'sr']),
              sr               = VSubmitSR('sr'),
              subscribers_only = VBoolean('subscribers_only'),
              disable_comments = VBoolean('disable_comments'),
              expire           = VOneOf('expire', ['nomodify', 'expirein', 'cancel']),
              timelimitlength  = VInt('timelimitlength',1,1000),
              timelimittype    = VOneOf('timelimittype',['hours','days','weeks']))
    def POST_edit_promo(self, res, ip,
                        title, url, sr, subscribers_only,
                        disable_comments,
                        expire = None, timelimitlength = None, timelimittype = None,
                        l = None):
        res._update('status', innerHTML = '')
        if isinstance(url, str):
            # VUrl may have modified the URL to make it valid, like
            # adding http://
            res._update('url', value=url)
        elif isinstance(url, tuple) and isinstance(url[0], Link):
            # there's already one or more links with this URL, but
            # we're allowing mutliple submissions, so we really just
            # want the URL
            url = url[0].url

        if res._chk_error(errors.NO_TITLE):
            res._focus('title')
        elif res._chk_errors((errors.NO_URL,errors.BAD_URL)):
            res._focus('url')
        elif (not l or url != l.url) and res._chk_error(errors.ALREADY_SUB):
            #if url == l.url, we're just editting something else
            res._focus('url')
        elif res._chk_error(errors.SUBREDDIT_NOEXIST) or res._chk_error(errors.SUBREDDIT_FORBIDDEN):
            res._focus('sr')
        elif expire == 'expirein' and res._chk_error(errors.BAD_NUMBER):
            res._focus('timelimitlength')
        elif l:
            l.title = title
            old_url = l.url
            l.url = url

            l.promoted_subscribersonly = subscribers_only
            l.disable_comments = disable_comments

            if expire == 'cancel':
                l.promote_until = None
            elif expire == 'expirein' and timelimitlength and timelimittype:
                l.promote_until = timefromnow("%d %s" % (timelimitlength, timelimittype))

            l._commit()
            l.update_url_cache(old_url)

            res._redirect('/promote/edit_promo/%s' % to36(l._id))
        else:
            l = Link._submit(title, url, c.user, sr, ip, False)

            if expire == 'expirein' and timelimitlength and timelimittype:
                promote_until = timefromnow("%d %s" % (timelimitlength, timelimittype))
            else:
                promote_until = None

            promote(l, subscribers_only = subscribers_only,
                    promote_until = promote_until,
                    disable_comments = disable_comments)

            res._redirect('/promote/edit_promo/%s' % to36(l._id))

    def GET_link_thumb(self, *a, **kw):
        """
        See GET_upload_sr_image for rationale
        """
        return "nothing to see here."

    @validate(VSponsor(),
              link = VByName('link_id'),
              file = VLength('file',500*1024))
    def POST_link_thumb(self, link=None, file=None):
        errors = dict(BAD_CSS_NAME = "", IMAGE_ERROR = "")

        try:
            force_thumbnail(link, file)
        except cssfilter.BadImage:
            # if the image doesn't clean up nicely, abort
            errors["IMAGE_ERROR"] = _("Bad image")

        if any(errors.values()):
            return  UploadedImage("", "", "upload", errors = errors).render()
        else:
            return UploadedImage(_('Saved'), thumbnail_url(link), "upload",
                                 errors = errors).render()


    @Json
    @validate(ids = VLinkFullnames('ids'))
    def POST_onload(self, res, ids, *a, **kw):
        if not ids:
            res.object = {}
            return

        links = {}

        # make sure that they are really promoted
        promoted = Link._by_fullname(ids, data = True, return_dict = False)
        promoted = [ l for l in promoted if l.promoted ]

        for l in promoted:
            links[l._fullname] = [
                tracking.PromotedLinkInfo.gen_url(fullname=l._fullname,
                                                  ip = request.ip),
                tracking.PromotedLinkClickInfo.gen_url(fullname = l._fullname,
                                                       dest = l.url,
                                                       ip = request.ip)
                ]
        res.object = links

########NEW FILE########
__FILENAME__ = buttons
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from reddit_base import RedditController
from r2.lib.pages import Button, ButtonNoBody, ButtonEmbed, ButtonLite, \
    ButtonDemoPanel, WidgetDemoPanel, Bookmarklets, BoringPage, Socialite
from r2.models import *
from r2.lib.strings import Score
from r2.lib.utils import tup
from pylons import c, request
from validator import *
from pylons.i18n import _

class ButtonsController(RedditController):
    def buttontype(self):
        b = request.get.get('t') or 1
        try: 
            return int(b)
        except ValueError:
            return 1

    def get_link(self, url):
        try:
            sr = None if isinstance(c.site, FakeSubreddit) else c.site
            links = tup(Link._by_url(url, sr))
            #find the one with the highest score
            return max(links, key = lambda x: x._score)
        except:
            #we don't want to return 500s in other people's pages.
            import traceback
            g.log.debug("FULLPATH: get_link error in buttons code")
            g.log.debug(traceback.format_exc())
        
    def wrap_link(self, link):
        if link and hasattr(link, "_fullname"):
            rs = c.render_style
            c.render_style = 'html'
            from r2.controllers.listingcontroller import ListingController
            link_builder = IDBuilder(link._fullname, wrap = ListingController.builder_wrapper)
            
            # link_listing will be the one-element listing at the top
            link_listing = LinkListing(link_builder, nextprev=False).listing()
            
            # link is a wrapped Link object
            c.render_style = rs
            return link_listing.things[0]

    @validate(url = VSanitizedUrl('url'),
              title = nop('title'),
              css = nop('css'),
              vote = VBoolean('vote', default=True),
              newwindow = VBoolean('newwindow'),
              width = VInt('width', 0, 800),
              link = VByName('id'))
    def GET_button_content(self, url, title, css, vote, newwindow, width, link):
        l = self.wrap_link(link or self.get_link(url))
        if l: url = l.url

        #disable css hack 
        if (css != 'http://blog.wired.com/css/redditsocial.css' and
            css != 'http://www.wired.com/css/redditsocial.css'): 
            css = None 

        bt = self.buttontype()
        if bt == 1:
            score_fmt = Score.safepoints
        else:
            score_fmt = Score.number_only
            
        page_handler = Button
        if not vote:
            page_handler = ButtonNoBody

        if newwindow:
            target = "_new"
        else:
            target = "_parent"

        c.response.content = page_handler(button=bt, css=css,
                                    score_fmt = score_fmt, link = l, 
                                    url=url, title=title,
                                    vote = vote, target = target,
                                    bgcolor=c.bgcolor, width=width).render()
        return c.response

    
    @validate(buttontype = VInt('t', 1, 5),
              url = VSanitizedUrl("url"),
              _height = VInt('height', 0, 300),
              _width = VInt('width', 0, 800))
    def GET_button_embed(self, buttontype, _height, _width, url):
        c.render_style = 'js'
        c.response_content_type = 'text/javascript; charset=UTF-8'

        buttontype = buttontype or 1
        width, height = ((120, 22), (51, 69), (69, 52), (51, 52), (600, 52))[min(buttontype - 1, 4)]
        if _width: width = _width
        if _height: height = _height

        bjs = ButtonEmbed(button=buttontype,
                          width=width,
                          height=height,
                          url = url,
                          referer = request.referer).render()
        # we doing want the JS to be cached!
        c.used_cache = True
        return self.sendjs(bjs, callback='', escape=False)

    @validate(buttonimage = VInt('i', 0, 14),
              url = VSanitizedUrl('url'),
              newwindow = VBoolean('newwindow', default = False),
              styled = VBoolean('styled', default=True))
    def GET_button_lite(self, buttonimage, url, styled, newwindow):
        c.render_style = 'js'
        c.response_content_type = 'text/javascript; charset=UTF-8'
        if not url:
            url = request.referer
        if newwindow:
            target = "_new"
        else:
            target = "_parent"

        l = self.wrap_link(self.get_link(url))
        image = 1 if buttonimage is None else buttonimage

        bjs = ButtonLite(image = image, link = l, url = l.url if l else url,
                         target = target, styled = styled).render()
        # we don't want the JS to be cached!
        c.used_cache = True
        return self.sendjs(bjs, callback='', escape=False)



    def GET_button_demo_page(self):
        return BoringPage(_("Reddit buttons"),
                          show_sidebar = False, 
                          content=ButtonDemoPanel()).render()


    def GET_widget_demo_page(self):
        return BoringPage(_("Reddit widget"),
                          show_sidebar = False, 
                          content=WidgetDemoPanel()).render()

    def GET_socialite_demo_page(self):
        return BoringPage(_("Socialite toolbar"),
                          show_sidebar = False, 
                          content=Socialite()).render()

    
    def GET_bookmarklets(self):
        return BoringPage(_("Bookmarklets"),
                          show_sidebar = False, 
                          content=Bookmarklets()).render()

        

########NEW FILE########
__FILENAME__ = captcha
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from reddit_base import RedditController
import StringIO
import r2.lib.captcha as captcha
import re
from pylons import c

class CaptchaController(RedditController):
    def GET_captchaimg(self, iden):
        iden = re.sub("\.png$", "", iden, re.IGNORECASE)
        image = captcha.get_image(iden)
        f = StringIO.StringIO()
        image.save(f, "PNG")
        return self.sendpng(f.getvalue())
    

########NEW FILE########
__FILENAME__ = embed
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from reddit_base import RedditController, proxyurl
from pylons import request
from r2.lib.pages import Embed, BoringPage
from pylons.i18n import _
from urllib2 import HTTPError
from pylons import c


def force_redirect(dest):
    def _force_redirect(self, *a, **kw):
        return self.redirect(dest)
    return _force_redirect

class EmbedController(RedditController):

    def rendercontent(self, content):
        if content.startswith("<!--TEMPLATE-->"):
            return BoringPage(_("Help"),
                              content = Embed(content=content),
                              show_sidebar = None,
                              space_compress = False).render()
        else:
            return content

    def renderurl(self):
        try:
            content = proxyurl("http://reddit.infogami.com"+request.fullpath)
            return self.rendercontent(content)
        except HTTPError, e:
            if e.code == 404:
                return self.abort404()
            else:
                print "error %s" % e.code
                print e.fp.read()

    GET_help = POST_help = renderurl

    GET_blog = force_redirect("http://blog.reddit.com/")

########NEW FILE########
__FILENAME__ = error
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
import os.path

import paste.fileapp
from pylons.middleware import error_document_template, media_path
from pylons import c, request, g
from pylons.i18n import _
import random as rand

try:
    # place all r2 specific imports in here.  If there is a code error, it'll get caught and
    # the stack trace won't be presented to the user in production
    from reddit_base import RedditController
    from r2.models.subreddit import Default, Subreddit
    from r2.lib import pages
    from r2.lib.strings import rand_strings
except Exception, e:
    if g.debug:
        # if debug mode, let the error filter up to pylons to be handled
        raise e
    else:
        # production environment: protect the code integrity!
        print "HuffmanEncodingError: make sure your python compiles before deploying, stupid!"
        # kill this app
        import os
        os._exit(1)
    
redditbroke =  \
'''<html>
  <head>
    <title>LessWrong broke!</title>
  </head>
  <body>
    <div style="margin: auto; text-align: center">
      <p>
        <a href="/">
          <img border="0" src="/static/youbrokeit.png" alt="Error encountered" />
        </a>
      </p>
        %s
  </body>
</html>
'''            

toofast =  \
'''<html>
  <head><title>service temporarily unavailable</title></head>
  <body>
    the service you request is temporarily unavailable. please try again later.
  </body>
</html>
'''            

class ErrorController(RedditController):
    """Generates error documents as and when they are required.

    The ErrorDocuments middleware forwards to ErrorController when error
    related status codes are returned from the application.

    This behaviour can be altered by changing the parameters to the
    ErrorDocuments middleware in your config/middleware.py file.
    """
    allowed_render_styles = ('html', 'xml', 'js', 'embed', '')
    def __before__(self):
        try:
            c.error_page = True
            RedditController.__before__(self)
        except:
            handle_awful_failure("Error occurred in ErrorController.__before__")

    def __after__(self): 
        try:
            RedditController.__after__(self)
        except:
            handle_awful_failure("Error occurred in ErrorController.__after__")

    def __call__(self, environ, start_response):
        try:
            return RedditController.__call__(self, environ, start_response)
        except:
            return handle_awful_failure("something really awful just happened.")


    def send403(self):
        c.response.status_code = 403
        c.site = Default
        title = _("Forbidden (%(domain)s)") % dict(domain=g.domain)
        return pages.BoringPage(title,  loginbox=False,
                                show_sidebar = False, 
                                content=pages.ErrorPage()).render()

    def send404(self):
        c.response.status_code = 404
        if c.site._spam and not c.user_is_admin:
            msg = _("This category has been banned.")
            res =  pages.BoringPage(msg, loginbox = False,
                                    show_sidebar = False, 
                                    content = pages.ErrorPage(message = msg))
            return res.render()
        else:
            ch=rand.choice(['a','b','c','d','e'])
            res = pages.BoringPage(_("Page not found"),
                                   loginbox=False,
                                   show_sidebar = False, 
                                   content=pages.UnfoundPage(choice=ch))
            return res.render()

    def GET_document(self):
        try:
            code =  request.GET.get('code', '')
            srname = request.GET.get('srname', '')
            if srname:
                c.site = Subreddit._by_name(srname)
            if c.render_style not in self.allowed_render_styles:
                return str(code)
            elif code == '403':
                return self.send403()
            elif code == '500':
                return redditbroke % rand_strings.sadmessages
            elif code == '503':
                c.response.status_code = 503
                c.response.headers['Retry-After'] = 1
                c.response.content = toofast
                return c.response
            elif c.site:
                return self.send404()
            else:
                return "page not found"
        except:
            return handle_awful_failure("something really bad just happened.")

def handle_awful_failure(fail_text):
    """
    Makes sure that no errors generated in the error handler percolate
    up to the user unless debug is enabled.
    """
    if g.debug:
        import sys
        s = sys.exc_info()
        # reraise the original error with the original stack trace
        raise s[1], None, s[2]
    try:
        # log the traceback, and flag the "path" as the error location
        import traceback
        g.log.error("FULLPATH: %s" % fail_text)
        g.log.error(traceback.format_exc())
        return redditbroke % fail_text
    except:
        # we are doomed.  Admit defeat
        return "This is an error that should never occur.  You win."

########NEW FILE########
__FILENAME__ = feedback
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from reddit_base import RedditController
from pylons import c, request
from pylons.i18n import _
from r2.lib.pages import FormPage, Feedback, Captcha

class FeedbackController(RedditController):

    def _feedback(self, name = '', email = '', message='', 
                replyto='', action=''):
        title = _("Inquire about advertising on LessWrong") if action else ''
        captcha = Captcha() if not c.user_is_loggedin \
            or c.user.needs_captcha() else None
        if request.get.has_key("done"):
            success = _("Thanks for your message! you should hear back from us shortly.")
        else:
            success = ''
            return FormPage(_("Advertise") if action == 'ad_inq' \
                                else _("Feedback"),
                        content = Feedback(captcha=captcha,
                                           message=message, 
                                           replyto=replyto,
                                           email=email, name=name, 
                                           success=success,
                                           action=action,
                                           title=title),
                        loginbox = False).render()


    def GET_ad_inq(self):
        return self._feedback(action='ad_inq')

    def GET_feedback(self):
        return self._feedback()

########NEW FILE########
__FILENAME__ = front
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is Reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
#
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from validator import *
from pylons.controllers.util import redirect_to
from pylons.i18n import _, ungettext
from reddit_base import RedditController, base_listing
from api import link_listing_by_url
from r2 import config
from r2.models import *
from r2.lib.pages import *
from r2.lib.menus import *
from r2.lib.filters import _force_unicode
from r2.lib.utils import to36, sanitize_url, check_cheating, title_to_url, query_string, UrlParser
from r2.lib.template_helpers import get_domain
from r2.lib.emailer import has_opted_out, Email
from r2.lib.db.operators import desc
from r2.lib.strings import strings
from r2.lib.solrsearch import RelatedSearchQuery, SubredditSearchQuery, LinkSearchQuery
import r2.lib.db.thing as thing
from listingcontroller import ListingController
from pylons import c, request

import random as rand
import re
import time as time_module
from urllib import quote_plus

class FrontController(RedditController):

    @validate(article = VLink('article'),
              comment = VCommentID('comment'))
    def GET_oldinfo(self, article, type, dest, rest=None, comment=''):
        """Legacy: supporting permalink pages from '06,
           and non-search-engine-friendly links"""
        if not (dest in ('comments','related','details')):
                dest = 'comments'
        if type == 'ancient':
            #this could go in config, but it should never change
            max_link_id = 10000000
            new_id = max_link_id - int(article._id)
            return self.redirect('/info/' + to36(new_id) + '/' + rest)
        if type == 'old':
            new_url = "/%s/%s/%s" % \
                      (dest, article._id36,
                       quote_plus(title_to_url(article.title).encode('utf-8')))
            if not c.default_sr:
                new_url = "/r/%s%s" % (c.site.name, new_url)
            if comment:
                new_url = new_url + "/%s" % comment._id36
            if c.extension:
                new_url = new_url + "/.%s" % c.extension

            new_url = new_url + query_string(request.get)

            # redirect should be smarter and handle extensions, etc.
            return self.redirect(new_url, code=301)

    def GET_random(self):
        """The Serendipity button"""
        n = rand.randint(0, 9)
        sort = 'new' if n > 5 else 'hot'
        links = c.site.get_links(sort, 'all')
        if isinstance(links, thing.Query):
                links._limit = 25
                links = [x._fullname for x in links]
        else:
            links = links[:25]
        if links:
            name = links[rand.randint(0, min(24, len(links)-1))]
            link = Link._by_fullname(name, data = True)
            return self.redirect(link.url)
        else:
            return self.redirect('/')

    def GET_password(self):
        """The 'what is my password' page"""
        return BoringPage(_("Password"), content=Password()).render()

    @validate(user = VCacheKey('reset', ('key', 'name')),
              key = nop('key'))
    def GET_resetpassword(self, user, key):
        """page hit once a user has been sent a password reset email
        to verify their identity before allowing them to update their
        password."""
        done = False
        if not key and request.referer:
            referer_path =  request.referer.split(g.domain)[-1]
            done = referer_path.startswith(request.fullpath)
        elif not user:
            return self.abort404()
        return BoringPage(_("Reset password"),
                          content=ResetPassword(key=key, done=done)).render()

    @validate(VAdmin(),
              article = VLink('article'))
    def GET_details(self, article):
        """The (now depricated) details page.  Content on this page
        has been subsubmed by the presence of the LinkInfoBar on the
        rightbox, so it is only useful for Admin-only wizardry."""
        return DetailsPage(link = article).render()


    @validate(article      = VLink('article'),
              comment      = VCommentID('comment'),
              context      = VInt('context', min = 0, max = 8),
              sort         = VMenu('controller', CommentSortMenu),
              num_comments = VMenu('controller', NumCommentsMenu))
    def GET_comments(self, article, comment, context, sort, num_comments):
        """Comment page for a given 'article'."""
        if comment and comment.link_id != article._id:
            return self.abort404()

        if not c.default_sr and c.site._id != article.sr_id:
            return self.redirect(article.make_permalink_slow(), 301)

        # moderator is either reddit's moderator or an admin
        is_moderator = c.user_is_loggedin and c.site.is_moderator(c.user) or c.user_is_admin
        if article._spam and not is_moderator:
            return self.abort404()

        if not article.subreddit_slow.can_view(c.user):
            abort(403, 'forbidden')

        #check for 304
        self.check_modified(article, 'comments')

        # if there is a focal comment, communicate down to comment_skeleton.html who
        # that will be
        if comment:
            c.focal_comment = comment._id36

        # check if we just came from the submit page
        infotext = None
        if request.get.get('already_submitted'):
            infotext = strings.already_submitted % article.resubmit_link()

        check_cheating('comments')

        # figure out number to show based on the menu
        user_num = c.user.pref_num_comments or g.num_comments
        num = g.max_comments if num_comments == 'true' else user_num

        # Override sort if the link has a default set
        if hasattr(article, 'comment_sort_order'):
            sort = article.comment_sort_order

        builder = CommentBuilder(article, CommentSortMenu.operator(sort),
                                 comment, context)
        listing = NestedListing(builder, num = num,
                                parent_name = article._fullname)

        displayPane = PaneStack()

        # if permalink page, add that message first to the content
        if comment:
            permamessage = PermalinkMessage(
                comment.make_anchored_permalink(
                    context = context + 1 if context else 1,
                    anchor  = 'comments'
                ),
                has_more_comments = hasattr(comment, 'parent_id')
            )
            displayPane.append(permamessage)

        # insert reply box only for logged in user
        if c.user_is_loggedin and article.subreddit_slow.can_comment(c.user):
            displayPane.append(CommentReplyBox())
            #no comment box for permalinks
            if not comment:
                displayPane.append(CommentReplyBox(link_name =
                                                   article._fullname))
        # finally add the comment listing
        displayPane.append(listing.listing())

        loc = None if c.focal_comment or context is not None else 'comments'

        if article.comments_enabled:
            sort_menu = CommentSortMenu(default = sort, type='dropdown2')
            if hasattr(article, 'comment_sort_order'):
                sort_menu.enabled = False
            nav_menus = [sort_menu,
                         NumCommentsMenu(article.num_comments,
                                         default=num_comments)]

            content = CommentListing(
                content = displayPane,
                num_comments = article.num_comments,
                nav_menus = nav_menus,
            )
        else:
            content = PaneStack()

        is_canonical = article.canonical_url.endswith(_force_unicode(request.path)) and not request.GET

        res = LinkInfoPage(link = article, comment = comment,
                           content = content,
                           infotext = infotext,
                           is_canonical = is_canonical).render()

        if c.user_is_loggedin:
            article._click(c.user)

        return res

    @validate(VUser(),
              location = nop("location"))
    def GET_prefs(self, location=''):
        """Preference page"""
        kwargs = { 'content': None,
                   'infotext': None }
        if not location or location == 'options':
            kwargs['content'] = PrefOptions(done=request.get.get('done'))
        elif location == 'friends':
            kwargs['content'] = PaneStack()
            kwargs['infotext'] = strings.friends % Friends.path
            kwargs['content'].append(FriendList())
        elif location == 'update':
            kwargs['content'] = PrefUpdate()
        elif location == 'delete':
            kwargs['content'] = PrefDelete()
        elif location == 'wikiaccount':
            if c.user.wiki_account is not None: redirect_to('/prefs')

            kwargs['content'] = PrefWiki()
            kwargs['sidewiki'] = False

        return PrefsPage(**kwargs).render()

    @validate(VAdmin(),
              name = nop('name'))
    def GET_newreddit(self, name):
        """Create a reddit form"""
        title = _('Create a category')
        content=CreateSubreddit(name = name or '', listings = ListingController.listing_names())
        res = FormPage(_("Create a category"),
                       content = content,
                       ).render()
        return res

    def GET_stylesheet(self):
        if hasattr(c.site,'stylesheet_contents') and not g.css_killswitch:
            self.check_modified(c.site,'stylesheet_contents')
            c.response_content_type = 'text/css'
            c.response.content =  c.site.stylesheet_contents
            return c.response
        else:
            return self.abort404()

    @base_listing
    @validate(location = nop('location'))
    def GET_editreddit(self, location, num, after, reverse, count):
        """Edit reddit form."""
        if isinstance(c.site, FakeSubreddit):
            return self.abort404()

        # moderator is either reddit's moderator or an admin
        is_moderator = c.user_is_loggedin and c.site.is_moderator(c.user) or c.user_is_admin

        if is_moderator and location == 'edit':
            pane = CreateSubreddit(site = c.site, listings = ListingController.listing_names())
        elif location == 'moderators':
            pane = ModList(editable = is_moderator)
        elif location == 'editors':
            pane = EditorList(editable = c.user_is_admin)
        elif is_moderator and location == 'banned':
            pane = BannedList(editable = is_moderator)
        elif location == 'contributors' and c.site.type != 'public':
            pane = ContributorList(editable = is_moderator)
        elif (location == 'stylesheet'
              and c.site.can_change_stylesheet(c.user)
              and not g.css_killswitch):
            if hasattr(c.site,'stylesheet_contents_user') and c.site.stylesheet_contents_user:
                stylesheet_contents = c.site.stylesheet_contents_user
            elif hasattr(c.site,'stylesheet_contents') and c.site.stylesheet_contents:
                stylesheet_contents = c.site.stylesheet_contents
            else:
                stylesheet_contents = ''
            pane = SubredditStylesheet(site = c.site,
                                       stylesheet_contents = stylesheet_contents)
        elif is_moderator and location == 'reports':
            links = Link._query(Link.c.reported != 0,
                                Link.c._spam == False)
            comments = Comment._query(Comment.c.reported != 0,
                                      Comment.c._spam == False)
            query = thing.Merge((links, comments),
                                Link.c.sr_id == c.site._id,
                                sort = desc('_date'),
                                data = True)

            builder = QueryBuilder(query, num = num, after = after,
                                   count = count, reverse = reverse,
                                   wrap = ListingController.builder_wrapper)
            listing = LinkListing(builder)
            pane = listing.listing()

        elif is_moderator and location == 'spam':
            links = Link._query(Link.c._spam == True)
            comments = Comment._query(Comment.c._spam == True)
            query = thing.Merge((links, comments),
                                Link.c.sr_id == c.site._id,
                                sort = desc('_date'),
                                data = True)

            builder = QueryBuilder(query, num = num, after = after,
                                   count = count, reverse = reverse,
                                   wrap = ListingController.builder_wrapper)
            listing = LinkListing(builder)
            pane = listing.listing()
        else:
            return self.abort404()

        return EditReddit(content = pane).render()

    # def GET_stats(self):
    #     """The stats page."""
    #     return BoringPage(_("Stats"), content = UserStats()).render()

    # filter for removing punctuation which could be interpreted as lucene syntax
    related_replace_regex = re.compile('[?\\&|!{}+~^()":*-]+')
    related_replace_with  = ' '

    @base_listing
    @validate(article = VLink('article'))
    def GET_related(self, num, article, after, reverse, count):
        """Related page: performs a search using title of article as
        the search query."""

        title = c.site.name + ((': ' + article.title) if hasattr(article, 'title') else '')

        query = self.related_replace_regex.sub(self.related_replace_with,
                                               article.title)
        if len(query) > 1024:
            # could get fancier and break this into words, but titles
            # longer than this are typically ascii art anyway
            query = query[0:1023]

        q = RelatedSearchQuery(query, ignore = [article._fullname])
        num, t, pane = self._search(q,
                                    num = num, after = after, reverse = reverse,
                                    count = count)

        return LinkInfoPage(link = article, content = pane).render()

    @base_listing
    @validate(query = nop('q'))
    def GET_search_reddits(self, query, reverse, after,  count, num):
        """Search reddits by title and description."""
        q = SubredditSearchQuery(query)

        num, t, spane = self._search(q, num = num, reverse = reverse,
                                     after = after, count = count)

        res = SubredditsPage(content=spane,
                             prev_search = query,
                             elapsed_time = t,
                             num_results = num,
                             title = _("Search results")).render()
        return res

    verify_langs_regex = re.compile(r"^[a-z][a-z](,[a-z][a-z])*$")
    @base_listing
    @validate(query = nop('q'),
              time = VMenu('action', TimeMenu, remember = False),
              sort = VMenu('sort', SearchSortMenu, remember = False),
              langs = nop('langs'))
    def GET_search(self, query, num, time, reverse, after, count, langs, sort):
        """Search links page."""
        if query and '.' in query:
            url = sanitize_url(query, require_scheme = True)
            if url:
                return self.redirect("/submit" + query_string({'url':url}))

        if langs and self.verify_langs_regex.match(langs):
            langs = langs.split(',')
        else:
            langs = c.content_langs

        subreddits = None
        authors = None
        if c.site == subreddit.Friends and c.user_is_loggedin and c.user.friends:
            authors = c.user.friends
        elif isinstance(c.site, MultiReddit):
            subreddits = c.site.sr_ids
        elif not isinstance(c.site, FakeSubreddit):
            subreddits = [c.site._id]

        q = LinkSearchQuery(q = query, timerange = time, langs = langs,
                            subreddits = subreddits, authors = authors,
                            sort = SearchSortMenu.operator(sort))

        num, t, spane = self._search(q, num = num, after = after, reverse = reverse,
                                     count = count)

        if not isinstance(c.site,FakeSubreddit) and not c.cname:
            all_reddits_link = "%s/search%s" % (subreddit.All.path,
                                                query_string({'q': query}))
            d =  {'reddit_name':      c.site.name,
                  'reddit_link':      "http://%s/"%get_domain(cname = c.cname),
                  'all_reddits_link': all_reddits_link}
            infotext = strings.searching_a_reddit % d
        else:
            infotext = None

        res = SearchPage(_('Search results'), query, t, num, content=spane,
                         nav_menus = [TimeMenu(default = time),
                                      SearchSortMenu(default=sort)],
                         infotext = infotext).render()

        return res

    def _search(self, query_obj, num, after, reverse, count=0):
        """Helper function for interfacing with search.  Basically a
        thin wrapper for SearchBuilder."""
        builder = SearchBuilder(query_obj,
                                after = after, num = num, reverse = reverse,
                                count = count,
                                wrap = ListingController.builder_wrapper)

        listing = LinkListing(builder, show_nums=True)

        # have to do it in two steps since total_num and timing are only
        # computed after fetch_more
        res = listing.listing()
        timing = time_module.time() - builder.start_time

        return builder.total_num, timing, res

    def GET_search_results(self):
          return GoogleSearchResults(_('Search results')).render()

    def GET_login(self):
        """The /login form.  No link to this page exists any more on
        the site (all actions invoking it now go through the login
        cover).  However, this page is still used for logging the user
        in during submission or voting from the bookmarklets."""

        # dest is the location to redirect to upon completion
        dest = request.get.get('dest','') or request.referer or '/'
        return LoginPage(dest = dest).render()

    def GET_logout(self):
        """wipe login cookie and redirect to front page."""
        self.logout()
        return self.redirect('/')


    @validate(VUser())
    def GET_adminon(self):
        """Enable admin interaction with site"""
        #check like this because c.user_is_admin is still false
        if not c.user.name in g.admins:
            return self.abort404()
        self.login(c.user, admin = True)

        dest = request.referer or '/'
        return self.redirect(dest)

    @validate(VAdmin())
    def GET_adminoff(self):
        """disable admin interaction with site."""
        if not c.user.name in g.admins:
            return self.abort404()
        self.login(c.user, admin = False)

        dest = request.referer or '/'
        return self.redirect(dest)

    def GET_validuser(self):
        """checks login cookie to verify that a user is logged in and
        returns their user name"""
        c.response_content_type = 'text/plain'
        if c.user_is_loggedin:
            return c.user.name
        else:
            return ''


    @validate(VUser(),
              can_submit = VSRSubmitPage(),
              url = VRequired('url', None),
              title = VRequired('title', None),
              tags = VTags('tags'))
    def GET_submit(self, can_submit, url, title, tags):
        """Submit form."""
        if not can_submit:
            return BoringPage(_("Not Enough Karma"),
                    infotext="You do not have enough karma to post.",
                    content=NotEnoughKarmaToPost()).render()

        if url and not request.get.get('resubmit'):
            # check to see if the url has already been submitted
            listing = link_listing_by_url(url)
            redirect_link = None
            if listing.things:
                # if there is only one submission, the operation is clear
                if len(listing.things) == 1:
                    redirect_link = listing.things[0]
                # if there is more than one, check the users' subscriptions
                else:
                    subscribed = [l for l in listing.things
                                  if c.user_is_loggedin
                                  and l.subreddit.is_subscriber_defaults(c.user)]

                    #if there is only 1 link to be displayed, just go there
                    if len(subscribed) == 1:
                        redirect_link = subscribed[0]
                    else:
                        infotext = strings.multiple_submitted % \
                                   listing.things[0].resubmit_link()
                        res = BoringPage(_("Seen it"),
                                         content = listing,
                                         infotext = infotext).render()
                        return res

            # we've found a link already.  Redirect to its permalink page
            if redirect_link:
                return self.redirect(redirect_link.already_submitted_link)

        captcha = Captcha(tabular=False) if c.user.needs_captcha() else None
        srs = Subreddit.submit_sr(c.user)

        # Set the default sr to the user's draft when creating a new article
        try:
            sr = Subreddit._by_name(c.user.draft_sr_name)
        except NotFound:
            sr = None

        return FormPage(_("Submit Article"),
                        content=NewLink(title=title or '',
                                        subreddits = srs,
                                        tags=tags,
                                        sr_id = sr._id if sr else None,
                                        captcha=captcha)).render()

    @validate(VUser(),
              VSRSubmitPage(),
              article = VSubmitLink('article'))
    def GET_editarticle(self, article):
        author = Account._byID(article.author_id, data=True)
        subreddits = Subreddit.submit_sr(author)
        article_sr = Subreddit._byID(article.sr_id)
        if c.user_is_admin:
            # Add this admins subreddits to the list
            subreddits = list(set(subreddits).union([article_sr] + Subreddit.submit_sr(c.user)))
        elif article_sr.is_editor(c.user) and c.user != author:
            # An editor can save to the current subreddit irrspective of the original author's karma
            subreddits = [sr for sr in Subreddit.submit_sr(c.user) if sr.is_editor(c.user)]

        captcha = Captcha(tabular=False) if c.user.needs_captcha() else None

        return FormPage(_("Edit article"),
                      content=EditLink(article, subreddits=subreddits, tags=article.tag_names(), captcha=captcha)).render()

    def _render_opt_in_out(self, msg_hash, leave):
        """Generates the form for an optin/optout page"""
        email = Email.handler.get_recipient(msg_hash)
        if not email:
            return self.abort404()
        sent = (has_opted_out(email) == leave)
        return BoringPage(_("Opt out") if leave else _("Welcome back"),
                          content = OptOut(email = email, leave = leave,
                                           sent = sent,
                                           msg_hash = msg_hash)).render()

    @validate(msg_hash = nop('x'))
    def GET_optout(self, msg_hash):
        """handles /mail/optout to add an email to the optout mailing
        list.  The actual email addition comes from the user posting
        the subsequently rendered form and is handled in
        ApiController.POST_optout."""
        return self._render_opt_in_out(msg_hash, True)

    @validate(msg_hash = nop('x'))
    def GET_optin(self, msg_hash):
        """handles /mail/optin to remove an email address from the
        optout list. The actual email removal comes from the user
        posting the subsequently rendered form and is handled in
        ApiController.POST_optin."""
        return self._render_opt_in_out(msg_hash, False)

    def GET_frame(self):
        """used for cname support.  makes a frame and
        puts the proper url as the frame source"""
        sub_domain = request.environ.get('sub_domain')
        original_path = request.environ.get('original_path')
        sr = Subreddit._by_domain(sub_domain)
        return Cnameframe(original_path, sr, sub_domain).render()


    def GET_framebuster(self):
        if c.site.domain and c.user_is_loggedin:
            u = UrlParser(c.site.path + "/frame")
            u.put_in_frame()
            c.cname = True
            return self.redirect(u.unparse())

        return "fail"

    def GET_catchall(self):
        return self.abort404()

    @validate(VUser())
    def GET_verifyemail(self):
        return BoringPage(_("Verify Email"), content=VerifyEmail(), sidewiki=False).render()

    @validate(VUser(),
              article = VSubmitLink('article', redirect=False))
    def GET_imagebrowser(self, article):
        return ImageBrowser(article).render()

    #XXX These 'redirect pages' should be generalised
    def _redirect_to_link(self, link_id):
        try:
            link = Link._byID(int(link_id, 36), data=True)
        except NotFound:
            return self.abort404()
        return self.redirect(link.make_permalink(subreddit.Default))

    def GET_about(self):
        try:
            return self._redirect_to_link(g.about_post_id)
        except AttributeError:
            return self.abort404()

    def GET_issues(self):
        try:
            return self._redirect_to_link(g.issues_post_id)
        except AttributeError:
            return self.abort404()

    def GET_blank(self):
        return ''

########NEW FILE########
__FILENAME__ = i18n
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from pylons import request, g
from reddit_base import RedditController
from api import Json
from r2.lib.pages import UnfoundPage, AdminTranslations, AdminPage

from r2.lib.translation import Translator, TranslatorTemplate, get_translator
from validator import *
from gettext import _translations

from r2.lib.wrapped import Wrapped

class VTranslator(Validator):
    def run(self, lang):
        if (not c.user_is_admin and
            (not c.user_is_loggedin or not lang or
             c.user.name not in Translator.get_author(lang))):
            abort(404, 'page not found')

class VTranslationEnabled(Validator):
    def run(self):
        if not g.translator:
            abort(403, 'forbidden')
            

class I18nController(RedditController):

    @validate(VTranslationEnabled(),
              VAdmin(),
              lang = nop('lang'),
              a = VExistingUname('name'))
    def POST_adduser(self, lang, a):
        if a and Translator.exists(lang):
            tr = get_translator(locale = lang)
            tr.author.add(a.name)
            tr.save()
        return self.redirect("/admin/i18n")


    @validate(VTranslationEnabled(),
              VAdmin())
    def GET_list(self):
        res = AdminPage(content = AdminTranslations(),
                        title = 'translate reddit').render()
        return res


    @validate(VTranslationEnabled(),
              VAdmin(),
              lang = nop('name'))
    def POST_new(self, lang):
        if lang and not Translator.exists(lang):
            tr = get_translator(locale = lang)
            tr.save()
        return self.redirect("/admin/i18n")
        

    @validate(VTranslationEnabled(),
              VTranslator('lang'),
              lang = nop('lang'))
    def GET_edit(self, lang):
        if not lang and c.user_is_admin:
            content = Wrapped(TranslatorTemplate())
        elif Translator.exists(lang):
            content = Wrapped(get_translator(locale = lang))
        else:
            content = UnfoundPage()
        res = AdminPage(content = content, 
                        title = 'translate reddit').render()
        return res

    @validate(VTranslationEnabled(),
              VTranslator('lang'),
              lang = nop('lang'))
    def GET_try(self, lang):
        if lang:
            tr = get_translator(locale = lang)
            tr.save(compile=True)

            tran_keys = _translations.keys()
            for key in tran_keys:
                if key.endswith(tr._out_file('mo')):
                    del _translations[key]

            return self.redirect("http://%s.%s/" %
                                 (lang, g.domain))
        return abort(404, 'page not found')

    @validate(VTranslationEnabled(),
              VTranslator('lang'),
              post = nop('post'),
              try_trans = nop('try'),
              lang = nop('lang'))
    def POST_edit(self, lang, post, try_trans):
        if (lang and not Translator.exists(lang)):
            return self.redirect('/admin/i18n')

        if lang:
            tr = get_translator(locale = lang)
        else:
            tr = TranslatorTemplate()

        enabled = set()
        for k, val in request.post.iteritems():
            if k.startswith('trans_'):
                k = k.split('_')
                # check if this is a translation string
                if k[1:] and tr.get(k[1]):
                    tr.set(k[1], val, indx = int(k[2] if k[2:] else -1))
                # check if this is an admin editing the source/comment lines
                elif c.user_is_admin and tr.sources.get(k[1]):
                    source = tr.sources.get(k[1])
                    tr.source_trans[source] = val
            elif c.user_is_admin and k.startswith('enabled_'):
                k = k.split('_')
                enabled.add(k[1])

        # update the enabled state of the buttons
        if c.user_is_admin and enabled:
            strings = set(tr.string_dict.keys())
            disabled = strings - enabled
            for s in strings:
                tr.set_enabled(s, True)
            for s in disabled:
                tr.set_enabled(s, False)

        if request.post.get('nplurals'):
            try:
                tr.plural_names = [request.post.get('pluralform_%d' % i) \
                                   for i in xrange(tr.nplurals)]
                tr.nplurals = int(request.post.get('nplurals'))
            except ValueError:
                pass
        if request.post.get('langname'):
            tr.name = request.post['langname']
        if request.post.get('enlangname'):
            tr.en_name = request.post['enlangname']
            
        tr.save(compile=bool(try_trans))
        
        if try_trans:
            tran_keys = _translations.keys()
            for key in tran_keys:
                if key.endswith(tr._out_file('mo')):
                     del _translations[key]

            return self.redirect("http://%s/?lang=%s" %
                                 (g.domain, lang))
        
        whereto = request.post.get('bttn_num', '')
        if whereto:
            whereto = 'bttn_num_%s' % whereto
        return self.redirect("/admin/i18n/edit/%s#%s" % (lang or '', whereto))
        return res
        

########NEW FILE########
__FILENAME__ = listingcontroller
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is Reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
#
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from reddit_base import RedditController, base_listing
from validator import *

from r2.models import *
from r2.lib.pages import *
from r2.lib.menus import NewMenu, TimeMenu, DashboardTimeMenu, SortMenu, RecSortMenu, TagSortMenu
from r2.lib.rising import get_rising
from r2.lib.wrapped import Wrapped
from r2.lib.normalized_hot import normalized_hot, get_hot
from r2.lib.recommendation import get_recommended
from r2.lib.db.thing import Query, Merge, Relations
from r2.lib.db import queries
from r2.lib.strings import Score
from r2.lib import organic
from r2.lib.solrsearch import SearchQuery
from r2.lib.utils import iters, check_cheating, timeago
from r2.lib.filters import _force_unicode
from r2.lib.wikipagecached import WikiPageThing

from admin import admin_profile_query

from pylons.i18n import _

import random
from datetime import datetime

class ListingController(RedditController):
    """Generalized controller for pages with lists of links."""

    # toggle skipping of links based on the users' save/hide/vote preferences
    skip = True

    # toggles showing numbers
    show_nums = True

    # any text that should be shown on the top of the page
    infotext = None

    # builder class to use to generate the listing. if none, we'll try
    # to figure it out based on the query type
    builder_cls = None

    # page title
    title_text = ''

    # login box, subreddit box, submit box, etc, visible
    show_sidebar = True

    # class (probably a subclass of Reddit) to use to render the page.
    render_cls = Reddit

    #extra parameters to send to the render_cls constructor
    render_params = {}

    _link_listings = None

    # Robot (search engine) directives
    robots = None

    @classmethod
    def link_listings(cls, key = None):
        # This is done to defer generation of the dictionary until after
        # the classes have been defined
        if not cls._link_listings:
            cls._link_listings = {
                'blessed'       : BlessedController,
                'hot'           : HotController,
                'new'           : NewController,
                'top'           : ToplinksController,
                'controversial' : BrowseController,
            }

        return cls._link_listings if not key else cls._link_listings[key]

    @classmethod
    def listing_names(cls):
        return sorted(cls.link_listings().keys())

    @property
    def menus(self):
        """list of menus underneat the header (e.g., sort, time, kind,
        etc) to be displayed on this listing page"""
        return []

    @property
    def top_filter(self):
      return None

    @property
    def header_sub_nav(self):
      buttons = []
      if c.default_sr:
        buttons.append(NamedButton("promoted"))
        buttons.append(NamedButton("new"))
      else:
        buttons.append(NamedButton("new", aliases = ["/"]))

      buttons.append(NamedButton('top'))
      if c.user_is_loggedin:
        buttons.append(NamedButton('saved'))
      return buttons

    @base_listing
    def build_listing(self, num, after, reverse, count):
        """uses the query() method to define the contents of the
        listing and renders the page self.render_cls(..).render() with
        the listing as contents"""
        self.num = num
        self.count = count
        self.after = after
        self.reverse = reverse

        if after is not None:
            self.robots = "noindex,follow"

        self.query_obj = self.query()
        self.builder_obj = self.builder()
        self.listing_obj = self.listing()
        content = self.content()
        res =  self.render_cls(content = content,
                               show_sidebar = self.show_sidebar,
                               nav_menus = self.menus,
                               title = self.title(),
                               infotext = self.infotext,
                               robots = self.robots,
                               top_filter = self.top_filter,
                               header_sub_nav = self.header_sub_nav,
                               **self.render_params).render()
        return res


    def content(self):
        """Renderable object which will end up as content of the render_cls"""
        return self.listing_obj

    def query(self):
        """Query to execute to generate the listing"""
        raise NotImplementedError

    def builder(self):
        #store the query itself so it can be used elsewhere
        if self.builder_cls:
            builder_cls = self.builder_cls
        elif isinstance(self.query_obj, Query):
            builder_cls = QueryBuilder
        elif isinstance(self.query_obj, SearchQuery):
            builder_cls = SearchBuilder
        elif isinstance(self.query_obj, iters):
            builder_cls = IDBuilder
        elif isinstance(self.query_obj, queries.CachedResults):
            builder_cls = IDBuilder

        b = builder_cls(self.query_obj,
                        num = self.num,
                        skip = self.skip,
                        after = self.after,
                        count = self.count,
                        reverse = self.reverse,
                        wrap = self.builder_wrapper)

        return b

    def listing(self):
        """Listing to generate from the builder"""
        listing = LinkListing(self.builder_obj, show_nums = self.show_nums)
        return listing.listing()

    def title(self):
        """Page <title>"""
        return "%s - %s" % (self.title_text, c.site.title)

    def rightbox(self):
        """Contents of the right box when rendering"""
        pass

    @staticmethod
    def builder_wrapper(thing):
        w = Wrapped(thing)

        if isinstance(thing, Link):
            if thing.promoted:
                w = Wrapped(thing)
                w.render_class = PromotedLink
                w.rowstyle = 'promoted link'

            elif c.user.pref_compress:
                w.render_class = LinkCompressed
                w.score_fmt = Score.points

        return w

    def GET_listing(self, **env):
        return self.build_listing(**env)

class FixListing(object):
    """When sorting by hotness, computing a listing when the before/after
    link has a hottness of 0 is very slow. This class avoids drawing
    next/prev links when that will happen."""
    fix_listing = True

    def listing(self):
        listing = ListingController.listing(self)

        if not self.fix_listing:
            return listing

        #404 existing bad pages
        if self.after and self.after._hot == 0:
            self.abort404()

        #don't draw next/prev links for
        if listing.things:
            if listing.things[-1]._hot == 0:
                listing.next = None

            if listing.things[0]._hot == 0:
                listing.prev = None

        return listing

class HotController(FixListing, ListingController):
    where = 'hot'
    title_text = _('Popular articles')

    def organic(self):
        o_links, pos, calculation_key = organic.organic_links(c.user)
        if o_links:
            # get links in proximity to pos
            l = min(len(o_links) - 3, 8)
            disp_links = [o_links[(i + pos) % len(o_links)] for i in xrange(-2, l)]

            b = IDBuilder(disp_links, wrap = self.builder_wrapper)
            o = OrganicListing(b,
                               org_links = o_links,
                               visible_link = o_links[pos],
                               max_num = self.listing_obj.max_num,
                               max_score = self.listing_obj.max_score).listing()

            if len(o.things) > 0:
                # only pass through a listing if the links made it
                # through our builder
                organic.update_pos(pos+1, calculation_key)

                return o


    def query(self):
        #no need to worry when working from the cache
        if g.use_query_cache or c.site == Default:
            self.fix_listing = False

        if c.site == Default:
            user = c.user if c.user_is_loggedin else None
            sr_ids = Subreddit.user_subreddits(user)
            return normalized_hot(sr_ids)
        #if not using the query_cache we still want cached front pages
        elif (not g.use_query_cache
              and not isinstance(c.site, FakeSubreddit)
              and self.after is None
              and self.count == 0):
            return [l._fullname for l in get_hot(c.site)]
        else:
            return c.site.get_links('hot', 'all')

    def content(self):
        return self.listing_obj

    def GET_listing(self, **env):
        self.infotext = request.get.get('deleted') and strings.user_deleted
        return ListingController.GET_listing(self, **env)

class SavedController(ListingController):
    where = 'saved'
    skip = False
    title_text = _('Saved')

    def query(self):
        return queries.get_saved(c.user, not c.user_is_admin)

    @validate(VUser())
    def GET_listing(self, **env):
        return ListingController.GET_listing(self, **env)

class ToplinksController(ListingController):
    where = 'toplinks'
    title_text = _('Top scoring links')

    def query(self):
        return c.site.get_links('toplinks', 'all')

    def GET_listing(self, **env):
        return ListingController.GET_listing(self, **env)

class BlessedController(ListingController):
    where = 'blessed'

    def query(self):
        return c.site.get_links('blessed', 'all')

    def title(self):
        return c.site.title

    def GET_listing(self, **env):
        return ListingController.GET_listing(self, **env)

# This used to be RootController, but renamed since there is a new root controller
class PromotedController(ListingController):
   def __before__(self):
       ListingController.__before__(self)
       controller = self.link_listings(c.site.default_listing)
       self.__class__ = controller

class NewController(ListingController):
    where = 'new'
    title_text = _('Newest Submissions')

    def query(self):
        return c.site.get_links('new', 'all')

    def GET_listing(self, **env):
        return ListingController.GET_listing(self, **env)

class RecentpostsController(NewController):
    where = 'recentposts'
    title_text = _('Recent Posts')

    @staticmethod
    def builder_wrapper(thing):
        w = Wrapped(thing)

        if isinstance(thing, Link):
            w.render_class = InlineArticle

        return w

    def content(self):
        return RecentArticlesPage(content=self.listing_obj)

    def GET_listing(self, **env):
        if not env.has_key('limit'):
            env['limit'] = 250
        return NewController.GET_listing(self, **env)

class TagController(ListingController):
    where = 'tag'
    title_text = _('Articles Tagged')

    @property
    def menus(self):
        return [TagSortMenu(default = self.sort)]

    def query(self):
        q = LinkTag._query(LinkTag.c._thing2_id == self._tag._id,
                           LinkTag.c._name == 'tag',
                           LinkTag.c._t1_deleted == False,
                           sort = TagSortMenu.operator(self.sort),
                           eager_load = True,
                           thing_data = not g.use_query_cache
                      )
        q.prewrap_fn = lambda x: x._thing1
        return q

    def builder(self):
        b = SubredditTagBuilder(self.query_obj,
                                num = self.num,
                                skip = self.skip,
                                after = self.after,
                                count = self.count,
                                reverse = self.reverse,
                                wrap = self.builder_wrapper,
                                sr_ids = [c.current_or_default_sr._id])
        return b

    @validate(tag = VTagByName('tag'), sort = VMenu('where', TagSortMenu))
    def GET_listing(self, tag, sort, **env):
        self._tag = tag
        self.sort = sort
        TagController.title_text = _('Articles Tagged') + u' \N{LEFT SINGLE QUOTATION MARK}' + unicode(tag.name) + u'\N{RIGHT SINGLE QUOTATION MARK}'
        return ListingController.GET_listing(self, **env)

# This class renders /top/
class BrowseController(ListingController):
    where = 'browse'

    @property
    def top_filter(self):
        return TimeMenu(default = self.time, title = _('Filter'), type='dropdown2')

    def query(self):
        return c.site.get_links(self.sort, self.time)

    # TODO: this is a hack with sort.
    @validate(sort = VOneOf('sort', ('top', 'controversial')),
              time = VMenu('where', TimeMenu, default_item='quarter'))
    def GET_listing(self, sort, time, **env):
        self.sort = sort
        if sort == 'top':
            self.title_text = _('Top scoring articles')
        elif sort == 'controversial':
            self.title_text = _('Most controversial articles')
        self.time = time
        return ListingController.GET_listing(self, **env)


class RandomrisingController(ListingController):
    where = 'randomrising'
    title_text = _('You\'re really bored now, eh?')

    def query(self):
        links = get_rising(c.site)

        if not links:
            # just pull from the new page if the rising page isn't
            # populated for some reason
            links = c.site.get_links('new', 'all')
            if isinstance(links, Query):
                links._limit = 200
                links = [x._fullname for x in links]

        random.shuffle(links)

        return links

class EditsController(ListingController):
    title_text = _('Recent Edits')

    def query(self):
        return Edit._query(sort = desc('_date'))


class MeetupslistingController(ListingController):
    title_text = _('Upcoming Meetups')
    render_cls = MeetupIndexPage

    @property
    def header_sub_nav(self):
	    return []

    def query(self):
        return Meetup.upcoming_meetups_by_timestamp()

class ByIDController(ListingController):
    title_text = _('API')

    def query(self):
        return self.names

    @validate(names = VLinkFullnames("names"))
    def GET_listing(self, names, **env):
        if not names:
            return self.abort404()
        self.names = names
        return ListingController.GET_listing(self, **env)


class RecommendedController(ListingController):
    where = 'recommended'
    title_text = _('Recommended for you')

    @property
    def menus(self):
        return [RecSortMenu(default = self.sort)]

    def query(self):
        return get_recommended(c.user._id, sort = self.sort)

    @validate(VUser(),
              sort = VMenu("controller", RecSortMenu))
    def GET_listing(self, sort, **env):
        self.sort = sort
        return ListingController.GET_listing(self, **env)

class UserController(ListingController):
    render_cls = ProfilePage
    skip = False
    show_nums = False

    def title(self):
        titles = {'overview': _("Overview for %(user)s - %(site)s"),
                  'comments': _("Comments by %(user)s - %(site)s"),
                  'submitted': _("Submitted by %(user)s - %(site)s"),
                  'liked': _("Liked by %(user)s - %(site)s"),
                  'disliked': _("Disliked by %(user)s - %(site)s"),
                  'hidden': _("Hidden by %(user)s - %(site)s"),
                  'drafts': _("Drafts for %(user)s - %(site)s")}
        title = titles.get(self.where, _('Profile for %(user)s - %(site)s')) \
            % dict(user = _force_unicode(self.vuser.name), site = c.site.title)
        return title

    def query(self):
        q = None

        if self.where == 'profile':
            q = object  # dummy value

        if self.where == 'overview':
            self.skip = True
            self.check_modified(self.vuser, 'overview')
            q = queries.get_overview(self.vuser, 'new', 'all')

        elif self.where == 'overviewrss':
            self.check_modified(self.vuser, 'overviewrss')
            q = queries.get_overview(self.vuser, 'new', 'all')

        elif self.where == 'comments':
            self.check_modified(self.vuser, 'commented')
            q = queries.get_comments(self.vuser, 'new', 'all')

        elif self.where == 'commentsrss':
            self.check_modified(self.vuser, 'commented')
            q = queries.get_comments(self.vuser, 'new', 'all')

        elif self.where == 'submitted':
            self.skip = True
            self.check_modified(self.vuser, 'submitted')
            q = queries.get_submitted(self.vuser, 'new', 'all')

        elif self.where in ('liked', 'disliked'):
            self.check_modified(self.vuser, self.where)
            if self.where == 'liked':
                q = queries.get_liked(self.vuser, not c.user_is_admin)
            else:
                q = queries.get_disliked(self.vuser, not c.user_is_admin)

        elif self.where == 'hidden':
            q = queries.get_hidden(self.vuser, not c.user_is_admin)

        elif self.where == 'drafts':
            q = queries.get_drafts(self.vuser)

        if q is None:
            return self.abort404()

        return q

    def builder(self):
        if self.where == 'profile':
            return PrecomputedBuilder([self.wikipage])
        elif self.where in ('comments', 'overview', 'submitted'):
            builder_cls = ContextualCommentBuilder
            b = builder_cls(self.query_obj,
                             num = self.num,
                             skip = self.skip,
                             after = self.after,
                             count = self.count,
                             reverse = self.reverse,
                             wrap = self.builder_wrapper,
                             sr_ids = [c.current_or_default_sr._id, Subreddit._by_name('discussion')._id,
                                       Subreddit._by_name('meetups')._id])
            return b
        else:
            return ListingController.builder(self)

    def check_wiki_maybe_redirect_url(self):
        # If the user has a wiki page, show it. Otherwise, redirect to the
        # overview page so people aren't greeted with an error message when
        # clicking on a username.
        config = {'url': WikiPageCached.get_url_for_user_page(self.vuser)}
        self.wikipage = WikiPageThing(config)
        if not self.wikipage.success:
            return '/user/{0}/overview/'.format(urllib.quote(self.vuser.name))

    @staticmethod
    def builder_wrapper(thing):
        thing = ListingController.builder_wrapper(thing)
        thing.show_response_to = True
        return thing

    @validate(vuser = VExistingUname('username'))
    def GET_listing(self, where, vuser, **env):
        self.where = where
        self.vuser = vuser
        self.render_params = {'user' : vuser}
        c.profilepage = True

        # the validator will ensure that vuser is a valid account
        if not vuser:
            return self.abort404()

        # pretend deleted users don't exist (although they are in the db still)
        if vuser._deleted:
            return self.abort404()

        # hide spammers profile pages
        if (not c.user_is_loggedin or
            (c.user._id != vuser._id and not c.user_is_admin)) \
               and vuser._spam:
            return self.abort404()

        if (where not in ('profile', 'overview', 'submitted', 'comments', 'overviewrss', 'commentsrss')
            and not votes_visible(vuser)):
            return self.abort404()

        check_cheating('user')

        if where == 'profile':
            url = self.check_wiki_maybe_redirect_url()
            if url is not None:
                return self.redirect(url, 303)

        return ListingController.GET_listing(self, **env)


class MessageController(ListingController):
    show_sidebar = True
    render_cls = MessagePage

    def title(self, where = None):
        if where is None:
            where = self.where
        return "%s: %s - %s" % (_('Messages'), _(where.title()), c.site.title)

    @staticmethod
    def builder_wrapper(thing):
        if isinstance(thing, Comment):
            p = thing.make_permalink_slow()
            f = thing._fullname
            w = Wrapped(thing)
            w.render_class = Message
            w.to_id = c.user._id
            w.subject = _('Comment Reply')
            w.was_comment = True
            w.permalink, w._fullname = p, f
            return w
        else:
            return ListingController.builder_wrapper(thing)

    def query(self):
        if self.where == 'inbox':
            q = queries.get_inbox(c.user)

            #reset the inbox
            if c.have_messages:
                c.user.msgtime = False
                c.user._commit()

        elif self.where == 'sent':
            q = queries.get_sent(c.user)

        return q

    def builder(self):
        # This is (almost) copied and pasted from ListingController.builder.
        b = QueryBuilder(self.query_obj,
                         num = self.num,
                         skip = self.skip,
                         after = self.after,
                         count = self.count,
                         reverse = self.reverse,
                         wrap = self.builder_wrapper,
                         keep_fn = lambda i: True)
        return b

    @validate(VUser())
    def GET_listing(self, where, **env):
        self.where = where
        c.msg_location = where
        return ListingController.GET_listing(self, **env)

    @validate(VUser(),
              to = nop('to'),
              subject = nop('subject'),
              message = nop('message'),
              success = nop('success'))
    def GET_compose(self, to, subject, message, success):
        captcha = Captcha() if c.user.needs_captcha() else None
        content = MessageCompose(to = to, subject = subject,
                                 captcha = captcha,
                                 message = message,
                                 success = success)
        return MessagePage(content = content, title = self.title('compose')).render()

class KarmaawardController(ListingController):
    show_sidebar = True

    def title(self, where = None):
        if where is None:
            where = 'blah'
        return "%s - %s" % (_('Karma Awards'), c.site.title)

    def content(self):
        return KarmaPage(content=self.listing_obj)

    @staticmethod
    def builder_wrapper(thing):
        if isinstance(thing, Comment):
            p = thing.make_permalink_slow()
            f = thing._fullname
            w = Wrapped(thing)
            w.render_class = Message
            w.to_id = c.user._id
            w.subject = _('Comment Reply')
            w.was_comment = True
            w.permalink, w._fullname = p, f
            return w
        else:
            return ListingController.builder_wrapper(thing)

    def query(self):
        q = Award._query(sort = desc('_date'), data = True)
        return q


    def builder(self):
        # This is (almost) copied and pasted from ListingController.builder.
        b = QueryBuilder(self.query_obj,
                         num = self.num,
                         skip = self.skip,
                         after = self.after,
                         count = self.count,
                         reverse = self.reverse,
                         wrap = self.builder_wrapper,
                         keep_fn = lambda i: True)
        return b

    def GET_listing(self, **env):
        return ListingController.GET_listing(self, **env)

    @validate(VUser(),
              to = nop('to'),
              amount = nop('amount'),
              reason = nop('reason'),
              success = nop('success'))
    def GET_award(self, to, amount, reason, success):
        if c.user_is_admin:
            captcha = Captcha() if c.user.needs_captcha() else None
            content = KarmaAward(to = to, amount = amount,
                                     captcha = captcha,
                                     reason = reason,
                                     success = success)
            return KarmaAwardPage(content = content, title = self.title('Award Karma')).render()
        else:
            return self.abort404()


class RedditsController(ListingController):
    render_cls = SubredditsPage

    def title(self):
        return _('Categories')

    def query(self):
        if self.where == 'banned' and c.user_is_admin:
            reddits = Subreddit._query(Subreddit.c._spam == True,
                                       sort = desc('_date'))
        else:
            reddits = Subreddit._query()
            if self.where == 'new':
                reddits._sort = desc('_date')
            else:
                reddits._sort = desc('_downs')
            if c.content_langs != 'all':
                reddits._filter(Subreddit.c.lang == c.content_langs)
            if not c.over18:
                reddits._filter(Subreddit.c.over_18 == False)

        return reddits
    def GET_listing(self, where, **env):
        self.where = where
        return ListingController.GET_listing(self, **env)

class MyredditsController(ListingController):
    render_cls = MySubredditsPage

    @property
    def menus(self):
        buttons = (NavButton(plurals.subscriber,  'subscriber'),
                    NavButton(plurals.contributor, 'contributor'),
                    NavButton(plurals.moderator,   'moderator'))

        return [NavMenu(buttons, base_path = '/categories/mine/', default = 'subscriber', type = "flatlist")]

    def title(self):
        return _('Categories: ') + self.where

    def query(self):
        reddits = SRMember._query(SRMember.c._name == self.where,
                                  SRMember.c._thing2_id == c.user._id,
                                  #hack to prevent the query from
                                  #adding it's own date
                                  sort = (desc('_t1_ups'), desc('_t1_date')),
                                  eager_load = True,
                                  thing_data = True)
        reddits.prewrap_fn = lambda x: x._thing1
        return reddits

    def content(self):
        user = c.user if c.user_is_loggedin else None
        num_subscriptions = len(Subreddit.reverse_subscriber_ids(user))
        if self.where == 'subscriber' and num_subscriptions == 0:
            message = strings.sr_messages['empty']
        else:
            message = strings.sr_messages.get(self.where)

        stack = PaneStack()

        if message:
            stack.append(InfoBar(message=message))

        stack.append(self.listing_obj)

        return stack

    @validate(VUser())
    def GET_listing(self, where, **env):
        self.where = where
        return ListingController.GET_listing(self, **env)

class CommentsController(ListingController):
    title_text = _('Comments')
    builder_cls = UnbannedCommentBuilder
    show_nums = False

    @property
    def header_sub_nav(self):
	    return [NamedButton("newcomments", dest="comments"), NamedButton("topcomments")]

    def query(self):
        q = Comment._query(Comment.c._spam == (True,False),
                           Comment.c.sr_id == c.current_or_default_sr._id,
                           sort = desc('_date'), data = True)
        if not c.user_is_admin:
            q._filter(Comment.c._spam == False)

        return q

    def builder(self):
        if c.user.pref_show_parent_comments:
            builder_cls = ContextualCommentBuilder
        else:
            builder_cls = UnbannedCommentBuilder
        b = builder_cls(self.query_obj,
                             num = self.num,
                             skip = self.skip,
                             after = self.after,
                             count = self.count,
                             reverse = self.reverse,
                             wrap = self.builder_wrapper,
                             sr_ids = [c.current_or_default_sr._id])
        return b

    @staticmethod
    def builder_wrapper(thing):
        thing = ListingController.builder_wrapper(thing)
        if not c.user.pref_show_parent_comments:
            # In other words, if we're using UnbannedCommentBuilder rather
            # than ContextualCommentBuilder
            thing.show_response_to = True
        return thing


    def content(self):
        ps = PaneStack()
        ps.append(CommentReplyBox())
        ps.append(self.listing_obj)
        return ps

    def GET_listing(self, **env):
        c.full_comment_listing = True
        if not env.has_key('limit'):
            env['limit'] = 2 * c.user.pref_numsites
        return ListingController.GET_listing(self, **env)

class TopcommentsController(CommentsController):
	title_text = _('Top Comments')
	builder_cls = UnbannedCommentBuilder

	def query(self):
		q = Comment._query(Comment.c._spam == (True,False),
				Comment.c.sr_id == c.current_or_default_sr._id,
				sort = desc('_ups'), data = True)
		if not c.user_is_admin:
			q._filter(Comment.c._spam == False)

		if self.time != 'all':
			q._filter(queries.db_times[self.time])

		return q

	@property
	def top_filter(self):
		return TimeMenu(default = self.time, title = _('Filter'), type='dropdown2')

	@validate(time = VMenu('where', TimeMenu))
	def GET_listing(self, time, **env):
		self.time = time
		return CommentsController.GET_listing(self, **env)

def last_dashboard_visit():
    hc_key = "dashboard_visit-%s" % c.user.name
    cache_visit = g.permacache.get(hc_key, None)
    if cache_visit:
        return cache_visit
    else:
        last_visit = c.user.dashboard_visit
        g.permacache.set(hc_key, last_visit, time = int(g.dashboard_visits_period)) 
        c.user.dashboard_visit = datetime.now(g.tz)
        c.user._commit()
        return last_visit

class InterestingcommentsController(CommentsController):
    title_text = _('Leading Comments')
    builder_cls = UnbannedCommentBuilder

    @property
    def header_sub_nav(self):
	    return [NamedButton("leadingsubscribed", dest="dashboard/subscribed"),
                NamedButton("leadingposts", dest="dashboard/posts"),
                NamedButton("leadingcomments", dest="dashboard/comments")]

    def query(self):
        q = Comment._query(Comment.c._spam == (True,False),
                           sort = desc('_interestingness'),
                           eager_load = True, data = True)
        if not c.user_is_admin:
            q._filter(Comment.c._spam == False)

        if self.time == 'last':
            q._filter(Thing.c._date >= last_dashboard_visit())
        elif self.time != 'all':
            q._filter(queries.db_times[self.time])

        return q

    def builder(self):
        b = self.builder_cls(self.query_obj,
                             num = self.num,
                             skip = self.skip,
                             after = self.after,
                             count = self.count,
                             reverse = self.reverse,
                             wrap = self.builder_wrapper,
                             sr_ids = [c.current_or_default_sr._id, Subreddit._by_name('discussion')._id])
        return b

    @property
    def top_filter(self):
        return DashboardTimeMenu(default = self.time, title = _('Filter'), type='dropdown2')

    @validate(VUser(),
              time = VMenu('where', DashboardTimeMenu))
    def GET_listing(self, time, **env):
        self.time = time
        return CommentsController.GET_listing(self, **env)

class InterestingsubscribedController(CommentsController):
    title_text = _('Leading Subscribed Comments')
    builder_cls = UnbannedCommentBuilder

    @property
    def header_sub_nav(self):
	    return [NamedButton("leadingsubscribed", dest="dashboard/subscribed"),
                NamedButton("leadingposts", dest="dashboard/posts"),
                NamedButton("leadingcomments", dest="dashboard/comments")]

    def query(self):
        q = SubscriptionStorage._query(SubscriptionStorage.c._thing1_id == c.user._id,
                                       SubscriptionStorage.c._t2_deleted == False,
                                       SubscriptionStorage.c._name == 'subscriptionstorage',
                                       sort = desc('_t2_interestingness'),
                                       eager_load = True,
                                       thing_data = not g.use_query_cache
                                       )
        if not c.user_is_admin:
            q._filter(SubscriptionStorage.c._t2_spam == False)

        q.prewrap_fn = lambda x: x._thing2

        if self.time == 'last':
            q._filter(SubscriptionStorage.c._date >= last_dashboard_visit())
        elif self.time != 'all':
            q._filter(SubscriptionStorage.c._date >= timeago(queries.relation_db_times[self.time]))

        return q

    def builder(self):
        b = self.builder_cls(self.query_obj,
                             num = self.num,
                             skip = self.skip,
                             after = self.after,
                             count = self.count,
                             reverse = self.reverse,
                             wrap = self.builder_wrapper,
                             sr_ids = [c.current_or_default_sr._id, Subreddit._by_name('discussion')._id])
        return b

    @property
    def top_filter(self):
        return DashboardTimeMenu(default = self.time, title = _('Filter'), type='dropdown2')

    @validate(VUser(),
              time = VMenu('where', DashboardTimeMenu))
    def GET_listing(self, time, **env):
        self.time = time
        return CommentsController.GET_listing(self, **env)

class InterestingpostsController(CommentsController):
    title_text = _('Leading Posts')
    builder_cls = QueryBuilder

    @property
    def header_sub_nav(self):
	    return [NamedButton("leadingsubscribed", dest="dashboard/subscribed"),
                NamedButton("leadingposts", dest="dashboard/posts"),
                NamedButton("leadingcomments", dest="dashboard/comments")]

    def query(self):
        q = Link._query(Link.c._spam == (True,False),
                        sort = desc('_interestingness'),
                        eager_load = True, data = True)
        if not c.user_is_admin:
            q._filter(Link.c._spam == False)

        if self.time == 'last':
            q._filter(Thing.c._date >= last_dashboard_visit())
        elif self.time != 'all':
            q._filter(queries.db_times[self.time])

        return q

    def builder(self):
        b = self.builder_cls(self.query_obj,
                             num = self.num,
                             skip = self.skip,
                             after = self.after,
                             count = self.count,
                             reverse = self.reverse,
                             wrap = self.builder_wrapper,
                             sr_ids = [c.current_or_default_sr._id, Subreddit._by_name('discussion')._id])
        return b

    @property
    def top_filter(self):
        return DashboardTimeMenu(default = self.time, title = _('Filter'), type='dropdown2')

    @validate(VUser(),
              time = VMenu('where', DashboardTimeMenu))
    def GET_listing(self, time, **env):
        self.time = time
        return CommentsController.GET_listing(self, **env)

########NEW FILE########
__FILENAME__ = meetupscontroller
from datetime import datetime, timedelta
import json

from mako.template import Template
from pylons.i18n import _
from pylons import c,g,request

from reddit_base import RedditController
from r2.lib.errors import errors
from r2.lib.filters import python_websafe
from r2.lib.jsonresponse import Json
from r2.lib.menus import CommentSortMenu,NumCommentsMenu
from r2.lib.pages import BoringPage, ShowMeetup, NewMeetup, EditMeetup, PaneStack, CommentListing, LinkInfoPage, CommentReplyBox, NotEnoughKarmaToPost
from r2.models import Meetup,Link,Subreddit,CommentBuilder,PendingJob
from r2.models.listing import NestedListing
from validator import validate, VUser, VModhash, VRequired, VMeetup, VEditMeetup, VFloat, ValueOrBlank, ValidIP, VMenu, VCreateMeetup, VTimestamp
from routes.util import url_for


def meetup_article_text(meetup):
  t = Template(filename="r2/templates/showmeetup.html", output_encoding='utf-8', encoding_errors='replace')
  res = t.get_def("meetup_info").render_unicode(meetup=meetup)

  url = url_for(controller='meetups',action='show',id=meetup._id36)
  title = python_websafe(meetup.title)
  hdr = u"<h2>Discussion article for the meetup : <a href='%s'>%s</a></h2>"%(url,title)
  return hdr+res+hdr

def meetup_article_title(meetup):
  return "Meetup : %s"%meetup.title

class MeetupsController(RedditController):
  def response_func(self, **kw):
    return self.sendstring(json.dumps(kw))

  @validate(VUser(), 
            VCreateMeetup(),
            title = ValueOrBlank('title'),
            description = ValueOrBlank('description'),
            location = ValueOrBlank('location'),
            latitude = ValueOrBlank('latitude'),
            longitude = ValueOrBlank('longitude'),
            timestamp = ValueOrBlank('timestamp'),
            tzoffset = ValueOrBlank('tzoffset'))
  def GET_new(self, *a, **kw):
    return BoringPage(pagename = 'New Meetup', content = NewMeetup(*a, **kw)).render()

  @Json
  @validate(VUser(),
            VCreateMeetup(),
            VModhash(),
            ip = ValidIP(),
            title = VRequired('title', errors.NO_TITLE),
            description = VRequired('description', errors.NO_DESCRIPTION),
            location = VRequired('location', errors.NO_LOCATION),
            latitude = VFloat('latitude', error=errors.NO_LOCATION),
            longitude = VFloat('longitude', error=errors.NO_LOCATION),
            timestamp = VTimestamp('timestamp'),
            tzoffset = VFloat('tzoffset', error=errors.INVALID_DATE))
  def POST_create(self, res, title, description, location, latitude, longitude, timestamp, tzoffset, ip):
    if res._chk_error(errors.NO_TITLE):
      res._chk_error(errors.TITLE_TOO_LONG)
      res._focus('title')

    res._chk_errors((errors.NO_LOCATION,
                     errors.NO_DESCRIPTION,
                     errors.INVALID_DATE,
                     errors.NO_DATE))

    if res.error: return

    meetup = Meetup(
      author_id = c.user._id,

      title = title,
      description = description,

      location = location,
      latitude = latitude,
      longitude = longitude,

      timestamp = timestamp,
      tzoffset = tzoffset
    )

    # Expire all meetups in the render cache
    g.rendercache.invalidate_key_group(Meetup.group_cache_key())

    meetup._commit()

    l = Link._submit(meetup_article_title(meetup), meetup_article_text(meetup),
                     c.user, Subreddit._by_name('meetups'),ip, [])

    l.meetup = meetup._id36
    l._commit()
    meetup.assoc_link = l._id
    meetup._commit()

    when = datetime.now(g.tz) + timedelta(0, 3600)  # Leave a short window of time before notification, in case
                                                    # the meetup is edited/deleted soon after its creation
    PendingJob.store(when, 'process_new_meetup', {'meetup_id': meetup._id})

    #update the queries
    if g.write_query_queue:
      queries.new_link(l)

    res._redirect(url_for(action='show', id=meetup._id36))

  @Json
  @validate(VUser(),
            VModhash(),
            meetup = VEditMeetup('id'),
            title = VRequired('title', errors.NO_TITLE),
            description = VRequired('description', errors.NO_DESCRIPTION),
            location = VRequired('location', errors.NO_LOCATION),
            latitude = VFloat('latitude', error=errors.NO_LOCATION),
            longitude = VFloat('longitude', error=errors.NO_LOCATION),
            timestamp = VTimestamp('timestamp'),
            tzoffset = VFloat('tzoffset', error=errors.INVALID_DATE))
  def POST_update(self, res, meetup, title, description, location, latitude, longitude, timestamp, tzoffset):
    if res._chk_error(errors.NO_TITLE):
      res._chk_error(errors.TITLE_TOO_LONG)
      res._focus('title')

    res._chk_errors((errors.NO_LOCATION,
                     errors.NO_DESCRIPTION,
                     errors.INVALID_DATE,
                     errors.NO_DATE))

    if res.error: return

    meetup.title = title
    meetup.description = description

    meetup.location = location
    meetup.latitude = latitude
    meetup.longitude = longitude

    meetup.timestamp = timestamp
    meetup.tzoffset = tzoffset

    # Expire all meetups in the render cache
    g.rendercache.invalidate_key_group(Meetup.group_cache_key())

    meetup._commit()

    # Update the linked article
    article = Link._byID(meetup.assoc_link)
    article._load()
    article_old_url = article.url
    article.title = meetup_article_title(meetup)
    article.article = meetup_article_text(meetup)
    article._commit()
    article.update_url_cache(article_old_url)


    res._redirect(url_for(action='show', id=meetup._id36))

  @validate(VUser(),
            meetup = VEditMeetup('id'))
  def GET_edit(self, meetup):
    return BoringPage(pagename = 'Edit Meetup', content = EditMeetup(meetup,
                                                                     title=meetup.title,
                                                                     description=meetup.description,
                                                                     location=meetup.location,
                                                                     latitude=meetup.latitude,
                                                                     longitude=meetup.longitude,
                                                                     timestamp=int(meetup.timestamp * 1000),
                                                                     tzoffset=meetup.tzoffset)).render()

  # Show a meetup.  Most of this code was coped from GET_comments in front.py
  @validate(meetup = VMeetup('id'),
            sort         = VMenu('controller', CommentSortMenu),
            num_comments = VMenu('controller', NumCommentsMenu))
  def GET_show(self, meetup, sort, num_comments):
    article = Link._byID(meetup.assoc_link)

    # figure out number to show based on the menu
    user_num = c.user.pref_num_comments or g.num_comments
    num = g.max_comments if num_comments == 'true' else user_num

    builder = CommentBuilder(article, CommentSortMenu.operator(sort), None, None)
    listing = NestedListing(builder, num=num, parent_name = article._fullname)
    displayPane = PaneStack()
    
    # insert reply box only for logged in user
    if c.user_is_loggedin:
      displayPane.append(CommentReplyBox())
      displayPane.append(CommentReplyBox(link_name = 
                                         article._fullname))

    # finally add the comment listing
    displayPane.append(listing.listing())

    sort_menu = CommentSortMenu(default = sort, type='dropdown2')
    nav_menus = [sort_menu,
                 NumCommentsMenu(article.num_comments,
                                 default=num_comments)]

    content = CommentListing(
      content = displayPane,
      num_comments = article.num_comments,
      nav_menus = nav_menus,
      )


    # Update last viewed time, and return the previous last viewed time.  Actually tracked on the article
    lastViewed = None
    if c.user_is_loggedin:
      clicked = article._getLastClickTime(c.user)
      lastViewed = clicked._date if clicked else None
      article._click(c.user)

    res = ShowMeetup(meetup = meetup, content = content, 
                     fullname=article._fullname,
                     lastViewed = lastViewed)

    return BoringPage(pagename = meetup.title, 
                      content = res,
                      body_class = 'meetup').render()



########NEW FILE########
__FILENAME__ = post
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.lib.pages import *
from api import ApiController
from r2.lib.utils import Storage, query_string, UrlParser
from r2.lib.emailer import opt_in, opt_out
from pylons import request, c, g
from validator import *
from pylons.i18n import _
import hashlib

def to_referer(func, **params):
    def _to_referer(self, *a, **kw):
        res = func(self, *a, **kw)
        dest = res.get('redirect') or request.referer or '/'
        return self.redirect(dest + query_string(params))        
    return _to_referer


class PostController(ApiController):
    def response_func(self, **kw):
        return Storage(**kw)

#TODO: feature disabled for now
#     @to_referer
#     @validate(VUser(),
#               key = VOneOf('key', ('pref_bio','pref_location',
#                                    'pref_url')),
#               value = nop('value'))
#     def POST_user_desc(self, key, value):
#         setattr(c.user, key, value)
#         c.user._commit()
#         return {}

    def set_options(self, all_langs, pref_lang, **kw):
        if c.errors.errors:
            raise "Options are invalid"

        if all_langs == 'all':
            langs = 'all'
        elif all_langs == 'some':
            langs = []
            for lang in g.all_languages:
                if request.post.get('lang-' + lang):
                    langs.append(str(lang)) #unicode
            if langs:
                langs.sort()
                langs = tuple(langs)
            else:
                langs = 'all'

        for k, v in kw.iteritems():
            #startswith is important because kw has all sorts of
            #request info in it
            if k.startswith('pref_'):
                setattr(c.user, k, v)

        c.user.pref_content_langs = langs
        c.user.pref_lang = pref_lang
        c.user._commit()


    # @validate(pref_lang = VLang('lang'),
    #           all_langs = nop('all-langs', default = 'all'))
    # def POST_unlogged_options(self, all_langs, pref_lang):
    #     self.set_options( all_langs, pref_lang)
    #     return self.redirect(request.referer)

    @validate(VModhash(),
              pref_public_votes = VBoolean('public_votes'),
              pref_kibitz = VBoolean('kibitz'),
              pref_hide_ups = VBoolean('hide_ups'),
              pref_hide_downs = VBoolean('hide_downs'),
              pref_numsites = VInt('numsites', 1, 100),
              pref_lang = VLang('lang'),
              pref_min_link_score = VInt('min_link_score', -100, 100),
              pref_min_comment_score = VInt('min_comment_score', -100, 100),
              pref_num_comments = VInt('num_comments', 1, g.max_comments,
                                       default = g.num_comments),
              pref_url = VUserWebsiteUrl('url'),
              pref_location = VLocation('location'),
              pref_latitude = VFloat('latitude', allow_none=True),
              pref_longitude = VFloat('longitude', allow_none=True),
              pref_meetup_notify_enabled = VBoolean('meetup_notify_enabled'),
              pref_meetup_notify_radius = VInt('meetup_notify_radius', 1, 40000),
              pref_show_parent_comments = VBoolean('show_parent_comments'),
              all_langs = nop('all-langs', default = 'all'))
    def POST_options(self, all_langs, pref_lang, **kw):
        errors = list(c.errors)
        if errors:
            return PrefsPage(content = PrefOptions(), infotext="Unable to save preferences").render()

        self.set_options(all_langs, pref_lang, **kw)
        # Doesn't work when proxying to AWS
        #u = UrlParser(c.site.path + "prefs")
        #u.update_query(done = 'true')
        #if c.cname:
        #    u.put_in_frame()
        return self.redirect('/prefs?done=true')
            
    def GET_over18(self):
        return BoringPage(_("Over 18?"),
                          content = Over18()).render()

    @validate(over18 = nop('over18'),
              uh = nop('uh'),
              dest = nop('dest'))
    def POST_over18(self, over18, uh, dest):
        if over18 == 'yes':
            if c.user_is_loggedin and c.user.valid_hash(uh):
                c.user.pref_over_18 = True
                c.user._commit()
            else:
                ip_hash = hashlib.sha1(request.ip).hexdigest()
                domain = g.domain if not c.frameless_cname else None
                c.cookies.add('over18', ip_hash,
                              domain = domain)
            return self.redirect(dest)
        else:
            return self.redirect('/')


    @validate(msg_hash = nop('x'))
    def POST_optout(self, msg_hash):
        email, sent = opt_out(msg_hash)
        if not email:
            return self.abort404()
        return BoringPage(_("Opt out"),
                          content = OptOut(email = email, leave = True,
                                           sent = True,
                                           msg_hash = msg_hash)).render()

    @validate(msg_hash = nop('x'))
    def POST_optin(self, msg_hash):
        email, sent = opt_in(msg_hash)
        if not email:
            return self.abort404()
        return BoringPage(_("Welcome back"),
                          content = OptOut(email = email, leave = False,
                                           sent = True,
                                           msg_hash = msg_hash)).render()


    def POST_login(self, *a, **kw):
        res = ApiController.POST_login(self, *a, **kw)
        c.render_style = "html"
        c.response_content_type = ""

        errors = list(c.errors)
        if errors:
            for e in errors:
                if not e.endswith("_login"):
                    msg = c.errors[e].message
                    c.errors.remove(e)
                    c.errors._add(e + "_login", msg)

            dest = request.post.get('dest', request.referer or '/')
            return LoginPage(user_login = request.post.get('user_login'),
                             dest = dest).render()

        return self.redirect(res.redirect)

    def POST_reg(self, *a, **kw):
        res = ApiController.POST_register(self, *a, **kw)
        c.render_style = "html"
        c.response_content_type = ""

        errors = list(c.errors)
        if errors:
            for e in errors:
                if not e.endswith("_reg"):
                    msg = c.errors[e].message
                    c.errors.remove(e)
                    c.errors._add(e + "_reg", msg)

            dest = request.post.get('dest', request.referer or '/')
            return LoginPage(user_reg = request.post.get('user_reg'),
                             dest = dest).render()

        return self.redirect(res.redirect)

    def GET_login(self, *a, **kw):
        return self.redirect('/login' + query_string(dict(dest="/")))

########NEW FILE########
__FILENAME__ = promotecontroller
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from validator import *
from pylons.i18n import _
from r2.models import *
from r2.lib.pages import *
from r2.lib.menus import *
from r2.controllers import ListingController

from r2.controllers.reddit_base import RedditController

from r2.lib.promote import get_promoted
from r2.lib.utils import timetext

from datetime import datetime

class PromoteController(RedditController):
    @validate(VSponsor())
    def GET_index(self):
        return self.GET_current_promos()

    @validate(VSponsor())
    def GET_current_promos(self):
        current_list = get_promoted()

        b = IDBuilder(current_list)

        render_list = b.get_items()[0]

        for x in render_list:
            if x.promote_until:
                x.promote_expires = timetext(datetime.now(g.tz) - x.promote_until)

        page = PromotePage('current_promos',
                           content = PromotedLinks(render_list))
    
        return page.render()

    @validate(VSponsor())
    def GET_new_promo(self):
        page = PromotePage('new_promo',
                           content = PromoteLinkForm())
        return page.render()

    @validate(VSponsor(),
              link = VLink('link'))
    def GET_edit_promo(self, link):
        sr = Subreddit._byID(link.sr_id)

        names = [link._fullname]
        builder = IDBuilder(names, wrap = ListingController.builder_wrapper)
        listing = LinkListing(builder,
                              show_nums = False, nextprev = False)
        rendered = listing.listing().render()

        timedeltatext = ''
        if link.promote_until:
            timedeltatext = timetext(link.promote_until - datetime.now(g.tz),
                                     resultion=2)

        form = PromoteLinkForm(sr = sr, link = link,
                               listing = rendered,
                               timedeltatext = timedeltatext)
        page = PromotePage('new_promo', content = form)

        return page.render()


########NEW FILE########
__FILENAME__ = querycontroller
from reddit_base import RedditController
from validator import *
from r2.lib.db.queries import CachedResults

import cPickle as pickle
from urllib import unquote

class QueryController(RedditController):
    @validate(query = nop('query'))
    def POST_doquery(self, query):
        if g.enable_doquery:
            cr = pickle.loads(query)
            cr.update()
        else:
            abort(403, 'forbidden')

########NEW FILE########
__FILENAME__ = reddit_base
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is Reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
#
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
import r2.lib.helpers as h
from pylons import c, g, request
from pylons.controllers.util import abort, redirect_to
from pylons.i18n import _
from pylons.i18n.translation import LanguageError
from r2.lib.base import BaseController, proxyurl, current_login_cookie
from r2.lib import pages, utils, filters
from r2.lib.utils import http_utils
from r2.lib.cache import LocalCache
import random as rand
from r2.models.account import valid_cookie, FakeAccount
from r2.models.subreddit import Subreddit
import r2.config as config
from r2.models import *
from r2.lib.errors import ErrorSet
from validator import *
from r2.lib.template_helpers import add_sr
from r2.lib.jsontemplates import api_type

from copy import copy
from Cookie import CookieError
from datetime import datetime
import hashlib, inspect, simplejson
from urllib import quote, unquote

from r2.lib.tracking import encrypt, decrypt

NEVER = 'Thu, 31 Dec 2037 23:59:59 GMT'

cache_affecting_cookies = ('reddit_first','over18')

class Cookies(dict):
    def add(self, name, value, *k, **kw):
        self[name] = Cookie(value, *k, **kw)

class Cookie(object):
    def __init__(self, value, expires = None, domain = None, dirty = True):
        self.value = value
        self.expires = expires
        self.dirty = dirty
        if domain:
            self.domain = domain
        elif c.authorized_cname:
            self.domain = c.site.domain
        else:
            self.domain = g.domain

    def __repr__(self):
        return ("Cookie(value=%r, expires=%r, domain=%r, dirty=%r)"
                % (self.value, self.expires, self.domain, self.dirty))

class UnloggedUser(FakeAccount):
    _cookie = 'options'
    allowed_prefs = ('pref_content_langs', 'pref_lang')

    def __init__(self, browser_langs, *a, **kw):
        FakeAccount.__init__(self, *a, **kw)
        if browser_langs:
            lang = browser_langs[0]
            content_langs = list(browser_langs)
            content_langs.sort()
        else:
            lang = 'en'
            content_langs = 'all'
        self._defaults = self._defaults.copy()
        self._defaults['pref_lang'] = lang
        self._defaults['pref_content_langs'] = content_langs
        self._load()

    @property
    def name(self):
        raise NotImplementedError

    def _from_cookie(self):
        z = read_user_cookie(self._cookie)
        try:
            d = simplejson.loads(decrypt(z))
            return dict((k, v) for k, v in d.iteritems()
                        if k in self.allowed_prefs)
        except ValueError:
            return {}

    def _to_cookie(self, data):
        data = data.copy()
        for k in data.keys():
            if k not in self.allowed_prefs:
                del k
        set_user_cookie(self._cookie, encrypt(simplejson.dumps(data)))

    def _subscribe(self, sr):
        pass

    def _unsubscribe(self, sr):
        pass

    def _commit(self):
        if self._dirty:
            self._t.update(self._dirties)
            self._to_cookie(self._t)

    def _load(self):
        self._t.update(self._from_cookie())
        self._loaded = True

def read_user_cookie(name):
    uname = c.user.name if c.user_is_loggedin else ""
    cookie_name = uname + '_' + name
    if cookie_name in c.cookies:
        return c.cookies[cookie_name].value
    else:
        return ''

def set_user_cookie(name, val):
    uname = c.user.name if c.user_is_loggedin else ""
    c.cookies[uname + '_' + name] = Cookie(value = val)

valid_click_cookie = re.compile(r'(t[0-9]_[a-zA-Z0-9]+:)+').match
def read_click_cookie():
    if c.user_is_loggedin:
        click_cookie = read_user_cookie('click')
        if click_cookie and valid_click_cookie(click_cookie):
            ids = [s for s in click_cookie.split(':') if s]
            things = Thing._by_fullname(ids, return_dict = False)
            for t in things:
                def foo(t1, user):
                    return lambda: t1._click(user)
                #don't record clicks for the time being
                #utils.worker.do(foo(t, c.user))
    set_user_cookie('click', '')


def read_mod_cookie():
    cook = [s.split('=')[0:2] for s in read_user_cookie('mod').split(':') if s]
    if cook:
        set_user_cookie('mod', '')

def firsttime():
    if get_redditfirst('firsttime'):
        return False
    else:
        set_redditfirst('firsttime','first')
        return True

def get_redditfirst(key,default=None):
    try:
        cookie = simplejson.loads(c.cookies['reddit_first'].value)
        return cookie[key]
    except (ValueError,TypeError,KeyError),e:
        # it's not a proper json dict, or the cookie isn't present, or
        # the key isn't part of the cookie; we don't really want a
        # broken cookie to propogate an exception up
        return default

def set_redditfirst(key,val):
    try:
        cookie = simplejson.loads(c.cookies['reddit_first'].value)
        cookie[key] = val
    except (ValueError,TypeError,KeyError),e:
        # invalid JSON data; we'll just construct a new cookie
        cookie = {key: val}

    c.cookies['reddit_first'] = Cookie(simplejson.dumps(cookie),
                                       expires = NEVER)

# this cookie is also accessed by organic.js, so changes to the format
# will have to be made there as well
organic_pos_key = 'organic_pos'
def organic_pos():
    "organic_pos() -> (calc_date = str(), pos  = int())"
    try:
        d,p = get_redditfirst(organic_pos_key, ('',0))
    except ValueError:
        d,p = ('',0)
    return d,p

def set_organic_pos(key,pos):
    "set_organic_pos(str(), int()) -> None"
    set_redditfirst(organic_pos_key,[key,pos])


def over18():
    if c.user.pref_over_18 or c.user_is_admin:
        return True

    else:
        if 'over18' in c.cookies:
            cookie = c.cookies['over18'].value
            if cookie == hashlib.sha1(request.ip).hexdigest():
                return True

def set_subreddit():
    #the r parameter gets added by javascript for POST requests so we
    #can reference c.site in api.py
    sr_name = request.environ.get("subreddit", request.POST.get('r'))
    domain = request.environ.get("domain")

    if not sr_name:
        #check for cnames
        sub_domain = request.environ.get('sub_domain')
        sr = Subreddit._by_domain(sub_domain) if sub_domain else None
        c.site = sr or Default
    elif sr_name == 'r':
        #reddits
        c.site = Sub
    else:
        try:
            if '+' in sr_name:
                srs = set()
                sr_names = sr_name.split('+')
                real_path = sr_name
                for sr_name in sr_names:
                    srs.add(Subreddit._by_name(sr_name))
                sr_ids = [sr._id for sr in srs]
                c.site = MultiReddit(sr_ids, real_path)
            else:
                c.site = Subreddit._by_name(sr_name)
        except NotFound:
            c.site = Default
            if chksrname(sr_name):
                redirect_to("/categories/create?name=%s" % sr_name)
            elif not c.error_page:
                abort(404, "not found")

    #if we didn't find a subreddit, check for a domain listing
    if not sr_name and c.site == Default and domain:
        c.site = DomainSR(domain)

    if isinstance(c.site, FakeSubreddit):
        c.default_sr = True
        try:
            c.current_or_default_sr = Subreddit._by_name(g.default_sr)
        except NotFound:
            c.current_or_default_sr = None
    else:
        c.current_or_default_sr = c.site

    # check that the site is available:
    if c.site._spam and not c.user_is_admin and not c.error_page:
        abort(404, "not found")

def set_content_type():
    e = request.environ
    c.render_style = e['render_style']
    c.response_content_type = e['content_type']

    if e.has_key('extension'):
        ext = e['extension']
        if ext == 'api' or ext.startswith('json'):
            c.response_access_control = 'allow <*>'
        if ext in ('embed', 'wired'):
            c.response_wrappers.append(utils.to_js)

def get_browser_langs():
    browser_langs = []
    langs = request.environ.get('HTTP_ACCEPT_LANGUAGE')
    if langs:
        langs = langs.split(',')
        browser_langs = []
        seen_langs = set()
        # extract languages from browser string
        for l in langs:
            if ';' in l:
                l = l.split(';')[0]
            if l not in seen_langs:
                browser_langs.append(l)
                seen_langs.add(l)
            if '-' in l:
                l = l.split('-')[0]
            if l not in seen_langs:
                browser_langs.append(l)
                seen_langs.add(l)
    return browser_langs

def set_host_lang():
    # try to grab the language from the domain
    host_lang = request.environ.get('reddit-prefer-lang')
    if host_lang:
        c.host_lang = host_lang

def set_iface_lang():
    lang = ['en']
    # GET param wins
    if c.host_lang:
        lang = [c.host_lang]
    else:
        lang = [c.user.pref_lang]

    #choose the first language
    c.lang = lang[0]

    #then try to overwrite it if we have the translation for another
    #one
    for l in lang:
        try:
            h.set_lang(l)
            c.lang = l
            break
        except h.LanguageError:
            #we don't have a translation for that language
            h.set_lang('en', graceful_fail = True)

    #TODO: add exceptions here for rtl languages
    if c.lang in ('ar', 'he', 'fa'):
        c.lang_rtl = True

def set_content_lang():
    if c.user.pref_content_langs != 'all':
        c.content_langs = list(c.user.pref_content_langs)
        c.content_langs.sort()
    else:
        c.content_langs = c.user.pref_content_langs

def set_cnameframe():
    if (bool(request.params.get(utils.UrlParser.cname_get))
        or not request.host.split(":")[0].endswith(g.domain)):
        c.cname = True
        request.environ['REDDIT_CNAME'] = 1
        if request.params.has_key(utils.UrlParser.cname_get):
            del request.params[utils.UrlParser.cname_get]
        if request.get.has_key(utils.UrlParser.cname_get):
            del request.get[utils.UrlParser.cname_get]
    c.frameless_cname  = request.environ.get('frameless_cname',  False)
    if hasattr(c.site, 'domain'):
        c.authorized_cname = request.environ.get('authorized_cname', False)

def set_colors():
    theme_rx = re.compile(r'')
    color_rx = re.compile(r'^([a-fA-F0-9]){3}(([a-fA-F0-9]){3})?$')
    c.theme = None
    if color_rx.match(request.get.get('bgcolor') or ''):
        c.bgcolor = request.get.get('bgcolor')
    if color_rx.match(request.get.get('bordercolor') or ''):
        c.bordercolor = request.get.get('bordercolor')

def set_recent_reddits():
    names = read_user_cookie('recent_reddits')
    c.recent_reddits = []
    if names:
        try:
            names = filter(None, names.split(','))
            srs = Subreddit._by_fullname(names, data = True,
                                         return_dict = False)
            # Ensure all the objects returned are Subreddits. Due to the nature
            # of _by_fullname its possible to get any type back
            c.recent_reddits = filter(lambda x: isinstance(x, Subreddit), names)
        except:
            pass

def ratelimit_agents():
    user_agent = request.user_agent
    for s in g.agents:
        if s and user_agent and s in user_agent.lower():
            key = 'rate_agent_' + s
            if cache.get(s):
                abort(503, 'service temporarily unavailable')
            else:
                cache.set(s, 't', time = 1)

#TODO i want to get rid of this function. once the listings in front.py are
#moved into listingcontroller, we shouldn't have a need for this
#anymore
def base_listing(fn):
    @validate(num    = VLimit('limit'),
              after  = VByName('after'),
              before = VByName('before'),
              count  = VCount('count'))
    def new_fn(self, before, num, **env):
        kw = self.build_arg_list(fn, env)

        # Multiply the number per page by the per page multiplier for the reddit
        if num:
            kw['num'] = c.site.posts_per_page_multiplier * num

        #turn before into after/reverse
        kw['reverse'] = False
        if before:
            kw['after'] = before
            kw['reverse'] = True

        return fn(self, **kw)
    return new_fn

class RedditController(BaseController):

    @staticmethod
    def build_arg_list(fn, env):
        """given a fn and and environment the builds a keyword argument list
        for fn"""
        kw = {}
        argspec = inspect.getargspec(fn)

        # if there is a **kw argument in the fn definition,
        # just pass along the environment
        if argspec[2]:
            kw = env
        #else for each entry in the arglist set the value from the environment
        else:
            #skip self
            argnames = argspec[0][1:]
            for name in argnames:
                if name in env:
                    kw[name] = env[name]
        return kw

    def request_key(self):
        # note that this references the cookie at request time, not
        # the current value of it
        cookie_keys = []
        for x in cache_affecting_cookies:
            cookie_keys.append(request.cookies.get(x,''))

        key = ''.join((str(c.lang),
                       str(c.content_langs),
                       request.host,
                       str(c.cname),
                       str(request.fullpath),
                       str(c.over18),
                       ''.join(cookie_keys)))
        return key

    def cached_response(self):
        return c.response

    @staticmethod
    def login(user, admin = False, rem = False):
        c.cookies[g.login_cookie] = Cookie(value = user.make_cookie(admin = admin),
                                            expires = NEVER if rem else None)

    @staticmethod
    def logout(admin = False):
        c.cookies[g.login_cookie] = Cookie(value='')

    def pre(self):
        g.cache.caches = (LocalCache(),) + g.cache.caches[1:]

        #check if user-agent needs a dose of rate-limiting
        if not c.error_page:
            ratelimit_agents()

        # the domain has to be set before Cookies get initialized
        set_subreddit()
        set_cnameframe()

        # populate c.cookies
        c.cookies = Cookies()
        try:
            for k,v in request.cookies.iteritems():
                # we can unquote even if it's not quoted
                c.cookies[k] = Cookie(value=unquote(v), dirty=False)
        except CookieError:
            #pylons or one of the associated retarded libraries can't
            #handle broken cookies
            request.environ['HTTP_COOKIE'] = ''

        c.response_wrappers = []
        c.errors = ErrorSet()
        c.firsttime = firsttime()
        (c.user, maybe_admin) = valid_cookie(current_login_cookie())

        if c.user:
            c.user_is_loggedin = True
        else:
            c.user = UnloggedUser(get_browser_langs())
            c.user._load()

        if c.user_is_loggedin:
            if not c.user._loaded:
                c.user._load()
            c.modhash = c.user.modhash()
            if request.method.lower() == 'get':
                read_click_cookie()
                read_mod_cookie()
                if (c.user.wiki_account is None and
                    c.user.wiki_association_attempted_at is None):
                    c.user.attempt_wiki_association()
            if hasattr(c.user, 'msgtime') and c.user.msgtime:
                c.have_messages = c.user.msgtime
            c.user_is_admin = maybe_admin and c.user.name in g.admins

            c.user_is_sponsor = c.user_is_admin or c.user.name in g.sponsors

        c.over18 = over18()

        #set_browser_langs()
        set_host_lang()
        set_content_type()
        set_iface_lang()
        set_content_lang()
        set_colors()
        set_recent_reddits()

        # set some environmental variables in case we hit an abort
        if not isinstance(c.site, FakeSubreddit):
            request.environ['REDDIT_NAME'] = c.site.name

        # check if the user has access to this subreddit
        if not c.site.can_view(c.user) and not c.error_page:
            abort(403, "forbidden")

        #check over 18
        if (c.site.over_18 and not c.over18 and
            request.path not in  ("/frame", "/over18")
            and c.render_style == 'html'):
            return self.intermediate_redirect("/over18")

        #check whether to allow custom styles
        c.allow_styles = True
        if g.css_killswitch:
            c.allow_styles = False
        #if the preference is set and we're not at a cname
        elif not c.user.pref_show_stylesheets and not c.cname:
            c.allow_styles = False
        #if the site has a cname, but we're not using it
        elif c.site.domain and not c.cname:
            c.allow_styles = False

        #check content cache
        if not c.user_is_loggedin:
            r = cache.get(self.request_key())
            if r and request.method == 'GET':
                response = c.response
                response.headers = r.headers
                response.content = r.content

                for x in r.cookies.keys():
                    if x in cache_affecting_cookies:
                        cookie = r.cookies[x]
                        response.set_cookie(key     = x,
                                            value   = cookie.value,
                                            domain  = cookie.get('domain',None),
                                            expires = cookie.get('expires',None),
                                            path    = cookie.get('path',None))

                response.status_code = r.status_code
                request.environ['pylons.routes_dict']['action'] = 'cached_response'
                # make sure to carry over the content type
                c.response_content_type = r.headers['content-type']
                if r.headers.has_key('access-control'):
                    c.response_access_control = r.headers['access-control']
                c.used_cache = True
                # response wrappers have already been applied before cache write
                c.response_wrappers = []

    def post(self):
        response = c.response
        content = response.content
        if isinstance(content, (list, tuple)):
            content = ''.join(content)
        for w in c.response_wrappers:
            content = w(content)
        response.content = content
        if c.response_content_type:
            response.headers['Content-Type'] = c.response_content_type
        if c.response_access_control:
            c.response.headers['Access-Control'] = c.response_access_control

        if c.user_is_loggedin and 'Cache-Control' not in response.headers:
            response.headers['Cache-Control'] = 'no-cache'
            response.headers['Pragma'] = 'no-cache'

        # send cookies
        if not c.used_cache:
            # if we used the cache, these cookies should be set by the
            # cached response object instead
            for k,v in c.cookies.iteritems():
                if v.dirty:
                    response.set_cookie(key     = k,
                                        value   = quote(v.value),
                                        domain  = v.domain,
                                        expires = v.expires)

        #return
        #set content cache
        if (g.page_cache_time
            and request.method == 'GET'
            and not c.user_is_loggedin
            and not c.used_cache
            and response.content and response.content[0]):
            config.cache.set(self.request_key(),
                             response,
                             g.page_cache_time)

    def check_modified(self, thing, action):
        if c.user_is_loggedin:
            return

        date = utils.is_modified_since(thing, action, request.if_modified_since)
        if date is True:
            abort(304, 'not modified')
        else:
            c.response.headers['Last-Modified'] = http_utils.http_date_str(date)

    def abort404(self):
        abort(404, "not found")

    def sendpng(self, string):
        c.response_content_type = 'image/png'
        c.response.content = string
        return c.response

    def sendstring(self,string):
        '''sends a string and automatically escapes &, < and > to make sure no code injection happens'''
        c.response.headers['Content-Type'] = 'text/html; charset=UTF-8'
        c.response.content = filters.websafe_json(string)
        return c.response

    def update_qstring(self, dict):
        merged = copy(request.get)
        merged.update(dict)
        return request.path + utils.query_string(merged)

########NEW FILE########
__FILENAME__ = redirect
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from pylons.controllers.util import redirect_to
from r2.lib.base import BaseController
from pylons import c

class RedirectController(BaseController):
    def GET_redirect(self, dest):
        return redirect_to(str(dest))

########NEW FILE########
__FILENAME__ = template
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.lib.base import *

class TemplateController(BaseController):
    def view(self, url):
        """
        This is the last place which is tried during a request to try to find a 
        file to serve. It could be used for example to display a template::
        
            def view(self, url):
                return render_response(url)
        
        Or, if you're using Myghty and would like to catch the component not
        found error which will occur when the template doesn't exist; you
        can use the following version which will provide a 404 if the template
        doesn't exist::
        
            import myghty.exception
            
            def view(self, url):
                try:
                    return render_response('/'+url)
                except myghty.exception.ComponentNotFound:
                    return Response(code=404)
        
        The default is just to abort the request with a 404 File not found
        status message.
        """
        abort(404)

########NEW FILE########
__FILENAME__ = toolbar
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from reddit_base import RedditController
from r2.lib.pages import *
from r2.lib.filters import spaceCompress
from validator import *
from pylons import c

class ToolbarController(RedditController):

    @validate(link = VByName('id'))
    def GET_toolbar(self, link):
        if not link: return self.abort404()
        link_builder = IDBuilder((link._fullname,))
        link_listing = LinkListing(link_builder, nextprev=False).listing()
        res = FrameToolbar(link = link_listing.things[0]).render()
        return spaceCompress(res)

    @validate(link = VByName('id'),
              link2 = VLink('id', redirect = False))
    def GET_goto(self, link, link2):
        link = link2 if link2 else link
        if link:
            link._load()
            if c.user and c.user.pref_frame:
                return Frame(title = link.title,
                             url = link.url,
                             fullname = link._fullname).render()
            else:
                return self.redirect(link.url)
        return self.abort404()


########NEW FILE########
__FILENAME__ = validator
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from pylons import c, request, g
from pylons.i18n import _
from pylons.controllers.util import abort
from r2.lib import utils, captcha
from r2.lib.filters import unkeep_space, websafe, _force_utf8, _force_ascii
from r2.lib.wikipagecached import WikiPageCached
from r2.lib.db.operators import asc, desc
from r2.config import cache
from r2.lib.template_helpers import add_sr
from r2.lib.jsonresponse import json_respond
from r2.lib.errors import errors, UserRequiredException

from r2.models import *

from copy import copy
from datetime import datetime, timedelta
import pytz
import re

class Validator(object):
    default_param = None
    def __init__(self, param=None, default=None, post=True, get=True, url=True):
        if param:
            self.param = param
        else:
            self.param = self.default_param

        self.default = default
        self.post, self.get, self.url = post, get, url

    def __call__(self, url):
        a = []
        if self.param:
            for p in utils.tup(self.param):
                if self.post and request.post.get(p):
                    val = request.post[p]
                elif self.get and request.get.get(p):
                    val = request.get[p]
                elif self.url and url.get(p):
                    val = url[p]
                else:
                    val = self.default
                a.append(val)
        return self.run(*a)

def validate(*simple_vals, **param_vals):
    def val(fn):
        def newfn(self, *a, **env):
            try:
                for validator in simple_vals:
                    validator(env)
                
                kw = self.build_arg_list(fn, env)
                for var, validator in param_vals.iteritems():
                    kw[var] = validator(env)
                
                return fn(self, *a, **kw)

            except UserRequiredException:
                if request.method == "POST" and hasattr(self, "ajax_login_redirect"):
                    # ajax failure, so redirect accordingly
                    return  self.ajax_login_redirect("/")
                return self.intermediate_redirect('/login')
        return newfn
    return val


#### validators ####
class nop(Validator):
    def run(self, x):
        return x

class VLang(Validator):
    def run(self, lang):
        if lang:
            lang = str(lang.split('[')[1].strip(']'))
            if lang in g.all_languages:
                return lang
        #else
        return 'en'

class VRequired(Validator):
    def __init__(self, param, error, *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        self._error = error

    def error(self, e = None):
        if not e: e = self._error
        if e:
            c.errors.add(e)
        
    def run(self, item):
        if not item:
            self.error()
        else:
            return item

class VEmailVerify(Validator):
    def run(self, code):
        if not code:
            c.errors.add(errors.NO_CODE)
        elif not code == c.user.confirmation_code:
            c.errors.add(errors.WRONG_CODE)
        else:
            return code

class VAwardAmount(Validator):
    def __init__(self, param, error, *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        self._error = error

    def error(self, e = None):
        if not e: e = self._error
        if e:
            c.errors.add(e)
        
    def run(self, item):
        if not item:
            self.error()
        else:
            try:
                if int(item) <= 0:
                    c.errors.add(errors.AMOUNT_NEGATIVE)
                else:
                    return item
            except ValueError:
                c.errors.add(errors.AMOUNT_NOT_NUM)

class ValueOrBlank(Validator):
    def run(self, value):
        """Returns the value as is if present, else an empty string"""
        return '' if value is None else value

class VLink(Validator):
    def __init__(self, param, redirect = True, *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        self.redirect = redirect
    
    def run(self, link_id):
        if link_id:
            try:
                aid = int(link_id, 36)
                return Link._byID(aid, True)
            except (NotFound, ValueError):
                if self.redirect:
                    abort(404, 'page not found')
                else:
                    return None

class VCommentFullName(Validator):
    valid_re = re.compile(Comment._type_prefix + str(Comment._type_id) + r'_([0-9a-z]+)$')
    
    def run(self, thing_fullname):
        if thing_fullname:
            match = self.valid_re.match(thing_fullname)
            if match:
                try:
                    parsed_id = int(match.group(1), 36)
                    return Comment._byID(parsed_id, True)
                except Exception:
                    return None

class VMeetup(Validator):
    def __init__(self, param, redirect = True, *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        self.redirect = redirect

    def run(self, meetup_id36):
        if meetup_id36:
            try:
                meetup_id = int(meetup_id36, 36)
                return Meetup._byID(meetup_id, True)
            except (NotFound, ValueError):
                if self.redirect:
                    abort(404, 'page not found')
                else:
                    return None

class VEditMeetup(VMeetup):
    def __init__(self, param, redirect = True, *a, **kw):
        VMeetup.__init__(self, param, redirect = redirect, *a, **kw)

    def run(self, param):
        meetup = VMeetup.run(self, param)
        if meetup and not (c.user_is_loggedin and 
                           meetup.can_edit(c.user, c.user_is_admin)):
            abort(403, "forbidden")
        return meetup

class VTagByName(Validator):
    def __init__(self, param, *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        
    def run(self, name):
        if name:
            cleaned = _force_ascii(name)
            if cleaned == name:
                try:
                    return Tag._by_name(cleaned)
                except:
                    pass
            abort(404, 'page not found')

class VTags(Validator):
    comma_sep = re.compile('[,\s]+', re.UNICODE)
    
    def __init__(self, param, *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        
    def run(self, tag_field):
        tags = []
        if tag_field:
            # Tags are comma delimited
            tags = [x for x in self.comma_sep.split(tag_field) if x==_force_ascii(x)]
        return tags

class VMessage(Validator):
    def run(self, message_id):
        if message_id:
            try:
                aid = int(message_id, 36)
                return Message._byID(aid, True)
            except (NotFound, ValueError):
                abort(404, 'page not found')


class VCommentID(Validator):
    def run(self, cid):
        if cid:
            try:
                cid = int(cid, 36)
                return Comment._byID(cid, True)
            except (NotFound, ValueError):
                pass

class VCount(Validator):
    def run(self, count):
        try:
            count = int(count)
        except (TypeError, ValueError):
            count = 0
        return max(count, 0)


class VLimit(Validator):
    def run(self, limit):
        if limit is None:
            return c.user.pref_numsites 
        return min(max(int(limit), 1), 250)

class VCssMeasure(Validator):
    measure = re.compile(r"^\s*[\d\.]+\w{0,3}\s*$")
    def run(self, value):
        return value if value and self.measure.match(value) else ''

subreddit_rx = re.compile(r"^[\w]{3,20}$", re.UNICODE)

def chksrname(x):
    #notice the space before reddit.com
    if x in ('friends', 'all', ' reddit.com'):
        return False

    try:
        return str(x) if x and subreddit_rx.match(x) else None
    except UnicodeEncodeError:
        return None

class VLinkUrls(Validator):
    "A comma-separated list of link urls"
    splitter = re.compile('[ ,]+')
    id_re = re.compile('^/lw/([^/]+)/')
    def __init__(self, item, *a, **kw):
        self.item = item
        Validator.__init__(self, item, *a, **kw)
    
    def run(self, val):
        res=[]
        for v in self.splitter.split(val):
            link_id = self.id_re.match(v)
            if link_id:
                l = VLink(None,False).run(link_id.group(1))
                if l:
                    res.append(l)
        return res

class VLinkFullnames(Validator):
    "A space- or comma-separated list of fullnames for Links"
    valid_re = re.compile(r'^(' + Link._type_prefix + str(Link._type_id) +
                          r'_[0-9a-z]+[ ,]?)+$')
    splitter = re.compile('[ ,]+')

    def __init__(self, item, *a, **kw):
        self.item = item
        Validator.__init__(self, item, *a, **kw)
    
    def run(self, val):
        if val and self.valid_re.match(val):
            return self.splitter.split(val)
    
class VLength(Validator):
    def __init__(self, item, length = 10000,
                 empty_error = errors.BAD_COMMENT,
                 length_error = errors.COMMENT_TOO_LONG, **kw):
        Validator.__init__(self, item, **kw)
        self.length = length
        self.len_error = length_error
        self.emp_error = empty_error

    def run(self, title):
        if not title:
            if self.emp_error is not None:
                c.errors.add(self.emp_error)
        elif len(title) > self.length:
            c.errors.add(self.len_error)
        else:
            return title
        
class VTitle(VLength):
    only_whitespace = re.compile(r"^\s*$", re.UNICODE)
    
    def __init__(self, item, length = 200, **kw):
        VLength.__init__(self, item, length = length,
                         empty_error = errors.NO_TITLE,
                         length_error = errors.TITLE_TOO_LONG, **kw)

    def run(self, title):
        title = VLength.run(self, title)
        if title and self.only_whitespace.match(title):
            c.errors.add(errors.NO_TITLE)
        else:
            return title
    
class VComment(VLength):
    def __init__(self, item, length = 10000, **kw):
        VLength.__init__(self, item, length = length, **kw)

        
class VMessage(VLength):
    def __init__(self, item, length = 10000, **kw):
        VLength.__init__(self, item, length = length, 
                         empty_error = errors.NO_MSG_BODY, **kw)


class VSubredditName(VRequired):
    def __init__(self, item, *a, **kw):
        VRequired.__init__(self, item, errors.BAD_SR_NAME, *a, **kw)

    def run(self, name):
        name = chksrname(name)
        if not name:
            return self.error()
        else:
            try:
                a = Subreddit._by_name(name)
                return self.error(errors.SUBREDDIT_EXISTS)
            except NotFound:
                return name

class VSubredditTitle(Validator):
    def run(self, title):
        if not title:
            c.errors.add(errors.NO_TITLE)
        elif len(title) > 100:
            c.errors.add(errors.TITLE_TOO_LONG)
        else:
            return title

class VSubredditDesc(Validator):
    def run(self, description):
        if description and len(description) > 500:
            c.errors.add(errors.DESC_TOO_LONG)
        return unkeep_space(description or '')

class VAccountByName(VRequired):
    def __init__(self, param, error = errors.USER_DOESNT_EXIST, *a, **kw):
        VRequired.__init__(self, param, error, *a, **kw)
        
    def run(self, name):
        if name:
            try:
                return Account._by_name(name)
            except NotFound: pass
        return self.error()

class VByName(VRequired):
    def __init__(self, param, 
                 error = errors.NO_THING_ID, *a, **kw):
        VRequired.__init__(self, param, error, *a, **kw)

    def run(self, fullname):
        if fullname:
            try:
                return Thing._by_fullname(fullname, False, data=True)
            except NotFound:
                pass
        return self.error()

class VByNameIfAuthor(VByName):
    def run(self, fullname):
        thing = VByName.run(self, fullname)
        if thing:
            if not thing._loaded: thing._load()
            if c.user_is_loggedin and thing.author_id == c.user._id:
                return thing
        return self.error(errors.NOT_AUTHOR)

class VCaptcha(Validator):
    default_param = ('iden', 'captcha')
    
    def run(self, iden, solution):
        if (not c.user_is_loggedin or c.user.needs_captcha()):
            if not captcha.valid_solution(iden, solution):
                c.errors.add(errors.BAD_CAPTCHA)

class VUser(Validator):
    def run(self, password = None):
        if not c.user_is_loggedin:
            raise UserRequiredException

        if (password is not None) and not valid_password(c.user, password):
            c.errors.add(errors.WRONG_PASSWORD)
            
class VModhash(Validator):
    default_param = 'uh'
    def run(self, uh):
        if not c.user_is_loggedin:
            raise UserRequiredException

        if not c.user.valid_hash(uh):
            g.log.info("Invalid hash on form submission : "+str(c.user))
            raise UserRequiredException
            #abort(403, 'forbidden')

class VVotehash(Validator):
    def run(self, vh, thing_name):
        return True

class VAdmin(Validator):
    def run(self):
        if not c.user_is_admin:
            abort(404, "page not found")

class VSponsor(Validator):
    def run(self):
        if not c.user_is_sponsor:
            abort(403, 'forbidden')

class VSrModerator(Validator):
    def run(self):
        if not (c.user_is_loggedin and c.site.is_moderator(c.user) 
                or c.user_is_admin):
            abort(403, "forbidden")

class VSrCanBan(Validator):
    def run(self, thing_name):
        if c.user_is_admin:
            return True
        elif c.user_is_loggedin:
            item = Thing._by_fullname(thing_name,data=True)
            # will throw a legitimate 500 if this isn't a link or
            # comment, because this should only be used on links and
            # comments
            subreddit = item.subreddit_slow
            if subreddit.can_ban(c.user):
                return True
        abort(403,'forbidden')

class VSrSpecial(Validator):
    def run(self, thing_name):
        if c.user_is_admin:
            return True
        elif c.user_is_loggedin:
            item = Thing._by_fullname(thing_name,data=True)
            # will throw a legitimate 500 if this isn't a link or
            # comment, because this should only be used on links and
            # comments
            subreddit = item.subreddit_slow
            if subreddit.is_special(c.user):
                return True
        abort(403,'forbidden')

class VSRSubmitPage(Validator):
    def run(self):
        if not (c.default_sr or c.user_is_loggedin and c.site.can_submit(c.user)):
            return False
        else:
            return True

class VCreateMeetup(Validator):
    def run(self):
        if (c.user_is_loggedin and c.user.safe_karma >= g.discussion_karma_to_post):
            return True
        abort(403, "forbidden")

class VSubmitParent(Validator):
    def run(self, fullname):
        if fullname:
            parent = Thing._by_fullname(fullname, False, data=True)
            if isinstance(parent, Message):
                return parent
            else:
                sr = parent.subreddit_slow
                if c.user_is_loggedin and sr.can_comment(c.user):
                    return parent
        #else
        abort(403, "forbidden")
        
class VSubmitLink(VLink):
    def __init__(self, param, redirect = True, *a, **kw):
        VLink.__init__(self, param, redirect = redirect, *a, **kw)
        
    def run(self, link_name):
        link = VLink.run(self, link_name)
        if link and not (c.user_is_loggedin and link.can_submit(c.user)):
            abort(403, "forbidden")
        return link

class VSubmitSR(Validator):
    def run(self, sr_name):
        try:
            sr = Subreddit._by_name(sr_name)
        except NotFound:
            c.errors.add(errors.SUBREDDIT_NOEXIST)
            sr = None

        if sr and not (c.user_is_loggedin and sr.can_submit(c.user)):
            c.errors.add(errors.SUBREDDIT_FORBIDDEN)
            sr = None

        return sr
        
pass_rx = re.compile(r".{3,20}")

def chkpass(x):
    return x if x and pass_rx.match(x) else None

class VPassword(VRequired):
    def __init__(self, item, *a, **kw):
        VRequired.__init__(self, item, errors.BAD_PASSWORD, *a, **kw)
    def run(self, password, verify):
        if not chkpass(password):
            return self.error()
        elif verify != password:
            return self.error(errors.BAD_PASSWORD_MATCH)
        else:
            return password

user_rx = re.compile(r"^[\w-]{3,20}$", re.UNICODE)

def chkuser(x):
    try:
        return str(x) if user_rx.match(x) else None
    except TypeError:
        return None
    except UnicodeEncodeError:
        return None

def whyuserbad(x):
    if not x:
        return errors.BAD_USERNAME_CHARS
    if len(x)<3:
        return errors.BAD_USERNAME_SHORT
    if len(x)>20:
        return errors.BAD_USERNAME_LONG
    return errors.BAD_USERNAME_CHARS

class VUname(VRequired):
    def __init__(self, item, *a, **kw):
        VRequired.__init__(self, item, errors.BAD_USERNAME, *a, **kw)
    def run(self, user_name):
        original_user_name = user_name;
        user_name = chkuser(user_name)
        if not user_name:
            return self.error(whyuserbad(original_user_name))
        else:
            try:
                a = Account._by_name(user_name, True)
                return self.error(errors.USERNAME_TAKEN)
            except NotFound:
                return user_name

class VLogin(VRequired):
    def __init__(self, item, *a, **kw):
        VRequired.__init__(self, item, errors.WRONG_PASSWORD, *a, **kw)
        
    def run(self, user_name, password):
        user_name = chkuser(user_name)
        user = None
        if user_name:
            user = valid_login(user_name, password)
        if not user:
            return self.error()
        return user


class VSanitizedUrl(Validator):
    def run(self, url):
        return utils.sanitize_url(url)

class VUserWebsiteUrl(VSanitizedUrl):
    def run(self, url):
        val = VSanitizedUrl.run(self, url)
        if val is None:
            return ''
        else:
            return val

class VUrl(VRequired):
    def __init__(self, item, *a, **kw):
        VRequired.__init__(self, item, errors.NO_URL, *a, **kw)

    def run(self, url, sr = None):
        if sr is None and not isinstance(c.site, FakeSubreddit):
            sr = c.site
        elif sr:
            try:
                sr = Subreddit._by_name(sr)
            except NotFound:
                c.errors.add(errors.SUBREDDIT_NOEXIST)
                sr = None
        else:
            sr = None
        
        if not url:
            return self.error(errors.NO_URL)
        url = utils.sanitize_url(url)
        if url == 'self':
            return url
        elif url:
            try:
                l = Link._by_url(url, sr)
                self.error(errors.ALREADY_SUB)
                return utils.tup(l)
            except NotFound:
                return url
        return self.error(errors.BAD_URL)

class VExistingUname(VRequired):
    def __init__(self, item, *a, **kw):
        VRequired.__init__(self, item, errors.NO_USER, *a, **kw)

    def run(self, username):
        if username:
            try:
                name = _force_utf8(username)
                return Account._by_name(name)
            except (TypeError, UnicodeEncodeError, NotFound):
                return self.error(errors.USER_DOESNT_EXIST)
        self.error()

class VUserWithEmail(VExistingUname):
    def run(self, name):
        user = VExistingUname.run(self, name)
        if not user or not hasattr(user, 'email') or not user.email:
            return self.error(errors.NO_EMAIL_FOR_USER)
        return user

class VTimestamp(Validator):
    def run(self, val):
        if not val:
            c.errors.add(errors.INVALID_DATE)
            return

        try:
            val = float(val) / 1000.0
            datetime.fromtimestamp(val, pytz.utc)   # Check it can be converted to a datetime
            return val
        except ValueError:
            c.errors.add(errors.INVALID_DATE)

class VBoolean(Validator):
    def run(self, val):
        return val != "off" and bool(val)

class VLocation(VLength):
    def __init__(self, item, length = 100, **kw):
        VLength.__init__(self, item, length = length, 
                         length_error = errors.LOCATION_TOO_LONG,
                         empty_error = None, **kw)

    def run(self, val):
        val = VLength.run(self, val)
        if val == None:
            return ''
        else:
            return val

class VInt(Validator):
    def __init__(self, param, min=None, max=None, *a, **kw):
        self.min = min
        self.max = max
        Validator.__init__(self, param, *a, **kw)

    def run(self, val):
        if not val:
            return

        try:
            val = int(val)
            if self.min is not None and val < self.min:
                val = self.min
            elif self.max is not None and val > self.max:
                val = self.max
            return val
        except ValueError:
            c.errors.add(errors.BAD_NUMBER)

class VFloat(Validator):
    def __init__(self, param, min=None, max=None, allow_none=False, error=errors.BAD_NUMBER, *a, **kw):
        self.min = min
        self.max = max
        self.allow_none = allow_none
        self.error = error
        Validator.__init__(self, param, *a, **kw)

    def run(self, val):
        if not val:
            if self.allow_none:
                return None
            c.errors.add(self.error)
            return

        try:
            val = float(val)
            if self.min is not None and val < self.min:
                val = self.min
            elif self.max is not None and val > self.max:
                val = self.max
            return val
        except ValueError:
            c.errors.add(self.error)

class VCssName(Validator):
    """
    returns a name iff it consists of alphanumeric characters and
    possibly "-", and is below the length limit.
    """
    r_css_name = re.compile(r"^[a-zA-Z0-9\-]{1,100}$")
    def run(self, name):
        if name and self.r_css_name.match(name):
            return name
    
class VMenu(Validator):

    def __init__(self, param, menu_cls, remember = True, default_item = None, **kw):
        self.nav = menu_cls
        self.remember = remember
        self.default_item = default_item or self.nav.default
        param = (menu_cls.get_param, param)
        Validator.__init__(self, param, **kw)

    def run(self, sort, where):
        if self.remember:
            pref = "%s_%s" % (where, self.nav.get_param)
            user_prefs = copy(c.user.sort_options) if c.user else {}
            user_pref = user_prefs.get(pref)
    
            # check to see if a default param has been set
            if not sort:
                sort = user_pref

        if not sort:
            sort = self.default_item
            
        # validate the sort
        if sort not in self.nav.options:
            sort = self.nav.default

        # commit the sort if changed
        if self.remember and c.user_is_loggedin and sort != user_pref:
            user_prefs[pref] = sort
            c.user.sort_options = user_prefs
            user = c.user
            utils.worker.do(lambda: user._commit())

        return sort
            

class VRatelimit(Validator):
    def __init__(self, rate_user = False, rate_ip = False,
                 prefix = 'rate_', *a, **kw):
        self.rate_user = rate_user
        self.rate_ip = rate_ip
        self.prefix = prefix
        Validator.__init__(self, *a, **kw)

    def run (self):
        to_check = []
        if self.rate_user and c.user_is_loggedin:
            to_check.append('user' + str(c.user._id36))
        if self.rate_ip:
            to_check.append('ip' + str(request.ip))

        r = cache.get_multi(to_check, self.prefix)
        if r:
            expire_time = max(r.values())
            time = utils.timeuntil(expire_time)
            c.errors.add(errors.RATELIMIT, {'time': time})

    @classmethod
    def ratelimit(self, rate_user = False, rate_ip = False, prefix = "rate_"):
        to_set = {}
        seconds = g.RATELIMIT*60

        if seconds <= 0:
            return

        expire_time = datetime.now(g.tz) + timedelta(seconds = seconds)
        if rate_user and c.user_is_loggedin:
            to_set['user' + str(c.user._id36)] = expire_time
        if rate_ip:
            to_set['ip' + str(request.ip)] = expire_time

        cache.set_multi(to_set, prefix, time = seconds)

class VCommentIDs(Validator):
    #id_str is a comma separated list of id36's
    def run(self, id_str):
        if not id_str:
            return None
        cids = [int(i, 36) for i in id_str.split(',')]
        comments = Comment._byID(cids, data=True, return_dict = False)
        return comments

class VFullNames(Validator):
    #id_str is a comma separated list of id36's
    def run(self, id_str):
        tids = id_str.split(',')
        return Thing._by_fullname(tids, data=True, return_dict = False)

class VSubreddits(Validator):
    #the subreddits are just in the post, this is for the my.reddit pref page
    def run(self):
        subreddits = Subreddit._by_fullname(request.post.keys())
        return subreddits.values()

class VCacheKey(Validator):
    def __init__(self, cache_prefix, param, *a, **kw):
        self.cache_prefix = cache_prefix
        Validator.__init__(self, param, *a, **kw)

    def run(self, key, name):
        if key:
            uid = cache.get(str(self.cache_prefix + "_" + key))
            try:
                a = Account._byID(uid, data = True)
            except NotFound:
                return None
            if name and a.name.lower() != name.lower():
                c.errors.add(errors.BAD_USERNAME)
            if a:
                return a
        c.errors.add(errors.EXPIRED)

class VOneOf(Validator):
    def __init__(self, param, options = (), *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        self.options = options

    def run(self, val):
        if self.options and val not in self.options:
            c.errors.add(errors.INVALID_OPTION)
            return self.default
        else:
            return val

class VReason(Validator):
    def run(self, reason):
        if not reason:
            return

        if reason.startswith('redirect_'):
            dest = reason[9:]
            if (not dest.startswith(c.site.path) and 
                not dest.startswith("http:")):
                dest = (c.site.path + dest).replace('//', '/')
            return ('redirect', dest)
        if reason.startswith('vote_'):
            fullname = reason[5:]
            t = Thing._by_fullname(fullname, data=True)
            return ('redirect', t.make_permalink_slow())
        elif reason.startswith('share_'):
            fullname = reason[6:]
            t = Thing._by_fullname(fullname, data=True)
            return ('redirect', t.make_permalink_slow())
        elif reason.startswith('reply_'):
            fullname = reason[6:]
            t = Thing._by_fullname(fullname, data=True)
            return ('redirect', t.make_permalink_slow())
        elif reason.startswith('sr_change_'):
            sr_list = reason[10:].split(',')
            fullnames = dict(i.split(':') for i in sr_list)
            srs = Subreddit._by_fullname(fullnames.keys(), data = True,
                                         return_dict = False)
            sr_onoff = dict((sr, fullnames[sr._fullname] == 1) for sr in srs)
            return ('subscribe', sr_onoff)

class ValidEmail(Validator):
    """Validates an email address"""
    
    email_re  = re.compile(r'.+@.+\..+')

    def __init__(self, param, **kw):
        Validator.__init__(self, param = param, **kw)
        
    def run(self, email):
        if not email:
            c.errors.add(errors.NO_EMAIL)
        elif not self.email_re.match(email):
            c.errors.add(errors.BAD_EMAIL)
        else:
            return email

class ValidEmails(Validator):
    """Validates a list of email addresses passed in as a string and
    delineated by whitespace, ',' or ';'.  Also validates quantity of
    provided emails.  Returns a list of valid email addresses on
    success"""
    
    separator = re.compile(r'[^\s,;]+')
    email_re  = re.compile(r'.+@.+\..+')

    def __init__(self, param, num = 20, **kw):
        self.num = num
        Validator.__init__(self, param = param, **kw)
        
    def run(self, emails0):
        emails = set(self.separator.findall(emails0) if emails0 else [])
        failures = set(e for e in emails if not self.email_re.match(e))
        emails = emails - failures

        # make sure the number of addresses does not exceed the max
        if self.num > 0 and len(emails) + len(failures) > self.num:
            # special case for 1: there should be no delineators at all, so
            # send back original string to the user
            if self.num == 1:
                c.errors.add(errors.BAD_EMAILS,
                             {'emails': '"%s"' % emails0})
            # else report the number expected
            else:
                c.errors.add(errors.TOO_MANY_EMAILS,
                             {'num': self.num})
        # correct number, but invalid formatting
        elif failures:
            c.errors.add(errors.BAD_EMAILS,
                         {'emails': ', '.join(failures)})
        # no emails
        elif not emails:
            c.errors.add(errors.NO_EMAILS)
        else:
            # return single email if one is expected, list otherwise
            return list(emails)[0] if self.num == 1 else emails


class VCnameDomain(Validator):
    domain_re  = re.compile(r'^([\w]+\.)+[\w]+$')

    def run(self, domain):
        if (domain
            and (not self.domain_re.match(domain)
                 or domain.endswith('.reddit.com')
                 or len(domain) > 300)):
            c.errors.add(errors.BAD_CNAME)
        elif domain:
            try:
                return str(domain).lower()
            except UnicodeEncodeError:
                c.errors.add(errors.BAD_CNAME)


class VWikiPageURL(Validator):
    page_name_re = re.compile('^[ -.0-:A-Z_-z]+$')

    def run(self, url):
        if not url or not url.startswith(WikiPageCached.url_prefix):
            c.errors.add(errors.BAD_URL)
            return None
        page_name = url[len(WikiPageCached.url_prefix):]
        if not self.page_name_re.match(page_name):
            c.errors.add(errors.BAD_URL)
            return None
        return url


# NOTE: make sure *never* to have res check these are present
# otherwise, the response could contain reference to these errors...!
class ValidIP(Validator):
    def run(self):
        if is_banned_IP(request.ip):
            c.errors.add(errors.BANNED_IP)
        return request.ip

class ValidDomain(Validator):
    def run(self, url):
        if url and is_banned_domain(url):
            c.errors.add(errors.BANNED_DOMAIN)

########NEW FILE########
__FILENAME__ = wikipagecontroller
from validator import *
from reddit_base import RedditController
from r2.lib.pages import *
from wiki_pages_embed import allWikiPagesCached
from pylons import response
from pylons.controllers.util import etag_cache

# Controller for pages pulled from wiki
class WikipageController(RedditController):

    # Get a full page with the wiki page embedded
    @validate(skiplayout=VBoolean('skiplayout'))
    def GET_wikipage(self,name,skiplayout):
        p = allWikiPagesCached[name]
        content_type = p.get('content-type', 'text/html')
        if content_type == 'text/html':
          if skiplayout:
              # Get just the html of the wiki page
              html = WikiPageCached(p).content()
              return WikiPageInline(html=html, name=name, skiplayout=skiplayout, wiki_url=p['url']).render()
          else:
              return WikiPage(name,p,skiplayout=skiplayout).render()
        else:
          # Send the content back as is, with cache control
          page = WikiPageCached(p)
          response.headers['Content-Type'] = content_type
          response.headers['Cache-Control'] = 'max-age=%d' % 300
          etag_cache(page.etag())
          return page.content()

    @validate(VUser(),
              url=VWikiPageURL('wiki_url'))
    def POST_invalidate_cache(self, url):
        if not url:
            return "Error"
        WikiPageCached({'url': url.encode('utf-8')}).invalidate()
        return "Done"

########NEW FILE########
__FILENAME__ = app_globals
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from __future__ import with_statement
from pylons import config
import pytz, os, logging, sys, socket
from datetime import timedelta
from r2.lib.cache import LocalCache, Memcache, CacheChain
from r2.lib.db.stats import QueryStats
from r2.lib.translation import _get_languages
from r2.lib.lock import make_lock_factory

class Globals(object):

    int_props = ['page_cache_time',
                 'MIN_DOWN_LINK',
                 'MIN_UP_KARMA',
                 'MIN_DOWN_KARMA',
                 'MIN_RATE_LIMIT_KARMA',
                 'MIN_RATE_LIMIT_COMMENT_KARMA',
                 'HOT_PAGE_AGE',
                 'MODWINDOW',
                 'RATELIMIT',
                 'num_comments',
                 'max_comments',
                 'num_side_reddits',
                 'num_query_queue_workers',
                 'max_sr_images',
                 'karma_to_post',
                 'discussion_karma_to_post',
                 'karma_to_vote',
                 'karma_to_vote_in_overview',
                 'karma_percentage_to_be_voted',
                 'meetup_grace_period',
                 'poll_max_choices',
                 'downvoted_reply_score_threshold',
                 'downvoted_reply_karma_cost',
                 'hide_comment_threshold',
                 'side_meetups_max_age',
                 'side_comments_max_age',
                 'side_posts_max_age',
                 'side_tags_max_age',
                 'side_contributors_max_age',
                 'post_karma_multiplier',
                 'article_navigation_max_age',
                 'meetups_radius',
                 ]
    
    bool_props = ['debug',
                  'translator',
                  'sqlprinting',
                  'template_debug',
                  'uncompressedJS',
                  'enable_doquery',
                  'use_query_cache',
                  'write_query_queue',
                  'css_killswitch',
                  'disable_captcha',
                  'disable_tracking_js',
                  'trust_local_proxies',
                  ]

    tuple_props = ['memcaches',
                   'rec_cache',
                   'permacaches',
                   'rendercaches',
                   'admins',
                   'sponsors',
                   'monitored_servers',
                   'default_srs',
                   'agents',
                   'allowed_css_linked_domains',
                   'feedbox_urls']

    def __init__(self, global_conf, app_conf, paths, **extra):
        """
        Globals acts as a container for objects available throughout
        the life of the application.

        One instance of Globals is created by Pylons during
        application initialization and is available during requests
        via the 'g' variable.
        
        ``global_conf``
            The same variable used throughout ``config/middleware.py``
            namely, the variables from the ``[DEFAULT]`` section of the
            configuration file.
            
        ``app_conf``
            The same ``kw`` dictionary used throughout
            ``config/middleware.py`` namely, the variables from the
            section in the config file for your application.
            
        ``extra``
            The configuration returned from ``load_config`` in 
            ``config/middleware.py`` which may be of use in the setup of
            your global variables.
            
        """

        def to_bool(x):
            return (x.lower() == 'true') if x else None
        
        def to_iter(name, delim = ','):
            return (x.strip() for x in global_conf.get(name, '').split(delim))


        # slop over all variables to start with
        for k, v in  global_conf.iteritems():
            if not k.startswith("_") and not hasattr(self, k):
                if k in self.int_props:
                    v = int(v)
                elif k in self.bool_props:
                    v = to_bool(v)
                elif k in self.tuple_props:
                    v = tuple(to_iter(k))
                setattr(self, k, v)

        # initialize caches
        mc = Memcache(self.memcaches)
        self.cache = CacheChain((LocalCache(), mc))
        self.permacache = Memcache(self.permacaches)
        self.rendercache = Memcache(self.rendercaches)
        self.make_lock = make_lock_factory(mc)

        self.rec_cache = Memcache(self.rec_cache)
        
        # set default time zone if one is not set
        self.tz = pytz.timezone(global_conf.get('timezone'))

        #make a query cache
        self.stats_collector = QueryStats()

        # set the modwindow
        self.MODWINDOW = timedelta(self.MODWINDOW)

        self.REDDIT_MAIN = bool(os.environ.get('REDDIT_MAIN'))

        # turn on for language support
        self.languages, self.lang_name = _get_languages()

        all_languages = self.lang_name.keys()
        all_languages.sort()
        self.all_languages = all_languages

        # load the md5 hashes of files under static
        static_files = os.path.join(paths.get('static_files'), 'static')
        self.static_md5 = {}
        if os.path.exists(static_files):
            for f in os.listdir(static_files):
                if f.endswith('.md5'):
                    key = f[0:-4]
                    f = os.path.join(static_files, f)
                    with open(f, 'r') as handle:
                        md5 = handle.read().strip('\n')
                    self.static_md5[key] = md5


        #set up the logging directory
        log_path = self.log_path
        process_iden = global_conf.get('scgi_port', 'default')
        if log_path:
            if not os.path.exists(log_path):
                os.makedirs(log_path)
            for fname in os.listdir(log_path):
                if fname.startswith(process_iden):
                    full_name = os.path.join(log_path, fname)
                    os.remove(full_name)

        #setup the logger
        self.log = logging.getLogger('reddit')
        self.log.addHandler(logging.StreamHandler())
        if self.debug:
            self.log.setLevel(logging.DEBUG)

        #read in our CSS so that it can become a default for subreddit
        #stylesheets
        stylesheet_path = os.path.join(paths.get('static_files'),
                                       self.static_path.lstrip('/'),
                                       self.stylesheet)
        with open(stylesheet_path) as s:
            self.default_stylesheet = s.read()

        self.reddit_host = socket.gethostname()
        self.reddit_pid  = os.getpid()

    def __del__(self):
        """
        Put any cleanup code to be run when the application finally exits 
        here.
        """
        pass


########NEW FILE########
__FILENAME__ = base
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################

from pylons import Response, c, g, cache, request, session, config
from pylons.controllers import WSGIController, Controller
from pylons.i18n import N_, _, ungettext, get_lang
import r2.lib.helpers as h
from r2.lib.utils import to_js
from r2.lib.filters import spaceCompress, _force_unicode, _force_utf8
from utils import storify, string2js, read_http_date

import re, md5
from urllib import quote 

#TODO hack
import logging
from r2.lib.utils import UrlParser, query_string
logging.getLogger('scgi-wsgi').setLevel(logging.CRITICAL)


def is_local_address(ip):
    # TODO: support the /20 and /24 private networks? make this configurable?
    return ip.startswith('10.')

class BaseController(WSGIController):
    def __after__(self):
        self.post()

    def __before__(self):
        self.pre()

    def __call__(self, environ, start_response):
        true_client_ip = environ.get('HTTP_TRUE_CLIENT_IP')
        ip_hash = environ.get('HTTP_TRUE_CLIENT_IP_HASH')
        forwarded_for = environ.get('HTTP_X_FORWARDED_FOR', ())
        remote_addr = environ.get('REMOTE_ADDR')

        if (g.ip_hash
            and true_client_ip
            and ip_hash
            and md5.new(true_client_ip + g.ip_hash).hexdigest() \
            == ip_hash.lower()):
            request.ip = true_client_ip
        elif g.trust_local_proxies and forwarded_for and is_local_address(remote_addr):
            request.ip = forwarded_for.split(',')[-1]
        else:
            request.ip = environ['REMOTE_ADDR']

        #if x-dont-decode is set, pylons won't unicode all the paramters
        if environ.get('HTTP_X_DONT_DECODE'):
            request.charset = None

        request.get = storify(request.GET)
        request.post = storify(request.POST)
        request.referer = environ.get('HTTP_REFERER')
        request.path = _force_utf8(environ.get('PATH_INFO'))      # Enforce only valid utf8 chars in request path
        request.user_agent = environ.get('HTTP_USER_AGENT')
        request.fullpath = environ.get('FULLPATH', request.path)
        request.port = environ.get('request_port')
        
        if_modified_since = environ.get('HTTP_IF_MODIFIED_SINCE')
        if if_modified_since:
            request.if_modified_since = read_http_date(if_modified_since)
        else:
            request.if_modified_since = None

        #set the function to be called
        action = request.environ['pylons.routes_dict'].get('action')
        if action:
            meth = request.method.upper()
            if meth == 'HEAD':
                meth = 'GET'
            request.environ['pylons.routes_dict']['action'] = \
                    meth + '_' + action

        c.response = Response()
        res = WSGIController.__call__(self, environ, start_response)
        return res
            
    def pre(self): pass
    def post(self): pass


    @classmethod
    def format_output_url(cls, url, **kw):
        """
        Helper method used during redirect to ensure that the redirect
        url (assisted by frame busting code or javasctipt) will point
        to the correct domain and not have any extra dangling get
        parameters.  The extensions are also made to match and the
        resulting url is utf8 encoded.

        Node: for development purposes, also checks that the port
        matches the request port
        """
        u = UrlParser(url)

        if u.is_reddit_url():
            # make sure to pass the port along if not 80
            if not kw.has_key('port'):
                kw['port'] = request.port
    
            # disentagle the cname (for urls that would have cnameframe=1 in them)
            u.mk_cname(**kw)
    
            # make sure the extensions agree with the current page
            if c.extension:
                u.set_extension(c.extension)

        # unparse and encode it un utf8
        return _force_unicode(u.unparse()).encode('utf8')


    @classmethod
    def intermediate_redirect(cls, form_path):
        """
        Generates a /login or /over18 redirect from the current
        fullpath, after having properly reformated the path via
        format_output_url.  The reformatted original url is encoded
        and added as the "dest" parameter of the new url.
        """
        from r2.lib.template_helpers import add_sr
        dest = cls.format_output_url(request.fullpath)
        path = add_sr(form_path + query_string({"dest": dest}))
        return cls.redirect(path)
    
    @classmethod
    def redirect(cls, dest, code = 302):
        """
        Reformats the new Location (dest) using format_output_url and
        sends the user to that location with the provided HTTP code.
        """
        dest = cls.format_output_url(dest)
        c.response.headers['Location'] = dest
        c.response.status_code = code
        return c.response

    def sendjs(self,js, callback="document.write", escape=True):
        c.response.headers['Content-Type'] = 'text/javascript'
        c.response.content = to_js(js, callback, escape)
        return c.response

import urllib2
class EmbedHandler(urllib2.BaseHandler, urllib2.HTTPHandler, 
                   urllib2.HTTPErrorProcessor, urllib2.HTTPDefaultErrorHandler):
    @staticmethod
    def redirect(_status):
        def _redirect(url, status = None):
            MethodController.redirect(url, code = _status)
        return _redirect
    
    def http_redirect(self, req, fp, code, msg, hdrs):
        codes = [301, 302, 303, 307]
        map = dict((x, self.redirect(x)) for x in codes)
        to = hdrs['Location'].replace('reddit.infogami.com', g.domain)
        map[code](to)
        raise StopIteration

    http_error_301 = http_redirect
    http_error_302 = http_redirect
    http_error_303 = http_redirect
    http_error_307 = http_redirect

embedopen = urllib2.OpenerDirector()
embedopen.add_handler(EmbedHandler())

def proxyurl(url):
    cstrs = ['%s="%s"' % (k, v.value) for k, v in c.cookies.iteritems()]
    cookiestr = "; ".join(cstrs)
    headers = {"Cookie":cookiestr}

    # TODO make this work on POST
    data = None
    r = urllib2.Request(url, data, headers)
    content = embedopen.open(r).read()
    return content

def current_login_cookie():
    return c.cookies[g.login_cookie].value if (g.login_cookie in c.cookies) else ''

__all__ = [__name for __name in locals().keys() if not __name.startswith('_') \
           or __name == '_']



########NEW FILE########
__FILENAME__ = cache
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from threading import local

from utils import lstrips
from contrib import memcache

class CacheUtils(object):
    def incr_multi(self, keys, amt=1, prefix=''):
        for k in keys:
            try:
                self.incr(prefix + k, amt)
            except ValueError:
                pass

    def add_multi(self, keys, prefix=''):
        for k,v in keys.iteritems():
            self.add(prefix+str(k), v)

    def get_multi(self, keys, prefix='', partial=True):
        if prefix:
            key_map = dict((prefix+str(k), k) for k in keys)
        else:
            key_map = dict((str(k), k) for k in keys)

        r = self.simple_get_multi(key_map.keys())

        if not partial and len(r.keys()) < len(key_map):
            return None

        return dict((key_map[k], r[k]) for k in r.keys())

    def get_key_group_value(self, prefix):
        """ This is used to allow the expiring of a group of keys
        Use the value returned by "get_key_group_value" in all cache
        keys for items you want to expire together """
        return self.get(prefix, 0)

    def invalidate_key_group(self, prefix):
        """ Expire a group of keys - use together with get_key_group_value """
        self.add(prefix, 0)
        self.incr(prefix)

class Memcache(CacheUtils, memcache.Client):
    simple_get_multi = memcache.Client.get_multi

    def set_multi(self, keys, prefix='', time=0):

        new_keys = {}
        for k,v in keys.iteritems():
            new_keys[str(k)] = v
        memcache.Client.set_multi(self, new_keys, key_prefix = prefix,
                                  time = time)

    def get(self, key, default=None):
        r = memcache.Client.get(self, key)
        if r is None: return default
        return r

    def set(self, key, val, time=0):
        memcache.Client.set(self, key, val, time = time)

    def delete(self, key, time=0):
        memcache.Client.delete(self, key, time=time)

    def delete_multi(self, keys, prefix='', time=0):
        memcache.Client.delete_multi(self, keys, seconds = time,
                                     key_prefix = prefix)

class LocalCache(dict, CacheUtils):
    def __init__(self, *a, **kw):
        return dict.__init__(self, *a, **kw)

    def _check_key(self, key):
        if not isinstance(key, str):
            raise TypeError('Key must be a string.')

    def get(self, key, default=None):
        r = dict.get(self, key)
        if r is None: return default
        return r

    def simple_get_multi(self, keys):
        out = {}
        for k in keys:
            if self.has_key(k):
                out[k] = self[k]
#        print "Local cache answers: " + str(out)
        return out

    def set(self, key, val, time = 0):
        self._check_key(key)
        self[key] = val

    def set_multi(self, keys, prefix='', time=0):
        for k,v in keys.iteritems():
            self.set(prefix+str(k), v)

    def add(self, key, val):
        self._check_key(key)
        self.setdefault(key, val)

    def delete(self, key):
        if self.has_key(key):
            del self[key]

    def delete_multi(self, keys):
        for key in keys:
            if self.has_key(key):
                del self[key]

    def incr(self, key, amt=1):
        if self.has_key(key):
            self[key] += amt

    def decr(self, key, amt=1): 
        if self.has_key(key):
            self[key] -= amt

    def flush_all(self):
        self.clear()

class CacheChain(CacheUtils, local):
    def __init__(self, caches):
        self.caches = caches

    def make_set_fn(fn_name):
        def fn(self, *a, **kw):
            for c in self.caches:
                getattr(c, fn_name)(*a, **kw)
        return fn

    set = make_set_fn('set')
    set_multi = make_set_fn('set_multi')
    add = make_set_fn('add')
    incr = make_set_fn('incr')
    decr = make_set_fn('decr')
    delete = make_set_fn('delete')
    delete_multi = make_set_fn('delete_multi')
    flush_all = make_set_fn('flush_all')

    def get(self, key, default=None):
        for c in self.caches:
            val = c.get(key, default)
            if val is not None:
                #update other caches
                for d in self.caches:
                    if c == d:
                        break;
                    d.set(key, val)
                return val
        #didn't find anything
        return default

    def simple_get_multi(self, keys):
        out = {}
        need = set(keys)
        for c in self.caches:
            if len(out) == len(keys):
                break
            r = c.simple_get_multi(need)
            #update other caches
            if r:
                for d in self.caches:
                    if c == d:
                        break;
                    d.set_multi(r)
                r.update(out)
                out = r
                need = need - set(r.keys())
        return out

#smart get multi
def sgm(cache, keys, miss_fn, prefix='', time=0):
    keys = set(keys)
    s_keys = dict((str(k), k) for k in keys)
    r = cache.get_multi(s_keys.keys(), prefix)
    if miss_fn and len(r.keys()) < len(keys):
        need = set(s_keys.keys()) - set(r.keys())
        #TODO i can't send a generator
        nr = miss_fn([s_keys[i] for i in need])
        nr = dict((str(k), v) for k,v in nr.iteritems())
        r.update(nr)
        cache.set_multi(nr, prefix, time = time)

    return dict((s_keys[k], v) for k,v in r.iteritems())

def test_cache(cache):
    #basic set/get
    cache.set('1', 1)
    assert(cache.get('1') == 1)

    #python data
    cache.set('2', [1,2,3])
    assert(cache.get('2') == [1,2,3])

    #set multi, no prefix
    cache.set_multi({'3':3, '4': 4})
    assert(cache.get_multi(('3', '4')) == {'3':3, '4': 4})

    #set multi, prefix
    cache.set_multi({'3':3, '4': 4}, prefix='p_')
    assert(cache.get_multi(('3', 4), prefix='p_') == {'3':3, 4: 4})
    assert(cache.get_multi(('p_3', 'p_4')) == {'p_3':3, 'p_4': 4})

    #incr
    cache.set('5', 1)
    cache.set('6', 1)
    cache.incr('5')
    assert(cache.get('5'), 2)
    cache.incr('5',2)
    assert(cache.get('5'), 4)
    cache.incr_multi(('5', '6'), 1)
    assert(cache.get('5'), 5)    
    assert(cache.get('6'), 2)

# a cache that occasionally dumps itself to be used for long-running
# processes
class SelfEmptyingCache(LocalCache):
    def __init__(self,max_size=100*1000):
        self.max_size = max_size

    def maybe_reset(self):
        if len(self) > self.max_size:
            self.clear()

    def set(self,key,val,time = 0):
        self.maybe_reset()
        return LocalCache.set(self,key,val,time)
    def add(self,key,val):
        return self.set(key,val)

########NEW FILE########
__FILENAME__ = captcha
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
import random, string
#TODO find a better way to cache the captchas
from r2.config import cache
from Captcha.Base import randomIdentifier
from Captcha.Visual import Text, Backgrounds, Distortions, ImageCaptcha
from pylons import g

IDEN_LENGTH = 32
SOL_LENGTH = 6

class RandCaptcha(ImageCaptcha):
    defaultSize = (120, 50)
    fontFactory = Text.FontFactory(18, "vera/VeraBd.ttf")

    def getLayers(self, solution="blah"):
        self.addSolution(solution)
        return ((Backgrounds.Grid(size=8, foreground="white"),
                 Distortions.SineWarp(amplitudeRange=(5,9))),
                (Text.TextLayer(solution,
                               textColor = 'white',
                               fontFactory = self.fontFactory),
                 Distortions.SineWarp()))

def get_iden():
    return randomIdentifier(length=IDEN_LENGTH)

def make_solution():
    return randomIdentifier(alphabet=string.ascii_letters, length = SOL_LENGTH).upper()

def get_image(iden):
    solution = cache.get(str(iden))
    if not solution:
        solution = make_solution()
        cache.set(str(iden), solution, time = 300)
    r = RandCaptcha(solution=solution)
    return r.render()

def valid_solution(iden, solution):
    if getattr(g,'disable_captcha', False):
        return True
    if (not iden
        or not solution
        or len(iden) != IDEN_LENGTH
        or len(solution) != SOL_LENGTH
        or solution.upper() != cache.get(str(iden))): 
        solution = make_solution()
        cache.set(str(iden), solution, time = 300)
        return False
    else:
        cache.delete(str(iden))
        return True

########NEW FILE########
__FILENAME__ = categories
from r2.models import Subreddit, Account

def create_as_user_named(username):
    categories = {
        'academia': 'Academia',
        # 'ads': 'Ads',
        # 'age': 'Age',
        'ai': 'Ai',
        'arts': 'Arts',
        'bayesian': 'Bayesian',
        # 'charity': 'Charity',
        # 'currentaffairs': 'Current Affairs',
        'disagreement': 'Disagreement',
        # 'disaster': 'Disaster',
        # 'epistemology': 'Epistemology',
        # 'fiction': 'Fiction',
        # 'finance': 'Finance',
        'future': 'Future',
        # 'gender': 'Gender',
        # 'humor': 'Humor',
        # 'hypocrisy': 'Hypocrisy',
        # 'innovation': 'Innovation',
        'law': 'Law',
        'mating': 'Mating',
        'media': 'Media',
        'medicine': 'Medicine',
        'meta': 'Meta',
        'morality': 'Morality',
        # 'music': 'Music',
        # 'naturalism': 'Naturalism',
        # 'openthread': 'Open Thread',
        'overconfidence': 'Overconfidence',
        # 'personal': 'Personal',
        'philosophy': 'Philosophy',
        'politics': 'Politics',
        'predictionmarkets': 'Prediction markets',
        'psychology': 'Psychology',
        # 'reductionism': 'Reductionism',
        # 'regulation': 'Regulation',
        'religion': 'Religion',
        'science': 'Science',
        'selfdeception': 'Self-deception',
        # 'signaling': 'Signaling',
        # 'socialscience': 'Social science',
        # 'sports': 'Sports',
        'standardbiases': 'Standard biases',
        'statistics': 'Statistics',
        # 'status': 'Status',
        # 'war': 'War',
        # 'webtech': 'Web/tech',
    }

    kw = {
        'lang': 'en',
        'over_18': False,
        'show_media': False,
        'type': 'public',
        'default_listing': 'hot',
    }

    user = Account._by_name(username)

    for name, title in categories.iteritems():
        kw['title'] = title
        sr = Subreddit._create_and_subscribe(name, user, kw)
        print "Created " + name

########NEW FILE########
__FILENAME__ = comment_tree
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from __future__ import with_statement

from pylons import g

from r2.models import *

def comments_key(link_id):
    return 'comments_' + str(link_id)

def lock_key(link_id):
    return 'comment_lock_' + str(link_id)

def add_comment(comment):
    with g.make_lock(lock_key(comment.link_id)):
        add_comment_nolock(comment)

def add_comment_nolock(comment):
    cm_id = comment._id
    p_id = comment.parent_id if hasattr(comment, 'parent_id') else None
    link_id = comment.link_id

    cids, comment_tree, depth, num_children = link_comments(link_id)

    # Only add this comment if it is not already present.  In
    # link_comments a cache miss will rebuild all comments including
    # the one we are adding now.
    if comment._id not in cids:
        #add to comment list
        cids.append(comment._id)

        #add to tree
        comment_tree.setdefault(p_id, []).append(cm_id)

        #add to depth
        depth[cm_id] = depth[p_id] + 1 if p_id else 0

        #update children
        num_children[cm_id] = 0

        #dfs to find the list of parents for the new comment
        def find_parents():
            stack = [cid for cid in comment_tree[None]]
            parents = []
            while stack:
                cur_cm = stack.pop()
                if cur_cm == cm_id:
                    return parents
                elif comment_tree.has_key(cur_cm):
                    #make cur_cm the end of the parents list
                    parents = parents[:depth[cur_cm]] + [cur_cm]
                    for child in comment_tree[cur_cm]:
                        stack.append(child)


        #if this comment had a parent, find the parent's parents
        if p_id:
            for p_id in find_parents():
                num_children[p_id] += 1

        g.permacache.set(comments_key(link_id),
                         (cids, comment_tree, depth, num_children))

def delete_comment(comment):
    #nothing really to do here, atm
    pass

def link_comments(link_id):
    key = comments_key(link_id)
    r = g.permacache.get(key)
    if r:
        return r
    else:
        with g.make_lock(lock_key(link_id)):
            r = load_link_comments(link_id)
            g.permacache.set(key, r)
        return r

def load_link_comments(link_id):
    q = Comment._query(Comment.c.link_id == link_id,
                       Comment.c._deleted == (True, False),
                       Comment.c._spam == (True, False),
                       data = True)
    comments = list(q)
    cids = [c._id for c in comments]

    #make a tree
    comment_tree = {}
    for cm in comments:
        p_id = cm.parent_id if hasattr(cm, 'parent_id') else None
        comment_tree.setdefault(p_id, []).append(cm._id)

    #calculate the depths
    depth = {}
    level = 0
    cur_level = comment_tree.get(None, ())
    while cur_level:
        next_level = []
        for cm_id in cur_level:
            depth[cm_id] = level
            next_level.extend(comment_tree.get(cm_id, ()))
        cur_level = next_level
        level += 1

    #calc the number of children
    num_children = {}
    for cm_id in cids:
        num = 0
        todo = [cm_id]
        while todo:
            more = comment_tree.get(todo.pop(0), ())
            num += len(more)
            todo.extend(more)
        num_children[cm_id] = num

    return cids, comment_tree, depth, num_children

########NEW FILE########
__FILENAME__ = markdown
#!/usr/bin/python
import re, md5, sys, string

"""markdown.py: A Markdown-styled-text to HTML converter in Python.

Usage:
  ./markdown.py textfile.markdown
 
Calling:
  import markdown
  somehtml = markdown.markdown(sometext)
"""

__version__ = '1.0.1-2' # port of 1.0.1
__license__ = "GNU GPL 2"
__author__ = [
  'John Gruber <http://daringfireball.net/>',
  'Tollef Fog Heen <tfheen@err.no>', 
  'Aaron Swartz <me@aaronsw.com>'
]

def htmlquote(text):
    """Encodes `text` for raw use in HTML."""
    text = text.replace("&", "&amp;") # Must be done first!
    text = text.replace("<", "&lt;")
    text = text.replace(">", "&gt;")
    text = text.replace("'", "&#39;")
    text = text.replace('"', "&quot;")
    return text

def mangle_text(text):
    from pylons import g
    return md5.new(text + g.SECRET).hexdigest()

def semirandom(seed):
    from pylons import g
    x = 0
    for c in md5.new(seed + g.SECRET).digest(): x += ord(c)
    return x / (255*16.)

class _Markdown:
    emptyelt = " />"
    tabwidth = 4

    escapechars = '\\`*_{}[]()>#+-.!'
    escapetable = {}
    for char in escapechars:
        escapetable[char] = mangle_text(char)
    
    r_multiline = re.compile("\n{2,}")
    r_stripspace = re.compile(r"^[ \t]+$", re.MULTILINE)
    def parse(self, text):
        self.urls = {}
        self.titles = {}
        self.html_blocks = {}
        self.list_level = 0
        
        text = text.replace("\r\n", "\n")
        text = text.replace("\r", "\n")
        text += "\n\n"
        text = self._Detab(text)
        text = self.r_stripspace.sub("", text)
        text = self._HashHTMLBlocks(text)
        text = self._StripLinkDefinitions(text)
        text = self._RunBlockGamut(text)
        text = self._UnescapeSpecialChars(text)
        return text
    
    r_StripLinkDefinitions = re.compile(r"""
    ^[ ]{0,%d}\[(.+)\]:  # id = $1
      [ \t]*\n?[ \t]*
    <?(\S+?)>?           # url = $2
      [ \t]*\n?[ \t]*
    (?:
      (?<=\s)            # lookbehind for whitespace
      [\"\(]             # " is backlashed so it colorizes our code right
      (.+?)              # title = $3
      [\"\)]
      [ \t]*
    )?                   # title is optional
    (?:\n+|\Z)
    """ % (tabwidth-1), re.MULTILINE|re.VERBOSE)
    def _StripLinkDefinitions(self, text):
        def replacefunc(matchobj):
            (t1, t2, t3) = matchobj.groups()
            #@@ case sensitivity?
            self.urls[t1.lower()] = self._EncodeAmpsAndAngles(t2)
            if t3 is not None:
                self.titles[t1.lower()] = t3.replace('"', '&quot;')
            return ""

        text = self.r_StripLinkDefinitions.sub(replacefunc, text)
        return text

    blocktagsb = r"p|div|h[1-6]|blockquote|pre|table|dl|ol|ul|script|math"
    blocktagsa = blocktagsb + "|ins|del"
    
    r_HashHTMLBlocks1 = re.compile(r"""
    (            # save in $1
    ^            # start of line  (with /m)
    <(%s)        # start tag = $2
    \b           # word break
    (.*\n)*?     # any number of lines, minimally matching
    </\2>        # the matching end tag
    [ \t]*       # trailing spaces/tabs
    (?=\n+|$)    # followed by a newline or end of document
    )
    """ % blocktagsa, re.MULTILINE | re.VERBOSE)

    r_HashHTMLBlocks2 = re.compile(r"""
    (            # save in $1
    ^            # start of line  (with /m)
    <(%s)        # start tag = $2
    \b           # word break
    (.*\n)*?     # any number of lines, minimally matching
    .*</\2>      # the matching end tag
    [ \t]*       # trailing spaces/tabs
    (?=\n+|\Z)   # followed by a newline or end of document
    )
    """ % blocktagsb, re.MULTILINE | re.VERBOSE)

    r_HashHR = re.compile(r"""
    (?:
    (?<=\n\n)    # Starting after a blank line
    |            # or
    \A\n?        # the beginning of the doc
    )
    (            # save in $1
    [ ]{0,%d}
    <(hr)        # start tag = $2
    \b           # word break
    ([^<>])*?    # 
    /?>          # the matching end tag
    [ \t]*
    (?=\n{2,}|\Z)# followed by a blank line or end of document
    )
    """ % (tabwidth-1), re.VERBOSE)
    r_HashComment = re.compile(r"""
    (?:
    (?<=\n\n)    # Starting after a blank line
    |            # or
    \A\n?        # the beginning of the doc
    )
    (            # save in $1
    [ ]{0,%d}
    (?: 
      <!
      (--.*?--\s*)+
      >
    )
    [ \t]*
    (?=\n{2,}|\Z)# followed by a blank line or end of document
    )
    """ % (tabwidth-1), re.VERBOSE)

    def _HashHTMLBlocks(self, text):
        def handler(m):
            key = m.group(1)
            try:
                key = key.encode('utf8')
            except UnicodeDecodeError:
                key = ''.join(k for k in key if ord(k) < 128)
            key = mangle_text(key)
            self.html_blocks[key] = m.group(1)
            return "\n\n%s\n\n" % key

        text = self.r_HashHTMLBlocks1.sub(handler, text)
        text = self.r_HashHTMLBlocks2.sub(handler, text)
        oldtext = text
        text = self.r_HashHR.sub(handler, text)
        text = self.r_HashComment.sub(handler, text)
        return text

    #@@@ wrong!
    r_hr1 = re.compile(r'^[ ]{0,2}([ ]?\*[ ]?){3,}[ \t]*$', re.M)
    r_hr2 = re.compile(r'^[ ]{0,2}([ ]?-[ ]?){3,}[ \t]*$', re.M)
    r_hr3 = re.compile(r'^[ ]{0,2}([ ]?_[ ]?){3,}[ \t]*$', re.M)
	
    def _RunBlockGamut(self, text):
        text = self._DoHeaders(text)
        for x in [self.r_hr1, self.r_hr2, self.r_hr3]:
            text = x.sub("\n<hr%s\n" % self.emptyelt, text);
        text = self._DoLists(text)
        text = self._DoCodeBlocks(text)
        text = self._DoBlockQuotes(text)

    	# We did this in parse()
    	# to escape the source
    	# now it's stuff _we_ made
    	# so we don't wrap it in <p>s.
        text = self._HashHTMLBlocks(text)
        text = self._FormParagraphs(text)
        return text

    r_NewLine = re.compile(" {2,}\n")
    def _RunSpanGamut(self, text):
        text = self._DoCodeSpans(text)
        text = self._EscapeSpecialChars(text)
        text = self._DoImages(text)
        text = self._DoAnchors(text)
        text = self._DoAutoLinks(text)
        text = self._EncodeAmpsAndAngles(text)
        text = self._DoItalicsAndBold(text)
        text = self.r_NewLine.sub(" <br%s\n" % self.emptyelt, text)
        return text

    def _EscapeSpecialChars(self, text):
        tokens = self._TokenizeHTML(text)
        text = ""
        for cur_token in tokens:
            if cur_token[0] == "tag":
                cur_token[1] = cur_token[1].replace('*', self.escapetable["*"])
                cur_token[1] = cur_token[1].replace('_', self.escapetable["_"])
                text += cur_token[1]
            else:
                text += self._EncodeBackslashEscapes(cur_token[1])
        return text

    r_DoAnchors1 = re.compile(
          r""" (                 # wrap whole match in $1
                  \[
                    (.*?)        # link text = $2 
                    # [for bracket nesting, see below]
                  \]

                  [ ]?           # one optional space
                  (?:\n[ ]*)?    # one optional newline followed by spaces

                  \[
                    (.*?)        # id = $3
                  \]
                )
    """, re.S|re.VERBOSE)
    r_DoAnchors2 = re.compile(
          r""" (                   # wrap whole match in $1
                  \[
                    (.*?)          # link text = $2
                  \]
                  \(               # literal paren
                        [ \t]*
                        <?(.+?)>?  # href = $3
                        [ \t]*
                        (          # $4
                          ([\'\"]) # quote char = $5
                          (.*?)    # Title = $6
                          \5       # matching quote
                        )?         # title is optional
                  \)
                )
    """, re.S|re.VERBOSE)
    def _DoAnchors(self, text): 
        # We here don't do the same as the perl version, as python's regex
        # engine gives us no way to match brackets.

        def handler1(m):
            whole_match = m.group(1)
            link_text = m.group(2)
            link_id = m.group(3).lower()
            if not link_id: link_id = link_text.lower()
            title = self.titles.get(link_id, None)
                

            if self.urls.has_key(link_id):
                url = self.urls[link_id]
                url = url.replace("*", self.escapetable["*"])
                url = url.replace("_", self.escapetable["_"])
                res = '<a href="%s"' % htmlquote(url)

                if not re.search('lesswrong|overcomingbias', res):
                    res += ' rel="nofollow"'

                if title:
                    title = title.replace("*", self.escapetable["*"])
                    title = title.replace("_", self.escapetable["_"])
                    res += ' title="%s"' % htmlquote(title)
                res += ">%s</a>" % htmlquote(link_text)
            else:
                res = whole_match
            return res

        def handler2(m):
            whole_match = m.group(1)
            link_text = m.group(2)
            url = m.group(3)
            title = m.group(6)

            url = url.replace("*", self.escapetable["*"])
            url = url.replace("_", self.escapetable["_"])
            res = '''<a href="%s"''' % htmlquote(url)

            if not re.search('lesswrong|overcomingbias', res):
                res += ' rel="nofollow"'

            if title:
                title = title.replace('"', '&quot;')
                title = title.replace("*", self.escapetable["*"])
                title = title.replace("_", self.escapetable["_"])
                res += ' title="%s"' % htmlquote(title)
            res += ">%s</a>" % htmlquote(link_text)
            return res

        #text = self.r_DoAnchors1.sub(handler1, text)
        text = self.r_DoAnchors2.sub(handler2, text)
        return text

    r_DoImages1 = re.compile(
           r""" (                       # wrap whole match in $1
                  !\[
                    (.*?)               # alt text = $2
                  \]

                  [ ]?                  # one optional space
                  (?:\n[ ]*)?           # one optional newline followed by spaces

                  \[
                    (.*?)               # id = $3
                  \]

                )
    """, re.VERBOSE|re.S)

    r_DoImages2 = re.compile(
          r""" (                        # wrap whole match in $1
                  !\[
                    (.*?)               # alt text = $2
                  \]
                  \(                    # literal paren
                        [ \t]*
                        <?(\S+?)>?      # src url = $3
                        [ \t]*
                        (               # $4
                        ([\'\"])        # quote char = $5
                          (.*?)         # title = $6
                          \5            # matching quote
                          [ \t]*
                        )?              # title is optional
                  \)
                )
    """, re.VERBOSE|re.S)

    def _DoImages(self, text):
        def handler1(m):
            whole_match = m.group(1)
            alt_text = m.group(2)
            link_id = m.group(3).lower()

            if not link_id:
                link_id = alt_text.lower()

            alt_text = alt_text.replace('"', "&quot;")
            if self.urls.has_key(link_id):
                url = self.urls[link_id]
                url = url.replace("*", self.escapetable["*"])
                url = url.replace("_", self.escapetable["_"])
                res = '''<img src="%s" alt="%s"''' % (htmlquote(url), htmlquote(alt_text))
                if self.titles.has_key(link_id):
                    title = self.titles[link_id]
                    title = title.replace("*", self.escapetable["*"])
                    title = title.replace("_", self.escapetable["_"])
                    res += ' title="%s"' % htmlquote(title)
                res += self.emptyelt
            else:
                res = whole_match
            return res

        def handler2(m):
            whole_match = m.group(1)
            alt_text = m.group(2)
            url = m.group(3)
            title = m.group(6) or ''
            
            alt_text = alt_text.replace('"', "&quot;")
            title = title.replace('"', "&quot;")
            url = url.replace("*", self.escapetable["*"])
            url = url.replace("_", self.escapetable["_"])
            res = '<img src="%s" alt="%s"' % (htmlquote(url), htmlquote(alt_text))
            if title is not None:
                title = title.replace("*", self.escapetable["*"])
                title = title.replace("_", self.escapetable["_"])
                res += ' title="%s"' % htmlquote(title)
            res += self.emptyelt
            return res

        text = self.r_DoImages1.sub(handler1, text)
        text = self.r_DoImages2.sub(handler2, text)
        return text
    
    r_DoHeaders = re.compile(r"^(\#{1,6})[ \t]*(.+?)[ \t]*\#*\n+", re.VERBOSE|re.M)
    def _DoHeaders(self, text):
        def findheader(text, c, n):
            textl = text.split('\n')
            for i in xrange(len(textl)):
                if i >= len(textl): continue
                count = textl[i].strip().count(c)
                if count > 0 and count == len(textl[i].strip()) and textl[i+1].strip() == '' and textl[i-1].strip() != '':
                    textl = textl[:i] + textl[i+1:]
                    textl[i-1] = '<h'+n+'>'+self._RunSpanGamut(textl[i-1])+'</h'+n+'>'
                    textl = textl[:i] + textl[i+1:]
            text = '\n'.join(textl)
            return text
        
        def handler(m):
            level = len(m.group(1))
            header = self._RunSpanGamut(m.group(2))
            return "<h%s>%s</h%s>\n\n" % (level, header, level)

        text = findheader(text, '=', '1')
        text = findheader(text, '-', '2')
        text = self.r_DoHeaders.sub(handler, text)
        return text
    
    rt_l = r"""
    (
      (
        [ ]{0,%d}
        ([*+-]|\d+[.])
        [ \t]+
      )
      (?:.+?)
      (
        \Z
      |
        \n{2,}
        (?=\S)
        (?![ \t]* ([*+-]|\d+[.])[ \t]+)
      )
    )
    """ % (tabwidth - 1)
    r_DoLists = re.compile('^'+rt_l, re.M | re.VERBOSE | re.S)
    r_DoListsTop = re.compile(
      r'(?:\A\n?|(?<=\n\n))'+rt_l, re.M | re.VERBOSE | re.S)
    
    def _DoLists(self, text):
        def handler(m):
            list_type = "ol"
            if m.group(3) in [ "*", "-", "+" ]:
                list_type = "ul"
            listn = m.group(1)
            listn = self.r_multiline.sub("\n\n\n", listn)
            res = self._ProcessListItems(listn)
            res = "<%s>\n%s</%s>\n" % (list_type, res, list_type)
            return res
            
        if self.list_level:
            text = self.r_DoLists.sub(handler, text)
        else:
            text = self.r_DoListsTop.sub(handler, text)
        return text

    r_multiend = re.compile(r"\n{2,}\Z")
    r_ProcessListItems = re.compile(r"""
    (\n)?                            # leading line = $1
    (^[ \t]*)                        # leading whitespace = $2
    ([*+-]|\d+[.]) [ \t]+            # list marker = $3
    ((?:.+?)                         # list item text = $4
    (\n{1,2}))
    (?= \n* (\Z | \2 ([*+-]|\d+[.]) [ \t]+))
    """, re.VERBOSE | re.M | re.S)

    def _ProcessListItems(self, text):
        self.list_level += 1
        text = self.r_multiend.sub("\n", text)
        
        def handler(m):
            item = m.group(4)
            leading_line = m.group(1)
            leading_space = m.group(2)

            if leading_line or self.r_multiline.search(item):
                item = self._RunBlockGamut(self._Outdent(item))
            else:
                item = self._DoLists(self._Outdent(item))
                if item[-1] == "\n": item = item[:-1] # chomp
                item = self._RunSpanGamut(item)
            return "<li>%s</li>\n" % item

        text = self.r_ProcessListItems.sub(handler, text)
        self.list_level -= 1
        return text
    
    r_DoCodeBlocks = re.compile(r"""
    (?:\n\n|\A)
    (                 # $1 = the code block
    (?:
    (?:[ ]{%d} | \t)  # Lines must start with a tab or equiv
    .*\n+
    )+
    )
    ((?=^[ ]{0,%d}\S)|\Z) # Lookahead for non-space/end of doc
    """ % (tabwidth, tabwidth), re.M | re.VERBOSE)
    def _DoCodeBlocks(self, text):
        def handler(m):
            codeblock = m.group(1)
            codeblock = self._EncodeCode(self._Outdent(codeblock))
            codeblock = self._Detab(codeblock)
            codeblock = codeblock.lstrip("\n")
            codeblock = codeblock.rstrip()
            res = "\n\n<pre><code>%s\n</code></pre>\n\n" % codeblock
            return res

        text = self.r_DoCodeBlocks.sub(handler, text)
        return text
    r_DoCodeSpans = re.compile(r"""
    (`+)            # $1 = Opening run of `
    (.+?)           # $2 = The code block
    (?<!`)
    \1              # Matching closer
    (?!`)
    """, re.I|re.VERBOSE)
    def _DoCodeSpans(self, text):
        def handler(m):
            c = m.group(2)
            c = c.strip()
            c = self._EncodeCode(c)
            return "<code>%s</code>" % c

        text = self.r_DoCodeSpans.sub(handler, text)
        return text
    
    def _EncodeCode(self, text):
        text = text.replace("&","&amp;")
        text = text.replace("<","&lt;")
        text = text.replace(">","&gt;")
        for c in "*_{}[]\\":
            text = text.replace(c, self.escapetable[c])
        return text

    
    r_DoBold = re.compile(r"(\*\*|__) (?=\S) (.+?[*_]*) (?<=\S) \1", re.VERBOSE | re.S)
    r_DoItalics = re.compile(r"(\*|_) (?=\S) (.+?) (?<=\S) \1", re.VERBOSE | re.S)
    def _DoItalicsAndBold(self, text):
        text = self.r_DoBold.sub(r"<strong>\2</strong>", text)
        text = self.r_DoItalics.sub(r"<em>\2</em>", text)
        return text
    
    r_start = re.compile(r"^", re.M)
    ####r_DoBlockQuotes1 = re.compile(r"^[ \t]*>[ \t]?", re.M)
    r_DoBlockQuotes1 = re.compile(r"^[ \t]*&gt;[ \t]?", re.M)
    r_DoBlockQuotes2 = re.compile(r"^[ \t]+$", re.M)
    r_DoBlockQuotes3 = re.compile(r"""
    (                       # Wrap whole match in $1
     (
       ^[ \t]*&gt;[ \t]?       # '>' at the start of a line
       .+\n                 # rest of the first line
       (.+\n)*              # subsequent consecutive lines
       \n*                  # blanks
      )+
    )""", re.M | re.VERBOSE)
    r_protectpre = re.compile(r'(\s*<pre>.+?</pre>)', re.S)
    r_propre = re.compile(r'^  ', re.M)

    def _DoBlockQuotes(self, text):
        def prehandler(m):
            return self.r_propre.sub('', m.group(1))
                
        def handler(m):
            bq = m.group(1)
            bq = self.r_DoBlockQuotes1.sub("", bq)
            bq = self.r_DoBlockQuotes2.sub("", bq)
            bq = self._RunBlockGamut(bq)
            bq = self.r_start.sub("  ", bq)
            bq = self.r_protectpre.sub(prehandler, bq)
            return "<blockquote>\n%s\n</blockquote>\n\n" % bq
            
        text = self.r_DoBlockQuotes3.sub(handler, text)
        return text

    r_tabbed = re.compile(r"^([ \t]*)")
    def _FormParagraphs(self, text):
        text = text.strip("\n")
        grafs = self.r_multiline.split(text)

        for g in xrange(len(grafs)):
            t = grafs[g].strip() #@@?
            if not self.html_blocks.has_key(t):
                t = self._RunSpanGamut(t)
                t = self.r_tabbed.sub(r"<p>", t)
                t += "</p>"
                grafs[g] = t

        for g in xrange(len(grafs)):
            t = grafs[g].strip()
            if self.html_blocks.has_key(t):
                grafs[g] = self.html_blocks[t]
        
        return "\n\n".join(grafs)

    r_EncodeAmps = re.compile(r"&(?!#?[xX]?(?:[0-9a-fA-F]+|\w+);)")
    r_EncodeAngles = re.compile(r"<(?![a-z/?\$!])")
    def _EncodeAmpsAndAngles(self, text):
        text = self.r_EncodeAmps.sub("&amp;", text)
        text = self.r_EncodeAngles.sub("&lt;", text)
        return text

    def _EncodeBackslashEscapes(self, text):
        for char in self.escapechars:
            text = text.replace("\\" + char, self.escapetable[char])
        return text
    
    r_link = re.compile(r"<((https?|ftp):[^\'\">\s]+)>", re.I)
    r_email = re.compile(r"""
      <
      (?:mailto:)?
      (
         [-.\w]+
         \@
         [-a-z0-9]+(\.[-a-z0-9]+)*\.[a-z]+
      )
      >""", re.VERBOSE|re.I)
    def _DoAutoLinks(self, text):
        text = self.r_link.sub(r'<a href="\1" rel="nofollow">\1</a>', text)

        def handler(m):
            l = m.group(1)
            return self._EncodeEmailAddress(self._UnescapeSpecialChars(l))
    
        text = self.r_email.sub(handler, text)
        return text
    
    r_EncodeEmailAddress = re.compile(r">.+?:")
    def _EncodeEmailAddress(self, text):
        encode = [
            lambda x: "&#%s;" % ord(x),
            lambda x: "&#x%X;" % ord(x),
            lambda x: x
        ]

        text = "mailto:" + text
        addr = ""
        for c in text:
            if c == ':': addr += c; continue
            
            r = semirandom(addr)
            if r < 0.45:
                addr += encode[1](c)
            elif r > 0.9 and c != '@':
                addr += encode[2](c)
            else:
                addr += encode[0](c)

        text = '<a href="%s">%s</a>' % (addr, addr)
        text = self.r_EncodeEmailAddress.sub('>', text)
        return text

    def _UnescapeSpecialChars(self, text):
        for key in self.escapetable.keys():
            text = text.replace(self.escapetable[key], key)
        return text
    
    tokenize_depth = 6
    tokenize_nested_tags = '|'.join([r'(?:<[a-z/!$](?:[^<>]'] * tokenize_depth) + (')*>)' * tokenize_depth)
    r_TokenizeHTML = re.compile(
      r"""(?: <! ( -- .*? -- \s* )+ > ) |  # comment
          (?: <\? .*? \?> ) |              # processing instruction
          %s                               # nested tags
    """ % tokenize_nested_tags, re.I|re.VERBOSE)
    def _TokenizeHTML(self, text):
        pos = 0
        tokens = []
        matchobj = self.r_TokenizeHTML.search(text, pos)
        while matchobj:
            whole_tag = matchobj.string[matchobj.start():matchobj.end()]
            sec_start = matchobj.end()
            tag_start = sec_start - len(whole_tag)
            if pos < tag_start:
                tokens.append(["text", matchobj.string[pos:tag_start]])

            tokens.append(["tag", whole_tag])
            pos = sec_start
            matchobj = self.r_TokenizeHTML.search(text, pos)

        if pos < len(text):
            tokens.append(["text", text[pos:]])
        return tokens

    r_Outdent = re.compile(r"""^(\t|[ ]{1,%d})""" % tabwidth, re.M)
    def _Outdent(self, text):
        text = self.r_Outdent.sub("", text)
        return text    

    def _Detab(self, text): return text.expandtabs(self.tabwidth)

def Markdown(*args, **kw): return _Markdown().parse(*args, **kw)
markdown = Markdown

if __name__ == '__main__':
    if len(sys.argv) > 1:
        print Markdown(open(sys.argv[1]).read())
    else:
        print Markdown(sys.stdin.read())

########NEW FILE########
__FILENAME__ = memcache
#!/usr/bin/env python

"""
client module for memcached (memory cache daemon)

Overview
========

See U{the MemCached homepage<http://www.danga.com/memcached>} for more about memcached.

Usage summary
=============

This should give you a feel for how this module operates::

    import memcache
    mc = memcache.Client(['127.0.0.1:11211'], debug=0)

    mc.set("some_key", "Some value")
    value = mc.get("some_key")

    mc.set("another_key", 3)
    mc.delete("another_key")

    mc.set("key", "1")   # note that the key used for incr/decr must be a string.
    mc.incr("key")
    mc.decr("key")

The standard way to use memcache with a database is like this::

    key = derive_key(obj)
    obj = mc.get(key)
    if not obj:
        obj = backend_api.get(...)
        mc.set(key, obj)

    # we now have obj, and future passes through this code
    # will use the object from the cache.

Detailed Documentation
======================

More detailed documentation is available in the L{Client} class.
"""

import sys
import socket
import time
import os
import re
from hashlib import md5
try:
    import cPickle as pickle
except ImportError:
    import pickle

from binascii import crc32   # zlib version is not cross-platform
def cmemcache_hash(key):
    return((((crc32(key) & 0xffffffff) >> 16) & 0x7fff) or 1)
serverHashFunction = cmemcache_hash

def useOldServerHashFunction():
    """Use the old python-memcache server hash function."""
    global serverHashFunction
    serverHashFunction = crc32

try:
    from zlib import compress, decompress
    _supports_compress = True
except ImportError:
    _supports_compress = False
    # quickly define a decompress just in case we recv compressed data.
    def decompress(val):
        raise _Error("received compressed data but I don't support compression (import error)")

try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO


#  Original author: Evan Martin of Danga Interactive
__author__    = "Sean Reifschneider <jafo-memcached@tummy.com>"
__version__ = "1.48"
__copyright__ = "Copyright (C) 2003 Danga Interactive"
#  http://en.wikipedia.org/wiki/Python_Software_Foundation_License
__license__   = "Python Software Foundation License"

SERVER_MAX_KEY_LENGTH = 250
#  Storing values larger than 1MB requires recompiling memcached.  If you do,
#  this value can be changed by doing "memcache.SERVER_MAX_VALUE_LENGTH = N"
#  after importing this module.
SERVER_MAX_VALUE_LENGTH = 1024*1024


class _Error(Exception):
    pass


class _ConnectionDeadError(Exception):
    pass


try:
    # Only exists in Python 2.4+
    from threading import local
except ImportError:
    # TODO:  add the pure-python local implementation
    class local(object):
        pass


_DEAD_RETRY = 1  # number of seconds before retrying a dead server.
_SOCKET_TIMEOUT = 3  #  number of seconds before sockets timeout.


class Client(local):
    """
    Object representing a pool of memcache servers.

    See L{memcache} for an overview.

    In all cases where a key is used, the key can be either:
        1. A simple hashable type (string, integer, etc.).
        2. A tuple of C{(hashvalue, key)}.  This is useful if you want to avoid
        making this module calculate a hash value.  You may prefer, for
        example, to keep all of a given user's objects on the same memcache
        server, so you could use the user's unique id as the hash value.

    @group Setup: __init__, set_servers, forget_dead_hosts, disconnect_all, debuglog
    @group Insertion: set, add, replace, set_multi
    @group Retrieval: get, get_multi
    @group Integers: incr, decr
    @group Removal: delete, delete_multi
    @sort: __init__, set_servers, forget_dead_hosts, disconnect_all, debuglog,\
           set, set_multi, add, replace, get, get_multi, incr, decr, delete, delete_multi
    """
    _FLAG_PICKLE  = 1<<0
    _FLAG_INTEGER = 1<<1
    _FLAG_LONG    = 1<<2
    _FLAG_COMPRESSED = 1<<3

    _SERVER_RETRIES = 10  # how many times to try finding a free server.

    # exceptions for Client
    class MemcachedKeyError(Exception):
        pass
    class MemcachedKeyLengthError(MemcachedKeyError):
        pass
    class MemcachedKeyCharacterError(MemcachedKeyError):
        pass
    class MemcachedKeyNoneError(MemcachedKeyError):
        pass
    class MemcachedKeyTypeError(MemcachedKeyError):
        pass
    class MemcachedStringEncodingError(Exception):
        pass

    def __init__(self, servers, debug=0, pickleProtocol=0,
                 pickler=pickle.Pickler, unpickler=pickle.Unpickler,
                 pload=None, pid=None,
                 server_max_key_length=SERVER_MAX_KEY_LENGTH,
                 server_max_value_length=SERVER_MAX_VALUE_LENGTH,
                 dead_retry=_DEAD_RETRY, socket_timeout=_SOCKET_TIMEOUT,
                 cache_cas = False):
        """
        Create a new Client object with the given list of servers.

        @param servers: C{servers} is passed to L{set_servers}.
        @param debug: whether to display error messages when a server can't be
        contacted.
        @param pickleProtocol: number to mandate protocol used by (c)Pickle.
        @param pickler: optional override of default Pickler to allow subclassing.
        @param unpickler: optional override of default Unpickler to allow subclassing.
        @param pload: optional persistent_load function to call on pickle loading.
        Useful for cPickle since subclassing isn't allowed.
        @param pid: optional persistent_id function to call on pickle storing.
        Useful for cPickle since subclassing isn't allowed.
        @param dead_retry: number of seconds before retrying a blacklisted
        server. Default to 30 s.
        @param socket_timeout: timeout in seconds for all calls to a server. Defaults
        to 3 seconds.
        @param cache_cas: (default False) If true, cas operations will be
        cached.  WARNING: This cache is not expired internally, if you have
        a long-running process you will need to expire it manually via
        "client.reset_cas(), or the cache can grow unlimited.
        @param server_max_key_length: (default SERVER_MAX_KEY_LENGTH)
        Data that is larger than this will not be sent to the server.
        @param server_max_value_length: (default SERVER_MAX_VALUE_LENGTH)
        Data that is larger than this will not be sent to the server.
        """
        local.__init__(self)
        self.debug = debug
        self.dead_retry = dead_retry
        self.socket_timeout = socket_timeout
        self.set_servers(servers)
        self.stats = {}
        self.cache_cas = cache_cas
        self.reset_cas()

        # Allow users to modify pickling/unpickling behavior
        self.pickleProtocol = pickleProtocol
        self.pickler = pickler
        self.unpickler = unpickler
        self.persistent_load = pload
        self.persistent_id = pid
        self.server_max_key_length = server_max_key_length
        self.server_max_value_length = server_max_value_length

        #  figure out the pickler style
        file = StringIO()
        try:
            pickler = self.pickler(file, protocol = self.pickleProtocol)
            self.picklerIsKeyword = True
        except TypeError:
            self.picklerIsKeyword = False

    def reset_cas(self):
        """
        Reset the cas cache.  This is only used if the Client() object
        was created with "cache_cas=True".  If used, this cache does not
        expire internally, so it can grow unbounded if you do not clear it
        yourself.
        """
        self.cas_ids = {}


    def set_servers(self, servers):
        """
        Set the pool of servers used by this client.

        @param servers: an array of servers.
        Servers can be passed in two forms:
            1. Strings of the form C{"host:port"}, which implies a default weight of 1.
            2. Tuples of the form C{("host:port", weight)}, where C{weight} is
            an integer weight value.
        """
        self.servers = [_Host(s, self.debug, dead_retry=self.dead_retry,
                              socket_timeout=self.socket_timeout)
                        for s in servers]
        self._init_buckets()

    def get_stats(self, stat_args = None):
        '''Get statistics from each of the servers.

        @param stat_args: Additional arguments to pass to the memcache
            "stats" command.

        @return: A list of tuples ( server_identifier, stats_dictionary ).
            The dictionary contains a number of name/value pairs specifying
            the name of the status field and the string value associated with
            it.  The values are not converted from strings.
        '''
        data = []
        for s in self.servers:
            if not s.connect(): continue
            if s.family == socket.AF_INET:
                name = '%s:%s (%s)' % ( s.ip, s.port, s.weight )
            else:
                name = 'unix:%s (%s)' % ( s.address, s.weight )
            if not stat_args:
                s.send_cmd('stats')
            else:
                s.send_cmd('stats ' + stat_args)
            serverData = {}
            data.append(( name, serverData ))
            readline = s.readline
            while 1:
                line = readline()
                if not line or line.strip() == 'END': break
                stats = line.split(' ', 2)
                serverData[stats[1]] = stats[2]

        return(data)

    def get_slabs(self):
        data = []
        for s in self.servers:
            if not s.connect(): continue
            if s.family == socket.AF_INET:
                name = '%s:%s (%s)' % ( s.ip, s.port, s.weight )
            else:
                name = 'unix:%s (%s)' % ( s.address, s.weight )
            serverData = {}
            data.append(( name, serverData ))
            s.send_cmd('stats items')
            readline = s.readline
            while 1:
                line = readline()
                if not line or line.strip() == 'END': break
                item = line.split(' ', 2)
                #0 = STAT, 1 = ITEM, 2 = Value
                slab = item[1].split(':', 2)
                #0 = items, 1 = Slab #, 2 = Name
                if slab[1] not in serverData:
                    serverData[slab[1]] = {}
                serverData[slab[1]][slab[2]] = item[2]
        return data

    def flush_all(self):
        'Expire all data currently in the memcache servers.'
        for s in self.servers:
            if not s.connect(): continue
            s.send_cmd('flush_all')
            s.expect("OK")

    def debuglog(self, str):
        if self.debug:
            sys.stderr.write("MemCached: %s\n" % str)

    def _statlog(self, func):
        if func not in self.stats:
            self.stats[func] = 1
        else:
            self.stats[func] += 1

    def forget_dead_hosts(self):
        """
        Reset every host in the pool to an "alive" state.
        """
        for s in self.servers:
            s.deaduntil = 0

    def _init_buckets(self):
        self.buckets = []
        for server in self.servers:
            for i in range(server.weight):
                self.buckets.append(server)

    def _get_server(self, key):
        if isinstance(key, tuple):
            serverhash, key = key
        else:
            serverhash = serverHashFunction(key)

        # the original version suffered from a failure rate of
        # 1 in n^_SERVER_RETRIES, where n = number of bukets
        # if one server is down.  This is particularly bad for
        # n = 2, and the hashing is poor enough to guarantee
        # that making _SERVER_RETRIES larger doesn't help.

        #make a copy
        good_servers = list(self.buckets)
        while good_servers:
            server = good_servers.pop(serverhash % len(good_servers))
            if server.connect():
                return server, key

        # removed by chris
        #for i in range(Client._SERVER_RETRIES):
        #    server = self.buckets[serverhash % len(self.buckets)]
        #    if server.connect():
        #        #print "(using server %s)" % server,
        #        return server, key
        #    serverhash = serverHashFunction(str(serverhash) + str(i))

        return None, key

    def disconnect_all(self):
        for s in self.servers:
            s.close_socket()

    def delete_multi(self, keys, time=0, key_prefix=''):
        '''
        Delete multiple keys in the memcache doing just one query.

        >>> notset_keys = mc.set_multi({'key1' : 'val1', 'key2' : 'val2'})
        >>> mc.get_multi(['key1', 'key2']) == {'key1' : 'val1', 'key2' : 'val2'}
        1
        >>> mc.delete_multi(['key1', 'key2'])
        1
        >>> mc.get_multi(['key1', 'key2']) == {}
        1


        This method is recommended over iterated regular L{delete}s as it reduces total latency, since
        your app doesn't have to wait for each round-trip of L{delete} before sending
        the next one.

        @param keys: An iterable of keys to clear
        @param time: number of seconds any subsequent set / update commands should fail. Defaults to 0 for no delay.
        @param key_prefix:  Optional string to prepend to each key when sending to memcache.
            See docs for L{get_multi} and L{set_multi}.

        @return: 1 if no failure in communication with any memcacheds.
        @rtype: int

        '''

        self._statlog('delete_multi')

        server_keys, prefixed_to_orig_key = self._map_and_prefix_keys(keys, key_prefix)

        # send out all requests on each server before reading anything
        dead_servers = []

        rc = 1
        for server in server_keys.iterkeys():
            bigcmd = []
            write = bigcmd.append
            if time != None:
                 for key in server_keys[server]: # These are mangled keys
                     write("delete %s %d\r\n" % (key, time))
            else:
                for key in server_keys[server]: # These are mangled keys
                  write("delete %s\r\n" % key)
            try:
                server.send_cmds(''.join(bigcmd))
            except socket.error, msg:
                rc = 0
                if isinstance(msg, tuple): msg = msg[1]
                server.mark_dead(msg)
                dead_servers.append(server)

        # if any servers died on the way, don't expect them to respond.
        for server in dead_servers:
            del server_keys[server]

        for server, keys in server_keys.iteritems():
            try:
                for key in keys:
                    server.expect("DELETED")
            except socket.error, msg:
                if isinstance(msg, tuple): msg = msg[1]
                server.mark_dead(msg)
                rc = 0
        return rc

    def delete(self, key, time=0):
        '''Deletes a key from the memcache.

        @return: Nonzero on success.
        @param time: number of seconds any subsequent set / update commands
        should fail. Defaults to None for no delay.
        @rtype: int
        '''
        server, key = self._get_server(key)
        key = self.check_key(key)
        if not server:
            return 0
        self._statlog('delete')
        if time != None and time != 0:
            cmd = "delete %s %d" % (key, time)
        else:
            cmd = "delete %s" % key

        try:
            server.send_cmd(cmd)
            line = server.readline()
            if line and line.strip() in ['DELETED', 'NOT_FOUND']: return 1
            self.debuglog('Delete expected DELETED or NOT_FOUND, got: %s'
                    % repr(line))
        except socket.error, msg:
            if isinstance(msg, tuple): msg = msg[1]
            server.mark_dead(msg)
        return 0

    def incr(self, key, delta=1):
        """
        Sends a command to the server to atomically increment the value
        for C{key} by C{delta}, or by 1 if C{delta} is unspecified.
        Returns None if C{key} doesn't exist on server, otherwise it
        returns the new value after incrementing.

        Note that the value for C{key} must already exist in the memcache,
        and it must be the string representation of an integer.

        >>> mc.set("counter", "20")  # returns 1, indicating success
        1
        >>> mc.incr("counter")
        21
        >>> mc.incr("counter")
        22

        Overflow on server is not checked.  Be aware of values approaching
        2**32.  See L{decr}.

        @param delta: Integer amount to increment by (should be zero or greater).
        @return: New value after incrementing.
        @rtype: int
        """
        if delta < 0:
            return self._incrdecr("decr", key, -delta)
        else:
            return self._incrdecr("incr", key, delta)

    def decr(self, key, delta=1):
        """
        Like L{incr}, but decrements.  Unlike L{incr}, underflow is checked and
        new values are capped at 0.  If server value is 1, a decrement of 2
        returns 0, not -1.

        @param delta: Integer amount to decrement by (should be zero or greater).
        @return: New value after decrementing.
        @rtype: int
        """
        if delta < 0:
            return self._incrdecr("incr", key, -delta)
        else:
            return self._incrdecr("decr", key, delta)

    def _incrdecr(self, cmd, key, delta):
        server, key = self._get_server(key)
        key = self.check_key(key)
        if not server:
            return 0
        self._statlog(cmd)
        cmd = "%s %s %d" % (cmd, key, delta)
        try:
            server.send_cmd(cmd)
            line = server.readline()
            if line == None or line.strip() =='NOT_FOUND': return None
            return int(line)
        except socket.error, msg:
            if isinstance(msg, tuple): msg = msg[1]
            server.mark_dead(msg)
            return None

    def add(self, key, val, time = 0, min_compress_len = 0):
        '''
        Add new key with value.

        Like L{set}, but only stores in memcache if the key doesn't already exist.

        @return: Nonzero on success.
        @rtype: int
        '''
        return self._set("add", key, val, time, min_compress_len)

    def append(self, key, val, time=0, min_compress_len=0):
        '''Append the value to the end of the existing key's value.

        Only stores in memcache if key already exists.
        Also see L{prepend}.

        @return: Nonzero on success.
        @rtype: int
        '''
        return self._set("append", key, val, time, min_compress_len)

    def prepend(self, key, val, time=0, min_compress_len=0):
        '''Prepend the value to the beginning of the existing key's value.

        Only stores in memcache if key already exists.
        Also see L{append}.

        @return: Nonzero on success.
        @rtype: int
        '''
        return self._set("prepend", key, val, time, min_compress_len)

    def replace(self, key, val, time=0, min_compress_len=0):
        '''Replace existing key with value.

        Like L{set}, but only stores in memcache if the key already exists.
        The opposite of L{add}.

        @return: Nonzero on success.
        @rtype: int
        '''
        return self._set("replace", key, val, time, min_compress_len)

    def set(self, key, val, time=0, min_compress_len=0):
        '''Unconditionally sets a key to a given value in the memcache.

        The C{key} can optionally be an tuple, with the first element
        being the server hash value and the second being the key.
        If you want to avoid making this module calculate a hash value.
        You may prefer, for example, to keep all of a given user's objects
        on the same memcache server, so you could use the user's unique
        id as the hash value.

        @return: Nonzero on success.
        @rtype: int
        @param time: Tells memcached the time which this value should expire, either
        as a delta number of seconds, or an absolute unix time-since-the-epoch
        value. See the memcached protocol docs section "Storage Commands"
        for more info on <exptime>. We default to 0 == cache forever.
        @param min_compress_len: The threshold length to kick in auto-compression
        of the value using the zlib.compress() routine. If the value being cached is
        a string, then the length of the string is measured, else if the value is an
        object, then the length of the pickle result is measured. If the resulting
        attempt at compression yeilds a larger string than the input, then it is
        discarded. For backwards compatability, this parameter defaults to 0,
        indicating don't ever try to compress.
        '''
        return self._set("set", key, val, time, min_compress_len)


    def cas(self, key, val, time=0, min_compress_len=0):
        '''Sets a key to a given value in the memcache if it hasn't been
        altered since last fetched. (See L{gets}).

        The C{key} can optionally be an tuple, with the first element
        being the server hash value and the second being the key.
        If you want to avoid making this module calculate a hash value.
        You may prefer, for example, to keep all of a given user's objects
        on the same memcache server, so you could use the user's unique
        id as the hash value.

        @return: Nonzero on success.
        @rtype: int
        @param time: Tells memcached the time which this value should expire,
        either as a delta number of seconds, or an absolute unix
        time-since-the-epoch value. See the memcached protocol docs section
        "Storage Commands" for more info on <exptime>. We default to
        0 == cache forever.
        @param min_compress_len: The threshold length to kick in
        auto-compression of the value using the zlib.compress() routine. If
        the value being cached is a string, then the length of the string is
        measured, else if the value is an object, then the length of the
        pickle result is measured. If the resulting attempt at compression
        yeilds a larger string than the input, then it is discarded. For
        backwards compatability, this parameter defaults to 0, indicating
        don't ever try to compress.
        '''
        return self._set("cas", key, val, time, min_compress_len)


    def _map_and_prefix_keys(self, key_iterable, key_prefix):
        """Compute the mapping of server (_Host instance) -> list of keys to stuff onto that server, as well as the mapping of
        prefixed key -> original key.


        """
        # Check it just once ...
        key_extra_len=len(key_prefix)
        #changed by steve
        #if key_prefix:
            #self.check_key(key_prefix)

        # server (_Host) -> list of unprefixed server keys in mapping
        server_keys = {}

        prefixed_to_orig_key = {}
        # build up a list for each server of all the keys we want.
        for orig_key in key_iterable:
            if isinstance(orig_key, tuple):
                # Tuple of hashvalue, key ala _get_server(). Caller is essentially telling us what server to stuff this on.
                # Ensure call to _get_server gets a Tuple as well.
                str_orig_key = str(orig_key[1])
                server, key = self._get_server((orig_key[0], key_prefix + str_orig_key)) # Gotta pre-mangle key before hashing to a server. Returns the mangled key.
            else:
                str_orig_key = str(orig_key) # set_multi supports int / long keys.
                server, key = self._get_server(key_prefix + str_orig_key)

            # Now check to make sure key length is proper ...
            #changed by steve
            #self.check_key(str_orig_key, key_extra_len=key_extra_len)
            key = self.check_key(key_prefix + str_orig_key)

            if not server:
                continue

            if server not in server_keys:
                server_keys[server] = []
            server_keys[server].append(key)
            prefixed_to_orig_key[key] = orig_key

        return (server_keys, prefixed_to_orig_key)

    def set_multi(self, mapping, time=0, key_prefix='', min_compress_len=0):
        '''
        Sets multiple keys in the memcache doing just one query.

        >>> notset_keys = mc.set_multi({'key1' : 'val1', 'key2' : 'val2'})
        >>> mc.get_multi(['key1', 'key2']) == {'key1' : 'val1', 'key2' : 'val2'}
        1


        This method is recommended over regular L{set} as it lowers the number of
        total packets flying around your network, reducing total latency, since
        your app doesn't have to wait for each round-trip of L{set} before sending
        the next one.

        @param mapping: A dict of key/value pairs to set.
        @param time: Tells memcached the time which this value should expire, either
        as a delta number of seconds, or an absolute unix time-since-the-epoch
        value. See the memcached protocol docs section "Storage Commands"
        for more info on <exptime>. We default to 0 == cache forever.
        @param key_prefix:  Optional string to prepend to each key when sending to memcache. Allows you to efficiently stuff these keys into a pseudo-namespace in memcache:
            >>> notset_keys = mc.set_multi({'key1' : 'val1', 'key2' : 'val2'}, key_prefix='subspace_')
            >>> len(notset_keys) == 0
            True
            >>> mc.get_multi(['subspace_key1', 'subspace_key2']) == {'subspace_key1' : 'val1', 'subspace_key2' : 'val2'}
            True

            Causes key 'subspace_key1' and 'subspace_key2' to be set. Useful in conjunction with a higher-level layer which applies namespaces to data in memcache.
            In this case, the return result would be the list of notset original keys, prefix not applied.

        @param min_compress_len: The threshold length to kick in auto-compression
        of the value using the zlib.compress() routine. If the value being cached is
        a string, then the length of the string is measured, else if the value is an
        object, then the length of the pickle result is measured. If the resulting
        attempt at compression yeilds a larger string than the input, then it is
        discarded. For backwards compatability, this parameter defaults to 0,
        indicating don't ever try to compress.
        @return: List of keys which failed to be stored [ memcache out of memory, etc. ].
        @rtype: list

        '''

        self._statlog('set_multi')

        server_keys, prefixed_to_orig_key = self._map_and_prefix_keys(mapping.iterkeys(), key_prefix)

        # send out all requests on each server before reading anything
        dead_servers = []
        notstored = [] # original keys.

        for server in server_keys.iterkeys():
            bigcmd = []
            write = bigcmd.append
            try:
                for key in server_keys[server]: # These are mangled keys
                    store_info = self._val_to_store_info(
                            mapping[prefixed_to_orig_key[key]],
                            min_compress_len)
                    if store_info:
                        write("set %s %d %d %d\r\n%s\r\n" % (key, store_info[0],
                                time, store_info[1], store_info[2]))
                    else:
                        notstored.append(prefixed_to_orig_key[key])
                server.send_cmds(''.join(bigcmd))
            except socket.error, msg:
                if isinstance(msg, tuple): msg = msg[1]
                server.mark_dead(msg)
                dead_servers.append(server)

        # if any servers died on the way, don't expect them to respond.
        for server in dead_servers:
            del server_keys[server]

        #  short-circuit if there are no servers, just return all keys
        if not server_keys: return(mapping.keys())

        for server, keys in server_keys.iteritems():
            try:
                for key in keys:
                    line = server.readline()
                    if line == 'STORED':
                        continue
                    else:
                        notstored.append(prefixed_to_orig_key[key]) #un-mangle.
            except (_Error, socket.error), msg:
                if isinstance(msg, tuple): msg = msg[1]
                server.mark_dead(msg)
        return notstored

    def _val_to_store_info(self, val, min_compress_len):
        """
           Transform val to a storable representation, returning a tuple of the flags, the length of the new value, and the new value itself.
        """
        flags = 0
        if isinstance(val, str):
            pass
        elif isinstance(val, int):
            flags |= Client._FLAG_INTEGER
            val = "%d" % val
            # force no attempt to compress this silly string.
            min_compress_len = 0
        elif isinstance(val, long):
            flags |= Client._FLAG_LONG
            val = "%d" % val
            # force no attempt to compress this silly string.
            min_compress_len = 0
        else:
            flags |= Client._FLAG_PICKLE
            file = StringIO()
            if self.picklerIsKeyword:
                pickler = self.pickler(file, protocol = self.pickleProtocol)
            else:
                pickler = self.pickler(file, self.pickleProtocol)
            if self.persistent_id:
                pickler.persistent_id = self.persistent_id
            pickler.dump(val)
            val = file.getvalue()

        lv = len(val)
        # We should try to compress if min_compress_len > 0 and we could
        # import zlib and this string is longer than our min threshold.
        if min_compress_len and _supports_compress and lv > min_compress_len:
            comp_val = compress(val)
            # Only retain the result if the compression result is smaller
            # than the original.
            if len(comp_val) < lv:
                flags |= Client._FLAG_COMPRESSED
                val = comp_val

        #  silently do not store if value length exceeds maximum
        if self.server_max_value_length != 0 and \
           len(val) > self.server_max_value_length: return(0)

        return (flags, len(val), val)

    def _set(self, cmd, key, val, time, min_compress_len=0):
        server, key = self._get_server(key)
        key = self.check_key(key)
        if not server:
            return 0

        def _unsafe_set():
            self._statlog(cmd)

            store_info = self._val_to_store_info(val, min_compress_len)
            if not store_info: return(0)

            if cmd == 'cas':
                if key not in self.cas_ids:
                    return self._set('set', key, val, time, min_compress_len)
                fullcmd = "%s %s %d %d %d %d\r\n%s" % (
                        cmd, key, store_info[0], time, store_info[1],
                        self.cas_ids[key], store_info[2])
            else:
                fullcmd = "%s %s %d %d %d\r\n%s" % (
                        cmd, key, store_info[0], time, store_info[1], store_info[2])

            try:
                server.send_cmd(fullcmd)
                return(server.expect("STORED") == "STORED")
            except socket.error, msg:
                if isinstance(msg, tuple): msg = msg[1]
                server.mark_dead(msg)
            return 0

        try:
            return _unsafe_set()
        except _ConnectionDeadError:
            # retry once
            try:
                server._get_socket()
                return _unsafe_set()
            except (_ConnectionDeadError, socket.error), msg:
                server.mark_dead(msg)
            return 0

    def _get(self, cmd, key):
        server, key = self._get_server(key)
        key = self.check_key(key)
        if not server:
            return None

        def _unsafe_get():
            self._statlog(cmd)

            try:
                server.send_cmd("%s %s" % (cmd, key))
                rkey = flags = rlen = cas_id = None

                if cmd == 'gets':
                    rkey, flags, rlen, cas_id, = self._expect_cas_value(server)
                    if rkey and self.cache_cas:
                        self.cas_ids[rkey] = cas_id
                else:
                    rkey, flags, rlen, = self._expectvalue(server)

                if not rkey:
                    return None
                try:
                    value = self._recv_value(server, flags, rlen)
                finally:
                    server.expect("END")
            except (_Error, socket.error), msg:
                if isinstance(msg, tuple): msg = msg[1]
                server.mark_dead(msg)
                return None

            return value

        try:
            return _unsafe_get()
        except _ConnectionDeadError:
            # retry once
            try:
                if server.connect():
                    return _unsafe_get()
                return None
            except (_ConnectionDeadError, socket.error), msg:
                server.mark_dead(msg)
            return None

    def get(self, key):
        '''Retrieves a key from the memcache.

        @return: The value or None.
        '''
        return self._get('get', key)

    def gets(self, key):
        '''Retrieves a key from the memcache. Used in conjunction with 'cas'.

        @return: The value or None.
        '''
        return self._get('gets', key)

    def get_multi(self, keys, key_prefix=''):
        '''
        Retrieves multiple keys from the memcache doing just one query.

        >>> success = mc.set("foo", "bar")
        >>> success = mc.set("baz", 42)
        >>> mc.get_multi(["foo", "baz", "foobar"]) == {"foo": "bar", "baz": 42}
        1
        >>> mc.set_multi({'k1' : 1, 'k2' : 2}, key_prefix='pfx_') == []
        1

        This looks up keys 'pfx_k1', 'pfx_k2', ... . Returned dict will just have unprefixed keys 'k1', 'k2'.
        >>> mc.get_multi(['k1', 'k2', 'nonexist'], key_prefix='pfx_') == {'k1' : 1, 'k2' : 2}
        1

        get_mult [ and L{set_multi} ] can take str()-ables like ints / longs as keys too. Such as your db pri key fields.
        They're rotored through str() before being passed off to memcache, with or without the use of a key_prefix.
        In this mode, the key_prefix could be a table name, and the key itself a db primary key number.

        >>> mc.set_multi({42: 'douglass adams', 46 : 'and 2 just ahead of me'}, key_prefix='numkeys_') == []
        1
        >>> mc.get_multi([46, 42], key_prefix='numkeys_') == {42: 'douglass adams', 46 : 'and 2 just ahead of me'}
        1

        This method is recommended over regular L{get} as it lowers the number of
        total packets flying around your network, reducing total latency, since
        your app doesn't have to wait for each round-trip of L{get} before sending
        the next one.

        See also L{set_multi}.

        @param keys: An array of keys.
        @param key_prefix: A string to prefix each key when we communicate with memcache.
            Facilitates pseudo-namespaces within memcache. Returned dictionary keys will not have this prefix.
        @return:  A dictionary of key/value pairs that were available. If key_prefix was provided, the keys in the retured dictionary will not have it present.

        '''

        self._statlog('get_multi')

        server_keys, prefixed_to_orig_key = self._map_and_prefix_keys(keys, key_prefix)

        # send out all requests on each server before reading anything
        dead_servers = []
        for server in server_keys.iterkeys():
            try:
                server.send_cmd("get %s" % " ".join(server_keys[server]))
            except socket.error, msg:
                if isinstance(msg, tuple): msg = msg[1]
                server.mark_dead(msg)
                dead_servers.append(server)

        # if any servers died on the way, don't expect them to respond.
        for server in dead_servers:
            del server_keys[server]

        retvals = {}
        for server in server_keys.iterkeys():
            try:
                line = server.readline()
                while line and line != 'END':
                    rkey, flags, rlen = self._expectvalue(server, line)
                    #  Bo Yang reports that this can sometimes be None
                    if rkey is not None:
                        val = self._recv_value(server, flags, rlen)
                        retvals[prefixed_to_orig_key[rkey]] = val   # un-prefix returned key.
                    line = server.readline()
            except (_Error, socket.error), msg:
                if isinstance(msg, tuple): msg = msg[1]
                server.mark_dead(msg)
        return retvals

    def _expect_cas_value(self, server, line=None):
        if not line:
            line = server.readline()

        if line and line[:5] == 'VALUE':
            resp, rkey, flags, len, cas_id = line.split()
            return (rkey, int(flags), int(len), int(cas_id))
        else:
            return (None, None, None, None)

    def _expectvalue(self, server, line=None):
        if not line:
            line = server.readline()

        if line and line[:5] == 'VALUE':
            resp, rkey, flags, len = line.split()
            flags = int(flags)
            rlen = int(len)
            return (rkey, flags, rlen)
        else:
            return (None, None, None)

    def _recv_value(self, server, flags, rlen):
        rlen += 2 # include \r\n
        buf = server.recv(rlen)
        if len(buf) != rlen:
            raise _Error("received %d bytes when expecting %d"
                    % (len(buf), rlen))

        if len(buf) == rlen:
            buf = buf[:-2]  # strip \r\n

        if flags & Client._FLAG_COMPRESSED:
            buf = decompress(buf)

        if  flags == 0 or flags == Client._FLAG_COMPRESSED:
            # Either a bare string or a compressed string now decompressed...
            val = buf
        elif flags & Client._FLAG_INTEGER:
            val = int(buf)
        elif flags & Client._FLAG_LONG:
            val = long(buf)
        elif flags & Client._FLAG_PICKLE:
            try:
                file = StringIO(buf)
                unpickler = self.unpickler(file)
                if self.persistent_load:
                    unpickler.persistent_load = self.persistent_load
                val = unpickler.load()
            except Exception, e:
                self.debuglog('Pickle error: %s\n' % e)
                return None
        else:
            self.debuglog("unknown flags on get: %x\n" % flags)

        return val

    def check_key(self, key, key_extra_len=0):
        """Checks sanity of key.  Fails if:
            Key length is > SERVER_MAX_KEY_LENGTH (Raises MemcachedKeyLength).
            Contains control characters  (Raises MemcachedKeyCharacterError).
            Is not a string (Raises MemcachedStringEncodingError)
            Is an unicode string (Raises MemcachedStringEncodingError)
            Is not a string (Raises MemcachedKeyError)
            Is None (Raises MemcachedKeyError)
        """
        if isinstance(key, tuple): key = key[1]
        if not key:
            raise Client.MemcachedKeyNoneError("Key is None")
        if isinstance(key, unicode):
            raise Client.MemcachedStringEncodingError(
                    "Keys must be str()'s, not unicode.  Convert your unicode "
                    "strings using mystring.encode(charset)!")
        if not isinstance(key, str):
            raise Client.MemcachedKeyTypeError("Key must be str()'s")

        #if isinstance(key, basestring):
        #    if self.server_max_key_length != 0 and \
        #        len(key) + key_extra_len > self.server_max_key_length:
        #        raise Client.MemcachedKeyLengthError("Key length is > %s"
        #                 % self.server_max_key_length)
        #    for char in key:
        #        if ord(char) < 33 or ord(char) == 127:
        #            raise Client.MemcachedKeyCharacterError(
        #                    "Control characters not allowed")

        return md5(key).hexdigest()


class _Host(object):

    def __init__(self, host, debug=0, dead_retry=_DEAD_RETRY,
                 socket_timeout=_SOCKET_TIMEOUT):
        self.dead_retry = dead_retry
        self.socket_timeout = socket_timeout
        self.debug = debug
        if isinstance(host, tuple):
            host, self.weight = host
        else:
            self.weight = 1

        #  parse the connection string
        m = re.match(r'^(?P<proto>unix):(?P<path>.*)$', host)
        if not m:
            m = re.match(r'^(?P<proto>inet):'
                    r'(?P<host>[^:]+)(:(?P<port>[0-9]+))?$', host)
        if not m: m = re.match(r'^(?P<host>[^:]+)(:(?P<port>[0-9]+))?$', host)
        if not m:
            raise ValueError('Unable to parse connection string: "%s"' % host)

        hostData = m.groupdict()
        if hostData.get('proto') == 'unix':
            self.family = socket.AF_UNIX
            self.address = hostData['path']
        else:
            self.family = socket.AF_INET
            self.ip = hostData['host']
            self.port = int(hostData.get('port', 11211))
            self.address = ( self.ip, self.port )

        self.deaduntil = 0
        self.socket = None

        self.buffer = ''

    def debuglog(self, str):
        if self.debug:
            sys.stderr.write("MemCached: %s\n" % str)

    def _check_dead(self):
        if self.deaduntil and self.deaduntil > time.time():
            return 1
        self.deaduntil = 0
        return 0

    def connect(self):
        if self._get_socket():
            return 1
        return 0

    def mark_dead(self, reason):
        self.debuglog("MemCache: %s: %s.  Marking dead." % (self, reason))
        self.deaduntil = time.time() + self.dead_retry
        self.close_socket()

    def _get_socket(self):
        if self._check_dead():
            return None
        if self.socket:
            return self.socket
        s = socket.socket(self.family, socket.SOCK_STREAM)
        if hasattr(s, 'settimeout'): s.settimeout(self.socket_timeout)
        try:
            s.connect(self.address)
        except socket.timeout, msg:
            self.mark_dead("connect: %s" % msg)
            return None
        except socket.error, msg:
            if isinstance(msg, tuple): msg = msg[1]
            self.mark_dead("connect: %s" % msg[1])
            return None
        self.socket = s
        self.buffer = ''
        return s

    def close_socket(self):
        if self.socket:
            self.socket.close()
            self.socket = None

    def send_cmd(self, cmd):
        self.socket.sendall(cmd + '\r\n')

    def send_cmds(self, cmds):
        """ cmds already has trailing \r\n's applied """
        self.socket.sendall(cmds)

    def readline(self):
        buf = self.buffer
        recv = self.socket.recv
        while True:
            index = buf.find('\r\n')
            if index >= 0:
                break
            data = recv(4096)
            if not data:
                # connection close, let's kill it and raise
                self.close_socket()
                raise _ConnectionDeadError()

            buf += data
        self.buffer = buf[index+2:]
        return buf[:index]

    def expect(self, text):
        line = self.readline()
        if line != text:
            self.debuglog("while expecting '%s', got unexpected response '%s'"
                    % (text, line))
        return line

    def recv(self, rlen):
        self_socket_recv = self.socket.recv
        buf = self.buffer
        while len(buf) < rlen:
            foo = self_socket_recv(max(rlen - len(buf), 4096))
            buf += foo
            if not foo:
                raise _Error( 'Read %d bytes, expecting %d, '
                        'read returned 0 length bytes' % ( len(buf), rlen ))
        self.buffer = buf[rlen:]
        return buf[:rlen]

    def __str__(self):
        d = ''
        if self.deaduntil:
            d = " (dead until %d)" % self.deaduntil

        if self.family == socket.AF_INET:
            return "inet:%s:%d%s" % (self.address[0], self.address[1], d)
        else:
            return "unix:%s%s" % (self.address, d)


def _doctest():
    import doctest, memcache
    servers = ["127.0.0.1:11211"]
    mc = Client(servers, debug=1)
    globs = {"mc": mc}
    return doctest.testmod(memcache, globs=globs)

if __name__ == "__main__":
    failures = 0
    print "Testing docstrings..."
    _doctest()
    print "Running tests:"
    print
    serverList = [["127.0.0.1:11211"]]
    if '--do-unix' in sys.argv:
        serverList.append([os.path.join(os.getcwd(), 'memcached.socket')])

    for servers in serverList:
        mc = Client(servers, debug=1)

        def to_s(val):
            if not isinstance(val, basestring):
                return "%s (%s)" % (val, type(val))
            return "%s" % val
        def test_setget(key, val):
            global failures
            print "Testing set/get {'%s': %s} ..." % (to_s(key), to_s(val)),
            mc.set(key, val)
            newval = mc.get(key)
            if newval == val:
                print "OK"
                return 1
            else:
                print "FAIL"; failures = failures + 1
                return 0


        class FooStruct(object):
            def __init__(self):
                self.bar = "baz"
            def __str__(self):
                return "A FooStruct"
            def __eq__(self, other):
                if isinstance(other, FooStruct):
                    return self.bar == other.bar
                return 0

        test_setget("a_string", "some random string")
        test_setget("an_integer", 42)
        if test_setget("long", long(1<<30)):
            print "Testing delete ...",
            if mc.delete("long"):
                print "OK"
            else:
                print "FAIL"; failures = failures + 1
            print "Checking results of delete ..."
            if mc.get("long") == None:
                print "OK"
            else:
                print "FAIL"; failures = failures + 1
        print "Testing get_multi ...",
        print mc.get_multi(["a_string", "an_integer"])

        #  removed from the protocol
        #if test_setget("timed_delete", 'foo'):
        #    print "Testing timed delete ...",
        #    if mc.delete("timed_delete", 1):
        #        print "OK"
        #    else:
        #        print "FAIL"; failures = failures + 1
        #    print "Checking results of timed delete ..."
        #    if mc.get("timed_delete") == None:
        #        print "OK"
        #    else:
        #        print "FAIL"; failures = failures + 1

        print "Testing get(unknown value) ...",
        print to_s(mc.get("unknown_value"))

        f = FooStruct()
        test_setget("foostruct", f)

        print "Testing incr ...",
        x = mc.incr("an_integer", 1)
        if x == 43:
            print "OK"
        else:
            print "FAIL"; failures = failures + 1

        print "Testing decr ...",
        x = mc.decr("an_integer", 1)
        if x == 42:
            print "OK"
        else:
            print "FAIL"; failures = failures + 1
        sys.stdout.flush()

        # sanity tests
        print "Testing sending spaces...",
        sys.stdout.flush()
        try:
            x = mc.set("this has spaces", 1)
        except Client.MemcachedKeyCharacterError, msg:
            print "OK"
        else:
            print "FAIL"; failures = failures + 1

        print "Testing sending control characters...",
        try:
            x = mc.set("this\x10has\x11control characters\x02", 1)
        except Client.MemcachedKeyCharacterError, msg:
            print "OK"
        else:
            print "FAIL"; failures = failures + 1

        print "Testing using insanely long key...",
        try:
            x = mc.set('a'*SERVER_MAX_KEY_LENGTH, 1)
        except Client.MemcachedKeyLengthError, msg:
            print "FAIL"; failures = failures + 1
        else:
            print "OK"
        try:
            x = mc.set('a'*SERVER_MAX_KEY_LENGTH + 'a', 1)
        except Client.MemcachedKeyLengthError, msg:
            print "OK"
        else:
            print "FAIL"; failures = failures + 1

        print "Testing sending a unicode-string key...",
        try:
            x = mc.set(u'keyhere', 1)
        except Client.MemcachedStringEncodingError, msg:
            print "OK",
        else:
            print "FAIL",; failures = failures + 1
        try:
            x = mc.set((u'a'*SERVER_MAX_KEY_LENGTH).encode('utf-8'), 1)
        except:
            print "FAIL",; failures = failures + 1
        else:
            print "OK",
        import pickle
        s = pickle.loads('V\\u4f1a\np0\n.')
        try:
            x = mc.set((s*SERVER_MAX_KEY_LENGTH).encode('utf-8'), 1)
        except Client.MemcachedKeyLengthError:
            print "OK"
        else:
            print "FAIL"; failures = failures + 1

        print "Testing using a value larger than the memcached value limit...",
        x = mc.set('keyhere', 'a'*SERVER_MAX_VALUE_LENGTH)
        if mc.get('keyhere') == None:
            print "OK",
        else:
            print "FAIL",; failures = failures + 1
        x = mc.set('keyhere', 'a'*SERVER_MAX_VALUE_LENGTH + 'aaa')
        if mc.get('keyhere') == None:
            print "OK"
        else:
            print "FAIL"; failures = failures + 1

        print "Testing set_multi() with no memcacheds running",
        mc.disconnect_all()
        errors = mc.set_multi({'keyhere' : 'a', 'keythere' : 'b'})
        if errors != []:
            print "FAIL"; failures = failures + 1
        else:
            print "OK"

        print "Testing delete_multi() with no memcacheds running",
        mc.disconnect_all()
        ret = mc.delete_multi({'keyhere' : 'a', 'keythere' : 'b'})
        if ret != 1:
            print "FAIL"; failures = failures + 1
        else:
            print "OK"

    if failures > 0:
        print '*** THERE WERE FAILED TESTS'
        sys.exit(1)
    sys.exit(0)


# vim: ts=4 sw=4 et :

########NEW FILE########
__FILENAME__ = pysolr
# -*- coding: utf-8 -*-
"""
All we need to create a Solr connection is a url.

>>> conn = Solr('http://127.0.0.1:8983/solr/')

First, completely clear the index.

>>> conn.delete(q='*:*')

For now, we can only index python dictionaries. Each key in the dictionary
will correspond to a field in Solr.

>>> docs = [
...     {'id': 'testdoc.1', 'order_i': 1, 'name': 'document 1', 'text': u'Paul Verlaine'},
...     {'id': 'testdoc.2', 'order_i': 2, 'name': 'document 2', 'text': u'Владимир Маякoвский'},
...     {'id': 'testdoc.3', 'order_i': 3, 'name': 'document 3', 'text': u'test'},
...     {'id': 'testdoc.4', 'order_i': 4, 'name': 'document 4', 'text': u'test'}
... ]


We can add documents to the index by passing a list of docs to the connection's
add method.

>>> conn.add(docs)

>>> results = conn.search('Verlaine')
>>> len(results)
1

>>> results = conn.search(u'Владимир')
>>> len(results)
1


Simple tests for searching. We can optionally sort the results using Solr's
sort syntax, that is, the field name and either asc or desc.

>>> results = conn.search('test', sort='order_i asc')
>>> for result in results:
...     print result['name']
document 3
document 4

>>> results = conn.search('test', sort='order_i desc')
>>> for result in results:
...     print result['name']
document 4
document 3


To update documents, we just use the add method.

>>> docs = [
...     {'id': 'testdoc.4', 'order_i': 4, 'name': 'document 4', 'text': u'blah'}
... ]
>>> conn.add(docs)

>>> len(conn.search('blah'))
1
>>> len(conn.search('test'))
1


We can delete documents from the index by id, or by supplying a query.

>>> conn.delete(id='testdoc.1')
>>> conn.delete(q='name:"document 2"')

>>> results = conn.search('Verlaine')
>>> len(results)
0


Docs can also have multiple values for any particular key. This lets us use
Solr's multiValue fields.

>>> docs = [
...     {'id': 'testdoc.5', 'cat': ['poetry', 'science'], 'name': 'document 5', 'text': u''},
...     {'id': 'testdoc.6', 'cat': ['science-fiction',], 'name': 'document 6', 'text': u''},
... ]

>>> conn.add(docs)
>>> results = conn.search('cat:"poetry"')
>>> for result in results:
...     print result['name']
document 5

>>> results = conn.search('cat:"science-fiction"')
>>> for result in results:
...     print result['name']
document 6

>>> results = conn.search('cat:"science"')
>>> for result in results:
...     print result['name']
document 5

NOTE: PySolr is an open-source Python module
<http://code.google.com/p/pysolr/> that falls under the New BSD
Licence <http://www.opensource.org/licenses/bsd-license.php>, NOT the
licence covering the rest of Reddit. Reddit's modifications to this
module also fall under the New BSD Licence. The New BSD Licence
requires that re-distributions of the source, modified or not, display
the original copyright notice, but PySolr does not, as of import-time,
display a copyright notice or licence, except on its Google Code
information page. Therefore for licencing information, I point you to
PySolr's Google Code information page, URL above.

"""

# TODO: unicode support is pretty sloppy. define it better.

from httplib import HTTPConnection
from urllib import urlencode
from urlparse import urlsplit
from datetime import datetime, date
from time import strptime, strftime
from r2.lib.utils import unicode_safe
try:
    # for python 2.5
    from xml.etree import ElementTree
    from xml.parsers.expat import ExpatError
except ImportError:
    from elementtree import ElementTree,ExpatError

__all__ = ['Solr']

class SolrError(Exception):
    pass

class Results(object):
    def __init__(self, docs, hits):
        self.docs = docs
        self.hits = hits

    def __len__(self):
        return len(self.docs)

    def __iter__(self):
        return iter(self.docs)

    def __getitem__(self,x):
        return self.docs[x]

class Solr(object):
    def __init__(self, url):
        self.url = url
        scheme, netloc, path, query, fragment = urlsplit(url)
        netloc = netloc.split(':')
        self.host = netloc[0]
        if len(netloc) == 1:
            self.host, self.port = netloc[0], None
        else:
            self.host, self.port = netloc
        self.path = path.rstrip('/')

    def _select(self, params):
        # encode the query as utf-8 so urlencode can handle it
        params['q'] = unicode_safe(params['q'])
        path = '%s/select/?%s' % (self.path, urlencode(params))
        conn = HTTPConnection(self.host, self.port)
        conn.request('GET', path)
        return conn.getresponse()

    def _update(self, message):
        """
        Posts the given xml message to http://<host>:<port>/solr/update and
        returns the result.
        """
        path = '%s/update/' % self.path
        conn = HTTPConnection(self.host, self.port)
        conn.request('POST', path, message, {'Content-type': 'text/xml'})
        return conn.getresponse()

    def _extract_error(self, response):
        """
        Extract the actual error message from a solr response. Unfortunately,
        this means scraping the html.
        """
        try:
            et = ElementTree.parse(response)
            error = et.findtext('body/pre')
            return error
        except ExpatError,e:
            return "%s: %s (%d/%s)" % (e,response.read(),response.status,response.reason)

    # Converters #############################################################

    @staticmethod
    def _from_python(value):
        """
        Converts python values to a form suitable for insertion into the xml
        we send to solr.
        """
        if isinstance(value, datetime):
            value = value.strftime('%Y-%m-%dT%H:%M:%S.000Z')
        elif isinstance(value, date):
            value = value.strftime('%Y-%m-%dT00:00:00.000Z')
        elif isinstance(value, bool):
            if value:
                value = 'true'
            else:
                value = 'false'
        else:
            value = unicode_safe(value)
        return value

    def bool_to_python(self, value):
        """
        Convert a 'bool' field from solr's xml format to python and return it.
        """
        if value == 'true':
            return True
        elif value == 'false':
            return False

    def str_to_python(self, value):
        """
        Convert an 'str' field from solr's xml format to python and return it.
        """
        return unicode_safe(value)

    def int_to_python(self, value):
        """
        Convert an 'int' field from solr's xml format to python and return it.
        """
        return int(value)

    def date_to_python(self, value):
        """
        Convert a 'date' field from solr's xml format to python and return it.
        """
        # this throws away fractions of a second
        return datetime(*strptime(value[:-5], "%Y-%m-%dT%H:%M:%S")[0:6])

    # API Methods ############################################################

    def search(self, q, sort=None, start=0, rows=20, other_params = {}):
        """Performs a search and returns the results."""
        params = {'q': q, 'start': start, 'rows': rows}

        for x,y in other_params.iteritems():
            params[x] = y
        if sort:
            params['sort'] = sort

        response = self._select(params)
        if response.status != 200:
            raise SolrError(self._extract_error(response))

        # TODO: make result retrieval lazy and allow custom result objects
        # also, this has become rather ugly and definitely needs some cleanup.
        et = ElementTree.parse(response)
        result = et.find('result')
        hits = int(result.get('numFound'))
        docs = result.findall('doc')
        results = []
        for doc in docs:
            result = {}
            for element in doc.getchildren():
                if element.tag == 'arr':
                    result_val = []
                    for array_element in element.getchildren():
                        converter_name = '%s_to_python' % array_element.tag
                        converter = getattr(self, converter_name)
                        result_val.append(converter(array_element.text))
                else:
                    converter_name = '%s_to_python' % element.tag
                    converter = getattr(self, converter_name)
                    result_val = converter(element.text)
                result[element.get('name')] = result_val
            results.append(result)
        return Results(results, hits)

    def add(self, docs, commit=False):
        """Adds or updates documents. For now, docs is a list of dictionaies
        where each key is the field name and each value is the value to index.
        """
        message = ElementTree.Element('add')
        for doc in docs:
            message.append(doc_to_elemtree(doc))
        m = ElementTree.tostring(message)
        response = self._update(m)
        if response.status != 200:
            raise SolrError(self._extract_error(response))
        # TODO: Supposedly, we can put a <commit /> element in the same post body
        # as the add element. That isn't working for some reason, and it would save us
        # an extra trip to the server. This works for now.
        if commit:
            self.commit()

    def delete(self, id=None, q=None, commit=False):
        """Deletes documents."""
        if id is None and q is None:
            raise ValueError('You must specify "id" or "q".')
        elif id is not None and q is not None:
            raise ValueError('You many only specify "id" OR "q", not both.')
        elif id is not None:
            m = '<delete><id>%s</id></delete>' % id
        elif q is not None:
            m = '<delete><query>%s</query></delete>' % q
        response = self._update(m)
        if response.status != 200:
            raise SolrError(self._extract_error(response))
        # TODO: Supposedly, we can put a <commit /> element in the same post body
        # as the delete element. That isn't working for some reason, and it would save us
        # an extra trip to the server. This works for now.
        if commit:
            self.commit()

    def commit(self):
        response = self._update('<commit />')
        if response.status != 200:
            raise SolrError(self._extract_error(response))

    def optimize(self):
        response = self._update('<optimize />')
        if response.status != 200:
            raise SolrError(self._extract_error(response))

solr_magic_fields = ('boost',)
def doc_to_elemtree(doc):
    d = ElementTree.Element('doc')
    for key, value in doc.iteritems():
        
        if key in solr_magic_fields:
            # handle special fields that are attributes, not fields
            d.set(key,Solr._from_python(value))
        elif (not isinstance(value,str)) and hasattr(value, '__iter__'):
            # handle lists, tuples, and other iterabes
            for v in value:
                f = ElementTree.Element('field', name=key)
                f.text = Solr._from_python(v)
                d.append(f)
                # handle strings and unicode
        else:
            f = ElementTree.Element('field', name=key)
            f.text = Solr._from_python(value)
            d.append(f)

    return d


if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = count
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################

from r2.models import Link, Subreddit
from r2.lib import utils
from pylons import g

count_period = g.rising_period

#stubs

def incr_counts(wrapped):
    pass

def get_link_counts(period = count_period):
    links = Link._query(Link.c._date >= utils.timeago(period),
                        limit=50, data = True)
    return dict((l._fullname, (0, l.sr_id)) for l in links)

def get_sr_counts(period = count_period):
    srs = Subreddit._query()
    return dict((l._fullname, (0, l.sr_id)) for l in links)

def clear_sr_counts(names):
    pass

try:
    from r2admin.lib.count import *
except ImportError:
    pass

########NEW FILE########
__FILENAME__ = cssfilter
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from __future__ import with_statement

from r2.models import *
from r2.lib.utils import sanitize_url, domain, randstr
from r2.lib.strings import string_dict

from pylons import g, c
from pylons.i18n import _

import re

import cssutils
from cssutils import CSSParser
from cssutils.css import CSSStyleRule
from cssutils.css import CSSValue, CSSValueList
from cssutils.css import CSSPrimitiveValue
from cssutils.css import cssproperties
from xml.dom import DOMException

msgs = string_dict['css_validator_messages']

custom_macros = {
    'num': r'[-]?\d+|[-]?\d*\.\d+',
    'percentage': r'{num}%',
    'length': r'0|{num}(em|ex|px|in|cm|mm|pt|pc)',
    'int': r'[-]?\d+',
    'w': r'\s*',
    
    # From: http://www.w3.org/TR/2008/WD-css3-color-20080721/#svg-color
    'x11color': r'aliceblue|antiquewhite|aqua|aquamarine|azure|beige|bisque|black|blanchedalmond|blue|blueviolet|brown|burlywood|cadetblue|chartreuse|chocolate|coral|cornflowerblue|cornsilk|crimson|cyan|darkblue|darkcyan|darkgoldenrod|darkgray|darkgreen|darkgrey|darkkhaki|darkmagenta|darkolivegreen|darkorange|darkorchid|darkred|darksalmon|darkseagreen|darkslateblue|darkslategray|darkslategrey|darkturquoise|darkviolet|deeppink|deepskyblue|dimgray|dimgrey|dodgerblue|firebrick|floralwhite|forestgreen|fuchsia|gainsboro|ghostwhite|gold|goldenrod|gray|green|greenyellow|grey|honeydew|hotpink|indianred|indigo|ivory|khaki|lavender|lavenderblush|lawngreen|lemonchiffon|lightblue|lightcoral|lightcyan|lightgoldenrodyellow|lightgray|lightgreen|lightgrey|lightpink|lightsalmon|lightseagreen|lightskyblue|lightslategray|lightslategrey|lightsteelblue|lightyellow|lime|limegreen|linen|magenta|maroon|mediumaquamarine|mediumblue|mediumorchid|mediumpurple|mediumseagreen|mediumslateblue|mediumspringgreen|mediumturquoise|mediumvioletred|midnightblue|mintcream|mistyrose|moccasin|navajowhite|navy|oldlace|olive|olivedrab|orange|orangered|orchid|palegoldenrod|palegreen|paleturquoise|palevioletred|papayawhip|peachpuff|peru|pink|plum|powderblue|purple|red|rosybrown|royalblue|saddlebrown|salmon|sandybrown|seagreen|seashell|sienna|silver|skyblue|slateblue|slategray|slategrey|snow|springgreen|steelblue|tan|teal|thistle|tomato|turquoise|violet|wheat|white|whitesmoke|yellow|yellowgreen',
    'csscolor': r'(maroon|red|orange|yellow|olive|purple|fuchsia|white|lime|green|navy|blue|aqua|teal|black|silver|gray|ActiveBorder|ActiveCaption|AppWorkspace|Background|ButtonFace|ButtonHighlight|ButtonShadow|ButtonText|CaptionText|GrayText|Highlight|HighlightText|InactiveBorder|InactiveCaption|InactiveCaptionText|InfoBackground|InfoText|Menu|MenuText|Scrollbar|ThreeDDarkShadow|ThreeDFace|ThreeDHighlight|ThreeDLightShadow|ThreeDShadow|Window|WindowFrame|WindowText)|#[0-9a-f]{3}|#[0-9a-f]{6}|rgb\({w}{int}{w},{w}{int}{w},{w}{int}{w}\)|rgb\({w}{num}%{w},{w}{num}%{w},{w}{num}%{w}\)',
    'color': '{x11color}|{csscolor}',
    
    'single-text-shadow': r'({color}\s+)?{length}\s+{length}(\s+{length})?|{length}\s+{length}(\s+{length})?(\s+{color})?',

    'box-shadow-pos': r'{length}\s+{length}(\s+{length})?',
}

custom_values = {
    '_height': r'{length}|{percentage}|auto|inherit',
    '_width': r'{length}|{percentage}|auto|inherit',
    '_overflow': r'visible|hidden|scroll|auto|inherit',
    'color': r'{color}',
    'background-color': r'{color}',
    'border-color': r'{color}',
    'background-position': r'(({percentage}|{length}){0,3})?\s*(top|center|left)?\s*(left|center|right)?',
    'opacity': r'{num}',
    'filter': r'alpha\(opacity={num}\)',
}

nonstandard_values = {
    # http://www.w3.org/TR/css3-background/#border-top-right-radius
    '-moz-border-radius': r'(({length}|{percentage}){w}){1,2}',
    '-moz-border-radius-topleft': r'(({length}|{percentage}){w}){1,2}',
    '-moz-border-radius-topright': r'(({length}|{percentage}){w}){1,2}',
    '-moz-border-radius-bottomleft': r'(({length}|{percentage}){w}){1,2}',
    '-moz-border-radius-bottomright': r'(({length}|{percentage}){w}){1,2}',
    '-webkit-border-radius': r'(({length}|{percentage}){w}){1,2}',
    '-webkit-border-top-left-radius': r'(({length}|{percentage}){w}){1,2}',
    '-webkit-border-top-right-radius': r'(({length}|{percentage}){w}){1,2}',
    '-webkit-border-bottom-left-radius': r'(({length}|{percentage}){w}){1,2}',
    '-webkit-border-bottom-right-radius': r'(({length}|{percentage}){w}){1,2}',
    
    # http://www.w3.org/TR/css3-text/#text-shadow
    'text-shadow': r'none|({single-text-shadow}{w},{w})*{single-text-shadow}',
    
    # http://www.w3.org/TR/css3-background/#the-box-shadow
    # (This description doesn't support multiple shadows)
    'box-shadow': 'none|(?:({box-shadow-pos}\s+)?{color}|({color}\s+?){box-shadow-pos})',
}
custom_values.update(nonstandard_values);

def _expand_macros(tokdict,macrodict):
    """ Expand macros in token dictionary """
    def macro_value(m):
        return '(?:%s)' % macrodict[m.groupdict()['macro']]
    for key, value in tokdict.items():
        while re.search(r'{[a-z][a-z0-9-]*}', value):
            value = re.sub(r'{(?P<macro>[a-z][a-z0-9-]*)}',
                           macro_value, value)
        tokdict[key] = value
    return tokdict
def _compile_regexes(tokdict):
    """ Compile all regular expressions into callable objects """
    for key, value in tokdict.items():
        tokdict[key] = re.compile('^(?:%s)$' % value, re.I).match
    return tokdict
_compile_regexes(_expand_macros(custom_values,custom_macros))

class ValidationReport(object):
    def __init__(self, original_text=''):
        self.errors        = []
        self.original_text = original_text.split('\n') if original_text else ''

    def __str__(self):
        "only for debugging"
        return "Report:\n" + '\n'.join(['\t' + str(x) for x in self.errors])

    def append(self,error):
        if hasattr(error,'line'):
            error.offending_line = self.original_text[error.line-1]
        self.errors.append(error)

class ValidationError(Exception):
    def __init__(self, message, obj = None, line = None):
        self.message  = message
        if obj is not None:
            self.obj  = obj
        # self.offending_line is the text of the actual line that
        #  caused the problem; it's set by the ValidationReport that
        #  owns this ValidationError

        if obj is not None and line is None and hasattr(self.obj,'_linetoken'):
            (_type1,_type2,self.line,_char) = obj._linetoken
        elif line is not None:
            self.line = line

    def __cmp__(self, other):
        if hasattr(self,'line') and not hasattr(other,'line'):
            return -1
        elif hasattr(other,'line') and not hasattr(self,'line'):
            return 1
        else:
            return cmp(self.line,other.line)


    def __str__(self):
        "only for debugging"
        line = (("(%d)" % self.line)
                if hasattr(self,'line') else '')
        obj = str(self.obj) if hasattr(self,'obj') else ''
        return "ValidationError%s: %s (%s)" % (line, self.message, obj)

# local urls should be in the static directory
local_urls = re.compile(r'^/static/[a-z./-]+$')
# substitutable urls will be css-valid labels surrounded by "%%"
custom_img_urls = re.compile(r'%%([a-zA-Z0-9\-]+)%%')
def valid_url(prop,value,report):
    """
    checks url(...) arguments in CSS, ensuring that the contents are
    officially sanctioned.  Sanctioned urls include:
     * anything in /static/
     * image labels %%..%% for images uploaded on /about/stylesheet
     * urls with domains in g.allowed_css_linked_domains
    """
    url = value.getStringValue()
    # local urls are allowed
    if local_urls.match(url):
        pass
    # custom urls are allowed, but need to be transformed into a real path
    elif custom_img_urls.match(url):
        name = custom_img_urls.match(url).group(1)
        # the label -> image number lookup is stored on the subreddit
        if c.site.images.has_key(name):
            num = c.site.images[name]
            value._setCssText("url(http:/%s%s_%d.png?v=%s)"
                              % (g.s3_thumb_bucket, c.site._fullname, num,
                                 randstr(36)))
        else:
            # unknown image label -> error
            report.append(ValidationError(msgs['broken_url']
                                          % dict(brokenurl = value.cssText),
                                          value))
    # allowed domains are ok
    elif domain(url) in g.allowed_css_linked_domains:
        pass
    else:
        report.append(ValidationError(msgs['broken_url']
                                      % dict(brokenurl = value.cssText),
                                      value))
    #elif sanitize_url(url) != url:
    #    report.append(ValidationError(msgs['broken_url']
    #                                  % dict(brokenurl = value.cssText),
    #                                  value))


def valid_value(prop,value,report):
    if not (value.valid and value.wellformed):
        if (value.wellformed
            and prop.name in cssproperties.cssvalues
            and cssproperties.cssvalues[prop.name](prop.value)):
            # it's actually valid. cssutils bug.
            pass
        elif (not value.valid
              and value.wellformed
              and prop.name in custom_values
              and custom_values[prop.name](prop.value)):
            # we're allowing it via our own custom validator
            value.valid = True

            # see if this suddenly validates the entire property
            prop.valid = True
            prop.cssValue.valid = True
            if prop.cssValue.cssValueType == CSSValue.CSS_VALUE_LIST:
                for i in range(prop.cssValue.length):
                    if not prop.cssValue.item(i).valid:
                        prop.cssValue.valid = False
                        prop.valid = False
                        break
        elif not (prop.name in cssproperties.cssvalues or prop.name in custom_values):
            error = (msgs['invalid_property']
                     % dict(cssprop = prop.name))
            report.append(ValidationError(error,value))
        else:
            error = (msgs['invalid_val_for_prop']
                     % dict(cssvalue = value.cssText,
                            cssprop  = prop.name))
            report.append(ValidationError(error,value))

    if value.primitiveType == CSSPrimitiveValue.CSS_URI:
        valid_url(prop,value,report)

error_message_extract_re = re.compile('.*\\[([0-9]+):[0-9]*:.*\\]$')
only_whitespace          = re.compile('^\s*$')
def validate_css(string):
    p = CSSParser(raiseExceptions = True)

    if not string or only_whitespace.match(string):
        return ('',ValidationReport())

    report = ValidationReport(string)
    
    # avoid a very expensive parse
    max_size_kb = 100;
    if len(string) > max_size_kb * 1024:
        report.append(ValidationError((msgs['too_big']
                                       % dict (max_size = max_size_kb))))
        return (string, report)

    try:
        parsed = p.parseString(string)
    except DOMException,e:
        # yuck; xml.dom.DOMException can't give us line-information
        # directly, so we have to parse its error message string to
        # get it
        line = None
        line_match = error_message_extract_re.match(e.message)
        if line_match:
            line = line_match.group(1)
            if line:
                line = int(line)
        error_message=  (msgs['syntax_error']
                         % dict(syntaxerror = e.message))
        report.append(ValidationError(error_message,e,line))
        return (None,report)

    for rule in parsed.cssRules:
        if rule.type == CSSStyleRule.IMPORT_RULE:
            report.append(ValidationError(msgs['no_imports'],rule))
        elif rule.type == CSSStyleRule.COMMENT:
            pass
        elif rule.type == CSSStyleRule.STYLE_RULE:
            style = rule.style
            for prop in style.getProperties():

                if prop.cssValue.cssValueType == CSSValue.CSS_VALUE_LIST:
                    for i in range(prop.cssValue.length):
                        valid_value(prop,prop.cssValue.item(i),report)
                    if not (prop.cssValue.valid and prop.cssValue.wellformed):
                        report.append(ValidationError(msgs['invalid_property_list']
                                                      % dict(proplist = prop.cssText),
                                                      prop.cssValue))
                elif prop.cssValue.cssValueType == CSSValue.CSS_PRIMITIVE_VALUE:
                    valid_value(prop,prop.cssValue,report)

                # cssutils bug: because valid values might be marked
                # as invalid, we can't trust cssutils to properly
                # label valid properties, so we're going to rely on
                # the value validation (which will fail if the
                # property is invalid anyway). If this bug is fixed,
                # we should uncomment this 'if'

                # a property is not valid if any of its values are
                # invalid, or if it is itself invalid. To get the
                # best-quality error messages, we only report on
                # whether the property is valid after we've checked
                # the values
                #if not (prop.valid and prop.wellformed):
                #    report.append(ValidationError(Invalid property'),prop))
            
        else:
            report.append(ValidationError(msgs['unknown_rule_type']
                                          % dict(ruletype = rule.cssText),
                                          rule))

    return parsed,report

def find_preview_comments(sr):
    comments = Comment._query(Comment.c.sr_id == c.site._id,
                              limit=25, data=True)
    comments = list(comments)
    if not comments:
        comments = Comment._query(limit=25, data=True)
        comments = list(comments)

    return comments

def find_preview_links(sr):
    from r2.lib.normalized_hot import get_hot

    # try to find a link to use, otherwise give up and return
    links = get_hot(c.site)
    if not links:
        sr = Subreddit._by_name(g.default_sr)
        if sr:
            links = get_hot(sr)

    return links

def rendered_link(id, res, links, media, compress):
    from pylons.controllers.util import abort
    from r2.controllers import ListingController

    try:
        render_style    = c.render_style

        c.render_style = 'html'

        with c.user.safe_set_attr:
            c.user.pref_compress = compress
            c.user.pref_media    = media

            b = IDBuilder([l._fullname for l in links],
                          num = 1, wrap = ListingController.builder_wrapper)
            l = LinkListing(b, nextprev=False,
                            show_nums=True).listing().render(style='html')
            res._update(id, innerHTML=l)

    finally:
        c.render_style = render_style

def rendered_comment(id, res, comments):
    try:
        render_style    = c.render_style

        c.render_style = 'html'

        b = IDBuilder([x._fullname for x in comments],
                      num = 1)
        l = LinkListing(b, nextprev=False,
                        show_nums=False).listing().render(style='html')
        res._update('preview_comment', innerHTML=l)

    finally:
        c.render_style = render_style

class BadImage(Exception): pass

def clean_image(data,format):
    import Image
    from StringIO import StringIO

    try:
        in_file = StringIO(data)
        out_file = StringIO()

        im = Image.open(in_file)
        im = im.resize(im.size)

        im.save(out_file,format)
        ret = out_file.getvalue()
    except IOError,e:
        raise BadImage(e)
    finally:
        out_file.close()
        in_file.close()

    return ret
    
def save_sr_image(sr, data, num = None):
    """
    uploades image data to s3 as a PNG and returns its new url.  Urls
    will be of the form:
      http:/${g.s3_thumb_bucket}/${sr._fullname}[_${num}].png?v=${md5hash}
    [Note: g.s3_thumb_bucket begins with a "/" so the above url is valid.]
    """
    import tempfile
    from r2.lib import s3cp
    from md5 import md5

    hash = md5(data).hexdigest()

    try:
        f = tempfile.NamedTemporaryFile(suffix = '.png')
        f.write(data)
        f.flush()

        resource = g.s3_thumb_bucket + sr._fullname
        if num is not None:
            resource += '_' + str(num)
        resource += '.png'
        
        s3cp.send_file(f.name, resource, 'image/png', 'public-read', 
                       None, False)
    finally:
        f.close()

    return 'http:/%s%s?v=%s' % (g.s3_thumb_bucket, 
                                resource.split('/')[-1], hash)

 



########NEW FILE########
__FILENAME__ = alter_db
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
import tdb_sql
import sqlalchemy as sa

def thing_tables():
    for type in tdb_sql.types_id.values():
        yield type.thing_table

    for table in tdb_sql.extra_thing_tables.values():
        yield table

def rel_tables():
    for type in tdb_sql.rel_types_id.values():
        yield type.rel_table[0]

def dtables():
    for type in tdb_sql.types_id.values():
        yield type.data_table[0]

    for type in tdb_sql.rel_types_id.values():
        yield type.rel_table[3]

def exec_all(command, data=False, rel = False, print_only = False):
    if data:
        tables = dtables()
    elif rel:
        tables = rel_tables()
    else:
        tables = thing_tables()

    for tt in tables:
        #print tt
        engine = tt.engine
        if print_only:
            print command % dict(type=tt.name)
        else:
            try:
                engine.execute(command % dict(type=tt.name))
            except:
                print "FAILED!"

"alter table %(type)s add primary key (thing_id, key)"
"drop index idx_thing_id_%(type)s"

"create index concurrently idx_thing1_name_date_%(type)s on %(type)s (thing1_id, name, date);"

########NEW FILE########
__FILENAME__ = exporter
import os
import sys
from datetime import datetime

# XXX: We have to cap the cache size to stop the export from eating
# all available memory. We also have to adjust the cache before
# importing r2.lib.db.thing, which takes its own reference to whatever
# cache is currently set.
import pylons
from r2.lib.cache import Memcache, SelfEmptyingCache, CacheChain
pylons.g.cache = CacheChain((SelfEmptyingCache(max_size=1000), Memcache(pylons.g.memcaches)))

from r2.lib.db import tdb_sql as tdb
from r2.lib.db.thing import NotFound, Relation
from r2.models import Link, Comment, Account, Vote, Subreddit

from sqlalchemy import *

class Exporter:

    def __init__(self, output_db):
        """Initialise with path to output SQLite DB file"""
        # If the output file exists, delete it so that the db is
        # created from scratch
        if os.path.exists(output_db):
            os.unlink(output_db)
        self.db = create_engine("sqlite:///%s" % output_db)

        # Python's encoding handling is reallly annoying
        # http://stackoverflow.com/questions/3033741/sqlalchemy-automatically-converts-str-to-unicode-on-commit
        self.db.raw_connection().connection.text_factory = str
        self.init_db()
        self.started_at = datetime.now()

    def export_db(self):
        self.export_users()
        self.export_links()
        self.export_comments()
        self.export_votes()
        self.create_indexes()
        print >>sys.stderr, "Finished, total run time %d secs" % ((datetime.now() - self.started_at).seconds,)

    def export_thing(self, thing_class, table, row_extract):
        processed = 0
        max_id = self.max_thing_id(thing_class)
        print >>sys.stderr, "%d %s to process" % (max_id, table.name)
        for thing_id in xrange(max_id):
            try:
                thing = thing_class._byID(thing_id, data=True)
            except NotFound:
                continue

            try:
                row = row_extract(thing)
            except AttributeError:
                print >>sys.stderr, "  thing with id %d is broken, skipping" % thing_id
                continue

            table.insert(values=row).execute()
            processed += 1
            self.update_progress(processed)

    def user_row_extract(self, account):
        return (
            account._id,
            self.utf8(account.name),
            account.email if hasattr(account, 'email') else None,
            account.link_karma,
            account.comment_karma
        )

    def export_users(self):
        self.export_thing(Account, self.users, self.user_row_extract)

    def article_row_extract(self, link):
        sr = Subreddit._byID(link.sr_id, data=True)
        row = (
            link._id,
            self.utf8(link.title),
            self.utf8(link.article),
            link.author_id,
            link._date,
            sr.name
        )
        return row

    def export_links(self):
        self.export_thing(Link, self.articles, self.article_row_extract)

    def comment_row_extract(self, comment):
        return (
            comment._id,
            comment.author_id,
            comment.link_id,
            comment.body,
            comment._date
        )

    def export_comments(self):
        self.export_thing(Comment, self.comments, self.comment_row_extract)

    def export_votes(self):
        self.export_rel_votes(Link, self.article_votes)
        self.export_rel_votes(Comment, self.comment_votes)

    def export_rel_votes(self, votes_on_cls, table):
        # Vote.vote(c.user, link, action == 'like', request.ip)
        processed = 0
        rel = Vote.rel(Account, votes_on_cls)
        max_id = self.max_rel_type_id(rel)
        print >>sys.stderr, "%d %s to process" % (max_id, table.name)
        for vote_id in xrange(max_id):
            try:
                vote = rel._byID(vote_id, data=True)
            except NotFound:
                continue

            try:
                row = (
                    vote._id,
                    vote._thing1_id, # Account
                    vote._thing2_id, # Link/Comment (votes_on_cls)
                    vote._name, # Vote value
                    vote._date
                )
            except AttributeError:
                print >>sys.stderr, "  vote with id %d is broken, skipping" % vote_id
                continue

            table.insert(values=row).execute()
            processed += 1
            self.update_progress(processed)

    def max_rel_type_id(self, rel_thing):
        thing_type = tdb.rel_types_id[rel_thing._type_id]
        thing_tbl = thing_type.rel_table[0]
        rows = select([func.max(thing_tbl.c.rel_id)]).execute().fetchall()
        return rows[0][0]

    def max_thing_id(self, thing):
        thing_type = tdb.types_id[thing._type_id]
        thing_tbl = thing_type.thing_table
        rows = select([func.max(thing_tbl.c.thing_id)]).execute().fetchall()
        return rows[0][0]

    def utf8(self, text):
        if isinstance(text, unicode):
            try:
                text = text.encode('utf-8')
            except UnicodeEncodeError:
                print >>sys.stderr, "UnicodeEncodeError, using 'ignore' error mode" % link._id
                text = text.encode('utf-8', errors='ignore')
        elif isinstance(text, str):
            try:
                text = text.decode('utf-8').encode('utf-8')
            except UnicodeError:
                print >>sys.stderr, "UnicodeError, using 'ignore' error mode" % link._id
                text = text.decode('utf-8', errors='ignore').encode('utf-8', errors='ignore')

        return text

    def init_db(self):
        self.users = Table('users', self.db,
            Column('id', Integer, primary_key=True),
            Column('name', VARCHAR()),
            Column('email', VARCHAR()),
            Column('article_karma', Integer),
            Column('comment_karma', Integer),
        )
        self.users.create()

        self.articles = Table('articles', self.db,
            Column('id', Integer, primary_key=True),
            Column('title', VARCHAR()),
            Column('body', TEXT()),
            Column('author_id', Integer, ForeignKey('users.id')),
            Column('updated_at', DateTime()),
            Column('subreddit', VARCHAR()),
        )
        self.articles.create()

        self.comments = Table('comments', self.db,
            Column('id', Integer, primary_key=True),
            Column('author_id', Integer, ForeignKey('users.id')),
            Column('article_id', Integer, ForeignKey('articles.id')),
            Column('body', TEXT()),
            Column('updated_at', DateTime()),
        )
        self.comments.create()

        self.article_votes = Table('article_votes', self.db,
            Column('id', Integer, primary_key=True),
            Column('user_id', Integer, ForeignKey('users.id')),
            Column('article_id', Integer, ForeignKey('articles.id')),
            Column('vote', Integer()),
            Column('updated_at', DateTime()),
        )
        self.article_votes.create()

        self.comment_votes = Table('comment_votes', self.db,
            Column('id', Integer, primary_key=True),
            Column('user_id', Integer, ForeignKey('users.id')),
            Column('comment_id', Integer, ForeignKey('comments.id')),
            Column('vote', Integer()),
            Column('updated_at', DateTime()),
        )
        self.comment_votes.create()

    def create_indexes(self):
        #i = Index('someindex', sometable.c.col5)
        print >>sys.stderr, "Creating indexes on users table"
        Index('ix_users_id', self.users.c.id).create()
        Index('ix_users_name', self.users.c.name).create()
        Index('ix_users_email', self.users.c.email).create()
        print >>sys.stderr, "Creating indexes on articles table"
        Index('ix_articles_id', self.articles.c.id).create()
        Index('ix_articles_author_id', self.articles.c.author_id).create()
        Index('ix_articles_title', self.articles.c.title).create()
        print >>sys.stderr, "Creating indexes on comments table"
        Index('ix_comments_id', self.comments.c.id).create()
        Index('ix_comments_author_id', self.comments.c.author_id).create()
        Index('ix_comments_article_id', self.comments.c.article_id).create()
        print >>sys.stderr, "Creating indexes on article_votes table"
        Index('ix_article_votes_id', self.article_votes.c.id).create()
        Index('ix_article_votes_author_id', self.article_votes.c.user_id).create()
        Index('ix_article_votes_article_id', self.article_votes.c.article_id).create()
        Index('ix_article_votes_vote', self.article_votes.c.vote).create()
        print >>sys.stderr, "Creating indexes on comment_votes table"
        Index('ix_comment_votes_id', self.comment_votes.c.id).create()
        Index('ix_comment_votes_author_id', self.comment_votes.c.user_id).create()
        Index('ix_comment_votes_comment_id', self.comment_votes.c.comment_id).create()
        Index('ix_comment_votes_vote', self.comment_votes.c.vote).create()

    def update_progress(self, done):
        """print a progress message"""
        if done % 100 == 0:
            print >>sys.stderr, "  %d processed, run time %d secs" % (done, (datetime.now() - self.started_at).seconds)

########NEW FILE########
__FILENAME__ = operators
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
class BooleanOp(object):
    def __init__(self, *ops):
        self.ops = ops

    def __repr__(self):
        return '<%s_ %s>' % (self.__class__.__name__, str(self.ops))

class or_(BooleanOp): pass
class and_(BooleanOp): pass

class op(object):
    def __init__(self, lval, lval_name, rval):
        self.lval = lval
        self.rval = rval
        self.lval_name = lval_name

    def __repr__(self):
        return '<%s: %s, %s>' % (self.__class__.__name__, self.lval, self.rval)

    #sorts in a consistent order, required for Query._iden()
    def __cmp__(self, other):
        return cmp(repr(self), repr(other))

class eq(op): pass
class ne(op): pass
class lt(op): pass
class lte(op): pass
class gt(op): pass
class gte(op): pass

class Slot(object):
    def __init__(self, lval):
        if isinstance(lval, Slot):
            self.name = lval.name
            self.lval = lval
        else:
            self.name = lval

    def __repr__(self):
        return '<%s: %s>' % (self.__class__.__name__, self.name)

    def __eq__(self, other):
        return eq(self, self.name, other)

    def __ne__(self, other):
        return ne(self, self.name, other)

    def __lt__(self, other):
        return lt(self, self.name, other)

    def __le__(self, other):
        return lte(self, self.name, other)

    def __gt__(self, other):
        return gt(self, self.name, other)

    def __ge__(self, other):
        return gte(self, self.name, other)

class Slots(object):
    def __getattr__(self, attr):
        return Slot(attr)

    def __getitem__(self, attr):
        return Slot(attr)
        
def op_iter(ops):
    for o in ops:
        if isinstance(o, op):
            yield o
        elif isinstance(o, BooleanOp):
            for p in op_iter(o.ops):
                yield p

class query_func(Slot): pass
class lower(query_func): pass
class ip_network(query_func): pass
class base_url(query_func): pass

class timeago(object):
    def __init__(self, interval):
        self.interval = interval

    def __repr__(self):
        return '<interval: %s>' % self.interval

class sort(object):
    def __init__(self, col):
        self.col = col

    def __repr__(self):
        return '<sort:%s %s>' % (self.__class__.__name__, str(self.col))

    def __eq__(self, other):
        return self.col == other.col

class asc(sort): pass
class desc(sort):pass

########NEW FILE########
__FILENAME__ = queries
from r2.models import Account, Link, Comment, Vote, SaveHide
from r2.models import Message, Inbox, Subreddit
from r2.lib.db.thing import Thing, Merge
from r2.lib.db.operators import asc, desc, timeago
from r2.lib.db import query_queue
from r2.lib.db.sorts import epoch_seconds
from r2.lib.utils import fetch_things2, worker
from r2.lib.solrsearch import DomainSearchQuery

from datetime import datetime

from pylons import g
query_cache = g.permacache

precompute_limit = 1000

db_sorts = dict(hot = (desc, '_hot'),
                new = (desc, '_date'),
                top = (desc, '_score'),
                controversial = (desc, '_controversy'),
                old = (asc, '_date'),
                toplinks = (desc, '_hot'),
                blessed = (desc, '_date'))

def db_sort(sort):
    cls, col = db_sorts[sort]
    return cls(col)

search_sort = dict(hot = 'hot desc',
                   new = 'date desc',
                   top = 'points desc',
                   controversial = 'controversy desc',
                   old = 'date asc')

db_times = dict(all = None,
                hour = Thing.c._date >= timeago('1 hour'),
                day = Thing.c._date >= timeago('1 day'),
                week = Thing.c._date >= timeago('1 week'),
                month = Thing.c._date >= timeago('1 month'),
                quarter = Thing.c._date >= timeago('3 months'),
                year = Thing.c._date >= timeago('1 year'))

relation_db_times = dict(all = None,
                         hour = '1 hour',
                         day = '1 day',
                         week = '1 week',
                         month = '1 month',
                         quarter = '3 months',
                         year = '1 year')

#we need to define the filter functions here so cachedresults can be pickled
def filter_identity(x):
    return x

def filter_thing2(x):
    """A filter to apply to the results of a relationship query returns
    the object of the relationship."""
    return x._thing2

class CachedResults(object):
    """Given a query returns a list-like object that will lazily look up
    the query from the persistent cache. """
    def __init__(self, query, filter):
        self.query = query
        self.query._limit = precompute_limit
        self.filter = filter
        self.iden = self.query._iden()
        self.sort_cols = [s.col for s in self.query._sort]
        self.data = []
        self._fetched = False

    def fetch(self):
        """Loads the query from the cache."""
        if not self._fetched:
            self._fetched = True
            self.data = query_cache.get(self.iden) or []

    def make_item_tuple(self, item):
        """Given a single 'item' from the result of a query build the tuple
        that will be stored in the query cache. It is effectively the
        fullname of the item after passing through the filter plus the
        columns of the unfiltered item to sort by."""
        filtered_item = self.filter(item)
        lst = [filtered_item._fullname]
        for col in self.sort_cols:
            #take the property of the original 
            attr = getattr(item, col)
            #convert dates to epochs to take less space
            if isinstance(attr, datetime):
                attr = epoch_seconds(attr)
            lst.append(attr)
        return tuple(lst)

    def can_insert(self):
        """True if a new item can just be inserted, which is when the
        query is only sorted by date."""
        return self.query._sort == [desc('_date')]

    def can_delete(self):
        """True if a item can be removed from the listing, always true for now."""
        return True

    def insert(self, item):
        """Inserts the item at the front of the cached data. Assumes the query
        is sorted by date descending"""
        self.fetch()
        t = self.make_item_tuple(item)
        changed = False
        if t not in self.data:
            self.data.insert(0, t)
            changed = True

        if changed:
            query_cache.set(self.iden, self.data[:precompute_limit])

    def delete(self, item):
        """Deletes an item from the cached data."""
        self.fetch()
        t = self.make_item_tuple(item)
        changed = False
        while t in self.data:
            self.data.remove(t)
            changed = True
            
        if changed:
            query_cache.set(self.iden, self.data)
        
    def update(self):
        """Runs the query and stores the result in the cache. It also stores
        the columns relevant to the sort to make merging with other
        results faster."""
        self.data = [self.make_item_tuple(i) for i in self.query]
        self._fetched = True
        query_cache.set(self.iden, self.data)

    def __repr__(self):
        return '<CachedResults %s %s>' % (self.query._rules, self.query._sort)

    def __iter__(self):
        self.fetch()

        for x in self.data:
            yield x[0]

def merge_cached_results(*results):
    """Given two CachedResults, mergers their lists based on the sorts of
    their queries."""
    if len(results) == 1:
        return list(results[0])

    #make sure the sorts match
    sort = results[0].query._sort
    assert(all(r.query._sort == sort for r in results[1:]))

    def thing_cmp(t1, t2):
        for i, s in enumerate(sort):
            #t1 and t2 are tuples of (fullname, *sort_cols), so we can
            #get the value to compare right out of the tuple
            v1, v2 = t1[i + 1], t2[i + 1]
            if v1 != v2:
                return cmp(v1, v2) if isinstance(s, asc) else cmp(v2, v1)
        #they're equal
        return 0

    all_items = []
    for r in results:
        r.fetch()
        all_items.extend(r.data)

    #all_items = Thing._by_fullname(all_items, return_dict = False)
    return [i[0] for i in sorted(all_items, cmp = thing_cmp)]

def make_results(query, filter = filter_identity):
    if g.use_query_cache:
        return CachedResults(query, filter)
    else:
        query.prewrap_fn = filter
        return query

def merge_results(*results):
    if g.use_query_cache:
        return merge_cached_results(*results)
    else:
        m = Merge(results, sort = results[0]._sort)
        #assume the prewrap_fn's all match
        m.prewrap_fn = results[0].prewrap_fn
        return m

def get_links(sr, sort, time, link_cls = Link):
    """General link query for a subreddit."""
    q = link_cls._query(link_cls.c.sr_id == sr._id,
                    sort = db_sort(sort))

    if sort == 'toplinks':
        q._filter(link_cls.c.top_link == True)
    elif sort == 'blessed':
        q._filter(link_cls.c.blessed == True)

    if time != 'all':
        q._filter(db_times[time])
    return make_results(q)

def get_domain_links(domain, sort, time):
    return DomainSearchQuery(domain, sort=search_sort[sort], timerange=time)

def user_query(kind, user, sort, time):
    """General profile-page query."""
    q = kind._query(kind.c.author_id == user._id,
                    kind.c._spam == (True, False),
                    sort = db_sort(sort))
    if time != 'all':
        q._filter(db_times[time])
    return make_results(q)

def get_comments(user, sort, time):
    return user_query(Comment, user, sort, time)

def get_submitted(user, sort, time):
    return user_query(Link, user, sort, time)

def get_overview(user, sort, time):
    return merge_results(get_comments(user, sort, time),
                         get_submitted(user, sort, time))
    
def user_rel_query(rel, user, name, hide_spam=True):
    """General user relationship query."""
    q = rel._query(rel.c._thing1_id == user._id,
                   rel.c._t2_deleted == False,
                   rel.c._name == name,
                   sort = desc('_date'),
                   eager_load = True,
                   thing_data = not g.use_query_cache
                   )

    if hide_spam:
        q._filter(rel.c._t2_spam == False)
       
    return make_results(q, filter_thing2)

vote_rel = Vote.rel(Account, Link)

def get_liked(user, hide_spam=True):
    return user_rel_query(vote_rel, user, '1', hide_spam)

def get_disliked(user, hide_spam=True):
    return user_rel_query(vote_rel, user, '-1', hide_spam)

def get_hidden(user, hide_spam=True):
    return user_rel_query(SaveHide, user, 'hide', hide_spam)

def get_saved(user, hide_spam=True):
    return user_rel_query(SaveHide, user, 'save', hide_spam)

def get_drafts(user):
    draft_sr = Subreddit._by_name(user.draft_sr_name)
    return get_links(draft_sr, 'new', 'all')

inbox_message_rel = Inbox.rel(Account, Message)
def get_inbox_messages(user):
    return user_rel_query(inbox_message_rel, user, 'inbox')

inbox_comment_rel = Inbox.rel(Account, Comment)
def get_inbox_comments(user):
    return user_rel_query(inbox_comment_rel, user, 'inbox')

def get_inbox(user):
    return merge_results(get_inbox_comments(user),
                         get_inbox_messages(user))

def get_sent(user):
    q = Message._query(Message.c.author_id == user._id,
                       Message.c._spam == (True, False),
                       sort = desc('_date'))
    return make_results(q)

def add_queries(queries, insert_item = None, delete_item = None):
    """Adds multiple queries to the query queue. If insert_item or
    delete_item is specified, the query may not need to be recomputed at
    all."""
    def _add_queries():
        for q in queries:
            if not isinstance(q, CachedResults):
                continue

            if insert_item and q.can_insert():
                q.insert(insert_item)
            elif delete_item and q.can_delete():
                q.delete(delete_item)
            else:
                query_queue.add_query(q)
    worker.do(_add_queries)

#can be rewritten to be more efficient
def all_queries(fn, obj, *param_lists):
    """Given a fn and a first argument 'obj', calls the fn(obj, *params)
    for every permutation of the parameters in param_lists"""
    results = []
    params = [[obj]]
    for pl in param_lists:
        new_params = []
        for p in pl:
            for c in params:
                new_param = list(c)
                new_param.append(p)
                new_params.append(new_param)
        params = new_params

    results = [fn(*p) for p in params]
    return results

def display_jobs(jobs):
    for r in jobs:
        print r
    print len(jobs)

## The following functions should be called after their respective
## actions to update the correct listings.
def new_link(link):
    sr = Subreddit._byID(link.sr_id)
    author = Account._byID(link.author_id)

    results = all_queries(get_links, sr, ('hot', 'new', 'old'), ['all'])
    results.extend(all_queries(get_links, sr, ('top', 'controversial'), db_times.keys()))
    results.append(get_submitted(author, 'new', 'all'))
    results.append(get_links(sr, 'toplinks', 'all'))
    
    if link._deleted:
        add_queries(results, delete_item = link)
    else:
        add_queries(results, insert_item = link)

def new_comment(comment, inbox_rel):
    author = Account._byID(comment.author_id)
    job = [get_comments(author, 'new', 'all')]
    if comment._deleted:
        add_queries(job, delete_item = comment)
    else:
        add_queries(job, insert_item = comment)

    if inbox_rel:
        inbox_owner = inbox_rel._thing1
        add_queries([get_inbox_comments(inbox_owner)],
                    insert_item = inbox_rel)

def new_vote(vote):
    user = vote._thing1
    item = vote._thing2

    if not isinstance(item, Link):
        return

    if vote.valid_thing:
        sr = item.subreddit_slow
        results = all_queries(get_links, sr, ('hot', 'new'), ['all'])
        results.extend(all_queries(get_links, sr, ('top', 'controversial'), db_times.keys()))
        results.append(get_links(sr, 'toplinks', 'all'))
        add_queries(results)
    
    #must update both because we don't know if it's a changed vote
    if vote._name == '1':
        add_queries([get_liked(user)], insert_item = vote)
        add_queries([get_disliked(user)], delete_item = vote)
    elif vote._name == '-1':
        add_queries([get_liked(user)], delete_item = vote)
        add_queries([get_disliked(user)], insert_item = vote)
    else:
        add_queries([get_liked(user)], delete_item = vote)
        add_queries([get_disliked(user)], delete_item = vote)
    
def new_message(message, inbox_rel):
    from_user = Account._byID(message.author_id)
    to_user = Account._byID(message.to_id)

    add_queries([get_sent(from_user)], insert_item = message)
    add_queries([get_inbox_messages(to_user)], insert_item = inbox_rel)

def new_savehide(rel):
    user = rel._thing1
    name = rel._name
    if name == 'save':
        add_queries([get_saved(user)], insert_item = rel)
    elif name == 'unsave':
        add_queries([get_saved(user)], delete_item = rel)
    elif name == 'hide':
        add_queries([get_hidden(user)], insert_item = rel)
    elif name == 'unhide':
        add_queries([get_hidden(user)], delete_item = rel)

def add_all_srs():
    """Adds every listing query for every subreddit to the queue."""
    q = Subreddit._query(sort = asc('_date'))
    for sr in fetch_things2(q):
        add_queries(all_queries(get_links, sr, ('hot', 'new', 'old'), ['all']))
        add_queries(all_queries(get_links, sr, ('top', 'controversial'), db_times.keys()))
        add_queries([get_links(sr, 'toplinks', 'all')])


def update_user(user):
    if isinstance(user, str):
        user = Account._by_name(user)
    elif isinstance(user, int):
        user = Account._byID(user)

    results = [get_inbox_messages(user),
               get_inbox_comments(user),
               get_sent(user),
               get_liked(user),
               get_disliked(user),
               get_saved(user),
               get_hidden(user),
               get_submitted(user, 'new', 'all'),
               get_comments(user, 'new', 'all')]
    add_queries(results)

def add_all_users():
    q = Account._query(sort = asc('_date'))
    for user in fetch_things2(q):
        update_user(user)

########NEW FILE########
__FILENAME__ = query_queue
from __future__ import with_statement
from r2.lib.workqueue import WorkQueue
from r2.config.databases import query_queue_engine, tz
from r2.lib.db.tdb_sql import make_metadata, settings, create_table, index_str

import cPickle as pickle
from datetime import datetime
from urllib2 import Request, urlopen
from urllib import urlencode
from threading import Lock
import time

import sqlalchemy as sa
from sqlalchemy.exceptions import SQLError

from pylons import g

#the current 
running = set()
running_lock = Lock()

def make_query_queue_table():
    metadata = make_metadata(query_queue_engine)
    table =  sa.Table(settings.DB_APP_NAME + '_query_queue', metadata,
                      sa.Column('iden', sa.String, primary_key = True),
                      sa.Column('query', sa.Binary),
                      sa.Column('date', sa.DateTime(timezone = True)))
    date_idx = index_str(table, 'date', 'date')
    create_table(table, [date_idx])
    return table

query_queue_table = make_query_queue_table()

def add_query(cached_results):
    """Adds a CachedResults instance to the queue db, ignoring duplicates"""
    d = dict(iden = cached_results.query._iden(),
             query = pickle.dumps(cached_results, -1),
             date = datetime.now(tz))
    try:
        query_queue_table.insert().execute(d)
    except SQLError, e:
        #don't worry about inserting duplicates
        if not 'IntegrityError' in str(e):
            raise

def remove_query(iden):
    """Removes a row identified with iden from the query queue. To be
    called after a CachedResults is updated."""
    table = query_queue_table
    d = table.delete(table.c.iden == iden)
    d.execute()

def get_query():
    """Gets the next query off the queue, ignoring the currently running
    queries."""
    table = query_queue_table

    s = table.select(order_by = sa.asc(table.c.date), limit = 1)
    s.append_whereclause(sa.and_(*[table.c.iden != i for i in running]))
    r = s.execute().fetchone()

    if r:
        return r.iden, r.query
    else:
        return None, None

def make_query_job(iden, pickled_cr):
    """Creates a job to send to the query worker. Updates a cached result
    then removes the query from both the queue and the running set. If
    sending the job fails, the query is only remove from the running
    set."""
    precompute_worker = g.query_queue_worker
    def job():
        try:
            finished = False
            r = Request(url = precompute_worker + '/doquery',
                        data = urlencode([('query', pickled_cr)]),
                        #this header prevents pylons from turning the
                        #parameter into unicode, which breaks pickling
                        headers = {'x-dont-decode':'true'})
            urlopen(r)
            finished = True
        finally:
            with running_lock:
                running.remove(iden)
                #if finished is false, we'll leave the query in the db
                #so we can try again later (e.g. in the event the
                #worker is down)
                if finished:
                    remove_query(iden)
    return job

def run():
    """Pull jobs from the queue, creates a job, and sends them to a
    WorkQueue for processing."""
    num_workers = g.num_query_queue_workers
    wq = WorkQueue(num_workers = num_workers)
    wq.start()

    while True:
        job = None
        #limit the total number of jobs in the WorkQueue. we don't
        #need to load the entire db queue right away (the db queue can
        #get quite large).
        if len(running) < 2 * num_workers:
            with running_lock:
                iden, pickled_cr = get_query()
                if pickled_cr is not None:
                    if not iden in running:
                        running.add(iden)
                        job = make_query_job(iden, pickled_cr)
                        wq.add(job)

        #if we didn't find a job, sleep before trying again
        if not job:
            time.sleep(1)

########NEW FILE########
__FILENAME__ = sorts
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from math import log, sqrt
from datetime import datetime, timedelta
from r2.config.databases import tz

epoch = datetime(1970, 1, 1, tzinfo = tz)

def epoch_seconds(date):
    """Returns the number of seconds from the epoch to date. Should match
    the number returned by the equivalent function in postgres."""
    td = date - epoch
    return td.days * 86400 + td.seconds + (float(td.microseconds) / 1000000)

def interestingness(ups, downs, descendants):
    return ups - downs + descendants / 2
    
def score(ups, downs):
    return ups - downs

def hot(ups, downs, date):
    """The hot formula. Should match the equivalent function in postgres."""
    s = score(ups, downs)
    order = log(max(abs(s), 1), 10)
    sign = 1 if s > 0 else -1 if s < 0 else 0
    seconds = epoch_seconds(date) - 1134028003
    return round(order + sign * seconds / 45000, 7)

def controversy(ups, downs):
    """The controversy sort."""
    return float(ups + downs) / max(abs(score(ups, downs)), 1)


def _confidence(ups, downs):
    """The confidence sort.
       http://www.evanmiller.org/how-not-to-sort-by-average-rating.html"""
    n = ups + downs

    if n == 0:
        return 0

    z = 1.281551565545 # 80% confidence
    p = float(ups) / n

    left = p + 1/(2*n)*z*z
    right = z*sqrt(p*(1-p)/n + z*z/(4*n*n))
    under = 1+1/n*z*z

    return (left - right) / under

# precompute low values
up_range = 400
down_range = 100
_confidences = []
for _ups in xrange(up_range):
    for _downs in xrange(down_range):
        _confidences.append(_confidence(_ups, _downs))

def confidence(ups, downs):
    if ups + downs == 0:
        return 0
    elif ups < up_range and downs < down_range:  # check if pair is precomputed
        return _confidences[downs + ups * down_range]
    else:
        return _confidence(ups, downs)


########NEW FILE########
__FILENAME__ = stats
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
cache_key = 'popular_queries'
reset_num = 100
min_run = 2
cache_time = 70
max_queries = 50

class QueryStats(object):
    def __init__(self):
        self.reset()

    def reset(self):
        self.query_count = {}
        self.total_count = 0
        self.queries = {}

    def add(self, query):
        iden = query._iden()
        if self.query_count.has_key(iden):
            self.query_count[iden] += 1
        else:
            self.queries[iden] = query
            self.query_count[iden] = 1
        self.total_count += 1

        #update every reset_num queries
        if self.total_count > reset_num:
            self.update_cache()
            self.reset()
        
    def update_cache(self):
        #sort count
        idens = self.query_count.keys()
        idens.sort(key = lambda x: self.query_count[x], reverse = True)
        idens = idens[:max_queries]
        
        #cache queries with min occurances
        queries = [self.queries[i]
                   for i in idens if self.query_count[i] > min_run]
        from pylons import g
        cache = g.cache
        cache.set(cache_key, queries)

def default_queries():
    from r2.models import Link, Subreddit
    from r2.lib.db.operators import desc
    from copy import deepcopy
    queries = []

    q = Link._query(Link.c.sr_id == Subreddit.user_subreddits(None),
                    sort = desc('_hot'),
                    limit = 37)

    queries.append(q)
    #add a higher limit one too
    q = deepcopy(q)
    q._limit = 75
    queries.append(q)

    return queries

def run_queries():
    from r2.models import subreddit
    from pylons import g
    cache = g.cache
    queries = cache.get(cache_key) or default_queries()
    
    for q in queries:
        q._read_cache = False
        q._write_cache = True
        q._cache_time = cache_time
        q._list()

    #find top
    q = default_queries()[0]
    q._limit = 1
    top_link = list(q)[0]
    if top_link:
        top_link._load()
        top_link.top_link = True
        top_link._commit()

########NEW FILE########
__FILENAME__ = tdb_sql
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is Reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
#
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.lib.utils import storage, storify, iters, Results, tup, TransSet
from r2.config.databases import dbm, tz
from pylons import g

import operators
import sqlalchemy as sa
from sqlalchemy.databases import postgres
from datetime import datetime
import cPickle as pickle

from copy import deepcopy

import logging
log_format = logging.Formatter('sql: %(message)s')

settings = storage()
settings.DEBUG = g.debug
settings.DB_CREATE_TABLES = True
settings.DB_APP_NAME = 'reddit'

max_val_len = 1000

transactions = TransSet()

BigInteger = postgres.PGBigInteger

def alias_generator():
    n = 1
    while True:
        yield 'alias_%d' % n
        n += 1

def make_metadata(engine):
    metadata = sa.BoundMetaData(engine)
    metadata.bind.echo = g.sqlprinting
    return metadata

def create_table(table, index_commands=None):
    t = table
    if settings.DB_CREATE_TABLES:
        #@@hackish?
        if not t.engine.has_table(t.name):
            t.create(checkfirst = False)
            if index_commands:
                for i in index_commands:
                    t.engine.execute(i)

def index_str(table, name, on, where = None):
    index_str = 'create index idx_%s_' % name
    index_str += table.name
    index_str += ' on '+ table.name + ' (%s)' % on
    if where:
        index_str += ' where %s' % where
    return index_str


def index_commands(table, type):
    commands = []

    if type == 'thing':
        commands.append(index_str(table, 'id', 'thing_id'))
        commands.append(index_str(table, 'date', 'date'))
        commands.append(index_str(table, 'deleted_spam', 'deleted, spam'))
        commands.append(index_str(table, 'hot', 'hot(ups, downs, date), date'))
        commands.append(index_str(table, 'score', 'score(ups, downs), date'))
        commands.append(index_str(table, 'controversy', 'controversy(ups, downs), date'))
        if table.columns.has_key('descendant_karma'):
            commands.append(index_str(table, 'interestingness', 'interestingness(ups, downs, descendant_karma)'))
    elif type == 'data':
        commands.append(index_str(table, 'id', 'thing_id'))
        commands.append(index_str(table, 'thing_id', 'thing_id'))
        commands.append(index_str(table, 'key_value', 'key, substring(value, 1, %s)' \
                                  % max_val_len))

        #lower name
        commands.append(index_str(table, 'lower_key_value', 'key, lower(value)',
                                  where = "key = 'name'"))
        #ip
        commands.append(index_str(table, 'ip_network', 'ip_network(value)',
                                  where = "key = 'ip'"))
        #base_url
        commands.append(index_str(table, 'base_url', 'base_url(lower(value))',
                                  where = "key = 'url'"))
        #author_id (or votes)
        commands.append(index_str(table, 'author_id', 'value',
                                  where = "key = 'author_id'"))
    elif type == 'rel':
        commands.append(index_str(table, 'thing1_name_date', 'thing1_id, name, date'))
        commands.append(index_str(table, 'thing2_name_date', 'thing2_id, name, date'))
        commands.append(index_str(table, 'thing1_id', 'thing1_id'))
        commands.append(index_str(table, 'thing2_id', 'thing2_id'))
        commands.append(index_str(table, 'name', 'name'))
        commands.append(index_str(table, 'date', 'date'))
    return commands

def get_type_table(metadata):
    table = sa.Table(settings.DB_APP_NAME + '_type', metadata,
                     sa.Column('id', sa.Integer, primary_key = True),
                     sa.Column('name', sa.String, nullable = False))
    return table

def get_rel_type_table(metadata):
    table = sa.Table(settings.DB_APP_NAME + '_type_rel', metadata,
                     sa.Column('id', sa.Integer, primary_key = True),
                     sa.Column('type1_id', sa.Integer, nullable = False),
                     sa.Column('type2_id', sa.Integer, nullable = False),
                     sa.Column('name', sa.String, nullable = False))
    return table


def get_thing_table(metadata, name):
    table = sa.Table(settings.DB_APP_NAME + '_thing_' + name, metadata,
                     sa.Column('thing_id', BigInteger, primary_key = True),
                     sa.Column('ups', sa.Integer, default = 0, nullable = False),
                     sa.Column('downs',
                               sa.Integer,
                               default = 0,
                               nullable = False),
                     sa.Column('deleted',
                               sa.Boolean,
                               default = False,
                               nullable = False),
                     sa.Column('spam',
                               sa.Boolean,
                               default = False,
                               nullable = False),
                     sa.Column('date',
                               sa.DateTime(timezone = True),
                               default = sa.func.now(),
                               nullable = False))

    if name in ('comment', 'link'):
        table.append_column(sa.Column('descendant_karma',
                            sa.Integer,
                            default = 0,
                            nullable = False))

    return table

def get_data_table(metadata, name):
    data_table = sa.Table(settings.DB_APP_NAME + '_data_' + name, metadata,
                          sa.Column('thing_id', BigInteger, nullable = False,
                                    primary_key = True),
                          sa.Column('key', sa.String, nullable = False,
                                    primary_key = True),
                          sa.Column('value', sa.String),
                          sa.Column('kind', sa.String))
    return data_table

def get_rel_table(metadata, name):
    rel_table = sa.Table(settings.DB_APP_NAME + '_rel_' + name, metadata,
                         sa.Column('rel_id', BigInteger, primary_key = True),
                         sa.Column('thing1_id', BigInteger, nullable = False),
                         sa.Column('thing2_id', BigInteger, nullable = False),
                         sa.Column('name', sa.String, nullable = False),
                         sa.Column('date', sa.DateTime(timezone = True),
                                   default = sa.func.now(), nullable = False),
                         sa.UniqueConstraint('thing1_id', 'thing2_id', 'name'))
    return rel_table

#get/create the type tables
def make_type_table():
    metadata = make_metadata(dbm.type_db)
    table = get_type_table(metadata)
    create_table(table)
    return table
type_table = make_type_table()

def make_rel_type_table():
    metadata = make_metadata(dbm.relation_type_db)
    table = get_rel_type_table(metadata)
    create_table(table)
    return table
rel_type_table = make_rel_type_table()

#lookup dicts
types_id = {}
types_name = {}
rel_types_id = {}
rel_types_name = {}
extra_thing_tables = {}
thing_engines = {}

def check_type(table, selector, insert_vals):
    #check for type in type table, create if not existent
    r = table.select(selector).execute().fetchone()
    if not r:
        r = table.insert().execute(**insert_vals)
        type_id = r.last_inserted_ids()[0]
    else:
        type_id = r.id
    return type_id

#make the thing tables
def build_thing_tables():
    for name, thing_engine, data_engine in dbm.things():
        type_id = check_type(type_table,
                             type_table.c.name == name,
                             dict(name = name))

        thing_engines[name] = thing_engine

        #make thing table
        thing_table = get_thing_table(make_metadata(thing_engine), name)
        create_table(thing_table,
                     index_commands(thing_table, 'thing'))

        #make data tables
        data_metadata = make_metadata(data_engine)
        data_table = get_data_table(data_metadata, name)
        create_table(data_table,
                     index_commands(data_table, 'data'))

        #do we need another table?
        if thing_engine == data_engine:
            data_thing_table = thing_table
        else:
            #we're in a different engine, but do we need to maintain the extra table?
            if dbm.extra_data.get(data_engine):
                data_thing_table = get_thing_table(data_metadata, 'data_' + name)
                extra_thing_tables.setdefault(type_id, set()).add(data_thing_table)
                create_table(data_thing_table,
                             index_commands(data_thing_table, 'thing'))
            else:
                data_thing_table = get_thing_table(data_metadata, name)

        thing = storage(type_id = type_id,
                        name = name,
                        thing_table = thing_table,
                        data_table = (data_table, data_thing_table))

        types_id[type_id] = thing
        types_name[name] = thing
build_thing_tables()

#make relation tables
def build_rel_tables():
    for name, type1_name, type2_name, engine in dbm.relations():
        type1_id = types_name[type1_name].type_id
        type2_id = types_name[type2_name].type_id
        type_id = check_type(rel_type_table,
                             rel_type_table.c.name == name,
                             dict(name = name,
                                  type1_id = type1_id,
                                  type2_id = type2_id))

        metadata = make_metadata(engine)

        #relation table
        rel_table = get_rel_table(metadata, name)
        create_table(rel_table,
                     index_commands(rel_table, 'rel'))

        #make thing1 table if required
        if engine == thing_engines[type1_name]:
            rel_t1_table = types_name[type1_name].thing_table
        else:
            #need to maintain an extra thing table?
            if dbm.extra_thing1.get(engine):
                rel_t1_table = get_thing_table(metadata, 'rel_' + name + '_type1')
                create_table(rel_t1_table, index_commands(rel_t1_table, 'thing'))
                extra_thing_tables.setdefault(type_id, set()).add(rel_t1_table)
            else:
                rel_t1_table = get_thing_table(metadata, type1_name)

        #make thing2 table if required
        if type1_id == type2_id:
            rel_t2_table = rel_t1_table
        elif engine == thing_engines[type2_name]:
            rel_t2_table = types_name[type2_name].thing_table
        else:
            if dbm.extra_thing2.get(engine):
                rel_t2_table = get_thing_table(metadata, 'rel_' + name + '_type2')
                create_table(rel_t2_table, index_commands(rel_t2_table, 'thing'))
                extra_thing_tables.setdefault(type_id, set()).add(rel_t2_table)
            else:
                rel_t2_table = get_thing_table(metadata, type2_name)

        #build the data
        rel_data_table = get_data_table(metadata, 'rel_' + name)
        create_table(rel_data_table,
                     index_commands(rel_data_table, 'data'))

        rel = storage(type_id = type_id,
                      type1_id = type1_id,
                      type2_id = type2_id,
                      name = name,
                      rel_table = (rel_table, rel_t1_table, rel_t2_table, rel_data_table))

        rel_types_id[type_id] = rel
        rel_types_name[name] = rel
build_rel_tables()

def get_type_id(name):
    return types_name[name][0]

def get_rel_type_id(name):
    return rel_types_name[name][0]

#TODO does the type actually exist?
def make_thing(type_id, ups, downs, date, deleted, spam, id=None):
    table = types_id[type_id].thing_table

    params = dict(ups = ups, downs = downs,
                  date = date, deleted = deleted, spam = spam)

    if id:
        params['thing_id'] = id

    def do_insert(t):
        transactions.add_engine(t.engine)
        r = t.insert().execute(**params)
        new_id = r.last_inserted_ids()[0]
        return new_id

    try:
        id = do_insert(table)
        params['thing_id'] = id
        for t in extra_thing_tables.get(type_id, ()):
            do_insert(t)

        return id
    except sa.exceptions.SQLError, e:
        if not 'IntegrityError' in e.message:
            raise
        # wrap the error to prevent db layer bleeding out
        raise CreationError, "Thing exists (%s)" % str(params)


def set_thing_props(type_id, thing_id, **props):
    table = types_id[type_id].thing_table

    if not props:
        return

    #use real columns
    def do_update(t):
        transactions.add_engine(t.engine)
        new_props = dict((t.c[prop], val) for prop, val in props.iteritems())
        u = t.update(t.c.thing_id == thing_id, values = new_props)
        u.execute()

    do_update(table)
    for t in extra_thing_tables.get(type_id, ()):
        do_update(t)

def incr_thing_prop(type_id, thing_id, prop, amount):
    table = types_id[type_id].thing_table

    def do_update(t):
        transactions.add_engine(t.engine)
        u = t.update(t.c.thing_id == thing_id,
                     values={t.c[prop] : t.c[prop] + amount})
        u.execute()

    do_update(table)
    for t in extra_thing_tables.get(type_id, ()):
        do_update(t)

def incr_things_prop(type_id, thing_ids, prop, amount):
    table = types_id[type_id].thing_table

    def render_list(thing_ids):
        if len(thing_ids) == 1:
            return '(' + str(int(thing_ids[0])) + ')'
        else:
            return tuple([int(id) for id in thing_ids])

    u = """UPDATE %(table)s SET %(prop)s=%(prop)s+%(amount)s
        WHERE %(table)s.thing_id IN %(thing_ids)s"""
    u = u % dict(prop = prop,
                 table = table.name,
                 amount = amount,
                 thing_ids = render_list(thing_ids))

    table.engine.execute(u)


class CreationError(Exception): pass

#TODO does the type exist?
#TODO do the things actually exist?
def make_relation(rel_type_id, thing1_id, thing2_id, name, date=None):
    table = rel_types_id[rel_type_id].rel_table[0]
    transactions.add_engine(table.engine)

    if not date: date = datetime.now(tz)
    try:
        r = table.insert().execute(thing1_id = thing1_id,
                                   thing2_id = thing2_id,
                                   name = name,
                                   date = date)
        return r.last_inserted_ids()[0]
    except sa.exceptions.SQLError, e:
        if not 'IntegrityError' in e.message:
            raise
        # wrap the error to prevent db layer bleeding out
        raise CreationError, "Relation exists (%s, %s, %s)" % (name, thing1_id, thing2_id)


def set_rel_props(rel_type_id, rel_id, **props):
    t = rel_types_id[rel_type_id].rel_table[0]

    if not props:
        return

    #use real columns
    transactions.add_engine(t.engine)
    new_props = dict((t.c[prop], val) for prop, val in props.iteritems())
    u = t.update(t.c.rel_id == rel_id, values = new_props)
    u.execute()


def py2db(val, return_kind=False):
    if isinstance(val, bool):
        val = 't' if val else 'f'
        kind = 'bool'
    elif isinstance(val, (str, unicode)):
        kind = 'str'
    elif isinstance(val, (int, float, long)):
        kind = 'num'
    elif val is None:
        kind = 'none'
    else:
        kind = 'pickle'
        val = pickle.dumps(val)

    if return_kind:
        return (val, kind)
    else:
        return val

def db2py(val, kind):
    if kind == 'bool':
        val = True if val is 't' else False
    elif kind == 'num':
        try:
            val = int(val)
        except ValueError:
            val = float(val)
    elif kind == 'none':
        val = None
    elif kind == 'pickle':
        val = pickle.loads(val)

    return val

#TODO i don't need type_id
def set_data(table, type_id, thing_id, **vals):
    s = sa.select([table.c.key], sa.and_(table.c.thing_id == thing_id))

    transactions.add_engine(table.engine)
    keys = [x.key for x in s.execute().fetchall()]

    i = table.insert(values = dict(thing_id = thing_id))
    u = table.update(sa.and_(table.c.thing_id == thing_id,
                             table.c.key == sa.bindparam('key')))

    inserts = []
    for key, val in vals.iteritems():
        val, kind = py2db(val, return_kind=True)

        #TODO one update?
        if key in keys:
            u.execute(key = key, value = val, kind = kind)
        else:
            inserts.append({'key':key, 'value':val, 'kind': kind})

    #do one insert
    if inserts:
        i.execute(*inserts)

def incr_data_prop(table, type_id, thing_id, prop, amount):
    t = table
    transactions.add_engine(t.engine)
    u = t.update(sa.and_(t.c.thing_id == thing_id,
                         t.c.key == prop),
                 values={t.c.value : sa.cast(t.c.value, sa.Float) + amount})
    u.execute()

def fetch_query(table, id_col, thing_id):
    """pull the columns from the thing/data tables for a list or single
    thing_id"""
    single = False

    if not isinstance(thing_id, iters):
        single = True
        thing_id = (thing_id,)

    s = sa.select([table], sa.or_(*[id_col == tid
                                    for tid in thing_id]))
    r = s.execute().fetchall()
    return (r, single)

#TODO specify columns to return?
def get_data(table, thing_id):
    r, single = fetch_query(table, table.c.thing_id, thing_id)

    #if single, only return one storage, otherwise make a dict
    res = storage() if single else {}
    for row in r:
        val = db2py(row.value, row.kind)
        stor = res if single else res.setdefault(row.thing_id, storage())
        stor[row.key] = val
    return res

def set_thing_data(type_id, thing_id, **vals):
    table = types_id[type_id].data_table[0]
    return set_data(table, type_id, thing_id, **vals)

def incr_thing_data(type_id, thing_id, prop, amount):
    table = types_id[type_id].data_table[0]
    return incr_data_prop(table, type_id, thing_id, prop, amount)

def get_thing_data(type_id, thing_id):
    table = types_id[type_id].data_table[0]
    return get_data(table, thing_id)

def get_thing(type_id, thing_id):
    table = types_id[type_id].thing_table
    r, single = fetch_query(table, table.c.thing_id, thing_id)

    #if single, only return one storage, otherwise make a dict
    res = {} if not single else None
    for row in r:
        stor = storage(ups = row.ups,
                       downs = row.downs,
                       date = row.date,
                       deleted = row.deleted,
                       spam = row.spam)
        if single:
            res = stor
        else:
            res[row.thing_id] = stor
    return res

def del_thing(type_id, thing_id):
    thing_table = types_id[type_id].thing_table
    data_table = types_id[type_id].data_table[0]

    transactions.add_engine(thing_table.engine)
    transactions.add_engine(data_table.engine)

    thing_table.delete(thing_table.c.thing_id == thing_id).execute()
    data_table.delete(data_table.c.thing_id == thing_id).execute()

def set_rel_data(rel_type_id, thing_id, **vals):
    table = rel_types_id[rel_type_id].rel_table[3]
    return set_data(table, rel_type_id, thing_id, **vals)

def incr_rel_data(rel_type_id, thing_id, prop, amount):
    table = rel_types_id[rel_type_id].rel_table[3]
    return incr_data_prop(table, rel_type_id, thing_id, prop, amount)

def get_rel_data(rel_type_id, rel_id):
    table = rel_types_id[rel_type_id].rel_table[3]
    return get_data(table, rel_id)

def get_rel(rel_type_id, rel_id):
    r_table = rel_types_id[rel_type_id].rel_table[0]
    r, single = fetch_query(r_table, r_table.c.rel_id, rel_id)

    res = {} if not single else None
    for row in r:
        stor = storage(thing1_id = row.thing1_id,
                       thing2_id = row.thing2_id,
                       name = row.name,
                       date = row.date)
        if single:
            res = stor
        else:
            res[row.rel_id] = stor
    return res

def del_rel(rel_type_id, rel_id):
    tables = rel_types_id[rel_type_id].rel_table
    table = tables[0]
    data_table = tables[3]

    transactions.add_engine(table.engine)
    transactions.add_engine(data_table.engine)

    table.delete(table.c.rel_id == rel_id).execute()
    data_table.delete(data_table.c.thing_id == rel_id).execute()

def sa_op(op):
    #if BooleanOp
    if isinstance(op, operators.or_):
        return sa.or_(*[sa_op(o) for o in op.ops])
    elif isinstance(op, operators.and_):
        return sa.and_(*[sa_op(o) for o in op.ops])

    #else, assume op is an instance of op
    if isinstance(op, operators.eq):
        fn = lambda x,y: x == y
    elif isinstance(op, operators.ne):
        fn = lambda x,y: x != y
    elif isinstance(op, operators.gt):
        fn = lambda x,y: x > y
    elif isinstance(op, operators.lt):
        fn = lambda x,y: x < y
    elif isinstance(op, operators.gte):
        fn = lambda x,y: x >= y
    elif isinstance(op, operators.lte):
        fn = lambda x,y: x <= y

    rval = tup(op.rval)

    if not rval:
        return '2+2=5'
    else:
        return sa.or_(*[fn(op.lval, v) for v in rval])

def translate_sort(table, column_name, lval = None, rewrite_name = True):
    if isinstance(lval, operators.query_func):
        fn_name = lval.__class__.__name__
        sa_func = getattr(sa.func, fn_name)
        return sa_func(translate_sort(table,
                                      column_name,
                                      lval.lval,
                                      rewrite_name))

    if rewrite_name:
        if column_name == 'id':
            return table.c.thing_id
        elif column_name == 'hot':
            return sa.func.hot(table.c.ups, table.c.downs, table.c.date)
        elif column_name == 'score':
            return sa.func.score(table.c.ups, table.c.downs)
        elif column_name == 'controversy':
            return sa.func.controversy(table.c.ups, table.c.downs)
        elif column_name == 'interestingness':
            return sa.func.interestingness(table.c.ups, table.c.downs, table.c.descendant_karma)
    #else
    return table.c[column_name]

#TODO - only works with thing tables
def add_sort(sort, t_table, select):
    sort = tup(sort)

    prefixes = t_table.keys() if isinstance(t_table, dict) else None
    #sort the prefixes so the longest come first
    prefixes.sort(key = lambda x: len(x))
    cols = []

    def make_sa_sort(s):
        orig_col = s.col

        col = orig_col
        if prefixes:
            table = None
            for k in prefixes:
                if k and orig_col.startswith(k):
                    table = t_table[k]
                    col = orig_col[len(k):]
            if not table:
                table = t_table[None]
        else:
            table = t_table

        real_col = translate_sort(table, col)

        #TODO a way to avoid overlap?
        #add column for the sort parameter using the sorted name
        select.append_column(real_col.label(orig_col))

        #avoids overlap temporarily
        select.use_labels = True

        #keep track of which columns we added so we can add joins later
        cols.append((real_col, table))

        #default to asc
        return (sa.desc(real_col) if isinstance(s, operators.desc)
                else sa.asc(real_col))

    sa_sort = [make_sa_sort(s) for s in sort]
    select.order_by(*sa_sort)
    return cols

def translate_thing_value(rval):
    if isinstance(rval, operators.timeago):
        return sa.text("current_timestamp - interval '%s'" % rval.interval)
    else:
        return rval

#will assume parameters start with a _ for consistency
def find_things(type_id, get_cols, sort, limit, constraints):
    table = types_id[type_id].thing_table
    constraints = deepcopy(constraints)

    s = sa.select([table.c.thing_id.label('thing_id')])

    for op in operators.op_iter(constraints):
        #assume key starts with _
        #if key.startswith('_'):
        key = op.lval_name
        op.lval = translate_sort(table, key[1:], op.lval)
        op.rval = translate_thing_value(op.rval)

    for op in constraints:
        s.append_whereclause(sa_op(op))

    if sort:
        add_sort(sort, {'_': table}, s)

    if limit:
        s.limit = limit

    r = s.execute()
    return Results(r, lambda(row): row if get_cols else row.thing_id)

def translate_data_value(alias, op):
    lval = op.lval
    need_substr = False if isinstance(lval, operators.query_func) else True
    lval = translate_sort(alias, 'value', lval, False)

    #add the substring func
    if need_substr:
        lval = sa.func.substring(lval, 1, max_val_len)

    op.lval = lval

    #convert the rval to db types
    #convert everything to strings for pg8.3
    op.rval = tuple(str(py2db(v)) for v in tup(op.rval))


#TODO sort by data fields
#TODO sort by id wants thing_id
def find_data(type_id, get_cols, sort, limit, constraints):
    d_table, t_table = types_id[type_id].data_table
    constraints = deepcopy(constraints)

    aliases = alias_generator()

    used_first = False
    s = None
    need_join = False
    have_data_rule = False
    first_alias = d_table.alias(aliases.next())
    s = sa.select([first_alias.c.thing_id.label('thing_id')])#, distinct=True)

    for op in operators.op_iter(constraints):
        key = op.lval_name
        vals = tup(op.rval)

        if key == '_id':
            op.lval = first_alias.c.thing_id
        elif key.startswith('_'):
            need_join = True
            op.lval = translate_sort(t_table, key[1:], op.lval)
            op.rval = translate_thing_value(op.rval)
        else:
            have_data_rule = True
            id_col = None
            if not used_first:
                alias = first_alias
                used_first = True
            else:
                alias = d_table.alias(aliases.next())
                id_col = first_alias.c.thing_id

            if id_col:
                s.append_whereclause(id_col == alias.c.thing_id)

            s.append_column(alias.c.value.label(key))
            s.append_whereclause(alias.c.key == key)

            #add the substring constraint if no other functions are there
            translate_data_value(alias, op)

    for op in constraints:
        s.append_whereclause(sa_op(op))

    if not have_data_rule:
        raise Exception('Data queries must have at least one data rule.')

    #TODO in order to sort by data columns, this is going to need to be smarter
    if sort:
        need_join = True
        add_sort(sort, {'_':t_table}, s)

    if need_join:
        s.append_whereclause(first_alias.c.thing_id == t_table.c.thing_id)

    if limit:
        s.limit = limit

    r = s.execute()

    return Results(r, lambda(row): row if get_cols else row.thing_id)


def find_rels(rel_type_id, get_cols, sort, limit, constraints):
    r_table, t1_table, t2_table, d_table = rel_types_id[rel_type_id].rel_table
    constraints = deepcopy(constraints)

    aliases = alias_generator()
    t1_table, t2_table = t1_table.alias(aliases.next()), t2_table.alias(aliases.next())

    s = sa.select([r_table.c.rel_id.label('rel_id')])
    need_join1 = ('thing1_id', t1_table)
    need_join2 = ('thing2_id', t2_table)
    joins_needed = set()

    for op in operators.op_iter(constraints):
        #vals = con.rval
        key = op.lval_name
        prefix = key[:4]

        if prefix in ('_t1_', '_t2_'):
            #not a thing attribute
            key = key[4:]

            if prefix == '_t1_':
                join = need_join1
                joins_needed.add(join)
            elif prefix == '_t2_':
                join = need_join2
                joins_needed.add(join)

            table = join[1]
            op.lval = translate_sort(table, key, op.lval)
            op.rval = translate_thing_value(op.rval)
            #ors = [sa_op(con, key, v) for v in vals]
            #s.append_whereclause(sa.or_(*ors))

        elif prefix.startswith('_'):
            op.lval = r_table.c[key[1:]]

        else:
            alias = d_table.alias(aliases.next())
            s.append_whereclause(r_table.c.rel_id == alias.c.thing_id)
            s.append_column(alias.c.value.label(key))
            s.append_whereclause(alias.c.key == key)

            translate_data_value(alias, op)

    for op in constraints:
        s.append_whereclause(sa_op(op))

    if sort:
        cols = add_sort(sort,
                        {'_':r_table, '_t1_':t1_table, '_t2_':t2_table},
                        s)

        #do we need more joins?
        for (col, table) in cols:
            if table == need_join1[1]:
                joins_needed.add(need_join1)
            elif table == need_join2[1]:
                joins_needed.add(need_join2)

    for j in joins_needed:
        col, table = j
        s.append_whereclause(r_table.c[col] == table.c.thing_id)

    if limit:
        s.limit = limit

    r = s.execute()
    return Results(r, lambda (row): (row if get_cols else row.rel_id))

if logging.getLogger('sqlalchemy').handlers:
    logging.getLogger('sqlalchemy').handlers[0].formatter = log_format

########NEW FILE########
__FILENAME__ = thing
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
#TODO byID use Things?
from __future__ import with_statement

import operators
import tdb_sql as tdb
import sorts
from .. utils import iters, Results, tup, to36, Storage
from r2.config import cache
from r2.config.databases import tz
from r2.lib.cache import sgm

import new, sys, hashlib
from datetime import datetime
from copy import copy, deepcopy

class NotFound(Exception): pass
CreationError = tdb.CreationError

thing_types = {}
rel_types = {}

def begin():
    tdb.transactions.begin()

def commit():
    tdb.transactions.commit()

def rollback():
    tdb.transactions.rollback()

def obj_id(things):
    return tuple(t if isinstance(t, (int, long)) else t._id for t in things)

def thing_prefix(cls_name, id=None):
    p = cls_name + '_'
    if id:
        p += str(id)
    return p

class SafeSetAttr:
    def __init__(self, cls):
        self.cls = cls

    def __enter__(self):
        self.cls.__safe__ = True

    def __exit__(self, type, value, tb):
        self.cls.__safe__ = False

class DataThing(object):
    _base_props = ()
    _int_props = ()
    _data_int_props = ()
    _int_prop_prefixes = ()
    _defaults = {}
    c = operators.Slots()
    __safe__ = False

    def __init__(self):
        safe_set_attr = SafeSetAttr(self)
        with safe_set_attr:
            self.safe_set_attr = safe_set_attr
            self._dirties = {}
            self._t = {}
            self._created = False
            self._loaded = True

    #TODO some protection here?
    def __setattr__(self, attr, val, make_dirty=True):
        if attr.startswith('__') or self.__safe__:
            object.__setattr__(self, attr, val)
            return 

        if attr.startswith('_'):
            #assume baseprops has the attr
            if make_dirty and hasattr(self, attr):
                old_val = getattr(self, attr)
            object.__setattr__(self, attr, val)
            if not attr in self._base_props:
                return
        else:
            old_val = self._t.get(attr, self._defaults.get(attr))
            self._t[attr] = val

        if make_dirty and val != old_val:
            self._dirties[attr] = val

    def __getattr__(self, attr):
        #makes pickling work for some reason
        if attr.startswith('__'):
            raise AttributeError

        try:
            if hasattr(self, '_t'):
                return self._t[attr]
            else:
                raise AttributeError, attr
        except KeyError:
            try:
                return getattr(self, '_defaults')[attr]
            except KeyError:
                if self._loaded:
                    raise AttributeError, '%s not found' % attr
                else:
                    raise AttributeError,\
                              attr + ' not found. thing is not loaded'

    def _commit(self, keys=None):
        if not self._created:
            self._create()

        if self._dirty:
            if keys:
                keys = tup(keys)
                to_set = dict((k, self._dirties[k])
                              for k in keys if self._dirties.has_key(k))
            else:
                to_set = self._dirties

            data_props = {}
            thing_props = {}
            for k, v in to_set.iteritems():
                if k.startswith('_'):
                    thing_props[k[1:]] = v
                else:
                    data_props[k] = v

            if data_props:
                self._set_data(self._type_id, self._id, **data_props)
            
            if thing_props:
                self._set_props(self._type_id, self._id, **thing_props)
            
            if keys:
                for k in keys:
                    if self._dirties.has_key(k):
                        del self._dirties[k]
            else:
                self._dirties.clear()

        # always set the cache
        cache.set(thing_prefix(self.__class__.__name__, self._id), self)

    def _delete_from_db(self):
        """
        Usually Things are soft-deleted, so this should be called rarely, and
        only in cases where you're sure the Thing isn't referenced anywhere else.
        """
        if not self._created:
            return

        tdb.del_thing(self._type_id, self._id)
        cache.delete(thing_prefix(self.__class__.__name__, self._id))

    @classmethod
    def _load_multi(cls, need):
        need = tup(need)
        need_ids = [n._id for n in need]
        datas = cls._get_data(cls._type_id, need_ids)
        to_save = {}
        for i in need:
            #if there wasn't any data, keep the empty dict
            i._t.update(datas.get(i._id, i._t))
            i._loaded = True
            to_save[i._id] = i

        prefix = thing_prefix(cls.__name__)

        #avoid race condition when incrementing data int props by
        #putting all the int props into the cache.

        #prop prefix
        def pp(prop, id):
            return prop + '_' + str(i._id)

        #do defined data props first, this also checks default values
        for prop in cls._data_int_props:
            for i in need:
                to_save[pp(prop, i._id)] = getattr(i, prop)

        #int props based on the suffix
        for i in need:
            for prop, val in i._t.iteritems():
                if any(prop.startswith(s) for s in cls._int_prop_prefixes):
                    to_save[pp(prop, i._id)] = val

        cache.set_multi(to_save, prefix)
        

    def _load(self):
        self._load_multi(self)

    def _safe_load(self):
        if not self._loaded:
            self._load()

    def _incr(self, prop, amt = 1):

        if self._dirty:
            raise ValueError, "cannot incr dirty thing"

        prefix = thing_prefix(self.__class__.__name__)
        key =  prefix + prop + '_' + str(self._id)
        cache_val = old_val = cache.get(key)
        if old_val is None:
            old_val = getattr(self, prop)

        if self._defaults.has_key(prop) and self._defaults[prop] == old_val:
            #potential race condition if the same property gets incr'd
            #from default at the same time
            setattr(self, prop, old_val + amt)
            self._commit(prop)
        else:
            self.__setattr__(prop, old_val + amt, False)
            #db
            if prop.startswith('_'):
                tdb.incr_thing_prop(self._type_id, self._id, prop[1:], amt)
            else:
                self._incr_data(self._type_id, self._id, prop, amt)
            cache.set(prefix + str(self._id), self)
            
        #cache
        if cache_val:
            cache.incr(key, amt)
        else:
            cache.set(key, getattr(self, prop))

    @property
    def _id36(self):
        return to36(self._id)

    @property
    def _fullname(self):
        return self._type_prefix + to36(self._type_id) + '_' + to36(self._id)

    #TODO error when something isn't found?
    @classmethod
    def _byID(cls, ids, data=False, return_dict=True, extra_props=None):
        ids, single = tup(ids, True)
        prefix = thing_prefix(cls.__name__)

        def items_db(ids):
            items = cls._get_item(cls._type_id, ids)
            for i in items.keys():
                items[i] = cls._build(i, items[i])

            #avoid race condition when incrmenting int props (data int
            #props are set in load_multi)
            for prop in cls._int_props:
                keys = dict((i, getattr(item, prop))
                            for i, item in items.iteritems())
                cache.set_multi(keys, prefix + prop + '_' )

            return items

        bases = sgm(cache, ids, items_db, prefix)

        #check to see if we found everything we asked for
        if any(i not in bases for i in ids):
            missing = [i for i in ids if i not in bases]
            raise NotFound, '%s %s' % (cls.__name__, missing)

        if data:
            need = [v for v in bases.itervalues() if not v._loaded]
            if need:
                cls._load_multi(need)

        #e.g. add the sort prop
        if extra_props:
            for _id, props in extra_props.iteritems():
                for k, v in props.iteritems():
                    bases[_id].__setattr__(k, v, False)

        if single:
            return bases[ids[0]]
        elif return_dict:
            return bases
        else:
            return filter(None, (bases.get(i) for i in ids))

    @classmethod
    def _by_fullname(cls, names,
                     return_dict = True, 
                     data=False, extra_props=None):
        names, single = tup(names, True)

        table = {}
        lookup = {}
        # build id list by type
        for fullname in names:
            try:
                real_type, thing_id = fullname.split('_')
                #distinguish between things and realtions
                if real_type[0] == 't':
                    type_dict = thing_types
                elif real_type[0] == 'r':
                    type_dict = rel_types
                real_type = type_dict[int(real_type[1:], 36)]
                thing_id = int(thing_id, 36)
                lookup[fullname] = (real_type, thing_id)
                table.setdefault(real_type, []).append(thing_id)
            except ValueError:
                if single:
                    raise NotFound

        # lookup ids for each type
        identified = {}
        for real_type, thing_ids in table.iteritems():
            i = real_type._byID(thing_ids, data = data,
                                extra_props = extra_props)
            identified[real_type] = i

        # interleave types in original order of the name
        res = []
        for fullname in names:
            if lookup.has_key(fullname):
                real_type, thing_id = lookup[fullname]
                res.append((fullname,
                            identified.get(real_type, {}).get(thing_id)))

        if single:
            return res[0][1]
        elif return_dict:
            return dict(res)
        else:
            return [x for i, x in res]

    @property
    def _dirty(self):
        return bool(len(self._dirties))

    @classmethod
    def _query(cls, *a, **kw):
        raise NotImplementedError()

    @classmethod
    def _build(*a, **kw):
        raise NotImplementedError()

    def _get_data(*a, **kw):
        raise NotImplementedError()

    def _set_data(*a, **kw):
        raise NotImplementedError()

    def _incr_data(*a, **kw):
        raise NotImplementedError()

    def _get_item(*a, **kw):
        raise NotImplementedError

    def _create(self):
        base_props = (getattr(self, prop) for prop in self._base_props)
        self._id = self._make_fn(self._type_id, *base_props)
        self._created = True

class ThingMeta(type):
    def __init__(cls, name, bases, dct):
        if name == 'Thing' or hasattr(cls, '_nodb') and cls._nodb: return
        #print "checking thing", name

        #TODO exceptions
        cls._type_name = name.lower()
        try:
            cls._type_id = tdb.types_name[cls._type_name].type_id
        except KeyError:
            raise KeyError, 'is the thing database %s defined?' % name

        global thing_types
        thing_types[cls._type_id] = cls

        super(ThingMeta, cls).__init__(name, bases, dct)
    
    def __repr__(cls):
        return '<thing: %s>' % cls._type_name

class Thing(DataThing):
    __metaclass__ = ThingMeta
    _base_props = ('_ups', '_downs', '_date', '_deleted', '_spam')
    _int_props = ('_ups', '_downs')
    _make_fn = staticmethod(tdb.make_thing)
    _set_props = staticmethod(tdb.set_thing_props)
    _get_data = staticmethod(tdb.get_thing_data)
    _set_data = staticmethod(tdb.set_thing_data)
    _get_item = staticmethod(tdb.get_thing)
    _incr_data = staticmethod(tdb.incr_thing_data)
    _type_prefix = 't'

    def __init__(self, ups = 0, downs = 0, date = None, deleted = False,
                 spam = False, id = None, **attrs):
        DataThing.__init__(self)

        with self.safe_set_attr:
            if id:
                self._id = id
                self._created = True
                self._loaded = False

            if not date: date = datetime.now(tz)
            
            self._ups = ups
            self._downs = downs
            self._date = date
            self._deleted = deleted
            self._spam = spam

        #new way
        for k, v in attrs.iteritems():
            self.__setattr__(k, v, not self._created)
        
    def __repr__(self):
        return '<%s %s>' % (self.__class__.__name__,
                            self._id if self._created else '[unsaved]')

    def _set_id(self, thing_id):
        if not self._created:
            with self.safe_set_attr:
                self._base_props += ('_thing_id',)
                self._thing_id = thing_id

    @property
    def _hot(self):
        return sorts.hot(self._ups, self._downs, self._date)

    @property
    def _score(self):
        return sorts.score(self._ups, self._downs)

    @property
    def _controversy(self):
        return sorts.controversy(self._ups, self._downs)

    @property
    def _confidence(self):
        return sorts.confidence(self._ups, self._downs)

    @property
    def _interestingness(self):
        return sorts.interestingness(self._ups, self._downs, self._descendant_karma)

    def score_triplet(self, likes = None):
        u = self._ups - (likes == True)
        d = self._downs - (likes == False)
        return [(u, d + 1), (u, d), (u + 1, d)]

    @classmethod
    def _build(cls, id, bases):
        return cls(bases.ups, bases.downs, bases.date,
                   bases.deleted, bases.spam, id)

    @classmethod
    def _query(cls, *rules, **kw):
        need_deleted = True
        need_spam = True
        #add default spam/deleted
        for r in rules:
            if not isinstance(r, operators.op):
                continue
            if r.lval_name == '_deleted':
                need_deleted = False
            elif r.lval_name == '_spam':
                need_spam = False

        if need_deleted or need_spam:
            rules = list(rules)

        if need_deleted:
            rules.append(cls.c._deleted == False)

        if need_spam:
            rules.append(cls.c._spam == False)

        return Things(cls, *rules, **kw)

    def __getattr__(self, attr):
        return DataThing.__getattr__(self, attr)
            
        

class RelationMeta(type):
    def __init__(cls, name, bases, dct):
        if name == 'RelationCls': return
        #print "checking relation", name

        cls._type_name = name.lower()
        try:
            cls._type_id = tdb.rel_types_name[cls._type_name].type_id
        except KeyError:
            raise KeyError, 'is the relationship database %s defined?' % name

        global rel_types
        rel_types[cls._type_id] = cls

        super(RelationMeta, cls).__init__(name, bases, dct)

    def __repr__(cls):
        return '<relation: %s>' % cls._type_name

def Relation(type1, type2, denorm1 = None, denorm2 = None):
    class RelationCls(DataThing):
        __metaclass__ = RelationMeta
        if not (issubclass(type1, Thing) and issubclass(type2, Thing)):
                raise TypeError('Relation types must be subclass of %s' % Thing)

        _type1 = type1
        _type2 = type2

        _base_props = ('_thing1_id', '_thing2_id', '_name', '_date')
        _make_fn = staticmethod(tdb.make_relation)
        _set_props = staticmethod(tdb.set_rel_props)
        _get_data = staticmethod(tdb.get_rel_data)
        _set_data = staticmethod(tdb.set_rel_data)
        _get_item = staticmethod(tdb.get_rel)
        _incr_data = staticmethod(tdb.incr_rel_data)
        _type_prefix = 'r'

        def __init__(self, thing1, thing2, name, date = None, id = None, **attrs):
            DataThing.__init__(self)

            def id_and_obj(in_thing):
                if isinstance(in_thing, (int, long)):
                    return in_thing
                else:
                    return in_thing._id

            with self.safe_set_attr:
                if id:
                    self._id = id
                    self._created = True
                    self._loaded = False

                if not date: date = datetime.now(tz)


                #store the id, and temporarily store the actual object
                #because we may need it later
                self._thing1_id = id_and_obj(thing1)
                self._thing2_id = id_and_obj(thing2)
                self._name = name
                self._date = date

            for k, v in attrs.iteritems():
                self.__setattr__(k, v, not self._created)

            def denormalize(denorm, src, dest):
                if denorm:
                    setattr(dest, denorm[0], getattr(src, denorm[1]))
                
            #denormalize
            if not self._created:
                denormalize(denorm1, thing2, thing1)
                denormalize(denorm2, thing1, thing2)

        def __getattr__(self, attr):
            if attr == '_thing1':
                return self._type1._byID(self._thing1_id)
            elif attr == '_thing2':
                return self._type2._byID(self._thing2_id)
            elif attr.startswith('_t1'):
                return getattr(self._thing1, attr[3:])
            elif attr.startswith('_t2'):
                return getattr(self._thing2, attr[3:])
            else:
                return DataThing.__getattr__(self, attr)
                            
        def __repr__(self):
            return ('<%s %s: <%s %s> - <%s %s> %s>' %
                    (self.__class__.__name__, self._name,
                     self._type1.__name__, self._thing1_id,
                     self._type2.__name__,self._thing2_id,
                     '[unsaved]' if not self._created else '\b'))

        def _commit(self):
            DataThing._commit(self)
            #if i denormalized i need to check here
            if denorm1: self._thing1._commit(denorm1[0])
            if denorm2: self._thing2._commit(denorm2[0])
            #set fast query cache
            cache.set(thing_prefix(self.__class__.__name__)
                      + str((self._thing1_id, self._thing2_id, self._name)),
                      self._id)

        def _delete(self):
            tdb.del_rel(self._type_id, self._id)
            
            #clear cache
            prefix = thing_prefix(self.__class__.__name__)
            #TODO - there should be just one cache key for a rel?
            cache.delete(prefix + str(self._id))
            #update fast query cache
            cache.set(prefix + str((self._thing1_id,
                                    self._thing2_id,
                                    self._name)), None)
            #temporarily set this property so the rest of this request
            #know it's deleted. save -> unsave, hide -> unhide
            self._name = 'un' + self._name

        @classmethod
        def _uncache(cls, thing1, thing2, name):
            # Remove a rel from from the fast query cache
            prefix = thing_prefix(cls.__name__)
            cache.delete(prefix + str((thing1._id,
                                       thing2._id,
                                       name)))

        @classmethod
        def _fast_query(cls, thing1s, thing2s, name, data=True):
            """looks up all the relationships between thing1_ids and thing2_ids
            and caches them"""
            prefix = thing_prefix(cls.__name__)

            thing1_dict = dict((t._id, t) for t in thing1s)
            thing2_dict = dict((t._id, t) for t in thing2s)

            thing1_ids = thing1_dict.keys()
            thing2_ids = thing2_dict.keys()

            name = tup(name)

            pairs = set((x, y, n)
                        for x in thing1_ids
                        for y in thing2_ids
                        for n in name)

            def items_db(pairs):
                t1_ids = set()
                t2_ids = set()
                names = set()
                for t1, t2, name in pairs:
                    t1_ids.add(t1)
                    t2_ids.add(t2)
                    names.add(name)

                q = cls._query(cls.c._thing1_id == t1_ids,
                               cls.c._thing2_id == t2_ids,
                               cls.c._name == names,
                               eager_load = True,
                               data = data)

                rel_ids = {}
                for rel in q:
                    #TODO an alternative for multiple
                    #relations with the same keys
                    #l = rel_ids.setdefault((rel._thing1_id, rel._thing2_id), [])
                    #l.append(rel._id)
                    rel_ids[(rel._thing1._id, rel._thing2._id, rel._name)] = rel._id
                
                for p in pairs:
                    if p not in rel_ids:
                        rel_ids[p] = None
                        
                return rel_ids

            res = sgm(cache, pairs, items_db, prefix)
            #convert the keys back into objects
            #we can assume the rels will be in the cache and just call
            #_byID lots
            res_obj = {}
            for k, rid in res.iteritems():
                obj_key = (thing1_dict[k[0]], thing2_dict[k[1]], k[2])
                res_obj[obj_key] = cls._byID(rid, data=data) if rid else None
                
            return res_obj
            
        @classmethod
        def _gay(cls):
            return cls._type1 == cls._type2

        @classmethod
        def _build(cls, id, bases):
            return cls(bases.thing1_id, bases.thing2_id, bases.name, bases.date, id)

        @classmethod
        def _query(cls, *a, **kw):
            return Relations(cls, *a, **kw)


    return RelationCls

class Query(object):
    def __init__(self, kind, *rules, **kw):
        self._rules = []
        self._kind = kind

        self._read_cache = kw.get('read_cache')
        self._write_cache = kw.get('write_cache')
        self._cache_time = kw.get('cache_time', 0)
        self._stats_collector = kw.get('stats_collector')
        self._limit = kw.get('limit')
        self._data = kw.get('data')
        self._sort = kw.get('sort', ())

        self._filter(*rules)
    
    def _setsort(self, sorts):
        sorts = tup(sorts)
        #make sure sorts are wrapped in a Sort obj
        have_date = False
        op_sorts = []
        for s in sorts:
            if not isinstance(s, operators.sort):
                s = operators.asc(s)
            op_sorts.append(s)
            if s.col.endswith('_date'):
                have_date = True
        if op_sorts and not have_date:
            op_sorts.append(operators.desc('_date'))

        self._sort_param = op_sorts
        return self

    def _getsort(self):
        return self._sort_param

    _sort = property(_getsort, _setsort)

    def _reverse(self):
        for s in self._sort:
            if isinstance(s, operators.asc):
                s.__class__ = operators.desc
            else:
                s.__class__ = operators.asc

    def _list(self, data = False):
        if data:
            self._data = data

        return list(self)

    def _dir(self, thing, reverse):
        ors = []
        #for each sort add and a comparison operator
        for i in range(len(self._sort)):
            s = self._sort[i]

            if isinstance(s, operators.asc):
                op = operators.gt
            else:
                op = operators.lt

            if reverse:
                op = operators.gt if op == operators.lt else operators.lt

            #remember op takes lval and lval_name
            ands = [op(s.col, s.col, getattr(thing, s.col))]

            #for each sort up to the last add an equals operator
            for j in range(0, i):
                s = self._sort[j]
                ands.append(thing.c[s.col] == getattr(thing, s.col))

            ors.append(operators.and_(*ands))

        return self._filter(operators.or_(*ors))

    def _before(self, thing):
        return self._dir(thing, True)

    def _after(self, thing):
        return self._dir(thing, False)

    def _count(self):
        return self._cursor().rowcount()


    def _filter(*a, **kw):
        raise NotImplementedError

    def _cursor(*a, **kw):
        raise NotImplementedError

    def _iden(self):
        i = str(self._sort) + str(self._kind) + str(self._limit)
        if self._rules:
            rules = copy(self._rules)
            rules.sort()
            for r in rules:
                i += str(r)
        return hashlib.sha1(i).hexdigest()

    def __iter__(self):
        used_cache = False

        if self._stats_collector:
            self._stats_collector.add(self)

        lst = None
        if self._read_cache:
            names = cache.get(self._iden())
            if names is not None:
                lst = Thing._by_fullname(names, data = self._data, return_dict = False)

        if lst is None:
            #hit the db
            lst = self._cursor().fetchall()
        else:
            used_cache = True

        if self._write_cache and not used_cache:
            names = tuple(i._fullname for i in lst)
            cache.set(self._iden(), names, self._cache_time)

        for i in lst:
            yield i

class Things(Query):
    def __init__(self, kind, *rules, **kw):
        self._use_data = False
        Query.__init__(self, kind, *rules, **kw)

    def _filter(self, *rules):
        for op in operators.op_iter(rules):
            if not op.lval_name.startswith('_'):
                self._use_data = True

        self._rules += rules
        return self

            
    def _cursor(self):
        #TODO why was this even here?
        #get_cols = bool(self._sort_param)
        get_cols = False
        params = (self._kind._type_id,
                  get_cols,
                  self._sort,
                  self._limit,
                  self._rules)
        if self._use_data:
            c = tdb.find_data(*params)
        else:
            c = tdb.find_things(*params)
            
        #TODO simplfy this! get_cols is always false?
        #called on a bunch of rows to fetch their properties in batch
        def row_fn(rows):
            #if have a sort, add the sorted column to the results
            if get_cols:
                extra_props = {}
                for r in rows:
                    for sc in (s.col for s in self._sort):
                        #dict of ids to the extra sort params
                        props = extra_props.setdefault(r.thing_id, {})
                        props[sc] = getattr(r, sc)
                _ids = extra_props.keys()
            else:
                _ids = rows
                extra_props = {}
            return self._kind._byID(_ids, self._data, False, extra_props)

        return Results(c, row_fn, True)

def load_things(rels, load_data=False):
    rels = tup(rels)
    kind = rels[0].__class__

    t1_ids = set()
    t2_ids = t1_ids if kind._gay() else set()
    for rel in rels:
        t1_ids.add(rel._thing1_id)
        t2_ids.add(rel._thing2_id)
    kind._type1._byID(t1_ids, load_data)
    if not kind._gay():
        t2_items = kind._type2._byID(t2_ids, load_data)

class Relations(Query):
    #params are thing1, thing2, name, date
    def __init__(self, kind, *rules, **kw):
        self._eager_load = kw.get('eager_load')
        self._thing_data = kw.get('thing_data')
        Query.__init__(self, kind, *rules, **kw)

    def _filter(self, *rules):
        self._rules += rules
        return self

    def _eager(self, eager, thing_data = False):
        #load the things (id, ups, down, etc.)
        self._eager_load = eager
        #also load the things' data
        self._thing_data = thing_data
        return self

    def _make_rel(self, rows):
        rels = self._kind._byID(rows, self._data, False)
        if rels and self._eager_load:
            load_things(rels, self._thing_data)
        return rels

    def _cursor(self):
        c = tdb.find_rels(self._kind._type_id,
                          False,
                          sort = self._sort,
                          limit = self._limit,
                          constraints = self._rules)
        return Results(c, self._make_rel, True)

class MultiCursor(object):
    def __init__(self, *execute_params):
        self._execute_params = execute_params
        self._cursor = None

    def fetchone(self):
        if not self._cursor:
            self._cursor = self._execute(*self._execute_params)
            
        return self._cursor.next()
                
    def fetchall(self):
        if not self._cursor:
            self._cursor = self._execute(*self._execute_params)

        return [i for i in self._cursor]

class MergeCursor(MultiCursor):
    def _execute(self, cursors, sorts):
        #a "pair" is a (cursor, item, done) tuple
        def safe_next(c):
            try:
                #hack to keep searching even if fetching a thing returns notfound
                while True:
                    try:
                        return [c, c.fetchone(), False]
                    except NotFound:
                        #skips the broken item
                        pass
            except StopIteration:
                return c, None, True

        def undone(pairs):
            return [p for p in pairs if not p[2]]

        pairs = undone(safe_next(c) for c in cursors)
        while pairs:
            #only one query left, just dump it
            if len(pairs) == 1:
                c, item, done = pair = pairs[0]
                while not done:
                    yield item
                    c, item, done = safe_next(c)
                    pair[:] = c, item, done
            else:
                #by default, yield the first item
                yield_pair = pairs[0]
                for s in sorts:
                    col = s.col
                    #sort direction?
                    max_fn = min if isinstance(s, operators.asc) else max

                    #find the max (or min) val
                    vals = [(getattr(i[1], col), i) for i in pairs]
                    max_pair = vals[0]
                    all_equal = True
                    for pair in vals[1:]:
                        if all_equal and pair[0] != max_pair[0]:
                            all_equal = False
                        max_pair = max_fn(max_pair, pair, key=lambda x: x[0])

                    if not all_equal:
                        yield_pair = max_pair[1]
                        break

                c, item, done = yield_pair
                yield item
                yield_pair[:] = safe_next(c)

            pairs = undone(pairs)
        raise StopIteration

class MultiQuery(Query):
    def __init__(self, queries, *rules, **kw):
        self._queries = queries
        Query.__init__(self, None, *rules, **kw)

    def _iden(self):
        return ''.join(q._iden() for q in self._queries)

    def _cursor(self):
        raise NotImplementedError()

    def _reverse(self):
        for q in self._queries:
            q._reverse()

    def _setdata(self, data):
        for q in self._queries:
            q._data = data

    def _getdata(self):
        if self._queries:
            return self._queries[0]._data

    _data = property(_getdata, _setdata)

    def _setsort(self, sorts):
        for q in self._queries:
            q._sort = deepcopy(sorts)

    def _getsort(self):
        if self._queries:
            return self._queries[0]._sort

    _sort = property(_getsort, _setsort)

    def _filter(self, *rules):
        for q in self._queries:
            q._filter(*rules)

    def _getrules(self):
        return [q._rules for q in self._queries]

    def _setrules(self, rules):
        for q,r in zip(self._queries, rules):
            q._rules = r

    _rules = property(_getrules, _setrules)

    def _getlimit(self):
        return self._queries[0]._limit

    def _setlimit(self, limit):
        for q in self._queries:
            q._limit = limit

    _limit = property(_getlimit, _setlimit)

class Merge(MultiQuery):
    def _cursor(self):
        if (any(q._sort for q in self._queries) and
            not reduce(lambda x,y: (x == y) and x,
                      (q._sort for q in self._queries))):
            raise "The sorts should be the same"

        return MergeCursor((q._cursor() for q in self._queries),
                           self._sort)

def MultiRelation(name, *relations):
    rels_tmp = {}
    for rel in relations:
        t1, t2 = rel._type1, rel._type2
        clsname = name + '_' + t1.__name__.lower() + '_' + t2.__name__.lower()
        cls = new.classobj(clsname, (rel,), {'__module__':t1.__module__})
        setattr(sys.modules[t1.__module__], clsname, cls)
        rels_tmp[(t1, t2)] = cls

    class MultiRelationCls(object):
        c = operators.Slots()
        rels = rels_tmp

        def __init__(self, thing1, thing2, *a, **kw):
            r = self.rel(thing1, thing2)
            self.__class__ = r
            self.__init__(thing1, thing2, *a, **kw)

        @classmethod
        def rel(cls, thing1, thing2):
            t1 = thing1 if isinstance(thing1, ThingMeta) else thing1.__class__
            t2 = thing2 if isinstance(thing2, ThingMeta) else thing2.__class__
            return cls.rels[(t1, t2)]

        @classmethod
        def _query(cls, *rules, **kw):
            #TODO it should be possible to send the rules and kw to
            #the merge constructor
            queries = [r._query(*rules, **kw) for r in cls.rels.values()]
            return Merge(queries)

        @classmethod
        def _fast_query(cls, sub, obj, name, data=True):
            #divide into types
            def type_dict(items):
                types = {}
                for i in items:
                    types.setdefault(i.__class__, []).append(i)
                return types
            
            sub_dict = type_dict(tup(sub))
            obj_dict = type_dict(tup(obj))

            #for each pair of types, see if we have a query to send
            res = {}
            for types, rel in cls.rels.iteritems():
                t1, t2 = types
                if sub_dict.has_key(t1) and obj_dict.has_key(t2):
                    res.update(rel._fast_query(sub_dict[t1], obj_dict[t2], name,
                                               data = True))

            return res

    return MultiRelationCls

# class JoinCursor(MultiCursor):
#     def _execute(self, c1, c2, col_fn1, col_fn2):
#         orig_c1 = c1
#         orig_c2 = c2

#         done1 = False
#         done2 = False

#         c1_item = c1.fetchone()
#         c2_item = c2.fetchone()

#         def safe_next(c, cur):
#             try: return c.fetchone(), False
#             except StopIteration: return cur, True

#         while not (done1 and done2):
#             if col_fn1(c1_item) == col_fn2(c2_item):
#                 if c1 == orig_c1:
#                     yield (c1_item, c2_item)
#                 else:
#                     yield (c2_item, c1_item)
#             else:
#                 c1, c2 = c2, c1
#                 col_fn1, col_fn2 = col_fn2, col_fn1
#                 done1, done2 = done2, done1
#                 c1_item, c2_item = c2_item, c1_item

#             c2_item, done2 = safe_next(c2, c2_item)
#             if done2:
#                 c1_item, done1 = safe_next(c1, c1_item)

#         raise StopIteration

#TODO the constructors on these classes are dumb
# class Join(MultiQuery):
#     cursor_cls = JoinCursor

#     def __init__(self, query1, query2, rule):
#         MultiQuery.__init__(self, query1, query2)
#         self._a = (rule[0].lookup, rule, rule[1].lookup)


##used to be in class Query
#     def __getattr__(self, attr):
#         if attr.startswith('__'):
#             raise AttributeError
#         else:
#             return QueryAttr(attr)

##user to be in class Query
#TODO can this be more efficient?
# class QueryAttr(object):
#     __slots__ = ('cols',)
#     def __init__(self, *cols):
#         self.cols = cols

#     def __eq__(self, other):
#         return (self, other)

#     def lookup(self, obj):
#         return reduce(getattr, self.cols, obj)

#     def __getattr__(self, attr):
#         return QueryAttr(*list(self.cols) + [attr])


########NEW FILE########
__FILENAME__ = userrel
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.lib.memoize import memoize, clear_memo

def UserRel(name, relation):
    all_memo_str = name + '.all_ids'
    reverse_memo_str = name + 'reverse'
    exists_name = 'is_' + name

    def userrel_exists(self, user):
        if not user:
            return False

        r = relation._fast_query([self], [user], name)
        r = r.get((self, user, name))
        if r:
            return r

    def userrel_add(self, user):
        fn = getattr(self, exists_name)
        if not fn(user):
            s = relation(self, user, name)
            s._commit()
            clear_memo(all_memo_str, self)
            clear_memo(reverse_memo_str, user)
            return s
    
    def userrel_remove(self, user):
        fn = getattr(self, exists_name)
        s = fn(user)
        if s:
            s._delete()
            clear_memo(all_memo_str, self)
            clear_memo(reverse_memo_str, user)
            return True

    @memoize(all_memo_str)
    def userrel_ids(self):
        q = relation._query(relation.c._thing1_id == self._id,
                            relation.c._name == name)
        #removed set() here, shouldn't be required
        return [r._thing2_id for r in q]

    @staticmethod
    @memoize(reverse_memo_str)
    def reverse_ids(user):
        q = relation._query(relation.c._thing2_id == user._id,
                            relation.c._name == name)
        return [r._thing1_id for r in q]

    class UR: pass

    setattr(UR, 'is_' + name, userrel_exists)
    setattr(UR, 'add_' + name, userrel_add)
    setattr(UR, 'remove_' + name, userrel_remove)
    setattr(UR, name + '_ids', userrel_ids)
    setattr(UR, 'reverse_' + name + '_ids', reverse_ids)

    return UR
        

########NEW FILE########
__FILENAME__ = emailer
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is Reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
#
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from email.MIMEText import MIMEText
from pylons.i18n import _
from pylons import c, g, request
from r2.lib.pages import PasswordReset, MeetupNotification, Share, Mail_Opt, EmailVerify, WikiSignupFail, WikiSignupFail, WikiAPIError, WikiIncompatibleName, WikiSignupNotification, WikiUserExists
from r2.lib.utils import timeago
from r2.models import passhash, Email, Default, has_opted_out
from r2.config import cache
import os, random, datetime
import smtplib, traceback, sys

def email_address(name, address):
    return '"%s" <%s>' % (name, address) if name else address
feedback = email_address('reddit feedback', g.feedback_email)

def send_mail(msg, fr, to):
    if g.debug:
        g.log.debug(msg.as_string())
        return

    session = smtplib.SMTP(g.smtp_server)
    session.sendmail(fr, to, msg.as_string())
    session.quit()

def simple_email(to, fr, subj, body):
    # FIXME: Ugly hack because the templating system has no way to
    # mark strings as safe, and insists on html-escaping templates for
    # a text/plain email.
    body = body.replace('&amp;', '&')

    def utf8(s):
        return s.encode('utf8') if isinstance(s, unicode) else s
    msg = MIMEText(utf8(body))
    msg.set_charset('utf8')
    msg['To']      = utf8(to)
    msg['From']    = utf8(fr)
    msg['Subject'] = utf8(subj)
    send_mail(msg, fr, to)

def password_email(user):
    key = passhash(random.randint(0, 1000), user.email)
    passlink = 'http://' + g.domain + '/resetpassword/' + key
    cache.set("reset_%s" %key, user._id, time=1800)
    simple_email(user.email, 'contact@lesswrong.com',
                 'lesswrong.com password reset',
                 PasswordReset(user=user, passlink=passlink).render(style='email'))

def confirmation_email(user):
    simple_email(user.email, 'contact@lesswrong.com',
                 'lesswrong.com email verification',
                 EmailVerify(user=user, link='http://'+g.domain+'/verifyemail').render(style='email'))

def wiki_failed_email(user):
    simple_email(user.email, 'contact@lesswrong.com',
                 'LessWrong Wiki sign-up failed',
                 WikiSignupFail(user=user, link='http://'+g.domain+'/prefs/wikiaccount/').render(style='email'))

def unknown_wiki_error(error):
    simple_email(g.email_to, g.error_email_from,
                 'the wiki API gave an unknown error',
                 WikiAPIError(error=error).render(style='email'))

def wiki_incompatible_name_email(user):
    simple_email(user.email, 'contact@lesswrong.com',
                 'LessWrong account name incompatible with wiki',
                 WikiIncompatibleName(user=user, link='http://'+g.wiki_host+'/mediawiki/index.php?title=Special:UserLogin&type=signup').render(style='email'))

def wiki_signup_notification_email(user):
    simple_email(user.email, 'contact@lesswrong.com',
                 'LessWrong Wiki sign-up',
                 WikiSignupNotification(link='http://'+g.wiki_host).render(style='email'))

def wiki_user_exists_email(user):
    simple_email(user.email, 'contact@lesswrong.com',
                 'LessWrong Wiki account exists',
                 WikiUserExists(user=user,
                                link_base='http://'+g.wiki_host,
                                link='http://'+g.wiki_host+'/mediawiki/index.php?title=Special:UserLogin&type=signup').render(style='email'))

def meetup_email(user, meetup):
    simple_email(user.email, 'contact@lesswrong.com',
                 'lesswrong.com meetup notification',
                 MeetupNotification(user=user, meetup=meetup).render(style='email'))


def _feedback_email(email, body, kind, name='', reply_to = ''):
    """Function for handling feedback and ad_inq emails.  Adds an
    email to the mail queue to the feedback email account."""
    Email.handler.add_to_queue(c.user if c.user_is_loggedin else None,
                               None, [feedback], name, email,
                               datetime.datetime.now(),
                               request.ip, kind, body = body,
                               reply_to = reply_to)

def feedback_email(email, body, name='', reply_to = ''):
    """Queues a feedback email to the feedback account."""
    return _feedback_email(email, body,  Email.Kind.FEEDBACK, name = name,
                           reply_to = reply_to)

def ad_inq_email(email, body, name='', reply_to = ''):
    """Queues a ad_inq email to the feedback account."""
    return _feedback_email(email, body,  Email.Kind.ADVERTISE, name = name,
                           reply_to = reply_to)


def share(link, emails, from_name = "", reply_to = "", body = ""):
    """Queues a 'share link' email."""
    now = datetime.datetime.now(g.tz)
    ival = now - timeago(g.new_link_share_delay)
    date = max(now,link._date + ival)
    Email.handler.add_to_queue(c.user, link, emails, from_name, g.share_reply,
                               date, request.ip, Email.Kind.SHARE,
                               body = body, reply_to = reply_to)

def send_queued_mail():
    """sends mail from the mail queue to smtplib for delivery.  Also,
    on successes, empties the mail queue and adds all emails to the
    sent_mail list."""
    now = datetime.datetime.now(g.tz)
    if not c.site:
        c.site = Default

    clear = False
    session = smtplib.SMTP(g.smtp_server)
    # convienence funciton for sending the mail to the singly-defined session and
    # marking the mail as read.
    def sendmail(email):
        try:
            session.sendmail(email.fr_addr, email.to_addr,
                             email.to_MIMEText().as_string())
            email.set_sent(rejected = False)
        # exception happens only for local recipient that doesn't exist
        except (smtplib.SMTPRecipientsRefused, smtplib.SMTPSenderRefused):
            # handle error and print, but don't stall the rest of the queue
            print "Handled error sending mail (traceback to follow)"
            traceback.print_exc(file = sys.stdout)
            email.set_sent(rejected = True)


    try:
        for email in Email.get_unsent(now):
            clear = True

            should_queue = email.should_queue()
            # check only on sharing that the mail is invalid
            if email.kind == Email.Kind.SHARE and should_queue:
                email.body = Share(username = email.from_name(),
                                   msg_hash = email.msg_hash,
                                   link = email.thing,
                                   body = email.body).render(style = "email")
                email.subject = _("[reddit] %(user)s has shared a link with you") % \
                                {"user": email.from_name()}
                sendmail(email)
            elif email.kind == Email.Kind.OPTOUT:
                email.body = Mail_Opt(msg_hash = email.msg_hash,
                                      leave = True).render(style = "email")
                email.subject = _("[reddit] email removal notice")
                sendmail(email)

            elif email.kind == Email.Kind.OPTIN:
                email.body = Mail_Opt(msg_hash = email.msg_hash,
                                      leave = False).render(style = "email")
                email.subject = _("[reddit] email addition notice")
                sendmail(email)

            elif email.kind in (Email.Kind.FEEDBACK, Email.Kind.ADVERTISE):
                if email.kind == Email.Kind.FEEDBACK:
                    email.subject = "[feedback] feedback from '%s'" % \
                                    email.from_name()
                else:
                    email.subject = "[ad_inq] feedback from '%s'" % \
                                    email.from_name()
                sendmail(email)
            # handle failure
            else:
                email.set_sent(rejected = True)

    finally:
        session.quit()

    # clear is true if anything was found and processed above
    if clear:
        Email.handler.clear_queue(now)



def opt_out(msg_hash):
    """Queues an opt-out email (i.e., a confirmation that the email
    address has been opted out of receiving any future mail)"""
    email, added =  Email.handler.opt_out(msg_hash)
    if email and added:
        Email.handler.add_to_queue(None, None, [email], "reddit.com",
                                   datetime.datetime.now(g.tz),
                                   '127.0.0.1', Email.Kind.OPTOUT)
    return email, added

def opt_in(msg_hash):
    """Queues an opt-in email (i.e., that the email has been removed
    from our opt out list)"""
    email, removed =  Email.handler.opt_in(msg_hash)
    if email and removed:
        Email.handler.add_to_queue(None, None, [email], "reddit.com",
                                   datetime.datetime.now(g.tz),
                                   '127.0.0.1', Email.Kind.OPTIN)
    return email, removed

########NEW FILE########
__FILENAME__ = errors
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.lib.utils import Storage
from pylons.i18n import _
from copy import copy

error_list = dict((
        ('NO_URL', _('Url required')),
        ('BAD_URL', _('You should check that url')),
        ('NO_TITLE', _('Title required')),
        ('TITLE_TOO_LONG', _('Title too long')),
        ('LOCATION_TOO_LONG', _('Location is too long')),
        ('COMMENT_TOO_LONG', _('Comment too long')),
        ('BAD_CAPTCHA', _('Incorrect, try again')),
        ('BAD_USERNAME', _('Invalid user name')),
        ('BAD_USERNAME_SHORT', _('Username is too short')),
        ('BAD_USERNAME_LONG', _('Username is too long')),
        ('BAD_USERNAME_CHARS', _('Username may not contain special characters')),
        ('USERNAME_TAKEN', _('That username is already taken')),
        ('NO_THING_ID', _('Id not specified')),
        ('NOT_AUTHOR', _("Only the author can do that")),
        ('AMOUNT_NOT_NUM', _("Not a valid award amount")),
        ('BAD_COMMENT', _('Please enter a comment')),
        ('BAD_PASSWORD', _('Invalid password')),
        ('WRONG_PASSWORD', _('Incorrect password')),
        ('BAD_PASSWORD_MATCH', _('Passwords do not match')),
        ('AMOUNT_NEGATIVE', _('Karma awards must be greater than zero')),
        ('NO_NAME', _('Please enter a name')),
        ('NO_EMAIL', _('Please enter an email address')),
        ('BAD_EMAIL', _('Invalid email address')),
        ('NO_EMAIL_FOR_USER', _('No email address for that user')),
        ('NO_MESSAGE', _('Please enter a message')),
        ('NO_AMOUNT', _('Please enter an amount')),
        ('NO_MSG_BODY', _('Please enter a message')),
        ('NO_SUBJECT', _('Please enter a subject')),
        ('USER_DOESNT_EXIST', _("That user doesn't exist")),
        ('NO_USER', _('Please enter a username')),
        ('BAD_NUMBER', _("That number isn't in the right range")),
        ('ALREADY_SUB', _("That link has already been submitted")),
        ('SUBREDDIT_EXISTS', _('That category already exists')),
        ('SUBREDDIT_NOEXIST', _('That category doesn\'t exist')),
        ('SUBREDDIT_FORBIDDEN', _("You don't have permission to submit to that category.")),
        ('BAD_SR_NAME', _('That name isn\'t going to work')),
        ('RATELIMIT', _('You are trying to submit too fast. try again in %(time)s.')),
        ('EXPIRED', _('Your session has expired')),
        ('DRACONIAN', _('You must accept the terms first')),
        ('NO_CODE', _('Please enter your confirmation code')),
        ('WRONG_CODE', _('Wrong Code, please check your email')),
        ('BANNED_IP', "IP banned"),
        ('BANNED_DOMAIN', "Domain banned"),
        ('BAD_CNAME', "that domain isn't going to work"),
        ('USED_CNAME', "that domain is already in use"),
        ('INVALID_OPTION', _('That option is not valid')),
        ('DESC_TOO_LONG', _('Description is too long')),
        ('CHEATER', 'what do you think you\'re doing there?'),
        ('BAD_EMAILS', _('The following emails are invalid: %(emails)s')),
        ('NO_EMAILS', _('Please enter at least one email address')),
        ('TOO_MANY_EMAILS', _('Please only share to %(num)s emails at a time.')),
        ('NO_LOCATION', _('You must supply a location')),
        ('NO_DATE', _('The time and date of the meetup is required')),
        ('INVALID_DATE', _('Must be a valid date and time')),
        ('NO_DESCRIPTION', _('You must supply a description')),
        ('CANNOT_DELETE', _('Cannot delete that comment')),
        ('NOT_ENOUGH_KARMA', _('You do not have enough karma')),
        ('BAD_POLL_SYNTAX', _('Error in poll syntax')),
        ('BAD_POLL_BALLOT', _('Error in poll ballot')),
        ('WIKI_DOWN', _('Connection with wiki failed, try again later')),
        ('WIKI_ACCOUNT_CREATION_FAILED', _('Wiki account creation failed. Check your email for further instructions.'))
    ))
errors = Storage([(e, e) for e in error_list.keys()])

class Error(object):
    #__slots__ = ('name', 'message')
    def __init__(self, name, i18n_message, msg_params = None):
        self.name = name
        self.i18n_message = i18n_message
        self.msg_params = msg_params or {}
        
    @property
    def message(self):
        return _(self.i18n_message) % self.msg_params

    def __iter__(self):
         #yield ('num', self.num)
        yield ('name', self.name)
        yield ('message', _(self.message))

    def __repr__(self):
        return '<Error: %s>' % self.name

class ErrorSet(object):
    def __init__(self):
        self.errors = {}

    def __contains__(self, error_name):
        return self.errors.has_key(error_name)

    def __getitem__(self, name):
        return self.errors[name]

    def __repr__(self):
        return "<ErrorSet %s>" % list(self)

    def __iter__(self):
        for x in self.errors:
            yield x
        
    def _add(self, error_name, msg, msg_params = None):
        self.errors[error_name] = Error(error_name, msg, msg_params)
        
    def add(self, error_name, msg_params = None):
        msg = error_list[error_name]
        self._add(error_name,  msg, msg_params = msg_params)

    def remove(self, error_name):
        if self.errors.has_key(error_name):
            del self.errors[error_name]

class UserRequiredException(Exception): pass

########NEW FILE########
__FILENAME__ = filters
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is Reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
#
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from pylons import c

import cgi
import urllib
import re

import lxml.html
from lxml.html import soupparser
from lxml.html.clean import Cleaner, autolink_html

from r2.lib.memoize import memoize, sha1_args

MD_START = '<div class="md">'
MD_END = '</div>'


# Cleaner is initialised with differences to the defaults
# embedded: We want to allow flash movies in posts
# style: enable removal of style
# safe_attrs_only: need to allow strange arguments to <object>
sanitizer = Cleaner(embedded=False,safe_attrs_only=False)
comment_sanitizer = Cleaner(embedded=False,style=True,safe_attrs_only=False)

def python_websafe(text):
    return text.replace('&', "&amp;").replace("<", "&lt;").replace(">", "&gt;").replace('"', "&quot;")

def python_websafe_json(text):
    return text.replace('&', "&amp;").replace("<", "&lt;").replace(">", "&gt;")

try:
    from Cfilters import uwebsafe as c_websafe, uwebsafe_json as c_websafe_json
except ImportError:
    c_websafe      = python_websafe
    c_websafe_json = python_websafe_json

# There is a C implementation of this in Cfilters, but it's out-of-date and
# currently unused.
_spaces = re.compile(r'(\s)\s+')
def spaceCompress(content):
    return _spaces.sub(r'\1', content.strip())

class _Unsafe(unicode): pass

def _force_unicode(text):
    try:
        text = unicode(text, 'utf-8', 'ignore')
    except TypeError:
        text = unicode(text)
    return text

def _force_utf8(text):
    return str(_force_unicode(text).encode('utf8'))

def _force_ascii(text):
    return _force_unicode(text).encode('ascii', 'ignore')

def unsafe(text=''):
    return _Unsafe(_force_unicode(text))

def unsafe_wrap_md(html=''):
    return unsafe(MD_START + html + MD_END)

def websafe_json(text=""):
    return c_websafe_json(_force_unicode(text))

def websafe(text=''):
    if text.__class__ == _Unsafe:
        return text
    elif text.__class__ != unicode:
        text = _force_unicode(text)
    return c_websafe(text)

from mako.filters import url_escape
def edit_comment_filter(text = ''):
    try:
        text = unicode(text, 'utf-8')
    except TypeError:
        text = unicode(text)
    return url_escape(text)

#TODO is this fast?
url_re = re.compile(r"""
    (\[[^\]]*\]:?)?           # optional leading pair of square brackets
    \s*                       # optional whitespace
    (\()?                     # optional open bracket
    (?<![<])                  # No angle around link already
    (https?://[^\s\'\"\]\)]+) # a http or https uri
    (?![>])                   # No angle around link already
    (\))?                     # optional close bracket
    """, re.VERBOSE)
jscript_url = re.compile('<a href="(?!http|ftp|mailto|/).*</a>', re.I | re.S)
href_re = re.compile('<a href="([^"]+)"', re.I | re.S)
code_re = re.compile('<code>([^<]+)</code>')
a_re    = re.compile('>([^<]+)</a>')

def wrap_urls(text):
    #wrap urls in "<>" so that markdown will handle them as urls
    matches = url_re.finditer(text)
    def check(match):
        square_brackets, open_bracket, link, close_bracket = match.groups()
        return match if link and not square_brackets else None

    matched = filter(None, [check(match) for match in matches])
    segments = []
    start = 0
    for match in matched:
        segments.extend([text[start:match.start(3)], '<', match.group(3), '>'])
        start = match.end(3)

    # Tack on any trailing bits
    segments.append(text[start:])

    return ''.join(segments)

#TODO markdown should be looked up in batch?
#@memoize('markdown')
def safemarkdown(text, div=True):
    from contrib.markdown import markdown
    if text:
        # increase escaping of &, < and > once
        text = text.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
        text = wrap_urls(text)

        try:
            text = markdown(text)
        except RuntimeError:
            text = "<p><em>Comment Broken</em></p>"
        #wipe malicious javascript
        text = jscript_url.sub('', text)
        def href_handler(m):
            x = m.group(1).replace('&amp;', '&')
            if c.cname:
                return '<a target="_top" href="%s"' % x
            else:
                return '<a href="%s"' % x
        def code_handler(m):
            l = m.group(1)
            return '<code>%s</code>' % l.replace('&amp;','&')
        #unescape double escaping in links
        def inner_a_handler(m):
            l = m.group(1)
            return '>%s</a>' % l.replace('&amp;','&')
        # remove the "&" escaping in urls
        text = href_re.sub(href_handler, text)
        text = code_re.sub(code_handler, text)
        text = a_re.sub(inner_a_handler, text)
        return MD_START + text + MD_END if div else text

def keep_space(text):
    text = websafe(text)
    for i in " \n\r\t":
        text=text.replace(i,'&#%02d;' % ord(i))
    return unsafe(text)

def unkeep_space(text):
    return text.replace('&#32;', ' ').replace('&#10;', '\n').replace('&#09;', '\t')

whitespace_re = re.compile('^\s*$')
def killhtml(html=''):
    html_doc = soupparser.fromstring(remove_control_chars(html))
    text = filter(lambda text: not whitespace_re.match(text), html_doc.itertext())
    cleaned_html = ' '.join([fragment.strip() for fragment in text])
    return cleaned_html

control_chars = re.compile('[\x00-\x08\x0b\x0c\x0e-\x1f]')   # Control characters *except* \t \r \n
def remove_control_chars(text):
    return control_chars.sub('',text)

@memoize('r2.filters.cleanhtml', hash=sha1_args, time=60 * 60 * 12) # 12 hours
def cleanhtml(html='', cleaner=None):
    html_doc = soupparser.fromstring(remove_control_chars(html))
    if not cleaner:
        cleaner = sanitizer
    cleaned_html = cleaner.clean_html(html_doc)
    return lxml.html.tostring(autolink_html(cleaned_html))

def clean_comment_html(html=''):
    return cleanhtml(html, comment_sanitizer)

block_tags = r'h1|h2|h3|h4|h5|h6|table|ol|dl|ul|menu|dir|p|pre|center|form|fieldset|select|blockquote|address|div|hr'
linebreaks_re = re.compile(r'(\n{2}|\r{2}|(?:\r\n){2}|</?(?:%s)[^>]*?>)' % block_tags)
tags_re = re.compile(r'</?(?:%s)' % block_tags)
def format_linebreaks(html=''):
    paragraphs = ['<p>%s</p>' % p if not tags_re.match(p) else p
                  for p in linebreaks_re.split(html.strip())
                  if not whitespace_re.match(p)]
    return ''.join(paragraphs)

########NEW FILE########
__FILENAME__ = find_tz
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.models.link import Link
from r2.lib.db import tdb_sql
import pytz
import sqlalchemy as sa
from r2.lib.db.operators import desc
import r2.lib.db.sorts as sorts
from datetime import datetime

def find_tz():
    q = Link._query(sort = desc('_hot'), limit = 1)
    link = list(q)[0]
    t = tdb_sql.types_id[Link._type_id].thing_table

    s = sa.select([sa.func.hot(t.c.ups, t.c.downs, t.c.date),
                   t.c.thing_id],
                  t.c.thing_id == link._id)
    db_hot = s.execute().fetchall()[0].hot.__float__()

    db_hot == round(db_hot, 7)

    for tz_name in pytz.common_timezones:
        tz = pytz.timezone(tz_name)
        sorts.epoch = datetime(1970, 1, 1, tzinfo = tz)
        
        if db_hot == link._hot:
            print tz_name

if __name__ == '__main__':
    find_tz()

########NEW FILE########
__FILENAME__ = helpers
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
"""
Helper functions

All names available in this module will be available under the Pylons h object.
"""
from webhelpers import *
from pylons.controllers.util import log
from pylons.i18n import get_lang

from r2.lib.translation import set_lang, LanguageError

########NEW FILE########
__FILENAME__ = html_source
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from HTMLParser import HTMLParser

_indent = '  '

def tagstr(tag):
    return '<span style="font-weight: bold;color:blue">%s</span>' % tag

def tagend(tag, line=0):
    if not line:
        return error(tag)
    return '<a href="#line_%d">%s</a>' % (line, tagstr(tag))

def error(strng):
    return '<span style="color:red; font-weight:bold">%s</span>' % strng

class HTMLValidationParser(HTMLParser):
    def __init__(self, *a, **kw):
        self.indent = '';
        HTMLParser.__init__(self, *a, **kw)
        self.processed_text = ''
        self.tagtracker = []
        self.error_line = 0
        self.line_number = 1

    def nextLine(self, text):
        self.processed_text += '<a id="line_%s" />' % self.line_number
        self.processed_text += text
        self.line_number += 1

    def handle_starttag(self, tag, attrs):
        self.tagtracker.append((tag.lower(), self.line_number))
        atts = ' '.join(['%s="%s"' %(x,y) for (x, y) in attrs])
        res =  "%s&lt;%s%s&gt;\n" % \
            (self.indent, tagstr(tag), atts and ' ' + atts or '')
        self.indent += _indent
        self.nextLine(res)

    def handle_endtag(self, tag):
        line = 0
        if self.tagtracker:
            if self.tagtracker[-1][0] == tag.lower():
                line = self.tagtracker[-1][1]
                self.tagtracker = self.tagtracker[:-1]
            else:
                self.error_line = self.line_number
                
        if(self.indent):
            self.indent = self.indent[:-len(_indent)]
        self.nextLine("%s&lt;/%s&gt;\n" % (self.indent, tagend(tag, line)))

    def handle_startendtag(self, tag, attrs):
        atts =  ' '.join(['%s="%s"' %(x,y) for (x, y) in attrs])
        res = "%s&lt;%s%s/&gt;\n" % \
            (self.indent, tagstr(tag), atts and ' ' + atts or '')
        self.nextLine(res)

    def handle_data(self, data):
        data2 = data = data.replace('\n', '')
        if data2.replace('\t', '').replace(' ', ''):
            self.nextLine(self.indent + data + '\n')

    def feed(self, text):
        HTMLParser.feed(self, text)
        pretext = ''
        if self.error_line:
            el = self.error_line
            if self.tagtracker:
                etag, etagl = self.tagtracker[-1]
                pretext =  '<p>Error on <a href="#line_%d">line %d</a>.  Unclosed %s tag on <a href="#line_%d">line %d</a></p>' % (el, el, etag, etagl, etagl)
            else:
                pretext =  '<p>Error on <a href="#line_%d">line %d</a>.  Extra closing tag</p>' % (el, el)
                

        return pretext + "<pre>" + self.processed_text + "</pre>"

########NEW FILE########
__FILENAME__ = importer
import sys
import os
import re
import datetime
import pytz
import yaml
import urlparse

from random import Random
from r2.models import Link,Comment,Account,Subreddit
from r2.models.account import AccountExists, register
from r2.lib.db.thing import NotFound

###########################
# Constants
###########################

MAX_RETRIES = 100

# Constants for the characters to compose a password from.
# Easilty confused characters like I and l, 0 and O are omitted
PASSWORD_NUMBERS='123456789'
PASSWORD_LOWER_CHARS='abcdefghjkmnpqrstuwxz'
PASSWORD_UPPER_CHARS='ABCDEFGHJKMNPQRSTUWXZ'
PASSWORD_OTHER_CHARS='@#$%^&*'
ALL_PASSWORD_CHARS = ''.join([PASSWORD_NUMBERS,PASSWORD_LOWER_CHARS,PASSWORD_UPPER_CHARS,PASSWORD_OTHER_CHARS])

DATE_FORMAT = '%m/%d/%Y %I:%M:%S %p'
INPUT_TIMEZONE = pytz.timezone('America/New_York')

rng = Random()
def generate_password():
    password = []
    for i in range(8):
        password.append(rng.choice(ALL_PASSWORD_CHARS))
    return ''.join(password)

class Importer(object):

    def __init__(self, url_handler=None):
        """Constructs an importer that takes a data structure based on a yaml file.

        Args:
        url_handler: A optional URL transformation function that will be
        called with urls detected in post and comment bodies.
        """

        self.url_handler = url_handler if url_handler else self._default_url_handler

        self.username_mapping = {}

    @staticmethod
    def _default_url_handler(match):
        return match.group()

    def process_comment(self, comment_data, comment, post):
        # Prepare data for import
        ip = '127.0.0.1'
        if comment_data:
            naive_date = datetime.datetime.strptime(comment_data['dateCreated'], DATE_FORMAT)
            local_date = INPUT_TIMEZONE.localize(naive_date, is_dst=False) # Pick the non daylight savings time
            utc_date = local_date.astimezone(pytz.utc)

            # Determine account to use for this comment
            account = self._get_or_create_account(comment_data['author'], comment_data['authorEmail'])

        if comment_data and not comment:
            # Create new comment
            comment, inbox_rel = Comment._new(account, post, None, comment_data['body'], ip, date=utc_date)
            comment.is_html = True
            comment.ob_imported = True
            comment._commit()
        elif comment_data and comment:
            # Overwrite existing comment
            comment.author_id = account._id
            comment.body = comment_data['body']
            comment.ip = ip
            comment._date = utc_date
            comment.is_html = True
            comment.ob_imported = True
            comment._commit()
        elif not comment_data and comment:
            # Not enough comment data being imported to overwrite all comments
            print 'WARNING: More comments in lesswrong than we are importing, ignoring additional comment in lesswrong'

    kill_tags_re = re.compile(r'</?[iub]>')
    transform_categories_re = re.compile(r'[- ]')

    def process_post(self, post_data, sr):
        # Prepare data for import
        title = self.kill_tags_re.sub('', post_data['title'])
        article = u'%s%s' % (post_data['description'],
                             Link._more_marker + post_data['mt_text_more'] if post_data['mt_text_more'] else u'')
        ip = '127.0.0.1'
        tags = [self.transform_categories_re.sub('_', tag.lower()) for tag in post_data.get('category', [])]
        naive_date = datetime.datetime.strptime(post_data['dateCreated'], DATE_FORMAT)
        local_date = INPUT_TIMEZONE.localize(naive_date, is_dst=False) # Pick the non daylight savings time
        utc_date = local_date.astimezone(pytz.utc)

        # Determine account to use for this post
        account = self._get_or_create_account(post_data['author'], post_data['authorEmail'])

        # Look for an existing post created due to a previous import
        post = self._query_post(Link.c.ob_permalink == post_data['permalink'])

        if not post:
            # Create new post
            post = Link._submit(title, article, account, sr, ip, tags, date=utc_date)
            post.blessed = True
            post.comment_sort_order = 'old'
            post.ob_permalink = post_data['permalink']
            post._commit()
        else:
            # Update existing post
            post.title = title
            post.article = article
            post.author_id = account._id
            post.sr_id = sr._id
            post.ip = ip
            post.set_tags(tags)
            post._date = utc_date
            post.blessed = True
            post.comment_sort_order = 'old'
            post._commit()

        # Process each comment for this post
        comments = self._query_comments(Comment.c.link_id == post._id, Comment.c.ob_imported == True)
        [self.process_comment(comment_data, comment, post)
         for comment_data, comment in map(None, post_data.get('comments', []), comments)]

    def substitute_ob_url(self, url):
        try:
            url = self.post_mapping[url].url
        except KeyError:
            pass
        return url

    # Borrowed from http://stackoverflow.com/questions/161738/what-is-the-best-regular-expression-to-check-if-a-string-is-a-valid-url/163684#163684
    url_re = re.compile(r"""(?:https?|ftp|file)://[-A-Z0-9+&@#/%?=~_|!:,.;]*[-A-Z0-9+&@#/%=~_|]""", re.IGNORECASE)
    def rewrite_ob_urls(self, text):
        if text:
            if isinstance(text, str):
                text = text.decode('utf-8')

            # Double decode needed to handle some wierd characters
            text = text.encode('utf-8')
            text = self.url_re.sub(lambda match: self.substitute_ob_url(match.group()), text)

        return text

    def post_process_post(self, post):
        """Perform post processsing to rewrite URLs and generate mapping
           between old and new permalinks"""
        post.article = self.rewrite_ob_urls(post.article)
        post._commit()
        
        comments = Comment._query(Comment.c.link_id == post._id, data = True)
        for comment in comments:
            comment.body = self.rewrite_ob_urls(comment.body)
            comment._commit()

    def _post_process(self, rewrite_map_file):
        def unicode_safe(text):
            if isinstance(text, unicode):
                return text.encode('utf-8')
            else:
                return text

        posts = list(Link._query(Link.c.ob_permalink != None, data = True))

        # Generate a mapping between ob permalinks and imported posts
        self.post_mapping = {}
        for post in posts:
            self.post_mapping[post.ob_permalink] = post

        # Write out the rewrite map
        for old_url, post in self.post_mapping.iteritems():
            ob_url = urlparse.urlparse(old_url)
            new_url = post.canonical_url
            try:
                rewrite_map_file.write("%s %s\n" % (unicode_safe(ob_url.path), unicode_safe(new_url)))
            except UnicodeEncodeError, uee:
                print "Unable to write to rewrite map file:"
                print unicode_safe(ob_url.path)
                print unicode_safe(new_url)

        # Update URLs in the posts and comments
        print 'Post processing imported content'
        for post in posts:
            self.post_process_post(post)

    def import_into_subreddit(self, sr, data, rewrite_map_file):
        for post_data in data:
            try:
                print post_data['title']
                self.process_post(post_data, sr)
            except Exception, e:
                print 'Unable to create post:\n%s\n%s\n%s' % (type(e), e, post_data)
                raise

        self._post_process(rewrite_map_file)

    def _query_account(self, *args):
        account = None
        kwargs = {'data': True}
        q = Account._query(*args, **kwargs)
        accounts = list(q)
        if accounts:
            account = accounts[0]
        return account

    def _query_post(self, *args):
        post = None
        kwargs = {'data': True}
        q = Link._query(*args, **kwargs)
        posts = list(q)
        if posts:
            post = posts[0]
        return post

    def _query_comments(self, *args):
        kwargs = {'data': True}
        q = Comment._query(*args, **kwargs)
        comments = list(q)
        return comments

    def _username_from_name(self, name):
        """Convert a name into a username"""
        return name.replace(' ', '_')

    def _find_account_for(self, name, email):
        """Try to find an existing account using derivations of the name"""

        try:
            # Look for an account we have cached
            account = self.username_mapping[(name, email)]
        except KeyError:
            # Look for an existing account that was created due to a previous import
            account = self._query_account(Account.c.ob_account_name == name,
                                          Account.c.email == email)
            if not account:
                # Look for an existing account based on derivations of the name
                candidates = (
                    name,
                    name.replace(' ', ''),
                    self._username_from_name(name)
                )

                account = None
                for candidate in candidates:
                    account = self._query_account(Account.c.name == candidate,
                                                  Account.c.email == email)
                    if account:
                        account.ob_account_name = name
                        account._commit()
                        break

            # Cache the result for next time
            self.username_mapping[(name, email)] = account

        if not account:
            raise NotFound

        return account

    def _get_or_create_account(self, full_name, email):
        try:
            account = self._find_account_for(full_name, email)
        except NotFound:
            retry = 2 # First retry will by name2
            name = self._username_from_name(full_name)
            username = name
            while True:
                # Create a new account
                try:
                    account = register(username, generate_password(), email)
                    account.ob_account_name = full_name
                    account._commit()
                except AccountExists:
                    # This username is taken, generate another, but first limit the retries
                    if retry > MAX_RETRIES:
                        raise StandardError("Unable to create account for '%s' after %d attempts" % (full_name, retry - 1))
                else:
                    # update cache with the successful account
                    self.username_mapping[(full_name, email)] = account
                    break
                username = "%s%d" % (name, retry)
                retry += 1

        return account


########NEW FILE########
__FILENAME__ = jsonresponse
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.lib.utils import tup
from r2.lib.captcha import get_iden
from r2.lib.wrapped import Wrapped
from r2.lib.filters import websafe_json
from r2.lib.template_helpers import replace_render
from r2.lib.jsontemplates import get_api_subtype
from r2.lib.base import BaseController
from r2.lib.errors import Error, error_list
import simplejson

def json_respond(x):
    from pylons import c
    if get_api_subtype():
        res = JsonResponse()
        res.object = tup(x)
        res = dict(res)
    else:
        res = x or ''
    return websafe_json(simplejson.dumps(res))


class JsonListingStub(object):
    """used in JsonResponse._thing to set default listing behavior on
    things that are pre-wrapped"""
    _js_cls = "Listing"

class JsonResponse():
    # handled entried in the response object
    __slots__ = ['update', 'blur', 'focus', 'object', 'hide', 'show',
                 'captcha', 'success', 'call']

    def __init__(self):
        self.update = []
        self.hide = []
        self.show = []
        self.focus = None
        self.blur = None
        self.object = {}
        self.captcha = None
        self.error = None
        self.success = None
        self.redirect = None
        self.call = []

    def _call(self, fn):
        self.call.append(fn)

    def _success(self):
        self.success = 1

    def _hide(self, name):
        self.hide.append(dict(name=name))

    def _show(self, name):
        self.show.append(dict(name=name))


    def _focus(self, f): 
        self.focus = f

    def _blur(self, f): 
        self.blur = f

    def _redirect(self, red):
        from pylons import c, request
        if c.cname:
            red = BaseController.format_output_url(red, subreddit = c.site,
                                                   require_frame = False)
        self.redirect = red

    def _update(self, name, **kw):
        k = kw.copy()
        k['id'] = name
        self.update.append(k)

    def _clear_error(self, error_name, err_on_thing = ''):
        errid = error_name + (('_' + err_on_thing) if err_on_thing else '')
        self._update(errid, innerHTML='')
        self._hide(errid)
        if self.error and self.error.name == error_name:
            self.error_thing_id = ''
            self.error = None

    def _set_error_obj(self, error, err_on_thing = ''):
        if not self.error:
            self.error = error
            self.error_thing_id = err_on_thing

    def _set_error(self, error_name, err_on_thing = '', msg = None):
        from pylons import c
        if not self.error:
            c.errors.add(error_name)
            msg = msg or error_list[error_name]
            obj = Error(error_name, msg)
            self._set_error_obj(obj, err_on_thing)

    def _chk_error(self, error_name, err_on_thing = ''):
        from pylons import c
        if error_name in c.errors:
            error = c.errors[error_name]
            self._set_error_obj(error, err_on_thing)
            return True
        else:
            self._clear_error(error_name, err_on_thing)
            return False

    def _chk_errors(self, errors, err_on_thing = ''):
        if errors:
           return reduce(lambda x, y: x or y,
                          [self._chk_error(e, err_on_thing = err_on_thing) for e in errors])
        return False

    def _chk_captcha(self, err, err_on_thing = ''):
        if self._chk_error(err, err_on_thing):
            self.captcha = {'iden' : get_iden(), 'refresh' : True, 'id': err_on_thing}
            self._focus('captcha')
            return True
        return False

    @property
    def response(self): 
        res = {}
        for k in self.__slots__:
            v = getattr(self, k)
            if v: res[k] = v
        return res

    def _thing(self, thing, action = None):
        d = replace_render(JsonListingStub(), thing)
        if action:
            d['action'] = action
        return d

    def _send_things(self, things, action=None):
        from r2.models import IDBuilder
        things = tup(things)
        if not all(isinstance(t, Wrapped) for t in things):
            b = IDBuilder([t._fullname for t in things])
            things = b.get_items()[0]
        self.object = [self._thing(thing, action=action) for thing in things]

    def __iter__(self):
        if self.error:
            e = dict(self.error)
            if self.error_thing_id:
                e['id'] = self.error_thing_id
            yield 'error', e
        if self.response:
            yield 'response', self.response
        if self.redirect:
            yield 'redirect', self.redirect
        
def Json(func):
    def _Json(self, *a, **kw):
        from pylons import c
        from jsontemplates import api_type
        c.render_style = api_type('html')
        c.response_content_type = 'application/json; charset=UTF-8'
        res = JsonResponse()
        val = func(self, res, *a, **kw)
        if val: return val
        return self.response_func(**dict(res))
    return _Json


########NEW FILE########
__FILENAME__ = jsontemplates
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from utils import to36, tup, iters
from wrapped import Wrapped
from mako.template import Template

def api_type(subtype = ''):
    return 'api-' + subtype if subtype else 'api'

def is_api(subtype = ''):
    from pylons import c
    return c.render_style and c.render_style.startswith(api_type(subtype))
    
def get_api_subtype():
    from pylons import c
    if is_api() and c.render_style.startswith('api-'):
        return c.render_style[4:]

def make_typename(typ):
    return 't%s' % to36(typ._type_id)

def make_fullname(typ, _id):
    return '%s_%s' % (make_typename(typ), to36(_id))

def mass_part_render(thing, **kw):
    from r2.lib.filters import spaceCompress
    return dict([(k, spaceCompress(thing.part_render(v)).strip(' ')) \
                 for k, v in kw.iteritems()])

class JsonTemplate(Template):
    def __init__(self): pass

    def render(self, thing = None, *a, **kw):
        return {}

class ThingJsonTemplate(JsonTemplate):
    __data_attrs__ = dict()
    
    def points(self, wrapped):
        scores = wrapped.score_triplet(wrapped.likes)
        return map(wrapped.score_fmt, scores)
        
    
    def kind(self, wrapped):
        _thing = wrapped.lookups[0] if isinstance(wrapped, Wrapped) else wrapped
        return make_typename(_thing.__class__)

    def rendered_data(self, thing):
        from r2.lib.filters import spaceCompress
        from r2.lib.template_helpers import replace_render
        from pylons import c
        listing = thing.listing if hasattr(thing, "listing") else None
        return dict(id = thing._fullname,
                    vl = self.points(thing),
                    content = spaceCompress(replace_render(listing, thing,
                                                           style=get_api_subtype())))

    def raw_data(self, thing):
        def strip_data(x):
            if isinstance(x, dict):
                return dict((k, strip_data(v)) for k, v in x.iteritems())
            elif isinstance(x, iters):
                return [strip_data(y) for y in x]
            elif isinstance(x, Wrapped):
                return x.render()
            else:
                return x
        
        return dict((k, strip_data(self.thing_attr(thing, v)))
                    for k, v in self.__data_attrs__.iteritems())
            
    def thing_attr(self, thing, attr):
        import time
        if attr == "author":
            return thing.author.name
        elif attr == "created":
            return time.mktime(thing._date.timetuple())
        return getattr(thing, attr) if hasattr(thing, attr) else None

    def data(self, thing):
        from pylons import c
        if get_api_subtype():
            return self.rendered_data(thing)
        else:
            return self.raw_data(thing)
        
    def render(self, thing = None, action = None, *a, **kw):
        return dict(kind = self.kind(thing), data = self.data(thing))
        
class SubredditJsonTemplate(ThingJsonTemplate):
    __data_attrs__ = dict(id           = "_id36",
                          name         = "_fullname",
                          subscribers  = "score",
                          title        = "title",
                          url          = "path",
                          description  = "description",
                          created      = "created")

class LinkJsonTemplate(ThingJsonTemplate):
    __data_attrs__ = dict(id           = "_id36",
                          name         = "_fullname",
                          ups          = "upvotes",
                          downs        = "downvotes",
                          score        = "score",
                          saved        = "saved",
                          clicked      = "clicked",
                          hidden       = "hidden",
                          likes        = "likes",
                          domain       = "domain",
                          title        = "title",
                          url          = "url",
                          author       = "author", 
                          num_comments = "num_comments",
                          created      = "created",
                          subreddit    = "subreddit",
                          subreddit_id = "subreddit_id")

    def thing_attr(self, thing, attr):
        if attr == 'subreddit':
            return thing.subreddit.name
        elif attr == 'subreddit_id':
            return thing.subreddit._fullname
        return ThingJsonTemplate.thing_attr(self, thing, attr)
                          
    def rendered_data(self, thing):
        d = ThingJsonTemplate.rendered_data(self, thing)
        d['sr'] = thing.subreddit._fullname
        return d



class CommentJsonTemplate(ThingJsonTemplate):
    __data_attrs__ = dict(id           = "_id36",
                          name         = "_fullname",
                          ups          = "upvotes",
                          downs        = "downvotes",
                          replies      = "child",
                          body         = "body",
                          likes        = "likes",
                          author       = "author", 
                          created      = "created",
                          link_id      = "link_id",
                          parent_id    = "parent_id",
                          )

    def thing_attr(self, thing, attr):
        from r2.models import Comment, Link
        if attr == 'link_id':
            return make_fullname(Link, thing.link_id)
        elif attr == "parent_id":
            try:
                return make_fullname(Comment, thing.parent_id)
            except AttributeError:
                return make_fullname(Link, thing.link_id)
        return ThingJsonTemplate.thing_attr(self, thing, attr)

    def kind(self, wrapped):
        from r2.models import Comment
        return make_typename(Comment)

    def rendered_data(self, wrapped):
        from r2.models import Comment, Link
        try:
            parent_id = wrapped.parent_id
        except AttributeError:
            parent_id = make_fullname(Link, wrapped.link_id)
        else:
            parent_id = make_fullname(Comment, parent_id)
        d = ThingJsonTemplate.rendered_data(self, wrapped)
        d.update(mass_part_render(wrapped, contentHTML = 'commentBody',
                                  contentTxt = 'commentText'))
        d['parent'] = parent_id
        d['link'] = make_fullname(Link, wrapped.link_id)
        return d

class MoreCommentJsonTemplate(CommentJsonTemplate):
    __data_attrs__ = dict(id           = "_id36",
                          name         = "_fullname")
    def points(self, wrapped):
        return []

    def kind(self, wrapped):
        return "more"

class MessageJsonTemplate(ThingJsonTemplate):
    __data_attrs__ = dict(id           = "_id36",
                          name         = "_fullname",
                          new          = "new",
                          subject      = "subject",
                          body         = "body",
                          author       = "author",
                          dest         = "dest",
                          created      = "created")

    def thing_attr(self, thing, attr):
        if attr == "dest":
            return thing.to.name
        return ThingJsonTemplate.thing_attr(self, thing, attr)

    def rendered_data(self, wrapped):
        from r2.models import Message
        try:
            parent_id = wrapped.parent_id
        except AttributeError:
            parent_id = None
        else:
            parent_id = make_fullname(Message, parent_id)
        d = ThingJsonTemplate.rendered_data(self, wrapped)
        d['parent'] = parent_id
        return d


class RedditJsonTemplate(JsonTemplate):
    def render(self, thing = None, *a, **kw):
        return thing.content().render() if thing else {}

class PanestackJsonTemplate(JsonTemplate):
    def render(self, thing = None, *a, **kw):
        res = [t.render() for t in thing.stack] if thing else []
        res = [x for x in res if x]
        if not res:
            return {}
        return res if len(res) > 1 else res[0] 

class NullJsonTemplate(JsonTemplate):
    def render(self, thing = None, *a, **kw):
        return None

class ListingJsonTemplate(ThingJsonTemplate):
    __data_attrs__ = dict(children = "things")
    
    def points(self, w):
        return []

    def rendered_data(self, thing):
        from r2.lib.filters import spaceCompress
        from r2.lib.template_helpers import replace_render

        res = []
        for a in thing.things:
            a.listing = thing
            r = replace_render(thing, a, style = 'api')
            if isinstance(r, str):
                r = spaceCompress(r)
            res.append(r)
        return res
    
    def kind(self, wrapped):
        return "Listing"

    def render(self, *a, **kw):
        res = ThingJsonTemplate.render(self, *a, **kw)
        return res

class OrganicListingJsonTemplate(ListingJsonTemplate):
    def kind(self, wrapped):
        return "OrganicListing"

########NEW FILE########
__FILENAME__ = lock
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################

from __future__ import with_statement
from threading import Lock
from time import sleep
from datetime import datetime
import sys
import threading

from pylons import c

class TimeoutExpired(Exception): pass

_LOG_LOCK = Lock()

class MemcacheLock(object):
    """A simple global lock based on the memcache 'add' command. We
    attempt to grab a lock by 'adding' the lock name. If the response
    is True, we have the lock. If it's False, someone else has it."""

    def __init__(self, key, cache, time = 30, timeout = 30):
        self.key = key
        self.cache = cache
        self.time = time
        self.timeout = timeout
        self.have_lock = False
        self.log('__init__')

    def __delitem__(self, key):
        self.log('__del__')

    def __enter__(self):
        self.acquire()
        return self

    def __exit__(self, type, value, tb):
        self.release()

    def log(self, msg, *args):
        return
        with _LOG_LOCK:
            print >>sys.stderr, datetime.utcnow().isoformat(' '), \
                '[MemcacheLock tid={0!r} id={1!r} key={2!r}]'.format(
                    threading.currentThread().ident, id(self), self.key), \
                msg.format(*args)
            sys.stderr.flush()

    def acquire(self):
        """
        Repeatedly try to acquire the lock, for `self.timeout` seconds, before
        giving up and raising an exception.
        """
        self.log('acquire enter')

        start = datetime.now()

        # try and fetch the lock, looping until it's available
        while not self.try_acquire():
            if (datetime.now() - start).seconds > self.timeout:
                raise TimeoutExpired
            sleep(0.1)

        self.log('acquire exit')

    def try_acquire(self):
        """
        Make one attempt to acquire the lock, and return immediately. Return
        `True` if we hold the lock upon returning from this method, and `False`
        if it's currently held elsewhere.
        """
        if not c.locks:
            c.locks = {}

        # if this thread already has this lock, move on
        if c.locks.get(self.key):
            return True

        # memcached will return true if the key was added, and false if it
        # already existed
        if not self.cache.add(self.key, 1, time = self.time):
            return False

        # tell this thread we have this lock so we can avoid deadlocks
        # of requests for the same lock in the same thread
        c.locks[self.key] = True
        self.have_lock = True
        return True

    def release(self):
        self.log('release enter')

        # only release the lock if we gained it in the first place
        if self.have_lock:
            self.cache.delete(self.key)
            del c.locks[self.key]

        self.log('release exit')


def make_lock_factory(cache):
    def factory(key):
        return MemcacheLock(key, cache)
    return factory

########NEW FILE########
__FILENAME__ = logger
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from __future__ import with_statement
import cPickle as pickle
import os, shutil, time
from utils import Storage
from datetime import datetime, timedelta


class LoggedSlots(object):

    def __init__(self, logfile, **kw):
        for k, v in kw.iteritems():
            super(LoggedSlots, self).__setattr__(k, v)
        self.__logfile = logfile
        self.load_slots()
        
    def __setattr__(self, k, v):
        super(LoggedSlots, self).__setattr__(k, v)
        if k in self.__slots__:
            self.dump_slots()
        
    def load_slots(self):
        d = self._get_slots(self.__logfile)
        for k, v in d.iteritems():
            super(LoggedSlots, self).__setattr__(k, v)

    def dump_slots(self):
        if self.__logfile:
            with WithWriteLock(self.__logfile) as handle:
                d = {}
                for s in self.__slots__:
                    try:
                        d[s] = getattr(self, s)
                    except AttributeError:
                        continue
                pickle.dump(d, handle)

    @classmethod
    def _get_slots(self, file):
        if os.path.exists(file):
            with open(file) as handle:
                return Storage(pickle.load(handle))
        return Storage()
            
        

class WriteLockExistsException(): pass

class WithWriteLock():
    def __init__(self, file_name, mode = 'w', force = False, age = 60):
        self.fname = file_name
        self.lock_file = file_name + ".write_lock"
        self.time = datetime.now()
        self.handle = None
        self.created = True

        if self.exists():
            if force:
                self.destroy()
            elif not self.try_expire(age):
                raise WriteLockExistsException

        # back up the file to be written to
        if os.path.exists(self.fname):
            shutil.copyfile(self.fname, self.backup_file)
        # write out a lock file
        with open(self.lock_file, 'w') as handle:
            pickle.dump(self.time, handle)
        # lastly, open the file!
        self.handle = open(file_name, mode)

    def write(self, *a, **kw):
        self.handle.write(*a, **kw)
            

    @property
    def backup_file(self):
        return "%s-%s.bak" % (self.fname,
                              time.mktime(self.time.timetuple()))
        
    def exists(self):
        return os.path.exists(self.lock_file)

    def try_expire(self, age):
        '''destroys an existing lock file if it is more than age seconds old'''
        with open(self.lock_file, 'r') as handle:
            time = pickle.load(handle)
        if self.time - time > timedelta(0, age):
            self.destroy()
            return True
        return False

    def destroy(self):
        # close any open handles
        if self.handle:
            self.handle.close()
            self.handle = None

        # wipe the lock file and the back-up
        if self.created:
            self.created = False
            if self.exists():
                os.unlink(self.lock_file)
            if os.path.exists(self.backup_file):
                os.unlink(self.backup_file)

    def close(self):
        self.destroy()

    def rollback(self):
        # close any open handles
        if self.handle:
            self.handle.close()
            self.handle = None

        if self.created:
            # clobber any changes to the file with our archive
            if os.path.exists(self.backup_file):
                shutil.copyfile(self.backup_file, self.fname)

        #destroy as usual
        self.destroy()
                
    def __enter__(self):
        return self
    

    def __exit__(self, type, value, tb):
        if tb is None:
            self.close()
        else:
            self.rollback()


########NEW FILE########
__FILENAME__ = maintenance
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Less Wrong.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is TrikeApps
# 
# All portions of the code written by TrikeApps are Copyright (c) 2013
# TrikeApps Pty Ltd. All Rights Reserved.
################################################################################

"""Routines for displaying maintenance messages"""

import pytz
import calendar
from datetime import datetime
import r2.lib.utils as utils
from pylons import g

MAINTENANCE_KEY = 'maintenance_scheduled_at'

def active():
  sched = scheduled_at()
  return sched and (datetime.now(pytz.utc) >= sched)

def scheduled():
  return scheduled_at() is not None

def scheduled_at():
  scheduled_timestamp = g.permacache.get(MAINTENANCE_KEY)
  if scheduled_timestamp is not None:
    try:
        return datetime.fromtimestamp(scheduled_timestamp, pytz.utc)
    except ValueError:
        return None

  return None

def schedule_at(date_time):
  timestamp = calendar.timegm(date_time.utctimetuple())
  g.permacache.set(MAINTENANCE_KEY, timestamp)

def complete():
  g.permacache.delete(MAINTENANCE_KEY)

def timeuntil():
  time = scheduled_at()
  return utils.timeuntil(time, resultion = 2) if time is not None else ""

def humantime():
  time = scheduled_at()
  return time.strftime('%H:%M on %d %b %Y %Z') if time is not None else ""

########NEW FILE########
__FILENAME__ = db_manager
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
import sqlalchemy as sa

def get_engine(name, db_host='', db_user='', db_pass='', pool_size = 1, max_overflow = 9):
    host = db_host if db_host else '' 
    if db_user:
        if db_pass:
            host = "%s:%s@%s" % (db_user, db_pass, db_host)
        else:
            host = "%s@%s" % (db_user, db_host)
    return sa.create_engine('postgres://%s/%s' % (host, name),
                            strategy='threadlocal',
                            pool_size = pool_size,
                            max_overflow = max_overflow)

class db_manager:
    def __init__(self):
        self.type_db = None
        self.relation_type_db = None
        self.thing_dbs = {}
        self.relation_dbs = {}

        self.extra_data = {}
        self.extra_thing1 = {}
        self.extra_thing2 = {}

    def thing(self, name, thing_db, data_db, need_extra = False):
        self.thing_dbs[name] = (thing_db, data_db)
        if need_extra:
            self.extra_data[data_db] = True

    def relation(self, name, type1, type2, relation_db,
                 need_extra1 = False, need_extra2 = False):
        self.relation_dbs[name] = (type1, type2, relation_db)
        if need_extra1:
            self.extra_thing1[relation_db] = True
        if need_extra2:
            self.extra_thing2[relation_db] = True

    def things(self):
        return [(name, d[0], d[1]) for name, d in self.thing_dbs.items()]

    def relations(self):
        return [(name, d[0], d[1], d[2])
                for name, d in self.relation_dbs.items()]

########NEW FILE########
__FILENAME__ = tp_manager
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
import pylons
from mako.template import Template as mTemplate
from mako.exceptions import TemplateLookupException
from r2.lib.filters import websafe, unsafe

from r2.lib.utils import Storage

import inspect, re, os

class tp_manager:
    def __init__(self, engine = 'mako', template_cls = mTemplate):
        self.templates = {}
        self.engine = engine
        self.Template = template_cls

    def add(self, name, style, file = None):
        key = (name.lower(), style.lower())
        if file is None:
            file = "/%s.%s" % (name, style)
        elif not file.startswith('/'):
            file = '/' + file
        self.templates[key] = file

    def add_handler(self, name, style, handler):
        key = (name.lower(), style.lower())
        self.templates[key] = handler

    def get(self, thing, style, cache = True):
        if not isinstance(thing, type(object)):
            thing = thing.__class__

        style = style.lower()
        top_key = (thing.__name__.lower(), style)

        template = None
        for cls in inspect.getmro(thing):
            name = cls.__name__.lower()
            key = (name, style)
            if not self.templates.has_key(key):
                self.add(name, style)
            if isinstance(self.templates[key], self.Template):
                template = self.templates[key]
            else:
                try:
                    _loader = pylons.buffet.engines[self.engine]['engine']
                    template = _loader.load_template(self.templates[key])
                    if cache:
                        self.templates[key] = template
                        # cache also for the base class so
                        # introspection is not required on subsequent passes
                        if key != top_key:
                            self.templates[top_key] = template
                except TemplateLookupException:
                    continue
            break

        if not template or not isinstance(template, self.Template):
            raise AttributeError, ("template doesn't exist for %s" % str(top_key))
        return template

########NEW FILE########
__FILENAME__ = media
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################

from pylons import g, config

from r2.models.link import Link
from r2.lib.workqueue import WorkQueue
from r2.lib import s3cp
from r2.lib.utils import timeago, fetch_things2
from r2.lib.db.operators import desc
from r2.lib.scraper import make_scraper, str_to_image, image_to_str, prepare_image

import tempfile
from Queue import Queue

s3_thumbnail_bucket = g.s3_thumb_bucket
media_period = g.media_period
threads = 20
log = g.log

def thumbnail_url(link):
    """Given a link, returns the url for its thumbnail based on its fullname"""
    return 'http:/%s%s.png' % (s3_thumbnail_bucket, link._fullname)

def upload_thumb(link, image):
    """Given a link and an image, uploads the image to s3 into an image
    based on the link's fullname"""
    f = tempfile.NamedTemporaryFile(suffix = '.png')
    image.save(f)

    resource = s3_thumbnail_bucket + link._fullname + '.png'
    log.debug('uploading to s3: %s' % link._fullname)
    s3cp.send_file(f.name, resource, 'image/png', 'public-read', None, False)
    log.debug('thumbnail %s: %s' % (link._fullname, thumbnail_url(link)))

def make_link_info_job(results, link, useragent):
    """Returns a unit of work to send to a work queue that downloads a
    link's thumbnail and media object. Places the result in the results
    dict"""
    def job():
        try:
            scraper = make_scraper(link.url)

            thumbnail = scraper.thumbnail()
            media_object = scraper.media_object()

            if thumbnail:
                upload_thumb(link, thumbnail)

            results[link] = (thumbnail, media_object)
        except:
            log.warning('error fetching %s %s' % (link._fullname, link.url))
            raise

    return job

def update_link(link, thumbnail, media_object):
    """Sets the link's has_thumbnail and media_object attributes iin the
    database."""
    if thumbnail:
        link.has_thumbnail = True

    if media_object:
        link.media_object = media_object

    link._commit()

def process_new_links(period = media_period, force = False):
    """Fetches links from the last period and sets their media
    properities. If force is True, it will fetch properities for links
    even if the properties already exist"""
    links = Link._query(Link.c._date > timeago(period), sort = desc('_date'),
                        data = True)
    results = {}
    jobs = []
    for link in fetch_things2(links):
        if link.is_self or link.promoted:
            continue
        elif not force and (link.has_thumbnail or link.media_object):
            continue

        jobs.append(make_link_info_job(results, link, g.useragent))

    #send links to a queue
    wq = WorkQueue(jobs, num_workers = 20, timeout = 30)
    wq.start()
    wq.jobs.join()

    #when the queue is finished, do the db writes in this thread
    for link, info in results.items():
        update_link(link, info[0], info[1])

def set_media(link):
    """Sets the media properties for a single link."""
    results = {}
    make_link_info_job(results, link, g.useragent)()
    update_link(link, *results[link])

def force_thumbnail(link, image_data):
    image = str_to_image(image_data)
    image = prepare_image(image)
    upload_thumb(link, image)
    update_link(link, thumbnail = True, media_object = None)
    

########NEW FILE########
__FILENAME__ = memoize
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is Reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
#
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################

from hashlib import sha1

class NoneResult(object): pass

def sha1_args(*args, **kwargs):
    '''Stringify args and kwargs, concatenate and sha1 the result.'''
    return sha1(str(args) + str(kwargs)).hexdigest()

def memoize(iden, time = 0, hash=None):
    def default_hash(*args, **kwargs):
        '''Not much of a hash, but good enough for small args.'''
        return str(args) + str(kwargs)

    if hash is None: hash = default_hash

    def memoize_fn(fn):
        from r2.lib.memoize import NoneResult
        def new_fn(*a, **kw):
            from r2.config import cache

            key = iden + hash(*a, **kw)
            #print 'CHECKING', key
            res = cache.get(key)
            if res is None:
                res = fn(*a, **kw)
                if res is None:
                    res = NoneResult
                cache.set(key, res, time = time)
            if res == NoneResult:
                res = None
            return res
        return new_fn
    return memoize_fn

def clear_memo(iden, *a, **kw):
    from r2.config import cache
    key = iden + str(a) + str(kw)
    #print 'CLEARING', key
    cache.delete(key)

@memoize('test')
def test(x, y):
    import time
    time.sleep(1)
    if x + y == 10:
        return None
    else:
        return x + y

########NEW FILE########
__FILENAME__ = menus
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is Reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
#
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from wrapped import Wrapped
from pylons import c, request, g
from utils import  query_string, timeago
from strings import StringHandler, plurals
from r2.lib.db import operators
from r2.lib.filters import _force_unicode
from pylons.i18n import _
#from r2.config import cache


class MenuHandler(StringHandler):
    """Bastard child of StringHandler and plurals.  Menus are
    typically a single word (and in some cases, a single plural word
    like 'moderators' or 'contributors' so this class first checks its
    own dictionary of string translations before falling back on the
    plurals list."""
    def __getattr__(self, attr):
        try:
            return StringHandler.__getattr__(self, attr)
        except KeyError:
            return getattr(plurals, attr)

# selected menu styles, primarily used on the main nav bar
menu_selected=StringHandler(hot          = _("Popular"),
                            new          = _("What's new"),
                            top          = _("Top scoring"),
                            controversial= _("Most controversial"),
                            saved        = _("Saved"),
                            recommended  = _("Recommended"),
                            promote      = _('Promote'),
                            )

# translation strings for every menu on the site
menu =   MenuHandler(hot          = _('Popular'),
                     new          = _('New'),
                     old          = _('Old'),
                     ups          = _('Ups'),
                     downs        = _('Downs'),
                     top          = _('Top'),
                     more         = _('More'),
                     relevance    = _('Relevance'),
                     controversial  = _('Controversial'),
                     confidence   = _('Best'),
                     interestingness = ('Leading'),
                     saved        = _('Saved'),
                     recommended  = _('Recommended'),
                     rising       = _('Rising'),
                     admin        = _('Admin'),
                     drafts       = _('Drafts'),
                     blessed      = _('Promoted'),
                     comments     = _('Comments'),
                     posts        = _('Posts'),
                     topcomments     = _('Top Comments'),
                     newcomments     = _('New Comments'),
                     leadingsubscribed = _('Leading Subscribed'),
                     leadingcomments = _('Leading Comments'),
                     leadingposts = _('Leading Posts'),

                     # time sort words
                     hour         = _('This hour'),
                     day          = _('Today'),
                     week         = _('This week'),
                     month        = _('This month'),
                     quarter      = _('Last three months'),
                     year         = _('This year'),
                     all          = _('All time'),
                     last         = _('Since last visit'),

                     # "kind" words
                     spam         = _("Spam"),
                     autobanned   = _("Autobanned"),

                     # reddit header strings
                     adminon      = _("Turn admin on"),
                     adminoff     = _("Turn admin off"),
                     prefs        = _("Preferences"),
                     stats        = _("Stats"),
                     submit       = _("Create new article"),
                     meetupsnew   = _("Add new meetup"),
                     help         = _("Help"),
                     blog         = _("Blog"),
                     logout       = _("Log out"),

                     #reddit footer strings
                     feedback     = _("Feedback"),
                     bookmarklets = _("Bookmarklets"),
                     socialite    = _("Socialite"),
                     buttons      = _("Buttons"),
                     widget       = _("Widget"),
                     code         = _("Code"),
                     mobile       = _("Mobile"),
                     store        = _("Store"),
                     ad_inq       = _("Advertise"),

                     #preferences
                     options      = _('Options'),
                     friends      = _("Friends"),
                     update       = _("Password/email"),
                     delete       = _("Delete"),
                     wikiaccount  = _("Wiki Account"),

                     # messages
                     compose      = _("Compose"),
                     inbox        = _("Inbox"),
                     sent         = _("Sent"),

                     # comments
                     related      = _("Related"),
                     details      = _("Details"),

                     # reddits
                     main         = _("Main"),
                     discussion   = _("Discussion"),
                     wiki         = _("Wiki"),
                     sequences    = _("Sequences"),
                     about        = _("About"),
                     edit         = _("Edit"),
                     banned       = _("Banned"),
                     banusers     = _("Ban users"),

                     popular      = _("Popular"),
                     create       = _("Create"),
                     mine         = _("My reddits"),

                     i18n         = _("Translate site"),
                     promoted     = _("Promoted"),
                     reporters    = _("Reporters"),
                     reports      = _("Reports"),
                     reportedauth = _("Reported authors"),
                     info         = _("Info"),
                     share        = _("Share"),

                     overview     = _("Overview"),
                     submitted    = _("Submitted"),
                     liked        = _("Liked"),
                     disliked     = _("Disliked"),
                     hidden       = _("Hidden"),
                     deleted      = _("Deleted"),
                     reported     = _("Reported"),

                     promote      = _('Promote'),
                     new_promo    = _('New promoted link'),
                     current_promos = _('Promoted links'),
                     )

class Styled(Wrapped):
    """Rather than creating a separate template for every possible
    menu/button style we might want to use, this class overrides the
    render function to render only the <%def> in the template whose
    name matches 'style'.

    Additionally, when rendering, the '_id' and 'css_class' attributes
    are intended to be used in the outermost container's id and class
    tag.
    """
    def __init__(self, style, _id = '', css_class = '', **kw):
        self._id = _id
        self.css_class = css_class
        self.style = style
        Wrapped.__init__(self, **kw)

    def render(self, **kw):
        """Using the canonical template file, only renders the <%def>
        in the template whose name is given by self.style"""
        style = kw.get('style', c.render_style or 'html')
        return Wrapped.part_render(self, self.style, style = style, **kw)



def menu_style(type):
    """Simple manager function for the styled menus.  Returns a
    (style, css_class) pair given a 'type', defaulting to style =
    'dropdown' with no css_class."""
    default = ('select', '')
    d = dict(heavydrop = ('dropdown', 'heavydrop'),
             lightdrop = ('dropdown', 'lightdrop'),
             tabdrop = ('dropdown', 'tabdrop'),
             srdrop = ('dropdown', 'srdrop'),
             flatlist =  ('flatlist', ''),
             tabmenu = ('tabmenu', ''),
             buttons = ('userlinks', ''),
             select  = ('select', ''),
             navlist  = ('navlist', ''),
             dropdown2 = ('dropdown2', ''))
    return d.get(type, default)



class NavMenu(Styled):
    """generates a navigation menu.  The intention here is that the
    'style' parameter sets what template/layout to use to differentiate, say,
    a dropdown from a flatlist, while the optional _class, and _id attributes
    can be used to set individualized CSS."""

    def __init__(self, options, default = None, title = '', type = "dropdown",
                 base_path = '', separator = '|', **kw):
        self.options = options
        self.base_path = base_path
        kw['style'], kw['css_class'] = menu_style(type)

        #used by flatlist to delimit menu items
        self.separator = separator

        # since the menu contains the path info, it's buttons need a
        # configuration pass to get them pointing to the proper urls
        for opt in self.options:
            opt.build(self.base_path)

        # selected holds the currently selected button defined as the
        # one whose path most specifically matches the current URL
        # (possibly None)
        self.default = default
        self.selected = self.find_selected()
        self.enabled = True

        Styled.__init__(self, title = title, **kw)

    def find_selected(self):
        maybe_selected = [o for o in self.options if o.is_selected()]
        if maybe_selected:
            # pick the button with the most restrictive pathing
            maybe_selected.sort(lambda x, y:
                                len(y.bare_path) - len(x.bare_path))
            return maybe_selected[0]
        elif self.default:
            #lookup the menu with the 'dest' that matches 'default'
            for opt in self.options:
                if opt.dest == self.default:
                    return opt

    def __repr__(self):
        return "<NavMenu>"

    def __iter__(self):
        for opt in self.options:
            yield opt

class NavButton(Styled):
    """Smallest unit of site navigation.  A button once constructed
    must also have its build() method called with the current path to
    set self.path.  This step is done automatically if the button is
    passed to a NavMenu instance upon its construction."""
    def __init__(self, title, dest, sr_path = True,
                 nocname=False, opt = '', aliases = [],
                 target = "", style = "plain", **kw):

        # keep original dest to check against c.location when rendering
        self.aliases = set(a.rstrip('/') for a in aliases)
        self.aliases.add(dest.rstrip('/'))
        self.dest = dest

        Styled.__init__(self, style = style, sr_path = sr_path,
                        nocname = nocname, target = target,
                        title = title, opt = opt, **kw)

    def build(self, base_path = ''):
        '''Generates the href of the button based on the base_path provided.'''
        if self.style == "external":
            self.path = self.dest
            self.bare_path = self.dest
            return

        # append to the path or update the get params dependent on presence
        # of opt
        if self.opt:
            p = request.get.copy()
            p[self.opt] = self.dest
        else:
            p = {}
            base_path = ("%s/%s/" % (base_path, self.dest)).replace('//', '/')

        self.bare_path = _force_unicode(base_path.replace('//', '/')).lower()
        self.bare_path = self.bare_path.rstrip('/')

        # append the query string
        base_path += query_string(p)

        # since we've been sloppy of keeping track of "//", get rid
        # of any that may be present
        self.path = base_path.replace('//', '/')

    def is_selected(self):
        """Given the current request path, would the button be selected."""
        if hasattr(self, 'name') and self.name == 'home':
            return False
        if self.opt:
            return request.params.get(self.opt, '') in self.aliases
        else:
            stripped_path = request.path.rstrip('/').lower()
            ustripped_path = _force_unicode(stripped_path)
            if stripped_path == self.bare_path:
                return True
            if stripped_path in self.aliases:
                return True

    def selected_title(self):
        """returns the title of the button when selected (for cases
        when it is different from self.title)"""
        return self.title

class AbsButton(NavButton):
    """A button for linking to an absolute URL"""
    def __init__(self, title, dest):
        self.path = dest
        NavButton.__init__(self, title, dest, False)

    def build(self, base_path = ''):
        pass

    def is_selected(self):
        return False

class SubredditButton(NavButton):
    def __init__(self, sr):
        self.sr = sr
        NavButton.__init__(self, sr.name, sr.path, False)

    def build(self, base_path = ''):
        self.path = self.sr.path

    def is_selected(self):
        return c.site == self.sr


class NamedButton(NavButton):
    """Convenience class for handling the majority of NavButtons
    whereby the 'title' is just the translation of 'name' and the
    'dest' defaults to the 'name' as well (unless specified
    separately)."""

    def __init__(self, name, sr_path = True, nocname=False, dest = None, **kw):
        self.name = name.replace('/', '')
        NavButton.__init__(self, menu[self.name], name if dest is None else dest,
                           sr_path = sr_path, nocname=nocname, **kw)

    def selected_title(self):
        """Overrides selected_title to use menu_selected dictionary"""
        try:
            return menu_selected[self.name]
        except KeyError:
            return NavButton.selected_title(self)

class ExpandableButton(NamedButton):
    def __init__(self, name, sr_path = True, nocname=False, dest = None, 
                 sub_reddit = "/", sub_menus=[], **kw):
        self.sub = sub_menus
        self.sub_reddit  = sub_reddit
        NamedButton.__init__(self,name,sr_path,nocname,dest,**kw)

    def sub_menus(self):
        return self.sub

    def is_selected(self):
        return c.site.path == self.sub_reddit

class JsButton(NavButton):
    """A button which fires a JS event and thus has no path and cannot
    be in the 'selected' state"""
    def __init__(self, title, style = 'js', **kw):
        NavButton.__init__(self, title, '', style = style, **kw)

    def build(self, *a, **kw):
        self.path = 'javascript:void(0)'

    def is_selected(self):
        return False

class PageNameNav(Styled):
    """generates the links and/or labels which live in the header
    between the header image and the first nav menu (e.g., the
    subreddit name, the page name, etc.)"""
    pass

class SimpleGetMenu(NavMenu):
    """Parent class of menus used for sorting and time sensitivity of
    results.  More specifically, defines a type of menu which changes
    the url by adding a GET parameter with name 'get_param' and which
    defaults to 'default' (both of which are class-level parameters).

    The value of the GET parameter must be one of the entries in
    'cls.options'.  This parameter is also used to construct the list
    of NavButtons contained in this Menu instance.  The goal here is
    to have a menu object which 'out of the box' is self validating."""
    options   = []
    get_param = ''
    title     = ''
    default = None

    def __init__(self, type = 'select', **kw):
        kw['default'] = kw.get('default', self.default)
        kw['base_path'] = kw.get('base_path') or request.path
        buttons = [NavButton(self.make_title(n), n, opt = self.get_param)
                   for n in self.options]
        NavMenu.__init__(self, buttons, type = type, **kw)
        #if kw.get('default'):
        #    self.selected = kw['default']

    def make_title(self, attr):
        return menu[attr]

    @classmethod
    def operator(self, sort):
        """Converts the opt into a DB-esque operator used for sorting results"""
        return None

class SortMenu(SimpleGetMenu):
    """The default sort menu."""
    get_param = 'sort'
    default   = 'hot'
    options   = ('hot', 'new', 'top', 'old', 'controversial')

    def __init__(self, **kw):
        kw['title'] = _("Sort By") + ':'
        SimpleGetMenu.__init__(self, **kw)

    @classmethod
    def operator(self, sort):
        if sort == 'hot':
            return operators.desc('_hot')
        elif sort == 'new':
            return operators.desc('_date')
        elif sort == 'old':
            return operators.asc('_date')
        elif sort == 'top':
            return operators.desc('_score')
        elif sort == 'controversial':
            return operators.desc('_controversy')
        elif sort == 'confidence':
            return operators.desc('_confidence')
        elif sort == 'interestingness':
            return operators.desc('_interestingness')

class CommentSortMenu(SortMenu):
    """Sort menu for comments pages"""
    default   = 'confidence'
    options   = ('confidence', 'hot', 'new', 'controversial', 'top', 'old', 'interestingness')

class SearchSortMenu(SortMenu):
    """Sort menu for search pages."""
    default   = 'relevance'
    mapping   = dict(relevance = None,
                     hot = 'hot desc',
                     new = 'date desc',
                     old = 'date asc',
                     top = 'points desc')
    options   = mapping.keys()

    @classmethod
    def operator(cls, sort):
        return cls.mapping.get(sort, cls.mapping[cls.default])

class RecSortMenu(SortMenu):
    """Sort menu for recommendation page"""
    default   = 'new'
    options   = ('hot', 'new', 'top', 'controversial', 'relevance')

class NewMenu(SimpleGetMenu):
    get_param = 'sort'
    default   = 'new'
    options   = ('new', 'rising')
    type = 'flatlist'

    def __init__(self, **kw):
        kw['title'] = _("Sort by")
        SimpleGetMenu.__init__(self, **kw)

    @classmethod
    def operator(self, sort):
        if sort == 'new':
            return operators.desc('_date')


class KindMenu(SimpleGetMenu):
    get_param = 'kind'
    default = 'all'
    options = ('links', 'comments', 'messages', 'all')

    def __init__(self, **kw):
        kw['title'] = _("Kind")
        SimpleGetMenu.__init__(self, **kw)

    def make_title(self, attr):
        if attr == "all":
            return _("All")
        return menu[attr]

class TimeMenu(SimpleGetMenu):
    """Menu for setting the time interval of the listing (from 'hour' to 'all')"""
    get_param = 't'
    default   = 'all'
    options   = ('hour', 'day', 'week', 'month', 'quarter', 'year', 'all')

    def __init__(self, **kw):
        kw.setdefault('title', _("Links from"))
        SimpleGetMenu.__init__(self, **kw)

    @classmethod
    def operator(self, time):
        from r2.models import Link
        if time != 'all':
            return Link.c._date >= timeago(time)

class DashboardTimeMenu(SimpleGetMenu):
    """Menu for setting the time interval of the listing (from 'hour' to 'all').
       Has option for since last visit."""
    get_param = 't'
    default = 'last'
    options = ('hour', 'day', 'week', 'month', 'quarter', 'year', 'all', 'last')

    def __init__(self, **kw):
        kw.setdefault('title', _("Links from"))
        SimpleGetMenu.__init__(self, **kw)

    @classmethod
    def operator(self, time):
        from r2.models import Link
        if time != 'all':
            return Link.c._date >= timeago(time)

class NumCommentsMenu(SimpleGetMenu):
    """menu for toggling between the user's preferred number of
    comments and the max allowed in the display, assuming the number
    of comments in the listing exceeds one or both."""
    get_param = 'all'
    default   = 'false'
    options   = ('true', 'false')

    def __init__(self, num_comments, **context):
        context['title'] = _("Show") + ':'
        self.num_comments = num_comments
        SimpleGetMenu.__init__(self, **context)

    def make_title(self, attr):
        user_num = c.user.pref_num_comments
        if user_num > self.num_comments:
            # no menus needed if the number of comments is smaller
            # than any of the limits
            return ""
        elif self.num_comments > g.max_comments:
            # if the number present is larger than the global max,
            # label the menu as the user pref and the max number
            return dict(true=str(g.max_comments),
                        false=str(user_num))[attr]
        else:
            # if the number is less than the global max, display "all"
            # instead for the upper bound.
            return dict(true=_("All"),
                        false=str(user_num))[attr]


    def render(self, **kw):
        user_num = c.user.pref_num_comments
        if user_num > self.num_comments:
            return ""
        return SimpleGetMenu.render(self, **kw)

class SubredditMenu(NavMenu):
    def find_selected(self):
        """Always return False so the title is always displayed"""
        return None

class TagSortMenu(SimpleGetMenu):
    """Menu for listings by tag"""
    get_param = 'sort'
    default   = 'new'
    options   = ('old', 'new', 'top')

    def __init__(self, **kw):
        kw['title'] = _("Sort By") + ':'
        SimpleGetMenu.__init__(self, **kw)

    @classmethod
    def operator(self, sort):
        if sort == 'new':
            return operators.desc('_t1_date')
        elif sort == 'old':
            return operators.asc('_t1_date')
        elif sort == 'top':
            return operators.desc('_t1_score')

# --------------------
# TODO: move to admin area
class AdminReporterMenu(SortMenu):
    default = 'top'
    options = ('hot', 'new', 'top')

class AdminKindMenu(KindMenu):
    options = ('all', 'links', 'comments', 'spam', 'autobanned')


class AdminTimeMenu(TimeMenu):
    get_param = 't'
    default   = 'day'
    options   = ('hour', 'day', 'week')



########NEW FILE########
__FILENAME__ = normalized_hot
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.models import Link, Subreddit
from r2.lib.db.operators import desc, timeago
from r2.lib import utils 
from r2.config import cache
from r2.lib.memoize import memoize
from r2.lib.db.thing import Query

from pylons import g

from datetime import datetime, timedelta
import random

expire_delta = timedelta(minutes = 2)
TOP_CACHE = 1800

def access_key(sr):
    return sr.name + '_access'

def expire_key(sr):
    return sr.name + '_expire'

def expire_hot(sr):
    """Called when a subreddit should be recomputed: after a vote (hence,
    submit) or deletion."""
    cache.set(expire_key(sr), True)

def cached_query(query, sr):
    """Returns the results from running query. The results are cached and
    only recomputed after 'expire_delta'"""
    query._limit = 150
    query._write_cache = True
    iden = query._iden()

    read_cache = True
    #if query is in the cache, the expire flag is true, and the access
    #time is old, set read_cache = False
    if cache.get(iden) is not None:
        if cache.get(expire_key(sr)):
            access_time = cache.get(access_key(sr))
            if not access_time or datetime.now() > access_time + expire_delta:
                cache.delete(expire_key(sr))
                read_cache = False
    #if the query isn't in the cache, set read_cache to false so we
    #record the access time
    else:
        read_cache = False

    #set access time to the last time the query was actually run (now)
    if not read_cache:
        cache.set(access_key(sr), datetime.now())

    query._read_cache = read_cache
    res = list(query)

    return res

def get_hot(sr):
    """Get the hottest links for a subreddit. If g.use_query_cache is
    True, it'll use the query cache, otherwise it'll use cached_query()
    from above."""
    q = sr.get_links('hot', 'all')
    if isinstance(q, Query):
        return cached_query(q, sr)
    else:
        return Link._by_fullname(list(q)[:150], return_dict = False)

def only_recent(items):
    return filter(lambda l: l._date > utils.timeago('%d day' % g.HOT_PAGE_AGE),
                  items)

@memoize('normalize_hot', time = g.page_cache_time)
def normalized_hot_cached(sr_ids):
    """Fetches the hot lists for each subreddit, normalizes the scores,
    and interleaves the results."""
    results = []
    srs = Subreddit._byID(sr_ids, data = True, return_dict = False)
    for sr in srs:
        items = only_recent(get_hot(sr))

        if not items:
            continue

        top_score = max(items[0]._hot, 1)
        if items:
            results.extend((l, l._hot / top_score) for l in items)

    results.sort(key = lambda x: (x[1], x[0]._hot), reverse = True)
    return [l[0]._fullname for l in results]

def normalized_hot(sr_ids):
    sr_ids = list(sr_ids)
    sr_ids.sort()
    return normalized_hot_cached(sr_ids) if sr_ids else ()

########NEW FILE########
__FILENAME__ = notify
from r2.lib import emailer
from r2.models import Account


def get_users_to_notify_for_meetup(coords):
    # This query could definitely be optimized, but I don't expect it to be
    # run too often, so it's probably not worth the effort.
    users = Account._query(
        Account.c.pref_meetup_notify_enabled == True,
        Account.c.email != None,
        Account.c.pref_latitude != None,
        Account.c.pref_longitude != None)
    users = filter(lambda u: u.is_within_radius(coords, u.pref_meetup_notify_radius), users)
    return list(users)


def email_user_about_meetup(user, meetup):
    if meetup.author_id != user._id and user.email:
        emailer.meetup_email(user=user, meetup=meetup)

########NEW FILE########
__FILENAME__ = organic
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.models import *
from r2.lib.memoize import memoize
from r2.lib.normalized_hot import get_hot, only_recent
from r2.lib import count
from r2.lib.utils import UniqueIterator, timeago
from r2.lib.promote import get_promoted

from pylons import c

import random
from time import time

organic_lifetime = 5*60
organic_length   = 30

# how many regular organic links should show between promoted ones
promoted_every_n = 5

def keep_link(link):
    return not any((link.likes != None,
                    link.saved,
                    link.clicked,
                    link.hidden,
                    link._deleted,
                    link._spam))

def insert_promoted(link_names, sr_ids, logged_in):
    """
    Inserts promoted links into an existing organic list. Destructive
    on `link_names'
    """
    promoted_items = get_promoted()

    if not promoted_items:
        return

    def my_keepfn(l):
        if l.promoted_subscribersonly and l.sr_id not in sr_ids:
            return False
        else:
            return keep_link(l)

    # no point in running the builder over more promoted links than
    # we'll even use
    max_promoted = max(1,len(link_names)/promoted_every_n)

    # in the future, we may want to weight this sorting somehow
    random.shuffle(promoted_items)

    # remove any that the user has acted on
    builder = IDBuilder(promoted_items,
                        skip = True, keep_fn = my_keepfn,
                        num = max_promoted)
    promoted_items = builder.get_items()[0]

    if not promoted_items:
        return
    # don't insert one at the head of the list 50% of the time for
    # logged in users, and 50% of the time for logged-off users when
    # the pool of promoted links is less than 3 (to avoid showing the
    # same promoted link to the same person too often)
    if (logged_in or len(promoted_items) < 3) and random.choice((True,False)):
        promoted_items.insert(0, None)

    # insert one promoted item for every N items
    for i, item in enumerate(promoted_items):
        pos = i * promoted_every_n
        if pos > len(link_names):
            break
        elif item is None:
            continue
        else:
            link_names.insert(pos, promoted_items[i]._fullname)

@memoize('cached_organic_links', time = organic_lifetime)
def cached_organic_links(user_id, langs):
    if user_id is None:
        sr_ids = Subreddit.default_srs(langs, ids = True)
    else:
        user = Account._byID(user_id, data=True)
        sr_ids = Subreddit.user_subreddits(user)

    sr_count = count.get_link_counts()

    #only use links from reddits that you're subscribed to
    link_names = filter(lambda n: sr_count[n][1] in sr_ids, sr_count.keys())
    link_names.sort(key = lambda n: sr_count[n][0])

    #potentially add a up and coming link
    if random.choice((True, False)):
        sr = Subreddit._byID(random.choice(sr_ids))
        items = only_recent(get_hot(sr))
        if items:
            if len(items) == 1:
                new_item = items[0]
            else:
                new_item = random.choice(items[1:4])
            link_names.insert(0, new_item._fullname)

    # remove any that the user has acted on
    builder = IDBuilder(link_names,
                        skip = True, keep_fn = keep_link,
                        num = organic_length)
    link_names = [ x._fullname for x in builder.get_items()[0] ]

    calculation_key = str(time())
    update_pos(0, calculation_key)

    insert_promoted(link_names, sr_ids, user_id is not None)

    # remove any duplicates caused by insert_promoted
    ret = [ l for l in UniqueIterator(link_names) ]

    return (calculation_key, ret)

def organic_links(user):
    from r2.controllers.reddit_base import organic_pos
    
    sr_ids = Subreddit.user_subreddits(user, limit = None)
    # make sure that these are sorted so the cache keys are constant
    sr_ids.sort()

    if c.user_is_loggedin:
        cached_key, links = cached_organic_links(user._id, None)
    else:
        cached_key, links = cached_organic_links(None, c.content_langs)

    cookie_key, pos = organic_pos()
    # pos will be 0 if it wasn't specified
    if links and pos != 0:
        # make sure that we're not running off the end of the list
        pos = pos % len(links)

    return links, pos, cached_key

def update_pos(pos, key):
    "Update the user's current position within the cached organic list."
    from r2.controllers import reddit_base

    reddit_base.set_organic_pos(key, pos)

########NEW FILE########
__FILENAME__ = admin_pages
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from pylons         import c, g
from r2.lib.wrapped import Wrapped
from pages   import Reddit
from r2.lib.menus   import NamedButton, NavButton, menu, NavMenu

class AdminSidebar(Wrapped):
    def __init__(self, user):
        self.user = user


class Details(Wrapped):
    def __init__(self, link):
        Wrapped.__init__(self)
        self.link = link


class AdminPage(Reddit):
    create_reddit_box  = False
    submit_box         = False
    extension_handling = False
    
    def __init__(self, nav_menus = None, *a, **kw):
        #add admin options to the nav_menus
        if c.user_is_admin:
            buttons = []

            if g.translator:
                buttons.append(NavButton(menu.i18n, ""))

            admin_menu = NavMenu(buttons, title='show', base_path = '/admin',
                                 type="lightdrop")
            if nav_menus:
                nav_menus.insert(0, admin_menu)
            else:
                nav_menus = [admin_menu]

        Reddit.__init__(self, nav_menus = nav_menus, *a, **kw)

class AdminProfileMenu(NavMenu):
    def __init__(self, path):
        NavMenu.__init__(self, [], base_path = path,
                         title = 'admin', type="tabdrop")

try:
    from r2admin.lib.pages import *
except ImportError:
    pass

########NEW FILE########
__FILENAME__ = pages
# -*- coding: utf-8 -*-
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is Reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
#
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.lib.wrapped import Wrapped, NoTemplateFound
from r2.models import *
from r2.config import cache
from r2.lib.jsonresponse import json_respond
from r2.lib.jsontemplates import is_api
from pylons.i18n import _
from pylons import c, request, g
from pylons.controllers.util import abort

from r2.lib.captcha import get_iden
from r2.lib.filters import spaceCompress, _force_unicode, _force_utf8
from r2.lib.db.queries import db_sort
from r2.lib.menus import NavButton, NamedButton, NavMenu, JsButton, ExpandableButton, AbsButton
from r2.lib.menus import SubredditButton, SubredditMenu, menu
from r2.lib.strings import plurals, rand_strings, strings
from r2.lib.utils import title_to_url, query_string, UrlParser
from r2.lib.template_helpers import add_sr, get_domain
from r2.lib.promote import promote_builder_wrapper
from r2.lib.wikipagecached import WikiPageCached

import sys

datefmt = _force_utf8(_('%d %b %Y'))

def get_captcha():
    if not c.user_is_loggedin or c.user.needs_captcha():
        return get_iden()

class Reddit(Wrapped):
    '''Base class for rendering a page on reddit.  Handles toolbar creation,
    content of the footers, and content of the corner buttons.

    Constructor arguments:

        space_compress -- run r2.lib.filters.spaceCompress on render
        loginbox -- enable/disable rendering of the small login box in the right margin
          (only if no user is logged in; login box will be disabled for a logged in user)
        show_sidebar -- enable/disable content in the right margin

        infotext -- text to display in a <p class="infotext"> above the content
        nav_menus -- list of Menu objects to be shown in the area below the header
        content -- renderable object to fill the main content well in the page.

    settings determined at class-declaration time

        create_reddit_box  -- enable/disable display of the "Creat a reddit" box
        submit_box         -- enable/disable display of the "Submit" box
        searcbox           -- enable/disable display of the "search" box in the header
        extension_handling -- enable/disable rendering using non-html templates
          (e.g. js, xml for rss, etc.)
    '''

    create_reddit_box  = False
    submit_box         = False
    searchbox          = True
    extension_handling = True

    def __init__(self, space_compress = True, nav_menus = None, loginbox = True,
                 infotext = '', content = None, title = '', robots = None, sidewiki = True,
                 show_sidebar = True, body_class = None, top_filter = None, header_sub_nav = [], **context):
        Wrapped.__init__(self, **context)
        self.title          = title
        self.robots         = robots
        self.infotext       = infotext
        self.loginbox       = True
        self.show_sidebar   = show_sidebar
        self.space_compress = space_compress
        self.body_class     = body_class
        self.top_filter     = top_filter
        self.header_sub_nav = header_sub_nav
        self.sidewiki = sidewiki

        # by default, assume the canonical URLs are the ones without query params
        if request.GET:
            self.canonical_link = request.path

        #put the sort menus at the top
        self.nav_menu = MenuArea(menus = nav_menus) if nav_menus else None

        #add the infobar
        self.infobar = None
        if c.firsttime and c.site.firsttext and not infotext:
            infotext = c.site.firsttext
        if not infotext and hasattr(c.site, 'infotext'):
            infotext = c.site.infotext
        if infotext:
            self.infobar = InfoBar(message = infotext)

        self.srtopbar = None
        if not c.cname:
            self.srtopbar = SubredditTopBar()

        self._content = content
        self.toolbars = self.build_toolbars()

    def rightbox(self):
        """generates content in <div class="rightbox">"""

        ps = PaneStack(css_class='spacer')

        if self.searchbox:
            ps.append(GoogleSearchForm())

        if not c.user_is_loggedin and self.loginbox:
            ps.append(LoginFormWide())
        else:
            ps.append(ProfileBar(c.user, self.corner_buttons()))

        if (c.user_is_loggedin and
            c.user.wiki_account is None and
            c.user.email is not None and
            c.user.email_validated and
            self.sidewiki):
            ps.append(WikiCreateSide())

        filters_ps = PaneStack(div=True)
        for toolbar in self.toolbars:
            filters_ps.append(toolbar)

        if self.nav_menu:
            filters_ps.append(self.nav_menu)

        if not filters_ps.empty:
            ps.append(SideBox(filters_ps))

        #don't show the subreddit info bar on cnames
        if c.user_is_admin and not isinstance(c.site, FakeSubreddit) and not c.cname:
            ps.append(SubredditInfoBar())

        if self.extension_handling:
            ps.append(FeedLinkBar(getattr(self, 'canonical_link', request.path)))

        ps.append(SideBoxPlaceholder('side-meetups', _('Nearest Meetups'), '/meetups', sr_path=False))
        ps.append(SideBoxPlaceholder('side-comments', _('Recent Comments'), '/comments'))
        if c.site.name == 'discussion':
            ps.append(SideBoxPlaceholder('side-open', _('Recent Open Threads'), '/tag/open_thread'))
            ps.append(SideBoxPlaceholder('side-diary', _('Recent Rationality Diaries'), '/tag/group_rationality_diary'))
        else:
            ps.append(SideBoxPlaceholder('side-quote', _('Recent Rationality Quotes'), '/tag/quotes'))
        ps.append(SideBoxPlaceholder('side-posts', _('Recent Posts'), '/recentposts'))

        if g.recent_edits_feed:
            ps.append(RecentWikiEditsBox(g.recent_edits_feed))

        ps.append(FeedBox(g.feedbox_urls))

        ps.append(SideBoxPlaceholder('side-monthly-contributors', _('Top Contributors, 30 Days')))
        ps.append(SideBoxPlaceholder('karma-awards', _('Recent Karma Awards'), '/karma', sr_path=False))

        if g.site_meter_codename:
            ps.append(SiteMeter(g.site_meter_codename))

        return ps

    def render(self, *a, **kw):
        """Overrides default Wrapped.render with two additions
           * support for rendering API requests with proper wrapping
           * support for space compression of the result
        In adition, unlike Wrapped.render, the result is in the form of a pylons
        Response object with it's content set.
        """
        try:
            res = Wrapped.render(self, *a, **kw)
            if is_api():
                res = json_respond(res)
            elif self.space_compress:
                res = spaceCompress(res)
            c.response.content = res
        except NoTemplateFound, e:
            # re-raise the error -- development environment
            if g.debug:
                s = sys.exc_info()
                raise s[1], None, s[2]
            # die gracefully -- production environment
            else:
                abort(404, "not found")
        return c.response

    def corner_buttons(self):
        """set up for buttons in upper right corner of main page."""
        buttons = []
        if c.user_is_loggedin:
            if c.user.name in g.admins:
                if c.user_is_admin:
                   buttons += [NamedButton("adminoff", False,
                                           nocname=not c.authorized_cname,
                                           target = "_self")]
                else:
                   buttons += [NamedButton("adminon",  False,
                                           nocname=not c.authorized_cname,
                                           target = "_self")]

            buttons += [NamedButton('submit', sr_path = not c.default_sr,
                                    nocname=not c.authorized_cname)]
            if c.user.safe_karma >= g.discussion_karma_to_post:
                buttons += [NamedButton('meetups/new', False,
                                        nocname=not c.authorized_cname)]
            buttons += [NamedButton("prefs", False,
                                  css_class = "pref-lang")]
            buttons += [NamedButton("logout", False,
                                    nocname=not c.authorized_cname,
                                    target = "_self")]

        return NavMenu(buttons, base_path = "/", type = "buttons")

    def footer_nav(self):
        """navigation buttons in the footer."""
        buttons = [NamedButton("help", False, nocname=True),
                   NamedButton("blog", False, nocname=True),
                   NamedButton("stats", False, nocname=True),
                   NamedButton("feedback",     False),
                   NamedButton("bookmarklets", False),
                   NamedButton("socialite",    False),
                   NamedButton("buttons",      True),
                   NamedButton("widget",       True),
                   NamedButton("code",         False, nocname=True),
                   NamedButton("mobile",       False, nocname=True),
                   NamedButton("store",        False, nocname=True),
                   NamedButton("ad_inq",       False, nocname=True),
                   ]

        return NavMenu(buttons, base_path = "/", type = "flatlist")

    def header_nav(self):
        """Navigation menu for the header"""

        menu_stack = PaneStack()

        # Ensure the default button is the first tab
        #default_button_name = c.site.default_listing

        main_buttons = [
            ExpandableButton('main', dest = '/promoted', sr_path = False, sub_menus =
                             [ NamedButton('posts', dest = '/promoted', sr_path = False),
                               NamedButton('comments', dest = '/comments', sr_path = False)]),
            ExpandableButton('discussion', dest = "/r/discussion/new", sub_reddit = "/r/discussion/", sub_menus =
                             [ NamedButton('posts', dest = "/r/discussion/new", sr_path = False),
                               NamedButton('comments', dest = "/r/discussion/comments", sr_path = False)])
       ]

        menu_stack.append(NavMenu(main_buttons, title = _('Filter by'), _id='nav', type='navlist'))


        if self.header_sub_nav:
            menu_stack.append(NavMenu(self.header_sub_nav, title = _('Filter by'), _id='filternav', type='navlist'))

        return menu_stack

    def right_menu(self):
        """docstring for right_menu"""
        buttons = [
          AbsButton('wiki', 'http://'+g.wiki_host),
          NamedButton('sequences', sr_path=False),
          NamedButton('about', sr_path=False)
        ]

        return NavMenu(buttons, title = _('Filter by'), _id='rightnav', type='navlist')

    def build_toolbars(self):
        """Additional toolbars/menus"""
        return []

    def __repr__(self):
        return "<Reddit>"

    @staticmethod
    def content_stack(*a):
        """Helper method for reordering the content stack."""
        return PaneStack(filter(None, a))

    def content(self):
        """returns a Wrapped (or renderable) item for the main content div."""
        return self.content_stack(self.infobar, self._content)

class LoginFormWide(Wrapped):
    """generates a login form suitable for the 300px rightbox."""
    pass

class WikiCreateSide(Wrapped):
    """generates a sidebox for creating a wiki account."""
    pass

class SideBoxPlaceholder(Wrapped):
    """A minimal side box with a heading and an anchor.

    If javascript is off the anchor may be followed and if it is on
    then javascript will replace the content of the div with the HTML
    result of an ajax request.
    """

    def __init__(self, node_id, link_text, link_path=None, sr_path=True):
        Wrapped.__init__(self, node_id=node_id, link_text=link_text, link_path=link_path, sr_path=sr_path)

class SpaceCompressedWrapped(Wrapped):
    """Overrides default Wrapped.render to do space compression as well."""
    def render(self, *a, **kw):
        res = Wrapped.render(self, *a, **kw)
        res = spaceCompress(res)
        return res

class RecentItems(SpaceCompressedWrapped):
    def __init__(self, *args, **kwargs):
        self.things = self.init_builder()
        Wrapped.__init__(self, *args, **kwargs)

    def query(self):
        raise NotImplementedError

    def init_builder(self):
        return QueryBuilder(self.query(), wrap=self.wrap_thing)

    @staticmethod
    def wrap_thing(thing):
        w = Wrapped(thing)

        if isinstance(thing, Link):
            w.render_class = InlineArticle
        elif isinstance(thing, Comment):
            w.render_class = InlineComment

        return w

class RecentComments(RecentItems):
    def query(self):
        return c.current_or_default_sr.get_comments('new', 'all')

    def init_builder(self):
        return UnbannedCommentBuilder(
            self.query(),
            num = 5,
            wrap = RecentItems.wrap_thing,
            skip = True,
            sr_ids = [c.current_or_default_sr._id]
        )

class RecentTagged(RecentItems):
    """Finds the most recent post associated with a given tag and shows the most
       most recent top level comment in that thread"""
    def __init__(self, *args, **kwargs):
        self.tag = kwargs['tagtype']
        self.title = kwargs['title']
        self.things = self.init_builder()
        Wrapped.__init__(self, *args, **kwargs)

    def query(self):
        t = LinkTag._query(LinkTag.c._thing2_id == Tag._by_name(self.tag)._id,
                           LinkTag.c._name == 'tag',
                           LinkTag.c._t1_deleted == False,
                           sort = desc('_date'),
                           limit = 1,
                           eager_load = True,
                           thing_data = not g.use_query_cache
                      )
        temp = list(t)[0]._thing1
        relevantpost = temp._id
        self.url = temp.url
        q = Comment._query(Comment.c.link_id == relevantpost,
                            Comment.c._deleted == False,
                            Comment.c._spam == False,
                            sort = desc('_date'),
                            data = True)
        return q

    def init_builder(self):
        return ToplevelCommentBuilder(
            self.query(),
            num = 1,
            wrap = RecentItems.wrap_thing,
            skip = True,
            sr_ids = [c.current_or_default_sr._id]
        )

class RecentArticles(RecentItems):
    def query(self):
        q = c.current_or_default_sr.get_links('new', 'all')
        q._limit = 10
        return q

class RecentArticlesPage(Wrapped):
    """Compact recent article listing page"""
    def __init__(self, content, *a, **kw):
        Wrapped.__init__(self, content=content, *a, **kw)

class KarmaPage(Wrapped):
    """Compact recent article listing page"""
    def __init__(self, content, *a, **kw):
        Wrapped.__init__(self, content=content, *a, **kw)

class RecentPromotedArticles(RecentItems):
    def query(self):
        sr = DefaultSR()
        q = sr.get_links('blessed', 'all')
        q._limit = 4
        return q

class TopContributors(SpaceCompressedWrapped):
    def __init__(self, *args, **kwargs):
        from r2.lib.user_stats import top_users
        uids = top_users()
        users = Account._byID(uids, data=True, return_dict=False)

        # Filter out accounts banned from the default subreddit
        sr = Subreddit._by_name(g.default_sr)
        self.things = filter(lambda user: not sr.is_banned(user), users)

        Wrapped.__init__(self, *args, **kwargs)

class TopMonthlyContributors(SpaceCompressedWrapped):
    def __init__(self, *args, **kwargs):
        from r2.lib.user_stats import cached_monthly_top_users
        uids_karma = cached_monthly_top_users()
        uids = map(lambda x: x[0], uids_karma)
        users = Account._byID(uids, data=True, return_dict=False)

        # Add the monthly karma to the account objects
        karma_lookup = dict(uids_karma)
        for u in users:
            pair = karma_lookup[u._id]
            u.monthly_karma = pair[0] - pair[1]

        # Filter out accounts banned from the default subreddit
        sr = Subreddit._by_name(g.default_sr)
        self.things = filter(lambda user: not sr.is_banned(user), users)

        Wrapped.__init__(self, *args, **kwargs)

class TagCloud(SpaceCompressedWrapped):

    numbers = ('one','two','three','four','five','six','seven','eight','nine','ten')

    def nav(self):
        cloud = Tag.tag_cloud_for_subreddits([c.current_or_default_sr._id])

        buttons = []
        for tag, weight in cloud:
            buttons.append(NavButton(tag.name, tag.name, css_class=self.numbers[weight - 1]))

        return NavMenu(buttons, type="flatlist", separator=' ', base_path='/tag/')

class SubredditInfoBar(Wrapped):
    """When not on Default, renders a sidebox which gives info about
    the current reddit, including links to the moderator and
    contributor pages, as well as links to the banning page if the
    current user is a moderator."""
    def nav(self):
        is_moderator = c.user_is_loggedin and \
            c.site.is_moderator(c.user) or c.user_is_admin

        buttons = [NavButton(plurals.moderators, 'moderators')]
        if c.site.type != 'public':
            buttons.append(NavButton(plurals.contributors, 'contributors'))

        if is_moderator:
            buttons.append(NamedButton('edit'))
            buttons.extend([NavButton(menu.banusers, 'banned'),
                            NamedButton('spam')])
        return [NavMenu(buttons, type = "flatlist", base_path = "/about/")]

class SideBox(Wrapped):
    """Generic sidebox"""
    def __init__(self, content, _id = None, css_class = ''):
        Wrapped.__init__(self, content=content, _id = _id, css_class = css_class)


class PrefsPage(Reddit):
    """container for pages accessible via /prefs.  No extension handling."""

    extension_handling = False

    def __init__(self, show_sidebar = True, *a, **kw):
        Reddit.__init__(self, show_sidebar = show_sidebar,
                        title = "%s - %s" % (_("Preferences"), c.site.title),
                        *a, **kw)

    def header_nav(self):
        buttons = [NavButton(menu.options, ''),
                   NamedButton('friends'),
                   NamedButton('update'),
                   NamedButton('delete')]

        if c.user.wiki_account is None:
            buttons.append(NamedButton('wikiaccount'))
        elif c.user.wiki_account == '__error__':
            pass
        else:
            user_page_url = 'http://{0}/wiki/User:{1}'.format(g.wiki_host, c.user.wiki_account)
            buttons.append(NamedButton('wikiaccount', dest=user_page_url, style='external'))
        return NavMenu(buttons, base_path = "/prefs", _id='nav', type='navlist')

class PrefOptions(Wrapped):
    """Preference form for updating language and display options"""
    def __init__(self, done = False):
        Wrapped.__init__(self, done = done)

class PrefUpdate(Wrapped):
    """Preference form for updating email address and passwords"""
    pass

class PrefDelete(Wrapped):
    """preference form for deleting a user's own account."""
    pass

class PrefWiki(Wrapped):
    """Preference form for creating a Wiki account."""
    pass


class MessagePage(Reddit):
    """Defines the content for /message/*"""
    def __init__(self, *a, **kw):
        if not kw.has_key('show_sidebar'):
            kw['show_sidebar'] = True
        Reddit.__init__(self, *a, **kw)
        self.replybox = CommentReplyBox()

    def content(self):
        return self.content_stack(self.replybox, self.infobar, self._content)

    def header_nav(self):
        buttons =  [NamedButton('compose'),
                    NamedButton('inbox'),
                    NamedButton('sent')]
        return NavMenu(buttons, base_path = "/message", _id='nav', type='navlist')

class MessageCompose(Wrapped):
    """Compose message form."""
    def __init__(self,to='', subject='', message='', success='',
                 captcha = None):
        Wrapped.__init__(self, to = to, subject = subject,
                         message = message, success = success,
                         captcha = captcha)

class KarmaAwardPage(Reddit):
    """Defines the content for /message/*"""
    def __init__(self, *a, **kw):
        if not kw.has_key('show_sidebar'):
            kw['show_sidebar'] = True
        Reddit.__init__(self, *a, **kw)
        self.replybox = CommentReplyBox()

    def content(self):
        return self.content_stack(self.replybox, self.infobar, self._content)


class KarmaAward(Wrapped):
    """Compose message form."""
    def __init__(self,to='', amount='', reason='', success='',
                 captcha = None):
        Wrapped.__init__(self, to = to, amount = amount,
                         reason = reason, success = success,
                         captcha = captcha)

class BoringPage(Reddit):
    """parent class For rendering all sorts of uninteresting,
    sortless, navless form-centric pages.  The top navmenu is
    populated only with the text provided with pagename and the page
    title is 'reddit.com: pagename'"""

    extension_handling= False

    def __init__(self, pagename, **context):
        self.pagename = pagename
        Reddit.__init__(self, title = "%s - %s" % (_force_unicode(pagename), c.site.title),
                        **context)

class FormPage(BoringPage):
    """intended for rendering forms with no rightbox needed or wanted"""
    def __init__(self, pagename, show_sidebar = False, *a, **kw):
        BoringPage.__init__(self, pagename,  show_sidebar = show_sidebar,
                            *a, **kw)


class LoginPage(BoringPage):
    """a boring page which provides the Login/register form"""
    def __init__(self, **context):
        context['loginbox'] = False
        self.dest = context.get('dest', '')
        context['show_sidebar'] = False
        BoringPage.__init__(self,  _("Login or register"), **context)

    def content(self):
        kw = {}
        for x in ('user_login', 'user_reg'):
            kw[x] = getattr(self, x) if hasattr(self, x) else ''
        return Login(dest = self.dest, **kw)

class Login(Wrapped):
    """The two-unit login and register form."""
    def __init__(self, user_reg = '', user_login = '', dest=''):
        Wrapped.__init__(self, user_reg = user_reg, user_login = user_login,
                         dest = dest)

class VerifyEmail(Wrapped):
    def __init__(self, success=False):
        Wrapped.__init__(self, success = success)

class SearchPage(BoringPage):
    """Search results page"""
    searchbox = False

    def __init__(self, pagename, prev_search, elapsed_time, num_results, *a, **kw):
        self.searchbar = SearchBar(prev_search = prev_search,
                                   elapsed_time = elapsed_time,
                                   num_results = num_results)
        BoringPage.__init__(self, pagename, robots='noindex', *a, **kw)

    def content(self):
        return self.content_stack(self.searchbar, self.infobar, self._content)

class LinkInfoPage(Reddit):
    """Renders the varied /info pages for a link.  The Link object is
    passed via the link argument and the content passed to this class
    will be rendered after a one-element listing consisting of that
    link object.

    In addition, the rendering is reordered so that any nav_menus
    passed to this class will also be rendered underneath the rendered
    Link.
    """

    create_reddit_box  = False
    robots             = None

    @staticmethod
    def comment_permalink_wrapper(comment, link):
        wrapped = Wrapped(link, link_title=comment.make_permalink_title(link), for_comment_permalink=True)
        wrapped.render_class = CommentPermalink
        return wrapped

    def __init__(self, link = None, comment = None,
                 link_title = '', is_canonical = False, *a, **kw):

        link.render_full = True

        # TODO: temp hack until we find place for builder_wrapper
        from r2.controllers.listingcontroller import ListingController
        if comment:
            link_wrapper = lambda link: self.comment_permalink_wrapper(comment, link)
        else:
            link_wrapper = ListingController.builder_wrapper
        link_builder = IDBuilder(link._fullname,
                                 wrap = link_wrapper)

        # link_listing will be the one-element listing at the top
        self.link_listing = LinkListing(link_builder, nextprev=False).listing()

        # link is a wrapped Link object
        self.link = self.link_listing.things[0]

        link_title = ((self.link.title) if hasattr(self.link, 'title') else '')
        if comment:
            title = comment.make_permalink_title(link)

            # Comment permalinks should not be indexed, there's too many of them
            self.robots = 'noindex'

            if is_canonical == False:
                self.canonical_link = comment.make_permalink(link)
        else:
            params = {'title':_force_unicode(link_title), 'site' : c.site.title}
            title = strings.link_info_title % params

            if not (c.default_sr and is_canonical):
                # Not on the main page, so include a pointer to the canonical URL for this link
                self.canonical_link = link.canonical_url

        Reddit.__init__(self, title = title, body_class = 'post', robots = self.robots, *a, **kw)

    def content(self):
        return self.content_stack(self.infobar, self.link_listing, self._content)

    def build_toolbars(self):
        return []

class LinkInfoBar(Wrapped):
    """Right box for providing info about a link."""
    def __init__(self, a = None):
        if a:
            a = Wrapped(a)

        Wrapped.__init__(self, a = a, datefmt = datefmt)


class EditReddit(Reddit):
    """Container for the about page for a reddit"""
    extension_handling= False

    def __init__(self, *a, **kw):
        is_moderator = c.user_is_loggedin and \
            c.site.is_moderator(c.user) or c.user_is_admin

        title = _('Manage your category') if is_moderator else \
                _('About %(site)s') % dict(site=c.site.name)

        Reddit.__init__(self, title = title, *a, **kw)

class SubredditsPage(Reddit):
    """container for rendering a list of reddits."""
    submit_box   = False
    def __init__(self, prev_search = '', num_results = 0, elapsed_time = 0,
                 title = '', loginbox = True, infotext = None, *a, **kw):
        Reddit.__init__(self, title = title, loginbox = loginbox, infotext = infotext,
                        *a, **kw)
        self.sr_infobar = InfoBar(message = strings.sr_subscribe)

    def header_nav(self):
        buttons =  [NavButton(menu.popular, ""),
                    NamedButton("new")]
        if c.user_is_admin:
            buttons.append(NamedButton("banned"))

        return NavMenu(buttons, base_path = '/categories')

    def content(self):
        return self.content_stack(self.sr_infobar, self._content)

    def rightbox(self):
        ps = Reddit.rightbox(self)
        position = 1 if not c.user_is_loggedin else 0
        ps.insert(position, SubscriptionBox())
        return ps

class MySubredditsPage(SubredditsPage):
    """Same functionality as SubredditsPage, without the search box."""

    def content(self):
        return self.content_stack(self.infobar, self._content)


def votes_visible(user):
    """Determines whether to show/hide a user's votes.  They are visible:
     * if the current user is the user in question
     * if the user has a preference showing votes
     * if the current user is an administrator
    """
    return ((c.user_is_loggedin and c.user.name == user.name) or
            user.pref_public_votes or
            c.user_is_admin)

class ProfilePage(Reddit):
    """Container for a user's profile page.  As such, the Account
    object of the user must be passed in as the first argument, along
    with the current sub-page (to determine the title to be rendered
    on the page)"""

    searchbox         = False
    create_reddit_box = False
    submit_box        = False


    def __init__(self, user, *a, **kw):
        self.user     = user
        Reddit.__init__(self, body_class = "profile_page", *a, **kw)

    def header_nav(self):
        path = "/user/%s/" % self.user.name
        main_buttons = []

        # Only show the profile link if this user has a user page in the wiki
        wikipage = WikiPageCached({'url': WikiPageCached.get_url_for_user_page(self.user)})
        if wikipage.success:
            main_buttons.append(NavButton(_('Profile'), '/', aliases = ['/profile']))

        main_buttons += [
                   NavButton(menu.overview, '/overview'),
                   NavButton(_('Comments'), 'comments'),
                   NamedButton('submitted')]

        if votes_visible(self.user):
            main_buttons += [NamedButton('liked'),
                        NamedButton('disliked'),
                        NamedButton('hidden')]

        if c.user_is_loggedin and self.user._id == c.user._id:
            # User is looking at their own page
            main_buttons.append(NamedButton('drafts'))

        return NavMenu(main_buttons, base_path = path, title = _('View'), _id='nav', type='navlist')


    def rightbox(self):
        rb = Reddit.rightbox(self)
        if self.user != c.user:
            rb.push(ProfileBar(self.user))
        rb.push(GoogleSearchForm(label="Search this user's posts & comments:",
                                 query_prefix='"author: ' + self.user.name + '" '))
        return rb

class ProfileBar(Wrapped):
    """Draws a right box for info about the user (karma, etc)"""
    def __init__(self, user, buttons = None):
        Wrapped.__init__(self, user = user, buttons = buttons)
        self.isFriend = self.user._id in c.user.friends \
            if c.user_is_loggedin else False
        self.isMe = (self.user == c.user)

class MenuArea(Wrapped):
    """Draws the gray box at the top of a page for sort menus"""
    def __init__(self, menus = []):
        Wrapped.__init__(self, menus = menus)

class InfoBar(Wrapped):
    """Draws the yellow box at the top of a page for info"""
    def __init__(self, message = ''):
        Wrapped.__init__(self, message = message)

class UnfoundPage(Wrapped):
    """Wrapper for the 404 page"""
    def __init__(self, choice='a'):
        Wrapped.__init__(self, choice=choice)

class ErrorPage(Wrapped):
    """Wrapper for an error message"""
    def __init__(self, message = _("You aren't allowed to do that.")):
        Wrapped.__init__(self, message = message)

class Profiling(Wrapped):
    """Debugging template for code profiling using built in python
    library (only used in middleware)"""
    def __init__(self, header = '', table = [], caller = [], callee = [], path = ''):
        Wrapped.__init__(self, header = header, table = table, caller = caller,
                         callee = callee, path = path)

class Over18(Wrapped):
    """The creepy 'over 18' check page for nsfw content."""
    pass

class SubredditTopBar(Wrapped):
    """The horizontal strip at the top of most pages for navigating
    user-created reddits."""
    def __init__(self):
        Wrapped.__init__(self)

        my_reddits = []
        sr_ids = Subreddit.user_subreddits(c.user if c.user_is_loggedin else None)
        if sr_ids:
            my_reddits = Subreddit._byID(sr_ids, True,
                                         return_dict = False)
            my_reddits.sort(key = lambda sr: sr.name.lower())

        drop_down_buttons = []
        for sr in my_reddits:
            drop_down_buttons.append(SubredditButton(sr))

        #leaving the 'home' option out for now
        #drop_down_buttons.insert(0, NamedButton('home', sr_path = False,
        #                                        css_class = 'top-option',
        #                                        dest = '/'))
        drop_down_buttons.append(NamedButton('edit', sr_path = False,
                                             css_class = 'bottom-option',
                                             dest = '/categories/'))
        self.sr_dropdown = SubredditMenu(drop_down_buttons,
                                         title = _('My categories'),
                                         type = 'srdrop')


        pop_reddits = Subreddit.default_srs(c.content_langs, limit = 30)
        buttons = []
        for sr in c.recent_reddits:
            # Extra guarding for Issue #50
            if hasattr(sr, 'name'):
                buttons.append(SubredditButton(sr))

        for sr in pop_reddits:
            if sr not in c.recent_reddits:
                buttons.append(SubredditButton(sr))

        self.sr_bar = NavMenu(buttons, type='flatlist', separator = '-',
                                        _id = 'sr-bar')

class SubredditBox(Wrapped):
    """A content pane that has the lists of subreddits that go in the
    right pane by default"""
    def __init__(self):
        Wrapped.__init__(self)

        self.title = _('Other reddit communities')
        self.subtitle = 'Visit your subscribed categories (in bold) or explore new ones'
        self.create_link = ('/categories/', menu.more)
        self.more_link   = ('/categories/create', _('Create'))

        my_reddits = []
        sr_ids = Subreddit.user_subreddits(c.user if c.user_is_loggedin else None)
        if sr_ids:
            my_reddits = Subreddit._byID(sr_ids, True,
                                         return_dict = False)
            my_reddits.sort(key = lambda sr: sr._downs, reverse = True)

        display_reddits = my_reddits[:g.num_side_reddits]

        #remove the current reddit
        display_reddits = filter(lambda x: x != c.site, display_reddits)

        pop_reddits = Subreddit.default_srs(c.content_langs, limit = g.num_side_reddits)
        #add english reddits to the list
        if c.content_langs != 'all' and 'en' not in c.content_langs:
            en_reddits = Subreddit.default_srs(['en'])
            pop_reddits += [sr for sr in en_reddits if sr not in pop_reddits]

        for sr in pop_reddits:
            if len(display_reddits) >= g.num_side_reddits:
                break

            if sr != c.site and sr not in display_reddits:
                display_reddits.append(sr)

        col1, col2 = [], []
        cur_col, other = col1, col2
        for sr in display_reddits:
            cur_col.append((sr, sr in my_reddits))
            cur_col, other = other, cur_col

        self.cols = ((col1, col2))
        self.mine = my_reddits

class SubscriptionBox(Wrapped):
    """The list of reddits a user is currently subscribed to to go in
    the right pane."""
    def __init__(self):
        sr_ids = Subreddit.user_subreddits(c.user if c.user_is_loggedin else None)
        srs = Subreddit._byID(sr_ids, True, return_dict = False)
        srs.sort(key = lambda sr: sr.name.lower())
        b = IDBuilder([sr._fullname for sr in srs])
        self.reddits = LinkListing(b).listing().things

class CreateSubreddit(Wrapped):
    """reddit creation form."""
    def __init__(self, site = None, name = '', listings = []):
        Wrapped.__init__(self, site = site, name = name, listings = listings)

class SubredditStylesheet(Wrapped):
    """form for editing or creating subreddit stylesheets"""
    def __init__(self, site = None,
                 stylesheet_contents = ''):
        Wrapped.__init__(self, site = site,
                         stylesheet_contents = stylesheet_contents)

class CssError(Wrapped):
    """Rendered error returned to the stylesheet editing page via ajax"""
    def __init__(self, error):
        # error is an instance of cssutils.py:ValidationError
        Wrapped.__init__(self, error = error)

class UploadedImage(Wrapped):
    "The page rendered in the iframe during an upload of a header image"
    def __init__(self,status,img_src, name="", errors = {}):
        self.errors = list(errors.iteritems())
        Wrapped.__init__(self, status=status, img_src=img_src, name = name)

class ImageBrowser(Wrapped):
    "The page rendered in the tinyMCE image browser window"
    def __init__(self, article):
        self.article = article
        Wrapped.__init__(self)

class Password(Wrapped):
    """Form encountered when 'recover password' is clicked in the LoginFormWide."""
    def __init__(self, success=False):
        Wrapped.__init__(self, success = success)

class PasswordReset(Wrapped):
    """Template for generating an email to the user who wishes to
    reset their password (step 2 of password recovery, after they have
    entered their user name in Password.)"""
    pass

class ResetPassword(Wrapped):
    """Form for actually resetting a lost password, after the user has
    clicked on the link provided to them in the Password_Reset email
    (step 3 of password recovery.)"""
    pass

class EmailVerify(Wrapped):
    """Form for providing a confirmation code to a new user."""
    pass

class WikiSignupFail(Wrapped):
    """Email template. Tells a user that their automatic wiki account
    creation failed.
    """
    pass

class WikiSignupNotification(Wrapped):
    """Email template. Tells a user their account on the LessWrong Wiki
    has been created.
    """
    pass

class WikiAPIError(Wrapped):
    """Email template to notify devs of unknown account creation errors."""
    pass

class WikiUserExists(Wrapped):
    """Email template to tell a user that we tried to make their account
    but someone else already had it.
    """
    pass

class WikiIncompatibleName(Wrapped):
    """Email template to tell them their username doesn't allow automatic
    wikification.
    """
    pass

class Captcha(Wrapped):

    """Container for rendering robot detection device."""
    def __init__(self, error=None, tabular=True, label=True):
        self.error = _('Try entering those letters again') if error else ""
        self.iden = get_captcha()
        Wrapped.__init__(self, tabular=tabular, label=label)

class CommentReplyBox(Wrapped):
    """Used on LinkInfoPage to render the comment reply form at the
    top of the comment listing as well as the template for the forms
    which are JS inserted when clicking on 'reply' in either a comment
    or message listing."""
    def __init__(self, link_name='', captcha=None, action = 'comment'):
        Wrapped.__init__(self, link_name = link_name, captcha = captcha,
                         action = action)

class CommentListing(Wrapped):
    """Comment heading and sort, limit options"""
    def __init__(self, content, num_comments, nav_menus = []):
        Wrapped.__init__(self, content=content, num_comments=num_comments, menus = nav_menus)

class PermalinkMessage(Wrapped):
    """renders the box on comment pages that state 'you are viewing a
    single comment's thread'"""
    def __init__(self, comments_url, has_more_comments=False):
        self.comments_url = comments_url
        self.has_more_comments = has_more_comments
        Wrapped.__init__(self)


class PaneStack(Wrapped):
    """Utility class for storing and rendering a list of block elements."""

    def __init__(self, panes=[], div_id = None, css_class=None, div=False):
        div = div or div_id or css_class or False
        self.div_id    = div_id
        self.css_class = css_class
        self.div       = div
        self.stack     = list(panes)
        Wrapped.__init__(self)

    def append(self, item):
        """Appends an element to the end of the current stack"""
        self.stack.append(item)

    def push(self, item):
        """Prepends an element to the top of the current stack"""
        self.stack.insert(0, item)

    def insert(self, *a):
        """inerface to list.insert on the current stack"""
        return self.stack.insert(*a)

    @property
    def empty(self):
        """Return True if the stack has any items, False otherwise"""
        return len(self.stack) == 0

class SearchForm(Wrapped):
    """The simple search form in the header of the page.  prev_search
    is the previous search."""
    def __init__(self, prev_search = ''):
        Wrapped.__init__(self, prev_search = prev_search)

class GoogleSearchForm(Wrapped):
    """Shows Google Custom Search box"""
    def __init__(self, label='', query_prefix='', query_suffix=''):
        Wrapped.__init__(self, label=label, query_prefix=query_prefix, query_suffix=query_suffix)

class WikiPageList(Wrapped):
    """Shows Wiki Page List box"""
    def __init__(self, link):
        if link:
          self.articleurl = link.url
        else:
          self.articleurl = None
        Wrapped.__init__(self)

class GoogleSearchResultsFrame(Wrapped):
    """Shows Google Custom Search box"""
    def __init__(self):
        Wrapped.__init__(self)

class GoogleSearchResults(BoringPage):
    """Receieves search results from Google"""

    def __init__(self, pagename, *a, **kw):
        kw['content'] = GoogleSearchResultsFrame()
        BoringPage.__init__(self, pagename, robots='noindex', *a, **kw)

    def content(self):
      return self.content_stack(self._content)
        # return self.content_stack(self.infobar,
                                  # self.nav_menu, self._content)

class ArticleNavigation(Wrapped):
  """Generates article navigation fragment for the supplied link"""
  def __init__(self, link, author):
    Wrapped.__init__(self, article=link, author=author)

class SearchBar(Wrapped):
    """More detailed search box for /search and /categories pages.
    Displays the previous search as well as info of the elapsed_time
    and num_results if any."""
    def __init__(self, num_results = 0, prev_search = '', elapsed_time = 0, **kw):

        # not listed explicitly in args to ensure it translates properly
        self.header = kw.get('header', _("Previous search"))

        self.prev_search  = prev_search
        self.elapsed_time = elapsed_time

        # All results are approximate unless there are fewer than 10.
        if num_results > 10:
            self.num_results = (num_results / 10) * 10
        else:
            self.num_results = num_results

        Wrapped.__init__(self)


class Frame(Wrapped):
    """Frameset for the FrameToolbar used when a user hits /goto and
    has pref_toolbar set.  The top 30px of the page are dedicated to
    the toolbar, while the rest of the page will show the results of
    following the link."""
    def __init__(self, url='', title='', fullname=''):
        Wrapped.__init__(self, url = url, title = title, fullname = fullname)

class FrameToolbar(Wrapped):
    """The reddit voting toolbar used together with Frame."""
    extension_handling = False
    def __init__(self, link = None, **kw):
        self.title = link.title
        Wrapped.__init__(self, link = link, *kw)



class NewLink(Wrapped):
    """Render the link submission form"""
    def __init__(self, captcha = None, article = '', title= '', subreddits = (), tags = (), sr_id = None):
        Wrapped.__init__(self, captcha = captcha, article = article,
                         title = title, subreddits = subreddits, tags = tags,
                         sr_id = sr_id, notify_on_comment = True,
                         cc_licensed = True)

class EditLink(Wrapped):
    """Render the edit link form"""
    pass

class ShareLink(Wrapped):
    def __init__(self, link_name = "", emails = None):
        captcha = Captcha() if c.user.needs_captcha() else None
        Wrapped.__init__(self, link_name = link_name,
                         emails = c.user.recent_share_emails(),
                         captcha = captcha)

class Share(Wrapped):
    pass

class Mail_Opt(Wrapped):
    pass

class OptOut(Wrapped):
    pass

class OptIn(Wrapped):
    pass


# class UserStats(Wrapped):
#     """For drawing the stats page, which is fetched from the cache."""
#     def __init__(self):
#         Wrapped.__init__(self)
#         cache_stats = cache.get('stats')
#         if cache_stats:
#             top_users, top_day, top_week = cache_stats

#             #lookup user objs
#             uids = []
#             uids.extend(u    for u in top_users)
#             uids.extend(u[0] for u in top_day)
#             uids.extend(u[0] for u in top_week)
#             users = Account._byID(uids, data = True)

#             self.top_users = (users[u]            for u in top_users)
#             self.top_day   = ((users[u[0]], u[1]) for u in top_day)
#             self.top_week  = ((users[u[0]], u[1]) for u in top_week)
#         else:
#             self.top_users = self.top_day = self.top_week = ()


class ButtonEmbed(Wrapped):
    """Generates the JS wrapper around the buttons for embedding."""
    def __init__(self, button = None, width = 100, height=100, referer = "", url = ""):
        Wrapped.__init__(self, button = button, width = width, height = height,
                         referer=referer, url = url)

class ButtonLite(Wrapped):
    """Generates the JS wrapper around the buttons for embedding."""
    def __init__(self, image = None, link = None, url = "", styled = True, target = '_top'):
        Wrapped.__init__(self, image = image, link = link, url = url, styled = styled, target = target)

class Button(Wrapped):
    """the voting buttons, embedded with the ButtonEmbed wrapper, shown on /buttons"""
    extension_handling = False
    def __init__(self, link = None, button = None, css=None,
                 url = None, title = '', score_fmt = None, vote = True, target = "_parent",
                 bgcolor = None, width = 100):
        Wrapped.__init__(self, link = link, score_fmt = score_fmt,
                         likes = link.likes if link else None,
                         button = button, css = css, url = url, title = title,
                         vote = vote, target = target, bgcolor=bgcolor, width=width)

class ButtonNoBody(Button):
    """A button page that just returns the raw button for direct embeding"""
    pass

class ButtonDemoPanel(Wrapped):
    """The page for showing the different styles of embedable voting buttons"""
    pass


class Feedback(Wrapped):
    """The feedback and ad inquery form(s)"""
    def __init__(self, captcha=None, title=None, action='/feedback',
                    message='', name='', email='', replyto='', success = False):
        Wrapped.__init__(self, captcha = captcha, title = title, action = action,
                         message = message, name = name, email = email, replyto = replyto,
                         success = success)


class WidgetDemoPanel(Wrapped):
    """Demo page for the .embed widget."""
    pass

class Socialite(Wrapped):
    """Demo page for the socialite Firefox extension"""
    pass

class Bookmarklets(Wrapped):
    """The bookmarklets page."""
    def __init__(self, buttons=["reddit", "like", "dislike",
                             "save", "serendipity!"]):
        Wrapped.__init__(self, buttons = buttons)



class AdminTranslations(Wrapped):
    """The translator control interface, used for determining which
    user is allowed to edit which translation file and for providing a
    summary of what translation files are done and/or in use."""
    def __init__(self):
        from r2.lib.translation import list_translations
        Wrapped.__init__(self)
        self.translations = list_translations()


class Embed(Wrapped):
    """wrapper for embedding /help into reddit as if it were not on a separate wiki."""
    def __init__(self,content = ''):
        Wrapped.__init__(self, content = content)


class Page_down(Wrapped):
    def __init__(self, **kw):
        message = kw.get('message', _("This feature is currently unavailable. Sorry"))
        Wrapped.__init__(self, message = message)

# Classes for dealing with friend/moderator/contributor/banned lists

# TODO: if there is time, we could roll these Ajaxed classes into the
# JsonTemplates framework...
class Ajaxed():
    """Base class for allowing simple interaction of UserTableItem and
    UserItem classes to be edited via JS and AJax requests.  In
    analogy with Wrapped, this class provides an interface for
    'rendering' dictionary representations of the data which can be
    passed to the client via JSON over AJAX"""
    __slots__ = ['kind', 'action', 'data']

    def __init__(self, kind, action):
        self._ajax = dict(kind=kind,
                          action = None,
                          data = {})

    def for_ajax(self, action = None):
        self._ajax['action'] = action
        self._ajax['data'] = self.ajax_render()
        return self._ajax

    def ajax_render(self, style="html"):
        return {}


class UserTableItem(Wrapped, Ajaxed):
    """A single row in a UserList of type 'type' and of name
    'container_name' for a given user.  The provided list of 'cells'
    will determine what order the different columns are rendered in.
    Currently, this list can consist of 'user', 'sendmessage' and
    'remove'."""
    def __init__(self, user, type, cellnames, container_name, editable):
        self.user, self.type, self.cells = user, type, cellnames
        self.container_name = container_name
        self.name           = "tr_%s_%s" % (user.name, type)
        self.editable       = editable
        Wrapped.__init__(self)
        Ajaxed.__init__(self, 'UserTable', 'add')

    def ajax_render(self, style="html"):
        """Generates a 'rendering' of this item suitable for
        processing by JS for insert or removal from an existing
        UserList"""
        cells = []
        for cell in self.cells:
            r = Wrapped.part_render(self, 'cell_type', cell)
            cells.append(spaceCompress(r))
        return dict(cells=cells, id=self.type, name=self.name)

    def __repr__(self):
        return '<UserTableItem "%s">' % self.user.name

class UserList(Wrapped):
    """base class for generating a list of users"""
    form_title     = ''
    table_title    = ''
    type           = ''
    container_name = ''
    cells          = ('user', 'sendmessage', 'remove')
    _class         = ""

    def __init__(self, editable = True):
        self.editable = editable
        Wrapped.__init__(self)

    def ajax_user(self, user):
        """Convenience method for constructing a UserTableItem
        instance of the user with type, container_name, etc. of this
        UserList instance"""
        return UserTableItem(user, self.type, self.cells, self.container_name,
                             self.editable)

    @property
    def users(self, site = None):
        """Generates a UserTableItem wrapped list of the Account
        objects which should be present in this UserList."""
        uids = self.user_ids()
        if uids:
            users = Account._byID(uids, True, return_dict = False)
            return [self.ajax_user(u) for u in users]
        else:
            return ()

    def user_ids(self):
        """virtual method for fetching the list of ids of the Accounts
        to be listing in this UserList instance"""
        raise NotImplementedError

    @property
    def container_name(self):
        return c.site._fullname

class FriendList(UserList):
    """Friend list on /pref/friends"""
    type = 'friend'

    @property
    def form_title(self):
        return _('Add a friend')

    @property
    def table_title(self):
        return _('Your friends')

    def user_ids(self):
        return c.user.friends

    @property
    def container_name(self):
        return c.user._fullname

class ContributorList(UserList):
    """Contributor list on a restricted/private reddit."""
    type = 'contributor'

    @property
    def form_title(self):
        return _('Add contributor')

    @property
    def table_title(self):
        return _("Contributors to %(reddit)s") % dict(reddit = c.site.name)

    def user_ids(self):
        return c.site.contributors

class ModList(UserList):
    """Moderator list for a reddit."""
    type = 'moderator'

    @property
    def form_title(self):
        return _('Add moderator')

    @property
    def table_title(self):
        return _("Moderators to %(reddit)s") % dict(reddit = c.site.name)

    def user_ids(self):
        return c.site.moderators

class EditorList(UserList):
    """Editor list for a reddit."""
    type = 'editor'

    @property
    def form_title(self):
        return _('Add editor')

    @property
    def table_title(self):
        return _("Editors of %(reddit)s") % dict(reddit = c.site.name)

    def user_ids(self):
        return c.site.editors

class BannedList(UserList):
    """List of users banned from a given reddit"""
    type = 'banned'

    @property
    def form_title(self):
        return _('Ban users')

    @property
    def table_title(self):
        return  _('Banned users')

    def user_ids(self):
        return c.site.banned


class DetailsPage(LinkInfoPage):
    extension_handling= False

    def content(self):
        # TODO: a better way?
        from admin_pages import Details
        return self.content_stack(self.link_listing, Details(link = self.link))

class Cnameframe(Wrapped):
    """The frame page."""
    def __init__(self, original_path, subreddit, sub_domain):
        Wrapped.__init__(self, original_path=original_path)
        if sub_domain and subreddit and original_path:
            self.title = "%s - %s" % (subreddit.title, sub_domain)
            u = UrlParser(subreddit.path + original_path)
            u.hostname = get_domain(cname = False, subreddit = False)
            u.update_query(**request.get.copy())
            u.put_in_frame()
            self.frame_target = u.unparse()
        else:
            self.title = ""
            self.frame_target = None

class PromotePage(Reddit):
    create_reddit_box  = False
    submit_box         = False
    extension_handling = False

    def __init__(self, title, nav_menus = None, *a, **kw):
        buttons = [NamedButton('current_promos', dest = ''),
                   NamedButton('new_promo')]

        menu  = NavMenu(buttons, title='show', base_path = '/promote',
                        type='flatlist')

        if nav_menus:
            nav_menus.insert(0, menu)
        else:
            nav_menus = [menu]

        Reddit.__init__(self, title, nav_menus = nav_menus, *a, **kw)


class PromotedLinks(Wrapped):
    def __init__(self, current_list, *a, **kw):
        self.things = current_list

        Wrapped.__init__(self, datefmt = datefmt, *a, **kw)

class PromoteLinkForm(Wrapped):
    def __init__(self, sr = None, link = None, listing = '',
                 timedeltatext = '', *a, **kw):
        Wrapped.__init__(self, sr = sr, link = link,
                         datefmt = datefmt,
                         timedeltatext = timedeltatext,
                         listing = listing,
                         *a, **kw)

class FeedLinkBar(Wrapped):
    def __init__(self, request_path, *a, **kw):
        self.request_path = request_path
        Wrapped.__init__(self, *a, **kw)

class AboutBox(Wrapped): pass

class FeedBox(Wrapped):
    def __init__(self, feed_urls, *a, **kw):
        self.feed_urls = feed_urls
        Wrapped.__init__(self, *a, **kw)

class RecentWikiEditsBox(Wrapped):
    def __init__(self, feed_url, *a, **kw):
        self.feed_url = feed_url
        Wrapped.__init__(self, *a, **kw)

class SiteMeter(Wrapped):
    def __init__(self, codename, *a, **kw):
        self.codename = codename
        Wrapped.__init__(self, *a, **kw)


class PollWrapper(Wrapped):
    def __init__(self, outer_thing, outer_body, voted_on_all, *a, **kw):
        Wrapped.__init__(self, *a, **kw)
        self.outer_thing = outer_thing
        self.outer_body = outer_body
        self.voted_on_all = voted_on_all

class PollBallot(Wrapped):
    def __init__(self, poll, *a, **kw):
        self.poll = poll
        Wrapped.__init__(self, *a, **kw)

class PollResults(Wrapped):
    def __init__(self, poll, *a, **kw):
        self.poll = poll
        Wrapped.__init__(self, *a, **kw)

class MultipleChoicePollBallot(PollBallot):
    def __init__(self, poll, *a, **kw):
        PollBallot.__init__(self, poll, *a, **kw)

class MultipleChoicePollResults(PollResults):
    def __init__(self, poll, *a, **kw):
        PollResults.__init__(self, poll, *a, **kw)

class ScalePollBallot(PollBallot):
    def __init__(self, poll, *a, **kw):
        PollBallot.__init__(self, poll, *a, **kw)

class ScalePollResults(PollResults):
    def __init__(self, poll, *a, **kw):
        PollResults.__init__(self, poll, *a, **kw)

class ProbabilityPollBallot(PollBallot):
    def __init__(self, poll, *a, **kw):
        PollBallot.__init__(self, poll, *a, **kw)

class ProbabilityPollResults(PollResults):
    def __init__(self, poll, *a, **kw):
        PollResults.__init__(self, poll, *a, **kw)

class NumberPollBallot(PollBallot):
    def __init__(self, poll, *a, **kw):
        PollBallot.__init__(self, poll, *a, **kw)

class NumberPollResults(PollResults):
    def __init__(self, poll, *a, **kw):
        PollResults.__init__(self, poll, *a, **kw)


class UpcomingMeetups(SpaceCompressedWrapped):
    def __init__(self, location, max_distance, *a, **kw):
        meetups = Meetup.upcoming_meetups_near(location, max_distance, 2)
        Wrapped.__init__(self, meetups=meetups, location=location, *a, **kw)

class MeetupsMap(Wrapped):
    def __init__(self, location, max_distance, *a, **kw):
        meetups = Meetup.upcoming_meetups_near(location, max_distance)
        Wrapped.__init__(self, meetups=meetups, location=location, *a, **kw)

class NotEnoughKarmaToPost(Wrapped):
          pass

class ShowMeetup(Wrapped):
    """docstring for ShowMeetup"""
    def __init__(self, meetup, **kw):
        # title_params = {'title':_force_unicode(meetup.title), 'site' : c.site.title}
        # title = strings.show_meetup_title % title_params
        Wrapped.__init__(self, meetup = meetup, **kw)

class NewMeetup(Wrapped):
    def __init__(self, *a, **kw):
        Wrapped.__init__(self, *a, **kw)

class EditMeetup(Wrapped):
    pass

class MeetupIndexPage(Reddit):
  def __init__(self, **context):
    self.meetups = context.get("content", None)
    Reddit.__init__(self, **context)

  def content(self):
    return MeetupIndex(self.meetups)

class MeetupIndex(Wrapped):
  def __init__(self, meetups = [], *a, **kw):
    self.meetups = meetups
    Wrapped.__init__(self, *a, **kw)

  def meetups(self):
    return self.meetups

class MeetupNotification(Wrapped): pass


class WikiPageInline(Wrapped): pass

class WikiPage(Reddit):
    def __init__(self, name, page, skiplayout, **context):
        wikiPage = WikiPageCached(page)
        html = wikiPage.content()
        self.pagename = wikiPage.title()
        content = WikiPageInline(html=html, name=name, skiplayout=skiplayout,
            title=self.pagename, wiki_url=page['url'])
        Reddit.__init__(self,
                        content = content,
                        title = self.pagename,
                        space_compress=False,
                        **context)

########NEW FILE########
__FILENAME__ = promote
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from __future__ import with_statement

from r2.models import *
from r2.lib.memoize import memoize, clear_memo

from datetime import datetime

promoted_memo_lifetime = 30
promoted_memo_key = 'cached_promoted_links'
promoted_lock_key = 'cached_promoted_links_lock'

def promote(thing, subscribers_only = False, promote_until = None,
            disable_comments = False):

    thing.promoted = True
    thing.promoted_on = datetime.now(g.tz)

    if c.user:
        thing.promoted_by = c.user._id

    if promote_until:
        thing.promote_until = promote_until

    if disable_comments:
        thing.disable_comments = True

    if subscribers_only:
        thing.promoted_subscribersonly = True

    thing._commit()

    with g.make_lock(promoted_lock_key):
        promoted = get_promoted_direct()
        promoted.append(thing._fullname)
        set_promoted(promoted)

def unpromote(thing):
    thing.promoted = False
    thing.unpromoted_on = datetime.now(g.tz)
    thing.promote_until = None
    thing._commit()

    with g.make_lock(promoted_lock_key):
        promoted = [ x for x in get_promoted_direct()
                     if x != thing._fullname ]

        set_promoted(promoted)

def set_promoted(link_names):
    # caller is assumed to execute me inside a lock if necessary
    g.permacache.set(promoted_memo_key, link_names)

    clear_memo(promoted_memo_key)

@memoize(promoted_memo_key, time = promoted_memo_lifetime)
def get_promoted():
    # does not lock the list to return it, so (slightly) stale data
    # will be returned if called during an update rather than blocking
    return get_promoted_direct()

def get_promoted_direct():
    return g.permacache.get(promoted_memo_key, [])

def expire_promoted():
    """
        To be called periodically (e.g. by `cron') to clean up
        promoted links past their expiration date
    """
    with g.make_lock(promoted_lock_key):
        link_names = set(get_promoted_direct())
        links = Link._by_fullname(link_names, data=True, return_dict = False)

        link_names = []
        expired_names = []

        for x in links:
            if (not x.promoted
                or x.promote_until and x.promote_until < datetime.now(g.tz)):
                g.log.info('Unpromoting %s' % x._fullname)
                unpromote(x)
                expired_names.append(x._fullname)
            else:
                link_names.append(x._fullname)

        set_promoted(link_names)

    return expired_names

def get_promoted_slow():
    # to be used only by a human at a terminal
    with g.make_lock(promoted_lock_key):
        links = Link._query(Link.c.promoted == True,
                            data = True)
        link_names = [ x._fullname for x in links ]

        set_promoted(link_names)

    return link_names

#deprecated
def promote_builder_wrapper(alternative_wrapper):
    def wrapper(thing):
        if isinstance(thing, Link) and thing.promoted:
            w = Wrapped(thing)
            w.render_class = PromotedLink
            w.rowstyle = 'promoted link'

            return w
        else:
            return alternative_wrapper(thing)
    return wrapper



########NEW FILE########
__FILENAME__ = rancode
import random
import string
 
# Returns a random alphanumeric string of length 'length'
def random_key(length):
    return ''.join(random.sample(string.uppercase + string.digits, length))

########NEW FILE########
__FILENAME__ = recommendation
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from pylons import g, c
from r2.models import *
from r2.lib.utils import to36
from datetime import datetime, timedelta

from r2.lib.db import tdb_sql as tdb
import sqlalchemy as sa


def sgn(x):
    return 1 if x > 0 else 0 if x == 0 else -1

def get_recommended(userid, age = 2, sort='relevance', num_users=10):
    u = get_users_for_user(userid)[:num_users]
    if not u: return []

    voter = Vote.rels[(Account, Link)]

    votertable = tdb.rel_types_id[voter._type_id].rel_table[0]
    acct_col = votertable.c.thing1_id
    link_col = votertable.c.thing2_id
    date_col = votertable.c.date
    count = sa.func.count(acct_col)

    linktable = tdb.rel_types_id[voter._type_id].rel_table[2]
#    dlinktable, linktable = tdb.types_id[Link._type_id].data_table
    link_id_col = linktable.c.thing_id

    query = [sa.or_(*[acct_col == x for x in u]),
             date_col > datetime.now(g.tz)-timedelta(age)]
    cols = [link_col, count]

    if sort == 'new':
        sort = 'date'
    elif sort == 'top':
        sort = 'score'

    if sort and sort != 'relevance':
        query.append(link_id_col == link_col)
        s = tdb.translate_sort(linktable, sort)
        order = [sa.desc(s), sa.desc(link_id_col)]
        cols = [link_id_col, count]
        group_by = [link_id_col, s]
    else:
        order = [sa.desc(count), sa.desc(link_col)]
        group_by = link_col

#    #TODO: wish I could just use query_rules
#    if c.user and c.user.subreddits:
#        query.append(dlinktable.c.thing_id == linktable.c.thing_id)
#        q = sa.and_(dlinktable.c.key == 'sr_id',
#                    sa.or_(*[dlinktable.c.value == x
#                             for x in c.user.subreddits]))
#        query.append(q)

    res = sa.select(cols, sa.and_(*query),
                    group_by=group_by,
                    order_by=order).execute()


    prefix = "t%s" % to36(Link._type_id)
    return ["%s_%s" % (prefix, to36(x)) for x, y in res.fetchall()]
    


def get_users_for_user(userid, dateWeight = 0.1):
    e = load_from_mc(userid, True, dateWeight)
    u = []
    if e:
        users = dict((e[i], e[i+1]) for i in range(0, len(e), 2))
        u = users.keys()
        u.sort(lambda x, y: sgn(users[y] - users[x]))
    return u

def grab_int(str, start, end):
    rval = 0;
    entry = str[start:end]
    for x in entry[::-1]:
        rval = (rval << 8) | (ord(x) & 255)
    return rval;

def load_from_mc(userid, positiveOnly = True, dateWeight = 0):
   cachedEntry = g.rec_cache.get("recommend_" + str(userid))
   rval = []
   resortingHash = {}
   min_id = None;
   max_id = None

   if cachedEntry:
       record_size = ord(cachedEntry[0])
       num_records = grab_int(cachedEntry, 1, record_size)
       offset = record_size;
       for i in range(0, num_records):
           key = grab_int(cachedEntry, 
                          i*record_size + offset,
                          (i+1)*record_size + offset-1)
           value = float(ord(cachedEntry[(i+1)*record_size + offset-1]))/128.

           if not min_id or key < min_id:
               min_id = key
           if not max_id or key > max_id:
               max_id = key

           if value > 1: value -= 2
           if value < 0 and positiveOnly: continue
           rval += [key, value]
           resortingHash[key] = value
       
       if dateWeight > 0 and min_id != max_id:
           arts = resortingHash.keys()
           def sortingFunc(x, y):
               qx = ( dateWeight * float(x - min_id) / (max_id - min_id) +
                      (1-dateWeight) * resortingHash[x])
               qy = ( dateWeight * float(y - min_id) / (max_id - min_id) +
                      (1-dateWeight) * resortingHash[y])
               return cmp(qy, qx)
           arts.sort(sortingFunc)
           rval = []
           for x in arts:
               rval += [x, resortingHash[x]]

   return rval

def getQualityForUser(userid, min = 0, max = 100):
    cachedEntry = load_from_mc(userid, False)
    rhash = {}
    for i in range(0,len(cachedEntry)/2):
        rhash[cachedEntry[2*i]] = (max - min)*cachedEntry[2*i+1] + min
    return rhash

########NEW FILE########
__FILENAME__ = rising
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from pylons import g, c
from r2.models.link import Link, Subreddit
from r2.lib import utils
from r2.lib import count

from datetime import datetime

cache = g.cache

def calc_rising():
    sr_count = count.get_link_counts()
    link_count = dict((k, v[0]) for k,v in sr_count.iteritems())
    link_names = Link._by_fullname(sr_count.keys(), data=True)

    #max is half the average of the top 10 counts
    counts = link_count.values()
    counts.sort(reverse=True)
    maxcount = sum(counts[:10]) / 20
    
    #prune the list
    rising = [(n, link_names[n].sr_id)
              for n in link_names.keys() if link_count[n] < maxcount]

    cur_time = datetime.now(g.tz)

    def score(pair):
        name = pair[0]
        link = link_names[name]
        hours = (cur_time - link._date).seconds / 3600 + 1
        return float(link._ups) / (max(link_count[name], 1) * hours)

    def r(x):
        return 1 if x > 0 else -1 if x < 0 else 0

    rising.sort(lambda x, y: r(score(y) - score(x)))
    return rising

def set_rising():
    rising = calc_rising()
    cache.set('rising', rising)

def get_rising(sr):
    #get the sr_ids
    sr_ids = sr.rising_srs()
    rising = cache.get('rising') or ()
    return [p[0] for p in filter(lambda pair: pair[1] in sr_ids, rising)]

########NEW FILE########
__FILENAME__ = rpc
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
import socket, cPickle as pickle
from threading import Thread
from SocketServer import DatagramRequestHandler, StreamRequestHandler, ThreadingMixIn, UDPServer, TCPServer

class CustomThreadingMixIn(ThreadingMixIn):
    """Mix-in class to handle each request in a new thread."""
    
    def __init__(self, thread_class = Thread):
        self.thread_class = thread_class

    def process_request(self, request, client_address):
        """Start a new thread to process the request."""
        t = self.thread_class(target = self.process_request_thread,
                              args = (request, client_address))
        if self.daemon_threads:
            t.setDaemon (1)
            t.start()
            

class Responses:
    OK, ERROR = range(2)

class SimpleHandler:
    def handle(self):
        try:
            fn_name, a, kw = pickle.load(self.rfile)
            fn = getattr(self.server.container, fn_name)
            res = (Responses.OK, fn(*a, **kw))
        except Exception, e:
            res = (Responses.ERROR, e)
        try:
            self.wfile.write(pickle.dumps(res, -1))
        except:
            res = (Responses.ERROR, 'Error while pickling.' )
            self.wfile.write(pickle.dumps(res, -1))

class SimpleUDPHandler(SimpleHandler, DatagramRequestHandler): pass
class SimpleTCPHandler(SimpleHandler, StreamRequestHandler): pass

class ThreadedUDPServer(CustomThreadingMixIn, UDPServer): 
    def __init__(self, server_address, RequestHandlerClass, container,
                 thread_class = Thread):
        UDPServer.__init__(self, server_address, RequestHandlerClass)
        CustomThreadingMixIn.__init__(self, thread_class)
        self.container = container
        self.daemon_threads = True

class ThreadedTCPServer(CustomThreadingMixIn, TCPServer): 
    def __init__(self, server_address, RequestHandlerClass, container,
                 thread_class = Thread):
        self.allow_reuse_address = True
        TCPServer.__init__(self, server_address, RequestHandlerClass)
        CustomThreadingMixIn.__init__(self, thread_class)
        self.container = container
        self.daemon_threads = True


class Server:
    def __init__(self, container, addr='', port=5000,
                 daemon=True, tcp=False, thread_class = Thread):
        if tcp:
            self.s = ThreadedTCPServer((addr, port), SimpleTCPHandler,
                                       container, thread_class)
        else:
            self.s = ThreadedUDPServer((addr, port), SimpleUDPHandler,
                                       container, thread_class)

        self.handle_thread = thread_class(target = self.s.serve_forever)
        self.handle_thread.setDaemon(daemon)
        self.handle_thread.start()


class RemoteCall:
    def __init__(self, client, response_required):
        self.client = client
        self.response_required = response_required

    def __getattr__(self, attr):
        def fn(*a, **kw):
            return self.client.send(self.response_required, attr, a, kw)
        return fn

class Client:
    def __init__(self, host='localhost', port=5000, tcp=False):
        self.conninfo = (host, port)
        self.call = RemoteCall(self, True)
        self.call_nr = RemoteCall(self, False)
        self.tcp = tcp

    def send(self, response_required, fn, a, kw):
        msg = pickle.dumps((fn, a, kw), -1)

        #get socket + send
        if self.tcp:
            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            try:
                s.connect(self.conninfo)
            except:
                return
            s.send(msg)
        else:
            s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            s.settimeout(10)
            s.sendto(msg, self.conninfo)

        if response_required:
            infile = s.makefile('rb')
            error_code, res = pickle.load(infile)
            #close
            try:
                if self.tcp: s.close()
            except: pass
            #return
            if error_code == Responses.OK:
                return res
            else:
                raise Exception, res
        elif self.tcp:
            try:
                s.close()
            except: pass

class TH:
    def add(self, x,y):
        return x + y

    def echo(self, str):
        return str

#s = Server(TH)
#c = Client()
#print c.call.add(1,2)

def test_length(client):
    x = 0
    while True:
        print len(client.call.echo(['x' for i in range(x)]))
        x += 100

def perf_test(client):
    for x in range(1000):
        client.call.echo('test')

########NEW FILE########
__FILENAME__ = s3cp
#!/usr/bin/env python

# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################

import base64, hmac, hashlib, os, sys, getopt
from datetime import datetime
from pylons import g,config

KEY_ID = g.S3KEY_ID
SECRET_KEY = g.S3SECRET_KEY

class S3Exception(Exception): pass

def make_header(verb, date, amz_headers, resource, content_type):
    content_md5 = ''

    #amazon headers
    lower_head = dict((key.lower(), val)
                      for key, val in amz_headers.iteritems())
    keys = lower_head.keys()
    keys.sort()
    amz_lst = ['%s:%s' % (key, lower_head[key]) for key in keys]
    amz_str = '\n'.join(amz_lst)

    s = '\n'.join((verb,
                   content_md5,
                   content_type,
                   date,
                   amz_str,
                   resource))

    h = hmac.new(SECRET_KEY, s, hashlib.sha1)
    return base64.encodestring(h.digest()).strip()
                   
def send_file(filename, resource, content_type, acl, rate, meter):
    date = datetime.utcnow().strftime("%a, %d %b %Y %X GMT")
    amz_headers = {'x-amz-acl': acl}

    auth_header = make_header('PUT', date, amz_headers, resource, content_type)

    params = ['-T', filename,
              '-H', 'x-amz-acl: %s' % amz_headers['x-amz-acl'],
              '-H', 'Authorization: AWS %s:%s' % (KEY_ID, auth_header),
              '-H', 'Date: %s' % date]

    if content_type:
        params.append('-H')
        params.append('Content-Type: %s' % content_type)

    if rate:
        params.append('--limit-rate')
        params.append(rate)

    if meter:
        params.append('-o')
        params.append('s3cp.output')
    else:
        params.append('-s')

    params.append('https://s3.amazonaws.com%s' % resource)

    exit_code = os.spawnlp(os.P_WAIT, 'curl', 'curl', *params)
    if exit_code:
        raise S3Exception(exit_code)

               
if __name__ == '__main__':
    options = "a:c:l:m"
    try:
        opts, args = getopt.getopt(sys.argv[1:], options)
    except:
        sys.exit(2)
        
    opts = dict(opts)

    send_file(args[0], args[1],
              opts.get('-c', ''),
              opts.get('-a', 'private'),
              opts.get('-l'),
              opts.has_key('-m'))

########NEW FILE########
__FILENAME__ = scraper
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################

from pylons import g
from r2.lib import utils
from r2.lib.memoize import memoize

from urllib2 import Request, HTTPError, URLError, urlopen
from httplib import InvalidURL
import urlparse, re, urllib, logging, StringIO, logging
import Image, ImageFile, math
from BeautifulSoup import BeautifulSoup

log = g.log
useragent = g.useragent

chunk_size = 1024
thumbnail_size = 70, 70

def image_to_str(image):
    s = StringIO.StringIO()
    image.save(s, image.format)
    s.seek(0)
    return s.read()

def str_to_image(s):
    s = StringIO.StringIO(s)
    s.seek(0)
    image = Image.open(s)
    return image

def prepare_image(image):
    image = square_image(image)
    image.thumbnail(thumbnail_size, Image.ANTIALIAS)
    return image

def image_entropy(img):
    """calculate the entropy of an image"""
    hist = img.histogram()
    hist_size = sum(hist)
    hist = [float(h) / hist_size for h in hist]

    return -sum([p * math.log(p, 2) for p in hist if p != 0])

def square_image(img):
    """if the image is taller than it is wide, square it off. determine
    which pieces to cut off based on the entropy pieces."""
    x,y = img.size
    while y > x:
        #slice 10px at a time until square
        slice_height = min(y - x, 10)

        bottom = img.crop((0, y - slice_height, x, y))
        top = img.crop((0, 0, x, slice_height))

        #remove the slice with the least entropy
        if image_entropy(bottom) < image_entropy(top):
            img = img.crop((0, 0, x, y - slice_height))
        else:
            img = img.crop((0, slice_height, x, y))

        x,y = img.size

    return img

def clean_url(url):
    """url quotes unicode data out of urls"""
    s = url
    url = url.encode('utf8')
    url = ''.join([urllib.quote(c) if ord(c) >= 127 else c for c in url])
    return url

def fetch_url(url, referer = None, retries = 1, dimension = False):
    cur_try = 0
    log.debug('fetching: %s' % url)
    nothing = None if dimension else (None, None)
    url = clean_url(url)
    #just basic urls
    if not url.startswith('http://'):
        return nothing
    while True:
        try:
            req = Request(url)
            if useragent:
                req.add_header('User-Agent', useragent)
            if referer:
                req.add_header('Referer', referer)

            open_req = urlopen(req)

            #if we only need the dimension of the image, we may not
            #need to download the entire thing
            if dimension:
                content = open_req.read(chunk_size)
            else:
                content = open_req.read()
            content_type = open_req.headers.get('content-type')

            if not content_type:
                return nothing

            if 'image' in content_type:
                p = ImageFile.Parser()
                new_data = content
                while not p.image and new_data:
                    p.feed(new_data)
                    new_data = open_req.read(chunk_size)
                    content += new_data

                #return the size, or return the data
                if dimension and p.image:
                    return p.image.size
                elif dimension:
                    return nothing
            elif dimension:
                #expected an image, but didn't get one
                return nothing

            return content_type, content

        except (URLError, HTTPError, InvalidURL), e:
            cur_try += 1
            if cur_try >= retries:
                log.debug('error while fetching: %s referer: %s' % (url, referer))
                log.debug(e)
                return nothing
        finally:
            if 'open_req' in locals():
                open_req.close()

@memoize('media.fetch_size')
def fetch_size(url, referer = None, retries = 1):
    return fetch_url(url, referer, retries, dimension = True)

class Scraper:
    def __init__(self, url):
        self.url = url
        self.content = None
        self.content_type = None
        self.soup = None

    def download(self):
        self.content_type, self.content = fetch_url(self.url)
        if self.content_type and 'html' in self.content_type and self.content:
            self.soup = BeautifulSoup(self.content)

    def image_urls(self):
        #if the original url was an image, use that
        if 'image' in self.content_type:
            yield self.url
        elif self.soup:
            images = self.soup.findAll('img', src = True)
            for i in images:
                image_url = urlparse.urljoin(self.url, i['src'])
                yield image_url

    def largest_image_url(self):
        if not self.content:
            self.download()

        #if download didn't work
        if not self.content or not self.content_type:
            return None

        max_area = 0
        max_url = None

        for image_url in self.image_urls():
            size = fetch_size(image_url, referer = self.url)
            if not size:
                continue

            area = size[0] * size[1]

            #ignore little images
            if area < 5000:
                log.debug('ignore little %s' % image_url)
                continue

            #ignore excessively long/wide images
            if max(size) / min(size) > 1.5:
                log.debug('ignore dimensions %s' % image_url)
                continue

            if area > max_area:
                max_area = area
                max_url = image_url

        return max_url

    def thumbnail(self):
        image_url = self.largest_image_url()
        if image_url:
            content_type, image_str = fetch_url(image_url, referer = self.url)
            if image_str:
                image = str_to_image(image_str)
                try:
                    image = prepare_image(image)
                except IOError, e:
                    #can't read interlaced PNGs, ignore
                    if 'interlaced' in e.message:
                        return
                    raise
                return image

    def media_object(self):
        return None

class MediaScraper(Scraper):
    media_template = ""
    thumbnail_template = ""
    video_id_rx = None
    
    def __init__(self, url):
        m = self.video_id_rx.match(url)
        if m:
            self.video_id = m.groups()[0]
        else:
            #if we can't find the id just treat it like a normal page
            log.debug('reverting to regular scraper: %s' % url)
            self.__class__ = Scraper
        Scraper.__init__(self, url)

    def largest_image_url(self):
        return self.thumbnail_template.replace('$video_id', self.video_id)

    def media_object(self):
        return self.media_template.replace('$video_id', self.video_id)
    
def youtube_in_google(google_url):
    h = Scraper(google_url)
    h.download()
    try:
        youtube_url = h.soup.find('div', 'original-text').findNext('a')['href']
        log.debug('%s is really %s' % (google_url, youtube_url))
        return youtube_url
    except AttributeError, KeyError:
        pass

def make_scraper(url):
    domain = utils.domain(url)
    scraper = Scraper
    for suffix, cls in scrapers.iteritems():
        if domain.endswith(suffix):
            scraper = cls
            break
    
    #sometimes youtube scrapers masquerade as google scrapers
    if scraper == GootubeScraper:
        youtube_url = youtube_in_google(url)
        if youtube_url:
            return make_scraper(youtube_url)
    return scraper(url)


########## site-specific video scrapers ##########

#Youtube
class YoutubeScraper(MediaScraper):
    media_template = '<object width="425" height="350"><param name="movie" value="http://www.youtube.com/v/$video_id"></param><param name="wmode" value="transparent"></param><embed src="http://www.youtube.com/v/$video_id" type="application/x-shockwave-flash" wmode="transparent" width="425" height="350"></embed></object>'
    thumbnail_template = 'http://img.youtube.com/vi/$video_id/default.jpg'
    video_id_rx = re.compile('.*v=([A-Za-z0-9-_]+).*')

#Metacage
class MetacafeScraper(MediaScraper):
    media_template = '<embed src="$video_id" width="400" height="345" wmode="transparent" pluginspage="http://www.macromedia.com/go/getflashplayer" type="application/x-shockwave-flash"> </embed>'
    video_id_rx = re.compile('.*/watch/([^/]+)/.*')

    def media_object(self):
        if not self.soup:
            self.download()

        if self.soup:
            video_url =  self.soup.find('link', rel = 'video_src')['href']
            return self.media_template.replace('$video_id', video_url)

    def largest_image_url(self):
        if not self.soup:
            self.download()

        if self.soup:
            return self.soup.find('link', rel = 'image_src')['href']

#Google Video
gootube_thumb_rx = re.compile(".*thumbnail:\s*\'(http://[^/]+/ThumbnailServer2[^\']+)\'.*", re.IGNORECASE | re.S)
class GootubeScraper(MediaScraper):
    media_template = '<embed style="width:400px; height:326px;" id="VideoPlayback" type="application/x-shockwave-flash" src="http://video.google.com/googleplayer.swf?docId=$video_id&hl=en" flashvars=""> </embed>'
    video_id_rx = re.compile('.*videoplay\?docid=([A-Za-z0-9-_]+).*')    

    def largest_image_url(self):
        if not self.content:
            self.download()

        if not self.content:
            return None

        m = gootube_thumb_rx.match(self.content)
        if m:
            image_url = m.groups()[0]
            image_url = utils.safe_eval_str(image_url)
            return image_url

scrapers = {'youtube.com': YoutubeScraper,
            'video.google.com': GootubeScraper,
            'metacafe.com': MetacafeScraper}

def test():
    from r2.lib.pool2 import WorkQueue
    jobs = []
    f = open('/tmp/testurls.txt')
    for url in f:
        if url.startswith('#'):
            continue
        if url.startswith('/info'):
            continue
        
        def make_job(url):
            def fetch(url):
                print 'START', url
                url = url.strip()
                h = make_scraper(url)
                image_url = h.largest_image_url()
                print 'DONE', image_url
            return lambda: fetch(url)

        jobs.append(make_job(url))

    print jobs[0]()
    #wq = WorkQueue(jobs)
    #wq.start()            

if __name__ == '__main__':
    test()

########NEW FILE########
__FILENAME__ = set_reddit_pops
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.models import Subreddit
from r2.lib.db.operators import desc
from r2.lib import count
    
def run():
    sr_counts = count.get_sr_counts()
    names = [k for k, v in sr_counts.iteritems() if v != 0]
    srs = Subreddit._by_fullname(names)
    for name in names:
        sr,c = srs[name], sr_counts[name]
        if c != sr._downs and c > 0:
            sr._downs = max(c, 0)
            sr._commit()
    count.clear_sr_counts(names)

########NEW FILE########
__FILENAME__ = solrsearch
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
"""
    Module for communication reddit-level communication with
    Solr. Contains functions for indexing (`reindex_all`, `changed`)
    and searching (`search_things`). Uses pysolr (placed in r2.lib)
    for lower-level communication with Solr
"""

from __future__ import with_statement

from r2.models import *
from r2.lib.contrib import pysolr
from r2.lib.contrib.pysolr import SolrError
from r2.lib.utils import timeago, set_emptying_cache, IteratorChunker
from r2.lib.utils import psave, pload, unicode_safe, tup
from r2.lib.cache import SelfEmptyingCache
from Queue import Queue
from threading import Thread
import time
from datetime import datetime, date
from time import strftime
from pylons import g,config

## Changes to the list of searchable languages will require changes to
## Solr's configuration (specifically, the fields that are searched)
searchable_langs    = set(['dk','nl','en','fi','fr','de','it','no','nn','pt',
                           'ru','es','sv','zh','ja','ko','cs','el','th'])

## Adding types is a matter of adding the class to indexed_types here,
## adding the fields from that type to search_fields below, and adding
## those fields to Solr's configuration
indexed_types       = (Subreddit, Link)

class Field(object):
    """
       Describes a field of a Thing that is searchable by Solr. Used
       by `search_fields` below
    """
    def __init__(self, name, thing_attr_func = None, store = True,
                 tokenize=False, is_number=False, reverse=False,
                 is_date = False):
        self.name = name
        self.thing_attr_func = self.make_extractor(thing_attr_func)

    def make_extractor(self,thing_attr_func):
        if not thing_attr_func:
            return self.make_extractor(self.name)
        elif isinstance(thing_attr_func,str):
            return (lambda x: getattr(x,thing_attr_func))
        else:
            return thing_attr_func

    def extract_from(self,thing):
        return self.thing_attr_func(thing)

class ThingField(Field):
    """
        ThingField('field_name',Author,'author_id','name')
        is like:
          Field(name, lambda x: Author._byID(x.author_id,data=True).name)
        but faster because lookups are done in batch
    """
    def __init__(self,name,cls,id_attr,lu_attr_name):
        self.name = name

        self.cls          = cls          # the class of the looked-up object
        self.id_attr      = id_attr      # the attr of the source obj used to find the dest obj
        self.lu_attr_name = lu_attr_name # the attr of the dest class that we want to return

    def __str__(self):
        return ("<ThingField: (%s,%s,%s,%s)>"
                % (self.name,self.cls,self.id_attr,self.lu_attr_name))

def domain_permutations(s):
    """
      Takes a domain like `www.reddit.com`, and returns a list of ways
      that a user might search for it, like:
      * www
      * reddit
      * com
      * www.reddit.com
      * reddit.com
      * com
    """
    ret = []
    r = s.split('.')

    for x in xrange(len(r)):
        ret.append('.'.join(r[x:len(r)]))
    for x in r:
        ret.append(x)

    return set(ret)

# Describes the fields of Thing objects and subclasses that are passed
# to Solr for indexing. All must have a 'contents' field, since that
# will be used for language-agnostic searching, and will be copied
# into contents_en, contents_eo, et (see `tokenize_things` for a
# discussion of multi-language search. The 'boost' field is a
# solr-magic field that ends up being an attribute on the <doc>
# message (rather than a field), and is used to do an index-time boost
# (this magic is done in pysolr.dor_to_elemtree)
search_fields={Thing:     (Field('fullname', '_fullname'),
                           Field('date', '_date',   is_date = True, reverse=True),
                           Field('lang'),
                           Field('ups',   '_ups',   is_number=True, reverse=True),
                           Field('downs', '_downs', is_number=True, reverse=True),
                           Field('spam','_spam'),
                           Field('deleted','_deleted'),
                           Field('hot', lambda t: t._hot*1000, is_number=True, reverse=True),
                           Field('controversy', '_controversy', is_number=True, reverse=True),
                           Field('points', lambda t: (t._ups - t._downs), is_number=True, reverse=True)),
               Subreddit: (Field('contents',
                                 lambda s: ' '.join([unicode_safe(s.name),
                                                     unicode_safe(s.title),
                                                     unicode_safe(s.description),
                                                     unicode_safe(s.firsttext)]),
                                 tokenize = True),
                           Field('boost', '_downs'),
                           #Field('title'),
                           #Field('firsttext'),
                           #Field('description'),
                           #Field('over_18'),
                           #Field('sr_type','type'),
                           ),
               Link:      (Field('contents','title', tokenize = True),
                           Field('boost', lambda t: int(t._hot*1000),
                                 # yes, it's a copy of 'hot'
                                 is_number=True, reverse=True),
                           Field('author_id'),
                           ThingField('author',Account,'author_id','name'),
                           ThingField('subreddit',Subreddit,'sr_id','name'),
                           #ThingField('reddit',Subreddit,'sr_id','name'),
                           Field('sr_id'),
                           Field('url', tokenize = True),
                           #Field('domain',
                           #      lambda l: domain_permutations(domain(l.url))),
                           Field('site',
                                 lambda l: domain_permutations(domain(l.url))),
                           #Field('is_self','is_self'),
                           ),
               Comment:   (Field('contents', 'body', tokenize = True),
                           Field('boost', lambda t: int(t._hot*1000),
                                 # yes, it's a copy of 'hot'
                                 is_number=True, reverse=True),
                           ThingField('author',Account,'author_id','name'),
                           ThingField('subreddit',Subreddit,'sr_id','name'))}
                           #ThingField('reddit',Subreddit,'sr_id','name'))}

def tokenize_things(things,return_dict=False):
    """
        Here, we take a list of things, and return a list of
        dictionaries of fields, which will be sent to Solr. We take
        the `search_fields` dictionary above, and look for all classes
        for which each Thing is an instance (that is, a Comment will
        pick up fields for Thing as well as Comment), and extract the
        given fields. All tokenised Things are expected to have a
        'contents' attribute. That field is then copied to
        contents_XX, where XX is your two-letter language code, which
        becomes your default search field. Those language-specific
        fields are also set up with the proper language-stemming and
        tokenisers on Solr's end (in config/schema.xml), which allows
        for language-specific searching
    """
    global search_fields

    batched_classes = {}
    ret = {}
    for thing in things:
        try:
            t = {'type': []}
            for cls in ((thing.__class__,) + thing.__class__.__bases__):
                t['type'].append(cls.__name__.lower())
                
                if cls in search_fields:
                    for field in search_fields[cls]:
                        if field.__class__ == Field:
                            try:
                                val = field.extract_from(thing)
                                if val != None and val != '':
                                    t[field.name] = val
                            except AttributeError,e:
                                print e

                        elif field.__class__ == ThingField:
                            if not field.cls in batched_classes:
                                batched_classes[field.cls] = []
                            batched_classes[field.cls].append((thing,field))

            # copy 'contents' to ('contents_%s' % lang) and contents_ws
            t[lang_to_fieldname(thing.lang)] = t['contents']
            t['contents_ws'] = t['contents']

            ret[thing._fullname] = t
        except AttributeError,e:
            print e
        except KeyError,e:
            print e

    # batched_classes should now be a {cls: [(Thing,ThingField)]}.
    # This ugliness is to make it possible to batch Thing lookups, as
    # they were accounting for most of the indexing time
    for cls in batched_classes:
        ids = set()
        for (thing,field) in batched_classes[cls]:
            # extract the IDs
            try:
                id = getattr(thing,field.id_attr)
                ids.add(id)
            except AttributeError,e:
                print e
        found_batch = cls._byID(ids,data=True,return_dict=True)

        for (thing,field) in batched_classes[cls]:
            try:
                id = getattr(thing,field.id_attr)
                ret[thing._fullname][field.name] = (
                    getattr(found_batch[id],field.lu_attr_name))
            except AttributeError,e:
                print e
            except KeyError,e:
                print e

    return ret if return_dict else ret.values()

def lang_to_fieldname(l):
    """
        Returns the field-name for the given language, or `contents`
        if it isn't found
    """
    global searchable_langs

    code = l[:2]

    if code in searchable_langs:
        return ("contents_%s" % code)
    else:
        return "contents"

def tokenize(thing):
    return tokenize_things([thing])

def index_things(s=None,things=[]):
    "Sends the given Things to Solr to be indexed"
    tokenized = tokenize_things(things)

    if s:
        s.add(tokenized)
    else:
        with SolrConnection(commit=True) as s:
            s.add(tokenize_things(things))

def fetch_batches(t_class,size,since,until):
    """
        Convenience function to fetch all Things of class t_class with
        _date from `since` to `until`, returning them in batches of
        `size`. TODO: move to lib/utils, and merge to be the backend
        of `fetch_things`
    """
    q=t_class._query(t_class.c._date >= since,
                     t_class.c._spam == (True,False),
                     t_class.c._deleted == (True,False),
                     t_class.c._date <  until,
                     sort  = desc('_date'),
                     limit = size,
                     data  = True)
    orig_rules = deepcopy(q._rules)

    things = list(q)
    while things:
        yield things

        q._rules = deepcopy(orig_rules)
        q._after(things[len(things)-1])
        things = list(q)

solr_queue=Queue()
for i in range(20):
    solr_queue.put(pysolr.Solr(g.solr_url))
class SolrConnection(object):
    """
        Represents a connection to Solr, properly limited to N
        concurrent connections. Used like

            with SolrConnection() as s:
                s.add(things)
    """
    def __init__(self,commit=False,optimize=False):
        self.commit   = commit
        self.optimize = optimize
    def __enter__(self):
        self.conn = solr_queue.get()
        return self.conn
    def __exit__(self, _type, _value, _tb):
        if self.commit:
            self.conn.commit()
        if self.optimize:
            self.conn.optimize()
        solr_queue.task_done()
        solr_queue.put(self.conn)

def indexer_worker(q,delete_all_first=False):
    """
        The thread for mass-indexing that connects to Solr and submits
        tokenised objects
    """
    with SolrConnection(commit=True,optimize=True) as s:
        count = 0

        if delete_all_first:
            s.delete(q='*:*')

        t = q.get()
        while t != "done":
            # if it's not a list or a dictionary, I don't know how to
            # handle it, so die. It's probably an exception pushed in
            # by the handler in my parent
            if not (isinstance(t,list) and isinstance(t[0],dict)):
                raise t
            count += len(t)
            s.add(t)
            if count > 25000:
                print "Committing... (q:%d)" % (q.qsize(),)
                s.commit()
                count = 0
            q.task_done()

            t=q.get()
        q.task_done()

def reindex_all(types = None, delete_all_first=False):
    """
        Called from `paster run` to totally re-index everything in the
        database. Spawns a thread to connect to Solr, and sends it
        tokenised Things
    """
    global indexed_types

    start_t = datetime.now()

    if not types:
        types = indexed_types

    # We don't want the default thread-local cache (which is just a
    # dict) to grow un-bounded (normally, we'd use
    # utils.set_emptying_cache, except that that preserves memcached,
    # and we don't even want to get memcached for total indexing,
    # because it would dump out more recent stuff)
    g.cache.caches = (SelfEmptyingCache(),) # + g.cache.caches[1:]

    count = 0
    q=Queue(100)
    indexer=Thread(target=indexer_worker,
                   args=(q,delete_all_first))
    indexer.start()

    try:
        for cls in types:
            for batch in fetch_batches(cls,1000,
                                       timeago("50 years"),
                                       start_t):
                r = tokenize_things([ x for x in batch
                                      if not x._spam and not x._deleted ])

                count += len(r)
                print ("Processing %s #%d(%s): %s"
                       % (cls.__name__, count, q.qsize(), r[0]['contents']))

                if indexer.isAlive():
                    q.put(r)
                else:
                    raise Exception("'tis a shame that I have but one thread to give")
        q.put("done")
        indexer.join()

    except object,e:
        if indexer.isAlive():
            q.put(e,timeout=30)
        raise e
    except KeyboardInterrupt,e: # turns out KeyboardInterrupts aren't objects. Who knew?
        if indexer.isAlive():
            q.put(e,timeout=30)
        raise e

def changed(commit=True,optimize=False,delete_old=True):
    """
        Run by `cron` (through `paster run`) on a schedule to update
        all Things that have been created or have changed since the
        last run. Things add themselves to a `thing_changes` table,
        which we read, find the Things, tokenise, and re-submit them
        to Solr
    """
    set_emptying_cache()
    with SolrConnection(commit=commit,optimize=optimize) as s:
        changes = thing_changes.get_changed()
        if changes:
            max_date = max(x[1] for x in changes) 
            changed = IteratorChunker(x[0] for x in changes)
            
            while not changed.done:
                chunk = changed.next_chunk(200)
    
                # chunk =:= [(Fullname,Date) | ...]
                chunk = Thing._by_fullname(chunk,
                                           data=True, return_dict=False)
                chunk = [x for x in chunk if not x._spam and not x._deleted]
                to_delete = [x for x in chunk if x._spam or x._deleted]
    
                # note: anything marked as spam or deleted is not
                # updated in the search database. Since these are
                # filtered out in the UI, that's probably fine.
                if len(chunk) > 0:
                    chunk  = tokenize_things(chunk)
                    s.add(chunk)
    
                for i in to_delete:
                    s.delete(id=i._fullname)

    if delete_old:
        thing_changes.clear_changes(max_date = max_date)

def combine_searchterms(terms):
    """
        Convenience function to take a list like
            [ sr_id:1, sr_id:2 sr_id:3 subreddit:reddit.com ]
        and turn it into
            sr_id:(1 2 3) OR subreddit:reddit.com
    """
    combined = {}

    for (name,val) in terms:
        combined[name] = combined.get(name,[]) + [val]

    ret = []

    for (name,vals) in combined.iteritems():
        if len(vals) == 1:
            ret.append("%s:%s" % (name,vals[0]))
        else:
            ret.append("%s:(%s)" % (name," ".join(vals)))

    if len(ret) > 1:
        ret = "(%s)" % " OR ".join(ret)
    else:
        ret = " ".join(ret)

    return ret

def swap_strings(s,this,that):
    """
        Just swaps substrings, like:
            s = "hot asc"
            s = swap_strings(s,'asc','desc')
            s == "hot desc"

         uses 'tmp' as a replacment string, so don't use for anything
         very complicated
    """
    return s.replace(this,'tmp').replace(that,this).replace('tmp',that)

class SearchQuery(object):
    def __init__(self, q, sort, fields = [], subreddits = [], authors = [], 
                 types = [], timerange = None, spam = False, deleted = False):

        self.q = q
        self.fields = fields
        self.sort = sort
        self.subreddits = subreddits
        self.authors = authors
        self.types = types
        self.spam = spam
        self.deleted = deleted

        if timerange in ['hour','week','day','month','year']:
            self.timerange = (timeago("1 %s" % timerange),"NOW")
        elif timerange == 'all' or timerange is None:
            self.timerange = None
        else:
            self.timerange = timerange

    def run(self, after = None, num = 100, reverse = False):
        if not self.q or not g.solr_url:
            return pysolr.Results([],0)

        # there are two parts to our query: what the user typed
        # (parsed with Solr's DisMax parser), and what we are adding
        # to it. The latter is called the "boost" (and is parsed using
        # full Lucene syntax), and it can be added to via the `boost`
        # parameter
        boost = []

        if not self.spam:
            boost.append("-spam:true")
        if not self.deleted:
            boost.append("-deleted:true")

        if self.timerange:
            def time_to_searchstr(t):
                if isinstance(t, datetime):
                    t = t.strftime('%Y-%m-%dT%H:%M:%S.000Z')
                elif isinstance(t, date):
                    t = t.strftime('%Y-%m-%dT00:00:00.000Z')
                elif isinstance(t,str):
                    t = t
                return t

            (fromtime, totime) = self.timerange
            fromtime = time_to_searchstr(fromtime)
            totime   = time_to_searchstr(totime)
            boost.append("+date:[%s TO %s]"
                         % (fromtime,totime))

        if self.subreddits:
            def subreddit_to_searchstr(sr):
                if isinstance(sr,Subreddit):
                    return ('sr_id','%d' % sr.id)
                elif isinstance(sr,str) or isinstance(sr,unicode):
                    return ('subreddit',sr)
                else:
                    return ('sr_id','%d' % sr)

            s_subreddits = map(subreddit_to_searchstr, tup(self.subreddits))

            boost.append("+(%s)" % combine_searchterms(s_subreddits))

        if self.authors:
            def author_to_searchstr(a):
                if isinstance(a,Account):
                    return ('author_id','%d' % a.id)
                elif isinstance(a,str) or isinstance(a,unicode):
                    return ('author',a)
                else:
                    return ('author_id','%d' % a)

            s_authors = map(author_to_searchstr,tup(self.authors))

            boost.append('+(%s)^2' % combine_searchterms(s_authors))


        def type_to_searchstr(t):
            if isinstance(t,str):
                return ('type',t)
            else:
                return ('type',t.__name__.lower())
         
        s_types = map(type_to_searchstr,self.types)
        boost.append("+%s" % combine_searchterms(s_types))

        q,solr_params = self.solr_params(self.q,boost)

        try:
            search = self.run_search(q, self.sort, solr_params,
                                     reverse, after, num)
            return search

        except SolrError,e:
            g.log.error(str(e))
            return pysolr.Results([],0)

    @classmethod
    def run_search(cls, q, sort, solr_params, reverse, after, num):
        "returns pysolr.Results(docs=[fullname()],hits=int())"

        if reverse:
            sort = swap_strings(sort,'asc','desc')

        g.log.debug("Searching q = %s; params = %s" % (q,repr(solr_params)))

        with SolrConnection() as s:
            if after:
                # size of the pre-search to run in the case that we
                # need to search more than once. A bigger one can
                # reduce the number of searches that need to be run
                # twice, but if it's bigger than the default display
                # size, it could waste some
                PRESEARCH_SIZE = num

                # run a search and get back the number of hits, so
                # that we can re-run the search with that max_count.
                pre_search = s.search(q,sort,rows=PRESEARCH_SIZE,
                                      other_params = solr_params)

                if (PRESEARCH_SIZE >= pre_search.hits
                    or pre_search.hits == len(pre_search.docs)):
                    # don't run a second search if our pre-search
                    # found all of the elements anyway
                    search = pre_search
                else:
                    # we have to run a second search, but we can limit
                    # the duplicated transfer of the first few records
                    # since we already have those from the pre_search
                    second_search = s.search(q,sort,
                                             start=len(pre_search.docs),
                                             rows=pre_search.hits - len(pre_search.docs),
                                             other_params = solr_params)
                    search = pysolr.Results(pre_search.docs + second_search.docs,
                                            pre_search.hits)

                search.docs = [ i['fullname'] for i in search.docs ]
                search.docs = get_after(search.docs, after._fullname, num)
            else:
                search = s.search(q,sort,rows=num,
                                  other_params = solr_params)
                search.docs = [ i['fullname'] for i in search.docs ]

            return search

    def solr_params(self,*k,**kw):
        raise NotImplementedError

class UserSearchQuery(SearchQuery):
    "Base class for queries that use the dismax parser"
    def __init__(self, q, mm, sort=None, fields=[], langs=None, **kw):
        default_fields = ['contents^1.5','contents_ws^3'] + fields

        if sort is None:
            sort = 'score desc, hot desc, date desc'

        if langs is None:
            fields = default_fields
        else:
            if langs == 'all':
                langs = searchable_langs
            fields = set([("%s^2" % lang_to_fieldname(lang)) for lang in langs]
                         + default_fields)

        # minimum match. See http://lucene.apache.org/solr/api/org/apache/solr/util/doc-files/min-should-match.html
        self.mm = mm

        SearchQuery.__init__(self, q, sort, fields = fields, **kw)

    def solr_params(self, q, boost):
        return q, dict(fl = 'fullname',
                       qt = 'dismax',
                       bq = ' '.join(boost),
                       qf = ' '.join(self.fields),
                       mm = self.mm)

class LinkSearchQuery(UserSearchQuery):
    def __init__(self, q, mm = None, **kw):
        additional_fields = ['site^1','author^1', 'subreddit^1', 'url^1']

        if mm is None:
            mm = '4<75%'

        UserSearchQuery.__init__(self, q, mm = mm, fields = additional_fields,
                                 types=[Link], **kw)

class RelatedSearchQuery(LinkSearchQuery):
    def __init__(self, q, ignore = [], **kw):
        self.ignore = set(ignore) if ignore else set()

        LinkSearchQuery.__init__(self, q, mm = '3<100% 5<60% 8<50%',
                                 sort = 'score desc', **kw)

    def run(self, *k, **kw):
        search = LinkSearchQuery.run(self, *k, **kw)
        search.docs = [ x for x in search.docs if x not in self.ignore ]
        return search

class SubredditSearchQuery(UserSearchQuery):
    def __init__(self, q, **kw):
        # note that 'downs' is a measure of activity on subreddits
        UserSearchQuery.__init__(self, q, mm = '75%', sort = 'downs desc',
                                 types=[Subreddit], **kw)

class DomainSearchQuery(SearchQuery):
    def __init__(self, domain, **kw):
        q = '+site:%s' % domain

        SearchQuery.__init__(self, q = q, fields=['site'],types=[Link], **kw)

    def solr_params(self, q, boost):
        q = q + ' ' + ' '.join(boost)
        return q, dict(fl='fullname',
                       qt='standard')

def get_after(fullnames, fullname, num):
    for i, item in enumerate(fullnames):
        if item == fullname:
            return fullnames[i+1:i+num+1]
    else:
        return fullnames[:num]

########NEW FILE########
__FILENAME__ = strings
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
"""
Module for maintaining long or commonly used translatable strings,
removing the need to pollute the code with lots of extra _ and
ungettext calls.  Also provides a capacity for generating a list of
random strings which can be different in each language, though the
hooks to the UI are the same.
"""

import helpers as h
from pylons.i18n import _, ungettext
import random

__all__ = ['StringHandler', 'strings', 'PluralManager', 'plurals',
           'Score', 'rand_strings']

# here's where all of the really long site strings (that need to be
# translated) live so as not to clutter up the rest of the code.  This
# dictionary is not used directly but rather is managed by the single
# StringHandler instance strings
string_dict = dict(

    banned_by = "banned by %s",
    banned    = "banned",
    reports   = "reports: %d",
    
    # this accomodates asian languages which don't use spaces
    number_label = _("%d %s"),

    # this accomodates asian languages which don't use spaces
    float_label = _("%5.3f %s"),

    # this is for Japanese which treats people counds differently
    person_label = _("%(num)d %(persons)s"),

    firsttext = _("Less Wrong is a community blog devoted to refining the art of human rationality. Please visit our [About](/about-less-wrong/) page for more information."),

    already_submitted = _("That link has already been submitted, but you can try to [submit it again](%s)."),

    multiple_submitted = _("That link has been submitted to multiple categories. you can try to [submit it again](%s)."),

    user_deleted = _("Your account has been deleted, but we won't judge you for it."),

    cover_msg      = _("You'll need to login or register to do that"),
    cover_disclaim = _("(Don't worry, it only takes a few seconds)"),

    legal = _("I understand and agree that registration on or use of this site constitutes agreement to its %(user_agreement)s and %(privacy_policy)s."),
    
    friends = _('To view Less Wrong with only submissions from your friends, use [lesswrong.com/r/friends](%s)'),

    msg_add_friend = dict(
        friend = None,
        moderator = _("You have been added as a moderator to [%(title)s](%(url)s)."),
        contributor = _("You have been added as a contributor to [%(title)s](%(url)s)."),
        banned = _("You have been banned from posting to [%(title)s](%(url)s).")
        ),

    subj_add_friend = dict(
        friend = None,
        moderator = _("You are a moderator"),
        contributor = _("You are a contributor"),
        banned = _("You've been banned")
        ),
    
    sr_messages = dict(
        empty =  _('You have not subscribed to any categories.'),
        subscriber =  _('Below are the categories you have subscribed to'),
        contributor =  _('Below are the categories that you have contributor access to.'),
        moderator = _('Below are the categories that you have moderator access to.')
        ),
    
    sr_subscribe =  _('Click the ![add](/static/sr-add-button.png) or ![remove](/static/sr-remove-button.png) buttons to choose which categories appear on your front page.'),

    searching_a_reddit = _('You\'re searching within the [%(reddit_name)s](%(reddit_link)s) category. '+
                           'you can also search within [all categories](%(all_reddits_link)s)'),

    css_validator_messages = dict(
        broken_url = _('"%(brokenurl)s" is not a valid URL'),
        invalid_property = _('"%(cssprop)s" is not a valid CSS property'),
        invalid_val_for_prop = _('"%(cssvalue)s" is not a valid value for CSS property "%(cssprop)s"'),
        too_big = _('Too big. keep it under %(max_size)dkb'),
        syntax_error = _('Syntax error: "%(syntaxerror)s"'),
        no_imports = _('@imports are not allowed'),
        invalid_property_list = _('Invalid CSS property list "%(proplist)s"'),
        unknown_rule_type = _('Unknown CSS rule type "%(ruletype)s"')
    ),
    
    submit_box_text = _('To anything interesting: news article, blog entry, video, picture...'),
    permalink_title = _("%(author)s comments on %(title)s - %(site)s"),
    link_info_title = _("%(title)s - %(site)s"),
    show_meetup_title = _("%(title)s - %(site)s"),
    not_enough_downvote_karma = _('You do not have enough karma to downvote right now. You need %d more %s.')
)

class StringHandler(object):
    """Class for managing long translatable strings.  Allows accessing
    of strings via both getitem and getattr.  In both cases, the
    string is passed through the gettext _ function before being
    returned."""
    def __init__(self, **sdict):
        self.string_dict = sdict

    def __getitem__(self, attr):
        try:
            return self.__getattr__(attr)
        except AttributeError:
            raise KeyError
    
    def __getattr__(self, attr):
        rval = self.string_dict[attr]
        if isinstance(rval, (str, unicode)):
            return _(rval)
        elif isinstance(rval, dict):
            return dict((k, _(v)) for k, v in rval.iteritems())
        else:
            raise AttributeError

strings = StringHandler(**string_dict)


def P_(x, y):
    """Convenience method for handling pluralizations.  This identity
    function has been added to the list of keyword functions for babel
    in setup.cfg so that the arguments are translated without having
    to resort to ungettext and _ trickery."""
    return (x, y)

class PluralManager(object):
    """String handler for dealing with pluralizable forms.  plurals
    are passed in in pairs (sing, pl) and can be accessed via
    self.sing and self.pl.

    Additionally, calling self.N_sing(n) (or self.N_pl(n)) (where
    'sing' and 'pl' are placeholders for a (sing, pl) pairing) is
    equivalent to ungettext(sing, pl, n)
    """
    def __init__(self, plurals):
        self.string_dict = {}
        for s, p in plurals:
            self.string_dict[s] = self.string_dict[p] = (s, p)

    def __getattr__(self, attr):
        if attr.startswith("N_"):
            a = attr[2:]
            rval = self.string_dict[a]
            return lambda x: ungettext(rval[0], rval[1], x)
        else:
            rval = self.string_dict[attr]
            n = 1 if attr == rval[0] else 5
            return ungettext(rval[0], rval[1], n)

plurals = PluralManager([P_("comment",     "comments"),
                         P_("point",       "points"),
                         
                         # things
                         P_("link",        "links"),
                         P_("comment",     "comments"),
                         P_("message",     "messages"),
                         P_("subreddit",   "subreddits"),
                         
                         # people
                         P_("subscriber",  "subscribers"),
                         P_("contributor", "contributors"),
                         P_("moderator",   "moderators"),
                         
                         # time words
                         P_("milliseconds","milliseconds"),
                         P_("second",      "seconds"),
                         P_("minute",      "minutes"),
                         P_("hour",        "hours"),
                         P_("day",         "days"),
                         P_("month",       "months"),
                         P_("year",        "years"),
])


class Score(object):
    """Convienience class for populating '10 points' in a traslatible
    fasion, used primarily by the score() method in printable.html"""
    @staticmethod
    def number_only(pair):
        total = pair[0] - pair[1]
        return {'label': str(max(total, 0)), 'hover': ''}

    @staticmethod
    def signed_number(pair):
        total = pair[0] - pair[1]
        return {
            'label': str(total),
            'hover': '{0:.0%} positive'.format(sum(pair) and float(pair[0]) / sum(pair)),
        }

    @staticmethod
    def points(pair):
        total = pair[0] - pair[1]
        return {
            'label': strings.number_label % (total, plurals.N_points(total)),
            'hover': '{0:.0%} positive'.format(sum(pair) and float(pair[0]) / sum(pair)),
        }

    @staticmethod
    def safepoints(pair):
        total = pair[0] - pair[1]
        return {
            'label': strings.number_label % (max(total, 0), plurals.N_points(total)),
            'hover': '',
        }

    @staticmethod
    def subscribers(pair):
        total = pair[0] - pair[1]
        return {
            'label': strings.person_label % {'num': total,
                                             'persons': plurals.N_subscribers(total)},
            'hover': '',
        }

    @staticmethod
    def none(pair):
        return {'label': '', 'hover': ''}


def fallback_trans(x):
    """For translating placeholder strings the user should never see
    in raw form, such as 'funny 500 message'.  If the string does not
    translate in the current language, falls back on the 'en'
    translation that we've hopefully already provided"""
    t = _(x)
    if t == x:
        l = h.get_lang()
        h.set_lang('en', graceful_fail = True)
        t = _(x)
        if l and l[0] != 'en':
            h.set_lang(l[0])
    return t

class RandomString(object):
    """class for generating a translatable random string that is one
    of n choices.  The 'description' field passed to the constructor
    is only used to generate labels for the translation interface.

    Unlike other translations, this class is accessed directly by the
    translator classes and side-step babel.extract_messages.
    Untranslated, the strings return are of the form 'description n+1'
    for the nth string.  The user-facing versions of these strings are
    therefore completely determined by their translations."""
    def __init__(self, description, num):
        self.desc = description
        self.num = num
    
    def get(self, quantity = 0):
        """Generates a list of 'quantity' random strings.  If quantity
        < self.num, the entries are guaranteed to be unique."""
        l = []
        possible = []
        for x in range(max(quantity, 1)):
            if not possible:
                possible = range(self.num)
            irand = random.choice(possible)
            possible.remove(irand)
            l.append(fallback_trans(self._trans_string(irand)))

        return l if len(l) > 1 else l[0]

    def _trans_string(self, n):
        """Provides the form of the string that is actually translated by gettext."""
        return "%s %d" % (self.desc, n+1)

    def __iter__(self):
        for i in xrange(self.num):
            yield self._trans_string(i)
                   

class RandomStringManager(object):
    """class for keeping randomized translatable strings organized.
    New strings are added via add, and accessible by either getattr or
    getitem using the short name passed to add."""
    def __init__(self):
        self.strings = {}

    def __getitem__(self, attr):
        return self.strings[attr].get()

    def __getattr__(self, attr):
        try:
            return self[attr]
        except KeyError:
            raise AttributeError

    def get(self, attr, quantity = 0):
        """Convenience method for getting a list of 'quantity' strings
        from the RandomString named 'attr'"""
        return self.strings[attr].get(quantity)

    def add(self, name, description, num):
        """create a new random string accessible by 'name' in the code
        and explained in the translation interface with 'description'."""
        self.strings[name] = RandomString(description, num)

    def __iter__(self):
        """iterator primarily used by r2.lib.translations to fetch the
        list of random strings and to iterate over their names to
        insert them into the resulting .po file for a given language"""
        return self.strings.iteritems()

rand_strings = RandomStringManager()

rand_strings.add('sadmessages',   "Funny 500 page message", 1)
rand_strings.add('create_reddit', "Reason to create a reddit", 20)

########NEW FILE########
__FILENAME__ = template_helpers
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.models import *
from filters import unsafe, websafe
from r2.lib.utils import vote_hash, UrlParser

from mako.filters import url_escape
import simplejson
import os.path
import re
from copy import copy

from pylons import i18n, g, c

def static(file):
    # stip of "/static/" if already present
    fname = os.path.basename(file).split('?')[0]
    v = g.static_md5.get(fname, '')
    if v: v = "?v=" + v
    if re.match("%s?static" % os.path.sep, os.path.dirname(file)):
        return file + v
    return os.path.join(c.site.static_path, file) + v

def generateurl(context, path, **kw):
    if kw:
        return path + '?' + '&'.join(["%s=%s"%(k, url_escape(v)) \
                                      for k, v in kw.iteritems() if v])
    return path

def class_dict():
    t_cls = [Link, Comment, Message, Subreddit]
    l_cls = [Listing, OrganicListing]

    classes  = [('%s: %s') % ('t'+ str(cl._type_id), cl.__name__ ) for cl in t_cls] \
             + [('%s: %s') % (cl.__name__, cl._js_cls) for cl in l_cls]

    res = ', '.join(classes)
    return unsafe('{ %s }' % res)


def json(value):
    return unsafe(simplejson.dumps(value, separators=(',', ':')))


def path_info():
    loc = dict(path = request.path,
               params = dict(request.get))
    
    return unsafe(simplejson.dumps(loc))
    

def replace_render(listing, item, style = None, display = True):
    style = style or c.render_style or 'html'
    rendered_item = item.render(style = style)

    # for string rendered items
    def string_replace(x, y):
        return rendered_item.replace(x, y)

    # for JSON responses
    def dict_replace(x, y):
        try:
            res = rendered_item['data']['content']
            rendered_item['data']['content'] = res.replace(x, y)
        except AttributeError:
            pass
        except TypeError:
            pass
        return rendered_item

    child_txt = ( hasattr(item, "child") and item.child )\
        and item.child.render(style = style) or ""

    # handle API calls differently from normal request: dicts not strings are passed around
    if isinstance(rendered_item, dict):
        replace_fn = dict_replace
        try:
            rendered_item['data']['child'] = child_txt
        except AttributeError:
            pass
        except TypeError:
            pass
    else:
        replace_fn = string_replace
        rendered_item = replace_fn(u"$child", child_txt)

    #only LinkListing has a show_nums attribute
    if listing: 
        if hasattr(listing, "show_nums"):
            if listing.show_nums:
                num_str = str(item.num) 
                if hasattr(listing, "num_margin"):
                    num_margin = listing.num_margin
                else:
                    num_margin = "%5.2fex" % (len(str(listing.max_num))*1.1)
            else:
                num_str = ''
                num_margin = "0px"
    
            rendered_item = replace_fn(u"$numcolmargin", num_margin)
            rendered_item = replace_fn(u"$num", num_str)

        if hasattr(listing, "max_score"):
            mid_margin = len(str(listing.max_score)) 
            if hasattr(listing, "mid_margin"):
                mid_margin = listing.mid_margin
            elif mid_margin == 1:
                mid_margin = "15px"
            else:
                mid_margin = "%dex" % (mid_margin+1)

            rendered_item = replace_fn(u"$midcolmargin", mid_margin)

        # TODO: one of these things is not like the other.  We should & ->
        # $ elsewhere as it plays nicer with the websafe filter.
        rendered_item = replace_fn(u"$ListClass", listing._js_cls)

        #$votehash is only present when voting arrows are present
        if c.user_is_loggedin and u'$votehash' in rendered_item:
            hash = vote_hash(c.user, item, listing.vote_hash_type)
            rendered_item = replace_fn(u'$votehash', hash)
            
    rendered_item = replace_fn(u"$display", "" if display else "style='display:none'")
    return rendered_item

def get_domain(cname = False, subreddit = True, no_www = False):
    """
    returns the domain on the current subreddit, possibly including
    the subreddit part of the path, suitable for insertion after an
    "http://" and before a fullpath (i.e., something including the
    first '/') in a template.  The domain is updated to include the
    current port (request.port).  The effect of the arguments is:

     * no_www: if the domain ends up being g.domain, the default
       behavior is to prepend "www." to the front of it (for akamai).
       This flag will optionally disable it.

     * cname: whether to respect the value of c.cname and return
       c.site.domain rather than g.domain as the host name.

     * subreddit: if a cname is not used in the resulting path, flags
       whether or not to append to the domain the subreddit path (sans
       the trailing path).

    """
    domain = g.domain
    if not no_www and g.domain_prefix:
        domain = g.domain_prefix + "." + g.domain
    if cname and c.cname and c.site.domain:
        domain = c.site.domain
    if hasattr(request, "port") and request.port:
        domain += ":" + str(request.port)
    if (not c.cname or not cname) and subreddit:
        domain += c.site.path.rstrip('/')
    return domain

def dockletStr(context, type, browser):
    domain      = get_domain()

    # while site_domain will hold the (possibly) cnamed version
    site_domain = get_domain(True)

    if type == "serendipity!":
        return "http://"+site_domain+"/random"
    elif type == "reddit":
        return "javascript:location.href='http://"+site_domain+"/submit?url='+encodeURIComponent(location.href)+'&title='+encodeURIComponent(document.title)"
    else:
        return (("javascript:function b(){var u=encodeURIComponent(location.href);"
                 "var i=document.getElementById('redstat')||document.createElement('a');"
                 "var s=i.style;s.position='%(position)s';s.top='0';s.left='0';"
                 "s.zIndex='10002';i.id='redstat';"
                 "i.href='http://%(site_domain)s/submit?url='+u+'&title='+"
                 "encodeURIComponent(document.title);"
                 "var q=i.firstChild||document.createElement('img');"
                 "q.src='http://%(domain)s/d/%(type)s.png?v='+Math.random()+'&uh=%(modhash)s&u='+u;"
                 "i.appendChild(q);document.body.appendChild(i)};b()") %
                dict(position = "absolute" if browser == "ie" else "fixed",
                     domain = domain, site_domain = site_domain, type = type,
                     modhash = c.modhash if c.user else ''))



def add_sr(path, sr_path = True, nocname=False, force_hostname = False):
    """
    Given a path (which may be a full-fledged url or a relative path),
    parses the path and updates it to include the subreddit path
    according to the rules set by its arguments:

     * force_hostname: if True, force the url's hotname to be updated
       even if it is already set in the path, and subject to the
       c.cname/nocname combination.  If false, the path will still
       have its domain updated if no hostname is specified in the url.
    
     * nocname: when updating the hostname, overrides the value of
       c.cname to set the hotname to g.domain.  The default behavior
       is to set the hostname consistent with c.cname.

     * sr_path: if a cname is not used for the domain, updates the
       path to include c.site.path.
    """
    u = UrlParser(path)
    if sr_path and (nocname or not c.cname):
        u.path_add_subreddit(c.site)

    if not u.hostname or force_hostname:
        u.hostname = get_domain(cname = (c.cname and not nocname),
                                subreddit = False)

    if c.render_style == 'mobile':
        u.set_extension('mobile')

    return u.unparse()

def join_urls(*urls):
    """joins a series of urls together without doubles slashes"""
    if not urls:
        return
    
    url = urls[0]
    for u in urls[1:]:
        if not url.endswith('/'):
            url += '/'
        while u.startswith('/'):
            u = utils.lstrips(u, '/')
        url += u
    return url

old_user_rss_re = re.compile(r'^/user/([^/]+)/$')
old_user_rss_re2 = re.compile(r'^/user/([^/]+)$')
overview_rss_re = re.compile(r'^/user/([^/]+)/overview/$')
comments_rss_re = re.compile(r'^/user/([^/]+)/comments/$')
def get_rss_path(request_path):
    """Returns an appropriate path to an RSS feed for the current page."""

    # e.g. the wiki homepage (LW front page) in particular needs a sensible RSS link
    path = '/' if request_path.startswith("/wiki/") else request_path

    # On user profile pages pulled from the wiki the RSS feed should point to
    # the overview page's feed
    if old_user_rss_re.match(request_path) or old_user_rss_re2.match(request_path):
      path = path + "overviewrss/"

    if overview_rss_re.match(request_path):
      path = path[:-9] + "overviewrss/"

    if comments_rss_re.match(request_path):
      path = path[:-9] + "commentsrss/"

    return add_sr(join_urls(path, '.rss'))

def style_line(button_width = None, bgcolor = "", bordercolor = ""):
    style_line = ''
    bordercolor = c.bordercolor or bordercolor
    bgcolor     = c.bgcolor or bgcolor
    if bgcolor:
        style_line += "background-color: #%s;" % bgcolor
    if bordercolor:
        style_line += "border: 1px solid #%s;" % bordercolor
    if button_width:
        style_line += "width: %spx;" % button_width
    return style_line

def choose_width(link, width):
    if width:
        return width - 5
    else:
        if link:
            return 100 + (10 * (len(str(link._ups - link._downs))))
        else:
            return 110

########NEW FILE########
__FILENAME__ = test_cache
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from cache import *

c1 = LocalCache()
c2 = Memcache(('127.0.0.1:11211',))
c = CacheChain((c1, c2))

#basic set/get
c.set('1', 1)
assert(c1.get('1') == 1)
assert(c2.get('1') == 1)
assert(c.get('1') == 1)

#python data
c.set('2', [1,2,3])
assert(c1.get('2') == [1,2,3])
assert(c2.get('2') == [1,2,3])
assert(c.get('2') == [1,2,3])

#set multi, no prefix
c.set_multi({'3':3, '4': 4})
assert(c1.get_multi(('3', '4')) == {'3':3, '4': 4})
assert(c2.get_multi(('3', '4')) == {'3':3, '4': 4})
assert(c.get_multi(('3', '4')) == {'3':3, '4': 4})

#set multi, prefix
c.set_multi({'3':3, '4': 4}, prefix='p_')
assert(c1.get_multi(('3', 4), prefix='p_') == {'3':3, 4: 4})
assert(c2.get_multi(('3', 4), prefix='p_') == {'3':3, 4: 4})
assert(c.get_multi(('3', 4), prefix='p_') == {'3':3, 4: 4})

assert(c1.get_multi(('p_3', 'p_4')) == {'p_3':3, 'p_4': 4})
assert(c2.get_multi(('p_3', 'p_4')) == {'p_3':3, 'p_4': 4})
assert(c.get_multi(('p_3', 'p_4')) == {'p_3':3, 'p_4': 4})

#incr
c.set('5', 1)
c.set('6', 1)
c.incr('5')
assert(c1.get('5'), 2)
assert(c2.get('5'), 2)
assert(c.get('5'), 2)

c.incr('5',2)
assert(c1.get('5'), 4)
assert(c2.get('5'), 4)
assert(c.get('5'), 4)

c.incr_multi(('5', '6'), 1)
assert(c1.get('5'), 5)    
assert(c2.get('5'), 5)    
assert(c.get('5'), 5)    

assert(c1.get('6'), 2)
assert(c2.get('6'), 2)
assert(c.get('6'), 2)

c.flush_all()

c.set('1', 1)
c2.set('2', 2)
c2.set('1', 4)
c.set('3', 3)

assert(c.get_multi((1,2,3)) == {1:1, 2:2, 3:3})

########NEW FILE########
__FILENAME__ = test_wrapper
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from utils import Wrapped, storify

def t1():
    w = Wrapped(foo = 1, bar = 2)
    assert(w.foo == 1)
    assert(w.bar == 2)

class Foo(Wrapped):
    defaults = dict(foo = 1,
                    bar = 3)

def t2():
    f = Foo(bar = 2)
    assert(f.foo == 1)
    assert(f.bar == 2)

l1 = storify({'bar': 1})
l2 = storify({'bar': 2, 'baz': 3})

def t3():
    f = Foo(l1, l2, ok = 1)
    assert(f.bar == 1)
    assert(f.baz == 3)
    assert(f.ok == 1)
    #assert(f.blah == 5)

def t4():
    x = Wrapped(Foo())
    assert(x.foo == 1)
    assert(x.bar == 3)
    
t1()
t2()
t3()
t4()

########NEW FILE########
__FILENAME__ = tracking
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from base64 import standard_b64decode as b64dec, \
     standard_b64encode as b64enc
from Crypto.Cipher import AES
from random import choice
from pylons import g, c
from urllib import quote_plus, unquote_plus
import hashlib

key_len = 16
pad_len = 32

def pkcs5pad(text, padlen = 8):
    '''Insures the string is an integer multiple of padlen by appending to its end
    N characters which are chr(N).'''
    l = (padlen - len(text) % padlen) or padlen
    padding = ''.join([chr(l) for x in xrange(0,l)])
    return text + padding

def pkcs5unpad(text, padlen = 8):
    '''Undoes padding of pkcs5pad'''
    if text:
        key = ord(text[-1])
        if (key <= padlen and key > 0 and
            all(ord(x) == key for x in text[-key:])):
            text = text[:-key]
    return text

def cipher(lv):
    '''returns a pycrypto object used by encrypt and decrypt, with the key based on g.tracking_secret'''
    key = g.tracking_secret
    return AES.new(key[:key_len], AES.MODE_CBC, lv[:key_len])

def encrypt(text):
    '''generates an encrypted version of text.  The encryption is salted using the pad_len characters
    that randomly make up the front of the resulting string.  The string is base64 encoded, and url escaped
    so as to be suitable to be used as a GET parameter'''
    randstr = ''.join(choice('1234567890abcdefghijklmnopqrstuvwxyz' +
                             'ABCDEFGHIJKLMNOPQRSTUVWXYZ+/')
                      for x in xrange(pad_len))
    cip = cipher(randstr)
    text = b64enc(cip.encrypt(pkcs5pad(text, key_len)))
    return quote_plus(randstr + text, safe='')

def decrypt(text):
    '''Inverts encrypt'''
    # we can unquote even if text is not quoted.  
    text = unquote_plus(text)
    # grab salt
    randstr = text[:pad_len]
    # grab message
    text = text[pad_len:]
    cip = cipher(randstr)
    return pkcs5unpad(cip.decrypt(b64dec(text)), key_len)


def safe_str(text):
    '''That pesky function always needed to make sure nothing breaks if text is unicode.  if it is,
    it returns the utf8 transcode of it and returns a python str.'''
    try:
        if isinstance(text, unicode):
            return text.encode('utf8')
    except:
        g.log.error("unicode encoding exception in safe_str")
        return ''
    return text

class Info(object): 
    '''Class for generating and reading user tracker information.'''
    _tracked = []
    tracker_url = ""

    def __init__(self, text = '', **kw):
        for s in self._tracked:
            setattr(self, s, '')
            
        if text:
            try:
                data = decrypt(text).split('|')
            except:
                g.log.error("decryption failure on '%s'" % text)
                data = []
            for i, d in enumerate(data):
                if i < len(self._tracked):
                    setattr(self, self._tracked[i], d)
        else:
            self.init_defaults(**kw)
            
    def init_defaults(self, **kw):
        raise NotImplementedError
    
    def tracking_url(self):
        data = '|'.join(getattr(self, s) for s in self._tracked)
        data = encrypt(data)
        return "%s?v=%s" % (self.tracker_url, data)

    @classmethod
    def gen_url(cls, **kw):
        try:
            return cls(**kw).tracking_url()

        except Exception,e:
            g.log.error(e)
            try:
                randstr = ''.join(choice('1234567890abcdefghijklmnopqrstuvwxyz' +
                                         'ABCDEFGHIJKLMNOPQRSTUVWXYZ+')
                                  for x in xrange(pad_len))
                return "%s?v=%s" % (cls.tracker_url, randstr)
            except:
                g.log.error("fallback rendering failed as well")
                return ""

class UserInfo(Info):
    '''Class for generating and reading user tracker information.'''
    _tracked = ['name', 'site', 'lang']
    tracker_url = g.tracker_url

    def init_defaults(self):
        self.name = safe_str(c.user.name if c.user_is_loggedin else '')
        self.site = safe_str(c.site.name if c.site else '')
        self.lang = safe_str(c.lang if c.lang else '')
            
class PromotedLinkInfo(Info):
    _tracked = []
    tracker_url = g.adtracker_url

    def __init__(self, text = "", ip = "0.0.0.0", **kw):
        self.ip = ip
        Info.__init__(self, text = text, **kw)

    def init_defaults(self, fullname):
        self.fullname = fullname

    @classmethod
    def make_hash(cls, ip, fullname):
        return hashlib.sha1("%s%s%s" % (ip, fullname,
                                   g.tracking_secret)).hexdigest()

    def tracking_url(self):
        return (self.tracker_url + "?hash=" +
                self.make_hash(self.ip, self.fullname)
                + "&id=" + self.fullname)

class PromotedLinkClickInfo(PromotedLinkInfo):
    _tracked = []
    tracker_url = g.clicktracker_url

    def init_defaults(self, dest, **kw):
        self.dest = dest

        return PromotedLinkInfo.init_defaults(self, **kw)

    def tracking_url(self):
        s = (PromotedLinkInfo.tracking_url(self) + '&url=' + self.dest)
        return s
        
def benchmark(n = 10000):
    """on my humble desktop machine, this gives ~150 microseconds per gen_url"""
    import time
    t = time.time()
    for x in xrange(n):
        gen_url()
    t = time.time() - t
    print ("%d generations in %5.3f seconds (%5.3f us/gen)" % 
           (n, t, 10**6 * t/n))

########NEW FILE########
__FILENAME__ = translation
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from __future__ import with_statement
from distutils.cmd import Command
from distutils.errors import DistutilsOptionError

from pylons.i18n import _
from babel import Locale
import os, re
import cPickle as pickle
from wrapped import Wrapped
from utils import Storage
from hashlib import md5

from logger import WithWriteLock, LoggedSlots

from datetime import datetime, timedelta
import time

try:
    import reddit_i18n
    _i18n_path = os.path.dirname(reddit_i18n.__file__)
except ImportError:
    _i18n_path = os.path.abspath('r2/i18n')

import pylons
from pylons.i18n.translation import translation, LanguageError, NullTranslations

def _get_translator(lang, graceful_fail = False, **kwargs):
    from pylons import config as conf
    """Utility method to get a valid translator object from a language name"""
    if not isinstance(lang, list):
        lang = [lang]
    try:
        translator = translation(conf['pylons.package'], _i18n_path, 
                                 languages=lang, **kwargs)
    except IOError, ioe:
        if graceful_fail:
            translator = NullTranslations()
        else:
            raise LanguageError('IOError: %s' % ioe)
    translator.pylons_lang = lang
    return translator


def set_lang(lang, graceful_fail = False, **kwargs):
    """Set the i18n language used"""
    registry = pylons.request.environ['paste.registry']
    if not lang:
        registry.replace(pylons.translator, NullTranslations())
    else:
        translator = _get_translator(lang, graceful_fail = graceful_fail, **kwargs)
        registry.replace(pylons.translator, translator)


comment = re.compile(r'^\s*#')
msgid = re.compile(r'^\s*msgid\s+"')
msgid_pl = re.compile(r'^\s*msgid_plural\s+"')
msgstr = re.compile(r'^\s*msgstr(\[\d\])?\s+"')
str_only = re.compile(r'^\s*"')

substr = re.compile("(%(\([^\)]+\))?([\d\.]+)?[a-zA-Z])")

source_line = re.compile(": (\S+\.[^\.]+):(\d+)")

from r2.config.templates import tpm

tpm.add('translator',       'html', file = "translation.html")



_domain = 'r2'

def hax(string):
    """when site translation strings change, this will allow the subsequent 
       language translations to be updated without giving the translator more
       work"""
    hax_dict = { }
    return hax_dict.get(string, string)
    

class TranslatedString(Wrapped):
    class _SubstString:
        def __init__(self, string, enabled = True):
            self.str = hax(string)
            subst = substr.findall(string)
            self.subst = set(x[0] for x in subst)
            self.name = set(x[1].strip(')').strip('(') for x in subst if x[1])

        def __str__(self):
            return self.str

        def unicode(self):
            if isinstance(self.str, unicode):
                return self.str
            return unicode(self.str, "utf-8")
        
        def __repr__(self):
            return self.str

        def _po_str(self, header, cut = 60):
            if len(self.str) > cut:
                if '\\n' in self.str:
                    txt = [i + "\\n" for i in self.str.split('\\n') if i]
                else:
                    txt = [self.str] #[self.str[i:i+cut]
                           #for i in range(0, len(self.str), cut)]
                res = '%s ""\n' % header
                for t in txt:
                    res += '"%s"\n' % t.replace('"', '\\"')
            else:
                res = '%s "%s"\n' % (header, self.str.replace('"', '\\"'))
            if isinstance(res, unicode):
                return res.encode('utf8')
            return res

        def highlight(self, func):
            res = self.str
            for x in self.subst:
                res = res.replace(x, func(x))
            return res

        def valid(self):
            try:
                # enforce validatation for named replacement rules only
                if self.name:
                    x = self.str % dict((k, 0) for k in self.name if k)
                return True
            except:
                return False

        def compatible(self, other):
            # compatibility implies every substitution rule in other
            # must be in self.
            return other.valid() and (not self.name or other.subst.issubset(self.subst))

    def __init__(self, translator, sing, plural = '', message = '',
                 enabled = True, locale = '', tip = ''):
        Wrapped.__init__(self, self)

        self.translator = translator
        self.message = message
        self.enabled = enabled
        self.locale = locale
        self.tip = ''

        # figure out where this item appears in the source
        source = source_line.findall(message)
        if source:
            self.source, self.line = source[0]
        else:
            self.source = self.line = ''

        self.msgid = self._SubstString(sing)
        self.msgid_plural = self._SubstString(plural)
        if str(self.msgid_plural):
            self.msgstr = []
        else:
            self.msgstr = self._SubstString('')

    @property
    def singular(self):
        return self.msgid.unicode() or ''

    def _singular(self, func):
        return self.msgid.highlight(func)

    @property
    def plural(self):
        return self.msgid_plural.unicode() or ''

    def _plural(self, func):
        return self.msgid_plural.highlight(func)

    def is_translated(self):
        if str(self.msgid_plural):
            return bool(self.msgstr) and any([x.str for x in self.msgstr])
        else:
            return bool(self.msgstr.str)


    def translation(self, indx = 0):
        if self.plural:
            if indx < len(self.msgstr):
                return self.msgstr[indx].unicode() or ''
            return ''
        else:
            return self.msgstr.unicode() or ''

    def add(self, translation, index = -1):
        new = self._SubstString(translation)
        if unicode(self.msgid_plural):
            if index >= 0:
                while index >= len(self.msgstr):
                    self.msgstr.append('')
                self.msgstr[index] = new
            else:
                self.msgstr.append(new)
        else:
            self.msgstr = new

    def __getitem__(self, indx):
        return self.translation(indx)

    def __setitem__(self, indx, value):
        return self.add(value, index = indx)

    @property
    def md5(self):
        return md5(unicode(self.singular) + unicode(self.plural)).hexdigest()

    def __repr__(self):
        return "<TranslatioString>"

    def __str__(self):
        res = ''
        if self.message:
            res = '#' + self.message.replace('\n', '\n#')
            if res[-1] == '#': res = res[:-1]
        res += self.msgid._po_str('msgid')
        if unicode(self.msgid_plural):
            res += self.msgid_plural._po_str('msgid_plural')
            for i in range(0, min(len(self.msgstr), self.translator.nplurals)):
                res += self.msgstr[i]._po_str('msgstr[%d]'%i)
        else:
            res += self.msgstr._po_str('msgstr')
        res += "\n"
        try:
            return str(res)
        except UnicodeEncodeError:
            return unicode(res + "\n").encode('utf-8')


    def is_valid(self, indx = -1):
        if self.plural:
            if indx < 0:
                return all(self.is_valid(i) for i in range(0,len(self.msgstr)))
            elif indx < len(self.msgstr):
                return self.msgid.compatible(self.msgstr[indx]) or \
                       self.msgstr.compatible(self.msgstr[indx])
            return True
        else:
            return self.msgid.compatible(self.msgstr)
            

class GettextHeader(TranslatedString):
    def __init__(self, translator, sing, plural = '', message = '',
                 enabled = True, locale = ''):
        TranslatedString.__init__(self, translator, '', '', message=message,
                                  enabled = False, locale = locale)
        self.headers = []

    def add(self, translation, index = -1):
        if index < 0:
            header_keys = set()
            self.msgstr = self._SubstString(translation)
            for line in translation.split('\\n'):
                line = line.split(':')
                header_keys.add(line[0])
                self.headers.append([line[0], ':'.join(line[1:])])
            # http://www.gnu.org/software/gettext/manual/gettext.html#Plural-forms
            if "Plural-Forms" not in header_keys:
                self.headers.append(["Plural-Forms",
                                     "nplurals=2; plural=(n != 1);"])
                
        elif self.headers and len(self.headers) > index:
            self.headers[index][1] = translation
            t = '\\n'.join('%s:%s' % tuple(h) for h in self.headers if h[0])
            self.msgstr = self._SubstString(t)
        
    def __repr__(self):
        return "<GettextHeader>"

class Translator(LoggedSlots):

    __slots__ = ['enabled', 'num_completed', 'num_total', 'author',
                 'nplurals', 'plural_names', 'source_trans', 'name', 
                 'en_name', '_is_enabled']

    def __init__(self, domain = _domain, path = _i18n_path,
                 locale = ''):
        self.strings = []
        self.string_dict = {}
        self.sources = {}

        self.locale = locale
        self.input_file = TranslatorTemplate.outfile(locale)

        def _out_file(extension = None):
            d = dict(extension=extension) if extension is not None else {}
            return self.outfile(locale, path=path, domain=domain, **d)
        self._out_file = _out_file

        # create directory for translation
        if not os.path.exists(os.path.dirname(self._out_file('po'))):
            os.makedirs(os.path.dirname(self._out_file('po')))

        LoggedSlots.__init__(self, self._out_file('data'), 
                             plural_names = ['singular', 'plural'],
                             nplurals = 2,
                             source_trans = {},
                             author = set([]),
                             enabled = {},
                             num_completed = 0,
                             num_total = 0,
                             en_name = locale,
                             name = locale,
                             _is_enabled = False
                             )

        # no translation, use the infile to make one
        if not os.path.exists(self._out_file('po')):
            self.from_file(self.input_file)
        # translation exists: make sure infile is not newer
        else:
            i_stat = os.stat(self.input_file)
            o_stat = os.stat(self._out_file('po'))
            if i_stat.st_mtime > o_stat.st_mtime:
                self.from_file(self.input_file)
                self.load_specialty_strings()
                self.from_file(self._out_file('po'), merge=True)
                self.save()
            else:
                self.from_file(self._out_file('po'))
                self.load_specialty_strings()


    def is_valid(self):
        for x in self.get_invalid():
            return False
        return True

    def get_invalid(self):
        for k, indx in self.string_dict.iteritems():
            if not self.strings[indx].is_valid():
                yield (k, self.strings[indx].singular)
    
    def from_file(self, file, merge = False):
        with open(file, 'r') as handle:
            line = True
            while line:
                line = handle.readline()
                msg = ''
                while comment.match(line):
                    msg += '#'.join(line.split('#')[1:])
                    line = handle.readline()
                if msgid.match(line):
                    txt_pl = ''
                    r, txt_sing, line = get_next_str_block(line, handle)
                    # grab plural if it exists
                    if msgid_pl.match(line):
                        r, txt_pl, line = get_next_str_block(line, handle)
                    if txt_sing or txt_pl:
                        ts = TranslatedString(self, txt_sing, txt_pl, 
                                          message = msg,
                                          locale = self.locale)
                    else:
                        ts = GettextHeader(self, txt_sing, txt_pl, 
                                           message = msg,
                                           locale = self.locale)
                    key = ts.md5
                    if self.enabled.has_key(key):
                        ts.enabled = self.enabled[key]
                    while msgstr.match(line):
                        r, translation, line = get_next_str_block(line, handle)
                        ts.add(translation)

                    if not merge and not self.string_dict.has_key(key):
                        self.string_dict[key] = len(self.strings)
                        self.strings.append(ts)
                        self.sources[md5(ts.source).hexdigest()] = ts.source
                    elif merge and self.string_dict.has_key(key):
                        i = self.string_dict[key]
                        self.strings[i].msgstr = ts.msgstr
                        self.sources[md5(ts.source).hexdigest()] = ts.source

    def load_specialty_strings(self):
        from r2.lib.strings import rand_strings
        for name, rs in rand_strings:
            for desc in rs:
                message = ": randomstring:%s\n" % name
                ts = TranslatedString(self, desc, "", message = message,
                                      locale = self.locale)
                key = ts.md5
                if not self.string_dict.has_key(key):
                    self.string_dict[key] = len(self.strings)
                    self.strings.append(ts)
                else:
                    ts = self.strings[self.string_dict[key]]
                self.sources[md5(ts.source).hexdigest()] = ts.source
                ts.enabled = True


    def __getitem__(self, key):
        return self.strings[self.string_dict[key]]

    def get(self, key, alt = None):
        indx = self.string_dict.get(key)
        if indx is not None:
            return self.strings[self.string_dict[key]]
        else:
            return alt

    def set(self, key, val, indx = -1):
        s = self.get(key)
        if s: s[indx] = val


    def to_file(self, file, compile=False, mo_file=None):
        with WithWriteLock(file) as handle:
            for string in self.strings:
                handle.write(str(string))
        if compile and self.is_valid():
            if mo_file:
                out_file = mo_file
            elif file.endswith(".po"):
                out_file = file[:-3] + ".mo"
            else:
                out_file = file + ".mo"
            
            cmd = 'msgfmt -o "%s" "%s"' % (out_file, file)
            with os.popen(cmd) as handle:
                x = handle.read()
            

    def __iter__(self):
        for x in self.strings:
            yield x

    def __repr__(self):
        return "<Translation>"

    @classmethod
    def outfile(cls, locale, domain = _domain, path = _i18n_path,
                extension = 'po'):
        return os.path.join(path, locale, 'LC_MESSAGES',
                            domain + '.' + extension)

    @classmethod
    def in_use(cls, locale, domain = _domain, path = _i18n_path):
        return os.path.exists(cls.outfile(locale, domain=domain, path=path,
                                          extension='mo'))


    @classmethod
    def exists(cls, locale, domain = _domain, path = _i18n_path):
        return os.path.exists(cls.outfile(locale, domain=domain, path=path))


    def save(self, compile = False):
        self.to_file(self._out_file('po'), compile = compile,
                     mo_file = self._out_file('mo'))
        self.gen_stats()
        self.dump_slots()

    def __repr__(self):
        return "<Translator>"


    @classmethod
    def get_slots(cls, locale = 'en'):
        f = cls.outfile(locale, extension='data')
        return LoggedSlots._get_slots(f)

    def load_slots(self):
        LoggedSlots.load_slots(self)
        # clobber enabled and translation using primary template
        if self.input_file != self._out_file():
            parent_slots = TranslatorTemplate.get_slots()
            self.enabled = parent_slots.get('enabled',{})
            self.source_trans = parent_slots.get('source_trans', {})
            
        #if self.enabled:
        #    for key, state in self.enabled.iteritems():
        #        self.set_enabled(key, state)

    def gen_stats(self):
        enabled = {}
        num_completed = 0
        num_strings = 0
        for h, indx in self.string_dict.iteritems():
            s = self.strings[indx]
            enabled[h] = s.enabled
            if s.enabled:
                num_strings +=1
                if s.is_translated():
                    num_completed += 1
        self.enabled = enabled
        self.num_completed = num_completed
        self.num_total = num_strings


    @classmethod
    def get_author(cls, locale):
        slots = cls.get_slots(locale)
        return slots.get("author", set([]))

    @classmethod
    def get_name(cls, locale):
        slots = cls.get_slots(locale)
        return slots.get("name", locale)

    @classmethod
    def get_en_name(cls, locale):
        slots = cls.get_slots(locale)
        return slots.get("en_name", locale)

    @classmethod
    def is_enabled(cls, locale):
        slots = cls.get_slots(locale)
        return slots.get("_is_enabled", False)

    @classmethod
    def get_complete_frac(cls, locale):
        infos = cls.get_slots(locale)
        comp = infos.get('num_completed', 0)
        tot = infos.get('num_total', 1)
        return float(comp)/float(tot)

    def set_enabled(self, key, state = True):
        indx = self.string_dict.get(key, None)
        if indx is not None:
            self.strings[indx].enabled = state
    
def list_translations(path = _i18n_path):
    trans = []
    for lang in  os.listdir(path):
        x = os.path.join(path, lang, 'LC_MESSAGES')
        if os.path.exists(x) and os.path.isdir(x):
            if Translator.exists(lang):
                trans.append(lang)
    return trans
        
class rebuild_translations(Command):
    user_options = []
    def initialize_options(self):
        pass

    def finalize_options(self):
        pass

    def run(self, path = _i18n_path):
        _rebuild_trans(path)

def _rebuild_trans(path = _i18n_path):
    for lang in os.listdir(path):
        x = os.path.join(path, lang, 'LC_MESSAGES')
        if os.path.exists(x) and os.path.isdir(x):
            if Translator.exists(lang):
                print "recompiling '%s'" % (lang)
                t = get_translator(lang)
                t.save(compile = True)


def _get_languages(path = _i18n_path):
    trans = []
    trans_name = {}
    for lang in os.listdir(path):
        x = os.path.join(path, lang, 'LC_MESSAGES')
        if os.path.exists(x) and os.path.isdir(x):
            name = Translator.get_name(lang)
            trans_name[lang] = name
            if Translator.is_enabled(lang) and Translator.in_use(lang):
                # en is treated specially
                if lang != 'en':
                    trans.append(lang)
                    if Translator.get_complete_frac(lang) < .5:
                        name += ' (*)'
    trans.sort()
    trans.insert(0, 'en')
    trans_name['en'] = "English"
    return trans, trans_name
    


class TranslatorTemplate(Translator):
    @classmethod
    def outfile(cls, locale, domain = _domain, path = _i18n_path,
                extension = 'pot'):
        return os.path.join(path, domain + '.' + extension)

    # defunct to_file since pot file is uneditable
    def to_file(*a, **kw):
        pass

    

class AutoTranslator(Translator):
    def __init__(self, **kw):
        Translator.__init__(self, **kw)
        for string in self.strings:
            if not string.is_translated():
                string.add(self.translate(string.singular), index = 0)
                if string.plural:
                    string.add(self.translate(string.plural), index = 1)
                    
    def translate(self, string):
        s = string.split("%")
        s, d = s[0], s[1:]
        substr = re.compile("((\([^\)]+\))?([\d\.]+)?[a-zA-Z])(.*)")
        def _sub(m):
            g =  m.groups()
            return "%s%s" % (g[0], self.trans_rules(g[-1]))

        d = [self.trans_rules(s)] + [substr.sub(_sub, x) for x in d]
        return '%'.join(d)
        
    def trans_rules(self, text):
        return text

class Transliterator(AutoTranslator):
    def __init__(self, **kw):
        Translator.__init__(self, **kw)
        for string in self.strings:
            if string.is_translated() \
                    and not isinstance(string, GettextHeader):
                if string.plural:
                    string.add(self.translate(string.msgstr[0].unicode()), 
                               index = 0)
                    string.add(self.translate(string.msgstr[1].unicode()), 
                               index = 1)
                else:
                    string.add(self.translate(string.msgstr.unicode()), 
                               index = 0)
    

class USEnglishTranslator(AutoTranslator):
    def trans_rules(self, string):
        return string
        

class TamilTranslator(Transliterator):
    transliterator = dict([(u'a', u'\u0b85'),
                           (u'A', u'\u0b86'),
                           (u'i', u'\u0b87'),
                           (u'I', u'\u0b88'),
                           (u'u', u'\u0b89'),
                           (u'U', u'\u0b8a'),
                           (u'e', u'\u0b8e'),
                           (u'E', u'\u0b8f'),
                           (u'o', u'\u0b92'),
                           (u'O', u'\u0b93'),

                           (u'g', u'\u0b95\u0bcd'),
                           (u'c', u'\u0b95\u0bcd'),
                           (u'k', u'\u0b95\u0bcd'),
                           (u'q', u'\u0b95\u0bcd'),
                           (u'G', u'\u0b95\u0bcd'),
                           (u'K', u'\u0b95\u0bcd'),

                           (u's', u'\u0b9a\u0bcd'),
                           (u'C', u'\u0b9a\u0bcd'),

                           (u't', u'\u0b9f\u0bcd'),
                           (u'D', u'\u0b9f\u0bcd'),
                           (u'T', u'\u0b9f\u0bcd'),
                           (u'N', u'\u0ba3\u0bcd'),
                           (u'd', u'\u0ba4\u0bcd'),
                           (u'$', u'\u0ba8\u0bcd'), 
                           (u'n', u'\u0ba9\u0bcd'),
                           (u'B', u'\u0baa\u0bcd'),
                           (u'b', u'\u0baa\u0bcd'),
                           (u'f', u'\u0baa\u0bcd'),
                           (u'p', u'\u0baa\u0bcd'),
                           (u'F', u'\u0baa\u0bcd'),
                           (u'P', u'\u0baa\u0bcd'),
                           (u'm', u'\u0bae\u0bcd'),
                           (u'M', u'\u0bae\u0bcd'),
                           (u'y', u'\u0baf\u0bcd'),
                           (u'r', u'\u0bb0\u0bcd'),
                           (u'R', u'\u0bb1\u0bcd'),
                           (u'l', u'\u0bb2\u0bcd'),
                           (u'L', u'\u0bb3\u0bcd'),
                           (u'Z', u'\u0bb4\u0bcd'),
                           (u'z', u'\u0bb4\u0bcd'),
                           (u'v', u'\u0bb5\u0bcd'),
                           (u'w', u'\u0bb5\u0bcd'),
                           (u'V', u'\u0bb5\u0bcd'),
                           (u'W', u'\u0bb5\u0bcd'),

                           (u'Q', u'\u0b83'),

                           (u'h', u'\u0bb9\u0bcd'),
                           (u'j', u'\u0b9c\u0bcd'),
                           (u'J', u'\u0b9c\u0bcd'),
                           (u'S', u'\u0bb8\u0bcd'),
                           (u'H', u'\u0bb9\u0bcd'),

                           (u'Y', u'\u0b20\u0bcd'),
                           (u'^', u'\u0b20')])

    ligatures = ((u'\u0ba9\u0bcd\u0b95\u0bcd', u'\u0B99\u0BCD'), # ng
                 (u'\u0ba9\u0bcd\u0b9c\u0bcd', u'\u0B9e\u0BCD'), # nj
                 (u'\u0b95\u0bcd\u0bb9\u0bcd', u'\u0b9a\u0bcd'), # ch -> C
                 (u'\u0b9f\u0bcd\u0bb9\u0bcd', u'\u0ba4\u0bcd'), # th -> d
                 (u'\u0ba4\u0bcd\u0bb9\u0bcd', u'\u0ba4\u0bcd'), # dh -> d

                 (u'\u0b85\u0b85', u'\u0b86'), # aa -> A
                 (u'\u0b85\u0b87', u'\u0b90'), # ai
                 (u'\u0b85\u0b89', u'\u0b94'), # au
                 (u'\u0b87\u0b87', u'\u0b88'), # ii -> I
                 (u'\u0b89\u0b89', u'\u0b8a'), # uu -> U
                 (u'\u0b8e\u0b8e', u'\u0b8f'), # ee -> E
                 (u'\u0b92\u0b92', u'\u0b93'), # oo -> O
                 # remove accent from consonants and convert to ligature
                 # based on the subsequent vowell
                 (u'\u0bcd\u0b85', u''),
                 (u'\u0bcd\u0b86', u'\u0bbe'),
                 (u'\u0bcd\u0b87', u'\u0bbf'),
                 (u'\u0bcd\u0b88', u'\u0bc0'),
                 (u'\u0bcd\u0b89', u'\u0bc1'),
                 (u'\u0bcd\u0b8a', u'\u0bc2'),
                 (u'\u0bcd\u0b8e', u'\u0bc6'),
                 (u'\u0bcd\u0b8f', u'\u0bc7'),
                 (u'\u0bcd\u0b90', u'\u0bc8'),
                 (u'\u0bcd\u0b92', u'\u0bca'),
                 (u'\u0bcd\u0b93', u'\u0bcb'),
                 (u'\u0bcd\u0b94', u'\u0bcc'),
                 )
    
    def trans_rules(self, string):
        t = u''.join(self.transliterator.get(x, x) for x in string)
        for k, v in self.ligatures:
            t = t.replace(k, v)
        return t
            

                          

import random
class LeetTranslator(AutoTranslator):
    def trans_rules(self, string):
        key = dict(a=["4","@"], 
                   b=["8"], c=["("],
                   d=[")", "|)"], e=["3"], 
                   f=["ph"], g=["6"], 
                   i=["1", "!"], j=["_/"], 
                   k=["X"], l=["1"], o=["0"], 
                   q=["0_"], s=["5", "$"], t=["7"], 
                   z=["2"])
        s = string.lower()
        s = (random.choice(key.get(x, [x])) for x in s)
        return ''.join(s)

def get_translator(locale):
    if locale == 'leet':
        return LeetTranslator(locale = locale)
    elif locale == 'en':
        return USEnglishTranslator(locale = locale)
    elif locale == 'ta':
        return TamilTranslator(locale = locale)
    return Translator(locale = locale)
    
def get_next_str_block(line, handle):
    res = ''
    before, middle, after = qnd_parse(line)
    txt = [middle]
    res += line
    line = handle.readline()
    # grab multi-line strings for this element
    while True:
        if not str_only.match(line): break
        res += line
        b, m, a = qnd_parse(line)
        txt.append(m)
        line = handle.readline()
    return res, (''.join(txt)).replace('\\"', '"'), line


def qnd_parse(line):
    p = line.split('#')
    after = '#'.join(p[1:])
    if after: after = "#" + after
    s = p[0].split('"')
    after = s[-1] + after
    before = s[0]
    middle = '"'.join(s[1:-1])
    return before, middle,  after


########NEW FILE########
__FILENAME__ = user_stats
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from collections import defaultdict
from datetime import datetime, timedelta

import sqlalchemy as sa
from r2.models import Account, Vote, Link, Subreddit, Comment, KarmaAdjustment
from r2.lib.db import tdb_sql as tdb
from r2.lib import utils
import time

from pylons import g 
cache = g.cache

SECONDS_PER_MONTH = 86400 * 30
NUM_TOP_USERS = 15
CACHE_EXPIRY = 3600


def subreddits_with_custom_karma_multiplier():
    type = tdb.types_id[Subreddit._type_id]
    tt, dt = type.thing_table, type.data_table[0]

    aliases = tdb.alias_generator()
    karma = dt.alias(aliases.next())

    q = sa.select(
        [tt.c.thing_id],
        sa.and_(tt.c.spam == False,
              tt.c.deleted == False,
              karma.c.thing_id == tt.c.thing_id,
              karma.c.key == 'post_karma_multiplier'),
        group_by = [tt.c.thing_id],
    )

    sr_ids = [r.thing_id for r in q.execute().fetchall()]
    return Subreddit._byID(sr_ids, True, return_dict = False)


def karma_sr_weight_cases(table):
    key = table.c.key
    value_int = sa.cast(table.c.value, sa.Integer)
    cases = []

    for subreddit in subreddits_with_custom_karma_multiplier():
        mult = subreddit.post_karma_multiplier
        cases.append((key == 'karma_ups_link_' + subreddit.name, value_int * mult))
        cases.append((key == 'karma_downs_link_' + subreddit.name, value_int * -mult))
    cases.append((key.like('karma_ups_link_%'), value_int * g.post_karma_multiplier))
    cases.append((key.like('karma_downs_link_%'), value_int * -g.post_karma_multiplier))
    cases.append((key.like('karma_ups_%'), value_int))
    cases.append((key.like('karma_downs_%'), value_int * -1))
    return sa.case(cases, else_ = 0)


def top_users():
    type = tdb.types_id[Account._type_id]
    tt, dt = type.thing_table, type.data_table[0]

    aliases = tdb.alias_generator()
    account_data = dt.alias(aliases.next())

    s = sa.select(
        [tt.c.thing_id],
        sa.and_(tt.c.spam == False,
                tt.c.deleted == False,
                account_data.c.thing_id == tt.c.thing_id,
                account_data.c.key.like('karma_%')),
        group_by = [tt.c.thing_id],
        order_by = sa.desc(sa.func.sum(karma_sr_weight_cases(account_data))),
        limit = NUM_TOP_USERS)
    rows = s.execute().fetchall()
    return [r.thing_id for r in rows]


# Calculate the karma change for the given period and/or user
# TODO:  handle deleted users, spam articles and deleted articles, (and deleted comments?)
def all_user_change(*args, **kwargs):
    ret = defaultdict(lambda: (0, 0))

    for meth in user_vote_change_links, user_vote_change_comments, user_karma_adjustments:
        for aid, karma in meth(*args, **kwargs):
            karma_old = ret[aid]
            ret[aid] = (karma_old[0] + karma[0], karma_old[1] + karma[1])

    return ret


def user_vote_change_links(period=None, user=None):
    rel = Vote.rel(Account, Link)
    type = tdb.rel_types_id[rel._type_id]
    # rt = rel table
    # dt = data table
    rt, account_tt, link_tt, dt = type.rel_table

    aliases = tdb.alias_generator()
    author_dt = dt.alias(aliases.next())

    link_dt = tdb.types_id[Link._type_id].data_table[0].alias(aliases.next())

    # Create an SQL CASE statement for the subreddit vote multiplier
    cases = []
    for subreddit in subreddits_with_custom_karma_multiplier():
        cases.append( (sa.cast(link_dt.c.value,sa.Integer) == subreddit._id,
                      subreddit.post_karma_multiplier) )
    cases.append( (True, g.post_karma_multiplier) )       # The default article multiplier
    weight_cases = sa.case(cases)

    amount = sa.cast(rt.c.name, sa.Integer)
    cols = [
        author_dt.c.value,
        sa.func.sum(sa.case([(amount > 0, amount * weight_cases)], else_=0)),
        sa.func.sum(sa.case([(amount < 0, amount * -1 * weight_cases)], else_=0)),
    ]

    query = sa.and_(author_dt.c.thing_id == rt.c.rel_id,
                    author_dt.c.key == 'author_id',
                    link_tt.c.thing_id == rt.c.thing2_id,
                    link_dt.c.key == 'sr_id',
                    link_dt.c.thing_id == rt.c.thing2_id)
    if period is not None:
        earliest = datetime.now(g.tz) - timedelta(0, period)
        query.clauses.extend((rt.c.date >= earliest, link_tt.c.date >= earliest))
    if user is not None:
        query.clauses.append(author_dt.c.value == str(user._id))

    s = sa.select(cols, query, group_by=author_dt.c.value)

    rows = s.execute().fetchall()
    return [(int(r[0]), (r[1], r[2])) for r in rows]


def user_vote_change_comments(period=None, user=None):
    rel = Vote.rel(Account, Comment)
    type = tdb.rel_types_id[rel._type_id]
    # rt = rel table
    # dt = data table
    rt, account_tt, comment_tt, dt = type.rel_table

    aliases = tdb.alias_generator()
    author_dt = dt.alias(aliases.next())

    amount = sa.cast(rt.c.name, sa.Integer)
    cols = [
        author_dt.c.value,
        sa.func.sum(sa.case([(amount > 0, amount)], else_=0)),
        sa.func.sum(sa.case([(amount < 0, amount * -1)], else_=0)),
    ]

    query = sa.and_(author_dt.c.thing_id == rt.c.rel_id,
                    author_dt.c.key == 'author_id',
                    comment_tt.c.thing_id == rt.c.thing2_id)
    if period is not None:
        earliest = datetime.now(g.tz) - timedelta(0, period)
        query.clauses.extend((rt.c.date >= earliest, comment_tt.c.date >= earliest))
    if user is not None:
        query.clauses.append(author_dt.c.value == str(user._id))

    s = sa.select(cols, query, group_by=author_dt.c.value)

    rows = s.execute().fetchall()
    return [(int(r[0]), (r[1], r[2])) for r in rows]


def user_karma_adjustments(period=None, user=None):
    acct_info = tdb.types_id[Account._type_id]
    acct_thing, acct_data = acct_info.thing_table, acct_info.data_table[0]
    adj_info = tdb.types_id[KarmaAdjustment._type_id]
    adj_thing, adj_data = adj_info.thing_table, adj_info.data_table[0]

    aliases = tdb.alias_generator()
    adj_data_2 = adj_data.alias(aliases.next())

    amount = sa.cast(adj_data_2.c.value, sa.Integer)
    cols = [
        adj_data.c.value,
        sa.func.sum(sa.case([(amount > 0, amount)], else_=0)),
        sa.func.sum(sa.case([(amount < 0, amount * -1)], else_=0)),
    ]

    query = sa.and_(adj_data.c.thing_id == adj_thing.c.thing_id,
                    adj_data.c.key == 'account_id',
                    adj_data.c.thing_id == adj_data_2.c.thing_id,
                    adj_data_2.c.key == 'amount')
    if period is not None:
        earliest = datetime.now(g.tz) - timedelta(0, period)
        query.clauses.append(adj_thing.c.date >= earliest)
    if user is not None:
        query.clauses.append(adj_data.c.value == str(user._id))

    s = sa.select(cols, query, group_by=adj_data.c.value)

    rows = s.execute().fetchall()
    return [(int(r[0]), (r[1], r[2])) for r in rows]


def cache_key_user_karma(user, period):
    return 'account_{0}_karma_past_{1}_v2'.format(user._id, period)


def cached_monthly_user_change(user):
    key = cache_key_user_karma(user, SECONDS_PER_MONTH)
    ret = cache.get(key)
    if ret is not None:
        return ret

    ret = all_user_change(period=SECONDS_PER_MONTH, user=user)[user._id]
    cache.set(key, ret, CACHE_EXPIRY)
    return ret


def expire_user_change(user):
    cache.delete(cache_key_user_karma(user, SECONDS_PER_MONTH))


def cached_monthly_top_users():
    key = 'top_{0}_account_monthly_karma_v2'.format(NUM_TOP_USERS)
    ret = cache.get(key)
    if ret is not None:
        return ret

    start_time = time.time()
    ret = list(all_user_change(period=SECONDS_PER_MONTH).iteritems())
    ret.sort(key=lambda pair: -(pair[1][0] - pair[1][1]))  # karma, highest to lowest
    ret = ret[0:NUM_TOP_USERS]
    cache.set(key, ret, CACHE_EXPIRY)
    g.log.info("Calculate monthly_top_users took : %.2fs"%(time.time()-start_time))
    return ret

########NEW FILE########
__FILENAME__ = cmd_utils
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
"""
Contains utilities intended to be run from a command line
"""

from time import sleep
import sys

    
def bench_cache_lifetime(minutes):
    "Attempts to find how long a given memcached key can be expected to live"

    from pylons import g
    from r2.lib.cache import Memcache

    # we'll create an independent connection to memcached for this
    # test
    mc = Memcache(g.memcaches)

    # set N keys, and tell them not to live for longer than this test
    mc.set_multi(dict( ('bench_cache_%d' % x, x)
                       for x in xrange(minutes) ),
                 time=minutes*60)

    # and every minute, check to see that the keys are still present,
    # until we find one missing
    for x in xrange(minutes):
        if mc.get('bench_cache_%d' % x, None) is not None:
            sleep(60)
        else:
            # we found one missing
            return x-1
    else:
        # we're out of numbers, and we didn't find any missing
        # keys. Since we only set N keys, we can't check for anything
        # else
        print (("Cache lifetime is greater than %d minutes. Try again with a"+
                " higher 'minutes' value") % minutes)
        return None

def bench_cache_lifetime_multi(attempts=10, minutes=60*24):
    """
    Attempts to find the minimum, maximum, and average cache key lifetime

    Example:
        paster run production.ini r2/lib/utils/cmd_utils.py -c "bench_cache_lifetime_multi()"
    """
    total = 0
    attempts_so_far = 0
    minimum = sys.maxint
    maximum = 0

    for x in xrange(attempts):
        this_attempt = bench_cache_lifetime(minutes)
        maximum = max(this_attempt, maximum)
        minimum = min(this_attempt, minimum)

        total += this_attempt
        attempts_so_far += 1
        mean = float(total)/float(attempts_so_far)

        print ("Attempt #%d of %d: %d; min=%d, max=%d, mean=%.2f"
               % (x+1, attempts, this_attempt, minimum, maximum, mean))

    return (minimum, maximum, mean)

########NEW FILE########
__FILENAME__ = http_utils
import pytz
from datetime import datetime

DATE_RFC822 = '%a, %d %b %Y %H:%M:%S %Z'
DATE_RFC850 = '%A, %d-%b-%y %H:%M:%S %Z'
DATE_ANSI = '%a %b %d %H:%M:%S %Y'

def read_http_date(date_str):
    try:
        date = datetime.strptime(date_str, DATE_RFC822)
    except ValueError:
        try:
            date = datetime.strptime(date_str, DATE_RFC850)
        except ValueError:
            try:
                date = datetime.strptime(date_str, DATE_ANSI)
            except ValueError:
                return None
    date = date.replace(tzinfo = pytz.timezone('GMT'))
    return date

def http_date_str(date):
    date = date.astimezone(pytz.timezone('GMT'))
    return date.strftime(DATE_RFC822)

########NEW FILE########
__FILENAME__ = reporting
import sqlalchemy as sa
import datetime as dt
import time
import smtplib

from r2.lib.utils import timeago
from r2.lib.db import tdb_sql
from r2.models.mail_queue import Email

class Report(object):
    """Class for creating reports based on reddit data"""
    def __init__(self, period=None, date=None):
        """Sets up the date storage."""
        self.period = period
        self.date = date

    def append_date_clause(self, table, select, all_time=None):
        """Create the date portion of a where clause based on the time
           period specified."""
        if all_time:
            return select
        if self.period and not self.date:
            select.append_whereclause(table.c.date > timeago(self.period))
        if self.date:
            seconds = 24 * 60 * 60
            wheredate = dt.datetime.strptime(self.date,"%Y%m%d")
            select.append_whereclause(table.c.date >= wheredate)
            select.append_whereclause((table.c.date < wheredate
                                + dt.timedelta(0, seconds)))
        return select

    def total_things(self, table_name, spam=None, all_time=None):
        """Return totals based on items in the thing tables."""
        t = tdb_sql.types_name[table_name]['thing_table']
        s = sa.select([sa.func.count(t.c.thing_id)])
        if spam:
            s.append_whereclause(t.c.spam==spam)
            s.append_whereclause(t.c.deleted=='f')
        s = self.append_date_clause(t, s, all_time=all_time)

        return s.execute().fetchone()[0]

    def total_relation(self, table_name, key, value=None, all_time=None):
        """Return totals based on relationship data."""
        rel_table = tdb_sql.rel_types_name['%s_account_link' % table_name].rel_table
        t1, t2 = rel_table[0], rel_table[3]

        s = sa.select([sa.func.count(t1.c.date)], 
                      sa.and_(t1.c.rel_id == t2.c.thing_id, t2.c.key == key))
        if value:
            s.append_whereclause(t2.c.value == value)
        s = self.append_date_clause(t1, s, all_time=all_time)
        return s.execute().fetchone()[0]

    def email_stats(self, table_name, all_time=None):
        """Calculate stats based on the email tables."""
        t = getattr(Email.handler, '%s_table' % table_name)
        s = sa.select([sa.func.count(t.c.kind)])
        s = self.append_date_clause(t, s, all_time=all_time)
        return s.execute().fetchone()[0]
                      
    def css_stats(self, val, all_time=None):
        """Create stats related to custom css and headers."""
        t = tdb_sql.types_name['subreddit'].data_table[0]
        s = sa.select([sa.func.count(t.c.key)], t.c.key == val)
        return s.execute().fetchone()[0]
   

class TextReport(object):
    """Class for building text based reports"""
    def __init__(self, period, date):
        self.r = Report(period=period, date=date)
        self.rep = ''
        self.period = period
        self.date = date
        if period:
            self.phrase = "in the last"
            self.time = period
        if date:
            self.phrase = "on"
            self.time = self.pretty_date(date)

    def _thing_stats(self, thing, all_time=None):
        """return a header and a list of thing stats"""
        header = "%ss created " % thing
        if all_time:
            header += "since the beginning:"
        else:
            header += "%s %s:" % (self.phrase, self.time)
        columns = ['all', 'spam', 'non-spam']
        data = [str(self.r.total_things(thing, all_time=all_time)),
                str(self.r.total_things(thing, spam="t", all_time=all_time)),
                str(self.r.total_things(thing, spam="f", all_time=all_time))]
        return [[header], columns, data]

    def pretty_date(self, date):
        """Makes a pretty date from a date"""
        return time.strftime("%a, %b %d, %Y", time.strptime(date,"%Y%m%d"))

    def process_things(self, things, all_time=None):
        """builds a report for a list of things"""
        ret = ''
        for thing in things:
            (header, columns, data) = self._thing_stats(thing, all_time=all_time)
            ret += '\n'.join(['\t'.join(header), '\t'.join(columns), '\t'.join(data)])
            ret += '\n'
        return ret

    def process_relation(self, name, table_name, key, value, all_time=None):
        """build a report for a relation"""
        ret = ("%d\tTotal %s %s %s\n" % 
            (self.r.total_relation(table_name, key, value=value, all_time=all_time),
             name, self.phrase, self.time))
        return ret

    def process_other(self, type, name, table_name, all_time=None):
        """build other types of reports"""
        if type == 'email':
            f = self.r.email_stats
        if type == 'css':
            f = self.r.css_stats
        ret = ("%d\tTotal %s %s %s\n" % 
               (f(table_name, all_time=all_time), name, self.phrase, self.time))
        return ret

    def build(self, show_all_time=True):
        """build a complete text report"""
        rep = 'Subject: reddit stats %s %s\n\n' % (self.phrase, self.time)
        
        rep += self.process_things(['account','subreddit','link','message','comment'])
        
        rep += "\n"
        rep += self.process_relation('valid votes', 'vote', 'valid_thing', 't')
        rep += self.process_relation('organic votes', 'vote', 'organic', 't')
        rep += self.process_relation('votes', 'vote', 'valid_thing', None)
        rep += self.process_relation('reports', 'report', 'amount', None)

        rep += self.process_other('email', 'share emails sent', 'track')
        rep += self.process_other('email', 'share emails rejected', 'reject')

        if show_all_time:
            rep += self.process_other('css', 'subreddits with custom css', 
                                      'stylesheet_hash', all_time=True)
            rep += self.process_other('css', 'subreddits with a custom header', 
                                      'header', all_time=True)
            rep += "\n"
            rep += self.process_things(['account','subreddit','link','message','comment'], 
                                   all_time=True)

        return rep

def yesterday():
    """return yesterday's date"""
    return "%04d%02d%02d" % time.localtime(time.time() - 60*60*24)[0:3]
    
def run(period=None, date=None, show_all_time=True, 
        sender=None, recipients=None, smtpserver=None):
    if not date and not period:
        date = yesterday()
    report = TextReport(period, date).build(show_all_time=show_all_time)
    if sender:
        session = smtplib.SMTP(smtpserver)
        report = "To: %s\n" % ', '.join(recipients) + report
        smtpresult = session.sendmail(sender, recipients, report)
    else:
        print report

########NEW FILE########
__FILENAME__ = thing_utils
from datetime import datetime
import pytz

def make_last_modified():
    last_modified = datetime.now(pytz.timezone('GMT'))
    last_modified = last_modified.replace(microsecond = 0)
    return last_modified

def is_modified_since(thing, action, date):
    """Returns true if the date is older than the last_[action] date,
    which means a 304 should be returned. Otherwise returns the date
    that should be sent as the last-modified header."""
    from pylons import g

    prop = 'last_' + action
    if not hasattr(thing, prop):
        last_modified = make_last_modified()
        setattr(thing, prop, last_modified)
        thing._commit()
    else:
        last_modified = getattr(thing, prop)

    if not date or date < last_modified:
        return last_modified
    
    #if a date was passed in and it's equal to last modified
    return True

def set_last_modified(thing, action):
    from pylons import g
    setattr(thing, 'last_' + action, make_last_modified())
    thing._commit()

########NEW FILE########
__FILENAME__ = utils
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from urllib import unquote_plus, quote_plus, urlopen, urlencode
from urlparse import urlparse, urlunparse
from threading import local, Thread
import Queue
from copy import deepcopy
import cPickle as pickle
import re, datetime, math, random, string, os, yaml

from datetime import datetime, timedelta, tzinfo
from pylons.i18n import ungettext, _
from r2.lib.filters import _force_unicode
from mako.filters import url_escape, url_unescape
from pylons import g        

iters = (list, tuple, set)

def tup(item, ret_is_single=False):
    """Forces casting of item to a tuple (for a list) or generates a
    single element tuple (for anything else)"""
    #return true for iterables, except for strings, which is what we want
    if hasattr(item, '__iter__'):
        return (item, False) if ret_is_single else item
    else:
        return ((item,), True) if ret_is_single else (item,)

def randstr(len, reallyrandom = False):
    """If reallyrandom = False, generates a random alphanumeric string
    (base-36 compatible) of length len.  If reallyrandom, add
    uppercase and punctuation (which we'll call 'base-93' for the sake
    of argument) and suitable for use as salt."""
    alphabet = 'abcdefghijklmnopqrstuvwxyz0123456789'
    if reallyrandom:
        alphabet += 'ABCDEFGHIJKLMNOPQRSTUVWXYZ!#$%&\()*+,-./:;<=>?@[\\]^_{|}~'
    return ''.join(random.choice(alphabet)
                   for i in range(len))


class Storage(dict):
    """
    A Storage object is like a dictionary except `obj.foo` can be used
    in addition to `obj['foo']`.
    
        >>> o = storage(a=1)
        >>> o.a
        1
        >>> o['a']
        1
        >>> o.a = 2
        >>> o['a']
        2
        >>> del o.a
        >>> o.a
        Traceback (most recent call last):
            ...
        AttributeError: 'a'
    
    """
    def __getattr__(self, key): 
        try:
            return self[key]
        except KeyError, k:
            raise AttributeError, k

    def __setattr__(self, key, value): 
        self[key] = value

    def __delattr__(self, key):
        try:
            del self[key]
        except KeyError, k:
            raise AttributeError, k

    def __repr__(self):     
        return '<Storage ' + dict.__repr__(self) + '>'

storage = Storage

def storify(mapping, *requireds, **defaults):
    """
    Creates a `storage` object from dictionary `mapping`, raising `KeyError` if
    d doesn't have all of the keys in `requireds` and using the default 
    values for keys found in `defaults`.

    For example, `storify({'a':1, 'c':3}, b=2, c=0)` will return the equivalent of
    `storage({'a':1, 'b':2, 'c':3})`.
    
    If a `storify` value is a list (e.g. multiple values in a form submission), 
    `storify` returns the last element of the list, unless the key appears in 
    `defaults` as a list. Thus:
    
        >>> storify({'a':[1, 2]}).a
        2
        >>> storify({'a':[1, 2]}, a=[]).a
        [1, 2]
        >>> storify({'a':1}, a=[]).a
        [1]
        >>> storify({}, a=[]).a
        []
    
    Similarly, if the value has a `value` attribute, `storify will return _its_
    value, unless the key appears in `defaults` as a dictionary.
    
        >>> storify({'a':storage(value=1)}).a
        1
        >>> storify({'a':storage(value=1)}, a={}).a
        <Storage {'value': 1}>
        >>> storify({}, a={}).a
        {}
    
    """
    def getvalue(x):
        if hasattr(x, 'value'):
            return x.value
        else:
            return x
    
    stor = Storage()
    for key in requireds + tuple(mapping.keys()):
        value = mapping[key]
        if isinstance(value, list):
            if isinstance(defaults.get(key), list):
                value = [getvalue(x) for x in value]
            else:
                value = value[-1]
        if not isinstance(defaults.get(key), dict):
            value = getvalue(value)
        if isinstance(defaults.get(key), list) and not isinstance(value, list):
            value = [value]
        setattr(stor, key, value)

    for (key, value) in defaults.iteritems():
        result = value
        if hasattr(stor, key): 
            result = stor[key]
        if value == () and not isinstance(result, tuple): 
            result = (result,)
        setattr(stor, key, result)
    
    return stor

def _strips(direction, text, remove):
    if direction == 'l': 
        if text.startswith(remove): 
            return text[len(remove):]
    elif direction == 'r':
        if text.endswith(remove):   
            return text[:-len(remove)]
    else: 
        raise ValueError, "Direction needs to be r or l."
    return text

def rstrips(text, remove):
    """
    removes the string `remove` from the right of `text`

        >>> rstrips("foobar", "bar")
        'foo'
    
    """
    return _strips('r', text, remove)

def lstrips(text, remove):
    """
    removes the string `remove` from the left of `text`
    
        >>> lstrips("foobar", "foo")
        'bar'
    
    """
    return _strips('l', text, remove)

def strips(text, remove):
    """removes the string `remove` from the both sides of `text`

        >>> strips("foobarfoo", "foo")
        'bar'
    
    """
    return rstrips(lstrips(text, remove), remove)

class Results():
    def __init__(self, sa_ResultProxy, build_fn, do_batch=False):
        self.rp = sa_ResultProxy
        self.fn = build_fn
        self.do_batch = do_batch

    @property
    def rowcount(self):
        return self.rp.rowcount

    def _fetch(self, res):
        if self.do_batch:
            return self.fn(res)
        else:
            return [self.fn(row) for row in res]

    def fetchall(self):
        return self._fetch(self.rp.fetchall())

    def fetchmany(self, n):
        rows = self._fetch(self.rp.fetchmany(n))
        if rows:
            return rows
        else:
            raise StopIteration

    def fetchone(self):
        row = self.rp.fetchone()
        if row:
            if self.do_batch:
                row = tup(row)
                return self.fn(row)[0]
            else:
                return self.fn(row)
        else:
            raise StopIteration

def string2js(s):
    """adapted from http://svn.red-bean.com/bob/simplejson/trunk/simplejson/encoder.py"""
    ESCAPE = re.compile(r'[\x00-\x19\\"\b\f\n\r\t]')
    ESCAPE_ASCII = re.compile(r'([\\"/]|[^\ -~])')
    ESCAPE_DCT = {
        # escape all forward slashes to prevent </script> attack
        '/': '\\/',
        '\\': '\\\\',
        '"': '\\"',
        '\b': '\\b',
        '\f': '\\f',
        '\n': '\\n',
        '\r': '\\r',
        '\t': '\\t',
    }
    for i in range(20):
        ESCAPE_DCT.setdefault(chr(i), '\\u%04x' % (i,))

    def replace(match):
        return ESCAPE_DCT[match.group(0)]
    return '"' + ESCAPE.sub(replace, s) + '"'

r_base_url = re.compile("(?i)(?:.+?://)?(?:www[\d]*\.)?([^#]*[^#/])/?")
def base_url(url):
    res = r_base_url.findall(url)
    return (res and res[0]) or url

r_domain = re.compile("(?i)(?:.+?://)?(?:www[\d]*\.)?([^/:#?]*)")
def domain(s):
    """
        Takes a URL and returns the domain part, minus www., if
        present
    """
    res = r_domain.findall(s)
    domain = (res and res[0]) or s
    return domain.lower()

r_path_component = re.compile(".*?/(.*)")
def path_component(s):
    """
        takes a url http://www.foo.com/i/like/cheese and returns
        i/like/cheese
    """
    res = r_path_component.findall(base_url(s))
    return (res and res[0]) or s

r_title = re.compile('<title>(.*?)<\/title>', re.I|re.S)
r_charset = re.compile("<meta.*charset\W*=\W*([\w_-]+)", re.I|re.S)
r_encoding = re.compile("<?xml.*encoding=\W*([\w_-]+)", re.I|re.S)
def get_title(url):
    """Fetches the contents of url and extracts (and utf-8 encodes)
    the contents of <title>"""
    import chardet
    if not url or not url.startswith('http://'): return None
    try:
        content = urlopen(url).read()
        t = r_title.findall(content)
        if t:
            title = t[0].strip()
            en = (r_charset.findall(content) or
                  r_encoding.findall(content))
            encoding = en[0] if en else chardet.detect(content)["encoding"]
            if encoding:
                title = unicode(title, encoding).encode("utf-8")
            return title
    except: return None
       
valid_schemes = ('http', 'https', 'ftp', 'mailto')         
def sanitize_url(url, require_scheme = False):
    """Validates that the url is of the form

    scheme://domain/path/to/content#anchor?cruft

    using the python built-in urlparse.  If the url fails to validate,
    returns None.  If no scheme is provided and 'require_scheme =
    False' is set, the url is returned with scheme 'http', provided it
    otherwise validates"""
    if not url or ' ' in url:
        return

    url = url.strip()
    if url.lower() == 'self':
        return url

    u = urlparse(url)
    # first pass: make sure a scheme has been specified
    if not require_scheme and not u.scheme:
        url = 'http://' + url
        u = urlparse(url)

    if (u.scheme and u.scheme in valid_schemes
        and u.hostname and len(u.hostname) < 255
        and '%' not in u.netloc):
        return url

def timeago(interval):
    """Returns a datetime object corresponding to time 'interval' in
    the past.  Interval is of the same form as is returned by
    timetext(), i.e., '10 seconds'.  The interval must be passed in in
    English (i.e., untranslated) and the format is

    [num] second|minute|hour|day|week|month|year(s)
    """
    from pylons import g
    return datetime.now(g.tz) - timeinterval_fromstr(interval)

def timefromnow(interval):
    "The opposite of timeago"
    from pylons import g
    return datetime.now(g.tz) + timeinterval_fromstr(interval)
    
def timeinterval_fromstr(interval):
    "Used by timeago and timefromnow to generate timedeltas from friendly text"
    parts = interval.strip().split(' ')
    if len(parts) == 1:
        num = 1
        period = parts[0]
    elif len(parts) == 2:
        num, period = parts
        num = int(num)
    else:
        raise ValueError, 'format should be ([num] second|minute|etc)'
    period = rstrips(period, 's')

    d = dict(second = 1,
             minute = 60,
             hour   = 60 * 60,
             day    = 60 * 60 * 24,
             week   = 60 * 60 * 24 * 7,
             month  = 60 * 60 * 24 * 30,
             year   = 60 * 60 * 24 * 365)[period]
    delta = num * d
    return timedelta(0, delta)

def timetext(delta, resultion = 1, bare=True):
    """
    Takes a datetime object, returns the time between then and now
    as a nicely formatted string, e.g "10 minutes"
    Adapted from django which was adapted from
    http://blog.natbat.co.uk/archive/2003/Jun/14/time_since
    """
    chunks = (
      (60 * 60 * 24 * 365, lambda n: ungettext('year', 'years', n)),
      (60 * 60 * 24 * 30, lambda n: ungettext('month', 'months', n)),
      (60 * 60 * 24, lambda n : ungettext('day', 'days', n)),
      (60 * 60, lambda n: ungettext('hour', 'hours', n)),
      (60, lambda n: ungettext('minute', 'minutes', n)),
      (1, lambda n: ungettext('second', 'seconds', n))
    )
    delta = max(delta, timedelta(0))
    since = delta.days * 24 * 60 * 60 + delta.seconds
    for i, (seconds, name) in enumerate(chunks):
        count = math.floor(since / seconds)
        if count != 0:
            break

    from r2.lib.strings import strings
    if count == 0 and delta.seconds == 0 and delta != timedelta(0):
        n = math.floor(delta.microseconds / 1000)
        s = strings.number_label % (n, ungettext("millisecond", 
                                                 "milliseconds", n))
    else:
        s = strings.number_label % (count, name(int(count)))
        if resultion > 1:
            if i + 1 < len(chunks):
                # Now get the second item
                seconds2, name2 = chunks[i + 1]
                count2 = (since - (seconds * count)) / seconds2
                if count2 != 0:
                    s += ', %d %s' % (count2, name2(count2))
    if not bare: s += ' ' + _('ago')
    return s

def timesince(d, resultion = 1, bare = True):
    from pylons import g
    return timetext(datetime.now(g.tz) - d, resultion, bare)

def timeuntil(d, resultion = 1, bare = True):
    from pylons import g
    return timetext(d - datetime.now(g.tz), resultion, bare)

def epochtime(date):
    if not date:
        return "0"
    date = date.astimezone(g.tz)
    return date.strftime("%s")

def prettytime(date, seconds = False):
    date = date.astimezone(g.tz)
    return date.strftime('%d %B %Y %I:%M:%S%p' if seconds else '%d %B %Y %I:%M%p')

def rfc822format(date):
    return date.strftime('%a, %d %b %Y %H:%M:%S %z')

def usformat(date):
  """
  Format a datetime in US date format

  Makes the date readable by the Protoplasm datetime picker
  """
  return date.strftime('%m-%d-%Y %H:%M:%S')

def median(nums):
    """Find the median of a list of numbers, which is assumed to already be sorted."""
    count = len(nums)
    mid = count // 2
    if count % 2:
        return nums[mid]
    else:
        return (nums[mid - 1] + nums[mid]) / 2

def to_base(q, alphabet):
    if q < 0: raise ValueError, "must supply a positive integer"
    l = len(alphabet)
    converted = []
    while q != 0:
        q, r = divmod(q, l)
        converted.insert(0, alphabet[r])
    return "".join(converted) or '0'

def to36(q):
    return to_base(q, '0123456789abcdefghijklmnopqrstuvwxyz')

def query_string(dict):
    pairs = []
    for k,v in dict.iteritems():
        if v is not None:
            try:
                k = url_escape(unicode(k).encode('utf-8'))
                v = url_escape(unicode(v).encode('utf-8'))
                pairs.append(k + '=' + v)
            except UnicodeDecodeError:
                continue
    if pairs:
        return '?' + '&'.join(pairs)
    else:
        return ''

class UrlParser(object):
    """
    Wrapper for urlparse and urlunparse for making changes to urls.
    All attributes present on the tuple-like object returned by
    urlparse are present on this class, and are setable, with the
    exception of netloc, which is instead treated via a getter method
    as a concatenation of hostname and port.

    Unlike urlparse, this class allows the query parameters to be
    converted to a dictionary via the query_dict method (and
    correspondingly updated vi update_query).  The extension of the
    path can also be set and queried.

    The class also contains reddit-specific functions for setting,
    checking, and getting a path's subreddit.  It also can convert
    paths between in-frame and out of frame cname'd forms.

    """

    __slots__ = ['scheme', 'path', 'params', 'query',
                 'fragment', 'username', 'password', 'hostname',
                 'port', '_url_updates', '_orig_url', '_query_dict']

    valid_schemes = ('http', 'https', 'ftp', 'mailto')
    cname_get = "cnameframe"

    def __init__(self, url):
        u = urlparse(url)
        for s in self.__slots__:
            if hasattr(u, s):
                setattr(self, s, getattr(u, s))
        self._url_updates = {}
        self._orig_url    = url
        self._query_dict  = None

    def update_query(self, **updates):
        """
        Can be used instead of self.query_dict.update() to add/change
        query params in situations where the original contents are not
        required.
        """
        self._url_updates.update(updates)

    @property
    def query_dict(self):
        """
        Parses the `params' attribute of the original urlparse and
        generates a dictionary where both the keys and values have
        been url_unescape'd.  Any updates or changes to the resulting
        dict will be reflected in the updated query params
        """
        if self._query_dict is None:
            def _split(param):
                p = param.split('=')
                return (unquote_plus(p[0]),
                        unquote_plus('='.join(p[1:])))
            self._query_dict = dict(_split(p) for p in self.query.split('&')
                                    if p)
        return self._query_dict

    def path_extension(self):
        """
        Fetches the current extension of the path.
        """
        return self.path.split('/')[-1].split('.')[-1]

    def set_extension(self, extension):
        """
        Changes the extension of the path to the provided value (the
        "." should not be included in the extension as a "." is
        provided)
        """
        pieces = self.path.split('/')
        dirs = pieces[:-1]
        base = pieces[-1].split('.')
        base = '.'.join(base[:-1] if len(base) > 1 else base)
        if extension:
            base += '.' + extension
        dirs.append(base)
        self.path =  '/'.join(dirs)
        return self


    def unparse(self):
        """
        Converts the url back to a string, applying all updates made
        to the feilds thereof.

        Note: if a host name has been added and none was present
        before, will enforce scheme -> "http" unless otherwise
        specified.  Double-slashes are removed from the resultant
        path, and the query string is reconstructed only if the
        query_dict has been modified/updated.
        """
        # only parse the query params if there is an update dict
        q = self.query
        if self._url_updates or self._query_dict is not None:
            q = self._query_dict or self.query_dict
            q.update(self._url_updates)
            q = query_string(q).lstrip('?')

        # make sure the port is not doubly specified 
        if self.port and ":" in self.hostname:
            self.hostname = self.hostname.split(':')[0]

        # if there is a netloc, there had better be a scheme
        if self.netloc and not self.scheme:
            self.scheme = "http"
            
        return urlunparse((self.scheme, self.netloc,
                           self.path.replace('//', '/'),
                           self.params, q, self.fragment))

    def path_has_subreddit(self):
        """
        utility method for checking if the path starts with a
        subreddit specifier (namely /r/ or /categories/).
        """
        return (self.path.startswith('/r/') or
                self.path.startswith('/categories/'))

    def get_subreddit(self):
        """checks if the current url refers to a subreddit and returns
        that subreddit object.  The cases here are:
        
          * the hostname is unset or is g.domain, in which case it
            looks for /r/XXXX or /categories.  The default in this case
            is Default.
          * the hostname is a cname to a known subreddit.

        On failure to find a subreddit, returns None.
        """
        from pylons import g
        from r2.models import Subreddit, Sub, NotFound, Default
        try:
            if not self.hostname or self.hostname.startswith(g.domain):
                if self.path.startswith('/r/'):
                    return Subreddit._by_name(self.path.split('/')[2])
                elif self.path.startswith('/categories/'):
                    return Sub
                else:
                    return Default
            elif self.hostname:
                return Subreddit._by_domain(self.hostname)
        except NotFound:
            pass
        return None

    def is_reddit_url(self, subreddit = None):
        """utility method for seeing if the url is associated with
        reddit as we don't necessarily want to mangle non-reddit
        domains

        returns true only if hostname is nonexistant, a subdomain of
        g.domain, or a subdomain of the provided subreddit's cname.
        """
        from pylons import g
        return (not self.hostname or 
                self.hostname.endswith(g.domain) or
                (subreddit and subreddit.domain and
                 self.hostname.endswith(subreddit.domain)))

    def path_add_subreddit(self, subreddit):
        """ 
        Adds the subreddit's path to the path if another subreddit's
        prefix is not already present.
        """
        if not self.path_has_subreddit() and subreddit.path != '/categories/':
            self.path = (subreddit.path + self.path)
        return self

    @property
    def netloc(self):
        """
        Getter method which returns the hostname:port, or empty string
        if no hostname is present.
        """
        if not self.hostname:
            return ""
        elif self.port:
            return self.hostname + ":" + str(self.port)
        return self.hostname
    
    def mk_cname(self, require_frame = True, subreddit = None, port = None):
        """
        Converts a ?cnameframe url into the corresponding cnamed
        domain if applicable.  Useful for frame-busting on redirect.
        """

        # make sure the url is indeed in a frame
        if require_frame and not self.query_dict.has_key(self.cname_get):
            return self
        
        # fetch the subreddit and make sure it 
        subreddit = subreddit or self.get_subreddit()
        if subreddit and subreddit.domain:

            # no guarantee there was a scheme
            self.scheme = self.scheme or "http"

            # update the domain (preserving the port)
            self.hostname = subreddit.domain
            self.port = self.port or port

            # and remove any cnameframe GET parameters
            if self.query_dict.has_key(self.cname_get):
                del self._query_dict[self.cname_get]

            # remove the subreddit reference
            self.path = lstrips(self.path, subreddit.path)
            if not self.path.startswith('/'):
                self.path = '/' + self.path
        
        return self

    def is_in_frame(self):
        """
        Checks if the url is in a frame by determining if
        cls.cname_get is present.
        """
        return self.query_dict.has_key(self.cname_get)

    def put_in_frame(self):
        """
        Adds the cls.cname_get get parameter to the query string.
        """
        self.update_query(**{self.cname_get:random.random()})

    def __repr__(self):
        return "<URL %s>" % repr(self.unparse())


def to_js(content, callback="document.write", escape=True):
    before = after = ''
    if callback:
        before = callback + "("
        after = ");"
    if escape:
        content = string2js(content)
    return before + content + after

class TransSet(local):
    def __init__(self, items = ()):
        self.set = set(items)
        self.trans = False

    def begin(self):
        self.trans = True

    def add_engine(self, engine):
        if self.trans:
            return self.set.add(engine.begin())

    def clear(self):
        return self.set.clear()

    def __iter__(self):
        return self.set.__iter__()

    def commit(self):
        for t in self:
            t.commit()
        self.clear()

    def rollback(self):
        for t in self:
            t.rollback()
        self.clear()

    def __del__(self):
        self.commit()

def pload(fname, default = None):
    "Load a pickled object from a file"
    try:
        f = file(fname, 'r')
        d = pickle.load(f)
    except IOError:
        d = default
    else:
        f.close()
    return d

def psave(fname, d):
    "Save a pickled object into a file"
    f = file(fname, 'w')
    pickle.dump(d, f)
    f.close()

def unicode_safe(res):
    try:
        return str(res)
    except UnicodeEncodeError:
        return unicode(res).encode('utf-8')

def decompose_fullname(fullname):
    """
        decompose_fullname("t3_e4fa") ->
            (Thing, 3, 658918)
    """
    from r2.lib.db.thing import Thing,Relation
    if fullname[0] == 't':
        type_class = Thing
    elif fullname[0] == 'r':
        type_class = Relation

    type_id36, thing_id36 = fullname[1:].split('_')

    type_id = int(type_id36,36)
    id      = int(thing_id36,36)

    return (type_class, type_id, id)



class Worker:
    def __init__(self):
        self.q = Queue.Queue()
        self.t = Thread(target=self._handle)
        self.t.setDaemon(True)
        self.t.start()

    def _handle(self):
        while True:
            fn = self.q.get()
            try:
                fn()
            except:
                import traceback
                print traceback.format_exc()
                

    def do(self, fn):
        self.q.put(fn)

worker = Worker()

def asynchronous(func):
    def _asynchronous(*a, **kw):
        f = lambda: func(*a, **kw)
        worker.do(f)
    return _asynchronous
    
def cols(lst, ncols):
    """divides a list into columns, and returns the
    rows. e.g. cols('abcdef', 2) returns (('a', 'd'), ('b', 'e'), ('c',
    'f'))"""
    nrows = int(math.ceil(1.*len(lst) / ncols))
    lst = lst + [None for i in range(len(lst), nrows*ncols)]
    cols = [lst[i:i+nrows] for i in range(0, nrows*ncols, nrows)]
    rows = zip(*cols)
    rows = [filter(lambda x: x is not None, r) for r in rows]
    return rows

def fetch_things(t_class,since,until,batch_fn=None,
                 *query_params, **extra_query_dict):
    """
        Simple utility function to fetch all Things of class t_class
        (spam or not, but not deleted) that were created from 'since'
        to 'until'
    """

    from r2.lib.db.operators import asc

    if not batch_fn:
        batch_fn = lambda x: x

    query_params = ([t_class.c._date >= since,
                     t_class.c._date <  until,
                     t_class.c._spam == (True,False)]
                    + list(query_params))
    query_dict   = {'sort':  asc('_date'),
                    'limit': 100,
                    'data':  True}
    query_dict.update(extra_query_dict)

    q = t_class._query(*query_params,
                        **query_dict)
    
    orig_rules = deepcopy(q._rules)

    things = list(q)
    while things:
        things = batch_fn(things)
        for t in things:
            yield t
        q._rules = deepcopy(orig_rules)
        q._after(t)
        things = list(q)

def fetch_things2(query, chunk_size = 100, batch_fn = None):
    """Incrementally run query with a limit of chunk_size until there are
    no results left. batch_fn transforms the results for each chunk
    before returning."""
    orig_rules = deepcopy(query._rules)
    query._limit = chunk_size
    items = list(query)
    done = False
    while items and not done:
        #don't need to query again at the bottom if we didn't get enough
        if len(items) < chunk_size:
            done = True

        if batch_fn:
            items = batch_fn(items)

        for i in items:
            yield i

        if not done:
            query._rules = deepcopy(orig_rules)
            query._after(i)
            items = list(query)

def set_emptying_cache():
    """
        The default thread-local cache is a regular dictionary, which
        isn't designed for long-running processes. This sets the
        thread-local cache to be a SelfEmptyingCache, which naively
        empties itself out every N requests
    """
    from pylons import g
    from r2.lib.cache import SelfEmptyingCache
    g.cache.caches = [SelfEmptyingCache(),] + list(g.cache.caches[1:])

def find_recent_broken_things(from_time = None, delete = False):
    """
        Occasionally (usually during app-server crashes), Things will
        be partially written out to the database. Things missing data
        attributes break the contract for these things, which often
        breaks various pages. This function hunts for and destroys
        them as appropriate.
    """
    from r2.models import Link,Comment

    if not from_time:
        from_time = timeago("1 hour")

    to_time = timeago("60 seconds")

    for (cls,attrs) in ((Link,('author_id','sr_id')),
                        (Comment,('author_id','sr_id','body','link_id'))):
        find_broken_things(cls,attrs,
                           from_time, to_time,
                           delete=delete)

def find_broken_things(cls,attrs,from_time,to_time,delete = False):
    """
        Take a class and list of attributes, searching the database
        for Things of that class that are missing those attributes,
        deleting them if requested
    """
    for t in fetch_things(cls,from_time,to_time):
        for a in attrs:
            try:
                # try to retreive the attribute
                getattr(t,a)
            except AttributeError:
                # that failed; let's explicitly load it, and try again
                print "Reloading %s" % t._fullname
                t._load()
                try:
                    getattr(t,a)
                except AttributeError:
                    # it still broke. We should delete it
                    print "%s is missing '%s'" % (t._fullname,a)
                    if delete:
                        t._deleted = True
                        t._commit()
                    break

def timeit(func):
    "Run some function, and return (RunTimeInSeconds,Result)"
    before=time.time()
    res=func()
    return (time.time()-before,res)
def lineno():
    "Returns the current line number in our program."
    import inspect
    print "%s\t%s" % (datetime.now(),inspect.currentframe().f_back.f_lineno)

class IteratorChunker(object):
    def __init__(self,it):
        self.it = it
        self.done=False

    def next_chunk(self,size):
        chunk = []
        if not self.done:
            try:
                for i in xrange(size):
                    chunk.append(self.it.next())
            except StopIteration:
                self.done=True
        return chunk

def IteratorFilter(iterator, fn):
    for x in iterator:
        if fn(x):
            yield x

def UniqueIterator(iterator):
    """
    Takes an iterator and returns an iterator that returns only the
    first occurence of each entry
    """
    so_far = set()
    def no_dups(x):
        if x in so_far:
            return False
        else:
            so_far.add(x)
            return True

    return IteratorFilter(iterator, no_dups)

# def modhash(user, rand = None, test = False):
#     return user.name

# def valid_hash(user, hash):
#     return True

def check_cheating(loc):
    pass
        
def vote_hash(user, thing, note='valid'):
    return user.name

def valid_vote_hash(hash, user, thing):
    return True

def safe_eval_str(unsafe_str):
    return unsafe_str.replace('\\x3d', '=').replace('\\x26', '&')

rx_whitespace = re.compile('\s+', re.UNICODE)
rx_notsafe = re.compile('\W+', re.UNICODE)
rx_underscore = re.compile('_+', re.UNICODE)
def title_to_url(title, max_length = 50):
    """Takes a string and makes it suitable for use in URLs"""
    title = _force_unicode(title)           #make sure the title is unicode
    title = rx_whitespace.sub('_', title)   #remove whitespace
    title = rx_notsafe.sub('', title)       #remove non-printables
    title = rx_underscore.sub('_', title)   #remove double underscores
    title = title.strip('_')                #remove trailing underscores
    title = title.lower()                   #lowercase the title

    if len(title) > max_length:
        #truncate to nearest word
        title = title[:max_length]
        last_word = title.rfind('_')
        if (last_word > 0):
            title = title[:last_word]
    return title

def trace(fn):
    from pylons import g
    def new_fn(*a,**kw):
        ret = fn(*a,**kw)
        g.log.debug("Fn: %s; a=%s; kw=%s\nRet: %s"
                    % (fn,a,kw,ret))
        return ret
    return new_fn

def remote_addr(env):
  """
  Returns the remote address for the WSGI env passed

  Takes proxies into consideration
  """
  # In production the remote address is always the load balancer
  # So check X-Forwarded-For first
  # E.g. HTTP_X_FORWARDED_FOR: '66.249.72.73, 75.101.144.164'
  if env.has_key('HTTP_X_FORWARDED_FOR'):
    ips = re.split(r'\s*,\s*', env['HTTP_X_FORWARDED_FOR'])
    if len(ips) > 0:
      return ips[0]

  return env['REMOTE_ADDR']


# A class building tzinfo objects for a fixed offset.
class FixedOffset(tzinfo):
    """Fixed offset in hours east from UTC. name may be None"""
    def __init__(self, offset, name):
        self.offset = timedelta(hours = offset)
        self.name = name
        # tzinfo.__init__(self, name)

    def utcoffset(self, dt):
        return self.offset

    def tzname(self, dt):
        return self.name

    def dst(self, dt):
        return timedelta(0)


########NEW FILE########
__FILENAME__ = wiki
from r2.lib.utils import UrlParser

from urllib import urlopen
from r2.lib.filters import _force_ascii
import os, os.path, yaml, re
from lxml import etree

# Wiki is singleton like
# http://code.activestate.com/recipes/66531-singleton-we-dont-need-no-stinkin-singleton-the-bo/#c22

class Wiki(object):
  def __new__(cls, *args, **kwargs):
    if not '_the_instance' in cls.__dict__:
      cls._the_instance = object.__new__(cls)
    return cls._the_instance

  def __init__(self):
    self.filename = 'wiki.lesswrong.xml'
    object.__init__(self)

  @property
  def pathname(self):
    return os.path.join('..', 'public', 'files', self.filename)

  @property
  def cache_key(self):
    """Cache key for the wiki data"""
    statinfo = os.stat(self.pathname)
    return (self.filename + str(statinfo.st_mtime)).encode('ascii', 'ignore')

  @property
  def data(self):
    """Returns the data extracted from the wiki export XML file"""
    from pylons import g
    wiki_data = g.permacache.get(self.cache_key)

    if wiki_data is None:
      # Parse the XML file
      wiki_xml = etree.parse(self.pathname)
      wiki_data = self._process_data(wiki_xml)
      g.permacache.set(self.cache_key, wiki_data)

    return wiki_data

  def url_for_title(self, title):
      """Uses the MediaWiki API to get the URL for a wiki page
      with the given title"""
      if title is None:
          return None

      from pylons import g
      cache_key = ('wiki_url_%s' % title).encode('ascii', 'ignore')
      wiki_url = g.cache.get(cache_key)
      if wiki_url is None:
          # http://www.mediawiki.org/wiki/API:Query_-_Properties#info_.2F_in
          api = UrlParser(g.wiki_api_url)
          api.update_query(
              action = 'query',
              titles= title,
              prop = 'info',
              format = 'yaml',
              inprop = 'url'
          )

          try:
              response = urlopen(api.unparse()).read()
              parsed_response = yaml.load(response, Loader=yaml.CLoader)
              page = parsed_response['query']['pages'][0]
          except:
              return None

          wiki_url = page.get('fullurl').strip()

          # Things are created every couple of days so 12 hours seems
          # to be a reasonable cache time
          g.permacache.set(cache_key, wiki_url, time=3600 * 12)

      return wiki_url

  def sequences_for_article_url(self, url):
    """Return the article sequences extracted from the wiki export"""
    if url is None:
        return {}

    from pylons import g
    all_sequences = self.data['sequences']
    url = UrlParser(url)
    cache_key = _force_ascii(self.cache_key + url.path)

    sequences = g.permacache.get(cache_key)

    if sequences is None:
      sequences = {}
      for sequence in all_sequences:
        articles = sequence['articles']

        # Find the index of the given URL in the sequence's articles
        try:
          article_index = articles.index(url.path)
        except ValueError:
          article_index = None

        if article_index is not None:
          # The url passed is a part of the current sequence
          try:
            next_in_seq = articles[article_index + 1]
          except IndexError:
            next_in_seq = None
          prev_in_seq = articles[article_index - 1] if article_index > 0 else None

          # Add the result
          title = sequence['title']
          sequences[title] = {
            'title': title,
            'next': next_in_seq,
            'prev': prev_in_seq,
            'index': article_index
          }
      g.permacache.set(cache_key, sequences)

    return sequences

  def _process_data(self, wiki_xml):
    """This method processes the wiki data and extracts what is used"""
    MEDIAWIKI_NS = 'http://www.mediawiki.org/xml/export-0.3/'
    sequences = []
    lw_url_re = re.compile(r'\[(http://lesswrong\.com/lw/[^ ]+) [^\]]+\]')

    for page in wiki_xml.getroot().iterfind('.//{%s}page' % MEDIAWIKI_NS): # TODO: Change to use iterparse
      # Get the titles
      title = page.findtext('{%s}title' % MEDIAWIKI_NS)

      # See if this page is a sequence page
      sequence_elem = page.xpath("mw:revision[1]/mw:text[contains(., '[[Category:Sequences]]')]", namespaces={'mw': MEDIAWIKI_NS})

      if sequence_elem:
        sequence_elem = sequence_elem[0]
        articles = []

        # Find all the lesswrong urls
        for match in lw_url_re.finditer(sequence_elem.text):
          article_url = UrlParser(match.group(1))

          # Only store the path to the article
          article_path = article_url.path

          # Ensure path ends in slash
          if article_path[-1] != '/':
            article_path += '/'

          articles.append(article_path)

        sequences.append({
          'title': title,
          'articles': articles
        })
    return {'sequences': sequences}

########NEW FILE########
__FILENAME__ = wikipagecached
from pylons import g
from r2.lib.db.thing import Thing
from r2.lib.pages import *
from r2.lib.filters import remove_control_chars
from r2.models.printable import Printable
from pylons.i18n import _, ungettext
from urllib2 import Request, HTTPError, URLError, quote, urlopen
from urlparse import urlsplit,urlunsplit
from lxml.html import soupparser
from lxml.etree import tostring
from datetime import datetime

log = g.log

def missing_content():
    return "<h2>Unable to fetch wiki page.  Try again later</h2>"

def cache_time():
    return int(g.wiki_page_cache_time)

def base_url(url):
    u = urlsplit(url)
    return urlunsplit([u[0],u[1]]+['','',''])

def fetch(url):
    log.debug('fetching: %s' % url)
    req = Request(url)
    content = urlopen(req).read()
    log.debug('fetched %d bytes' % len(content))
    return content

def getParsedContent(str, elementid):
    parsed = soupparser.fromstring(remove_control_chars(str))
    try:
        elem = parsed.get_element_by_id(elementid)
    except KeyError:
        return parsed
    else:
        elem.attrib.pop('id')
        elem.set('class','wiki-content')
        return elem

class WikiPageCached:
    url_prefix = 'http://wiki.lesswrong.com/wiki/'
    needed_cache_keys = ('success', 'content', 'title', 'etag')

    def __init__(self, config):
        self.config = config
        self._page = None
        self._error = False

    @classmethod
    def get_url_for_user_page(cls, user):
        return cls.url_prefix + 'User:' + quote(user.name)

    def getPage(self):
        url = self.config['url']
        hit = g.rendercache.get(url)
        if hit and isinstance(hit, dict) and all(k in hit for k in self.needed_cache_keys):
            # The above isinstance check guards against an old format of cache items
            return hit

        try:
            txt = fetch(url)
            elem = getParsedContent(txt, self.config.get('id', 'content'))
            elem.make_links_absolute(base_url(url))
            headlines = elem.cssselect('h1 .mw-headline')
            if headlines and len(headlines)>0:
                title = headlines[0].text_content()
            else:
                title = ''

            content_type = self.config.get('content-type', 'text/html')
            etag = '"%s"' % datetime.utcnow().isoformat()
            if content_type == 'text/html':
                content = tostring(elem, method='html', encoding='utf8', with_tail=False)
            else:
                # text_content() returns an _ElementStringResult, which derives from str
                # but scgi_base.py in flup contains the following broken assertion:
                # assert type(data) is str, 'write() argument must be string'
                # it should be assert isinstance(data, str)
                # So we have to force the _ElementStringResult to be a str
                content = str(elem.text_content())
            ret = {'success': True, 'content': content, 'title': title, 'etag': etag}
        except Exception as e:
            log.warn("Unable to fetch wiki page: '%s' %s"%(url,e))
            self._error = True
            ret = {'success': False, 'content': missing_content(), 'title': '', 'etag': ''}

        g.rendercache.set(url, ret, cache_time())
        return ret

    @property
    def page(self):
        if self._page is None:
            self._page = self.getPage()
        return self._page

    @property
    def success(self):
        return self.page['success'] and 'There is currently no text in this page.' not in self.page['content']

    def content(self):
        return self.page['content']

    def etag(self):
        return self.page['etag']

    def title(self):
        return self.page['title']

    def invalidate(self):
        g.rendercache.delete(self.config['url'])
        log.debug('invalidated: %s' % self.config['url'])


class WikiPageThing(Thing, Printable):
    """
    Wiki pages are not Things. But sometimes we pretend they are, to make
    rendering more straightforward.
    """

    _nodb = True
    # These values have no real effect, but the attributes need to exist
    # in order for this class to successfully masquerade as a Thing.
    _type_name = 'wikipagething'
    _type_id = 0xdeadc0de
    _id = 0xbaddf00d

    def __init__(self, config):
        Thing.__init__(self)
        Printable.__init__(self)
        self.config = config
        self.wikipage = WikiPageCached(config)

    @staticmethod
    def cache_key(wrapped):
        return False

    @property
    def html(self):
        return self.wikipage.page['content']

    @property
    def wiki_url(self):
        return self.wikipage.config['url']

    @property
    def success(self):
        return self.wikipage.success

########NEW FILE########
__FILENAME__ = wiki_account
import urllib2, urllib, re
from pylons import g
from cookielib import CookieJar
from lxml import etree

class WikiError(Exception): pass


def create(name, password, email):
    '''Attempt to create a Less Wrong Wiki account with the given
    username, password and email. Raises a WikiError with the
    error code from MediaWiki if unsuccessful. See
    https://www.mediawiki.org/wiki/API:Account_creation#Possible_errors

    '''
    cj = CookieJar()
    endpoint = g.wiki_api_url + '?action=createaccount'
    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
    # input-type values from the html form
    formdata = { "format": "xml",
                 "name": name,
                 "password": password,
                 "email": email,
                 "language": "en",
                 "token": '' }

    content = opener.open(endpoint, urllib.urlencode(formdata)).read()
    token = etree.fromstring(content).find('.//createaccount').attrib['token']
    formdata['token'] = token
    xml = etree.fromstring(opener.open(endpoint,
                                       urllib.urlencode(formdata)).read())

    if xml.find('createaccount') is None:
        raise WikiError, xml.find('.//error').attrib['code']
    return True

def exists(name):
    '''Check if NAME is a registered account on the LessWrong Wiki.'''
    response = urllib2.urlopen(g.wiki_api_url + '?' +
                               urllib.urlencode([('action', 'query'),
                                                 ('format', 'xml'),
                                                 ('list', 'users'),
                                                 ('ususers', name)], True))
    if etree.fromstring(response.read()).find('.//user').attrib.get('missing') == '':
        return False
    return True

def valid_name(name):
    '''Check if NAME is allowable as a MediaWiki account name. MediaWiki
    will strip leading and trailing underscores and compress multiple
    consecutive underscores into a single space, so we disallow them.

    '''
    if name[0] == '_': return False
    if name[-1] == '_': return False
    if '__' in name: return False
    return True

########NEW FILE########
__FILENAME__ = workqueue
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################

from pylons import g
from Queue import Queue, Empty
from threading import Thread
from datetime import datetime, timedelta
import time

log = g.log

class WorkQueue(object):
    """A WorkQueue is a queue that takes a number of functions and runs
    them in parallel"""

    def __init__(self, jobs = [], num_workers = 5, timeout = None):
        """Creates a WorkQueue that will process jobs with num_workers
        threads. If a job takes longer than timeout seconds to run, WorkQueue
        won't wait for it to finish before claiming to be finished."""
        self.jobs = Queue()
        self.work_count = Queue(num_workers)
        self.workers = {}
        if timeout:
            self.timeout = timedelta(seconds = timeout)
        else:
            self.timeout = None

        for j in jobs:
            self.jobs.put(j)

    def monitor(self):
        """The monitoring thread. Every second it checks for finished, dead,
        or timed-out jobs and removes them from the queue."""
        while True:
            for worker, start_time in self.workers.items():
                if (not worker.isAlive() or
                    self.timeout
                    and datetime.now() - start_time > self.timeout): 

                    self.work_count.get_nowait()
                    self.jobs.task_done()
                    del self.workers[worker]

            time.sleep(1)

    def run(self):
        """The main thread for the queue. Pull a job off the job queue and
        create a thread for it."""
        while True:
            job = self.jobs.get()

            work_thread = Thread(target = job)
            work_thread.setDaemon(True)
            self.work_count.put(True)
            self.workers[work_thread] = datetime.now()
            work_thread.start()

    def start(self):
        """Spawn a monitoring thread and the main thread for this queue. """
        monitor_thread = Thread(target = self.monitor)
        monitor_thread.setDaemon(True)
        monitor_thread.start()

        main_thread = Thread(target = self.run)
        main_thread.setDaemon(True)
        main_thread.start()

    def add(self, job):
        """Put a new job on the queue."""
        self.jobs.put(job)

    def wait(self):
        """Blocks until every job that has been added to the queue is
        finished."""
        self.jobs.join()

def test():
    def make_job(n):
        import random, time
        def job():
            print 'starting %s' % n
            blah
            time.sleep(random.randint(1, 10))
            print 'ending %s' % n
        return job

    jobs = [make_job(n) for n in xrange(10)]
    wq = WorkQueue(jobs, timeout = 5)
    wq.start()
    wq.wait()

    #wq = WorkQueue()
    #wq.start()
    #wq.add(make_job(10))
    #print 'added job'
    #wq.add(make_job(3))
    #print 'added another'
    #q.wait()

    print 'DONE'


########NEW FILE########
__FILENAME__ = wrapped
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from filters import unsafe
from utils import storage

from itertools import chain
import sys
sys.setrecursionlimit(500)

class NoTemplateFound(Exception): pass

class Wrapped(object):
    
    def __init__(self, *lookups, **context):
        self.lookups = lookups
        for k, v in context.iteritems():
            setattr(self, k, v)

        if self.__class__ == Wrapped and lookups:
            self.render_class = lookups[0].__class__

    def __getattr__(self, attr):
        #print "GETATTR: " + str(attr)
        #one would think this would never happen
        if attr == 'lookups':
            raise AttributeError, attr

        res = None
        found = False
        for lookup in self.lookups:
            try:
                if attr.startswith('_t1'):
                    res = getattr(lookup, attr[3:])
                elif attr.startswith('_t2'):
                    res = getattr(lookup, attr[3:])
                else:
                    res = getattr(lookup, attr)
                found = True
                break
            except AttributeError:
                pass
            
        if not found:
            raise AttributeError, attr

        setattr(self, attr, res)
        return res


    def __repr__(self):
        return '<%s %s>' % (self.__class__.__name__, self.lookups)

    def template(self, style = 'html'):
        from r2.config.templates import tpm
        from pylons import g
        debug = g.template_debug
        template = None
        if self.__class__ == Wrapped:
            for lookup in chain(self.lookups, (self.render_class,)):
                try:
                    template = tpm.get(lookup, style, cache = not debug)
                except AttributeError:
                    continue
        else:
            try:
                template = tpm.get(self, style, cache = not debug)
            except AttributeError:
                raise NoTemplateFound, (repr(self), style)
                
        return template

    #TODO is this the best way to override style?
    def render(self, style = None):
        """Renders the template corresponding to this class in the given style."""
        from pylons import c
        style = style or c.render_style or 'html'
        template = self.template(style)
        if template:
            res = template.render(thing = self)
            return res if (style and style.startswith('api')) else unsafe(res)
        else:
            raise NoTemplateFound, repr(self)

    def part_render(self, attr, *a, **kw):
        """Renders the part of a template associated with the %def
        whose name is 'attr'.  This is used primarily by
        r2.lib.menus.Styled"""
        style = kw.get('style', 'html')
        template = self.template(style)
        dt = template.get_def(attr)
        return unsafe(dt.render(thing = self, *a, **kw))


def SimpleWrapped(**kw):
    class _SimpleWrapped(Wrapped):
        def __init__(self, *a, **kw1):
            kw.update(kw1)
            Wrapped.__init__(self, *a, **kw)
    return _SimpleWrapped

########NEW FILE########
__FILENAME__ = account
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is Reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
#
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################

from copy import copy
import time, hashlib, urllib2
from datetime import datetime
from lxml import etree

from geolocator import gislib
from pylons import c, g
from pylons.i18n import _
import sqlalchemy as sa

from r2.lib              import wiki_account
from r2.lib.db           import tdb_sql as tdb
from r2.lib.db.thing     import Thing, Relation, NotFound
from r2.lib.db.operators import lower
from r2.lib.db.userrel   import UserRel
from r2.lib.memoize      import memoize, clear_memo
from r2.lib.utils        import randstr
from r2.lib.strings      import strings, plurals
from r2.lib.base         import current_login_cookie
from r2.lib.rancode      import random_key

class AccountExists(Exception): pass
class NotEnoughKarma(Exception): pass

class Account(Thing):
    _data_int_props = Thing._data_int_props + ('report_made', 'report_correct',
                                               'report_ignored', 'spammer',
                                               'reported')
    _int_prop_prefixes = ('karma_',)
    _defaults = dict(pref_numsites = 10,
                     pref_frame = False,
                     pref_newwindow = False,
                     pref_public_votes = False,
                     pref_kibitz = False,
                     pref_hide_ups = False,
                     pref_hide_downs = True,
                     pref_min_link_score = -2,
                     pref_min_comment_score = -2,
                     pref_num_comments = g.num_comments,
                     pref_lang = 'en',
                     pref_content_langs = ('en',),
                     pref_over_18 = False,
                     pref_compress = False,
                     pref_organic = True,
                     pref_show_stylesheets = True,
                     pref_url = '',
                     pref_location = '',
                     pref_latitude = None,
                     pref_longitude = None,
                     pref_meetup_notify_enabled = False,
                     pref_meetup_notify_radius = 50,
                     pref_show_parent_comments = False,
                     email = None,
                     email_validated = True,
                     confirmation_code = 'abcde',
                     reported = 0,
                     report_made = 0,
                     report_correct = 0,
                     report_ignored = 0,
                     spammer = 0,
                     sort_options = {},
                     has_subscribed = False,
                     pref_media = 'subreddit',
                     share = {},
                     messagebanned = False,
                     dashboard_visit = datetime(2006,10,1, tzinfo = g.tz),
                     wiki_association_attempted_at = None, # None or datetime
                     wiki_account = None # None, str(account name) or the special string '__taken__', if a new
                                         # user didn't get an account because someone else already had the name.
                     )

    def karma_ups_downs(self, kind, sr = None):
        # NOTE: There is a legacy inconsistency in this method. If no subreddit
        # is specified, karma from all subreddits will be totaled, with each
        # scaled according to its karma multiplier before being summed. But if
        # a subreddit IS specified, the return value will NOT be scaled.

        assert kind in ('link', 'comment', 'adjustment')

        from subreddit import Subreddit  # prevent circular import

        # If getting karma for a single sr, it's easy
        if sr is not None:
            ups = getattr(self, 'karma_ups_{0}_{1}'.format(kind, sr.name), 0)
            downs = getattr(self, 'karma_downs_{0}_{1}'.format(kind, sr.name), 0)
            return (ups, downs)

        # Otherwise, loop through attributes and sum all karmas
        totals = [0, 0]
        for k, v in self._t.iteritems():
            for pre, idx in (('karma_ups_' + kind + '_', 0),
                              ('karma_downs_' + kind + '_', 1)):
                if k.startswith(pre):
                    karma_sr_name = k[len(pre):]
                    index = idx
                    break
            else:
                continue

            multiplier = 1
            if kind == 'link':
                try:
                    karma_sr = Subreddit._by_name(karma_sr_name)
                    multiplier = karma_sr.post_karma_multiplier
                except NotFound:
                    pass
            totals[index] += v * multiplier
        return tuple(totals)

    def karma(self, *args):
        ud = self.karma_ups_downs(*args)
        return ud[0] - ud[1]

    def percent_up(self):
        ups, downs = self.safe_karma_ups_downs

        if not downs:
            return 100.0
        else:
            return float(ups) / float(ups + downs) * 100

    def incr_karma(self, kind, sr, amt_up, amt_down):
        def do_incr(prop, amt):
            if hasattr(self, prop):
                self._incr(prop, amt)
            else:
                assert self._loaded
                setattr(self, prop, amt)
                self._commit()

        if amt_up:
            do_incr('karma_ups_{0}_{1}'.format(kind, sr.name), amt_up)
        if amt_down:
            do_incr('karma_downs_{0}_{1}'.format(kind, sr.name), amt_down)

        from r2.lib.user_stats import expire_user_change  # prevent circular import
        expire_user_change(self)

    @property
    def link_karma(self):
        return self.karma('link')

    @property
    def comment_karma(self):
        return self.karma('comment')

    @property
    def adjustment_karma(self):
        return self.karma('adjustment')

    @property
    def safe_karma_ups_downs(self):
        karmas = [self.karma_ups_downs(kind) for kind in 'link', 'comment', 'adjustment']
        return tuple(map(sum, zip(*karmas)))

    @property
    def safe_karma(self):
        pair = self.safe_karma_ups_downs
        karma = pair[0] - pair[1]
        return max(karma, 0) if karma > -1000 else karma

    @property
    def monthly_karma_ups_downs(self):
        from r2.lib.user_stats import cached_monthly_user_change
        return cached_monthly_user_change(self)

    @property
    def monthly_karma(self):
        ret = self.monthly_karma_ups_downs
        return ret[0] - ret[1]

    WIKI_INVITE = 'We were unable to determine if there is a Less Wrong wiki account registered to your account.  If you do not have an account and would like one, please go to [your preferences page](/prefs/wikiaccount).'

    def attempt_wiki_association(self):
        '''Attempt to find a wiki account with the same name as the user.'''
        with g.make_lock('wiki_associate_' + self.name):
            if self.wiki_association_attempted_at is not None: return

            from r2.models.link import Message
            self.wiki_association_attempted_at = datetime.now(g.tz)
            self.wiki_account = '__error__'

            if wiki_account.valid_name(self.name):
                try:
                    if wiki_account.exists(self.name):
                        self.wiki_account = self.name
                    else:
                        self.wiki_account = None
                        Message._new(Account._by_name(g.admin_account),
                                     self, 'Wiki Account', Account.WIKI_INVITE, None)
                except urllib2.URLError as e:
                    g.log.error('error in attempt_wiki_association()')

            self._commit()

    def create_associated_wiki_account(self, password,
                                       on_request_error=None,
                                       on_wiki_error=None):
        try:
            wiki_account.create(self.name, password, self.email)
            self.wiki_account = self.name
            self._commit()
            return True
        except urllib2.URLError as e:
            g.log.error('URLError creating wiki account')
            g.log.error(e)

            if on_request_error is not None: on_request_error()
        except wiki_account.WikiError as e:
            g.log.error('WikiError creating wiki account')
            g.log.error(e)

            from r2.lib import emailer
            if e.message == 'userexists':
                emailer.wiki_user_exists_email(self)
            else:
                emailer.wiki_failed_email(self)

            if on_wiki_error is not None: on_wiki_error()
        return False

    def downvote_cache_key(self, kind):
        """kind is 'link' or 'comment'"""
        return 'account_%d_%s_downvotes' % (self._id, kind)

    def check_downvote(self, vote_kind):
        """Checks whether this account has enough karma to cast a downvote.

        vote_kind is 'link' or 'comment' depending on the type of vote that's
        being cast.

        This makes the assumption that the user can't cast a vote for something
        on the non-current subreddit.
        """
        from r2.models.vote import Vote, Link, Comment

        def get_cached_downvotes(content_cls):
            kind = content_cls.__name__.lower()
            cache_key = self.downvote_cache_key(kind)
            downvotes = g.cache.get(cache_key)
            if downvotes is None:
                vote_cls = Vote.rel(Account, content_cls)

                # Get a count of content_cls downvotes
                type = tdb.rel_types_id[vote_cls._type_id]
                # rt = rel table
                # dt = data table
                # tt = thing table
                rt, account_tt, content_cls_tt, dt = type.rel_table

                cols = [ sa.func.count(rt.c.rel_id) ]
                where = sa.and_(rt.c.thing1_id == self._id, rt.c.name == '-1')
                query = sa.select(cols, where)
                downvotes = query.execute().scalar()

                g.cache.set(cache_key, downvotes)
            return downvotes

        link_downvote_karma = get_cached_downvotes(Link) * c.current_or_default_sr.post_karma_multiplier
        comment_downvote_karma = get_cached_downvotes(Comment)
        karma_spent = link_downvote_karma + comment_downvote_karma

        karma_balance = self.safe_karma * 4
        vote_cost = c.current_or_default_sr.post_karma_multiplier if vote_kind == 'link' else 1
        if karma_spent + vote_cost > karma_balance:
            points_needed = abs(karma_balance - karma_spent - vote_cost)
            msg = strings.not_enough_downvote_karma % (points_needed, plurals.N_points(points_needed))
            raise NotEnoughKarma(msg)

    def incr_downvote(self, delta, kind):

        """kind is link or comment"""
        try:
            g.cache.incr(self.downvote_cache_key(kind), delta)
        except ValueError, e:
            print 'Account.incr_downvote failed with: %s' % e

    def make_cookie(self, timestr = None, admin = False):
        if not self._loaded:
            self._load()
        timestr = timestr or time.strftime('%Y-%m-%dT%H:%M:%S')
        id_time = str(self._id) + ',' + timestr
        to_hash = ','.join((id_time, self.password, g.SECRET))
        if admin:
            to_hash += 'admin'
        return id_time + ',' + hashlib.sha1(to_hash).hexdigest()

    def needs_captcha(self):
        return self.safe_karma < 1

    def modhash(self):
        to_hash = ','.join((current_login_cookie(), g.SECRET))
        return hashlib.sha1(to_hash).hexdigest()

    def valid_hash(self, hash):
        return hash == self.modhash()

    @classmethod
    @memoize('account._by_name')
    def _by_name_cache(cls, name, allow_deleted = False):
        #relower name here, just in case
        deleted = (True, False) if allow_deleted else False
        q = cls._query(lower(Account.c.name) == name.lower(),
                       Account.c._spam == (True, False),
                       Account.c._deleted == deleted)

        q._limit = 1
        l = list(q)
        if l:
            return l[0]._id

    @classmethod
    def _by_name(cls, name, allow_deleted = False):
        #lower name here so there is only one cache
        uid = cls._by_name_cache(name.lower(), allow_deleted)
        if uid:
            return cls._byID(uid, True)
        else:
            raise NotFound, 'Account %s' % name

    @property
    def friends(self):
        return self.friend_ids()

    def delete(self):
        self._deleted = True
        self._commit()
        clear_memo('account._by_name', Account, self.name.lower(), False)

        #remove from friends lists
        q = Friend._query(Friend.c._thing2_id == self._id,
                          Friend.c._name == 'friend',
                          eager_load = True)
        for f in q:
            f._thing1.remove_friend(f._thing2)

    @property
    def subreddits(self):
        from subreddit import Subreddit
        return Subreddit.user_subreddits(self)

    @property
    def draft_sr_name(self):
      return self.name + "-drafts"

    @property
    def coords(self):
        if self.pref_latitude is not None and self.pref_longitude is not None:
            return (self.pref_latitude, self.pref_longitude)
        return None

    def is_within_radius(self, coords, radius):
        return self.coords is not None and \
            gislib.getDistance(self.coords, coords) <= radius

    def recent_share_emails(self):
        return self.share.get('recent', set([]))

    def add_share_emails(self, emails):
        if not emails:
            return

        if not isinstance(emails, set):
            emails = set(emails)

        self.share.setdefault('emails', {})
        share = self.share.copy()

        share_emails = share['emails']
        for e in emails:
            share_emails[e] = share_emails.get(e, 0) +1

        share['recent'] = emails

        self.share = share


class FakeAccount(Account):
    _nodb = True


def valid_cookie(cookie):
    try:
        uid, timestr, hash = cookie.split(',')
        uid = int(uid)
    except:
        return (False, False)

    try:
        account = Account._byID(uid, True)
        if account._deleted:
            return (False, False)
    except NotFound:
        return (False, False)

    if cookie == account.make_cookie(timestr, admin = False):
        return (account, False)
    elif cookie == account.make_cookie(timestr, admin = True):
        return (account, True)
    return (False, False)

def valid_login(name, password):
    try:
        a = Account._by_name(name)
    except NotFound:
        return False

    if not a._loaded: a._load()
    return valid_password(a, password)

def valid_password(a, password):
    try:
        if a.password == passhash(a.name, password, ''):
            #add a salt
            a.password = passhash(a.name, password, True)
            a._commit()
            return a
        else:
            salt = a.password[:3]
            if a.password == passhash(a.name, password, salt):
                return a
    except AttributeError:
        return False

def passhash(username, password, salt = ''):
    if salt is True:
        salt = randstr(3)
    tohash = '%s%s %s' % (salt, username, password)
    if isinstance(tohash, unicode):
        # Force tohash to be a byte string so it can be hashed
        tohash = tohash.encode('utf8')
    return salt + hashlib.sha1(tohash).hexdigest()

def change_password(user, newpassword):
    user.password = passhash(user.name, newpassword, True)
    user._commit()
    return True

#TODO reset the cache
def register(name, password, email):
    try:
        a = Account._by_name(name)
        raise AccountExists
    except NotFound:
        a = Account(name = name,
                    password = passhash(name, password, True))
        a.email = email

        a.confirmation_code = random_key(6)
        a.email_validated = False
        a.wiki_account = '__error__'

        a._commit()

        from r2.lib import emailer
        emailer.confirmation_email(a)

        if wiki_account.valid_name(name):
            def send_wiki_failed_email():
                emailer.wiki_failed_email(a)
            a.create_associated_wiki_account(password,
                                             on_request_error=send_wiki_failed_email)
        else:
            emailer.wiki_incompatible_name_email(a)

        # Clear memoization of both with and without deleted
        clear_memo('account._by_name', Account, name.lower(), True)
        clear_memo('account._by_name', Account, name.lower(), False)
        return a

class Friend(Relation(Account, Account)): pass
Account.__bases__ += (UserRel('friend', Friend),)

########NEW FILE########
__FILENAME__ = admintools
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.lib.utils import tup
from r2.models.link import Comment, Link

class AdminTools(object):
    def spam(self, thing, amount = 1, mark_as_spam = True, **kw):
        things = tup(thing)
        for t in things:
            if mark_as_spam:
                t._spam = (amount > 0)
                t._commit()

    def report(self, thing, amount = 1):
        pass

    def ban_info(self, thing):
        return {}

    def get_corrections(self, cls, min_date = None, max_date = None, limit = 50):
        return []

admintools = AdminTools()

def is_banned_IP(ip):
    return False

def is_banned_domain(dom):
    return False

def valid_thing(v, karma):
    return True

def valid_user(v, sr, karma):
    return True

def update_score(obj, up_change, down_change, new_valid_thing, old_valid_thing):
     obj._incr('_ups',   up_change)
     obj._incr('_downs', down_change)
     if isinstance(obj, Comment):
         if hasattr(obj, 'parent_id'):
             Comment._byID(obj.parent_id).incr_descendant_karma([], up_change - down_change)
         Link._byID(obj.link_id)._incr('_descendant_karma', up_change - down_change)

def compute_votes(wrapper, item):
    wrapper.upvotes   = item._ups
    wrapper.downvotes = item._downs


try:
    from r2admin.models.admintools import *
except:
    pass



########NEW FILE########
__FILENAME__ = award
from r2.lib.db.thing import Thing
from account import Account
from printable import Printable

from pylons import c, g, request

from datetime import datetime


class Award(Thing, Printable):
    _defaults = dict(reported = 0, 
                     moderator_banned = False,
                     banned_before_moderator = False,
                     is_html = False,
                     retracted = False,
                     show_response_to = False,
                     collapsed = False)

    @staticmethod
    def cache_key(wrapped):
        return False

    @classmethod
    def _new(cls, author, reason, amount, recipient, ip, date = None):
        award = Award(reason = reason,
                      amount = amount,
                      author_id = author._id,
                      recipient_id = recipient._id,
                      ip = ip,
                      date = date)
        award._commit()

    def recipient(self):
        return Account._byID(self.recipient_id, data=True)

    def _commit(self, *a, **kw):

        Thing._commit(self, *a, **kw)


########NEW FILE########
__FILENAME__ = builder
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from account import *
from link import *
from vote import *
from report import *
from subreddit import SRMember
from listing import Listing
from pylons import i18n, request, g

import subreddit

from r2.lib.wrapped import Wrapped
from r2.lib import utils
from r2.lib.db import operators
from r2.lib.cache import sgm
from r2.lib.comment_tree import link_comments
from r2.lib.menus import CommentSortMenu

from copy import deepcopy, copy

import time
from datetime import datetime,timedelta
from admintools import compute_votes, admintools

EXTRA_FACTOR = 1.5
MAX_RECURSION = 10
DEFAULT_WRAP = Wrapped

class Builder(object):
    def __init__(self, wrap = DEFAULT_WRAP, keep_fn = None):
        self.wrap = wrap
        self.keep_fn = keep_fn

    def keep_item(self, item):
        if self.keep_fn:
            return self.keep_fn(item)
        else:
            return item.keep_item(item)

    def wrap_items(self, items):
        user = c.user if c.user_is_loggedin else None

        #get authors
        #TODO pull the author stuff into add_props for links and
        #comments and messages?
        try:
            aids = set(l.author_id for l in items)
        except AttributeError:
            aids = None

        authors = Account._byID(aids, True) if aids else {}
        # srids = set(l.sr_id for l in items if hasattr(l, "sr_id"))
        subreddits = Subreddit.load_subreddits(items)

        if not user:
            can_ban_set = set()
        else:
            can_ban_set = set(id for (id,sr) in subreddits.iteritems()
                              if sr.can_ban(user))

        #get likes/dislikes
        #TODO Vote.likes should accept empty lists
        likes = Vote.likes(user, items) if user and items else {}
        reports = Report.fastreported(user, items) if user else {}

        uid = user._id if user else None

        # we'll be grabbing this in the spam processing below
        if c.user_is_admin:
            ban_info = admintools.ban_info([x for x in items if x._spam])
        elif user and len(can_ban_set) > 0:
            ban_info = admintools.ban_info(
                [ x for x in items
                  if (x._spam
                      and hasattr(x,'sr_id')
                      and x.sr_id in can_ban_set) ])
        else:
            ban_info = dict()

        types = {}
        wrapped = []
        count = 0
        for item in items:
            w = self.wrap(item)
            wrapped.append(w)

            types.setdefault(w.render_class, []).append(w)

            #TODO pull the author stuff into add_props for links and
            #comments and messages?
            try:
                w.author = authors.get(item.author_id)
                w.friend = item.author_id in user.friends if user else False
            except AttributeError:
                w.author = None
                w.friend = False

            if hasattr(item, "sr_id"):
                w.subreddit = subreddits[item.sr_id]

            vote = likes.get((user, item))
            if vote:
                w.likes = (True if vote._name == '1'
                             else False if vote._name == '-1'
                             else None)
            else:
                w.likes = None

            #definite
            w.timesince = utils.timesince(item._date)

            # update vote tallies
            compute_votes(w, item)
            
            w.score = [w.upvotes, w.downvotes]
            w.deleted = item._deleted

            w.rowstyle = w.rowstyle if hasattr(w,'rowstyle') else ''
            w.rowstyle += ' ' + ('even' if (count % 2) else 'odd')

            count += 1

            # would have called it "reported", but that is already
            # taken on the thing itself as "how many total
            # reports". Indicates whether this user reported this
            # item, and should be visible to all users
            w.report_made = reports.get((user, item, Report._field))

            # if the user can ban things on a given subreddit, or an
            # admin, then allow them to see that the item is spam, and
            # add the other spam-related display attributes
            w.show_reports = False
            w.show_spam    = False
            w.can_ban      = False
            if (c.user_is_admin
                or (user
                    and hasattr(item,'sr_id')
                    and item.sr_id in can_ban_set)):
                w.can_ban = True
                if item._spam:
                    w.show_spam = True
                    if not hasattr(item,'moderator_banned'):
                        w.moderator_banned = False

                    w.autobanned, w.banner = ban_info.get(item._fullname,
                                                              (False, None))

                elif hasattr(item,'reported') and item.reported > 0:
                    w.show_reports = True

        for cls in types.keys():
            cls.add_props(user, types[cls])

        return wrapped

    def get_items(self):
        raise NotImplementedError

    def item_iter(self, *a):
        """Iterates over the items returned by get_items"""
        raise NotImplementedError

    def must_skip(self, item):
        """whether or not to skip any item regardless of whether the builder
        was contructed with skip=true"""
        user = c.user if c.user_is_loggedin else None
        # Meetups are accessed through /meetups.
        if hasattr(item, 'subreddit') and not item.subreddit.can_view(user) and item.subreddit.name != 'meetups':
            return True

class PrecomputedBuilder(Builder):
    def __init__(self, items, **kwargs):
        Builder.__init__(self, **kwargs)
        self.items = items

    def get_items(self):
        items = self.wrap_items(self.items)
        return items, None, None, 0, len(items)

    def item_iter(self, *a):
        return a[0][0]

class QueryBuilder(Builder):
    def __init__(self, query, wrap = DEFAULT_WRAP, keep_fn = None,
                 skip = False, **kw):
        Builder.__init__(self, wrap, keep_fn)
        self.query = query
        self.skip = skip
        self.num = kw.get('num')
        self.start_count = kw.get('count', 0) or 0
        self.after = kw.get('after')
        self.reverse = kw.get('reverse')
        
        self.prewrap_fn = None
        if hasattr(query, 'prewrap_fn'):
            self.prewrap_fn = query.prewrap_fn
        #self.prewrap_fn = kw.get('prewrap_fn')

    def item_iter(self, a):
        """Iterates over the items returned by get_items"""
        for i in a[0]:
            yield i

    def init_query(self):
        q = self.query

        if self.reverse:
            q._reverse()

        q._data = True
        self.orig_rules = deepcopy(q._rules)
        if self.after:
            q._after(self.after)

    def fetch_more(self, last_item, num_have):
        done = False
        q = self.query
        if self.num:
            num_need = self.num - num_have
            if num_need <= 0:
                #will cause the loop below to break
                return True, None
            else:
                #q = self.query
                #check last_item if we have a num because we may need to iterate
                if last_item:
                    q._rules = deepcopy(self.orig_rules)
                    q._after(last_item)
                    last_item = None
                q._limit = max(int(num_need * EXTRA_FACTOR), 1)
        else:
            done = True
        new_items = list(q)

        return done, new_items

    def get_items(self):
        self.init_query()

        num_have = 0
        done = False
        items = []
        count = self.start_count
        first_item = None
        last_item = None
        have_next = True

        #for prewrap
        orig_items = {}

        #logloop
        self.loopcount = 0
        
        while not done:
            done, new_items = self.fetch_more(last_item, num_have)

            #log loop
            self.loopcount += 1
            if self.loopcount == 20:
                g.log.debug('BREAKING: %s' % self)
                done = True

            #no results, we're done
            if not new_items:
                break;

            #if fewer results than we wanted, we're done
            elif self.num and len(new_items) < self.num - num_have:
                done = True
                have_next = False

            if not first_item and self.start_count > 0:
                first_item = new_items[0]

            #pre-wrap
            if self.prewrap_fn:
                new_items2 = []
                for i in new_items:
                    new = self.prewrap_fn(i)
                    orig_items[new._id] = i
                    new_items2.append(new)
                new_items = new_items2

            #wrap
            if self.wrap:
                new_items = self.wrap_items(new_items)

            #skip and count
            while new_items and (not self.num or num_have < self.num):
                i = new_items.pop(0)
                count = count - 1 if self.reverse else count + 1
                if not (self.must_skip(i) or self.skip and not self.keep_item(i)):
                    items.append(i)
                    num_have += 1
                if self.wrap:
                    i.num = count
                last_item = i
        
        #unprewrap the last item
        if self.prewrap_fn and last_item:
            last_item = orig_items[last_item._id]

        if self.reverse:
            items.reverse()
            last_item, first_item = first_item, have_next and last_item
            before_count = count
            after_count = self.start_count - 1
        else:
            last_item = have_next and last_item
            before_count = self.start_count + 1
            after_count = count

        #listing is expecting (things, prev, next, bcount, acount)
        return (items,
                first_item,
                last_item,
                before_count,
                after_count)

class SubredditTagBuilder(QueryBuilder):
    def __init__(self, query, sr_ids, **kw):
        self.sr_ids = sr_ids
        QueryBuilder.__init__(self, query, **kw)

    def keep_item(self, item):
        if item.sr_id not in self.sr_ids:
            return False

        return True

class IDBuilder(QueryBuilder):
    def init_query(self):
        names = self.names = list(tup(self.query))

        if self.reverse:
            names.reverse()

        if self.after:
            try:
                i = names.index(self.after._fullname)
            except ValueError:
                self.names = ()
            else:
                self.names = names[i + 1:]

    def fetch_more(self, last_item, num_have):
        done = False
        names = self.names
        if self.num:
            num_need = self.num - num_have
            if num_need <= 0:
                return True, None
            else:
                if last_item:
                    last_item = None
                slice_size = max(int(num_need * EXTRA_FACTOR), 1)
        else:
            slice_size = len(names)
            done = True

        self.names, new_names = names[slice_size:], names[:slice_size]
        new_items = Thing._by_fullname(new_names, data = True, return_dict=False)

        return done, new_items

class SearchBuilder(QueryBuilder):
    def init_query(self):
        self.skip = True
        self.total_num = 0
        self.start_time = time.time()

        self.start_time = time.time()

    def keep_item(self,item):
        # doesn't use the default keep_item because we want to keep
        # things that were voted on, even if they've chosen to hide
        # them in normal listings
        if item._spam or item._deleted:
            return False
        else:
            return True


    def fetch_more(self, last_item, num_have):
        from r2.lib import solrsearch

        done = False
        limit = None
        if self.num:
            num_need = self.num - num_have
            if num_need <= 0:
                return True, None
            else:
                limit = max(int(num_need * EXTRA_FACTOR), 1)
        else:
            done = True

        search = self.query.run(after = last_item or self.after,
                                reverse = self.reverse,
                                num = limit)

        new_items = Thing._by_fullname(search.docs, data = True, return_dict=False)

        self.total_num = search.hits

        return done, new_items

class CommentBuilderMixin:
    def empty_listing(self, *things):
        parent_name = None
        for t in things:
            try:
                parent_name = t.parent_name
                break
            except AttributeError:
                continue
        l = Listing(None, None, parent_name = parent_name)
        l.things = list(things)
        return Wrapped(l)

class UnbannedCommentBuilder(QueryBuilder):
    def __init__(self, query, sr_ids, **kw):
        self.sr_ids = sr_ids
        QueryBuilder.__init__(self, query, **kw)

    def keep_item(self, item):
        link = Link._byID(item.link_id)
        if link._spam:
            return False

        if item.sr_id not in self.sr_ids:
            return False

        return super(UnbannedCommentBuilder, self).keep_item(item)

class ToplevelCommentBuilder(UnbannedCommentBuilder):
    def keep_item(self, item):
        try:
            item.parent_id
        except AttributeError:
            return True
        else:
            return False

class ContextualCommentBuilder(CommentBuilderMixin, UnbannedCommentBuilder):
    def __init__(self, query, sr_ids, **kw):
        UnbannedCommentBuilder.__init__(self, query, sr_ids, **kw)
        self.sort = CommentSortMenu.operator(CommentSortMenu.default)

    def keep_item(self, item):
        if isinstance(item, Link):
            author = Account._byID(item.author_id)
            if item.subreddit_slow.name == author.draft_sr_name and not c.user == author:
                return False
        if isinstance(item, Comment):
            link = Link._byID(item.link_id)
            if link._spam:
                return False

            if item.sr_id not in self.sr_ids:
                return False

            return super(UnbannedCommentBuilder, self).keep_item(item)
        return True

    def context_from_comment(self, comment):
        if isinstance(comment, Comment):
            link = Link._byID(comment.link_id)
            wrapped, = self.wrap_items((comment,))

            # If there are any child comments, add an expand link
            children = list(Comment._query(Comment.c.parent_id == comment._id))
            if children:
                more = Wrapped(MoreChildren(link, 0))
                more.children.extend(children)
                more.count = len(children)
                wrapped.child = self.empty_listing()
                wrapped.child.things.append(more)

            # If there's a parent comment, surround the child comment with it
            parent_id = getattr(comment, 'parent_id', None)
            if parent_id is not None:
                parent_comment = Comment._byID(parent_id)
                if parent_comment:
                    parent_wrapped, = self.wrap_items((parent_comment,))
                    parent_wrapped.child = self.empty_listing()
                    parent_wrapped.child.things.append(wrapped)
                    wrapped = parent_wrapped

            wrapped.show_response_to = True
        else:
            wrapped, = self.wrap_items((comment,))
        return wrapped

    def get_items(self):
        # call the base implementation, but defer wrapping until later
        old_wrap, self.wrap = self.wrap, None
        things, prev, next, bcount, acount = UnbannedCommentBuilder.get_items(self)
        self.wrap = old_wrap

        things = map(self.context_from_comment, things)
        return things, prev, next, bcount, acount

class CommentBuilder(CommentBuilderMixin, Builder):
    def __init__(self, link, sort, comment = None, context = None, wrap = DEFAULT_WRAP):
        Builder.__init__(self, wrap = wrap)
        self.link = link
        self.comment = comment
        self.context = context

        if sort.col == '_date':
            self.sort_key = lambda x: x._date
        else:
            self.sort_key = lambda x: (getattr(x, sort.col), x._date)
        self.rev_sort = True if isinstance(sort, operators.desc) else False

    def item_iter(self, a):
        for i in a:
            yield i
            if hasattr(i, 'child'):
                for j in self.item_iter(i.child.things):
                    yield j

    def get_items(self, num, nested = True, starting_depth = 0):
        r = link_comments(self.link._id)
        cids, comment_tree, depth, num_children = r
        if cids:
            comment_dict = Comment._byID(cids, data = True, return_dict = True)
        else:
            comment_dict = {}

        #convert tree from lists of IDs into lists of objects
        for pid, cids in comment_tree.iteritems():
            tree = [comment_dict.get(cid) for cid in cids]
            comment_tree[pid] = [c for c in tree if c is not None]

        items = []
        extra = {}
        top = None
        dont_collapse = []
        #loading a portion of the tree
        if isinstance(self.comment, utils.iters):
            candidates = []
            candidates.extend(self.comment)
            dont_collapse.extend(cm._id for cm in self.comment)
            top = self.comment[0]
            #assume the comments all have the same parent
            # TODO: removed by Chris to get rid of parent being sent
            # when morecomments is used.  
            #if hasattr(candidates[0], "parent_id"):
            #    parent = comment_dict[candidates[0].parent_id]
            #    items.append(parent)
        #if permalink
        elif self.comment:
            top = self.comment
            dont_collapse.append(top._id)
            #add parents for context
            while self.context > 0 and hasattr(top, 'parent_id'):
                self.context -= 1
                new_top = comment_dict[top.parent_id]
                comment_tree[new_top._id] = [top]
                num_children[new_top._id] = num_children[top._id] + 1
                dont_collapse.append(new_top._id)
                top = new_top
            candidates = [top]
        #else start with the root comments
        else:
            candidates = []
            candidates.extend(comment_tree.get(top, ()))

        #update the starting depth if required
        if top and depth[top._id] > 0:
            delta = depth[top._id]
            for k, v in depth.iteritems():
                depth[k] = v - delta

        def sort_candidates():
            candidates.sort(key = self.sort_key, reverse = self.rev_sort)
        
        #find the comments
        num_have = 0
        sort_candidates()
        while num_have < num and candidates:
            to_add = candidates.pop(0)
            if to_add._deleted and not comment_tree.has_key(to_add._id):
                pass
            elif depth[to_add._id] < MAX_RECURSION:
                #add children
                if comment_tree.has_key(to_add._id):
                    candidates.extend(comment_tree[to_add._id])
                    sort_candidates()
                items.append(to_add)
                num_have += 1
            else:
                #add the recursion limit
                p_id = to_add.parent_id
                w = Wrapped(MoreRecursion(self.link, 0,
                                          comment_dict[p_id]))
                w.children.append(to_add)
                extra[p_id] = w

        wrapped = self.wrap_items(items)

        cids = dict((cm._id, cm) for cm in wrapped)
        
        final = []
        #make tree

        for cm in wrapped:
            # don't show spam with no children
            if cm.deleted and not comment_tree.has_key(cm._id):
                continue

            cm.num_children = num_children[cm._id]
            if cm.collapsed and cm._id in dont_collapse:
                cm.collapsed = False
            if cm.collapse_in_link_threads:
                cm.collapsed = True

            parent = cids.get(cm.parent_id) if hasattr(cm, 'parent_id') else None
            if parent:
                if not hasattr(parent, 'child'):
                    parent.child = self.empty_listing()
                parent.child.parent_name = parent._fullname
                parent.child.things.append(cm)
            else:
                final.append(cm)

        #put the extras in the tree
        for p_id, morelink in extra.iteritems():
            parent = cids[p_id]
            parent.child = self.empty_listing(morelink)
            parent.child.parent_name = parent._fullname

        #put the remaining comments into the tree (the show more comments link)
        more_comments = {}
        while candidates:
            to_add = candidates.pop(0)
            direct_child = True
            #ignore top-level comments for now
            if not hasattr(to_add, 'parent_id'):
                p_id = None
            else:
                #find the parent actually being displayed
                #direct_child is whether the comment is 'top-level'
                p_id = to_add.parent_id
                while p_id and not cids.has_key(p_id):
                    p = comment_dict[p_id]
                    if hasattr(p, 'parent_id'):
                        p_id = p.parent_id
                    else:
                        p_id = None
                    direct_child = False

            mc2 = more_comments.get(p_id)
            if not mc2:
                mc2 = MoreChildren(self.link, depth[to_add._id],
                                   parent = comment_dict.get(p_id))
                more_comments[p_id] = mc2
                w_mc2 = Wrapped(mc2)
                if p_id is None:
                    final.append(w_mc2)
                else:
                    parent = cids[p_id]
                    if hasattr(parent, 'child'):
                        parent.child.things.append(w_mc2)
                    else:
                        parent.child = self.empty_listing(w_mc2)
                        parent.child.parent_name = parent._fullname

            #add more children
            if comment_tree.has_key(to_add._id):
                candidates.extend(comment_tree[to_add._id])
                
            if direct_child:
                mc2.children.append(to_add)

            mc2.count += 1

        return final

########NEW FILE########
__FILENAME__ = edit
from r2.lib.db.thing import Thing
from account import Account
from link import Link

import difflib

class Edit(Thing):
    """Used to track edits on Link"""

    @classmethod
    def _new(cls, link, user, new_content):
        return Edit(link_id = link._id,
                    author_id = user._id,
                    diff = Edit.create_diff(link.article, new_content))

    @staticmethod
    def create_diff(old_content, new_content):
        return list(difflib.unified_diff(old_content.splitlines(), new_content.splitlines()))

    @classmethod
    def add_props(cls, user, wrapped):
        for item in wrapped:
            item.permalink = item.link.make_permalink_slow()

    @staticmethod
    def cache_key(wrapped):
        return False

    def keep_item(self, wrapped):
        return True

    @property
    def link(self):
        return Link._byID(self.link_id, data=True)

    @property
    def link_author(self):
        return Account._byID(self.link.author_id, data=True)

    diff_marker_to_class = {
        "+" : "new",
        "-" : "del",
        "@" : "context"
    }

    @staticmethod
    def diff_line_style(line):
        """Used in the template to find the css class for each diff line"""
        if len(line)<=0: return ""
        return Edit.diff_marker_to_class.get(line[0],"")


########NEW FILE########
__FILENAME__ = image_holder
class ImageHolder(object):
    def get_images(self):
        """
        Iterator over list of (name, image_num) pairs which have been
        uploaded for custom styling of this subreddit.
        """
        if self.images is None:
            self.images = {}
        for name, img_num in self.images.iteritems():
            if isinstance(img_num, int):
                yield (name, img_num)

    def add_image(self, name, max_num = None):
        """
        Adds an image to the image holder's image list.  The resulting
        number of the image is returned.  Note that image numbers are
        non-sequential insofar as unused numbers in an existing range
        will be populated before a number outside the range is
        returned.  Imaged deleted with del_image are pushed onto the
        "/empties/" stack in the images dict, and those values are
        pop'd until the stack is empty.

        raises ValueError if the resulting number is >= max_num.

        The ImageHolder will be _dirty if a new image has been added to
        its images list, and no _commit is called.
        """
        if self.images is None:
            self.images = {}

        if name in self.images:
            # we've seen the image before, so just return the existing num
            return self.images[name]
        # copy and blank out the images list to flag as _dirty
        l = self.images
        self.images = None
        # initialize the /empties/ list
        l.setdefault('/empties/', [])
        try:
            num = l['/empties/'].pop() # grab old number if we can
        except IndexError:
            num = len(l) - 1 # one less to account for /empties/ key
        if max_num is not None and num >= max_num:
            raise ValueError, "too many images"
        # update the dictionary and rewrite to images attr
        l[name] = num
        self.images = l
        return num

    def del_image(self, name):
        """
        Deletes an image from the images dictionary assuming an image
        of that name is in the current dictionary.  The freed up
        number is pushed onto the /empties/ stack for later recycling
        by add_image.

        The Subreddit will be _dirty if image has been removed from
        its images list, and no _commit is called.
        """
        if self.images is None or name not in self.images:
            return
        l = self.images
        self.images = None
        l.setdefault('/empties/', [])
        # push the number on the empties list
        l['/empties/'].append(l[name])
        del l[name]
        self.images = l


########NEW FILE########
__FILENAME__ = karma_adjustment
from r2.lib.db.thing import Thing


class KarmaAdjustment(Thing):
    @classmethod
    def store(cls, account, sr, amount):
        adjustment = cls(account_id = account._id, sr_id = sr._id, amount = amount)
        adjustment._commit()


########NEW FILE########
__FILENAME__ = link
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.lib.db.thing import Thing, Relation, NotFound, MultiRelation, \
     CreationError
from r2.lib.utils import base_url, tup, domain, worker, title_to_url, \
     UrlParser, set_last_modified
from account import Account
from subreddit import Subreddit
from printable import Printable
import thing_changes as tc
from r2.config import cache
from r2.lib.memoize import memoize, clear_memo
from r2.lib import utils
from r2.lib.wiki import Wiki
from mako.filters import url_escape
from r2.lib.strings import strings, Score
from r2.lib.db.operators import lower
from r2.lib.db import operators
from r2.lib.filters import _force_unicode
from r2.models.subreddit import FakeSubreddit
from r2.models.image_holder import ImageHolder
from r2.models.poll import containspolls, parsepolls

from pylons import c, g, request
from pylons.i18n import ungettext

import re
import random
import urllib
from datetime import datetime

class LinkExists(Exception): pass

# defining types
class Link(Thing, Printable, ImageHolder):
    _data_int_props = Thing._data_int_props + ('num_comments', 'reported')
    _defaults = dict(is_self = False,
                     reported = 0, num_comments = 0,
                     moderator_banned = False,
                     banned_before_moderator = False,
                     media_object = None,
                     has_thumbnail = False,
                     promoted = False,
                     promoted_subscribersonly = False,
                     promote_until = None,
                     promoted_by = None,
                     disable_comments = False,
                     ip = '0.0.0.0',
                     render_full = False,
                     images = None,
                     blessed = False,
                     comments_enabled = True,
                     notify_on_comment = False,
                     cc_licensed = False,
                     _descendant_karma = 0)

    _only_whitespace = re.compile('^\s*$', re.UNICODE)
    _more_marker = '<a id="more"></a>'

    def __init__(self, *a, **kw):
        Thing.__init__(self, *a, **kw)

    @classmethod
    def by_url_key(cls, url):
        return base_url(url.lower()).encode('utf8')

    @classmethod
    def _by_url(cls, url, sr):
        from subreddit import Default
        if sr == Default:
            sr = None
            
        url = cls.by_url_key(url)
        link_ids = g.permacache.get(url)
        if link_ids:
            links = Link._byID(link_ids, data = True, return_dict = False)
            links = [l for l in links if not l._deleted]

            if links and sr:
                for link in links:
                    if sr._id == link.sr_id:
                        return link
            elif links:
                return links

        raise NotFound, 'Link "%s"' % url


    def can_submit(self, user):
        if c.user_is_admin:
            return True
        else:
            sr = Subreddit._byID(self.sr_id, data=True)

            if sr.is_editor(c.user):
                return True
            elif self.author_id == c.user._id:
                # They can submit if they are the author and still have access
                # to the subreddit of the article
                return sr.can_submit(user)
            else:
                return False

    def is_blessed(self):
        return self.blessed

    def set_url_cache(self):
        if self.url != 'self':
            key = self.by_url_key(self.url)
            link_ids = g.permacache.get(key) or []
            if self._id not in link_ids:
                link_ids.append(self._id)
            g.permacache.set(key, link_ids)

    def update_url_cache(self, old_url):
        """Remove the old url from the by_url cache then update the
        cache with the new url."""
        if old_url != 'self':
            key = self.by_url_key(old_url)
            link_ids = g.permacache.get(key) or []
            while self._id in link_ids:
                link_ids.remove(self._id)
            g.permacache.set(key, link_ids)
        self.set_url_cache()

    @property
    def already_submitted_link(self):
        return self.make_permalink_slow() + '?already_submitted=true'

    def resubmit_link(self, sr_url = False):
        submit_url  = self.subreddit_slow.path if sr_url else '/'
        submit_url += 'submit?resubmit=true&url=' + url_escape(self.url)
        return submit_url

    @classmethod
    def _submit(cls, title, article, author, sr, ip, tags, spam = False, date = None, **kwargs):
        # Create the Post and commit to db.
        l = cls(title = title,
                url = 'self',
                _spam = spam,
                author_id = author._id,
                sr_id = sr._id, 
                lang = sr.lang,
                ip = ip,
                article = article,
                date = date,
                **kwargs
                )
        l._commit()

        # Now that the post id is known update the Post with the correct permalink.
        l.url = l.make_permalink_slow()
        l.is_self = True
        l._commit()

        # Parse and create polls in the article
        l.set_article(article)

        l.set_url_cache()

        # Add tags
        for tag in tags:
            l.add_tag(tag)

        return l
        
    def set_article(self, article):
        self.article = article
        self._commit()
    


    def _summary(self):
        if hasattr(self, 'article'):
            return self.article.split(self._more_marker)[0]
            
    def _has_more(self):
        if hasattr(self, 'article'):
            return self.article.find(self._more_marker) >= 0
            
    def _more(self):
        if hasattr(self, 'article'):
            return self.article.split(self._more_marker)[1]

    def _meta_description(self):
        if not hasattr(self, 'article'):
            return None

        import lxml
        description = ''
        try:
            description = lxml.html.document_fromstring(self.article).text_content()
        except (lxml.etree.ParserError, lxml.etree.XMLSyntaxError):
            description = re.sub("<[^>]+>", "", self.article)
        except Exception as e:
            g.log.warning("Unexpected error parsing article for link %s: %s %s" % (self._id, e.__class__, str(e)))

        return description[:160]

    @classmethod
    def _somethinged(cls, rel, user, link, name):
        return rel._fast_query(tup(user), tup(link), name = name)

    def _something(self, rel, user, somethinged, name):
        try:
            saved = rel(user, self, name=name)
            saved._commit()
            return saved
        except CreationError, e:
            return somethinged(user, self)[(user, self, name)]

    def _unsomething(self, user, somethinged, name):
        saved = somethinged(user, self)[(user, self, name)]
        if saved:
            saved._delete()
            return saved

    @classmethod
    def _saved(cls, user, link):
        return cls._somethinged(SaveHide, user, link, 'save')

    def _save(self, user):
        return self._something(SaveHide, user, self._saved, 'save')

    def _unsave(self, user):
        return self._unsomething(user, self._saved, 'save')

    def add_subscriber(self, user):
        return self._something(Subscription, user, self.user_subscribed, 'subscription')

    def remove_subscriber(self, user):
        return self._unsomething(user, self.user_subscribed, 'subscription')

    @classmethod
    def user_subscribed(cls, user, link):
        return cls._somethinged(Subscription, user, link, 'subscription')

    @classmethod
    def link_subscribed(cls, user, link):
        return cls._somethinged(Subscription, user, link, 'subscription')[(user,link,'subscription')]

    @classmethod
    def _clicked(cls, user, link):
        return cls._somethinged(Click, user, link, 'click')

    def _updateClickFromObj(obj):
        obj = c[(user,self,'click')]
        obj._date = datetime.now(g.tz)
        obj._commit()

    def _tryUpdateClick(self, user):
        obj = Link._clicked(user,self)[(user,self,'click')]
        if obj:
            obj._date = datetime.now(g.tz)
            obj._commit()
            return True
        return False

    def _click(self, user):
        if self._tryUpdateClick(user):
            return
        # No click in the db to update, try and create.
        try:
            saved = Click(user, self, name='click')
            saved._commit()
            return
        except CreationError, e:
            # This is for a possible race.  It is possible the row in the db
            # has been created but the cache not updated yet. This explicitly
            # clears the cache then re-gets from the db
            g.log.info("Trying cache clear for lookup : "+str((user,self,'click')))
            Click._uncache(user, self, name='click')
            if self._tryUpdateClick(user):
                return

            raise Exception(user,self,e)

    def _getLastClickTime(self, user):
        c = Link._clicked(user,self)
        return c.get((user, self, 'click'))

    @classmethod
    def _hidden(cls, user, link):
        return cls._somethinged(SaveHide, user, link, 'hide')

    def _hide(self, user):
        return self._something(SaveHide, user, self._hidden, 'hide')

    def _unhide(self, user):
        return self._unsomething(user, self._hidden, 'hide')

    def keep_item(self, wrapped):
        user = c.user if c.user_is_loggedin else None

        if not c.user_is_admin:
            if self._spam and (not user or
                               (user and self.author_id != user._id)):
                return False
        
            #author_karma = wrapped.author.link_karma
            #if author_karma <= 0 and random.randint(author_karma, 0) != 0:
                #return False

        if user:
            if user.pref_hide_ups and wrapped.likes == True:
                return False
        
            if user.pref_hide_downs and wrapped.likes == False:
                return False

            if wrapped._score < user.pref_min_link_score:
                return False

            if wrapped.hidden:
                return False

        return True

    @staticmethod
    def cache_key(wrapped):
        if c.user_is_admin:
            return False
        if hasattr(wrapped, 'has_polls') and wrapped.has_polls:
            return False

        s = (str(i) for i in (wrapped.render_class.__name__,
                              wrapped._fullname,
                              bool(c.user_is_sponsor),
                              bool(c.user_is_loggedin),
                              wrapped.subreddit == c.site,
                              c.user.pref_newwindow,
                              c.user.pref_frame,
                              c.user.pref_compress,
                              c.user.pref_media,
                              request.host,
                              c.cname, 
                              wrapped.author == c.user,
                              wrapped.likes,
                              wrapped.saved,
                              wrapped.clicked,
                              wrapped.hidden,
                              wrapped.friend,
                              wrapped.show_spam,
                              wrapped.show_reports,
                              wrapped.can_ban,
                              wrapped.thumbnail,
                              wrapped.moderator_banned,
                              wrapped.render_full,
                              wrapped.comments_enabled,
                              wrapped.votable))
        # htmllite depends on other get params
        s = ''.join(s)
        if c.render_style == "htmllite":
            s += ''.join(map(str, [request.get.has_key('style'),
                                   request.get.has_key('expanded'),
                                   request.get.has_key('twocolumn'),
                                   c.bgcolor,
                                   c.bordercolor]))
        return s

    def make_permalink(self, sr, force_domain = False, sr_path = False):
        from r2.lib.template_helpers import get_domain
        p = "lw/%s/%s/" % (self._id36, title_to_url(self.title))
        if c.default_sr and not sr_path:
            res = "/%s" % p
        elif sr and not c.cname:
            res = "/r/%s/%s" % (sr.name, p)
        elif sr != c.site or force_domain:
            res = "http://%s/%s" % (get_domain(cname = (c.cname and sr == c.site),
                                               subreddit = not c.cname), p)
        else:
            res = "/%s" % p
        return res

    def make_permalink_slow(self):
        return self.make_permalink(self.subreddit_slow)
    
    @property
    def canonical_url(self):
        from r2.lib.template_helpers import get_domain
        p = "lw/%s/%s/" % (self._id36, title_to_url(self.title))
        return "http://%s/%s" % (get_domain(subreddit = False), p)

    @classmethod
    def add_props(cls, user, wrapped):
        from r2.lib.count import incr_counts
        from r2.lib.media import thumbnail_url
        from r2.lib.utils import timeago

        saved = Link._saved(user, wrapped) if user else {}
        hidden = Link._hidden(user, wrapped) if user else {}

        for item in wrapped:
            show_media = False
            if c.user.pref_compress:
                pass
            elif c.user.pref_media == 'on':
                show_media = True
            elif c.user.pref_media == 'subreddit' and item.subreddit.show_media:
                show_media = True
            elif (item.promoted
                  and item.has_thumbnail
                  and c.user.pref_media != 'off'):
                show_media = True

            if not show_media:
                item.thumbnail = ""
            elif item.has_thumbnail:
                item.thumbnail = thumbnail_url(item)
            else:
                item.thumbnail = g.default_thumb
            
            item.domain = (domain(item.url) if not item.is_self
                          else 'self.' + item.subreddit.name)
            if not hasattr(item,'top_link'):
                item.top_link = False
            item.urlprefix = ''
            item.saved = bool(saved.get((user, item, 'save')))
            item.hidden = bool(hidden.get((user, item, 'hide')))

            # Only check "last clicked time" on demand.  Otherwise it is expensive in big listings.  TODO - refactor to use "_getLastClickedTime"
            def clicked():
                c = Link._clicked(user, wrapped) if user else {}
                return c.get((user, item, 'click'))
            item.clicked = clicked

            item.num = None
            item.score_fmt = Score.signed_number
            item.permalink = item.make_permalink(item.subreddit)
            if item.is_self:
                item.url = item.make_permalink(item.subreddit, force_domain = True)

            if c.user_is_admin:
                item.hide_score = False
            elif item.promoted:
                item.hide_score = True
            elif c.user == item.author:
                item.hide_score = False
            elif item._date > timeago("2 hours"):
                item.hide_score = True
            else:
                item.hide_score = False

            # Don't allow users to vote on their own posts and don't
            # allow users to vote on collapsed posts shown when
            # viewing comment permalinks.
            item.votable = bool(c.user != item.author and
                                not getattr(item, 'for_comment_permalink', False))

            if c.user_is_loggedin and item.author._id == c.user._id:
                item.nofollow = False
            elif item.score <= 1 or item._spam or item.author._spam:
                item.nofollow = True
            else:
                item.nofollow = False

            if c.user_is_loggedin and item.subreddit.name == c.user.draft_sr_name:
              item.draft = True
            else:
              item.draft = False

        if c.user_is_loggedin:
            incr_counts(wrapped)

    @property
    def subreddit_slow(self):
        from subreddit import Subreddit
        """return's a link's subreddit. in most case the subreddit is already
        on the wrapped link (as .subreddit), and that should be used
        when possible. """
        return Subreddit._byID(self.sr_id, True, return_dict = False)

    def change_subreddit(self, new_sr_id):
        """Change the subreddit of the link and update its date"""
        if self.sr_id != new_sr_id:
            self.sr_id = new_sr_id
            self._date = datetime.now(g.tz)
            self.url = self.make_permalink_slow()
            self._commit()

            # Comments must be in the same subreddit as the link that
            # the comments belong to.  This is needed so that if a
            # comment is made on a draft link then when the link moves
            # to a public subreddit the comments also move and others
            # will be able to see and reply to the comment.
            for comment in Comment._query(Comment.c.link_id == self._id, data=True):
                comment.sr_id = new_sr_id
                comment._commit()

    def set_blessed(self, is_blessed):
        if self.blessed != is_blessed:
          self.blessed = is_blessed
          self._date = datetime.now(g.tz)
          self._commit()

    def add_tag(self, tag_name, name = 'tag'):
        """Adds a tag of the given name to the link. If the tag does not
           exist it is created"""
        if self._only_whitespace.match(tag_name):
            # Don't allow an empty tag
            return

        try:
            tag = Tag._by_name(tag_name)
        except NotFound:
            tag = Tag._new(tag_name)
            tag._commit()

        # See if link already has this tag
        tags = LinkTag._fast_query(tup(self), tup(tag), name=name)
        link_tag = tags[(self, tag, name)]
        if not link_tag:
            link_tag = LinkTag(self, tag, name=name)
            link_tag._commit()

        return link_tag

    def remove_tag(self, tag_name, name='tag'):
        """Removes a tag from the link. The tag is not deleted,
           just the relationship between the link and the tag"""
        try:
            tag = Tag._by_name(tag_name)
        except NotFound:
            return False

        tags = LinkTag._fast_query(tup(self), tup(tag), name=name)
        link_tag = tags[(self, tag, name)]
        if link_tag:
            link_tag._delete()
            return link_tag

    def get_tags(self):
        q = LinkTag._query(LinkTag.c._thing1_id == self._id,
                           LinkTag.c._name == 'tag',
                           LinkTag.c._t2_deleted == False,
                           eager_load = True,
                           thing_data = not g.use_query_cache
                      )
        return [link_tag._thing2 for link_tag in q]

    def set_tags(self, tags):
        """Adds and/or removes tags to match the list given"""
        current_tags = set(self.tag_names())
        updated_tags = set(tags)
        removed_tags = current_tags.difference(updated_tags)
        new_tags = updated_tags.difference(current_tags)
        
        for tag in new_tags:
            self.add_tag(tag)
        
        for tag in removed_tags:
            self.remove_tag(tag)
        
    def tag_names(self):
        """Returns just the names of the tags of this article"""
        return [tag.name for tag in self.get_tags()]

    def get_sequence_names(self):
      """Returns the names of the sequences"""
      return Wiki().sequences_for_article_url(self.url).keys()

    def _next_link_for_tag(self, tag, sort):
      """Returns a query navigation by tag using the supplied sort"""
      from r2.lib.db import tdb_sql as tdb
      import sqlalchemy as sa

      # List of the subreddit ids this user has access to
      sr = Subreddit.default()

      # Get a reference to reddit_rel_linktag
      linktag_type = tdb.rel_types_id[LinkTag._type_id]
      linktag_thing_table = linktag_type.rel_table[0]

      # Get a reference to the reddit_thing_link & reddit_data_link tables
      link_type = tdb.types_id[Link._type_id]
      link_data_table = link_type.data_table[0]
      link_thing_table = link_type.thing_table

      # Subreddit subquery aliased as link_sr
      link_sr = sa.select([
          link_data_table.c.thing_id,
          sa.cast(link_data_table.c.value, sa.INT).label('sr_id')],
          link_data_table.c.key == 'sr_id').alias('link_sr')

      # Determine the date clause based on the sort order requested
      if isinstance(sort, operators.desc):
        date_clause = link_thing_table.c.date < self._date
        sort = sa.desc(link_thing_table.c.date)
      else:
        date_clause = link_thing_table.c.date > self._date
        sort = sa.asc(link_thing_table.c.date)

      query = sa.select([linktag_thing_table.c.thing1_id],
                        sa.and_(linktag_thing_table.c.thing2_id == tag._id,
                                linktag_thing_table.c.thing1_id == link_sr.c.thing_id,
                                linktag_thing_table.c.thing1_id == link_thing_table.c.thing_id,
                                linktag_thing_table.c.name == 'tag',
                                link_thing_table.c.spam == False,
                                link_thing_table.c.deleted == False,
                                date_clause,
                                link_sr.c.sr_id == sr._id),
                        order_by = sort,
                        limit = 1)

      row = query.execute().fetchone()
      return Link._byID(row.thing1_id, data=True) if row else None

    def _link_for_query(self, query):
      """Returns a single Link result for the given query"""
      results = list(query)
      return results[0] if results else None

    # TODO: These navigation methods might be better in their own module
    def next_by_tag(self, tag):
      return self._next_link_for_tag(tag, operators.asc('_t1_date'))
      # TagNamesByTag.append(tag.name)
      # IndexesByTag.append(nextIndexByTag);
      # nextIndexByTag = nextIndexByTag + 1

    def prev_by_tag(self, tag):
      return self._next_link_for_tag(tag, operators.desc('_t1_date'))

    def next_in_sequence(self, sequence_name):
      sequence = Wiki().sequences_for_article_url(self.url).get(sequence_name)
      return sequence['next'] if sequence else None

    def prev_in_sequence(self, sequence_name):
      sequence = Wiki().sequences_for_article_url(self.url).get(sequence_name)
      return sequence['prev'] if sequence else None

    def _nav_query_date_clause(self, sort):
      if isinstance(sort, operators.desc):
        date_clause = Link.c._date < self._date
      else:
        date_clause = Link.c._date > self._date
      return date_clause

    def _link_nav_query(self, clause = None, sort = None):
      sr = Subreddit.default()

      q = Link._query(self._nav_query_date_clause(sort), Link.c._deleted == False, Link.c._spam == False, Link.c.sr_id == sr._id, limit = 1, sort = sort, data = True)
      if clause is not None:
        q._filter(clause)
      return q

    def next_by_author(self):
      q = self._link_nav_query(Link.c.author_id == self.author_id, operators.asc('_date'))
      return self._link_for_query(q)

    def prev_by_author(self):
      q = self._link_nav_query(Link.c.author_id == self.author_id, operators.desc('_date'))
      return self._link_for_query(q)

    def next_in_top(self):
      q = self._link_nav_query(Link.c.top_link == True, operators.asc('_date'))
      return self._link_for_query(q)

    def prev_in_top(self):
      q = self._link_nav_query(Link.c.top_link == True, operators.desc('_date'))
      return self._link_for_query(q)

    def next_in_promoted(self):
      q = self._link_nav_query(Link.c.blessed == True, operators.asc('_date'))
      return self._link_for_query(q)

    def prev_in_promoted(self):
      q = self._link_nav_query(Link.c.blessed == True, operators.desc('_date'))
      return self._link_for_query(q)

    def next_link(self):
      q = self._link_nav_query(sort = operators.asc('_date'))
      return self._link_for_query(q)

    def prev_link(self):
      q = self._link_nav_query(sort = operators.desc('_date'))
      return self._link_for_query(q)

    def _commit(self, *a, **kw):
        """Detect when we need to invalidate the sidebar recent posts.

        Whenever a post is created we need to invalidate.  Also invalidate when
        various post attributes are changed (such as moving to a different
        subreddit). If the post cache is invalidated the comment one is too.
        This is primarily for when a post is banned so that its comments
        dissapear from the sidebar too.
        """

        should_invalidate = (not self._created or
                             frozenset(('title', 'sr_id', '_deleted', '_spam')) & frozenset(self._dirties.keys()))

        Thing._commit(self, *a, **kw)

        if should_invalidate:
            g.rendercache.delete('side-posts' + '-' + c.site.name)
            g.rendercache.delete('side-comments' + '-' + c.site.name)
            tags = self.tag_names()
            if 'open_thread' in tags:
                g.rendercache.delete('side-open' + '-' + c.site.name)
            if 'quotes' in tags:
                g.rendercache.delete('side-quote' + '-' + c.site.name)
            if 'group_rationality_diary' in tags:
                g.rendercache.delete('side-diary' + '-' + c.site.name)

# Note that there are no instances of PromotedLink or LinkCompressed,
# so overriding their methods here will not change their behaviour
# (except for add_props). These classes are used to override the
# render_class on a Wrapped to change the template used for rendering

class PromotedLink(Link):
    _nodb = True

    @classmethod
    def add_props(cls, user, wrapped):
        Link.add_props(user, wrapped)

        try:
            if c.user_is_sponsor:
                promoted_by_ids = set(x.promoted_by
                                      for x in wrapped
                                      if hasattr(x,'promoted_by'))
                promoted_by_accounts = Account._byID(promoted_by_ids,
                                                     data=True)
            else:
                promoted_by_accounts = {}

        except NotFound:
            # since this is just cosmetic, we can skip it altogether
            # if one isn't found or is broken
            promoted_by_accounts = {}

        for item in wrapped:
            # these are potentially paid for placement
            item.nofollow = True
            if item.promoted_by in promoted_by_accounts:
                item.promoted_by_name = promoted_by_accounts[item.promoted_by].name
            else:
                # keep the template from trying to read it
                item.promoted_by = None

class LinkCompressed(Link):
    _nodb = True

    @classmethod
    def add_props(cls, user, wrapped):
        Link.add_props(user, wrapped)

        for item in wrapped:
            item.score_fmt = Score.points


class InlineArticle(Link):
    """Exists to gain a different render_class in Wrapped"""
    _nodb = True

class CommentPermalink(Link):
    """Exists to gain a different render_class in Wrapped"""
    _nodb = True

class TagExists(Exception): pass

class Tag(Thing):
    """A tag on a link/article"""
    @classmethod
    def _new(self, name, **kw):
        tag_name = name.lower()
        try:
            tag = Tag._by_name(tag_name)
            raise TagExists
        except NotFound:
            tag = Tag(name = tag_name, **kw)
            tag._commit()
            clear_memo('tag._by_name', Tag, name.lower())
            return tag

    @classmethod
    @memoize('tag._by_name')
    def _by_name_cache(cls, name):
        q = cls._query(lower(cls.c.name) == name.lower(), limit = 1)
        l = list(q)
        if l:
            return l[0]._id

    @classmethod
    def _by_name(cls, name):
        #lower name here so there is only one cache
        name = name.lower()

        tag_id = cls._by_name_cache(name)
        if tag_id:
            return cls._byID(tag_id, True)
        else:
            raise NotFound, 'Tag %s' % name

    @property
    def path(self):
        """Returns the path to the tag listing for this tag"""
        quoted_tag_name = urllib.quote(self.name.encode('utf8'))
        if not c.default_sr:
            return "/r/%s/tag/%s/" % (c.site.name, quoted_tag_name)
        else:
            return "/tag/%s/" % (quoted_tag_name)

    @classmethod
    # @memoize('tag.tag_cloud_for_subreddits') enable when it is cleared at appropiate points
    def tag_cloud_for_subreddits(cls, sr_ids):
        from r2.lib.db import tdb_sql as tdb
        import sqlalchemy as sa

        type = tdb.rel_types_id[LinkTag._type_id]
        linktag_thing_table = type.rel_table[0]

        link_type = tdb.types_id[Link._type_id]
        link_data_table = link_type.data_table[0]
        link_thing_table = link_type.thing_table

        link_sr = sa.select([
            link_data_table.c.thing_id,
            sa.cast(link_data_table.c.value, sa.INT).label('sr_id')],
            link_data_table.c.key == 'sr_id').alias('link_sr')

        query = sa.select([linktag_thing_table.c.thing2_id,
                          sa.func.count(linktag_thing_table.c.thing1_id)],
                          sa.and_(linktag_thing_table.c.thing1_id == link_sr.c.thing_id,
                                  linktag_thing_table.c.thing1_id == link_thing_table.c.thing_id,
                                  link_thing_table.c.spam == False,
                                  link_sr.c.sr_id.in_(*sr_ids)),
                          group_by = [linktag_thing_table.c.thing2_id],
                          having = sa.func.count(linktag_thing_table.c.thing1_id) > 1,
                          order_by = sa.desc(sa.func.count(linktag_thing_table.c.thing1_id)),
                          limit = 100)

        rows = query.execute().fetchall()
        tags = []
        for result in rows:
            tag = Tag._byID(result.thing2_id, data=True)
            tags.append((tag, result.count))

        # Order by tag name
        tags.sort(key=lambda x: _force_unicode(x[0].name))
        return cls.make_cloud(10, tags)

    @classmethod
    def make_cloud(cls, steps, input):
        # From: http://www.car-chase.net/2007/jan/16/log-based-tag-clouds-python/
        import math

        if len(input) <= 0:
          return []
        else:
            temp, newThresholds, results = [], [], []
            for item in input:
                temp.append(item[1])
            maxWeight = float(max(temp))
            minWeight = float(min(temp))
            newDelta = (maxWeight - minWeight)/float(steps)
            for i in range(steps + 1):
               newThresholds.append((100 * math.log((minWeight + i * newDelta) + 2), i))
            for tag in input:
                fontSet = False
                for threshold in newThresholds[1:int(steps)+1]:
                    if (100 * math.log(tag[1] + 2)) <= threshold[0] and not fontSet:
                        results.append(tuple([tag[0], threshold[1]]))
                        fontSet = True
            return results


class LinkTag(Relation(Link, Tag)):
    pass

class Comment(Thing, Printable):
    _data_int_props = Thing._data_int_props + ('reported',)
    _defaults = dict(reported = 0, 
                     moderator_banned = False,
                     banned_before_moderator = False,
                     is_html = False,
                     retracted = False,
                     show_response_to = False,
                     _descendant_karma = 0)

    def _markdown(self):
        pass

    def _delete(self):
        link = Link._byID(self.link_id, data = True)
        link._incr('num_comments', -1)
    
    @classmethod
    def _new(cls, author, link, parent, body, ip, spam = False, date = None):
        comment = Comment(body = body,
                          link_id = link._id,
                          sr_id = link.sr_id,
                          author_id = author._id,
                          ip = ip,
                          date = date)
        
        comment._spam = spam

        #these props aren't relations
        if parent:
            comment.parent_id = parent._id

        comment._commit()

        link._incr('num_comments', 1)

        inbox_rel = comment._send_post_notifications(link, comment, parent)

        #clear that chache
        clear_memo('builder.link_comments2', link._id)

        # flag search indexer that something has changed
        tc.changed(comment)

        #update last modified
        set_last_modified(author, 'overview')
        set_last_modified(author, 'commented')
        set_last_modified(link, 'comments')

        #update the comment cache
        from r2.lib.comment_tree import add_comment
        add_comment(comment)

        return (comment, inbox_rel)

    def try_parent(self, func, default):
        """
        If this comment has a parent, return `func(parent)`; otherwise
        return `default`.
        """
        if getattr(self, 'parent_id', None) is not None:
            parent = type(self)._byID(self.parent_id)
            return func(parent)
        return default

    @classmethod
    def _somethinged(cls, rel, user, link, name):
        return rel._fast_query(tup(user), tup(link), name = name)

    def _something(self, rel, user, somethinged, name):
        try:
            saved = rel(user, self, name=name)
            saved._commit()
            return saved
        except CreationError, e:
            return somethinged(user, self)[(user, self, name)]

    def _unsomething(self, user, somethinged, name):
        saved = somethinged(user, self)[(user, self, name)]
        if saved:
            saved._delete()
            return saved

    def add_subscriber(self, user):
        return self._something(CommentSubscription, user, self.user_subscribed, 'commentsubscription')

    def remove_subscriber(self, user):
        return self._unsomething(user, self.user_subscribed, 'commentsubscription')

    @classmethod
    def user_subscribed(cls, user, link):
        return cls._somethinged(CommentSubscription, user, link, 'commentsubscription')

    @classmethod
    def comment_subscribed(cls, user, link):
        return cls._somethinged(CommentSubscription, user, link, 'commentsubscription')[(user,link,'commentsubscription')]

    def _send_post_notifications(self, link, comment, parent):
        dashto = []
        for subscriber in Subscription._query(Subscription.c._thing2_id == (link._id),
                                              Subscription.c._name == 'subscription'):
            if not subscriber._thing1_id == comment.author_id:
                dashto.append(Account._byID(subscriber._thing1_id))
        if link.notify_on_comment and not link.author_id == comment.author_id:
                dashto.append(Account._byID(link.author_id))

        for user in dashto:
            s = SubscriptionStorage(user, comment, name='subscriptionstorage')
            s._commit()

        to = []
        if parent:
            if not parent.author_id == comment.author_id:
                to.append(Account._byID(parent.author_id))
            for subscriber in CommentSubscription._query(CommentSubscription.c._thing2_id == (parent._id),
                                                  CommentSubscription.c._name == 'commentsubscription'):
                if not subscriber._thing1_id == comment.author_id:
                    to.append(Account._byID(subscriber._thing1_id))
        else:
            for subscriber in Subscription._query(Subscription.c._thing2_id == (link._id),
                                                  Subscription.c._name == 'subscription'):
                if not subscriber._thing1_id == comment.author_id:
                    to.append(Account._byID(subscriber._thing1_id))
            if link.notify_on_comment and not link.author_id == comment.author_id:
                to.append(Account._byID(link.author_id))
        if len(to) == 0:
            return None
        # only global admins can be message spammed.
        if self._spam and to.name not in g.admins:
            return None

        for user in to:
            Inbox._add(user, self, 'inbox')

        return True

    def has_children(self):
        q = Comment._query(Comment.c.parent_id == self._id, limit=1)
        child = list(q)
        return len(child)>0

    def can_delete(self):
        if not self._loaded:
            self._load()
        return (c.user_is_loggedin and self.author_id == c.user._id and \
                self.retracted and not self.has_children())


    # Changes the body of this comment, parsing the new body for polls and
    # creating them if found, and commits.
    def set_body(self, body):
        self.has_polls = containspolls(body)
        self.body = parsepolls(body, self)
        self._commit()

    @property
    def subreddit_slow(self):
        from subreddit import Subreddit
        """return's a comments's subreddit. in most case the subreddit is already
        on the wrapped link (as .subreddit), and that should be used
        when possible. if sr_id does not exist, then use the parent link's"""
        self._safe_load()

        if hasattr(self, 'sr_id'):
            sr_id = self.sr_id
        else:
            l = Link._byID(self.link_id, True)
            sr_id = l.sr_id
        return Subreddit._byID(sr_id, True, return_dict = False)

    @property
    def collapse_in_link_threads(self):
        if c.user_is_admin:
            return False
        return self._score <= g.hide_comment_threshold

    @property
    def reply_costs_karma(self):
        if self._score <= g.downvoted_reply_score_threshold:
            return True
        return self.try_parent(lambda p: p.reply_costs_karma, False)

    def incr_descendant_karma(self, comments, amount):

        old_val = getattr(self, '_descendant_karma')

        comments.append(self._id)

        if hasattr(self, 'parent_id') and self.parent_id:
            Comment._byID(self.parent_id).incr_descendant_karma(comments, amount)
        else:
            from r2.lib.db import tdb_sql as tdb
            tdb.incr_things_prop(self._type_id, comments, 'descendant_karma', amount)

        self.__setattr__('_descendant_karma', old_val + amount, False)

        prefix = self.__class__.__name__ + '_'
        cache.set(prefix + str(self._id), self)

    def keep_item(self, wrapped):
        if c.user_is_admin:
            return True
        if self.collapse_in_link_threads:
            return False
        return self.try_parent(lambda p: p.keep_item(p), True)

    @staticmethod
    def cache_key(wrapped):
        if c.user_is_admin:
            return False
        if hasattr(wrapped, 'has_polls') and wrapped.has_polls:
            return False

        s = (str(i) for i in (c.profilepage,
                              c.full_comment_listing,
                              wrapped._fullname,
                              bool(c.user_is_loggedin),
                              c.focal_comment == wrapped._id36,
                              request.host,
                              c.cname, 
                              wrapped.author == c.user,
                              wrapped.likes,
                              wrapped.friend,
                              wrapped.collapsed,
                              wrapped.moderator_banned,
                              wrapped.show_spam,
                              wrapped.show_reports,
                              wrapped.can_ban,
                              wrapped.moderator_banned,
                              wrapped.can_reply,
                              wrapped.deleted,
                              wrapped.is_html,
                              wrapped.votable,
                              wrapped.retracted,
                              wrapped.can_be_deleted,
                              wrapped.show_response_to))
        s = ''.join(s)
        return s

    def make_permalink(self, link, sr=None):
        return link.make_permalink(sr) + self._id36

    def make_anchored_permalink(self, link=None, sr=None, context=1, anchor=None):
        if link:
            permalink = UrlParser(self.make_permalink(link, sr))
        else:
            permalink = UrlParser(self.make_permalink_slow())
        permalink.update_query(context=context)
        permalink.fragment = anchor if anchor else self._id36
        return permalink.unparse()

    def make_permalink_slow(self):
        l = Link._byID(self.link_id, data=True)
        return self.make_permalink(l, l.subreddit_slow)

    def make_permalink_title(self, link):
        author = Account._byID(self.author_id, data=True).name
        params = {'author' : _force_unicode(author), 'title' : _force_unicode(link.title), 'site' : c.site.title}
        return strings.permalink_title % params
          
    @classmethod
    def add_props(cls, user, wrapped):
        #fetch parent links
        links = Link._byID(set(l.link_id for l in wrapped), True)
        

        #get srs for comments that don't have them (old comments)
        for cm in wrapped:
            if not hasattr(cm, 'sr_id'):
                cm.sr_id = links[cm.link_id].sr_id
        
        subreddits = Subreddit._byID(set(cm.sr_id for cm in wrapped),
                                     data=True,return_dict=False)
        can_reply_srs = set(s._id for s in subreddits if s.can_comment(user))

        min_score = c.user.pref_min_comment_score

        cids = dict((w._id, w) for w in wrapped)

        for item in wrapped:
            item.link = links.get(item.link_id)
            if not hasattr(item, 'subreddit'):
                item.subreddit = item.subreddit_slow
            if hasattr(item, 'parent_id'):
                parent = Comment._byID(item.parent_id, data=True)
                parent_author = Account._byID(parent.author_id, data=True)
                item.parent_author = parent_author

                if not c.full_comment_listing and cids.has_key(item.parent_id):
                    item.parent_permalink = '#' + utils.to36(item.parent_id)
                else:
                    item.parent_permalink = parent.make_anchored_permalink(item.link, item.subreddit)
            else:
                item.parent_permalink = None
                item.parent_author = None

            item.can_reply = (item.sr_id in can_reply_srs)

            # Don't allow users to vote on their own comments
            item.votable = bool(c.user != item.author and not item.retracted)
            if item.votable and c.profilepage:
                # Can only vote on profile page under certain conditions
                item.votable = bool((c.user.safe_karma > g.karma_to_vote_in_overview) and (g.karma_percentage_to_be_voted > item.author.percent_up()))

            # not deleted on profile pages,
            # deleted if spam and not author or admin
            item.deleted = (not c.profilepage and
                           (item._deleted or
                            (item._spam and
                             item.author != c.user and
                             not item.show_spam)))

            # don't collapse for admins, on profile pages, or if deleted
            item.collapsed = ((item.score < min_score) and
                             not (c.profilepage or
                                  item.deleted or
                                  c.user_is_admin))
                
            if not hasattr(item,'editted'):
                item.editted = False
            #will get updated in builder
            item.num_children = 0
            item.score_fmt = Score.points
            item.permalink = item.make_permalink(item.link, item.subreddit)
            item.can_be_deleted = item.can_delete()

    def _commit(self, *a, **kw):
        """Detect when we need to invalidate the sidebar recent comments.

        Whenever a comment is created we need to invalidate.  Also
        invalidate when various comment attributes are changed.
        """

        should_invalidate = (not self._created or
                             frozenset(('body', '_deleted', '_spam')) & frozenset(self._dirties.keys()))

        Thing._commit(self, *a, **kw)

        if should_invalidate:
            g.rendercache.delete('side-comments' + '-' + c.site.name)
            tags = Link._byID(self.link_id, data = True).tag_names()
            if 'open_thread' in tags:
                g.rendercache.delete('side-open' + '-' + c.site.name)
            if 'quotes' in tags:
                g.rendercache.delete('side-quote' + '-' + c.site.name)
            if 'group_rationality_diary' in tags:
                g.rendercache.delete('side-diary' + '-' + c.site.name)


class InlineComment(Comment):
    """Exists to gain a different render_class in Wrapped"""
    _nodb = True

class MoreComments(object):
    show_spam = False
    show_reports = False
    is_special = False
    can_ban = False
    deleted = False
    rowstyle = 'even'
    reported = False
    collapsed = False
    author = None
    margin = 0

    @staticmethod
    def cache_key(item):
        return False
    
    def __init__(self, link, depth, parent=None):
        if parent:
            self.parent_id = parent._id
            self.parent_name = parent._fullname
            self.parent_permalink = parent.make_permalink(link, 
                                                          link.subreddit_slow)
        self.link_name = link._fullname
        self.link_id = link._id
        self.depth = depth
        self.children = []
        self.count = 0

    @property
    def _fullname(self):
        return self.children[0]._fullname if self.children else 't0_blah'

    @property
    def _id36(self):
        return self.children[0]._id36 if self.children else 't0_blah'


class MoreRecursion(MoreComments):
    pass

class MoreChildren(MoreComments):
    pass
    
class Message(Thing, Printable):
    _defaults = dict(reported = 0,)
    _data_int_props = Thing._data_int_props + ('reported', )

    @classmethod
    def _new(cls, author, to, subject, body, ip, spam = False):
        m = Message(subject = subject,
                    body = body,
                    author_id = author._id,
                    ip = ip)
        m._spam = spam
        m.to_id = to._id
        m._commit()

        #author = Author(author, m, 'author')
        #author._commit()

        # only global admins can be message spammed.
        inbox_rel = None
        if (not author.messagebanned) and ((not m._spam) or to.name in g.admins):
            inbox_rel = Inbox._add(to, m, 'inbox')

        return (m, inbox_rel)

    @classmethod
    def add_props(cls, user, wrapped):
        #TODO global-ish functions that shouldn't be here?
        #reset msgtime after this request
        msgtime = c.have_messages
        
        #load the "to" field if required
        to_ids = set(w.to_id for w in wrapped)
        tos = Account._byID(to_ids, True) if to_ids else {}

        for item in wrapped:
            item.to = tos[item.to_id]
            if msgtime and item._date >= msgtime:
                item.new = True
            else:
                item.new = False
            item.score_fmt = Score.none

 
    @staticmethod
    def cache_key(wrapped):
        #warning: inbox/sent messages
        #comments as messages
        return False

    def keep_item(self, wrapped):
        return True

class SaveHide(Relation(Account, Link)): pass
class Click(Relation(Account, Link)): pass
class Subscription(Relation(Account, Link)): pass
class CommentSubscription(Relation(Account, Comment)): pass
class SubscriptionStorage(Relation(Account, Comment)):pass


class Inbox(MultiRelation('inbox',
                          Relation(Account, Comment),
                          Relation(Account, Message))):
    @classmethod
    def _add(cls, to, obj, *a, **kw):
        i = Inbox(to, obj, *a, **kw)
        i._commit()

        if not to._loaded:
            to._load()
            
        #if there is not msgtime, or it's false, set it
        if not hasattr(to, 'msgtime') or not to.msgtime:
            to.msgtime = obj._date
            to._commit()
            
        return i

########NEW FILE########
__FILENAME__ = listing
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from account import *
from link import *
from vote import *
from report import *
from pylons import i18n, request, g

from r2.lib.wrapped import Wrapped
from r2.lib import utils
from r2.lib.db import operators
from r2.lib.cache import sgm

from copy import deepcopy, copy

class Listing(object):
    # class used in Javascript to manage these objects
    _js_cls = "Listing"

    def __init__(self, builder, nextprev = True, next_link = True,
                 prev_link = True, vote_hash_type = 'valid', **kw):
        self.builder = builder
        self.nextprev = nextprev
        self.next_link = True
        self.prev_link = True
        self.next = None
        self.prev = None
        self.max_num = 1
        self.vote_hash_type = vote_hash_type

    @property
    def max_score(self):
        scores = [x.score for x in self.things if hasattr(x, 'score')]
        return max(scores) if scores else 0

    def get_items(self, *a, **kw):
        """Wrapper around builder's get_items that caches the rendering."""
        builder_items = self.builder.get_items(*a, **kw)

        #render cache
        #fn to render non-boring items
        fullnames = {}
        for i in self.builder.item_iter(builder_items):
            rs = c.render_style
            key = i.render_class.cache_key(i)
            if key:
                fullnames[key + rs + c.lang] = i

        def render_items(names):
            r = {}
            for i in names:
                item = fullnames[i]
                r[i] = item.render()
            return r

        rendered_items = sgm(g.rendercache, fullnames, render_items, 'render_',
                             time = g.page_cache_time)

        #replace the render function
        for k, v in rendered_items.iteritems():
            def make_fn(v):
                default = c.render_style
                default_render = fullnames[k].render
                def r(style = default):
                    if style != c.render_style:
                        return default_render(style = style)
                    return v
                return r
            fullnames[k].render = make_fn(v)
        
        return builder_items

    def listing(self):
        self.things, prev, next, bcount, acount = self.get_items()

        self.max_num = max(acount, bcount)

        if self.nextprev and self.prev_link and prev and bcount > 1:
            p = request.get.copy()
            p.update({'after':None, 'before':prev._fullname, 'count':bcount})
            self.prev = (request.path + utils.query_string(p))
        if self.nextprev and self.next_link and next:
            p = request.get.copy()
            p.update({'after':next._fullname, 'before':None, 'count':acount})
            self.next = (request.path + utils.query_string(p))
        #TODO: need name for template -- must be better way
        return Wrapped(self)

class LinkListing(Listing):
    def __init__(self, *a, **kw):
        Listing.__init__(self, *a, **kw)

        self.show_nums = kw.get('show_nums', False)

class NestedListing(Listing):
    def __init__(self, *a, **kw):
        Listing.__init__(self, *a, **kw)

        self.nested = kw.get('nested', True)
        self.num = kw.get('num', g.num_comments)
        self.parent_name = kw.get('parent_name')
        
    def listing(self):
        ##TODO use the local builder with the render cache. this may
        ##require separating the builder's get_items and tree-building
        ##functionality
        wrapped_items = self.get_items(num = self.num, nested = True)

        self.things = wrapped_items

        #make into a tree thing
        return Wrapped(self)

class OrganicListing(Listing):
    # class used in Javascript to manage these objects
    _js_cls = "OrganicListing"

    def __init__(self, *a, **kw):
        kw['vote_hash_type'] = kw.get('vote_hash_type', 'organic')
        Listing.__init__(self, *a, **kw)
        self.nextprev   = False
        self.show_nums  = True
        self._max_num   = kw.get('max_num', 0)
        self._max_score = kw.get('max_score', 0)
        self.org_links  = kw.get('org_links', [])
        self.visible_link = kw.get('visible_link', '')

    @property
    def max_score(self):
        return self._max_score
    
    def listing(self):
        res = Listing.listing(self)
        # override score fields
        res.max_num = self._max_num
        res.max_score = self._max_score
        for t in res.things:
            t.num = ""
        return res

########NEW FILE########
__FILENAME__ = mail_queue
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
##############################################################################
from r2.config.databases import email_engine
from r2.lib.db.tdb_sql import make_metadata, settings
from sqlalchemy.databases.postgres import PGInet, PGBigInteger
from r2.models.thing_changes import changed, index_str, create_table
import sqlalchemy as sa
import datetime
from r2.lib.utils import Storage, timeago
from account import Account
from r2.lib.db.thing import Thing
from email.MIMEText import MIMEText
import hashlib
from r2.lib.memoize import memoize, clear_memo


def mail_queue(metadata):
    return sa.Table(settings.DB_APP_NAME + '_mail_queue', metadata,
                    sa.Column("uid", sa.Integer,
                              sa.Sequence('queue_id_seq'), primary_key=True),

                    # unique hash of the message to carry around
                    sa.Column("msg_hash", sa.String),
                    
                    # the id of the account who started it
                    sa.Column('account_id', PGBigInteger),

                    # the name (not email) for the from
                    sa.Column('from_name', sa.String),

                    # the "To" address of the email
                    sa.Column('to_addr', sa.String),

                    # the "From" address of the email
                    sa.Column('fr_addr', sa.String),
                    
                    # the "Reply-To" address of the email
                    sa.Column('reply_to', sa.String),

                    # fullname of the thing
                    sa.Column('fullname', sa.String),
                    
                    # when added to the queue
                    sa.Column('date',
                              sa.DateTime(timezone = True),
                              nullable = False),

                    # IP of original request
                    sa.Column('ip', PGInet),

                    # enum of kind of event
                    sa.Column('kind', sa.Integer),
                    
                    # any message that may have been included
                    sa.Column('body', sa.String),
                    
                    )

def sent_mail_table(metadata, name = 'sent_mail'):
    return sa.Table(settings.DB_APP_NAME + '_' + name, metadata,
                    # tracking hash of the email
                    sa.Column('msg_hash', sa.String, primary_key=True),
                    
                    # the account who started it
                    sa.Column('account_id', PGBigInteger),
                    
                    # the "To" address of the email
                    sa.Column('to_addr', sa.String),

                    # the "From" address of the email
                    sa.Column('fr_addr', sa.String),
                    
                    # the "reply-to" address of the email
                    sa.Column('reply_to', sa.String),

                    # IP of original request
                    sa.Column('ip', PGInet),

                    # fullname of the reference thing
                    sa.Column('fullname', sa.String),

                    # send date
                    sa.Column('date',
                              sa.DateTime(timezone = True),
                              default = sa.func.now(),
                              nullable = False),

                    # enum of kind of event
                    sa.Column('kind', sa.Integer),

                    )
                    

def opt_out(metadata):
    return sa.Table(settings.DB_APP_NAME + '_opt_out', metadata,
                    sa.Column('email', sa.String, primary_key = True),
                    # when added to the list
                    sa.Column('date',
                              sa.DateTime(timezone = True),
                              default = sa.func.now(),
                              nullable = False),
                    # why did they do it!?
                    sa.Column('msg_hash', sa.String),
                    )

class EmailHandler(object):
    def __init__(self, force = False):
        self.metadata = make_metadata(email_engine)
        self.queue_table = mail_queue(self.metadata)
        indices = [index_str(self.queue_table, "date", "date"),
                   index_str(self.queue_table, 'kind', 'kind')]
        create_table(self.queue_table, indices, force = force)

        self.opt_table = opt_out(self.metadata)
        indices = [index_str(self.opt_table, 'email', 'email')]
        create_table(self.opt_table, indices, force = force)

        self.track_table = sent_mail_table(self.metadata)
        self.reject_table = sent_mail_table(self.metadata, name = "reject_mail")
        
        def sent_indices(tab):
            indices = [index_str(tab, 'to_addr', 'to_addr'),
                       index_str(tab, 'date', 'date'),
                       index_str(tab, 'ip', 'ip'),
                       index_str(tab, 'kind', 'kind'),
                       index_str(tab, 'fullname', 'fullname'),
                       index_str(tab, 'account_id', 'account_id'),
                       index_str(tab, 'msg_hash', 'msg_hash'),
                       ]
        
        create_table(self.track_table, sent_indices(self.track_table), force = force)
        create_table(self.reject_table, sent_indices(self.reject_table), force = force)

    def __repr__(self):
        return "<email-handler>"

    def has_opted_out(self, email):
        o = self.opt_table
        s = sa.select([o.c.email], o.c.email == email, limit = 1)
        res = s.execute()
        return bool(res.fetchall())


    def opt_out(self, msg_hash):
        """Adds the recipient of the email to the opt-out list and returns
        that address."""
        email = self.get_recipient(msg_hash)
        if email:
            o = self.opt_table
            try:
                o.insert().execute({o.c.email: email, o.c.msg_hash: msg_hash})
                clear_memo('r2.models.mail_queue.has_opted_out', 
                           email)
                clear_memo('r2.models.mail_queue.opt_count')
                return (email, True)
            except sa.exceptions.SQLError:
                return (email, False)
        return (None, False)

    def opt_in(self, msg_hash):
        """Removes recipient of the email from the opt-out list"""
        email = self.get_recipient(msg_hash)
        if email:
            o = self.opt_table
            if self.has_opted_out(email):
                sa.delete(o, o.c.email == email).execute()
                clear_memo('r2.models.mail_queue.has_opted_out',
                           email)
                clear_memo('r2.models.mail_queue.opt_count')
                return (email, True)
            else:
                return (email, False)
        return (None, False)
        
    def get_recipient(self, msg_hash):
        t = self.track_table
        s = sa.select([t.c.to_addr], t.c.msg_hash == msg_hash).execute()
        res = s.fetchall()
        return res[0][0] if res and res[:1] else None

        
    def add_to_queue(self, user, thing, emails, from_name, fr_addr, date, ip,
                     kind, body = "", reply_to = ""):
        s = self.queue_table
        hashes = []
        for email in emails:
            uid = user._id if user else 0
            tid = thing._fullname if thing else ""
            key = hashlib.sha1(str((email, from_name, uid, tid, ip, kind, body,
                               datetime.datetime.now()))).hexdigest()
            s.insert().execute({s.c.to_addr : email,
                                s.c.account_id : uid,
                                s.c.from_name : from_name,
                                s.c.fr_addr : fr_addr,
                                s.c.reply_to : reply_to, 
                                s.c.fullname: tid, 
                                s.c.ip : ip,
                                s.c.kind: kind,
                                s.c.body: body,
                                s.c.date : date,
                                s.c.msg_hash : key})
            hashes.append(key)
        return hashes


    def from_queue(self, max_date, batch_limit = 50, kind = None):
        from r2.models import is_banned_IP, Account, Thing
        keep_trying = True
        min_id = None
        s = self.queue_table
        while keep_trying:
            where = [s.c.date < max_date]
            if min_id:
                where.append(s.c.uid > min_id)
            if kind:
                where.append(s.c.kind == kind)
                
            res = sa.select([s.c.to_addr, s.c.account_id,
                             s.c.from_name, s.c.fullname, s.c.body, 
                             s.c.kind, s.c.ip, s.c.date, s.c.uid,
                             s.c.msg_hash, s.c.fr_addr, s.c.reply_to],
                            sa.and_(*where),
                            order_by = s.c.uid, limit = batch_limit).execute()
            res = res.fetchall()

            if not res: break

            # batch load user accounts
            aids = [x[1] for x in res if x[1] > 0]
            accts = Account._byID(aids, data = True,
                                  return_dict = True) if aids else {}

            # batch load things
            tids = [x[3] for x in res if x[3]]
            things = Thing._by_fullname(tids, data = True,
                                        return_dict = True) if tids else {}

            # make sure no IPs have been banned in the mean time
            ips = set(x[6] for x in res)
            ips = dict((ip, is_banned_IP(ip)) for ip in ips)

            # get the lower bound date for next iteration
            min_id = max(x[8] for x in res)

            # did we not fetch them all?
            keep_trying = (len(res) == batch_limit)
        
            for (addr, acct, fname, fulln, body, kind, ip, date, uid, 
                 msg_hash, fr_addr, reply_to) in res:
                yield (accts.get(acct), things.get(fulln), addr,
                       fname, date, ip, ips[ip], kind, msg_hash, body,
                       fr_addr, reply_to)
                
    def clear_queue(self, max_date, kind = None):
        s = self.queue_table
        where = [s.c.date < max_date]
        if kind:
            where.append([s.c.kind == kind])
        sa.delete(s, sa.and_(*where)).execute()


class Email(object):
    handler = EmailHandler()

    Kind = ["SHARE", "FEEDBACK", "ADVERTISE", "OPTOUT", "OPTIN"]
    Kind = Storage((e, i) for i, e in enumerate(Kind))

    def __init__(self, user, thing, email, from_name, date, ip, banned_ip,
                 kind, msg_hash, body = '', from_addr = '',
                 reply_to = ''):
        self.user = user
        self.thing = thing
        self.to_addr = email
        self.fr_addr = from_addr
        self._from_name = from_name
        self.date = date
        self.ip = ip
        self.banned_ip = banned_ip
        self.kind = kind
        self.sent = False
        self.body = body
        self.subject = ''
        self.msg_hash = msg_hash
        self.reply_to = reply_to

    def from_name(self):
        if not self.user:
            name = "%(name)s"
        elif self._from_name != self.user.name:
            name = "%(name)s (%(uname)s)"
        else:
            name = "%(uname)s"
        return name % dict(name = self._from_name,
                           uname = self.user.name if self.user else '')

    @classmethod
    def get_unsent(cls, max_date, batch_limit = 50, kind = None):
        for e in cls.handler.from_queue(max_date, batch_limit = batch_limit,
                                        kind = kind):
            yield cls(*e)

    def should_queue(self):
        return (not self.user  or not self.user._spam) and \
               (not self.thing or not self.thing._spam) and \
               not self.banned_ip and \
               (self.kind == self.Kind.OPTOUT or
                not has_opted_out(self.to_addr))

    def set_sent(self, date = None, rejected = False):
        if not self.sent:
            from pylons import g
            self.date = date or datetime.datetime.now(g.tz)
            t = self.handler.reject_table if rejected else self.handler.track_table
            t.insert().execute({t.c.account_id:
                                self.user._id if self.user else 0,
                                t.c.to_addr :   self.to_addr,
                                t.c.fr_addr :   self.fr_addr,
                                t.c.reply_to :  self.reply_to,
                                t.c.ip :        self.ip,
                                t.c.fullname:
                                self.thing._fullname if self.thing else "",
                                t.c.date:       self.date,
                                t.c.kind :      self.kind,
                                t.c.msg_hash :  self.msg_hash,
                                })
            self.sent = True

    def to_MIMEText(self):
        def utf8(s):
            return s.encode('utf8') if isinstance(s, unicode) else s
        fr = '"%s" <%s>' % (self.from_name(), self.fr_addr)
        if not fr.startswith('-') and not self.to_addr.startswith('-'): # security
            msg = MIMEText(utf8(self.body))
            msg.set_charset('utf8')
            msg['To']      = utf8(self.to_addr)
            msg['From']    = utf8(fr)
            msg['Subject'] = utf8(self.subject)
            if self.user:
                msg['X-Reddit-username'] = utf8(self.user.name)
            msg['X-Reddit-ID'] = self.msg_hash
            if self.reply_to:
                msg['Reply-To'] = utf8(self.reply_to)
            return msg
        return None
            
@memoize('r2.models.mail_queue.has_opted_out')
def has_opted_out(email):
    o = Email.handler.opt_table
    s = sa.select([o.c.email], o.c.email == email, limit = 1)
    res = s.execute()
    return bool(res.fetchall())
    

@memoize('r2.models.mail_queue.opt_count')
def opt_count():
    o = Email.handler.opt_table
    s = sa.select([sa.func.count(o.c.email)])
    res = s.execute().fetchone()
    return int(res[0])

        
        
    

########NEW FILE########
__FILENAME__ = meetup
from r2.lib.db.thing import Thing

import time
from datetime import datetime
from r2.lib.utils import FixedOffset
from r2.lib.db.operators import desc
from r2.lib.template_helpers import get_domain
from geolocator import gislib
# must be here to stop bizarre NotImplementedErrors being raise in the datetime
# method below
import pytz
from r2.models.account import FakeAccount
from r2.models import Subreddit
from account import Account

from pylons import g
from geolocator.providers import MaxMindCityDataProvider

class Meetup(Thing):
  def datetime(self):
    utc_timestamp = datetime.fromtimestamp(self.timestamp, pytz.utc)
    tz = FixedOffset(self.tzoffset, None)
    return utc_timestamp.astimezone(tz)

  @classmethod
  def add_props(cls, user, items):
    pass

  @classmethod
  def upcoming_meetups_query(cls):
    """Return query for all meetups that are in the future"""
    # Warning, this timestamp inequality is actually done as a string comparison
    # in the db for some reason.  BUT, since epoch seconds won't get another digit
    # for another 275 years, we're good for now...
    return Meetup._query(Meetup.c.timestamp > time.time() - g.meetup_grace_period, data=True, sort='_date')

  @classmethod
  def upcoming_meetups_by_timestamp(cls):
    """Return upcoming meetups ordered by soonest first"""
    # This doesn't do nice db level paginations, but there should only
    # be a smallish number of meetups
    query = cls.upcoming_meetups_query()
    meetups = list(query)
    meetups.sort(key=lambda m: m.timestamp)
    return map(lambda m: m._fullname, meetups)


  @classmethod
  def upcoming_meetups_near(cls, location, max_distance, count = 5):
    query = cls.upcoming_meetups_query()
    meetups = list(query)

    if not location:
      meetups.sort(key=lambda m: m.timestamp)
    else:
      if max_distance:
        # Only find nearby ones, sorted by time
        meetups = filter(lambda m: m.distance_to(location) <= max_distance, meetups)
        meetups.sort(key=lambda m: m.timestamp)
      else:
        # No max_distance, so just order by distance
        meetups.sort(key=lambda m: m.distance_to(location))

    return meetups[:count]

  def distance_to(self, location):
    """
    Returns the distance from this meetup to the passed point. The point is
    tuple, (lat, lng)
    """
    return gislib.getDistance((self.latitude, self.longitude), location)

  def keep_item(self, item):
    return True

  def can_edit(self, user, user_is_admin=False):
    """Returns true if the supplied user is allowed to edit this meetup"""
    if user is None or isinstance(user, FakeAccount):
      return False
    elif user_is_admin or self.author_id == user._id:
      return True
    elif Subreddit._by_name('discussion').is_editor(user):
      return True
    else:
      return False

  @staticmethod
  def cache_key(item):
    return False

  @staticmethod
  def group_cache_key():
    """ Used with CacheUtils.get_key_group_value """
    return "meetup-inc-key"

  def author(self):
    return Account._byID(self.author_id, True)

  @property
  def coords(self):
    return (self.latitude, self.longitude)
    
  @property
  def canonical_url(self):
    domain = get_domain(subreddit=False)
    return 'http://{0}/meetups/{1}'.format(domain, self._id36)


  @staticmethod
  def geoLocateIp(ip):
    geo = MaxMindCityDataProvider(g.geoip_db_path, "GEOIP_STANDARD")
    try:
      location = geo.getLocationByIp(ip)
    except TypeError:
      # geolocate can attempt to index into a None result from GeoIP
      location = None
    return location

########NEW FILE########
__FILENAME__ = pending_job
from r2.lib.db.thing import Thing


class PendingJob(Thing):
    _defaults = dict(run_at = None,
                     data   = None)

    @classmethod
    def store(cls, run_at, action, data=None):
        adjustment = cls(run_at=run_at, action=action, data=data)
        adjustment._commit()

########NEW FILE########
__FILENAME__ = poll
from __future__ import with_statement
import re
import datetime
from pylons import c, g, request
from r2.lib.db.thing import Thing, Relation, NotFound, MultiRelation, CreationError
from account import Account
from r2.lib.utils import to36, median
from r2.lib.filters import safemarkdown, _force_unicode
pages = None  # r2.lib.pages imported dynamically further down


class PollError(Exception):
    def __init__(self, message):
        Exception.__init__(self)
        self.message = message


poll_re = re.compile(r"""
    \[\s*poll\s*                                   # [poll] or [polltype]
        (?::\s* ([^\]]*?) )?
    \s*\]
    ((?:\s* {\s*[^}]+\s*} )*)                        # Poll options enclosed in curly braces
    """, re.VERBOSE)
poll_options_re = re.compile(r"""
    {\s*([^}]+)\s*}
    """, re.VERBOSE)
pollid_re = re.compile(r"""
    \[\s*pollid\s*:\s*([a-zA-Z0-9]+)\s*\]
    """, re.VERBOSE)
scalepoll_re = re.compile(r"""^
    \s*([^.]+)\s*(\.{2,})\s*([^.]+)\s*
    $""", re.VERBOSE)


def parsepolls(text, thing, dry_run = False):
    """
    Look for poll markup syntax, ie "[poll:polltype]{options}". Parse it,
    create a poll object, and replace the raw syntax with "[pollid:123]".
    `PollError` is raised if there are any errors in the syntax.

    :param dry_run: If true, the syntax is still checked, but no database objects are created.
    """

    def checkmatch(match):
        optionsText = match.group(2)
        options = poll_options_re.findall(optionsText)
        poll = createpoll(thing, match.group(1), options, dry_run = dry_run)
        pollid = "" if dry_run else str(poll._id)
        return "[pollid:" + pollid + "]"

    return re.sub(poll_re, checkmatch, text)


def getpolls(text):
    polls = []
    matches = re.findall(pollid_re, text)
    for match in matches:
        try:
            pollid = int(str(match))
            polls.append(pollid)
        except: pass
    return polls

def containspolls(text):
    return bool(re.match(poll_re, text) or re.match(pollid_re, text))


# Look for poll IDs in a comment/article, like "[pollid:123]", find the
# matching poll in the database, and convert it into an HTML implementation
# of that poll. If there was at least one poll, puts poll options ('[]Vote
# Anonymously [Submit]/[View Results] [Raw Data]') at the bottom
def renderpolls(text, thing):
    polls_not_voted = []
    polls_voted = []
    oldballots = []

    def checkmatch(match):
        pollid = match.group(1)
        try:
            poll = Poll._byID(pollid, True)
            if poll.thingid != thing._id:
                return "Error: Poll belongs to a different comment"

            if poll.user_has_voted(c.user):
                polls_voted.append(pollid)
                return poll.render_results()
            else:
                polls_not_voted.append(pollid)
                return poll.render()
        except NotFound:
            return "Error: Poll not found!"

    text = re.sub(pollid_re, checkmatch, _force_unicode(text))

    if polls_voted or polls_not_voted:
        voted_on_all = not polls_not_voted
        page = _get_pageclass('PollWrapper')(thing, text, voted_on_all)
        text = page.render('html')

    return text

def pollsandmarkdown(text, thing):
    ret = renderpolls(safemarkdown(text), thing)
    return ret


def createpoll(thing, polltype, args, dry_run = False):
    poll = Poll.createpoll(thing, polltype, args, dry_run = dry_run)
    if g.write_query_queue:
        queries.new_poll(poll)
    return poll


def exportvotes(pollids):
    csv_rows = []
    aliases = {'next_alias': 1}
    for pollid in pollids:
        poll = Poll._byID(pollid)
        ballots = poll.get_ballots()
        for ballot in ballots:
            row = ballot.export_row(aliases)
            csv_rows.append(row)
    return exportheader() + '\n'.join(csv_rows)

def exportheader():
    return """#
# Exported poll results from Less Wrong
# Columns: user, pollid, response, date
# user is either a username or a number (if the 'voted anonymously' button was
# checked). Anonymous user numbers are shared between poll questions asked in a
# single comment, but not between comments.
# pollid is a site-wide unique identifier of the poll.
# response is the user's answer to the poll. For multiple-choice polls, this is
# the index of their choice, starting at zero. For scale polls, this is the
# distance of their choice from the left, starting at zero. For probability and
# numeric polls, this is a number.
#
"""


def _get_pageclass(name):
    # sidestep circular import issues
    global pages
    if not pages:
        from r2.lib import pages
    return getattr(pages, name)


class PollType:
    ballot_class = None
    results_class = None

    def render(self, poll):
        return _get_pageclass(self.ballot_class)(poll).render('html')

    def render_results(self, poll):
        return _get_pageclass(self.results_class)(poll).render('html')

    def _check_num_choices(self, num):
        if num < 2:
            raise PollError('Polls must have at least two choices')
        if num > g.poll_max_choices:
            raise PollError('Polls cannot have more than {0} choices'.format(g.poll_max_choices))

    def _check_range(self, num, func, min, max, message):
        try:
            num = func(num)
            if min <= num <= max:
                return str(num)
        except:
             pass
        raise PollError(message)


class MultipleChoicePoll(PollType):
    ballot_class = 'MultipleChoicePollBallot'
    results_class = 'MultipleChoicePollResults'

    def init_blank(self, poll):
        self._check_num_choices(len(poll.choices))
        poll.votes_for_choice = [0 for _ in poll.choices]

    def add_response(self, poll, response):
        poll.votes_for_choice[int(response)] = poll.votes_for_choice[int(response)] + 1

    def validate_response(self, poll, response):
        return self._check_range(response, int, 0, len(poll.choices) - 1, 'Invalid choice')


class ScalePoll(PollType):
    ballot_class = 'ScalePollBallot'
    results_class = 'ScalePollResults'

    def init_blank(self, poll):
        parsed_poll = re.match(scalepoll_re, poll.polltypestring)
        poll.scalesize = len(parsed_poll.group(2))
        poll.leftlabel = parsed_poll.group(1)
        poll.rightlabel = parsed_poll.group(3)

        self._check_num_choices(poll.scalesize)
        poll.votes_for_choice = [0 for _ in range(poll.scalesize)]

    def add_response(self, poll, response):
        poll.votes_for_choice[int(response)] = poll.votes_for_choice[int(response)] + 1

    def validate_response(self, poll, response):
        return self._check_range(response, int, 0, poll.scalesize - 1, 'Invalid choice')


class NumberPoll(PollType):
    ballot_class = 'NumberPollBallot'
    results_class = 'NumberPollResults'

    def init_blank(self, poll):
        poll.sum = 0
        poll.median = 0

    def add_response(self, poll, response):
        responsenum = float(response)
        poll.sum += responsenum
        responses = [float(ballot.response) for ballot in poll.get_ballots()]
        responses.sort()
        poll.median = median(responses)
        
    def validate_response(self, poll, response):
        return self._check_range(response, float, -2**64, 2**64, 'Invalid number')


class ProbabilityPoll(NumberPoll):
    ballot_class = 'ProbabilityPollBallot'
    results_class = 'ProbabilityPollResults'

    def validate_response(self, poll, response):
        return self._check_range(response, float, 0, 1, 'Probability must be between 0 and 1')


class Poll(Thing):
    @classmethod
    def createpoll(cls, thing, polltypestring, options, dry_run = False):
        assert dry_run == (thing is None)

        polltype = cls.normalize_polltype(polltypestring)

        poll = cls(thingid = thing and thing._id,
                   polltype = polltype,
                   polltypestring = polltypestring,
                   choices = options)

        polltype_class = poll.polltype_class()
        if not polltype_class:
            raise PollError(u"Invalid poll type '{0}'".format(polltypestring))

        poll.init_blank()

        if not dry_run:
            thing.has_polls = True
            poll._commit()

        return poll

    @classmethod
    def normalize_polltype(self, polltype):
        #If not specified, default to multiplechoice
        if not polltype:
            return 'multiplechoice'
        
        polltype = polltype.lower()
        
        #If the poll type has a dot in it, then it's a scale, like 'agree.....disagree'
        if re.match(scalepoll_re, polltype):
            return 'scale'
        
        #Check against lists of synonyms
        if polltype in {'choice':1, 'c':1, 'list':1}:
            return 'multiplechoice'
        elif polltype in {'probability':1, 'prob':1, 'p':1, 'chance':1, 'likelihood':1}:
            return 'probability'
        elif polltype in {'number':1, 'numeric':1, 'num':1, 'n':1, 'float':1, 'double':1}:
            return 'number'
        else:
            return 'invalid'

    def polltype_class(self):
        if self.polltype == 'multiplechoice':
            return MultipleChoicePoll()
        elif self.polltype == 'scale' :
            return ScalePoll()
        elif self.polltype == 'probability' :
            return ProbabilityPoll()
        elif self.polltype == 'number':
            return NumberPoll()
        else:
            return None
    
    def init_blank(self):
        self.num_votes = 0
        self.polltype_class().init_blank(self)
        
    def add_response(self, response):
        self.num_votes = self.num_votes + 1
        self.polltype_class().add_response(self, response)

        # Mark the votes_for_choice list as dirty to ensure it gets persisted
        if hasattr(self, 'votes_for_choice'):
          self._dirties['votes_for_choice'] = self.votes_for_choice
        self._commit()
    
    def validate_response(self, response):
        return self.polltype_class().validate_response(self, response)
    
    def render(self):
        return self.polltype_class().render(self)
    
    def render_results(self):
        return self.polltype_class().render_results(self)
    
    def user_has_voted(self, user):
        if not c.user_is_loggedin:
            return False
        oldballots = self.get_user_ballot(user)
        return (len(oldballots) > 0)
    
    def get_user_ballot(poll, user):
        return list(Ballot._query(Ballot.c._thing1_id == user._id,
                                  Ballot.c._thing2_id == poll._id,
                                  data = True))


    def get_ballots(self):
        return list(Ballot._query(Ballot.c._thing2_id == self._id,
                                  data = True))
    
    def num_votes_for(self, choice):
        if self.votes_for_choice:
            return self.votes_for_choice[choice]
        else:
            raise TypeError

    def bar_length(self, choice, max_length):
        max_votes = max(self.votes_for_choice)
        if max_votes == 0:
            return 0
        return int(float(self.num_votes_for(choice)) / max_votes * max_length)

    def fraction_for(self, choice):
        return float(self.num_votes_for(choice)) / self.num_votes * 100
    
    def rendered_percentage_for(self, choice):
        return str(int(round(self.fraction_for(choice)))) + "%"
    
    #Get the total number of votes on this poll as a correctly-pluralized noun phrase, ie "123 votes" or "1 vote"
    def num_votes_string(self):
        if self.num_votes == 1:
            return "1 vote"
        else:
            return str(self.num_votes) + " votes"
    
    def get_property(self, property):
        if property == 'mean':
            return self.sum / self.num_votes
        elif property == 'median':
            return self.median


class Ballot(Relation(Account, Poll)):
    @classmethod
    def submitballot(cls, user, comment, pollobj, response, anonymous, ip, spam):
        with g.make_lock('voting_on_%s' % pollobj._id):
            pollid = pollobj._id
            oldballot = list(cls._query(cls.c._thing1_id == user._id,
                                        cls.c._thing2_id == pollid))
            if len(oldballot):
                raise PollError('You already voted on this poll')

            ballot = Ballot(user, pollobj, response)
            ballot.ip = ip
            ballot.anonymous = anonymous
            ballot.date = datetime.datetime.now().isoformat()
            ballot.response = response
            ballot._commit()
            pollobj.add_response(response)
        return ballot
    
    def export_row(self, aliases):
        userid = self._thing1_id
        pollid = self._thing2_id
        if hasattr(self, 'anonymous') and self.anonymous:
            if not userid in aliases:
                aliases[userid] = aliases['next_alias']
                aliases['next_alias'] = aliases['next_alias'] + 1
            username = aliases[userid]
        else:
            username = Account._byID(userid, data = True).name
        return "\"{0}\",\"{1}\",\"{2}\",\"{3}\"".format(username, pollid, self.response, self.date)


########NEW FILE########
__FILENAME__ = populatedb
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.models import *
from r2.lib import promote

import random

def populate(sr_name = 'reddit.com', sr_title = "reddit.com: what's new online",
             num = 100):
    sr = Subreddit._new(name= sr_name, title = sr_title)
    sr._commit()
    create_accounts(num)
    create_links(num)
    
def create_accounts(num):
    chars = 'abcdefghijklmnopqrztuvwxyz'
    for i in range(num):
        name_ext = ''.join([ random.choice(chars)
                             for x
                             in range(int(random.uniform(1, 10))) ])
        name = 'test_' + name_ext
        try:
            register(name, name)
        except AccountExists:
            pass

def create_links(num):
    accounts = list(Account._query(limit = num, data = True))
    subreddits = list(Subreddit._query(limit = num, data = True))
    for i in range(num):
        id = random.uniform(1,100)
        title = url = 'http://google.com/?q=' + str(id)
        user = random.choice(accounts)
        sr = random.choice(subreddits)
        l = Link._submit(title, url, user, sr, '127.0.0.1')

        if random.choice(([False] * 50) + [True]):
            promote.promote(l)
            

########NEW FILE########
__FILENAME__ = printable
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
class Printable(object):
    @classmethod
    def add_props(cls, listing, wrapped):
        pass

    @property
    def permalink(self, *a, **kw):
        raise NotImplementedError

    def keep_item(self, wrapped):
        return True

########NEW FILE########
__FILENAME__ = report
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
import sqlalchemy as sa

from r2.lib.db import tdb_sql as tdb
from r2.lib.db.operators import desc
from r2.lib.db.thing import Thing, Relation, NotFound, MultiRelation,\
     thing_prefix

from r2.lib.utils import tup, Storage
from link import Link, Comment, Message, Subreddit
from account import Account
from vote import score_changes
from r2.lib.memoize import memoize, clear_memo

from r2.config import cache
from r2.lib.cache import sgm
import datetime
import thing_changes as tc
from admintools import admintools


class Report(MultiRelation('report',
                           Relation(Account, Link),
                           Relation(Account, Comment),
                           Relation(Account, Subreddit),
                           Relation(Account, Message)
                           )):

    _field = 'reported'
    @property
    def _user(self): return self._thing1

    @property
    def _thing(self): return self._thing2
    
    @classmethod
    def new(cls, user, thing):
        # check if this report exists already!
        rel = cls.rel(user, thing)
        oldreport = list(rel._query(rel.c._thing1_id == user._id,
                                    rel.c._thing2_id == thing._id,
                                    data = True))

        # stop if we've seen this before, so that we never get the
        # same report from the same user twice
        if oldreport: return oldreport[0]

        r = Report(user, thing, '0', amount = 0)
        if not thing._loaded: thing._load()

        # mark item as reported
        thing._incr(cls._field)

        # mark author as reported
        if hasattr(thing, 'author_id'):
            aid = thing.author_id
            author = Account._byID(aid)
            author._incr(cls._field)
        
        # mark user as having made a report
        user._incr('report_made')
        
        r._commit()

        admintools.report(thing)

        # if the thing is already marked as spam, accept the report
        if thing._spam:
            cls.accept(r)
        else:
            # set the report amount to 0, updating the cache in the process
            cls.set_amount(r, 0)
        return r


    @classmethod
    def set_amount(cls, r, amount):
        old_amount = int(r._name)
        if old_amount != amount:
            r._name = str(amount)
            r._commit()
            
        #update the cache for the amount = 0 and amount = None cases
        rel = cls.rels[(r._thing1.__class__, r._thing2.__class__)]
        for a in set((old_amount, amount, None)):
            # clear memoizing around this thing's author
            if not r._thing2._loaded: r._thing2._load()
            if hasattr(r._thing2, "author_id"):
                clear_memo('report._by_author', cls, r._thing2.author_id,
                           amount = a)

            for t in (r._thing1, r._thing2):
                thing_key = cls._cache_prefix(rel, t.__class__,
                                              amount = a) + str(t._id)
                v = cache.get(thing_key)
                if v is not None:
                    if a == old_amount and old_amount != amount and r._id in v:
                        v.remove(r._id)
                    elif r._id not in v:
                        v.append(r._id)
                    cache.set(thing_key, v)


    @classmethod
    def accept(cls, r, correct = True):
        ''' sets the various reporting fields, but does nothing to
        the corresponding spam fields (handled by unreport)'''
        amount = 1 if correct else -1
        oldamount = int(r._name)

        # do nothing if nothing has changed
        if amount == oldamount: return

        up_change, down_change = score_changes(amount, oldamount)
        
        # update the user who made the report
        r._thing1._incr('report_correct', up_change)
        r._thing1._incr('report_ignored', down_change)

        # update the amount
        cls.set_amount(r, amount)

        # update the thing's number of reports only if we made no
        # decision prior to this
        if oldamount == 0:
            # update the author and thing field
            if getattr(r._thing2, Report._field) > 0:
                r._thing2._incr(Report._field, -1)
            if hasattr(r._thing2, "author_id"):
                aid = r._thing2.author_id
                author = Account._byID(aid)
                if getattr(author, Report._field) > 0:
                    author._incr(Report._field, -1)

            admintools.report(r._thing2, -1)

    
    @classmethod
    @memoize('report._by_author')
    def _by_author_cache(cls, author_id, amount = None):
        res = {}
        for types, rel in cls.rels.iteritems():
            # grab the proper thing table
            thing_type = types[1]
            thing_dict = tdb.types_id[thing_type._type_id]
            dtable, table = thing_dict.data_table

            # and the proper relationship table
            rel_table = tdb.rel_types_id[rel._type_id].rel_table[0]
            rel_dtable = tdb.rel_types_id[rel._type_id].rel_table[-1]

            where = [dtable.c.key == 'author_id',
                     sa.func.substring(dtable.c.value, 1, 1000) == str(author_id),
                     dtable.c.thing_id == rel_table.c.thing2_id]
            if amount is not None:
                where.extend([rel_table.c.name == str(amount),
                              rel_table.c.rel_id == rel_dtable.c.thing_id])

            s = sa.select([rel_table.c.rel_id],
                          sa.and_(*where))
            rids = [x[0] for x in s.execute().fetchall()]
            if rids: res[types] = rids
        return res

    @classmethod
    def _by_author(cls, author, amount = None):
        res = []
        rdict = cls._by_author_cache(author._id, amount = amount)
        for types, rids in rdict.iteritems():
            res.extend(cls.rels[types]._byID(rids, data=True,
                                             return_dict = False))
        return res


    @classmethod
    def fastreported(cls, users, things, amount = None):
        if amount is None:
            amount = ('1', '0', '-1')
        res = cls._fast_query(users, things, amount)
        res = dict((tuple(k[:2]), v) for k, v in res.iteritems() if v)
        return res

    @classmethod
    def reported(cls, users = None, things = None,
                 return_dict=True, amount = None):

        # nothing given, nothing to give back
        if not users and not things:
            return {} if return_dict else []

        if users: users = tup(users)
        if things: things = tup(things)

        # if both are given, we can use fast_query
        if users and things:
            return cls.fastreported(users, things)

        # type_dict stores id keyed on (type, rel_key) 
        type_dict = {}

        # if users, we have to search all the rel types on thing1_id
        if users:
            db_key = '_thing1_id'
            uid = [t._id for t in users]
            for key in cls.rels.keys():
                type_dict[(Account, key)] = uid

        # if things, we have to search only on types present in the list
        if things:
            db_key = '_thing2_id'
            for t in things:
                key = (t.__class__, (Account, t.__class__))
                type_dict.setdefault(key, []).append(t._id)

        def db_func(rel, db_key, amount):
            def _db_func(ids):
                q = rel._query(getattr(rel.c, db_key) == ids,
                               data = True)
                if amount is not None:
                    q._filter(rel.c._name == str(amount))
                r_ids = {}
                
                # fill up the report listing from the query
                for r in q:
                    key = getattr(r, db_key)
                    r_ids.setdefault(key, []).append(r._id)

                # add blanks where no results were returned
                for i in ids:
                    if i not in r_ids:
                        r_ids[i] = []
                    
                return r_ids
            return _db_func
        
        rval = []
        for (thing_class, rel_key), ids in type_dict.iteritems():
            rel = cls.rels[rel_key]
            prefix = cls._cache_prefix(rel, thing_class, amount=amount)

            # load from cache
            res = sgm(cache, ids, db_func(rel, db_key, amount), prefix)

            # append *objects* to end of list
            res1 = []
            for x in res.values(): res1.extend(x)
            if res1:
                rval.extend(rel._byID(res1, data=True, return_dict=False))

        if return_dict:
            return dict(((r._thing1, r._thing2, cls._field), r) for r in rval)
        return rval

            
    @classmethod
    def _cache_prefix(cls, rel, t_class, amount = None):
        # encode the amount keyword on the prefix
        prefix = thing_prefix(rel.__name__) + '_' + \
                 thing_prefix(t_class.__name__)
        if amount is not None:
            prefix += ("_amount_%d" % amount)
        return prefix
        
    @classmethod
    def get_reported_authors(cls, time = None, sort = None):
        reports = {}
        for t_cls in (Link, Comment, Message):
            q = t_cls._query(t_cls.c._spam == False,
                             t_cls.c.reported != 0,
                             data = True)
            q._sort = desc("_date")
            if time:
                q._filter(time)
            reports.update(Report.reported(things = list(q), amount = 0))

        # at this point, we have a full list of reports made on the interval specified
        # build up an author to report list
        authors = Account._byID([k[1].author_id 
                                 for k, v in reports.iteritems()],
                                data = True) if reports else []

        # and build up a report on each author
        author_rep = {}
        for (tattler, thing, amount), r in reports.iteritems():
            aid = thing.author_id
            if not author_rep.get(aid):
                author_rep[aid] = Storage(author = authors[aid])
                author_rep[aid].num_reports = 1
                author_rep[aid].acct_correct = tattler.report_correct
                author_rep[aid].acct_wrong = tattler.report_ignored
                author_rep[aid].most_recent = r._date
                author_rep[aid].reporters = set([tattler])
            else:
                author_rep[aid].num_reports += 1
                author_rep[aid].acct_correct += tattler.report_correct
                author_rep[aid].acct_wrong += tattler.report_ignored
                if author_rep[aid].most_recent < r._date:
                    author_rep[aid].most_recent = r._date
                author_rep[aid].reporters.add(tattler)
                
        authors = author_rep.values()
        if sort == "hot":
            def report_hotness(a):
                return a.acct_correct / max(a.acct_wrong + a.acct_correct,1)
            def better_reporter(a, b):
                q = report_hotness(b) - report_hotness(a)
                if q == 0:
                    return b.acct_correct - a.acct_correct
                else:
                    return 1 if q > 0 else -1
            authors.sort(better_reporter)
        if sort == "top":
            authors.sort(lambda x, y: y.num_reports - x.num_reports)
        elif sort == "new":
            def newer_reporter(a, b):
                t = b.most_recent - a.most_recent
                t0 = datetime.timedelta(0)
                return 1 if t > t0 else -1 if t < t0 else 0
            authors.sort(newer_reporter)
        return authors
            
    @classmethod
    def get_reporters(cls, time = None, sort = None):
        query = cls._query(cls.c._name == '0', eager_load = False,
                           data = False, thing_data = False)
        if time:
            query._filter(time)
        query._sort = desc("_date")

        account_dict = {}
        min_report_time = {}
        for r in query:
            account_dict[r._thing1_id] = account_dict.get(r._thing1_id, 0) + 1
            if min_report_time.get(r._thing1_id):
                min_report_time[r._thing1_id] = min(min_report_time[r._thing1_id], r._date)
            else:
                min_report_time[r._thing1_id] = r._date
            
        # grab users in chunks of 50
        c_size = 50
        accounts = account_dict.keys()
        accounts = [Account._byID(accounts[i:i+c_size], return_dict = False, data = True)
                    for i in xrange(0, len(accounts), c_size)]
        accts = []
        for a in accounts:
            accts.extend(a)

        if sort == "hot" or sort == "top":
            def report_hotness(a):
                return a.report_correct / max(a.report_ignored + a.report_correct,1)
            def better_reporter(a, b):
                q = report_hotness(b) - report_hotness(a)
                if q == 0:
                    return b.report_correct - a.report_correct
                else:
                    return 1 if q > 0 else -1
            accts.sort(better_reporter)
        elif sort == "new":
            def newer_reporter(a, b):
                t = (min_report_time[b._id] - min_report_time[a._id])
                t0 = datetime.timedelta(0)
                return 1 if t > t0 else -1 if t < t0 else 0
            accts.sort(newer_reporter)
            
        return accts


# def karma_whack(author, cls, dir):
#     try:
#         field = 'comment_karma' if cls == Comment else 'link_karma'
#         # get karma scale (ignore negative) -> user karma times 10%
#         karma = max(getattr(author, field) * .1, 1)
        
#         # set the scale by the number of times this guy has been marked as a spammer
#         scale = max(author.spammer+1, 1)
        
#         # the actual hit is the min of the two
#         hit = min(karma, scale) * ( 1 if dir > 0 else -1 )
        
#         author._incr(field, int(hit))
#     except AttributeError:
#         pass
    

def unreport(things, correct=False, auto = False, banned_by = ''):
    things = tup(things)

    # load authors (to set the spammer flag)
    try:
        aids = set(t.author_id for t in things)
    except AttributeError:
        aids = None

    authors = Account._byID(tuple(aids), data=True) if aids else {}


    # load all reports (to set their amount to be +/-1)
    reports = Report.reported(things=things, amount = 0)

    # mark the reports as finalized:
    for r in reports.values(): Report.accept(r, correct)

    amount = 1 if correct else -1

    spammer = {}
    for t in things:
        # clean up inconsistencies
        if getattr(t, Report._field) != 0:
            setattr(t, Report._field, 0)
            t._commit()
            # flag search indexer that something has changed
            tc.changed(t)
            
        # update the spam flag
        if t._spam != correct and hasattr(t, 'author_id'):
            # tally the spamminess of the author
            spammer[t.author_id] = spammer.get(t.author_id,0) + amount
            #author = authors.get(t.author_id)
            #if author:
            #    karma_whack(author, t.__class__, -amount)

    #will be empty if the items didn't have authors
    for s, v in spammer.iteritems():
        if authors[s].spammer + v >= 0:
            authors[s]._incr('spammer', v)
            
    # mark all as spam
    admintools.spam(things, amount = amount, auto = auto, banned_by = banned_by)

def unreport_account(user, correct = True, types = (Link, Comment, Message),
                     auto = False, banned_by = ''):
    for typ in types:
        thing_dict = tdb.types_id[typ._type_id]
        dtable, table = thing_dict.data_table
        
        by_user_query = sa.and_(table.c.thing_id == dtable.c.thing_id,
                                dtable.c.key == 'author_id',
                                sa.func.substring(dtable.c.value, 1, 1000) == str(user._id))

        s = sa.select(["count(*)"],
                      sa.and_(by_user_query, table.c.spam == (not correct)))

        # update the author's spamminess
        count = s.execute().fetchone()[0] * (1 if correct else -1)

        if user.spammer + count >= 0:
            user._incr('spammer', count)
            
        #for i in xrange(count if count > 0 else -count):
        #    karma_whack(user, typ, -count)

        things= list(typ._query(typ.c.author_id == user._id,
                                typ.c._spam == (not correct),
                                data = False, limit=300))
        admintools.spam(things, amount = 1 if correct else -1,
                        mark_as_spam = False,
                        auto = auto, banned_by = banned_by)
        

        u = """UPDATE %(table)s SET spam='%(spam)s' FROM %(dtable)s
        WHERE %(table)s.thing_id = %(dtable)s.thing_id
        AND %(dtable)s.key = 'author_id'
        AND substring(%(dtable)s.value, 1, 1000) = '%(author_id)s'"""
        u = u % dict(spam = 't' if correct else 'f',
                     table = table.name,
                     dtable = dtable.name,
                     author_id = user._id)
        table.engine.execute(u)
        
        # grab a list of all the things we just blew away and update the cache
        s = sa.select([table.c.thing_id], by_user_query)
        tids = [t[0] for t in s.execute().fetchall()]
        keys = [thing_prefix(typ.__name__, i) for i in tids]
        cache.delete_multi(keys)

                           
    # mark the reports as finalized:
    reports = Report._by_author(user, amount = 0)
    for r in reports: Report.accept(r, correct)
    
def whack(user, correct = True, auto = False, ban_user = False, banned_by = ''):
    unreport_account(user, correct = correct,
                     auto = auto, banned_by = banned_by)
    if ban_user:
        user._spam = True
        user._commit()

########NEW FILE########
__FILENAME__ = subreddit
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is Reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
#
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from pylons import c, g
from pylons.i18n import _

from r2.lib.db.thing import Thing, Relation, NotFound
from account import Account
from printable import Printable
from r2.lib.db.userrel import UserRel
from r2.lib.db.operators import lower, or_, and_, desc
from r2.lib.memoize import memoize, clear_memo
from r2.lib.utils import tup
from r2.lib.strings import strings, Score
from r2.lib.filters import _force_unicode
from r2.models.image_holder import ImageHolder
from r2.lib.errors import UserRequiredException

import os.path
import random

class SubredditExists(Exception): pass

class Subreddit(Thing, Printable, ImageHolder):
    _defaults = dict(static_path = g.static_path,
                     stylesheet = None,
                     stylesheet_rtl = None,
                     stylesheet_contents = '',
                     stylesheet_hash     = '0',
                     firsttext = strings.firsttext,
                     header = None,
                     description = '',
                     images = {},
                     ad_file = os.path.join(g.static_path, 'ad_default.html'),
                     reported = 0,
                     valid_votes = 0,
                     show_media = False,
                     domain = None,
                     default_listing = 'hot',
                     post_karma_multiplier = g.post_karma_multiplier,
                     posts_per_page_multiplier = 1
                     )
    sr_limit = 50

    @classmethod
    def _new(self, name, title, lang = 'en', type = 'public',
             over_18 = False, **kw):
        try:
            sr = Subreddit._by_name(name)
            raise SubredditExists
        except NotFound:
            sr = Subreddit(name = name,
                           title = title,
                           lang = lang,
                           type = type,
                           over_18 = over_18,
                           **kw)
            sr._commit()
            clear_memo('subreddit._by_name', Subreddit, name.lower())
            clear_memo('subreddit.subreddits', Subreddit)
            return sr

    @classmethod
    def _create_and_subscribe(self, name, user, kw):
      # kw is expected to have been sanitised by the caller
      sr = Subreddit._new(name = name, **kw)

      # make sure this user is on the admin list of that site!
      if sr.add_subscriber(user):
        sr._incr('_ups', 1)
      sr.add_moderator(user)
      sr.add_contributor(user)
      return sr

    @classmethod
    def default(cls):
      try:
        return cls._by_name(g.default_sr)
      except NotFound:
        return DefaultSR()

    @classmethod
    @memoize('subreddit._by_name')
    def _by_name_cache(cls, name):
        q = cls._query(lower(cls.c.name) == name.lower(),
                       cls.c._spam == (True, False),
                       limit = 1)
        l = list(q)
        if l:
            return l[0]._id

    @classmethod
    def _by_name(cls, name):
        #lower name here so there is only one cache
        name = name.lower()

        if name == 'friends':
            return Friends
        elif name == 'all':
            return All
        else:
            sr_id = cls._by_name_cache(name)
            if sr_id:
                return cls._byID(sr_id, True)
            else:
                raise NotFound, 'Subreddit %s' % name

    @classmethod
    @memoize('subreddit._by_domain')
    def _by_domain_cache(cls, name):
        q = cls._query(cls.c.domain == name,
                       cls.c.over_18 == (True, False),
                       limit = 1)
        l = list(q)
        if l:
            return l[0]._id

    @classmethod
    def _by_domain(cls, domain):
        sr_id = cls._by_domain_cache(_force_unicode(domain).lower())
        if sr_id:
            return cls._byID(sr_id, True)
        else:
            return None

    @property
    def moderators(self):
        return self.moderator_ids()

    @property
    def editors(self):
        return self.editor_ids()

    @property
    def contributors(self):
        return self.contributor_ids()

    @property
    def banned(self):
        return self.banned_ids()

    @property
    def subscribers(self):
        return self.subscriber_ids()

    def can_comment(self, user):
        if c.user_is_admin:
            return True
        elif not c.user.email_validated:
            return False
        elif self.is_banned(user):
            return False
        elif self.type in ('public','restricted'):
            return True
        elif self.is_moderator(user) or self.is_contributor(user):
            #private requires contributorship
            return True
        # r/meetups is private, with all posts added through /meetups. People still need to be able to comment, so we override the private behavior.
        elif self == Subreddit._by_name('meetups'):
            return True
        else:
            return False

    def can_submit(self, user):
        if c.user_is_admin:
            return True
        elif self.type == 'private' and self.is_contributor(user):
            #restricted/private require contributorship
            return True
        elif not c.user.email_validated:
            return False
        elif self.is_banned(user):
            return False
        elif self.is_moderator(user) or self.is_editor(user):
            # moderators and editors can always submit
            return True
        elif self == Subreddit._by_name('discussion') and user.safe_karma < g.discussion_karma_to_post:
            return False
        elif self.type == 'public':
            return True
        elif self == Subreddit._by_name(g.default_sr) and user.safe_karma >= g.karma_to_post:
            return True
        else:
            return False

    def can_ban(self,user):
        return (user
                and (c.user_is_admin
                     or self.is_moderator(user)))

    def can_change_stylesheet(self, user):
        if c.user_is_loggedin:
            return c.user_is_admin or self.is_moderator(user)
        else:
            return False

    def is_special(self, user):
        return (user
                and (c.user_is_admin
                     or self.is_moderator(user)
                     or (self.type in ('restricted', 'private')
                         and self.is_contributor(user))))

    def can_give_karma(self, user):
        return self.is_special(user)

    def should_ratelimit(self, user, kind):
        if c.user_is_admin:
            return False

        if kind == 'comment':
            rl_karma = g.MIN_RATE_LIMIT_COMMENT_KARMA
        else:
            rl_karma = g.MIN_RATE_LIMIT_KARMA

        return not (self.is_special(user) or
                    user.karma(kind, self) >= rl_karma)

    def can_view(self, user):
        if c.user_is_admin:
            return True

        if self.type in ('public', 'restricted'):
            return True
        elif c.user_is_loggedin:
            #private requires contributorship
            return self.is_contributor(user) or self.is_moderator(user)

    @classmethod
    def load_subreddits(cls, links, return_dict = True):
        """returns the subreddits for a list of links. it also preloads the
        permissions for the current user."""
        srids = set(l.sr_id for l in links if hasattr(l, "sr_id"))
        subreddits = {}
        if srids:
            subreddits = cls._byID(srids, True)

        if subreddits and c.user_is_loggedin:
            # dict( {Subreddit,Account,name} -> Relationship )
            SRMember._fast_query(subreddits.values(), (c.user,),
                                 ('subscriber','contributor','moderator'))

        return subreddits if return_dict else subreddits.values()

    #rising uses this to know which subreddits to include, doesn't
    #work for all/friends atm
    def rising_srs(self):
        if c.default_sr or not hasattr(self, '_id'):
            user = c.user if c.user_is_loggedin else None
            sr_ids = self.user_subreddits(user)
        else:
            sr_ids = (self._id,)
        return sr_ids

    def get_links(self, sort, time, link_cls = None):
        from r2.lib.db import queries
        from r2.models import Link

        if not link_cls:
            link_cls = Link
        return queries.get_links(self, sort, time, link_cls)

    def get_comments(self, sort, time):
        """This method relies on the fact that Link and Comment can be
          queried with the same filters"""
        from r2.models import Comment
        return self.get_links(sort, time, Comment)

    @classmethod
    def add_props(cls, user, wrapped):
        names = ('subscriber', 'moderator', 'contributor')
        rels = (SRMember._fast_query(wrapped, [user], names) if user else {})
        defaults = Subreddit.default_srs(c.content_langs, ids = True)
        for item in wrapped:
            if not user or not user.has_subscribed:
                item.subscriber = item._id in defaults
            else:
                item.subscriber = rels.get((item, user, 'subscriber'))
            item.moderator = rels.get((item, user, 'moderator'))
            item.contributor = item.moderator or \
                rels.get((item, user, 'contributor'))
            item.score = [item._ups, item._downs]
            item.score_fmt = Score.subscribers

    #TODO: make this work
    @staticmethod
    def cache_key(wrapped):
        if c.user_is_admin:
            return False

        s = (str(i) for i in (wrapped._fullname,
                              bool(c.user_is_loggedin),
                              wrapped.subscriber,
                              wrapped.moderator,
                              wrapped.contributor,
                              wrapped._spam))
        s = ''.join(s)
        return s

    #TODO: make this work
    #@property
    #def author_id(self):
        #return 1

    @classmethod
    def default_srs(cls, lang, ids = False, limit = None):
        """Returns the default list of subreddits for a given language, sorted
        by popularity"""
        pop_reddits = Subreddit._query(Subreddit.c.type == ('public', 'restricted'),
                                       sort=desc('_downs'),
                                       limit = limit,
                                       data = True,
                                       read_cache = True,
                                       write_cache = True,
                                       cache_time = g.page_cache_time)
        if lang != 'all':
            pop_reddits._filter(Subreddit.c.lang == lang)

        if not c.over18:
            pop_reddits._filter(Subreddit.c.over_18 == False)

        pop_reddits._filter(Subreddit.c.name != 'discussion')

        pop_reddits = list(pop_reddits)

        if not pop_reddits and lang != 'en':
            pop_reddits = cls.default_srs('en')

        return [s._id for s in pop_reddits] if ids else list(pop_reddits)

    @classmethod
    def user_subreddits(cls, user, limit = sr_limit):
        """subreddits that appear in a user's listings. returns the default
        srs if there are no subscriptions."""
        if user and user.has_subscribed:
            sr_ids = Subreddit.reverse_subscriber_ids(user)
            if limit and len(sr_ids) > limit:
                return random.sample(sr_ids, limit)
            else:
                return sr_ids
        else:
            return cls.default_srs(c.content_langs, ids = True)

    def is_subscriber_defaults(self, user):
        if user.has_subscribed:
            return self.is_subscriber(user)
        else:
            return self in self.default_srs(c.content_langs)

    @classmethod
    def subscribe_defaults(cls, user):
        if not user.has_subscribed:
            for sr in Subreddit.default_srs(user.pref_lang):
                if sr.add_subscriber(user):
                    sr._incr('_ups', 1)
            user.has_subscribed = True
            user._commit()

    @classmethod
    def submit_sr(cls, user):
        """subreddit names that appear in a user's submit page. basically a
        sorted/rearranged version of user_subreddits()."""
        sub_ids = cls.user_subreddits(user, False)
        srs = Subreddit._byID(sub_ids, True,
                              return_dict = False)
        srs = [s for s in srs if s.can_submit(user) or s.name == g.default_sr]

        # Add the discussion subreddit manually. Need to do this because users
        # are not subscribed to it.
        try:
            discussion_sr = Subreddit._by_name('discussion')
            if discussion_sr._id not in sub_ids and discussion_sr.can_submit(user):
                srs.insert(0, discussion_sr)
        except NotFound:
          pass
        try:
            meetup_sr = Subreddit._by_name('meetups')
            if meetup_sr in srs:
                srs.remove(meetup_sr)
        except NotFound:
            pass

        srs.sort(key=lambda a:a.title)
        return srs

    @property
    def path(self):
        return "/r/%s/" % self.name


    def keep_item(self, wrapped):
        if c.user_is_admin:
            return True

        user = c.user if c.user_is_loggedin else None
        return self.can_view(user)

class FakeSubreddit(Subreddit):
    over_18 = False
    _nodb = True

    def __init__(self):
        Subreddit.__init__(self)
        self.title = ''

    def is_moderator(self, user):
        return c.user_is_loggedin and c.user_is_admin

    def can_view(self, user):
        return True

    def can_comment(self, user):
        return False

    def can_submit(self, user):
        return False

    def can_change_stylesheet(self, user):
        return False

    def is_banned(self, user):
        return False

class FriendsSR(FakeSubreddit):
    name = 'friends'
    title = 'Friends'

    def get_links(self, sort, time, link_cls = None):
        from r2.lib.db import queries
        from r2.models import Link

        if not c.user_is_loggedin:
            raise UserRequiredException

        if not link_cls:
            link_cls = Link

        q = link_cls._query(self.c.author_id == c.user.friends,
                        sort = queries.db_sort(sort))
        if time != 'all':
            q._filter(queries.db_times[time])
        return q

class AllSR(FakeSubreddit):
    name = 'all'
    title = 'All'

    def get_links(self, sort, time, link_cls = None):
        from r2.models import Link
        from r2.lib.db import queries

        if not link_cls:
            link_cls = Link
        q = link_cls._query(sort = queries.db_sort(sort))
        if time != 'all':
            q._filter(queries.db_times[time])
        return q


class DefaultSR(FakeSubreddit):
    #notice the space before reddit.com
    name = g.default_sr
    path = '/'
    header = '/static/logo_trans.png'

    def get_links_sr_ids(self, sr_ids, sort, time, link_cls = None):
        from r2.lib.db import queries
        from r2.models import Link

        if not link_cls:
            link_cls = Link

        if not sr_ids:
            srs = []
        else:
            srs = Subreddit._byID(sr_ids, return_dict = False)

        if g.use_query_cache:
            results = []
            for sr in srs:
                results.append(queries.get_links(sr, sort, time))
            return queries.merge_cached_results(*results)
        else:
            q = link_cls._query(link_cls.c.sr_id == sr_ids,
                            sort = queries.db_sort(sort))
            if sort == 'toplinks':
                q._filter(link_cls.c.top_link == True)
            elif sort == 'blessed':
                q._filter(link_cls.c.blessed == True)
            if time != 'all':
                q._filter(queries.db_times[time])
            return q

    def get_links(self, sort, time, link_cls = None):
        user = c.user if c.user_is_loggedin else None
        sr_ids = Subreddit.user_subreddits(user)
        return self.get_links_sr_ids(sr_ids, sort, time, link_cls)

    @property
    def title(self):
        return _(g.front_page_title)

    @property
    def default_listing(self):
        return 'blessed'

class MultiReddit(DefaultSR):
    name = 'multi'

    def __init__(self, sr_ids, path):
        DefaultSR.__init__(self)
        self.real_path = path
        self.sr_ids = sr_ids

    @property
    def path(self):
        return '/r/' + self.real_path

    def get_links(self, sort, time, link_cls = None):
        return self.get_links_sr_ids(self.sr_ids, sort, time, link_cls)

class SubSR(DefaultSR):
    stylesheet = 'subreddit.css'
    #this will make the javascript not send an SR parameter
    name = ''

    def can_view(self, user):
        return True

    def can_comment(self, user):
        return False

    def can_submit(self, user):
        return True

    @property
    def path(self):
        return "/categories/"

class DomainSR(FakeSubreddit):
    @property
    def path(self):
        return '/domain/' + self.domain

    def __init__(self, domain):
        FakeSubreddit.__init__(self)
        self.domain = domain
        self.name = domain
        self.title = domain + ' ' + _('on lesswrong.com')

    def get_links(self, sort, time, link_cls = None):
        from r2.lib.db import queries
        return queries.get_domain_links(self.domain, sort, time)

Sub = SubSR()
Friends = FriendsSR()
All = AllSR()
Default = DefaultSR()

class SRMember(Relation(Subreddit, Account)): pass
Subreddit.__bases__ += (UserRel('moderator', SRMember),
                        UserRel('editor', SRMember),
                        UserRel('contributor', SRMember),
                        UserRel('subscriber', SRMember),
                        UserRel('banned', SRMember))

########NEW FILE########
__FILENAME__ = thing_changes
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.config.databases import change_engine
import sqlalchemy as sa
from r2.lib.db.tdb_sql import make_metadata, settings, index_str
from r2.lib.utils import worker


def create_table(table, index_commands=None, force = False):
    t = table
    if settings.DB_CREATE_TABLES:
        if not t.engine.has_table(t.name) or force:
            try:
                t.create(checkfirst = False)
            except: pass
            if index_commands:
                for i in index_commands:
                    try:
                        t.engine.execute(i)
                    except: pass

def change_table(metadata):
    return sa.Table(settings.DB_APP_NAME + '_changes', metadata,
                    sa.Column('fullname', sa.String, nullable=False,
                              primary_key = True),
                    sa.Column('thing_type', sa.Integer, nullable=False),
                    sa.Column('date',
                              sa.DateTime(timezone = True),
                              default = sa.func.now(),
                              nullable = False)
                    )

def make_change_tables(force = False):
    metadata = make_metadata(change_engine)
    table = change_table(metadata)
    indices = [
        index_str(table, 'fullname', 'fullname'),
        index_str(table, 'date', 'date')
        ]
    create_table(table, indices, force = force)
    return table

_change_table = make_change_tables()

def changed(thing):
    def _changed():
        d = dict(fullname = thing._fullname,
                 thing_type = thing._type_id)
        try:
            _change_table.insert().execute(d)
        except sa.exceptions.SQLError:
            t = _change_table
            t.update(t.c.fullname == thing._fullname,
                     values = {t.c.date: sa.func.now()}).execute()
    from r2.lib.solrsearch import indexed_types
    if isinstance(thing, indexed_types):
        worker.do(_changed)


def _where(cls = None, min_date = None, max_date = None):
    t = _change_table
    where = []
    if cls:
        where.append(t.c.thing_type == cls._type_id)
    if min_date:
        where.append(t.c.date > min_date)
    if max_date:
        where.append(t.c.date <= max_date)
    if where:
        return sa.and_(*where)

def get_changed(cls = None, min_date = None, limit = None):
    t = _change_table
    res = sa.select([t.c.fullname, t.c.date], _where(cls, min_date = min_date),
                    order_by = t.c.date, limit = limit).execute()
    return res.fetchall()

def clear_changes(cls = None, min_date=None, max_date=None):
    t = _change_table
    t.delete(_where(cls, min_date = min_date, max_date = max_date)).execute()

########NEW FILE########
__FILENAME__ = types
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from r2.lib.db.thing import Thing, Relation, Vote

#defining types
class Link(Thing):
    _int_props = Thing._int_props + ('num_comments',)

class Account(Thing): pass

#defining relationships
class Tag(Relation(Account, Link)): pass
class LinkAuthor(Relation(Account, Link)): pass

class Friend(Relation(Account, Account)): 
    _int_props = ('extra',)

v= Vote((Account, Link))

#v.vote(spez, link, True)
#v.vote(spez, comment, False)

#v.likes(spez, links)
#v.likes(spez, comments)

#t = thing.Things(reddit.Account, name='spez')
#r = thing.Relations(reddit.Friend)
# s = Relations(Subreddit)
# a = Relations(Author, thing1_id='spezs id')

#friends of accounts named spez
#j = thing.Join(t, r, t._id == r._thing1_id)


#items in programming.reddit by spez
#join(s, a, s.thing2_id == a.thing2_id)

#create of instances
#link = Link()
#link.url = 'http://reddit.com'
#link.title = 'best website evar!'
# link.save()

# spez = Account()
# spez.name = 'spez'
# spez.password = 'tard'
# spez.save()

# t = Tag(spez, link, 'cool')
# t.save()

# #set different types of query functions
# class Link(Thing):
#     queries = dict(baseurl = sa.func(baseurl))
# q = Query(Link)
# q.filter(baseurl='google.com')

########NEW FILE########
__FILENAME__ = vote
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from __future__ import with_statement

from r2.lib.db.thing import MultiRelation, Relation, thing_prefix, cache
from r2.lib.utils import tup, timeago
from r2.lib.db.operators import ip_network
from r2.lib.normalized_hot import expire_hot
from r2.config.databases import tz

from account import Account
from link import Link, Comment
from subreddit import Subreddit

from datetime import timedelta, datetime

from pylons import g, c


def score_changes(amount, old_amount):
    uc = dc = 0
    a, oa = amount, old_amount
    if oa == 0 and a > 0: uc = a
    elif oa == 0 and a < 0: dc = -a
    elif oa > 0 and a == 0: uc = -oa
    elif oa < 0 and a == 0: dc = oa
    elif oa > 0 and a < 0: uc = -oa; dc = -a
    elif oa < 0 and a > 0: dc = oa; uc = a
    return uc, dc

class Vote(MultiRelation('vote',
                         Relation(Account, Link),
                         Relation(Account, Comment))):


    @classmethod
    def vote(cls, sub, obj, dir, ip, spam = False, organic = False):
        from admintools import valid_user, valid_thing, update_score
        from r2.lib.count import incr_counts

        # An account can only perform 1 voting operation at a time.
        with g.make_lock('account_%s_voting' % sub._id) as lock:
            kind = obj.__class__.__name__.lower()

            lock.log('voting checkpoint A')

            # If downvoting ensure that the user has enough karma, it
            # will raise an exception if not.
            if dir == False:
                sub.check_downvote(kind)

            lock.log('voting checkpoint B')

            # Do the voting.
            sr = obj.subreddit_slow
            karma = sub.karma(kind, sr)

            lock.log('voting checkpoint C')

            #check for old vote
            rel = cls.rel(sub, obj)
            oldvote = list(rel._query(rel.c._thing1_id == sub._id,
                                      rel.c._thing2_id == obj._id,
                                      data = True))

            amount = 1 if dir is True else 0 if dir is None else -1

            lock.log('voting checkpoint D')

            is_new = False
            #old vote
            if len(oldvote):
                v = oldvote[0]
                oldamount = int(v._name)
                v._name = str(amount)

                #these still need to be recalculated
                old_valid_thing = getattr(v, 'valid_thing', True)
                v.valid_thing = (valid_thing(v, karma)
                                 and v.valid_thing
                                 and not spam)
                v.valid_user = (v.valid_user
                                and v.valid_thing
                                and valid_user(v, sr, karma))
            #new vote
            else:
                is_new = True
                oldamount = 0
                v = rel(sub, obj, str(amount))
                v.author_id = obj.author_id
                v.ip = ip
                old_valid_thing = v.valid_thing = (valid_thing(v, karma) and
                                                   not spam)
                v.valid_user = v.valid_thing and valid_user(v, sr, karma)
                if organic:
                    v.organic = organic

            lock.log('voting checkpoint E')
            v._commit()
            lock.log('voting checkpoint F')

            # Record that this account has made a downvote.
            up_change, down_change = score_changes(amount, oldamount)
            if down_change:
                sub.incr_downvote(down_change, kind)

            lock.log('voting checkpoint G')

        # Release the lock since both the downvote count and the vote count
        # have been updated, and then continue by updating karmas.
        update_score(obj, up_change, down_change,
                     v.valid_thing, old_valid_thing)

        if v.valid_user:
            author = Account._byID(obj.author_id, data=True)
            author.incr_karma(kind, sr, up_change, down_change)

        #update the sr's valid vote count
        if is_new and v.valid_thing and kind == 'link':
            if sub._id != obj.author_id:
                incr_counts([sr])

        return v

    #TODO make this generic and put on multirelation?
    @classmethod
    def likes(cls, sub, obj):
        votes = cls._fast_query(sub, obj, ('1', '-1'), data=False)
        votes = dict((tuple(k[:2]), v) for k, v in votes.iteritems() if v)
        return votes



########NEW FILE########
__FILENAME__ = test_models
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################

########NEW FILE########
__FILENAME__ = test_filters
# -*- coding: utf-8 -*-
from r2.lib.filters import wrap_urls, killhtml, format_linebreaks, spaceCompress
import nose
import re

def is_equal(output, expected_output):
    assert output == expected_output

def test_wrap_urls():
    test_cases = (
        ('http://www.plainlink.com', '<http://www.plainlink.com>'),
        ('before http://www.plainlink.com and http://www.plainlink2.com/asdf#here more',
            'before <http://www.plainlink.com> and <http://www.plainlink2.com/asdf#here> more'),
        ('(http://www.plainlink.com)', '(<http://www.plainlink.com>)'),
        ('Blah (http://www.plainlink.com)', 'Blah (<http://www.plainlink.com>)'),
        ('[Blah](http://www.plainlink.com)', '[Blah](http://www.plainlink.com)'),
        ('[Blah](http://www.plainlink.com/ÜnîCöde¡っ)', '[Blah](http://www.plainlink.com/ÜnîCöde¡っ)'),
        ('http://www.plainlink.com/ÜnîCöde¡', '<http://www.plainlink.com/ÜnîCöde¡>'),
        ('This is [an example](http://example.com/ "Title") inline link.',
            'This is [an example](http://example.com/ "Title") inline link.'),
        ('[This link](http://example.net/) has no title attribute.',
            '[This link](http://example.net/) has no title attribute.'),
        ('See my [About](/about/) page for details.', 'See my [About](/about/) page for details.'),
        ('This is [an example][id] reference-style link.', 'This is [an example][id] reference-style link.'),
        ('This is [an example] [id] reference-style link.\n\n[id]: http://example.com/  "Optional Title Here"',
            'This is [an example] [id] reference-style link.\n\n[id]: http://example.com/  "Optional Title Here"'),
        ("""[foo]: http://example.com/  "Optional Title Here"
            [foo]: http://example.com/  'Optional Title Here'
            [foo]: http://example.com/  (Optional Title Here)""",
         """[foo]: http://example.com/  "Optional Title Here"
            [foo]: http://example.com/  'Optional Title Here'
            [foo]: http://example.com/  (Optional Title Here)"""),
        ('[id]: <http://example.com/>  "Optional Title Here"',
            '[id]: <http://example.com/>  "Optional Title Here"'),
        ('[Google]: http://google.com/', '[Google]: http://google.com/'),
        ('[Daring Fireball]: http://daringfireball.net/', '[Daring Fireball]: http://daringfireball.net/'),
        ('Blah <http://a.link.com/> more', 'Blah <http://a.link.com/> more'),
    )
    for input_text, expected_output in test_cases:
        yield is_equal, wrap_urls(input_text), expected_output

def test_killhtml():
    """Test killhtml removes all tags"""
    test_cases = (
        ('Just Text', 'Just Text'),
        ('<p>One</p><p>Two</p>', 'One Two'),
        ('<p>One</p>     \n  <p>Two</p>', 'One Two'),
        ('<p>One</p>\n<p>Two</p>', 'One Two'),
        ('Some Text<p>then tag</p>', 'Some Text then tag'),
        ('<p>Some Text</p>then no tag.', 'Some Text then no tag.'),
        ('Entities &amp; &lt;hr/&gt;', 'Entities & <hr/>'),
        ('<p>Unknown tags: <asdf>qwerty</asdf>', 'Unknown tags: qwerty'),
    )
    for input_text, expected_output in test_cases:
        yield is_equal, killhtml(input_text), expected_output

def test_spaceCompress():
    html = '   \t  test  \n    <br>  <br>  \v     foo    bar  \v\n\t  '
    shrunk = spaceCompress(html)
    assert len(shrunk) < len(html)
    assert re.match(r'test\s*<br>\s+<br>\s*foo\s+bar\s*', shrunk)

def test_spaceCompress_bad_utf8():
    baddata = '\x80 ; \xbf ; \x80\xbf\x80\xbf\x80 ; \xc0 ; \xe0 ; \xf7\xf0'
    spaceCompress(baddata)
    # all we care about is no exception thrown

def test_format_linebreaks():
    """Test replacing of line breaks with br tags"""
    test_cases = (
        ('Simple:\n\nLine two', '<p>Simple:</p><p>Line two</p>'),
        ('DOS:\r\n\r\nLine breaks', '<p>DOS:</p><p>Line breaks</p>'),
        ('Classic Mac:\r\rLine breaks', '<p>Classic Mac:</p><p>Line breaks</p>'),
        ('Consecutive:\n\n\n\n\n\nLine breaks', '<p>Consecutive:</p><p>Line breaks</p>'),
        ('Multiple:\r\n\r\nLine\r\n\r\nbreaks', '<p>Multiple:</p><p>Line</p><p>breaks</p>'),
        ('\nLeading and trailing\n', '<p>Leading and trailing</p>'),
        ('Single\ndoesn\'t wrap', '<p>Single\ndoesn\'t wrap</p>'),
        ('Quote:\n\n<blockquote>(1) One\n\n(2) Two</blockquote>\n\nAfter',
            '<p>Quote:</p><blockquote><p>(1) One</p><p>(2) Two</p></blockquote><p>After</p>'),
        ('Quote 2:\n\n<blockquote>(1) One\n\n(2) Two\n</blockquote>\n\nAfter',
            '<p>Quote 2:</p><blockquote><p>(1) One</p><p>(2) Two\n</p></blockquote><p>After</p>'),
    )
    for input_text, expected_output in test_cases:
        yield is_equal, format_linebreaks(input_text), expected_output

if __name__ == '__main__':
    nose.main()

########NEW FILE########
__FILENAME__ = test_link
import nose
from r2.tests import ModelTest
from mocktest import *
from r2.models import Link

class TestLink(TestCase):
    
    def test_do_stuff(self):
        """docstring for test"""
        self.assertTrue(True, 'The test should pass')

    def test_name(self):
        link = Link(name = 'Link Name')
        self.assertEqual(link.name, 'Link Name')

    # def test_make_permalink(self):
    #     m  = mox.Mox()
    #     subreddit = m.CreateMock(self.models.Subreddit)
    #     #subreddit.name.AndReturn('stuff')
    #     m.ReplayAll()
    # 
    #     pylons.c.default_sr = True #False
    #     pylons.c.cname = False
    #     link = self.models.Link(name = 'Link Name', url = 'self', title = 'A link title', sr_id = 1)
    #     link._commit()
    #     permalink = link.make_permalink(subreddit)
    # 
    #     m.VerifyAll()
    #     assert permalink == '/lw/%s/a_link_title/' % link._id36
    # 
    # 
    # def test_make_permalink_slow(self):
    #
    #
    #     link = self.models.Link(name = 'Link Name', url = 'self', sr_id = 1)
    #     m = mox.Mox()
    #     mock_subreddit = mox.MockObject(self.models.Subreddit)
    #
    #     m.StubOutWithMock(link, 'subreddit_slow', use_mock_anything=True)
    #     link.subreddit_slow().AndReturn(mock_subreddit)
    #
    #     m.ReplayAll()
    #
    #     permalink = link.make_permalink_slow()
    #
    #     m.UnsetStubs()
    #     m.VerifyAll()

    def test_more_marker(self):
        test_cases = (
            ('asdf<a id="more"></a>lkjh', 'asdf', 'lkjh'),
        )
        for input_text, expected_summary, expected_more in test_cases:
            link = Link(article = input_text)
            self.assertEqual(link._summary(), expected_summary)
            self.assertEqual(link._more(), expected_more)

    def test_(self):
        """docstring for test_"""
        pass
########NEW FILE########
__FILENAME__ = supervise_watcher
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
#!/usr/bin/env python
from pylons import g
import os, re, sys
from datetime import datetime, timedelta

cache = g.cache
host  = g.reddit_host

default_services = ['newreddit']
default_servers = g.monitored_servers 

class Service:
    maxlen = 300
    __slots__ = ['host', 'name', 'pid', 'load']


    def __init__(self, name, pid, age, time):
        self.host = host
        self.name = name
        self.pid = pid
        self.age = age
        self.time = time
        self._cpu = []
        self.load = 0
        self.mem = 0
        self.age = 0

    def __iter__(self):
        for x in self.__slots__:
            yield (x, getattr(self, x))

        yield ('last_seen', (datetime.now() - self.time).seconds)
        for t in (0, 5, 60, 300):
            yield ('cpu_%d' % t, self.cpu(t))
        yield ('mem', self.mem)
        yield ('age', self.age)
        

    def __str__(self):
        return ("%(host)s\t%(cpu_0)5.2f%%\t%(cpu_5)5.2f%%\t%(cpu_60)5.2f%%\t%(cpu_300)5.2f%%" + 
                "\t%(pid)s\t%(name)s\t(%(last_seen)s seconds)") % dict(self)

    def track_cpu(self, usage):
        self.time = datetime.now()
        self._cpu.append((usage, self.time))
        if len(self._cpu) > self.maxlen:
            self._cpu = self._cpu[-self.maxlen:]

    def track_mem(self, usage):
        self.mem = usage

    def track_age(self, usage):
        self.age = usage

    def cpu(self, interval = 60):
        time = datetime.now()
        if interval > 0:
            cpu = filter(lambda x: time - x[1] <= timedelta(0, interval), self._cpu)
        elif self._cpu:
            cpu = [self._cpu[-1]]
        else:
            cpu = []
        return sum(c[0] for c in cpu)/max(len(cpu), 1)

class Services:
    cache_key = "supervise_services_"

    def __init__(self, _host = host):
        self.last_update = None
        self._services = {}
        self._host = _host
        self.load = 0.
        
    
    def track(self, pid, cpu, mem, age):
        try:
            if isinstance(pid, str):
                pid = int(pid)
            if self._services.has_key(pid):
                self._services[pid].track_cpu(cpu)
                self._services[pid].track_mem(mem)
                self._services[pid].track_age(age)
        except ValueError:
            pass    
        
    def add(self, name, pid, age):
        self.last_update = datetime.now()
        if not self._services.has_key(pid):
            self._services[pid] = Service(name, pid, age, self.last_update)
        else:
            self._services[pid].time = self.last_update
            self._services[pid].age = age
   
    def __iter__(self):
        return self._services.itervalues()

    def get_cache(self):
        key = self.cache_key + str(self._host)
        res = cache.get(key)
        if isinstance(res, dict):
            services = res.get("services", [])
            self.load = res.get("load", 0)
        else:
            services = res
            self.load = services[0].get("load", 0) if services else 0

        return services

    def set_cache(self):
        key = self.cache_key + str(self._host)
        svs = [dict(s) for s in self]
        cache.set(key, dict(load = self.load, 
                            services = svs,
                            host = self._host))
    
    def clean_dead(self, age = 30):
        time = datetime.now()
        active = filter(lambda s: time - self._services[s].time <= timedelta(0, age), 
                        self._services.keys())
        existing = self._services.keys()
        for pid in existing:
            if pid not in active:
                del self._services[pid]

from r2.config.templates import tpm
from r2.lib.wrapped import Wrapped
tpm.add('service_page', 'html', file = "server_status_page.html")
tpm.add('service_page', 'htmllite', file = "server_status_page.htmllite")

class Service_Page(Wrapped):
    def __init__(self, machines = default_servers):
        self.services = [Services(m) for m in machines]
    def __repr__(self):
        return "service page"

def Alert(restart_list=['MEM','CPU']):
    import time
    import smtplib
    import re
    p=re.compile("/service/newreddit(\d+)\:")
    cache_key = 'already_alerted_'
    alert_recipients = ['nerds@reddit.com']
    alert_sender = 'nerds@reddit.com'
    smtpserver = 'nt03.wireddit.com'
    
    for m in default_servers:
        s = Services(m)
        services = s.get_cache() or []
        services.sort(lambda x, y: 1 if x['name'] > y['name'] else -1)
        for service in services:
            output = "\nCPU:   "
            #output += (str(service['host']) + " " + str(service['name']))
            pegged_count = 0
            need_restart = False

            # Check for pegged procs
            for x in (0, 5, 60, 300):
                val = service['cpu_' + str(x)]
                if val > 99:
                    pegged_count += 1
                output += " %6.2f%%" % val
            service_name = str(service['host']) + " " + str(service['name'])

            if (pegged_count > 3):
                if 'CPU' in restart_list:
                    need_restart = True

            # Check for out of memory situation
            output += "\nMEMORY: %6.2f%%" % service.get('mem', 0)
            mem_pegged = (service.get('mem', 0) > 10)
            if (mem_pegged):
                if 'MEM' in restart_list:
                    need_restart = True

            if (need_restart):
                mesg = ("To: nerds@gmail.com\n" + 
                        "Subject: " + service_name.replace("/service/","") 
                          +" needs attention\n\n" 
                        + service_name.replace("/service/","") 
                        + (" is out of mem: " if mem_pegged else " is pegged:" )
                        + output)
                m = p.match(service['name'])
                # If we can restart this process, we do it here
                if m:
                    proc_number = str(m.groups()[0])
                    cmd = "/usr/local/bin/push -h " + \
                        service['host'] + " -r " + proc_number
                    result = ""
                    result = os.popen3(cmd)[2].read()
                    # We override the other message to show we restarted it
                    mesg = "To: nerds@gmail.com\n" + "Subject: " + "Process " + \
                           proc_number + " on " + service['host'] + \
                           " was automatically restarted due to the following:\n\n" + \
                           output + "\n\n" + \
                           "Here was the output:\n" + result
                    # Uncomment this to disable restart messages
                    #mesg = ""
                last_alerted = cache.get(cache_key + service_name) or 0
                #last_alerted = 0
                if (time.time() - last_alerted < 300):
                    pass
                else:
                    cache.set(cache_key + service_name, time.time())
                    if mesg is not "":
                        session = smtplib.SMTP(smtpserver)
                        smtpresult = session.sendmail(alert_sender, 
                                                      alert_recipients, mesg)
                        session.quit()
                    #print mesg
                    #print "Email sent"
           
def Write(file = None, servers = default_servers):
    if file:
        handle = open(file, "w")
    else:
        handle = sys.stdout
    handle.write(Service_Page(servers).render())
    if file:
        handle.close()
        

def Run(srvname=None, loop = True, loop_time = 2):
    services = Services()        
    pidi = 0
    cpuid = 8
    memid = 9
    ageid = 10
    procid = 11
    text = re.compile('\S+')


    from time import sleep
    counter = 0
    while True:
        # reload the processes
        if counter % 10 == 0:
            handle = os.popen("/usr/local/bin/svstat /service/*")
            for line in handle:
                try:
                    name, status, blah, pid, time, label = line.split(' ')
                    pid = int(pid.strip(')'))
                    if not srvname or any(s in name for s in srvname):
                        services.add(name, pid, time)
                except ValueError:
                    pass
            services.clean_dead()
            handle.close()

        counter +=1
        cmd = ('/usr/bin/top -b -n 1 ' +
               ' '.join("-p%d"%x.pid for x in services))
        handle = os.popen(cmd)
        for line in handle:
            line = text.findall(line)
            try:
                services.track(line[pidi], float(line[cpuid]),
                               float(line[memid]), 
                               float(line[ageid].split(':')[0]))
            except (ValueError, IndexError):
                pass
        handle.close()

        handle = os.popen('/usr/bin/uptime')
        foo = handle.read()
        services.load=float(foo.split("average:")[1].strip(' ').split(',')[0])
        handle.close()

        res = ''
        services.set_cache()

        if loop: 
            sleep(loop_time)
        else:
            break

def Test(num, load = 1., pid = 0):
    services = Services()
    for i in xrange(num):
        name = 'testproc' + str(i)
        p = i or pid
        services.add(name, p, "10")
        
        services.track(p, 100. * (i+1) / (num),
                       20. * (i+1) / num, 1.)
    services.load = load
    services.set_cache()

if __name__ == '__main__':
    Run(sys.argv[1:] if sys.argv[1:] else default_services)

########NEW FILE########
__FILENAME__ = translation
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
# 
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
# 
# The Original Code is Reddit.
# 
# The Original Developer is the Initial Developer.  The Initial Developer of the
# Original Code is CondeNet, Inc.
# 
# All portions of the code written by CondeNet are Copyright (c) 2006-2008
# CondeNet, Inc. All Rights Reserved.
################################################################################
from distutils.cmd import Command
from babel.messages.frontend import new_catalog as _new_catalog, \
     extract_messages as _extract_messages
from distutils.cmd import Command
from distutils.errors import DistutilsOptionError
from babel import Locale
import os, shutil, re



class extract_messages(_extract_messages):
    def initialize_options(self):
        _extract_messages.initialize_options(self)
        self.output_file = 'r2/i18n/r2.pot'
        self.mapping_file = 'babel.cfg'

class new_catalog(_new_catalog):
    def initialize_options(self):
        _new_catalog.initialize_options(self)
        self.output_dir = 'r2/i18n'
        self.input_file = 'r2/i18n/r2.pot'
        self.domain = 'r2'

    def finalize_options(self):
        _new_catalog.finalize_options(self)
        if os.path.exists(self.output_file):
            file2 = self.output_file + "-sav"
            print " --> backing up existing PO file to '%s'" % file2
            shutil.copyfile(self.output_file, file2)

class commit_translation(Command):
    description = 'Turns PO into MO files'
    user_options = [('locale=', 'l',
                     'locale for the new localized catalog'), ]

    def initialize_options(self):
        self.locale = None
        self.output_dir = 'r2/i18n'
        self.domain = 'r2'
        self.string = '_what_'

    def finalize_options(self):
        if not self.locale:
            raise DistutilsOptionError('you must provide a locale for the '
                                       'catalog')

        self.input_file = os.path.join(self.output_dir, self.locale,
                                       'LC_MESSAGES', self.domain + '.po')
        self.output_file = os.path.join(self.output_dir, self.locale,
                                        'LC_MESSAGES', self.domain + '.mo')
    def run(self):
        cmd = 'msgfmt -o "%s" "%s"' % (self.output_file, self.input_file)
        handle = os.popen(cmd)
        print handle.read(),
        handle.close()

class test_translation(new_catalog):
    description = 'makes a mock-up PO and MO file for testing and sets to en'
    user_options = [('locale=', 'l',
                     'locale for the new localized catalog'),
                    ('string=', 's',
                     'global string substitution on translation'),
                    ]

    def initialize_options(self):
        new_catalog.initialize_options(self)
        self.locale = 'en'
        self.string = '_what_'

    def finalize_options(self):
        self.output_file = os.path.join(self.output_dir, self.locale,
                                        'LC_MESSAGES', self.domain + '.po')
        self.output_file_mo = os.path.join(self.output_dir, self.locale,
                                        'LC_MESSAGES', self.domain + '.mo')

        if not os.path.exists(os.path.dirname(self.output_file)):
            os.makedirs(os.path.dirname(self.output_file))

        self._locale = Locale.parse('en')
        self._locale.language = self.locale

    def run(self):
        new_catalog.run(self)
        handle = open(self.output_file)
        res = ''
        counter = 0
        formatting_string = False
        for line in handle:
            if not ('""' in line):
                strlen = len(re.findall(r"\S+", line)) - 1
            if "%" in line:
                formatting_string = re.findall(r"%\S+", line)
                strlen -= len(formatting_string)
                formatting_string = (', '.join(formatting_string)).strip('"')
            if '""' in line and not("msgid" in line):
                strlen = 1 if strlen < 1 else strlen
                string = ' '.join([self.string for x in range(0, strlen)])
                if formatting_string:
                    string = '%s [%s]' %(string, formatting_string)
                    formatting_string = ''
                string = '"%s"' % string
                res += line if counter < 1 else line.replace('""', string)
                counter += 1
            elif line.startswith('"Last-Translator:'):
                res += line.replace("FULL NAME", "Babel Phish")
            else:
                res += line
        handle.close()
        handle = open(self.output_file, 'w')
        handle.write(res)
        handle.close()
        cmd = 'msgfmt -o "%s" "%s"' % (self.output_file_mo, self.output_file)
        handle = os.popen(cmd)
        print "converting to MO file..."
        print handle.read(),
        handle.close()


########NEW FILE########
__FILENAME__ = wiki_pages_embed
# Configuration file for wiki pages to be embedded (and cached) on the main site.
# Pages are configured using a python hash.  
# - The hash key is a name used as a parameter to refer to the page in question
# url : is the page to be embedded
# route : The route to access the page at.  May be ommited
# htmlroute : If present, the *html only* of the page will be accessible at /wiki/{htmlroute}
#             Useful for getting wiki content via an ajax request


from pylons.i18n import _
from pylons import c

allWikiPagesCached = \
  {
    'about': { 'url' : 'http://wiki.lesswrong.com/wiki/Lesswrong:Aboutpage',
               'route' : 'Aboutpage'
               },

    'main': { 'url' : 'http://wiki.lesswrong.com/wiki/Lesswrong:Homepage',
              'route' : 'Homepage'
              },

    'comment-help' : { 'url' : 'http://wiki.lesswrong.com/wiki/Lesswrong:Commentmarkuphelp',
                       'route' : 'Commentmarkuphelp'
                       },

    'wiki-stylesheet' : { 'url' : 'http://wiki.lesswrong.com/wiki/Lesswrong:Stylesheet',
                          'route' : 'Stylesheet',
                          'id' : 'stylesheet',
                          'content-type' : 'text/css'
                       }
    }


########NEW FILE########
__FILENAME__ = accounts_without_emails
from r2.models import Account, PendingJob
import sqlalchemy as sa

from r2.lib.db import tdb_sql

def max_thing_id(thing_type):
    thing_type = tdb_sql.types_id[thing_type._type_id]
    thing_tbl = thing_type.thing_table
    return sa.select([sa.func.max(thing_tbl.c.thing_id)]).execute().scalar()

def run():

    STEP = 100
    thing = Account
    max_id = max_thing_id(thing)
    id_start = 0
    emailless = list()

    for id_low in xrange(id_start, max_id + 1, STEP):
        users = list(query_thing_id_range(thing, id_low, id_low + STEP))

        for user in users:
            if not user._loaded:
                user._load()
            if not hasattr(user, 'email'):
                emailless.append(user._id)

    print emailless
    return emailless


def query_thing_id_range(thing, id_low, id_high):
    return thing._query(thing.c._id >= id_low, thing.c._id < id_high,
                      eager_load=True)

run()

########NEW FILE########
__FILENAME__ = add_descendant_karma
from r2.models.link import Comment, Link
import sqlalchemy as sa

from r2.lib.db import tdb_sql

def max_thing_id(thing_type):
    thing_type = tdb_sql.types_id[thing_type._type_id]
    thing_tbl = thing_type.thing_table
    return sa.select([sa.func.max(thing_tbl.c.thing_id)]).execute().scalar()

def run():

    STEP = 100
    thing = Link
    max_id = max_thing_id(thing)
    id_start = 0

    for id_low in xrange(id_start, max_id + 1, STEP):
        print "Add desc karma for links %s to %s" % (id_low, id_low + STEP)

        links = list(query_thing_id_range(thing, id_low, id_low + STEP))

        for link in links:
            if not link._loaded:
                link._load()
            comments = list(Comment._query(Comment.c.link_id == link._id, eager_load = True))
            link_descendant_karma = 0
            for comment in comments:
                if not comment._loaded:
                    comment._load()
                if hasattr(comment, 'parent_id') and comment.parent_id:
                    Comment._byID(comment.parent_id).incr_descendant_karma([], comment._ups - comment._downs)
                link_descendant_karma += (comment._ups - comment._downs)

            link._incr('_descendant_karma', link_descendant_karma)

def query_thing_id_range(thing, id_low, id_high):
    return thing._query(thing.c._id >= id_low, thing.c._id < id_high,
                      eager_load=True)

run()

########NEW FILE########
__FILENAME__ = add_links_to_ob_export
import os
import sys
import yaml
import re

kill_whitespace_re = re.compile('\s')
kill_entities_re = re.compile('&#?[a-z0-9]{1,4};')
def kill_whitespace(body):
    body = kill_whitespace_re.sub('', body)
    body = kill_entities_re.sub('', body)
    body = body.replace('<p>', '')
    body = body.replace('</p>', '')
    body = body.replace('<br/>', '')
    
    return body

if __name__ == '__main__':
    if len(sys.argv) <= 4:
        print 'Usage: %s <export_file> <api_file> <user_map> <outputfile>' % os.path.basename(sys.argv[0])
        print
        print ' Uses the api_file to supplement the export_file with permalinks.'
        print ' Writes the result to outputfile.'
        sys.exit(-1)

    export_file = open(sys.argv[1])
    api_file = open(sys.argv[2])
    mapfile = open(sys.argv[3])
    output_file = open(sys.argv[4], 'w')
    mappings = yaml.load(api_file, Loader=yaml.CLoader)
    export   = yaml.load(export_file, Loader=yaml.CLoader)

    # Load the user mapping dict
    user_map = yaml.load(mapfile, Loader=yaml.CLoader)

    # Turn the mappings into a lookup table on title and content
    post_mapping = {}
    title_mapping = {}
    for post in mappings:
        title = post['title']
        body = post['description'] + post['mt_text_more']
        if not isinstance(body, unicode):
            body = unicode(body, 'utf-8')
        if not isinstance(title, unicode):
            title = unicode(title, 'utf-8')

        key = (kill_whitespace(body), kill_whitespace(title))
        post_mapping[key] = post
        title_mapping[kill_whitespace(title)] = key

    # Scan the export file
    new_export = []
    for entry in export:
        if 'Eliezer Yudkowsky' not in entry['author']:
            continue

        if entry['status'] != 'Publish':
            continue

        # Get the title and do a lookup on the permalink
        body = entry['description'] + entry['mt_text_more']
        body = body.decode('utf-8')
        title = entry['title']
        title = title.decode('utf-8')
        print title

        key = (kill_whitespace(body), kill_whitespace(title))
        try:
            api_post = post_mapping[key]
        except KeyError:
            print title_mapping[kill_whitespace(title)]
            print 
            print key
            print
            
            import difflib
            d = difflib.Differ()
            diff = d.compare(title_mapping[kill_whitespace(title)], key)
            import pprint
            pprint.pprint(list(diff))
            raise
        
        new_entry = entry
        new_entry['permalink'] = api_post['permaLink']
        new_entry['description'] = api_post['description']
        new_entry['mt_text_more'] = api_post['mt_text_more']
        new_entry['authorEmail'] = user_map.get(new_entry['author'], '').lower()
        
        # Process comments
        comments = new_entry.get('comments', [])
        for comment in comments:
            if not comment['authorEmail']:
                comment['authorEmail'] = user_map.get(comment['author'], '').lower()
            else:
                comment['authorEmail'] = comment['authorEmail'].lower()

        new_export.append(new_entry)
    
    # Print out the result
    yaml.dump(new_export, output_file, Dumper=yaml.CDumper)
        
########NEW FILE########
__FILENAME__ = clear_cache
from r2.models import Account
from r2.lib.memoize import clear_memo

def clear_account_by_name_cache():
    q = Account._query(Account.c._deleted == (True, False), data = True)
    for account in q:
        name = account.name
        clear_memo('account._by_name', Account, name.lower(), True)
        clear_memo('account._by_name', Account, name.lower(), False)
        print "Cleared cache for %s" % account.name

########NEW FILE########
__FILENAME__ = db_export
from r2.lib.db.exporter import Exporter

# features is a sequence of features to extract.  Use db_export.sh to
# run each export in a separate python process and hopefully keep
# memory usage down.
def export_to(dbfile, features=None):
    e = Exporter(dbfile)
    if features is None:
        e.export_db()
    else:
        if 'users'     in features: e.export_users()
        if 'links'     in features: e.export_links()
        if 'comments'  in features: e.export_comments()
        if 'votes'     in features: e.export_votes()
        if 'indexes'   in features: e.create_indexes()

########NEW FILE########
__FILENAME__ = fix_bare_links
import re
import sys
import codecs

bare_link_re = re.compile(r'([^-A-Z0-9+&@#/%?=~_|!:,.;">]|^)(/lw/[^/]+/[^/]+/)([^-A-Z0-9+&@#/%?=~_|!:;"<]|$)')
linked_bare_link_re = re.compile(r'(<a href="[^"]+">)(/lw/[^/]+/[^/]+[-A-Z0-9+&@#/%?=~_|!:,.;]*[-A-Z0-9+&@#/%=~_|]</a>)')
spaces_around_anchor_re = re.compile(r'<a href\s*=\s+"?([^">]+)"?\s*>', re.IGNORECASE)
no_quotes_on_anchor_re = re.compile(r'<a href=([^>\'"]+)>([^<]+)</a>', re.IGNORECASE)
single_quotes_on_anchor_re = re.compile(r'<a href=\'([^>]+)\'>')
well_formed_uppercase_re = re.compile(r'<A [Hh][Rr][Ee][Ff]="([^"]+)">([^<]+)</A>')

def sub_group_1(match):
    return "<a href=\"%s\">" % match.group(1)

def sub_with_end_tag(match):
    return "<a href=\"%s\">%s</a>" % (match.group(1), match.group(2))

def wrap_bare_link(match):
    return '%s<a href="%s">http://lesswrong.com%s</a>%s' % (match.group(1), match.group(2), match.group(2), match.group(3))

def add_host_to_linked_bare_link(match):
    return match.group(1) + 'http://lesswrong.com' + match.group(2)

def rewrite_bare_links(content):
    # Tidy up strange HTML first
    content = spaces_around_anchor_re.sub(sub_group_1, content)
    content = no_quotes_on_anchor_re.sub(sub_with_end_tag, content)
    content = single_quotes_on_anchor_re.sub(sub_group_1, content)
    content = well_formed_uppercase_re.sub(sub_with_end_tag, content)
    
    # Fix bare links
    content = bare_link_re.sub(wrap_bare_link, content)
    content = linked_bare_link_re.sub(add_host_to_linked_bare_link, content)

    return content

def fix_bare_links(apply=False):
    from r2.models import Comment
    from r2.lib.db.thing import NotFound
    
    fbefore = codecs.open('fix_bare_links_before.txt', 'w', 'utf-8')
    fafter  = codecs.open('fix_bare_links_after.txt', 'w', 'utf-8')
    
    comment_id = 1
    try:
        # The comments are retrieved like this to prevent the API from 
        # attempting to load all comments at once and then iterating over them
        while True:
            comment = Comment._byID(comment_id, data=True)
        
            if (hasattr(comment, 'ob_imported') and comment.ob_imported) and (hasattr(comment, 'is_html') and comment.is_html):
                body = comment.body
                if isinstance(body, str):
                    try:
                        body = body.decode('utf-8')
                    except UnicodeDecodeError:
                        print >>sys.stderr, "UnicodeDecodeError, using 'ignore' error mode, comment: %d" % comment._id
                        body = body.decode('utf-8', errors='ignore')
                new_content = rewrite_bare_links(body)
                
                if new_content != body:
                    print >>fbefore, body
                    print >>fafter, new_content
                    
                    if apply:
                        comment.body = new_content
                        comment._commit()
                    
                    try:
                        print >>sys.stderr, "Rewrote comment %s" % comment.make_permalink_slow().encode('utf-8')
                    except UnicodeError:
                        print >>sys.stderr, "Rewrote comment with id: %d" % comment._id
                    
            
            comment_id += 1
    except NotFound:
        # Assumes that comment ids are sequential and never deleted
        # (which I believe to true) -- wjm
        print >>sys.stderr, "Comment %d not found, exiting" % comment_id

    return

########NEW FILE########
__FILENAME__ = fix_broken_things
import r2.lib.utils as utils
from pylons import g
import datetime

def fix_all_broken_things(delete=False):
    from r2.models import Link,Comment

    # 2009-07-21 is the first broken thing at the time of writing.
    from_time = datetime.datetime(2009, 7, 21, tzinfo=g.tz)
    to_time   = utils.timeago('60 seconds')

    for (cls,attrs) in ((Link,('author_id','sr_id')),
                        (Comment,('author_id','sr_id','body','link_id'))):
        utils.find_broken_things(cls,attrs,
                                 from_time, to_time,
                                 delete=delete)


########NEW FILE########
__FILENAME__ = fix_imported_content_with_images
import urlparse
import re

def print_change(old, new):
    print "  %s => %s" % (old, new)

interesting_hosts = set(['www.overcomingbias.com', 'robinhanson.typepad.com'])
funny_imgs     = {
    'http://robinhanson.typepad.com/.a/6a00d8341c6a2c53ef010536c21d63970b-800wi': 'http://lesswrong.com/static/imported/6a00d8341c6a2c53ef010536c21d63970b-800wi.jpg',
}
ext_re = re.compile(r'.*\.(jpg|gif|png)$', re.IGNORECASE)
path_re = re.compile(r'/(images|uncategorized)/(\d{4}/\d{2}/\d{2}/[^/]+)$')
def substitute_ob_url(url):

    if url in funny_imgs:
        # Special case
        print_change(url, funny_imgs[url])
        return funny_imgs[url]

    (scheme, host, path, query, fragment) = urlparse.urlsplit(url)

    if host not in interesting_hosts:
        return url

    # Check if this is an image URL at OB
    match = ext_re.search(path) or ext_re.search(query)
    if match:
        match = path_re.search(match.group())
        if match:
            # Translate to new path
            host = 'lesswrong.com'
            path = '/static/imported/%s' % match.group(2)
            old_url = url
            url  = urlparse.urlunsplit((scheme, host, path, '', ''))
            print_change(old_url, url)
        else:
            print " Got unexpected image url: %s" % url

    return url

# Borrowed from the importer
url_re = re.compile(r"""(?:https?|ftp|file)://[-A-Z0-9+&@#/%?=~_|!:,.;]*[-A-Z0-9+&@#/%=~_|]""", re.IGNORECASE)
def process_content(html):
    if html:
        # if isinstance(text, str):
        #     text = text.decode('utf-8')
        #
        # # Double decode needed to handle some wierd characters
        # text = text.encode('utf-8')
        html = url_re.sub(lambda match: substitute_ob_url(match.group()), html)

    return html

# Main function
def fix_images(dryrun=True):
    from r2.models import Link, Comment

    links = Link._query(Link.c.ob_permalink != None, data = True)
    for link in links:
        ob_url = link.ob_permalink.strip()
        print "Processing %s" % ob_url

        new_content = process_content(link.article)
        if not dryrun:
            link.article = new_content
            link._commit()

        comments = Comment._query(Comment.c.link_id == link._id, data = True)
        for comment in comments:
            new_content = process_content(comment.body)
            if not dryrun:
                comment.body = new_content
                comment._commit()

########NEW FILE########
__FILENAME__ = fix_link_urls
import re

def convert_url(url):
    parts = filter(None, url.split('/'))
    return "/lw/%s/" % '/'.join(parts[-2:])

good_url_regex = re.compile('^/lw/.*|self')
def should_convert_url(url):
    return not good_url_regex.match(url)

def get_links():
    from r2.models import Link
    q = Link._query(data=True)
    return list(q)

def fix_link_urls(links, force=False):
    for link in links:
        if force or should_convert_url(link.url):
            new_url = convert_url(link.url)
            print "%s => %s" % (link.url, new_url)
            link.url = new_url
            link._commit()
            link.set_url_cache()

def print_link_urls(links, force=False):
    print "Link URLs to fix:"
    for link in links:
        if force or should_convert_url(link.url):
            new_url = convert_url(link.url)
            print "%s => %s" % (link.url, new_url)

def staging_links():
    from r2.models import Link
    ids = (
        int('1', 36),
        int('4', 36),
        int('5', 36),
        int('6', 36),
        int('8', 36),
        int('9', 36),
        int('a', 36),
        int('3', 36),
        int('7', 36),
        int('b', 36),
        int('c', 36),
        int('d', 36),
        int('e', 36),
        int('f', 36),
        int('k', 36),
        int('l', 36),
        int('j', 36),
        int('p', 36),
        int('q', 36),
        int('r', 36),
        int('t', 36),
        int('u', 36),
        int('y', 36),
        int('11', 36),
        int('12', 36),
        int('z', 36),
        int('2', 36),
        int('13', 36),
        int('o', 36),
        int('n', 36),
        int('10', 36),
        int('i', 36),
        int('h', 36),
        int('g', 36),
    )
    return Link._byID(ids, data=True, return_dict=False)

########NEW FILE########
__FILENAME__ = geolocate_users
# A one-time script to find all users with textual locations, and add
# corresponding latitude/longitude info to the database.


import json
import urllib, urllib2

from pylons import g

from r2.models import Account


def geolocate_users():
    users = list(Account._query(Account.c.pref_location != None,
                                data=True))
    log('Geolocating {0} users...'.format(len(users)))

    for user in users:
        if not user.pref_location or user.pref_latitude:
            continue
        coords = geolocate_address(user.pref_location)
        if coords:
            user.pref_latitude, user.pref_longitude = coords
            user._commit()
            log('{0} ({1!r}) => ({2:.3}, {3:.3})'.format(
                user.name, user.pref_location, user.pref_latitude, user.pref_longitude))


def geolocate_address(addr):
    base = 'http://maps.googleapis.com/maps/api/geocode/json?'
    url = base + urllib.urlencode({'sensor': 'false', 'address': addr})
    sock = urllib2.urlopen(url)
    try:
        raw = sock.read()
    finally:
        sock.close()

    try:
        data = json.loads(raw)
        if data['status'] != 'OK':
            log('Error geolocating {0!r} - {1}'.format(addr, data['status']))
            return None

        coords = data['results'][0]['geometry']['location']
        return coords['lat'], coords['lng']
    except Exception:
        log('Malformed response for address {0!r}'.format(addr))
        return None


def log(msg):
    print(msg)


geolocate_users()

########NEW FILE########
__FILENAME__ = import_missing_comments
import re
import yaml
import pytz
import urlparse
import datetime
from random import Random
from BeautifulSoup import BeautifulSoup

from r2.models import Link,Comment,Account,Subreddit,FakeAccount
from r2.models.account import AccountExists, register
from r2.lib.db.thing import NotFound

DATE_FORMAT = '%m/%d/%Y %I:%M:%S %p'
INPUT_TIMEZONE = pytz.timezone('America/New_York')
MAX_RETRIES = 100

dryrun = True
username_mapping = {}

# Constants for the characters to compose a password from.
# Easilty confused characters like I and l, 0 and O are omitted
PASSWORD_NUMBERS='123456789'
PASSWORD_LOWER_CHARS='abcdefghjkmnpqrstuwxz'
PASSWORD_UPPER_CHARS='ABCDEFGHJKMNPQRSTUWXZ'
PASSWORD_OTHER_CHARS='@#$%^&*'
ALL_PASSWORD_CHARS = ''.join([PASSWORD_NUMBERS,PASSWORD_LOWER_CHARS,PASSWORD_UPPER_CHARS,PASSWORD_OTHER_CHARS])

rng = Random()
def generate_password():
    password = []
    for i in range(8):
        password.append(rng.choice(ALL_PASSWORD_CHARS))
    return ''.join(password)

def comment_excerpt(comment):
    excerpt = comment['body'].replace("\n", '')[0:50]
    try:
        excerpt = "comment by '%s': %s" % (comment['author'].decode('utf-8').encode('utf-8'), excerpt.decode('utf-8').encode('utf-8'))
    except UnicodeError:
        excerpt = '*'
    return excerpt

re_non_alphanum = re.compile(r'[^a-zA-Z0-9]*')
def comment_exists(post, comment):
    # Check if this comment already exists using brutal compare on content
    # BeautifulSoup is used to parse as HTML in order to remove markup
    content = ''.join(BeautifulSoup(comment['body']).findAll(text=True))
    key = re_non_alphanum.sub('', content)
    existing_comments = Comment._query(Comment.c.link_id == post._id, Comment.c.ob_imported == True, data=True)
    for existing_comment in existing_comments:
        author = Account._byID(existing_comment.author_id, data=True)
        content = ''.join(BeautifulSoup(existing_comment.body).findAll(text=True))
        existing_key = re_non_alphanum.sub('', content)
        if key == existing_key:
            print " Skipping existing %s" % comment_excerpt(comment)
            return True
        # else:
        #     print "%s *|NOT|* %s" % (key, existing_key)

    return False

def get_or_create_account(name):
    try:
        # Look for an account we have cached
        account = username_mapping[name]
    except KeyError:
        # See if there's a previously imported account
        account = list(Account._query(Account.c.ob_account_name == name, data=True))
        if len(account) == 1:
            account = account[0]
        elif len(account) > 1:
            print " Got more than one account for OB username '%s', select one below:" % name
            for i in range(len(account)):
                email = account[i].email if hasattr(account[i], 'email') else ''
                print "  %d. %s, %s" % (i, account[i].name, email)
            i += 1
            print "  %d. Create new" % i
            i += 1
            print "  %d. None, abort" % i
            
            max_choice = i
            choice = -1
            while choice < 0 or choice > max_choice:
                choice = raw_input("Enter selection: ")
                try:
                    choice = int(choice)
                except ValueError:
                    choice = -1
            if choice in range(len(account)):
                account = account[choice]
            elif choice == max_choice:
                raise Exception("Aborting")
            else:
                # Fall through to code below
                account = None
        else:
            # Try derivatives of the name that may exist
            candidates = (
                name,
                name.replace(' ', ''),
                name.replace(' ', '_')
            )

            for candidate in candidates:
                try:
                    account = Account._by_name(candidate)
                except NotFound:
                    continue

                if account:
                    if not dryrun:
                        account.ob_account_name = name
                        account._commit()
                    break

        # No account found, create a new one
        if not account:
            account = create_account(name)

        username_mapping[name] = account

    return account

def create_account(full_name):
    name = full_name.replace(' ', '_')
    retry = 2 # First retry will by name2
    username = name
    while True:
        # Create a new account
        try:
            if dryrun:
                try:
                    account = Account._by_name(username)
                    if account:
                        raise AccountExists
                except NotFound:
                    account = FakeAccount()
                    account.name = username
            else:
                account = register(username, generate_password(), None)
                account.ob_account_name = full_name
                account._commit()
        except AccountExists:
            # This username is taken, generate another, but first limit the retries
            if retry > MAX_RETRIES:
                raise StandardError("Unable to create account for '%s' after %d attempts" % (full_name, retry - 1))
        else:
            return account
        username = "%s%d" % (name, retry)
        retry += 1

def process_comments_on_post(post, comments):
    for comment in comments:
        if comment_exists(post, comment):
            continue

        # Prepare data for import
        ip = '127.0.0.1'
        naive_date = datetime.datetime.strptime(comment['dateCreated'], DATE_FORMAT)
        local_date = INPUT_TIMEZONE.localize(naive_date, is_dst=False) # Pick the non daylight savings time
        utc_date = local_date.astimezone(pytz.utc)

        # Determine account to use for this comment
        account = get_or_create_account(comment['author'])

        if not dryrun:
            # Create new comment
            new_comment, inbox_rel = Comment._new(account, post, None, comment['body'], ip, date=utc_date)
            new_comment.is_html = True
            new_comment.ob_imported = True
            new_comment._commit()

        try:
            print " Imported as '%s' %s" % (account.name.decode('utf-8').encode('utf-8'), comment_excerpt(comment).decode('utf-8').encode('utf-8'))
        except UnicodeError:
            print " Imported comment"

re_strip_path = re.compile(r'^/overcomingbias')
def adjust_permalink(permalink):
    """Transform:
    http://robinhanson.typepad.com/overcomingbias/2008/12/evolved-desires.html
    into:
    http://www.overcomingbias.com/2008/12/evolved-desires.html"""

    # Adjust the permalink to match those that were imported
    (scheme, host, path, query, fragment) = urlparse.urlsplit(permalink)
    host = 'www.overcomingbias.com'
    path = re_strip_path.sub('', path, 1)

    return urlparse.urlunsplit((scheme, host, path, query, fragment))

def import_missing_comments(filename, apply_changes=False):
    """Imports the comments from the supplied YAML"""
    missing_comments = yaml.load(open(filename), Loader=yaml.CLoader)
    global dryrun
    dryrun = not apply_changes

    total_posts = len(missing_comments)
    post_count = 0
    for post in missing_comments:
        if post['author'] != 'Eliezer Yudkowsky':
            # print "Skipping non-EY post (%s): %s" % (post['author'], post['permalink'])
            continue

        ob_permalink = adjust_permalink(post['permalink'])

        # Attempt to retrieve the post that was imported into Less Wrong
        imported_post = list(Link._query(Link.c.ob_permalink == ob_permalink, data=True))
        if len(imported_post) < 1:
            print "Unable to retrieve imported post: %s" % ob_permalink
            continue
        elif len(imported_post) > 1:
            print "Got more than one result for: %s" % ob_permalink
            raise Exception
        else:
            imported_post = imported_post[0]

        post_count += 1
        try:
            print "Importing (%d of %d) comments on: %s" % (post_count, total_posts, imported_post.canonical_url)
        except UnicodeError:
            print "Importing comments on post (%d of %d)"
        process_comments_on_post(imported_post, post['comments'])


########NEW FILE########
__FILENAME__ = karma_report
import code
from r2.config import databases
from sqlalchemy.sql import text

# These functions will fail badly if the databases (main, comment,
# vote) are ever split.

def list_voters(user):
    sql = '''
SELECT
  voter.value,
  SUM(vote.name::integer) AS votes
FROM
  reddit_rel_vote_account_comment AS vote
  INNER JOIN reddit_data_account AS voter ON voter.thing_id = vote.thing1_id
  INNER JOIN reddit_data_comment AS comment ON comment.thing_id = vote.thing2_id
WHERE
  voter.key = 'name' AND
  comment.key = 'author_id' AND
  comment.value = (SELECT thing_id
                   FROM reddit_data_account
                   WHERE key = 'name' AND value = :user)::text
GROUP BY
  voter.value
ORDER BY
  votes ASC
'''
    res = databases.main_engine.execute(text(sql), user=user)
    for row in res:
        print(row[0], row[1])

def num_comments(user):
    sql = '''
SELECT
  COUNT(*)
FROM
  reddit_data_comment AS comment
WHERE
  comment.key = 'author_id' AND
  comment.value = (SELECT thing_id
                   FROM reddit_data_account
                   WHERE key = 'name' AND value = :user)::text
'''
    res = databases.main_engine.execute(text(sql), user=user)
    for row in res:
        print row[0]

def count_replies(author, replier):
    sql = '''
SELECT
  COUNT(*)
FROM
  reddit_data_comment AS comment
WHERE
  comment.key = 'author_id' AND
  comment.value = (SELECT thing_id
                   FROM reddit_data_account
                   WHERE key = 'name' AND value = :replier)::text AND
  comment.thing_id IN (-- thing_ids of replies to comment in set
                       SELECT
                         thing_id::integer
                       FROM
                         reddit_data_comment AS comment
                       WHERE
                         comment.key = 'parent_id' AND
                         comment.value IN (-- thing_id of author's comments
                                           SELECT
                                             thing_id::text
                                           FROM
                                             reddit_data_comment AS comment
                                           WHERE
                                             comment.key = 'author_id' AND
                                             comment.value = (SELECT thing_id
                                                              FROM reddit_data_account
                                                              WHERE key = 'name' AND value = :author)::text))
'''
    res = databases.main_engine.execute(text(sql), author=author, replier=replier)
    for row in res:
        print row[0]

code.interact(local=locals())

########NEW FILE########
__FILENAME__ = message_banning
from r2.models import Account

def ban():
    user = #place name of user here
    banned = list(Account._query(Account.c.name == user))[0]
    banned.messagebanned = True
    banned._commit()

########NEW FILE########
__FILENAME__ = migrate_wiki
#!/usr/bin/env python

import os
import datetime
import optparse
import urllib
import urllib2
import multipartposthandler
from lxml import etree
from pprint import pprint

parser = etree.HTMLParser()

def get_namespaces():
    """Return the list of namespaces.

    Get the Special:AllPages page and extract the full list of
    namespaces from it.

    """

    print 'Getting namespaces'
    tree = etree.parse('http://lesswrong.wikia.com/wiki/Special:AllPages', parser)
    options = tree.xpath('//select[@id="namespace"]/option')
    namespaces = [option.get('value') for option in options]
    pprint(namespaces)
    return namespaces


def get_all_pages_for_namespace(ns):
    """Return the list of wiki pages for the specified namespace.

    Get the Special:AllPages page for the specified namespace and
    extract the list of wiki pages from it.

    """

    print 'Getting pages in namespace %s' % ns
    url = 'http://lesswrong.wikia.com/index.php?title=Special:AllPages&from=&to=&namespace=%s' % ns
    tree = etree.parse(url, parser)
    pages = tree.xpath('//table[2]//a[@title]')
    page_names = [page.get('title') for page in pages]
    pprint(page_names)
    return page_names


def get_export(pages_param, out_filename):
    """Send the POST export request and save the XML export response.

    Send a POST to the export page of the wiki requesting an export of
    the specified pages.  The post specifies that templates should be
    included and also that all revisions should be included.  The
    returned XML export is saved to the specified file.

    """

    url = 'http://lesswrong.wikia.com/index.php?title=Special:Export&action=submit'
    data = urllib.urlencode({'catname': '', 'pages': pages_param, 'templates': '1'})
    feed = urllib2.urlopen(url, data)
    buf = feed.read()

    out = open(out_filename, 'w')
    out.write(buf)

    print 'Export saved %s' % out_filename


def export_at_once(export_dir, page_names):
    """Perform a single export containing all pages."""

    print 'Getting export with full list of pages'
    pages_param = '\n'.join(page_names)
    export_filename = os.path.join(export_dir, 'wiki_export.xml')
    get_export(pages_param, export_filename)

    return [export_filename]


def export_page_at_time(export_dir, page_names):
    """Perform an export for each page."""

    export_filenames = []
    for idx, pages_param in enumerate(page_names):
        print 'Getting export for page %s' % pages_param
        export_filename = os.path.join(export_dir, 'wiki_export%04d.xml' % idx)
        get_export(pages_param, export_filename)
        export_filenames.append(export_filename)

    return export_filenames


def export_wiki(export_dir, at_once):
    """Perform an export of a mediawiki wiki.

    First determine all the pages for all the namespaces.  Second
    request the export of the pages and save the returned xml export
    file.

    According to the doc at http://meta.wikimedia.org/wiki/Help:Export
    there is a max number of revisions that will be returned.  Hence
    it may be necessary to export each page separately.

    """

    namespaces = get_namespaces()

    page_names = []
    for ns in namespaces:
        page_names.extend(get_all_pages_for_namespace(ns))

    print 'Full list of pages:'
    pprint(page_names)

    if at_once:
        export_filenames = export_at_once(export_dir, page_names)
    else:
        export_filenames = export_page_at_time(export_dir, page_names)

    return export_filenames


def setup_opener():
    """Build a url opener that can handle cookies and multipart form data."""

    print 'Building url opener'
    cookie_handler = urllib2.HTTPCookieProcessor()
    multipart_handler = multipartposthandler.MultipartPostHandler()
    opener = urllib2.build_opener(cookie_handler, multipart_handler)
    urllib2.install_opener(opener)


def login(password):
    """Login to the destination wiki as the Admin user.

    This will setup our cookies so that we can do the import as the
    admin user.

    """

    print 'Logging in as the Admin user'
    url = 'http://shank.trikeapps.com/mediawiki/index.php?title=Special:UserLogin&action=submitlogin&type=login'
    data = urllib.urlencode({'wpLoginattempt': 'Log in', 'wpName': 'Admin', 'wpPassword': password})
    feed = urllib2.urlopen(url, data)
    buf = feed.read()
    tree = etree.fromstring(buf, parser)
    nodes = tree.xpath('//a[@title="Log out"]')
    if not nodes:
        raise Exception('Failed to login to destination wiki')


def get_edit_token():
    """Return the edit token that is needed to do an import."""

    print 'Getting edit token'
    url = 'http://shank.trikeapps.com/mediawiki/index.php?title=Special:Import'
    feed = urllib2.urlopen(url)
    buf = feed.read()
    tree = etree.fromstring(buf, parser)
    nodes = tree.xpath('//input[@name="editToken"]')
    if not nodes or 'value' not in nodes[0].attrib:
        raise Exception('Failed to get edit token needed for importing')
    token = nodes[0].get('value')
    return token


def do_import(export_filename, token):
    """Send the POST import request with the file to be imported."""

    print 'Importing %s' % export_filename
    url = 'http://shank.trikeapps.com/mediawiki/index.php?title=Special:Import&action=submit'
    export_file = open(export_filename, 'rb')
    data = {'source': 'upload', 'log-comment': 'migrate_wiki.py script', 'xmlimport': export_file, 'editToken': token }
    feed = urllib2.urlopen(url, data)
    buf = feed.read()
    tree = etree.fromstring(buf, parser)
    nodes = tree.xpath('//div[@id="bodyContent"]/p[2]')
    if not nodes or not nodes[0].text.startswith('Import finished!'):
        raise Exception('Failed to upload file, perhaps export file exceeds max size, try without the --at-once option')


def import_wiki(export_filenames, password):
    """Import the specified export files into the destination wiki."""

    setup_opener()

    login(password)

    token = get_edit_token()

    for export_filename in export_filenames:
        do_import(export_filename, token)


def main():
    """Migrate a mediawiki wiki."""

    opt_parser = optparse.OptionParser()
    opt_parser.add_option('-p', '--password',
                          dest='password',
                          help='destination wiki admin password (REQUIRED)')
    opt_parser.add_option('-a', '--at-once',
                          action='store_true', default=False,
                          help='get single export file in one go, otherwise get export file for each wiki page')
    options, args = opt_parser.parse_args()

    if options.password:
        export_dir = os.path.join(os.getcwd(), 'wiki_export_%s' % datetime.datetime.now().strftime('%Y%m%d%H%M%S'))
        os.makedirs(export_dir)

        export_filenames = export_wiki(export_dir, options.at_once)
        import_wiki(export_filenames, options.password)
    else:
        opt_parser.print_help()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = movabletype2yaml
#!/usr/bin/env python

# Copyright 2008 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os.path
import logging
import re
import sys
import time


import yaml

# Derived from code at:
# http://code.google.com/p/google-blog-converters-appengine/
__author__ = 'Wesley Moore'

########################
# Constants
########################

CATEGORY_NS = 'http://www.blogger.com/atom/ns#'
CATEGORY_KIND = 'http://schemas.google.com/g/2005#kind'
POST_KIND = 'http://schemas.google.com/blogger/2008/kind#post'
COMMENT_KIND = 'http://schemas.google.com/blogger/2008/kind#comment'
ATOM_TYPE = 'application/atom+xml'
HTML_TYPE = 'text/html'
ATOM_THREADING_NS = 'http://purl.org/syndication/thread/1.0'
DUMMY_URI = 'http://www.blogger.com/'


###########################
# Translation class
###########################

class MovableType2Yaml(object):
  """Performs the translation of MovableType text export to YAML
  """

  def __init__(self):
    self.next_id = 1
    self.user_map = set()
  
  def Translate(self, infile, outfile, mapfile):
    """Performs the actual translation to YAML.

    Args:
      infile: The input MovableType export file
      outfile: The output file that should receive the translated document
    """
    # Create the top-level feed object
    feed = []
    comments = []

    # Calculate the last updated time by inspecting all of the posts
    last_updated = 0

    # These three variables keep the state as we parse the file
    post_entry = {}    # The current post atom.Entry to populate
    comment_entry = {} # The current comment atom.Entry to populate
    last_entry = None    # The previous post atom.Entry if exists
    tag_name = None      # The current name of multi-line values
    tag_contents = ''    # The contents of multi-line values

    # Loop through the text lines looking for key/value pairs
    split_re = re.compile('^[A-Z ]+:')
    for line in infile:

      # Remove whitespace
      line = line.strip()

      # Check for the post ending token
      if line == '-' * 8 and tag_name != 'BODY':
        if post_entry:
          # Add the post to our feed
          sys.stderr.write("Adding post %s\n" % post_entry['title'])
          self.add_to_user_map(post_entry.get('author'), post_entry.get('authorEmail'))
          feed.insert(0, post_entry)
          last_entry = post_entry

        # Reset the state variables
        post_entry = {}
        comment_entry = {}
        tag_name = None
        tag_contents = ''
        continue

      # Check for the tag ending separator
      elif line == '-' * 5:
        # Get the contents of the body and set the entry contents
        if tag_name == 'BODY':
          post_entry['description'] = self._Encode(tag_contents)

        # This is the start of the COMMENT section.  Create a new entry for
        # the comment and add a link to the original post.
        elif tag_name == 'COMMENT':
          comment_entry['body'] = self._Encode(tag_contents)
          post_entry.setdefault('comments', []).append(comment_entry)
          self.add_to_user_map(comment_entry.get('author'), comment_entry.get('authorEmail'))
          comment_entry = {}

        # Get the contents of the extended body
        elif tag_name == 'EXTENDED BODY':
          if post_entry:
            post_entry['mt_text_more'] = self._Encode(tag_contents)
          elif last_entry:
            last_entry['mt_text_more'] = self._Encode(tag_contents)

        # Convert any keywords (comma separated values) into Blogger labels
        elif tag_name == 'KEYWORDS':
          post_entry['mt_keywords'] = tag_contents

        # Reset the current tag and its contents
        tag_name = None
        tag_contents = ''
        continue

      # Split the line into key/value pairs
      key = line
      value = ''
      if split_re.match(line):
        elems = line.split(':')
        key = elems[0]
        if len(elems) > 1:
          value = ':'.join(elems[1:]).strip()

      # The author key indicates the start of a post as well as the author of
      # the post entry or comment
      if key == 'AUTHOR':
        # Create a new entry 
        entry = {}

        # Add the author's name
        author_name = self._Encode(value)
        if not author_name:
          author_name = 'Anonymous'
        entry['author'] = author_name

        # Add the appropriate kind, either a post or a comment
        if tag_name == 'COMMENT':
          entry['postid'] = post_entry['postid']
          comment_entry = entry
        else:
          entry['postid'] = 'post-' + self._GetNextId()
          post_entry = entry

      # The title only applies to new posts
      elif key == 'TITLE' and tag_name != 'PING':
        post_entry['title'] = self._Encode(value)

      # If the status is a draft, mark it as so in the entry.  If the status
      # is 'Published' there's nothing to do here
      elif key == 'STATUS':
        post_entry['status'] = value

      # Turn categories into labels
      elif key == 'CATEGORY':
        post_entry.setdefault('category', []).append(value)

      # Convert the date and specify it as the published/updated time
      elif key == 'DATE' and tag_name != 'PING':
        entry = post_entry
        if tag_name == 'COMMENT':
          entry = comment_entry
        entry['dateCreated'] = value

        # Check to see if this was the last post published (so far)
        # seconds = time.mktime(time_val)
        # last_updated = max(seconds, last_updated)

      # Convert all tags into Blogger labels
      elif key == 'TAGS':
        post_entry.setdefault('tags', []).append(value)

      # Update the author's email if it is present and not empty
      elif tag_name == 'COMMENT' and key == 'EMAIL':
        comment_entry['authorEmail'] = value

      # Update the author's URI if it is present and not empty
      elif tag_name == 'COMMENT' and key == 'URL':
        comment_entry['authorUrl'] = value

      # If any of these keys are used, they contain information beyond this key
      # on following lines
      elif key in ('COMMENT', 'BODY', 'EXTENDED BODY', 'EXCERPT', 'KEYWORDS', 'PING'):
        tag_name = key

      # These lines can be safely ignored
      elif key in ('BASENAME', 'ALLOW COMMENTS', 'CONVERT BREAKS', 
                   'ALLOW PINGS', 'PRIMARY CATEGORY', 'IP', 'URL', 'EMAIL'):
        continue

      # If the line is empty and we're processing the body, add a line break
      elif (tag_name == 'BODY' or tag_name == 'EXTENDED BODY' or tag_name == 'COMMENT') and len(line) == 0:
        tag_contents += '\n'

      # This would be a line of content beyond a key/value pair
      elif len(key) != 0:
        tag_contents += line + '\n'


    # Update the feed with the last updated time
    # feed.updated = atom.Updated(self._ToBlogTime(time.gmtime(last_updated)))

    # Serialize the feed object
    yaml.dump(feed, outfile, Dumper=yaml.CDumper)
    
    # Write out the user map
    user_map_dict = {}
    for name, email in self.user_map:
      user_map_dict[name] = email
    yaml.dump(user_map_dict, mapfile, Dumper=yaml.CDumper)

  def _GetNextId(self):
    """Returns the next entry identifier as a string."""
    ret = self.next_id
    self.next_id += 1
    return str(self.next_id)

  def _CreateSnippet(self, content):
    """Creates a snippet of content.  The maximum size being 53 characters,
    50 characters of data followed by elipses.
    """
    content = re.sub('</?[^>/]+/?>', '', content)
    if len(content) < 50:
      return content
    return content[0:49] + '...'

  def _TranslateContents(self, content):
    #content = content.replace('\n', '<br/>')
    return self._Encode(content)

  def _Encode(self, content):
    return content.decode('utf-8', 'replace').encode('utf-8')

  def _FromMtTime(self, mt_time):
    return time.strptime(mt_time, "%m/%d/%Y %I:%M:%S %p")

  def _ToBlogTime(self, time_tuple):
    """Converts a time struct to a Blogger time/date string."""
    return time.strftime('%Y-%m-%dT%H:%M:%SZ', time_tuple)

  def add_to_user_map(self, name, email):
    if email:
      user_mapping = (name, email)
      self.user_map.add(user_mapping)

if __name__ == '__main__':
  if len(sys.argv) <= 3:
    print 'Usage: %s <movabletype_export_file> <outputfile.yml> <user_map.yml>' % os.path.basename(sys.argv[0])
    print
    print ' Outputs the converted file to <outputfile> and <user_map>.'
    sys.exit(-1)
    
  mt_file = open(sys.argv[1])
  outfile = open(sys.argv[2], 'w')
  mapfile = open(sys.argv[3], 'w')
  translator = MovableType2Yaml()
  translator.Translate(mt_file, outfile, mapfile)
  mapfile.close()
  outfile.close()
  mt_file.close()

    

########NEW FILE########
__FILENAME__ = multipartposthandler
#!/usr/bin/python

####
# 02/2006 Will Holcomb <wholcomb@gmail.com>
# 
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
# 
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
"""
Usage:
  Enables the use of multipart/form-data for posting forms

Inspirations:
  Upload files in python:
    http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/146306
  urllib2_file:
    Fabien Seisen: <fabien@seisen.org>

Example:
  import MultipartPostHandler, urllib2, cookielib

  cookies = cookielib.CookieJar()
  opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookies),
                                MultipartPostHandler.MultipartPostHandler)
  params = { "username" : "bob", "password" : "riviera",
             "file" : open("filename", "rb") }
  opener.open("http://wwww.bobsite.com/upload/", params)

Further Example:
  The main function of this file is a sample which downloads a page and
  then uploads it to the W3C validator.
"""

import urllib
import urllib2
import mimetools, mimetypes
import os, stat

class Callable:
    def __init__(self, anycallable):
        self.__call__ = anycallable

# Controls how sequences are uncoded. If true, elements may be given multiple values by
#  assigning a sequence.
doseq = 1

class MultipartPostHandler(urllib2.BaseHandler):
    handler_order = urllib2.HTTPHandler.handler_order - 10 # needs to run first

    def http_request(self, request):
        data = request.get_data()
        if data is not None and type(data) != str:
            v_files = []
            v_vars = []
            try:
                 for(key, value) in data.items():
                     if type(value) == file:
                         v_files.append((key, value))
                     else:
                         v_vars.append((key, value))
            except TypeError:
                systype, value, traceback = sys.exc_info()
                raise TypeError, "not a valid non-string sequence or mapping object", traceback

            if len(v_files) == 0:
                data = urllib.urlencode(v_vars, doseq)
            else:
                boundary, data = self.multipart_encode(v_vars, v_files)
                contenttype = 'multipart/form-data; boundary=%s' % boundary
                if(request.has_header('Content-Type')
                   and request.get_header('Content-Type').find('multipart/form-data') != 0):
                    print "Replacing %s with %s" % (request.get_header('content-type'), 'multipart/form-data')
                request.add_unredirected_header('Content-Type', contenttype)

            request.add_data(data)
        return request

    def multipart_encode(vars, files, boundary = None, buffer = None):
        if boundary is None:
            boundary = mimetools.choose_boundary()
        if buffer is None:
            buffer = ''
        for(key, value) in vars:
            buffer += '--%s\r\n' % boundary
            buffer += 'Content-Disposition: form-data; name="%s"' % key
            buffer += '\r\n\r\n' + value + '\r\n'
        for(key, fd) in files:
            file_size = os.fstat(fd.fileno())[stat.ST_SIZE]
            filename = os.path.basename(fd.name)
            contenttype = mimetypes.guess_type(filename)[0] or 'application/octet-stream'
            buffer += '--%s\r\n' % boundary
            buffer += 'Content-Disposition: form-data; name="%s"; filename="%s"\r\n' % (key, filename)
            buffer += 'Content-Type: %s\r\n' % contenttype
            # buffer += 'Content-Length: %s\r\n' % file_size
            fd.seek(0)
            buffer += '\r\n' + fd.read() + '\r\n'
        buffer += '--%s--\r\n\r\n' % boundary
        return boundary, buffer
    multipart_encode = Callable(multipart_encode)

    https_request = http_request

def main():
    import tempfile, sys

    validatorURL = "http://validator.w3.org/check"
    opener = urllib2.build_opener(MultipartPostHandler)

    def validateFile(url):
        temp = tempfile.mkstemp(suffix=".html")
        os.write(temp[0], opener.open(url).read())
        params = { "ss" : "0",            # show source
                   "doctype" : "Inline",
                   "uploaded_file" : open(temp[1], "rb") }
        print opener.open(validatorURL, params).read()
        os.remove(temp[1])

    if len(sys.argv[1:]) > 0:
        for arg in sys.argv[1:]:
            validateFile(arg)
    else:
        validateFile("http://www.google.com")

if __name__=="__main__":
    main()

########NEW FILE########
__FILENAME__ = ob_export_remove_invalid_dashes
#!/usr/bin/env python

import sys
import re

def main():
    infilename, outfilename = sys.argv[1:3]

    infile = open(infilename)
    buf = infile.read()
    buf = buf.decode('utf-8')

    re_invalid_dashes = re.compile(ur'^-----(---)?(\r)?\n(?!--------|BODY:|EXTENDED BODY:|EXCERPT:|KEYWORDS:|AUTHOR:|COMMENT:|PING:|\Z)', re.MULTILINE)
    buf = re_invalid_dashes.sub(ur'----\n', buf)

    re_control_chars = re.compile(ur'(\s)[]')
    buf = re_control_chars.sub(ur'\1', buf)
    re_control_chars = re.compile(ur'[](\s)')
    buf = re_control_chars.sub(ur'\1', buf)
    re_control_chars = re.compile(ur'[]')
    buf = re_control_chars.sub(ur' ', buf)

    buf = buf.encode('utf-8')
    outfile = open(outfilename, 'w')
    outfile.write(buf)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = ob_import_run
import yaml
from r2.models import Subreddit
from r2.lib.importer import Importer
import pylons

def import_posts(input_filename, rewrite_filename, sr_name):
    pylons.c.default_sr = True
    sr = Subreddit._by_name(sr_name)

    input_file = open(input_filename)
    rewrite_file = open(rewrite_filename, 'w')

    data = yaml.load(input_file, Loader=yaml.CLoader)

    importer = Importer()
    importer.import_into_subreddit(sr, data, rewrite_file)

########NEW FILE########
__FILENAME__ = posts_with_divs
import sys
import sqlalchemy as sa
from r2.lib.db import tdb_sql as tdb
from r2.models import Link
from r2.lib.db.thing import NotFound

def max_link_id():
    thing_type = tdb.types_id[Link._type_id]
    thing_tbl = thing_type.thing_table

    q = sa.select([sa.func.count(thing_tbl.c.thing_id)],
        thing_tbl.c.deleted == False
    )

    rows = q.execute().fetchall()
    return rows[0][0]

def posts_with_divs():
    link_count = max_link_id()
    print >>sys.stderr, "# %d links to process" % link_count
    for link_id in xrange(link_count):
        try:
            link = Link._byID(link_id, data=True)
        except NotFound:
            continue
    
        if hasattr(link, 'ob_permalink'):
            article = link.article
            if isinstance(article, str):
                try:
                    article = article.decode('utf-8')
                except UnicodeDecodeError:
                    print >>sys.stderr, "UnicodeDecodeError, using 'ignore' error mode, link: %d" % link._id
                    article = article.decode('utf-8', errors='ignore')
                
            if '<div' in article:
                print >>sys.stderr, link.canonical_url.encode('utf-8')

########NEW FILE########
__FILENAME__ = post_tools
from r2.models import Account, Link, Subreddit, Vote

def create_about_post():
    user = Account._by_name('Eliezer_Yudkowsky')
    sr = Subreddit._by_name('admin')
    link = Link._submit('About LessWrong', 'TBC', user, sr, '::1', [])

def fix_about_post():
    user = Account._by_name('Eliezer_Yudkowsky')
    l = Link._byID(1, data=True)
    # l = Link._byID(int('1i', 36))
    if l.url.lower() == 'self':
        l.url = l.make_permalink_slow()
        l.is_self = True
        l._commit()
        l.set_url_cache()
    v = Vote.vote(user, l, True, l.ip, False)

def disable_comments_on_post(id36):
    # l = Link._byID(int('10', 36))
    l = Link._byID(int(id36,36), data=True)
    l.comments_enabled = False
    l._commit()


########NEW FILE########
__FILENAME__ = recalc_karma
#!/usr/bin/env python

"""
A script for a one-time migration, from keeping track of karma in terms of
totals, to keeping track of up- and down-votes independently. It stores a SQLite
database with its progress in the current directory (independent from the actual
site database), and can be killed anytime and will resume its progress from
where it left off.
"""


from collections import namedtuple
from datetime import datetime
import os

import sqlalchemy as sa

from r2.lib.db import tdb_sql
from r2.models import Account, Comment, KarmaAdjustment, Link, Subreddit, Vote


########################### DATABASE ###################################

HERE = os.getcwd()  #os.path.dirname(__file__)
TEMP_DB_FILE = os.path.join(HERE, 'karma.db')

Kind = namedtuple('Kind', 'id desc')
kind_ids = {1: Kind(1, 'link'), 2: Kind(2, 'comment'), 3: Kind(3, 'adjustment')}
kinds = dict((k.desc, k) for k in kind_ids.values())

metadata = sa.MetaData()
kvstore = sa.Table('kvstore', metadata,
    sa.Column('id',         sa.Integer, primary_key=True),
    sa.Column('name',       sa.VARCHAR(255), nullable=False, unique=True),
    sa.Column('value',      sa.VARCHAR(255), nullable=False),
)
karmatotals = sa.Table('karmatotals', metadata,
    sa.Column('id',         sa.Integer, primary_key=True),
    sa.Column('account_id', sa.Integer, nullable=False),
    sa.Column('sr_id',      sa.Integer, nullable=False),
    sa.Column('kind',       sa.Integer, nullable=False),
    sa.Column('direction',  sa.BOOLEAN, nullable=False),
    sa.Column('amount',     sa.Integer, nullable=False),
)
sa.Index('idx_kt_acct_sr_kind_dir',
    karmatotals.c.account_id, karmatotals.c.sr_id,
    karmatotals.c.kind, karmatotals.c.direction,
    unique=True)


############################### CODE #################################

def main():
    KarmaCalc().run()


class KarmaCalc(object):
    def __init__(self):
        self.state = MigrateState()
        self.subreds_by_id = {}

    def run(self):
        self.read_votes(Link, 'link', 'vote_link')
        self.read_votes(Comment, 'comment', 'vote_comment')
        self.migrate_scan_adjustments()
        self.state.commit()

        self.write_karmas()
        self.state.commit()

        print('Terminus with success!')

    def migrate_scan_adjustments(self):
        # These should hopefully all fit in memory at once because this feature
        # is relatively new, but dividing up the work is still necessary in
        # order to run this script more than once.

        STEP = 100
        max_id = self.max_thing_id(KarmaAdjustment)
        id_start = int(self.state.kvstore.get('karmaadjustment.cur_read_id', '0'))

        print('Scanning {0}. Max id is {1}, starting at {2}'.format(
            'adjustments', max_id, id_start))

        for id_low in xrange(id_start, max_id + 1, STEP):
            adjs = list(KarmaAdjustment._query(
                KarmaAdjustment.c._id >= id_low,
                KarmaAdjustment.c._id < id_low + STEP, data=True))
            print('{0}: {1}, {2} of {3}'.format(
                datetime.now().isoformat(' '), 'adjustments', id_low, max_id))

            for adj in adjs:
                # adj.amount can be either positive or negative
                self.state.tally_karma(adj.account_id, adj.sr_id, 'adjustment', adj.amount)

            if adjs:
                max_id = max(a._id for a in adjs)
                self.state.kvstore['karmaadjustment.cur_read_id'] = str(max_id + 1)
                self.state.commit()

    def read_votes(self, cls2, karma_kind, kv_namespace):
        STEP = 100
        rel = Vote.rel(Account, cls2)
        max_id = self.max_rel_type_id(rel)
        id_start = int(self.state.kvstore.get(kv_namespace + '.cur_read_id', '0'))

        print('Scanning {0}. Highest vote id is {1}; starting at {2}'.format(
            rel._type_name, max_id, id_start))

        for id_low in xrange(id_start, max_id + 1, STEP):
            votes = list(self.query_rel_id_range(rel, id_low, id_low + STEP))
            print('{0}: {1}, {2} of {3}'.format(
                datetime.now().isoformat(' '), rel._type_name, id_low, max_id))

            for vote in votes:
                thing = cls2._byID(vote._thing2_id, data=True)
                amt = int(vote._name)  # can be either positive or negative
                self.state.tally_karma(thing.author_id, thing.sr_id, karma_kind, amt)

            if votes:
                max_id = max(v._id for v in votes)
                self.state.kvstore[kv_namespace + '.cur_read_id'] = str(max_id + 1)
                self.state.commit()

        print('Done with {0}!'.format(rel._type_name))

    def query_rel_id_range(self, rel, id_low, id_high):
        return rel._query(rel.c._rel_id >= id_low, rel.c._rel_id < id_high,
                          eager_load=True)

    def max_thing_id(self, thing_type):
        thing_type = tdb_sql.types_id[thing_type._type_id]
        thing_tbl = thing_type.thing_table
        return sa.select([sa.func.max(thing_tbl.c.thing_id)]).execute().scalar()

    def max_rel_type_id(self, rel_thing):
        thing_type = tdb_sql.rel_types_id[rel_thing._type_id]
        thing_tbl = thing_type.rel_table[0]
        return sa.select([sa.func.max(thing_tbl.c.rel_id)]).execute().scalar()


    def write_karmas(self):
        STEP = 100
        account_id_max = sa.select([sa.func.max(karmatotals.c.account_id)]).scalar()
        account_id_start = 0  #int(self.state.kvstore.get('karma.cur_write_account_id', '0'))

        print('Writing karma keys, starting at account {0}, max account id is {1}'.format(
            account_id_start, account_id_max))

        for account_id_low in xrange(account_id_start, account_id_max + 1, STEP):
            accounts = list(Account._query(
                Account.c._id >= account_id_low,
                Account.c._id < account_id_low + STEP))
            accounts = dict((a._id, a) for a in accounts)
            karmas = karmatotals.select(
                sa.and_(karmatotals.c.account_id >= account_id_low,
                        karmatotals.c.account_id < account_id_low + STEP)).execute().fetchall()

            print('{0}: writing karmas, {1} of {2} accounts'.format(
                datetime.now().isoformat(' '), account_id_low, account_id_max))

            for k in karmas:
                account = accounts.get(k['account_id'])
                if account is not None:
                    key = self.make_karma_key(k)
                    setattr(account, key, k['amount'])

            for ac in accounts.values():
                ac._commit()
            #self.state.kvstore['karma.cur_write_account_id'] = str(account_id_low + STEP)
            #self.state.commit()

    def get_sr_by_id(self, sr_id):
        sr = self.subreds_by_id.get(sr_id)
        if sr is None:
            sr = self.subreds_by_id[sr_id] = Subreddit._byID(sr_id, data=True)
        return sr

    def make_karma_key(self, karma):
        return 'karma_{0}_{1}_{2}'.format(
            ('downs', 'ups')[karma['direction']],
            kind_ids[karma['kind']].desc,
            self.get_sr_by_id(karma.sr_id).name)


class MigrateState(object):
    def __init__(self):
        metadata.bind = sa.create_engine('sqlite:///' + TEMP_DB_FILE).connect()
        self.bind = metadata.bind
        metadata.create_all()
        names_values = [kvstore.c.name, kvstore.c.value]
        self.kvstore = dict(self.bind.execute(sa.select(names_values)).fetchall())

        self.trans = self.bind.begin()

    def tally_karma(self, account_id, sr_id, kind, amount):
        # The total which is increased will be 'ups' if amount > 0, otherwise 'downs'
        upsert(self.bind, karmatotals, {
            'account_id': account_id,
            'sr_id': sr_id,
            'kind': kinds[kind].id,
            'direction': amount >= 0,
        }, {
            'amount': karmatotals.c.amount + abs(amount),
        }, {
            'amount': abs(amount),
        })

    def commit(self):
        for k, v in self.kvstore.iteritems():
            upsert(self.bind, kvstore, {'name': k}, {'value': v})
        self.trans.commit()
        self.trans = self.bind.begin()


def upsert(bind, table, pivots, values_update, values_insert=None):
    where = sa.and_(*[table.columns[k] == v for k, v in pivots.items()])
    row = bind.execute(table.select(where)).fetchone()
    if row:
        where = sa.and_(*[c == row[c] for c in table.primary_key])
        bind.execute(table.update(where, values_update))
    else:
        if values_insert is None:
            values_insert = values_update
        values_insert.update(pivots)
        bind.execute(table.insert(values_insert))


main()

########NEW FILE########
__FILENAME__ = run_pending_jobs
#!/usr/bin/env python

"""
This script is run periodically by cron. It scans through a database table of
pending jobs and executes them. A memcache lock is used to ensure that each job
is claimed by only one node. If an exception occurs while running a job, an
error will be logged and the job will remain in the queue to be attempted next
time this script is run.
"""
import re, traceback, urllib2

from sys import stderr
from datetime import datetime, timedelta

from pylons import g

from r2.lib.lock import MemcacheLock
from r2.lib.rancode import random_key
from r2.lib import notify, emailer
from r2.lib.db.thing import NotFound
from r2.models import Account, Meetup, PendingJob

from lxml import etree


class JobProcessor:
    def run(self):
        jobs = PendingJob._query(data=True)

        for job in jobs:
            self.process_job(job)

    def process_job(self, job):
        if job.run_at is not None and job.run_at > datetime.now(g.tz):
            return

        runner = globals().get('job_' + job.action)
        if not runner:
            print >>stderr, 'Unknown job action {0!r}'.format(job.action)
            return

        # If we can't acquire the lock, the job has already been claimed,
        # so we skip it.
        lock = g.make_lock('pending_job_{0}'.format(job._id))
        if not lock.try_acquire():
            return

        try:
            data = job.data or {}
            runner(**data)
        except Exception as ex:
            print >>stderr, 'Exception while running job id {0} ({1}): {2}'.format(
                job._id, job.action, ex)
        else:
            self.mark_as_completed(job)
        finally:
            lock.release()

    def mark_as_completed(self, job):
        job._delete_from_db()


def job_process_new_meetup(meetup_id):
    # Find all users near the meetup who opted to be notified, and add a child
    # job for each of them, so that the scope of any problems is limited.
    # These child jobs will run on the next run-through of this script.
    meetup = Meetup._byID(meetup_id, data=True)
    users = notify.get_users_to_notify_for_meetup(meetup.coords)
    for user in users:
        data = {'username': user.name, 'meetup_id': meetup._id}
        PendingJob.store(None, 'send_meetup_email_to_user', data)


def job_send_meetup_email_to_user(meetup_id, username):
    meetup = Meetup._byID(meetup_id, data=True)
    try:
      user = Account._by_name(username)
      notify.email_user_about_meetup(user, meetup)
    except NotFound:
      # Users can get deleted so ignore if not found
      pass


try:
    JobProcessor().run()
except Exception as ex:
    print >>stderr, 'Critical failure processing job queue: {0}'.format(ex)

########NEW FILE########
__FILENAME__ = test_fix_bare_links
# -*- coding: utf-8 -*-
from fix_bare_links import rewrite_bare_links

import yaml
import os.path
import unittest

class TestFixImportedImages(unittest.TestCase):
        
    def test_correction(self):
        test_cases = yaml.load(open(os.path.normpath(os.path.join(__file__, '..', "fix_bare_links_test_cases.yml"))), Loader=yaml.CLoader)
        for source, expected_output in test_cases:
            try:
                self.assertEqual(rewrite_bare_links(source), expected_output)
            except TypeError:
                import pprint
                pprint.pprint(source)
                pprint.pprint(expected_output)
                raise
            
if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_fix_imported_content_with_images
# -*- coding: utf-8 -*-
from fix_imported_content_with_images import process_content
import unittest

class TestFixImportedImages(unittest.TestCase):
    sample_content = (
        ('asdf http://robinhanson.typepad.com/.a/6a00d8341c6a2c53ef010536c21d63970b-800wi asdf', 'asdf http://lesswrong.com/static/imported/6a00d8341c6a2c53ef010536c21d63970b-800wi.jpg asdf'),
        ('http://robinhanson.typepad.com/photos/uncategorized/2007/09/19/lindacorrelation.png', 'http://lesswrong.com/static/imported/2007/09/19/lindacorrelation.png'),
        ('http://robinhanson.typepad.com/photos/uncategorized/2008/05/06/bayestheorem.png', 'http://lesswrong.com/static/imported/2008/05/06/bayestheorem.png'),
        ('http://www.overcomingbias.com/images/2007/08/10/monsterwithgirl_2.jpg', 'http://lesswrong.com/static/imported/2007/08/10/monsterwithgirl_2.jpg'),
        ('http://www.overcomingbias.com/images/2008/09/30/zebra_4.jpg', 'http://lesswrong.com/static/imported/2008/09/30/zebra_4.jpg'),
        ('<a href="http://www.overcomingbias.com/images/2008/05/27/jbarbourrelative.png"><img src="http://www.overcomingbias.com/images/2008/05/27/jbarbourrelative.png" /></a>', '<a href="http://lesswrong.com/static/imported/2008/05/27/jbarbourrelative.png"><img src="http://lesswrong.com/static/imported/2008/05/27/jbarbourrelative.png" /></a>'),
        ("Not OB http://www.notovercomingbias.com/images/2008/05/27/jbarbourrelative.png", "Not OB http://www.notovercomingbias.com/images/2008/05/27/jbarbourrelative.png"),
        (u'••• http://www.overcomingbias.com/images/2007/08/10/monsterwithgirl_2.jpg', u'••• http://lesswrong.com/static/imported/2007/08/10/monsterwithgirl_2.jpg'),
        ('http://robinhanson.typepad.com/.shared/image.html?/photos/uncategorized/2008/05/22/mindscaleparochial.png',       'http://lesswrong.com/static/imported/2008/05/22/mindscaleparochial.png'),
        ('http://robinhanson.typepad.com/photos/uncategorized/2008/05/26/schrodinger.gif', 'http://lesswrong.com/static/imported/2008/05/26/schrodinger.gif')
    )
        
    def test_correction(self):
        for source, expected_output in self.sample_content:
            self.assertEqual(process_content(source), expected_output)
            
if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_fix_link_urls
# -*- coding: utf-8 -*-
from fix_link_urls import convert_url, should_convert_url
import unittest

class TestFixLinksUrls(unittest.TestCase):
    inputs = (
        (u'/r/wmoore-drafts/comments/1/continue_editing/', True, '/lw/1/continue_editing/'),
        (u'/a/1l/image_post/', True, '/lw/1l/image_post/'),
        (u'/r/currentaffairs/comments/s/hi_im_new_here/', True, u'/lw/s/hi_im_new_here/'),
        (u'/r/New-User-drafts/comments/t/draft_post/', True, u'/lw/t/draft_post/'),
        (u'/r/selfdeception/comments/p/mixed_case_tags/', True, u'/lw/p/mixed_case_tags/'),
        (u'/r/hypocrisy/comments/16/unknown_tag/', True, u'/lw/16/unknown_tag/'),
        (u'/r/politics/comments/17/an_african_folktale/', True, u'/lw/17/an_african_folktale/'),
        (u'/r/future/comments/1a/ウ/', True, u'/lw/1a/ウ/'),
        (u'/r/New-User-drafts/comments/1b/embeded_flash_player/', True, u'/lw/1b/embeded_flash_player/'),
        (u'/comments/1c/where_do_i_go/', True, u'/lw/1c/where_do_i_go/'),
        (u'/r/wmoore-drafts/comments/h/a_new_article/', True, u'/lw/h/a_new_article/'),
        (u'self', False, u'/lw/self/'),
        (u'/r/meta/comments/1h/about_lesswrong/', True, u'/lw/1h/about_lesswrong/'),
        (u'/r/ads/lw/1t/submit_to_ads/', True, u'/lw/1t/submit_to_ads/'),
        (u'/r/admin/comments/1i/about_lesswrong/', True, u'/lw/1i/about_lesswrong/'),
        (u'/article/1j/htmlinptitlep/', True, u'/lw/1j/htmlinptitlep/'),
        (u'/a/1k/phtmlp/', True, u'/lw/1k/phtmlp/'),
        (u'/comments/x/just_be_glad_its_not_2girls1cup/', True, u'/lw/x/just_be_glad_its_not_2girls1cup/'),
        (u'/lw/1k/arrgos_ijgl_sldfjg_dsfg/', False, u'/lw/1k/arrgos_ijgl_sldfjg_dsfg/'),
        (u'/lw/15/another_relative_url_test/', False, u'/lw/15/another_relative_url_test/'),
        (u'/lw/y/removed_html_in_title_2/', False, u'/lw/y/removed_html_in_title_2/'),
        (u'/article/y/removed_html_in_title_2/', True, u'/lw/y/removed_html_in_title_2/'),
    )
        
    def test_pattern_match(self):
        for url, should_convert, converted_url in self.inputs:
            self.assertEqual(should_convert_url(url), should_convert )
        
    def test_conversion(self):
        for url, should_convert, converted_url in self.inputs:
            self.assertEqual(convert_url(url), converted_url)
            
            
            
if __name__ == '__main__':
    unittest.main()
########NEW FILE########
__FILENAME__ = user_downvote_karma
from r2.models import Account, Vote
from pylons import g

def user_downvote_karma_count(filename):
    users = Account._query(data=True)
    
    f = open(filename, 'w')
    f.write("Username,Karma,Down Votes\n")
    
    for user in users:
        downvote_count = g.cache.get(user.vote_cache_key())
        if downvote_count is None:
            downvote_count = len(list(Vote._query(Vote.c._thing1_id == user._id,
                                                  Vote.c._name == str(-1))))

        f.write("%s,%d,%d\n" % (user.name, user.safe_karma, downvote_count))

    f.close()

########NEW FILE########
__FILENAME__ = user_sort_options
from copy import copy
from r2.models import Account

# Changes a saved default sort order for the top links page from all to quarter
def user_sort_options():
    pref = 'browse_sort'
    users = Account._query(data=True)
    for user in users:
        print user.name,
        user_prefs = copy(user.sort_options)
        user_pref = user_prefs.get(pref)
        if user_pref and user_pref == 'all':
            user_prefs[pref] = 'quarter'
            user.sort_options = user_prefs
            user._commit()
            print " *"
        else:
            print

########NEW FILE########
__FILENAME__ = wiki_account_script
from r2.models import Account, PendingJob
import sqlalchemy as sa

from r2.lib.db import tdb_sql

def max_thing_id(thing_type):
    thing_type = tdb_sql.types_id[thing_type._type_id]
    thing_tbl = thing_type.thing_table
    return sa.select([sa.func.max(thing_tbl.c.thing_id)]).execute().scalar()

def run():

    STEP = 100
    thing = Account
    max_id = max_thing_id(thing)
    id_start = 0

    for id_low in xrange(id_start, max_id + 1, STEP):
        users = list(query_thing_id_range(thing, id_low, id_low + STEP))

        for user in users:
            if not user._loaded:
                user._load()
            if hasattr(user, 'email'):
                data = {'name' : user.name, 'password' : None, 'email' : user.email, 'attempt' : 0}
                PendingJob.store(None, 'create_wiki_account', data)

def query_thing_id_range(thing, id_low, id_high):
    return thing._query(thing.c._id >= id_low, thing.c._id < id_high,
                      eager_load=True)

run()

########NEW FILE########
