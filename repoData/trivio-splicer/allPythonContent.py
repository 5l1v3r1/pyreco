__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------



# Additional templates that should be rendered to pages, maps page names to
# template names.
# the 'redirect_home.html' page redirects using a http meta refresh which, according
# to official sources is more or less equivalent of a 301.

html_additional_pages = {
    #'concepts/introduction': 'redirect_home.html',
    #'builder/basics': 'redirect_build.html',
    }



# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinxcontrib.httpdomain']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

#disable the parmalinks on headers, I find them really annoying
html_add_permalinks = None


# The master toctree document.
master_doc = 'toctree'

# General information about the project.
project = u'Splicer'
copyright = u'2013, triv.io'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
import splicer
version = splicer.__version__
# The full version, including alpha/beta/rc tags.
release = '0'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'flask'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []
html_theme_path = ['../theme']

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.

# We use a png favicon. This is not compatible with internet explorer, but looks
# much better on all other browsers. However, sphynx doesn't like it (it likes
# .ico better) so we have just put it in the template rather than used this setting
# html_favicon = 'favicon.png'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['static_files']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
html_show_sourcelink = False

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'Splicerdoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'Splicer.tex', u'Splicer Documentation',
   u'triv.io', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'splicer', u'Splicer Documentation',
     [u'triv.io'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'Splicer', u'Splicer Documentation',
   u'triv.io', 'Splicer', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

########NEW FILE########
__FILENAME__ = dict_adapter
from splicer import Table
from . import Adapter

class DictAdapter(Adapter):
  """
  An adapter for working with lists of dictionaries.
  """
  def __init__(self, **tables):
    """

    Examples:
    Dictionary(
      users=[dict(),dict(),...],
      other=dict(
        schema=[],
        rows=[dict(),dict(),...]
      )
    )

    """
    self._tables = {}

    for name, table in tables.items():
      if isinstance(table, dict):
        schema = table['schema']
        rows=table['rows']
      else:
        rows = table
        schema = self.guess_schema(rows)

      self._tables[name] = DictTable(
        self,
        name, 
        schema=schema, 
        rows=rows
      )


  @property
  def relations(self):
    return [
      (name, table.schema)
      for name, table in self._tables.items()
    ]


  def has(self, relation):
    return self._tables.has_key(relation)

  def get_relation(self, name):
    return self._tables.get(name)

  def table_scan(self, name, ctx):
    return self._tables[name]



class DictTable(Table):
  def __init__(self, adapter, name, schema, rows):
    super(self.__class__, self).__init__(adapter, name, schema)
    self.key_index = [
      (f.name, () if f.mode == 'REPEATED' else None)
      for f in self.schema.fields
    ]
    self._rows = rows


  def __iter__(self):
    key_index = self.key_index

    return (
      tuple(row.get(key, default) for key, default in key_index)
      for row in self._rows
    )

########NEW FILE########
__FILENAME__ = dir_adapter
from splicer import Schema, Field
from splicer.adapters import Adapter
from splicer.ast import *
from splicer.path import pattern_regex, tokenize_pattern, regex_str, columns


class DirAdapter(Adapter):
  def __init__(self, **relations):
    self._relations = {
      name:FileTable(name, **args) for name,args in relations.items()
    }

  @property
  def relations(self):
    return [
      (name, relation.schema)
      for name, relation in self._relations.items()
    ]

  def get_relation(self, name):
    return self._relations.get(name)

  def table_scan(self, name, ctx):
    return self._relations.get(name)

  def has(self, name):
    return self._relations.has_key(name)

  def evaluate(self, loc):
    relation = self._relations[loc.node().name]

    op = Function(
      'files', 
      Const(relation.root_dir)
    )

    if relation.pattern:
      loc = rewrite_tree(
        relation.path_schema.field_map.keys(),
        loc.replace(
          Function(
            'extract_path', 
            op, 
            Const(relation.root_dir + relation.pattern)
          )
        )
      )

    else:
      loc = loc.relpace(op)
      
    if relation.content_column:
      loc = loc.replace(
        Function('contents', loc.node(), Const(relation.content_column))
      )

    if relation.decode != 'none':
      loc = loc.replace(Function(
        'decode', 
        loc.node(), 
        Const(relation.decode),
        Const('path'),
        Const(relation.schema)
      ))

    return loc.leftmost_descendant()



class FileTable(object):
  def __init__(self,  name, root_dir, **options):
    self.name = name

    if not root_dir.endswith('/'):
      root_dir += '/'
    self.root_dir = root_dir

    self.pattern  = options.pop('pattern', None)

    if self.pattern:
      tokens = tokenize_pattern(self.pattern)
      self.path_schema = Schema([
        Field(name=c, type="STRING")
        for c in columns(tokens)
      ])



    self.content_column = options.pop('content_column', None)
    self.filename_column = options.pop('filename_column', None)
  
    self.decode = options.pop('decode', "none")

    schema = options.pop('schema',None)
    self.schema = schema and Schema(**schema)

    if options:
      raise ValueError("Unrecognized options {}".format(options.keys()))



def rewrite_tree(partioned_fields, query_loc):

  current = query_loc.node()
  loc = query_loc

  changes = []
  while True:
    next_loc = loc.ancestor(lambda l: isinstance(l.node(),SelectionOp))
    if next_loc is None:
      break
    else:
      loc = next_loc

    sel_op = loc.node()
    record_filter, key_filter = rewrite_bool_op(
      partioned_fields,
      sel_op.bool_op
    )

    if key_filter:
      changes.append(key_filter)
      if record_filter:
        loc = loc.replace(sel_op.new(bool_op=record_filter))
      else:
        # remove the sel_op
        loc = loc.replace(loc.down().node())


  loc = loc.find(lambda l: l.node() is current)
  assert loc, "Could not restore location" # should never happen

  if changes:
    bool_op = changes.pop(0)
    while changes:
      bool_op = And(bool_op, changes.pop(0))

    return loc.replace(SelectionOp(current, bool_op))
  else:
    return loc



def rewrite_bool_op(partioned_fields, bin_op):
  bin_type = type(bin_op)
  
  if bin_type == And:
    recordfilter, keyfilter =  rewrite_and_op(partioned_fields, bin_op)
  elif bin_type == Or:
    recordfilter, keyfilter =  rewrite_or_op(partioned_fields, bin_op)
  elif issubclass(bin_type, BinaryOp):
    recordfilter, keyfilter =  rewrite_binary_op(partioned_fields, bin_op)
  else:
    recordfilter = bin_op
    keyfilter = None
  return recordfilter,keyfilter

def is_partitionable(partioned_fields, op):
  """
  Returns true if the operation only uses variables from the key names
  and constants otherwise false
  """

  t = type(op)
  if t == Var:
    return op.path in partioned_fields
  elif t == Function:
    # all function args have to be partitionable
    return all(is_partitionable(partioned_fields, a) for a in op.args)
  elif t == Tuple:
    # all elements must be partitionable
    return all(is_partitionable(partioned_fields, e) for e in op.exprs)
  elif issubclass(t, Const):
    return True
  elif issubclass(t, BinaryOp):
    return all(is_partitionable(partioned_fields, e) for e in (op.lhs, op.rhs))
  else:
    raise RuntimeError("Can't determine if type %s is partitionable." % t)



def rewrite_binary_op(partioned_fields,op):
  if all(is_partitionable(partioned_fields,e) for e in (op.lhs,op.rhs)):
    return None, op
  else:
    return op, None


def rewrite_and_op(partioned_fields,op):
  def combine(exprs):
    l = len(exprs)
    if l == 0:
      return None
    elif l == 1:
      return exprs[0]
    else:
      return And(*exprs)
 
  lhs = rewrite_bool_op(partioned_fields,op.lhs)
  rhs = rewrite_bool_op(partioned_fields,op.rhs)


  return [
    combine(filter(None,exprs))
    for exprs in zip(lhs, rhs)
  ]

def rewrite_or_op(partioned_fields, op):
  rewrite = partial(rewrite_bool_op, partioned_fields)

  record_exprs, key_exprs = [
    filter(None, exprs)
    for exprs in zip(rewrite(op.lhs),rewrite(op.rhs))
  ]

  if len(record_exprs) == 0:
    l = len(key_exprs)
    if l == 1:
      return None, key_exprs[0]
    else:
      # this assumes keyexprs should be exactly 2
      return None, Or(*key_exprs)

  else: # no benifit from using the key filter
    return op, None




########NEW FILE########
__FILENAME__ = aggregate
class Aggregate(object):
  def __init__(self, function, returns, initial=None, finalize=None):
    self.function = function
    self.returns = returns
    self.initial  = initial
    self.finalize = finalize
    self.state = None

  def __call__(self, *args):
    return args
########NEW FILE########
__FILENAME__ = ast
from .immutable import ImmutableMixin

class Expr(ImmutableMixin):
  def __eq__(self, other):

    result = (
      isinstance(other, self.__class__)
      and all(
        getattr(self, attr) == getattr(other, attr)
        for attr in self.__slots__
      )
    )

    return result

class UnaryOp(Expr):
  __slots__ = ('expr',)
  def __init__(self, expr):
    self.expr = expr

class NegOp(UnaryOp):
  """ - (expr)"""
  __slots__ = ('expr',)

class NotOp(UnaryOp):
  """ not (expr)"""
  __slots__ = ('expr',)

class ItemGetterOp(Expr):
  """ (expr)[key] """
  __slots__ = ('key',)
  def __init__(self, key):
    self.key = key
  

class BinaryOp(Expr):
  __slots__ = ('lhs', 'rhs')

  def __init__(self, lhs, rhs):
    self.lhs = lhs
    self.rhs = rhs

class And(BinaryOp):
  """lhs and rhs"""
  __slots__ = ('lhs', 'rhs')

class Or(BinaryOp):
  """lhs or rhs"""
  __slots__ = ('lhs', 'rhs')

class LtOp(BinaryOp):
  """Less than"""
  __slots__ = ('lhs', 'rhs')

class LeOp(BinaryOp):
  """Less than or equal to"""
  __slots__ = ('lhs', 'rhs')

class EqOp(BinaryOp):
  """Equal to"""
  __slots__ = ('lhs', 'rhs')

class NeOp(BinaryOp):
  """Not equal to"""
  __slots__ = ('lhs', 'rhs')

class GeOp(BinaryOp):
  """Greater than or equal to"""
  __slots__ = ('lhs', 'rhs')

class GtOp(BinaryOp):
  """Greater than"""
  __slots__ = ('lhs', 'rhs')

class IsOp(BinaryOp):
  """x is y"""
  __slots__ = ('lhs', 'rhs')

class IsNotOp(BinaryOp):
  """x is not y"""
  __slots__ = ('lhs', 'rhs')


class AddOp(BinaryOp):
  """lhs + rhs"""
  __slots__ = ('lhs', 'rhs')

class SubOp(BinaryOp):
  """lhs - rhs"""
  __slots__ = ('lhs', 'rhs')

class MulOp(BinaryOp):
  """lhs * rhs"""
  __slots__ = ('lhs', 'rhs')

class DivOp(BinaryOp):
  """lhs / rhs"""
  __slots__ = ('lhs', 'rhs')



class Const(Expr):
  __slots__ = ('const',)

  def __init__(self, const):
    self.const = const

class NullConst(Const):
  """Null or None"""
  __slots__ = ()
  const = None
  def __init__(self):
    pass


class NumberConst(Const):
  """Integer or Float"""
  __slots__ = ('const',)

class StringConst(Const):
  """A string"""
  __slots__ = ('const',)


class BoolConst(Const):
  """A boolean const"""
  __slots__ = ()
  def __init__(self):
    pass

class TrueConst(BoolConst):
  """The constant True """
  __slots__ = ()
  const=True

class FalseConst(BoolConst):
  """The constant False """
  __slots__ = ()
  const=False


class Var(Expr):
  __slots__ = ('path',)
  def __init__(self, path):
    self.path = path

class Tuple(Expr):
  __slots__ = ('exprs',)
  def __init__(self, *exprs):
    self.exprs = exprs

class Function(Expr):
  __slots__ = ('name', 'args')
  def __init__(self, name, *args):
    self.name = name
    self.args = args

  

COMPARISON_OPS = {
  '<'  : LtOp,
  '<=' : LeOp,
  '='  : EqOp,
  '!=' : NeOp,
  '>=' : GeOp,
  '>'  : GtOp,
  'is' : IsOp,
  'is not' : IsNotOp
}


MULTIPLICATIVE_OPS ={
  '*'  : MulOp,
  '/'  : DivOp
}

ADDITIVE_OPS = {
  '+'  : AddOp,
  '-'  : SubOp
}

# sql specific expresions

class Asc(UnaryOp):
  """Sort from lowest to highest"""
  __slots__ = ('expr',)
  
class Desc(UnaryOp):
  """Sort from highest to lowest """
  __slots__ = ('expr',)


class ParamGetterOp(UnaryOp):
  """ ?<number> """
  __slots__ = ('expr',)
  

class SelectAllExpr(Expr):
  __slots__ = ('table',)

  def __init__(self, table=None):
    self.table = table

class RenameOp(Expr):
  __slots__ = ('name','expr')

  def __init__(self, name, expr):
    self.name = name
    self.expr  = expr



class RelationalOp(Expr):
  pass

class LoadOp(RelationalOp):
  """Load a relation with the given name"""
  __slots__ = ('name',)
  def __init__(self, name):
    self.name = name

class AliasOp(RelationalOp):
  """Rename the relation to the given name"""
  __slots__ = ('relation', 'name')
  def __init__(self, name, relation):
    self.name = name
    self.relation = relation


class ProjectionOp(RelationalOp):
  __slots__ = ('relation', 'exprs')
  def __init__(self, relation, *exprs):
    self.relation = relation
    self.exprs = exprs

class SelectionOp(RelationalOp):
  __slots__ = ('relation','bool_op',)
  def __init__(self, relation, bool_op):
    self.relation = relation
    self.bool_op = bool_op

class JoinOp(RelationalOp):
  __slots__ = ('left','right', 'bool_op')
  def __init__(self,  left, right, bool_op = TrueConst()):
    self.left = left
    self.right = right
    self.bool_op = bool_op


class OrderByOp(RelationalOp):
  __slots__ = ('relation', 'exprs')
  def __init__(self, relation, first, *exprs):
    self.relation = relation
    self.exprs = (first,) + exprs

  def new(self, **parts):
    # OrderByOp's __init__ doesn't match what's defined in __slots__
    # so we have to help it make a copy of this object
    exprs = parts.get('exprs', self.exprs)
    first = exprs[0]
    tail = exprs[1:]
    relation = parts.get('relation', self.relation)

    return self.__class__(relation, first, *tail)


class GroupByOp(RelationalOp):
  __slots__ = ('relation','aggregates','exprs',)
  def __init__(self, relation,  *exprs, **kw):
    self.relation   = relation
    self.exprs      = exprs

    # bit of a kludge, aggregates can't be resolved at
    # parse time, so they start as an empty list and
    # are set when the expression tree is evaluated.
    # See compilers.local.projection_op for details
    self.aggregates = kw.get('aggregates', ())

class SliceOp(RelationalOp):
  __slots__ = ('relation','start','stop')
  def __init__(self, relation, *args):
    self.relation = relation
    if len(args) == 1:
      self.start = 0
      self.stop = args[0]
    else:
      self.start, self.stop = args

  def new(self, **parts):
    # slice op's __init__ doesn't match what's defined in __slots__
    # so we have to help it make a copy of this object
    args = parts.get('start', self.start), parts.get('stop', self.stop)
    relation = parts.get('relation', self.relation)

    return self.__class__(relation, *args)

    

########NEW FILE########
__FILENAME__ = csv
from __future__ import absolute_import

import csv
from itertools import chain

from ..schema import Schema
from ..relation import Relation
from . import decodes


@decodes('text/csv')
def csv_decoder(stream):
  sample = stream.read(1024 ** 2)
  stream.seek(0)
  sniffer = csv.Sniffer()


  try:
    dialect = sniffer.sniff(sample)
    has_header = sniffer.has_header(sample)
  except csv.Error:
    # sniffer has problems detecting single
    # column csv files
    has_header = True
    dialect = csv.excel

  reader = csv.reader(stream, dialect)

  first_row = next(reader)  
  if has_header:
    headers = first_row
  else:
    headers = tuple("column_%s" % col for col  in range(len(first_row)) )
    reader = chain([first_row], reader)
  
  schema = Schema([
    dict(name=name, type="STRING")
    for name in headers
  ])

  return Relation(
    schema,
    reader
   )
########NEW FILE########
__FILENAME__ = join
from collections import defaultdict
from functools import partial
from sys import getsizeof

from ..ast import EqOp, And, Var
from .local import var_expr

B=1
K=1024
M=K**2
MAX_SIZE=10*M

def record_size(record):
  # size of outer tuple
  sz = getsizeof(record)

  # size of the individual element
  # note repeating attribites aren't summed yet. To
  # do that it would be best to pass in the schema and
  # compile an efficient summer

  return sz + sum(map(getsizeof, record))

def buffered(relation, max_size):
  block = []
  bytes = 0
  for row in relation:
    block.append(row)
    bytes += record_size(row)
    if bytes >= max_size:
      yield block
      block = []
      bytes = 0

  if block:
    yield block




def nested_block_join(r_op,s_op, comparison, ctx):
  buffer_size = ctx.get('sort_buffer_size', MAX_SIZE) / 2
  r = r_op(ctx)

  for r_block in buffered(r, buffer_size):
    s = s_op(ctx)
    for s_block in buffered(s, buffer_size):
      for s_row in s_block:
        for r_row in r_block:
          row = r_row + s_row
          if comparison(row, ctx):
            yield row

def hash_join(r_op,s_op, comparison, ctx):
  r = r_op(ctx)
  buffer_size = ctx.get('sort_buffer_size', MAX_SIZE) / 2

  left_key, right_key = comparison

  for r_block in buffered(r, buffer_size):
    probe = defaultdict(list)
    for row in r_block:
      probe[left_key(row,ctx)].append(row)

    for s_block in buffered(s_op(ctx), buffer_size):
      for s_row in s_block:
        for match in probe.get(right_key(s_row,ctx), ()):
          yield match + s_row

def join_keys(left, right, op):
  """
  Given two relations that need to be joined and
  a expression, returns two functions for extracting
  keys suitable for equi joins from the relations


  """

  def key_func(funcs, row, ctx):
    return tuple(
      f(row, ctx)
      for f in funcs
    )

  l_key, r_key =  zip(*join_keys_expr(left,right,op))

  return partial(key_func, l_key), partial(key_func, r_key) 

    

def join_keys_expr(left, right, op):
  """
  Rerturn the function for extracting the key values from the expresion
  or  (None,None) if the operation is not an EqOp with two vars.

  All of thes following expresions

  left.x = right.y
  right.y = left.x
  x = y
  y = x
  left.x = y
  y = left.x
  right.y = x
  x = right.y

  Are equivalent and should return a list of two functions
  the first function will extract x from a row in the left relation
  and extract y from a row in the right relation.
  """

  if isinstance(op, And):
    return (
      join_keys_expr(left, right, op.lhs) 
      + join_keys_expr(left, right, op.rhs) 
    )

  if not isinstance(op, EqOp):
    raise ValueError("Expression is not equijoinable")

  if not (isinstance(op.lhs, Var) and isinstance(op.rhs, Var)):
    # the query rewritter should push expresions that don't
    # involve both sides of the relation down the tree
    raise ValueError("Expression is not equijoinable")


  cols = [None, None]
  for var in (op.lhs, op.rhs):
    parts = var.path.split('.')
    if len(parts) == 1:
      if left.schema.field_map.get(parts[0]):
        cols[0] = var_expr(var, left, None)
      elif right.schema.field_map.get(parts[0]):
        cols[1] =  var_expr(var, right, None)
      else:
        raise ValueError('column "{}" does not exist'.format(var.path))
    else:
      relation_name = parts[0]
      if left.schema.name == relation_name:
        cols[0] = var_expr(Var(parts[1]), left, None)
      elif right.schema.name == relation_name:
        cols[1] = var_expr(Var(parts[1]), right, None)
      else: 
        raise ValueError('relation "{}" does not exist'.format(parts[0]))

  return (cols,)




########NEW FILE########
__FILENAME__ = local
import operator
from functools import partial
from itertools import islice

from ..ast import *

from ..relation import Relation
from ..operations import view_replacer, walk
from ..schema_interpreter import (
  field_from_expr, schema_from_projection_op, 
  schema_from_projection_schema, JoinSchema
)


def compile(query):
  return walk(
    query.operations, 
    visit_with(
      query.dataset,
      (isa(LoadOp), view_replacer),
      (isa(LoadOp), load_relation),
      (isa(ProjectionOp), projection_op),
      (isa_op, relational_op),
    )
  )


def isa(type):
  def test(loc):
    return isinstance(loc.node(), type)
  return test

def visit_with(dataset, *visitors):
  def visitor(loc):
    for test,f in visitors:
      if test(loc):
        loc = f(dataset, loc, loc.node())
    return loc
  return visitor


def isa_op(loc):
  return type(loc.node()) in RELATION_OPS

def relational_op(dataset, loc, operation):
  return loc.replace(RELATION_OPS[type(operation)](dataset,  operation))

def load_relation(dataset, loc, operation):
  adapter = dataset.adapter_for(operation.name)
  return adapter.evaluate(loc)

def alias_op(dataset, operation):
  def alias(ctx):
    relation = operation.relation(ctx)
    return Relation(
      relation.schema.new(name=operation.name),
      iter(relation)
    )

  return alias



def projection_op(dataset, loc, operation):

  aggs = aggregates(operation.exprs, dataset)
  if aggs:
    # we have an aggregate operations, push them up to the group by
    # operation or add a group by operation if all projection expresions
    # are aggregates
    u = loc.up()
    parent_op = u and u.node()
    if not isinstance(parent_op, GroupByOp):
      if len(aggs) != len(operation.exprs):
        raise group_by_error(operation.expresions, aggs)
      loc = loc.replace(GroupByOp(operation, aggregates=aggs)).down()
    else:
      loc = u.replace(parent_op.new(aggregates=aggs)).down()


  def projection(ctx):
    relation = operation.relation(ctx)
    columns = tuple([
      column
      for group in [
        column_expr(expr, relation, dataset)
        for expr in operation.exprs
      ]
      for column in group
    ])

    schema = schema_from_projection_schema(operation, relation.schema, dataset)
 

    return Relation(
      schema,
      (
        tuple( col(row, ctx) for col in columns )
        for row in relation
      )
    )

  return loc.replace(projection)



def selection_op(dataset, operation):

  if operation.bool_op is None:
    return lambda relation, ctx: relation


  def selection(ctx):
    relation = operation.relation(ctx)
    predicate  = value_expr(operation.bool_op, relation, dataset)

    return Relation(
      relation.schema,
      (
        row
        for row in relation
        if predicate(row, ctx)
      )
    )
  return selection

def join_op(dataset, operation):
  
  def join(ctx):
    left = operation.left(ctx)
    right = operation.right(ctx)


    schema = JoinSchema(left.schema, right.schema) 

    # seems silly to have to build a fake relation
    # just so var_expr (which is reached by value_expr)
    # can use the new schema. value_expr should just
    # take the schema as an arg...
    bogus = Relation(schema, None)


    try:
      comparison = join_keys(left, right, operation.bool_op)
      method = hash_join
    except ValueError:
      # icky cross product
      comparison = value_expr(operation.bool_op, bogus, dataset)
      method = nested_block_join


    return Relation(
      schema,
      method(operation.left, operation.right, comparison, ctx)
    )


  return join



def order_by_op(dataset, operation):

  def order_by(ctx):
    relation = operation.relation(ctx)

    columns = tuple(
      value_expr(expr, relation, dataset)
      for expr in operation.exprs
    )  

    def key(row):
      return tuple(c(row, ctx) for c in columns)

    return Relation(
      relation.schema, 
      iter(sorted(relation, key=key))
    )
  return order_by

def group_by_op(dataset, group_op):


  if group_op.exprs:
    load   = order_by_op(dataset, group_op)
  else:
    # we're aggregating the whole table
    load = group_op.relation 


  exprs      = group_op.exprs
  aggs       = group_op.aggregates

  initialize = initialize_op(aggs)
  accumalate = accumulate_op(aggs)
  finalize   = finalize_op(aggs)


  def group_by(ctx):
    ordered_relation = load(ctx)
    if group_op.exprs:
      key = key_op(group_op.exprs, ordered_relation.schema)
    else:
      # it's all aggregates with no group by elements
      # so no need to order the table
      key = lambda row,ctx: None

    schema = ordered_relation.schema

    def group():
      records = iter(ordered_relation)

      row = records.next()
      group = key(row, ctx)
      
      record = accumalate(initialize(row), row)

      for row in records:
        next = key(row, ctx)
        if next != group:
          yield finalize(record)
          group = next
          record = initialize(row)
        previous_row = accumalate(record, row)

      yield finalize(record)

    return Relation(
      schema,
      group()
    )

  # note that this method was triggered on ProjectionOp but we're returning the
  # location of the GroupByOp replaced with the group_by function
  # which will prevent the ProjectionOp from being inspected 
  # further.
  return group_by


def slice_op(dataset, expr):
  def limit(ctx):
    relation = expr.relation(ctx)
    return Relation(
      relation.schema,
      islice(relation, expr.start, expr.stop)
    )
  return limit

def relational_function(dataset, op):
  """Invokes a function that operates on a whole relation"""
  function = dataset.get_function(op.name)

  args = []
  for arg_expr in op.args:
    if isinstance(arg_expr, Const):
      args.append(lambda ctx, const=arg_expr.const: const)
    elif callable(arg_expr):
      args.append(arg_expr)
    else:
      raise ValueError(
        "Only Relational Operations and constants "
        "are allowed in table functions"
      )

  def invoke(ctx):
    return function(*[a(ctx) for a in args])
  return invoke

def is_aggregate(expr, dataset):
  """Returns true if the expr is an aggregate function."""
  if isinstance(expr, RenameOp):
    expr = expr.expr

  return isinstance(expr, Function) and expr.name in dataset.aggregates

def aggregate_expr(expr, dataset):
  if isinstance(expr, RenameOp):
    expr = expr.expr

  return dataset.aggregates[expr.name]

def group_by_error(exprs, aggs):
  """Used to raise an error highlighting the first
  offending column when a projection operation has a mix of
  aggregate expresions and non-aggregate expresions but no 
  group by.

  """

  agg_expr = dict(aggs)
  for col, expr in enumerate(exprs):
    if col not in aggs:
      return SyntaxError(
        (
          '"{}" must appear in the GROUP BY clause '
          'or be used in an aggregate function'
        ).format(expr)
      )


def key_op(exprs, schema):
  positions = tuple(
    schema.field_position(expr.path)
    for expr in exprs
  )
      
  def key(row, ctx):
    return tuple(row[pos] for pos in positions)
  return key

def initialize_op(pos_and_aggs):
  def initialize(row):
    # convert the tuple to a list so we can modify it
    record = list(row)
    for pos, agg in pos_and_aggs:
      record[pos] = agg.initial
    return record
  return initialize

def accumulate_op(pos_and_aggs):
  def accumulate(record, row):
    for pos, agg in pos_and_aggs:
      args = row[pos]
      state = record[pos]
      record[pos] = agg.function(state, *args)
    return record
  return accumulate

def finalize_op(pos_and_aggs):
  def finalize(record):
    # convert the tuple to a list so we can modify it
    for pos, agg in pos_and_aggs:
      if agg.finalize:
        state = record[pos]
        record[pos] = agg.function(state, *args)
    return tuple(record)
  return finalize



def aggregates(exprs, dataset):
  """Returns a list of list or the aggregates in the exprs.

  The first item is the column index of the aggregate the
  second is the aggregate itself.
  """
  return tuple(
    (pos, aggregate_expr(aggr, dataset))
    for pos, aggr in enumerate(exprs)
    if is_aggregate(aggr, dataset)
  )


def column_expr(expr, relation, dataset):
  # selectall returns a group of expresions, group solo to be flattened
  # by the outer loop

  if isinstance(expr, SelectAllExpr):
    return select_all_expr(expr, relation, dataset)
  else: 
    return (value_expr(expr,relation,dataset),)

def select_all_expr(expr, relation, dataset):

  schema = relation.schema
  if expr.table is None:
    fields = schema.fields
  else:
    prefix = expr.table + "."
    fields = [
      f
      for f in schema.fields
      if f.name.startswith(prefix)
    ]

  return [
    var_expr(Var(f.name), relation, dataset) for f in fields
  ]

def value_expr(expr, relation, dataset):
  return VALUE_EXPR[type(expr)](expr, relation, dataset)

def itemgetter_expr(expr, relation, dataset):
  key = expr.key
  def itemgetter(row, ctx):
    return row[key]
  return itemgetter

def sub_expr(expr, relation, dataset):
  return value_expr(expr.expr, relation, dataset)

def var_expr(expr, relation, dataset):
  pos = relation.schema.field_position(expr.path)
  def var(row, ctx):
    return row[pos]
  return var

def const_expr(expr, relation, dataset):
  def const(row, ctx):
    return expr.const
  return const

def null_expr(expr, relation, dataset):
  return lambda row, ctx: None

def function_expr(expr, relation, dataset):
  function = dataset.get_function(expr.name)
  arg_exprs = tuple(
    value_expr(arg_expr, relation, dataset)
    for arg_expr in expr.args
  )

  def _(row, ctx):
    args = tuple(
      arg_expr(row, ctx)
      for arg_expr in arg_exprs
    )
    return function(*args)

  _.__name__ = str(expr.name)
  return _



def desc_expr(expr, relation, dataset):
  field = field_from_expr(expr.expr, dataset, relation.schema)
  value = value_expr(expr.expr, relation, dataset)

  if field.type in ("INTEGER", "FLOAT", "DOUBLE"):

    def invert_number(record, ctx):
      return -value(record,ctx)

    return invert_number

  elif field.type == "STRING":
    def invert_string(record, ctx):
      return [-b for b in bytearray(value(record,ctx))]
    return invert_string
  else:
    return lambda r,c: None
 
def unary_op(operator, expr, relation, dataset):
  val = value_expr(expr.expr, relation, dataset)
  def _(row,ctx):
    return operator(val(row, ctx))
  _.__name__ = operator.__name__
  return _

def binary_op(operator, expr, relation, dataset):
  lhs = value_expr(expr.lhs, relation, dataset)
  rhs = value_expr(expr.rhs, relation, dataset)

  def _(row, ctx):
    return operator(lhs(row, ctx), rhs(row, ctx))
  _.__name__ = operator.__name__
  return _



VALUE_EXPR = {
  Var: var_expr,
  StringConst: const_expr,
  NumberConst: const_expr,

  NullConst: const_expr,
  TrueConst: const_expr,
  FalseConst: const_expr,

  Function: function_expr,

  NegOp: partial(unary_op, operator.neg),
  NotOp: partial(unary_op, operator.not_),

  And: partial(binary_op, operator.and_),
  Or: partial(binary_op, operator.or_),

  LtOp: partial(binary_op, operator.lt),
  LeOp: partial(binary_op, operator.le),
  EqOp: partial(binary_op, operator.eq),
  NeOp: partial(binary_op, operator.ne),
  GeOp: partial(binary_op, operator.ge),
  GtOp: partial(binary_op, operator.gt),
  IsOp: partial(binary_op, operator.is_),
  IsNotOp: partial(binary_op, operator.is_not),

  AddOp: partial(binary_op, operator.add),
  SubOp: partial(binary_op, operator.sub),

  MulOp: partial(binary_op, operator.mul),
  DivOp: partial(binary_op, operator.div),

  ItemGetterOp: itemgetter_expr,

  RenameOp: sub_expr,
  NullConst: null_expr,

  Asc: sub_expr,
  Desc: desc_expr



}

RELATION_OPS = {
  AliasOp: alias_op,
  ProjectionOp: projection_op,
  SelectionOp: selection_op,
  OrderByOp: order_by_op,
  GroupByOp: group_by_op,
  SliceOp: slice_op,
  JoinOp: join_op,
  Function: relational_function,
  
}

# sigh, oh python and your circular import
from .join import nested_block_join, hash_join, join_keys

########NEW FILE########
__FILENAME__ = dataset
from .query import Query
from .query_builder import QueryBuilder
from .query_parser import parse_statement
from .relation import NullRelation, NullAdapter

from .aggregate import Aggregate
from .compilers import local
from .field import Field

from .operations import replace_views

from . import functions

class DataSet(object):

  def __init__(self):
    self.adapters = [NullAdapter()]

    self.relation_cache = {}

    self.views = {}
    self.schema_cache = {}
    self.executor = None
    self.compile = local.compile
    self.dump_func = None
    self.udfs = {}

    self.aggregates = {}
    
    functions.init(self)



  def add_adapter(self, adapter):
    """
    Add the given adapter to the dataset.

    Multiple adds for the instance of the same
    adapter are ignored.

    """

    if adapter not in self.adapters:
      self.adapters.append(adapter)

  def create_view(self, name, query_or_operations):
    if isinstance(query_or_operations, basestring):
      operations = self.query(query_or_operations).operations
    else:
      operations = query_or_operations

    self.views[name] = operations
    
  def aggregate(self, returns=None, initial=None, name=None, finalize=None):
    def _(func, name):
      if name is None:
        name = func.__name__
      self.add_aggregate(name, func, returns, initial, finalize)
      return func
    return _

  def function(self, returns=None, name=None):
    """Decorator for registering functions

    @dataset.function
    def somefunction():
      pass


    """

    def _(func, name=name):
      if name is None:
        name = func.__name__
      self.add_function(name, func, returns)
      return func
    return _ 

  def add_aggregate(self, name, func, returns, initial, finalize=None):
    self.aggregates[name] = Aggregate(
      function=func, 
      returns=returns, 
      initial=initial,
      finalize=finalize
    )


  def add_function(self, name, function, returns=None):
    if name in self.udfs:
      raise ValueError("Function {} already registered".format(name))

    if returns:
      if callable(returns):
        function.returns = returns
      else:
        function.returns = Field(**returns)
    else:
      function.returns = None


    self.udfs[name] = function

  def get_function(self, name):
    function = self.udfs.get(name) or self.aggregates.get(name)
    if function:
      return function
    else:
      raise NameError("No function named {}".format(name))

  def get_view(self, name):
    view = self.views.get(name)
    if view:
      return replace_views(view,self)
    else:
      return None

  @property
  def relations(self):
    """
    Returns a list of all relations from all adapters
    note this could be a slow operation as remote
    calls must be made to each adapter. 

    As a side effect the relation_cache will be updated.
    """

    for adapter in self.adapters:
      for name, schema in adapter.relations:
        self.relation_cache[name] = schema
    return [item for item in self.relation_cache.items() if item[0] != '']



  def adapter_for(self, relation):
    for adapter in self.adapters:
      if adapter.has(relation):
        return adapter

  def get_relation(self, name):
    """Returns the relation for the given name.

    The dataset will search all adapters in the order the
    adapters were added to the dataset returning the first 
    relation with the given name.
    """
    relation = self.relation_cache.get(name)
    if relation is None:
      for adapter in self.adapters:
        relation = adapter.get_relation(name)
        if relation:
          self.relation_cache[name] = relation
          break

    return relation


  def get_schema(self, name):
    """
    Returns the schema for relation found with the given name.

    The search order is the same as dataset.get_relation()
    """

    return self.get_relation(name).schema

 

  def set_compiler(self, compile_fun):
    self.compile = compile_fun

  def set_dump_func(self, dump_func):
    self.dump_func = dump_func

  def dump(self, relation):
    self.dump_func(relation)

  def execute(self, query, *params):

    callable = self.compile(query)
    ctx = {
      'dataset': self,
      'params': params
    }

    return callable(ctx)

  def relpace_view(self, op):
    """
    Given an operation tree return a new operation tree with 
    LoadOps that reference views replaced with the operations 
    of the view.
    """

  def query(self, statement):
    """Parses the statement and returns a Query"""
    return Query(self, parse_statement(statement))

  def frm(self, relation_or_stmt):
    return QueryBuilder(self).frm(relation_or_stmt)

  def select(self, *cols):
    return QueryBuilder(self).select(*cols)


########NEW FILE########
__FILENAME__ = field
class Field(object):
  __slots__ = {
    'name': "-> string [REQUIRED]",
    'type': "-> string [REQUIRED] integer|float|string|boolean|date|datetime|time|record",
    'mode': "-> string [OPTIONAL] REQUIRED|NULLABLE|REPEATED: default NULLABLE",
    'fields': "-> list [OPTIONAL IF type = RECORD]",
  }

  def __init__(self, **attrs):
    self.name = attrs['name']
    self.type = attrs['type']
    self.mode = attrs.get('mode', 'NULLABLE')
    self.fields = [
      f if isinstance(f, Field) else Field(**f)
      for f in attrs.get('fields', [])
    ]


  def __repr__(self):
    return "<Field(name={name}, type={type} at {id}>".format(
      id=id(self),
      name=self.name,
      type=self.type
    )
      

  def __eq__(self, other):
    return (
      self.name == other.name
      and self.type == other.type
      and self.mode == other.mode
      and self.fields == other.fields
    )

  def new(self, **parts):
    attrs = {attr:getattr(self, attr) for attr in self.__slots__}
    attrs.update(parts)

    return self.__class__(**attrs)

  def to_dict(self):
    return dict(
      name = self.name,
      type = self.type,
      mode = self.mode,
      fields = [f.to_dict() for f in self.fields]
    )



########NEW FILE########
__FILENAME__ = aggregates
from ..field import Field

def init(dataset):
  dataset.add_aggregate(
    "count", 
    func=lambda state: state + 1,
    returns=Field(name="count", type="INTEGER"),
    initial=0
  )
  
  #@dataset.aggregate(returns="INTEGER",initial=0)
  #def count(state):
  #  return state+1

  #@dataset.aggregate(returns="FLOAT", initial=(0,0.0))
  #def avg((count, sum), val):
  #  return count+1, sum+val

  #@avg.finalize
  #def avg((count, sum)):
  #  return sum count

########NEW FILE########
__FILENAME__ = filesystem
import os
from os.path import join
from itertools import chain

from ..relation import Relation
from ..schema import Schema
from .. import codecs
from splicer.path import pattern_regex, regex_str


def init(dataset):
  dataset.add_function("files", files)
  dataset.add_function("decode", decode)
  dataset.add_function("contents", contents)
  dataset.add_function("extract_path", extract_path)


def contents(relation, path_column, content_column='contents'):
  """
  Given a relation and a path_column, returns a new relation
  whose rows are the same as the original relation with the
  addition of the contents of the file specified by path_column.

  Note: This method is not  suitable for large files,
  as entire contents of the file are read into memory.
  """
  field_pos = relation.schema.field_position(path_column)
 

  return Relation(
    Schema(relation.schema.fields + [dict(type='BINARY', name=content_column)]),
    (
      row + (open(row[field_pos]).read(), )
      for row in relation
    )
  )

def decode(relation, mime_type,  path_column="path", schema=None):
  field_pos = relation.schema.field_position(path_column)
  relation_from_path = codecs.relation_from_path

  if schema is None:
    it = iter(relation)
    first = next(it)

    schema = Schema(
      relation.schema.fields + 
      relation_from_path(first[field_pos], mime_type).schema.fields
    )
    relation = chain((first,), it)
  else:
    schema = Schema(relation.schema.fields + schema.fields)

  return Relation(
    schema,
    (
      r + tuple(s)
      for r in relation
      for s in relation_from_path(r[field_pos], mime_type)
    )
  )


def files(root_dir, filename_column="path"):
  """
  Return an iterator of all filenames starting at root_dir or below
  """ 

  return Relation(
    Schema([dict(type="STRING", name=filename_column)]),
    (
      (join(root,f), )
      for root, dirs, files in os.walk(root_dir)
      for f in files if not f.startswith('.')
    )
  )

def extract_path(files, pattern, path_column="path"):
  """
  Extracts patterns out of file paths and urls.

  Returns a relation where the path_column is matched 
  against a reular expression expressed by the pattern
  argument.
  the resulting group info appened to the matching row.

  Example
  ['/some/path'] -> ['/some/path', 'some', 'path']
  """

  field_pos = files.schema.field_position(path_column)
  regex, columns = pattern_regex(pattern)

  schema = Schema(files.schema.fields + [
    dict(name=c, type='STRING')
    for c in columns    
  ])

  def extract():
    for row in files:
      path = row[field_pos]
      m = regex.match(path)
      if m:
        yield row + m.groups()

  return Relation(schema, extract())


########NEW FILE########
__FILENAME__ = math
from __future__ import absolute_import
import math
import inspect


def init(dataset):
  for name,func in inspect.getmembers(math, inspect.isbuiltin):
    # builtins can't have attributes, which is where we store
    # the type info
    f = lambda *args: func(*args)
    dataset.add_function(
      name,
      f,
      returns=dict(name=name, type='NUMBER')
    )

########NEW FILE########
__FILENAME__ = relational
"""
Functions useable in the from clause
"""

from ..relation import Relation
from ..schema import Schema

def init(dataset):
  dataset.add_function("flatten", flatten, flatten_schema)


def flatten(relation, path):

  schema = relation.schema
  field = schema[path]


  if field.mode != "REPEATED" and not field.type=="RECORD":
    raise ValueError("Can not flatten non-repeating field {}".format(path))

  field_pos = schema.field_position(path)

  def flatten_repeated_scalar(row):

    for value in row[field_pos]:
      new_row = list(row)
      new_row[field_pos] = value 

      yield new_row

  def flatten_single_record(row):
    value  = row[field_pos]
    if value:
      new_row = list(row)
      values = [value.get(f.name) for f in field.fields]
      new_row[field_pos:field_pos+1] = values
      yield new_row

  def flatten_repeated_record(row):
    col = row[field_pos]
    if col:
      for value in row[field_pos]:
        new_row = list(row)
        values = [value.get(f.name) for f in field.fields]

        new_row[field_pos:field_pos+1] = values
        yield new_row


  if field.type == 'RECORD':
    if field.mode == "REPEATED":
      flatten_row = flatten_repeated_record
    else:
      flatten_row = flatten_single_record

  else:
    flatten_row = flatten_repeated_scalar

  #TODO: remove the need to wrap the results in a relation
  return Relation(
    flatten_schema(schema, path), 
    (
      new_row
      for row in relation
      for new_row in flatten_row(row)
    )
  )

def flatten_schema(schema, path):
  field = schema[path]
  new_fields = schema.fields[:]
  field_pos = schema.field_position(path)

  if field.type == 'RECORD':
    ffields = [
      f.new(name="{}_{}".format(field.name, f.name)) 
      for f in field.fields
    ]
  else:
    ffields = [field.new(mode="NULLABLE")]

  # replace the repated field with the flatten one
  new_fields[field_pos:field_pos+1] = ffields 

  return Schema(new_fields)



########NEW FILE########
__FILENAME__ = string
from __future__ import absolute_import
import string
import inspect

def null_if_arg_is_null(f):
  """
  Wrap a standard python function which inspects the args for None values.
  If they're present the function returns None as it's value otherwise
  returns the results of calling the function.S
  """
  def _(s, *args):
    if s is None:
      return None
    else:
      return f(s, *args)
  _.__name__ = f.__name__
  return _


def init(dataset):
  for name, func in inspect.getmembers(string, inspect.isfunction):
    if name == 'count':
      continue

    
    dataset.add_function(
      name,
      null_if_arg_is_null(func),
      returns=dict(name=name, type="STRING")
    )

  @dataset.function(returns=dict(name="length", type="INTEGER"))
  def length(s):
    return len(s)

########NEW FILE########
__FILENAME__ = immutable
import inspect

class ImmutableMixin(object):
  """
  Mixin used to provide immutable object support (kind of).

  This class allows you to quickly construct a new object
  based off an existing one.
  """

  def __eq__(self, other):
    other_slots = getattr(other, '__slots__', ())
    slots = self.__slots__
    if len(slots) != other_slots:
      return False

    return all([
      getattr(self, s) == getattr(other,o) 
      for s,o in zip(slots, other_slots)
    ])


  def new(self, **parts):
    """
    Returns a copy of the existing object with the specified attrs
    overwritten.
    """
    attrs = {attr:getattr(self, attr) for attr in self.__slots__}
    attrs.update(parts)

    arg_spec = inspect.getargspec(self.__class__.__init__)
    args = []
    for arg in arg_spec.args[1:]:
      args.append(attrs.pop(arg))

    args.extend(attrs.pop(arg_spec.varargs, ()))

    return self.__class__(*args, **attrs)

########NEW FILE########
__FILENAME__ = operations
from functools import partial

from zipper import zipper

# functions for manipulating operations go here
from ast import LoadOp, JoinOp, EqOp, RelationalOp, Function, Var



def query_zipper(operation):
  return zipper(operation, is_branch, children, make_node)

def is_branch(operation):
  return not isinstance(operation, LoadOp)

def children(operation):
  if isinstance(operation, JoinOp):
    return (operation.left, operation.right)
  elif isinstance(operation, Function):
    return tuple(
      arg 
      for arg in operation.args 
      if isinstance(arg, RelationalOp) or isinstance(arg, Function)
    )
  else:
    return (operation.relation,)

def make_node(operation, children):
  if isinstance(operation, JoinOp):
    left, right = children
    return operation.new(left=left, right=right)
  elif isinstance(operation, Function):
    pos = 0
    args = []
    for arg in operation.args:
      if isinstance(arg, RelationalOp) or isinstance(arg, Function):
        args.append(children[pos])
        pos += 1
      else:
        args.append(arg)
    return operation.new(args=args)
  else:
    return operation.new(relation=children[0])




def walk(operation, visitor):
  """
  walk is a method, used to produce a new query plan based on an
  exsiting plan.  

  Parameters:
  -----------
  operation: current node being visited

  visitor: method that takes a single argument loc. 
           it's return value will be used in place of the visited
           loc. Methods that want to keep the tree as  is should
           return the loc.
  """
  loc = query_zipper(operation).leftmost_descendant()

  while True:
    loc = visitor(loc)
    if loc.at_end():
      break
    else:
      loc = loc.postorder_next()

  return loc.root()

def view_replacer(dataset, loc, op):
  view = dataset.get_view(op.name)
  if view:
    loc = loc.replace(view).leftmost_descendant()
  return loc


def replace_views(operation, dataset):
  def adapt(loc):
    node = loc.node()
    if isinstance(node, LoadOp):
      return view_replacer(dataset, loc, node)
    else:
      return loc
  return walk(operation, adapt)




########NEW FILE########
__FILENAME__ = path
"""
path.py - functions for manipulation and extracting data from paths.
"""

import re


from codd import Tokens

def tokenize_pattern(pattern):
  t = Tokens(pattern)
  while not t.at_end():
    if t.current_char == '/':
      yield t.read_char()
    elif t.current_char == '{':
      yield t.read_until('}') + t.read_char()
    else:
      yield t.read_until('/{')


def regex_str(tokens):
  r = []
  for t in tokens:
    if t.startswith('{'):
      r.append('(?P<%s>[^/]*)' % t[1:-1])
    else:
      r.append(re.escape(t))
  return ''.join(r)

def columns(tokens):
  return [t[1:-1] for t in tokens if t.startswith('{')]

def pattern_regex(pattern_str):
  tokens = list(tokenize_pattern(pattern_str))
  
  return (
    re.compile(regex_str(tokens)),  # regex
    columns(tokens) # tokennames
  )

########NEW FILE########
__FILENAME__ = query
from .schema_interpreter import interpret as interpret_schema

class Query(object):
  __slots__ = {
    'dataset': '-> DataSet',
    'operations': 'ast.Expr',
  }


  def __init__(self, dataset,  operations):
    self.dataset = dataset
    self.operations = operations


  def __iter__(self):
    return iter(self.execute())


  @property
  def schema(self):
    return interpret_schema(self.dataset,  self.operations)

    
  def dump(self):
    self.dataset.dump(self.execute())

  def create_view(self, name):
    self.dataset.create_view(name, self.operations)
    
  def execute(self, *params):
    return self.dataset.execute(self, *params)






########NEW FILE########
__FILENAME__ = query_builder
from . import Query
from .ast import (
  RelationalOp, ProjectionOp, SelectionOp, GroupByOp, OrderByOp, 
  SliceOp, JoinOp, LoadOp,
  And, SelectAllExpr
)
import query_parser

from .compilers.local import is_aggregate

class QueryBuilder(object):
  """
  Helps build a query 
  """

  __slots__ = {
    'dataset': '-> DataSet',
    'column_exps': 'str -- uparsed column',
    'load': '-> nullary op',
    'ordering': '-> [Field] -- results ',
    'grouping': '-> [Field]',
    'qualifiers': '-> str',
    'stop': "int",
    'start': "int"
  }


  def new(self, **parts):
    derived_qb = self.__class__(self.dataset)

    for attr in self.__slots__:
      setattr(derived_qb, attr, getattr(self, attr, None))

    for attr, value in parts.items():
      setattr(derived_qb, attr, value)
    return derived_qb


  def __init__(self, dataset):
    self.dataset = dataset
    self.column_exps = "*"
    self.load = None
    self.qualifiers = ()
    self.ordering = None
    self.grouping = []
    self.start = None
    self.stop = None

  def __repr__(self):
    return repr(self.schema)

  def __iter__(self):
    return iter(self.execute())

  @property
  def schema(self):
    return self.query.schema

  def select(self, column_exps):
    return self.new(column_exps=column_exps)

  def frm(self, clause):
    if isinstance(clause, Query):
      relation = clause.operations
    else:
      relation = query_parser.parse_from(clause)
    return self.new(load = relation)

  def join(self, clause, on=None):
    if not self.load:
      raise ValueError('Specify at least one relation with frm(...) before using join')

    if isinstance(clause, QueryBuilder):
      load = JoinOp(self.load, clause.operations)
    else:
      load = query_parser.parse_join(clause, self.load)

    if on:
      load.bool_op = query_parser.parse(on)

    return self.new(load = load )


  def where(self, qualifiers):
    return self.new(qualifiers=self.qualifiers + (qualifiers,))

  def group_by(self, grouping_statement):
    exprs = query_parser.parse_group_by(grouping_statement)
    return self.new(grouping=exprs)

  def order_by(self, ordering_exp):
    ordering = query_parser.parse_order_by(ordering_exp)

    return self.new(ordering=ordering)

  def limit(self, limit):
    return self.new(stop=limit)

  def offset(self, offset):
    return self.new(start=offset)

  @property
  def query(self):
    """
    Returns a valid query suitable for execution, or raises an exception.
    """

    if not self.load:
      operations = LoadOp('')
    else:
      operations = self.load

    if self.qualifiers:
      qualifiers = iter(self.qualifiers)
      bool_op = query_parser.parse(qualifiers.next())
      for qualifier in qualifiers:
        bool_op = And(bool_op, query_parser.parse(qualifier))

      operations = SelectionOp(operations, bool_op)
      #operations.append(SelectionOp(bool_op))


    operations = query_parser.parse_select(operations, self.column_exps)


    if self.grouping or self.has_aggregates(operations):
      operations = GroupByOp(operations, *self.grouping)
    
    if self.ordering:
      # todo: eleminate ordering if it matches
      # the grouping since we already sort
      #operations.append(self.ordering)
      operations = OrderByOp(operations, *self.ordering)

    if self.stop is not None or self.start is not None:
      if self.start and self.stop:
        stop = self.start + self.stop
      else:
        stop = self.stop 
      operations = SliceOp(operations, self.start, stop)

    return Query(self.dataset,  operations)


  def has_aggregates(self, projection_op):
    """Returns true if the projection has aggregates"""
    if not isinstance(projection_op, ProjectionOp):
      return False

    for expr in projection_op.exprs:
      if is_aggregate(expr, self.dataset):
        return True
    else:
      return False

  @property
  def operations(self):
    return self.query.operations

  def dump(self):
    self.query.dump()

  def execute(self):
    return self.query.execute()

  def create_view(self, name):
    self.query.create_view(name)
    return self
########NEW FILE########
__FILENAME__ = query_parser
import string

from codd import Tokens

from . import Field
from . import Schema
from .ast import *


def parse(statement, root_exp = None):
  if root_exp is None:
    root_exp = and_exp

  tokens = list(Tokens(statement))
  exp = root_exp(tokens)
  if tokens: 
    raise SyntaxError('Incomplete statement {}'.format(tokens))
  return exp

def parse_statement(statement):
  return parse(statement, root_exp=select_stmt)

def parse_select(relation, statement):
  columns = parse(
    statement, 
    root_exp=lambda tokens: select_core_exp(tokens)
  )
  return projection_op(relation, columns)


def parse_from(statement):
  return parse(statement, root_exp=from_core_exp)

def parse_join(statement, left):
  return parse(statement, root_exp=lambda tokens: join_core_exp(tokens, left))


def parse_order_by(statement):
  return parse(
    statement, 
    root_exp=order_by_core_expr
  )

def parse_group_by(statement):
  return parse(
    statement, 
    root_exp=group_by_core_expr
  )


## parsing routines #######



def and_exp(tokens):    
  lhs = or_exp(tokens) 
  while len(tokens) and tokens[0] == 'and':
    tokens.pop(0)
    lhs = And(lhs, or_exp(tokens))
  return lhs
  
def or_exp(tokens):
  lhs = comparison_exp(tokens)
  while len(tokens) and tokens[0] == 'or':
    tokens.pop(0)
    lhs = Or(lhs, comparison_exp(tokens))
  return lhs
  
def comparison_exp(tokens):
  lhs = additive_exp(tokens)

  if len(tokens) and tokens[0] in COMPARISON_OPS:
    token = tokens.pop(0)
    if tokens and tokens[0] == 'not':
      token = 'is not'
      tokens.pop(0)

    Op = COMPARISON_OPS[token]
    rhs =  additive_exp(tokens)
    return Op(lhs, rhs)
  else:
    return lhs

def additive_exp(tokens):
  lhs = multiplicative_exp(tokens)
  while tokens:
    Op = ADDITIVE_OPS.get(tokens[0])
    if Op:
      tokens.pop(0)
      rhs = multiplicative_exp(tokens)
      lhs = Op(lhs, rhs)
    else:
      break
  return lhs

def multiplicative_exp(tokens):
  lhs = unary_exp(tokens)
  while len(tokens):
    Op = MULTIPLICATIVE_OPS.get(tokens[0])
    if Op:
      tokens.pop(0)
      rhs = unary_exp(tokens)
      lhs = Op(lhs, rhs)
    else:
      break
  return lhs

def unary_exp(tokens):
  
  if tokens[0] == '-':
    tokens.pop(0)
    value = value_exp(tokens)
    return NegOp(value)
  elif tokens[0] == 'not':
    tokens.pop(0)
    value = value_exp(tokens)
    return NotOp(value)
  elif tokens[0] == '+':
    tokens.pop(0)
    
  return value_exp(tokens)

def value_exp(tokens):
  """
  Returns a function that will return a value for the given token
  """
  token = tokens.pop(0)
  
  # todo: consider removing select $0 and instead
  # requiring table table.field[0]

  if token.startswith('$'):
    key = token[1:]
    try:
      key = int(key)
    except ValueError:
      pass

    return ItemGetterOp(key)

  if token.startswith('?'):
    pos = int(token[1:])
    return ParamGetterOp(pos)
  elif token == 'null':
    return NullConst()
  elif token[0] in string.digits:
    return NumberConst(int(token))
  elif token[0] in ("'",'"'):
    return StringConst(token[1:-1])
  elif token == '(':
    return tuple_exp(tokens)
  #elif token in SYMBOLS: 
  #  return lambda row, ctx: token
  else:

    if tokens and tokens[0] == '(':
      return function_exp(token, tokens)
    else:
      return var_exp(token, tokens)

def tuple_exp(tokens):
  args = []
  if tokens and tokens[0] != ')':
    args.append(and_exp(tokens))
    while tokens[0] == ',':
      tokens.pop(0)
      args.append(and_exp(tokens))
  if not tokens or tokens[0] != ')':
    raise SyntaxError("missing closing ')'")
  
  tokens.pop(0)

  return Tuple(*args)
 

def function_exp(name, tokens):
  token = tokens.pop(0)
  if token != '(':
    raise SyntaxError('Expecting "("')


  args = tuple_exp(tokens)
  return Function(name, *args.exprs)

reserved_words = ['is','in']
def var_exp(name, tokens, allowed=string.letters + '_'):
  if name in reserved_words:
    raise SyntaxError('invalid syntax')
  path = [name]

  while len(tokens) >= 2 and tokens[0] == '.' and tokens[1][0] in allowed:
    tokens.pop(0) # '.'
    path.append(tokens.pop(0)) 

  return Var('.'.join(path)) 


# sql specific parsing
terminators = ('from', 'where', 'limit', 'offset', 'having', 'group', 'order')

def projection_op(relation, columns):
  if len(columns) == 1 and isinstance(columns[0], SelectAllExpr) and columns[0].table is None:
    return relation
  else:
    return ProjectionOp(relation, *columns)


def select_core_exp(tokens):
  columns = []

  while tokens and tokens[0] not in terminators:
    col = result_column_exp(tokens)

    columns.append(col)
    if tokens and tokens[0] == ',':
      tokens.pop(0)

  return columns

def from_core_exp(tokens):
  left = join_core_exp(tokens, None).right

  while tokens and tokens[0] not in terminators:
    if tokens[0] != ',':
      raise SyntaxError('Expected ","')
    tokens.pop(0)

    left = join_core_exp(tokens, left)

  return left

def join_core_exp(tokens, left):
  load_op = LoadOp(tokens.pop(0))
  if tokens:
    if tokens[0] == 'as':
      tokens.pop(0)

  # todo parse 'on'
  if tokens:
    if tokens[0] != ',' and tokens[0] not in terminators:
      alias = tokens.pop(0)
      load_op = AliasOp(alias, load_op)

  return JoinOp(left, load_op)

def join_source(tokens):
  source = single_source(tokens)
  while tokens and tokens[0] == ',':
    tokens.pop(0)

    # if it's a relational function the statement
    # may look like 
    # select ... from func(select * from foo, 'some constant')
    # we use the fact that there's something other than a 
    # Var to end the statement... Feels hacky
    expr = value_exp(tokens[:1])
    if not isinstance(expr, Var):
       break

    right = single_source(tokens)
    if tokens and tokens[0] == 'on':
      tokens.pop(0)
      source = JoinOp(source, right, and_exp(tokens))
    else:
      source = JoinOp(source, right)

  return source

def single_source(tokens):
  if tokens[0] == '(':
    tokens.pop(0)
    if tokens[0] == 'select':
      source = select_stmt(tokens)
      #if tokens[0] != ',':
      if tokens and tokens[0] not in terminators:
        if tokens[0] == 'as':
          tokens.pop(0)
        alias = tokens.pop(0)
        source = AliasOp(alias, source)
    else:
      source = join_source(tokens)

    if tokens[0] != ')':
      raise SyntaxError('Expected ")"')
      return source
    else:
      tokens.pop(0)
  else:

    if tokens[1:2] == ['(']:
      source = relation_function_exp(tokens.pop(0), tokens)
    else:
      source = LoadOp(tokens.pop(0))

    if tokens and tokens[0] != ',' and tokens[0] not in terminators:
      if tokens[0] == 'as':
        tokens.pop(0)
      alias = tokens.pop(0)
      source = AliasOp(alias, source)
    return source


def relation_function_exp(name, tokens):
  token = tokens.pop(0)
  if token != '(':
    raise SyntaxError("Expecting '('")

  args = []

  while tokens and tokens[0] != ")":
    if tokens[0] == 'select':
      args.append(select_stmt(tokens))
    else:
      expr = value_exp(tokens)
      if isinstance(expr, Var):
        args.append(LoadOp(expr.path))
      elif isinstance(expr, Const):
        args.append(expr)
      else:
        raise ValueError("Only constants, relationame or select queries allowed")

    if tokens[0] == ',':
      tokens.pop(0)

 
  if tokens[0] != ')':
    raise SyntaxError("Expecting ')'")
  else:
    tokens.pop(0)

  return Function(name, *args)


def result_column_exp(tokens):

  if tokens[0] == '*':
    tokens.pop(0)
    return SelectAllExpr()
  else:
    exp = and_exp(tokens)
    if tokens and isinstance(exp, Var) and tokens[:2] == ['.','*']:
      tokens.pop(0) # '.'
      tokens.pop(0) # '*'
      return SelectAllExpr(exp.path)
    else:
      if tokens and tokens[0].lower() == 'as':
        tokens.pop(0) # 'as'
        alias = tokens.pop(0)
        return RenameOp(alias, exp)
      else:
        return exp


def where_core_expr(tokens, relation):
  return SelectionOp(relation, and_exp(tokens))

def order_by_core_expr(tokens):
  columns = []

  while tokens and tokens[0] not in terminators:
    col = value_exp(tokens)
    if tokens: 
      if tokens[0].lower() == "desc":
        col = Desc(col)
        tokens.pop(0)
      elif tokens[0].lower() == "asc":
        tokens.pop(0)

    columns.append(col)

    if tokens and tokens[0] == ',':
      tokens.pop(0)


  return columns

def group_by_core_expr(tokens):

  columns = []
  while tokens and tokens[0] not in terminators:
    token = tokens.pop(0)
    columns.append(var_exp(token, tokens))
    if tokens and tokens[0] == ',':
      tokens.pop(0)

  return columns


def select_stmt(tokens):
  if tokens[0] != 'select':
    raise SyntaxError
  tokens.pop(0)

  select_core = select_core_exp(tokens) 
  #while tokens and tokens[0] not in terminators:
  #  select_core.append(tokens.pop(0))

  if tokens and tokens[0] == 'from' :
    tokens.pop(0)
    relation = join_source(tokens) #from_core_exp(tokens)
  else:
    relation = LoadOp('')


  if tokens[:1] == ['where']:
    tokens.pop(0)
    relation = where_core_expr(tokens, relation)

  
  relation =  projection_op(relation, select_core)

  if tokens[:2] == ['group', 'by']:
    tokens.pop(0)
    tokens.pop(0)
  
    relation = GroupByOp(relation, *group_by_core_expr(tokens))


  if tokens[:2] == ['order', 'by']:
    tokens.pop(0)
    tokens.pop(0)
    relation = OrderByOp(relation, *order_by_core_expr(tokens))

  start = stop = None
  if tokens and tokens[0] =='limit':
    tokens.pop(0)
    stop = value_exp(tokens).const

  if tokens and tokens[0] == 'offset':
    start = value_exp(tokens).const
    if stop is not None:
      stop += start

  if not( start is None and stop is None):
    relation = SliceOp(relation, start, stop)
    
  return relation

########NEW FILE########
__FILENAME__ = relation
from .schema import Schema
from .adapters import Adapter

class Relation(object):
  def __init__(self, schema, iterator):
    """

    Endow an interable with a schema so that's it's suitable
    to be used as a relation

    Args:
      schema (Schema):  The schema for an iterable
      iterator (iterator): An iterable object 

    """

    self.schema = schema
    self.iterator = iterator


  def __iter__(self):
    return self.iterator

class NullAdapter(Adapter):
  def has(self, relation):
    if relation == '':
      return True

  def table_scan(self, name, ctx):
    return NullRelation()


# TODO: deprecate this
class NullRelation(object):
  """Relation used for queries that don't involve tables"""
  schema = Schema(name="", fields=[])
  
  def __iter__(self):
    return iter(((),))


########NEW FILE########
__FILENAME__ = schema
from itertools import chain

from .immutable import ImmutableMixin
from .field import Field

class Schema(ImmutableMixin):
  __slots__ = {
    'name': '-> str',
    'fields': '-> [Field]',
    '_field_map': '-> {<name:str>: Field}',
    '_field_pos': '-> {<name:str>: postion:int}'
  }
  
  def __init__(self, fields, name='', **kw):
    self.name = name
    self.fields = [ 
      field if isinstance(field, Field) else Field(**field) 
      for field in fields
    ]

    self._field_map =  {f.name:f for f in self.fields}
    self._field_pos = { f.name:i for i,f in enumerate(self.fields) }


  def __repr__(self):
    name = self.name or "(no name)"
    return "Relation " + name + ":\n" + "\n".join([ 
      ("  {name}:[{type}]" if f.mode == 'REPEATED' else "  {name}:{type}").format(
        name = f.name,
        type = f.type
      )

       for f in self.fields
    ])

  def __eq__(self, other):
    """Two schemas equal if their fields equal"""

    if len(self.fields) != len(other.fields):
      return False

    return  all([f1 == f2 for f1, f2 in zip(self.fields, other.fields)])

  def __getitem__(self, field_name):
    return self.field_map[field_name]

  @property
  def field_map(self):
    return self._field_map

  def field_position(self, path):
    return self._field_pos[path]

  def to_dict(self):
    return dict(fields=[f.to_dict() for f in self.fields])


class JoinSchema(Schema):
  """
  Represents the schema produced by joining multiple schemas.
  """

  def __init__(self, *schemas):
    fields = [
      f.new(name=(schema.name + '.' + f.name) if schema.name else (f.name))
      for schema in schemas
      for f in schema.fields
    ]

    super(self.__class__,self).__init__(fields)
    # TODO: add field names that don't conflict to _field_pos
    


########NEW FILE########
__FILENAME__ = schema_interpreter
# schema_interpreter.py
"""
Module used to interpret the AST into a schema based on a given relation.
"""

from .operations import replace_views
from .schema import Schema,JoinSchema

from .field import Field
from .ast import (
  ProjectionOp, SelectionOp, GroupByOp, RenameOp, LoadOp,
  JoinOp,
  Var, Function, 
  Const, UnaryOp, BinaryOp, AliasOp, SelectAllExpr,

  NumberConst, StringConst, BoolConst
)

def interpret(dataset, operations):
  """
  Returns the schema that will be produced if the given
  operations are applied to the starting schema.
  """

  # todo, consider pushing this up into the dataset
  # object as it maybe duplicated in the complie step
  # as well
  operations = replace_views(operations, dataset)
  op_type = type(operations)
  dispatch = op_type_to_schemas.get(
    op_type,
    schema_from_relation
  ) 

  return dispatch(operations, dataset)


def schema_from_relation(operation, dataset):
  return interpret(dataset, operation.relation)

def schema_from_function_op(operation, dataset):

  func = dataset.get_function(operation.name)
  if callable(func.returns):
    schema = interpret(dataset, operation.args[0])

    return func.returns(schema, *[a.const for a in operation.args[1:]])
  else:
    return func.returns

def schema_from_load(operation, dataset):
  return dataset.get_relation(operation.name).schema

def schema_from_projection_op(projection_op, dataset):
  """
  Given a projection_op, datset and existing schema, return the new
  schema.
  """

  schema = interpret(dataset, projection_op.relation)

  fields = [
    field
    for expr in projection_op.exprs
    for field in fields_from_expr(expr,dataset,schema)
  ]

  
  return Schema(fields)

def schema_from_projection_schema(projection_op, schema, dataset):
  fields = [
    field
    for expr in projection_op.exprs
    for field in fields_from_expr(expr,dataset,schema)
  ]
  return Schema(fields)


def schema_from_join_op(join_op, dataset):
  left = interpret(dataset, join_op.left)
  right = interpret(dataset, join_op.right)

  return JoinSchema(left,right)



def schema_from_alias_op(alias_op, dataset):
  schema = interpret(dataset, alias_op.relation)
  return schema.new(name=alias_op.name)

def fields_from_expr(expr, dataset, schema):
  if isinstance(expr, SelectAllExpr):
    for field in fields_from_select_all(expr, dataset, schema):
      yield field
  else:
    yield field_from_expr(expr, dataset, schema)

def field_from_expr(expr, dataset, schema):
  """
  """
  expr_type = type(expr)
  if expr_type == Var:
    return field_from_var(expr, schema)
  elif issubclass(expr_type, Const):
    return field_from_const(expr)
  elif expr_type == Function:
    return field_from_function(expr, dataset, schema)
  elif expr_type == RenameOp:
    return field_from_rename_op(expr, dataset, schema)
  elif issubclass(expr_type, UnaryOp):
    field = field_from_expr(expr.expr, dataset, schema)
    return field.new(name="{0}({1})".format(expr_type.__name__, field.name))
  elif issubclass(expr_type, BinaryOp):
    lhs_field = field_from_expr(expr.lhs, dataset, schema)
    rhs_field = field_from_expr(expr.lhs, dataset, schema)
    if lhs_field.type != rhs_field.type:
      raise ValueError(
        "Can't coerce {} to {}".format(lhs_fielt.type, rhs_field.type)
      )
    else:
      return lhs_field.new(name="?column?".format(
        expr_type.__name__, 
        lhs_field.name,
        rhs_field.name
      ))



def fields_from_select_all(expr, dataset, schema):
  if expr.table is None:
    fields = schema.fields
  else:
    prefix = expr.table + "."
    fields = [
      f
      for f in schema.fields
      if f.name.startswith(prefix)
    ]

  return [
    field_from_var(Var(f.name), schema) for f in fields
  ]


def field_from_const(expr):
  return Field(
    name ='?column?',
    type = {
      NumberConst: 'INTEGER',
      StringConst: 'STRING',
      BoolConst: 'BOOLEAN'
    }[type(expr)]
  )


def field_from_var(var_expr, schema):
  return schema[var_expr.path]

def field_from_function(function_expr, dataset, schema):
  name = function_expr.name
  function = dataset.get_function(function_expr.name)

  if function.returns:
    return function.returns
  #elif len(function_expr.args):
  #  # no return type specified guess the type based on the first
  #  # argument. Dataset.add_function should prevent functions
  #  # from being registered without args and return_types
  #  return field_from_expr(function_expr.args[0], dataset, schema)
  else:
    raise ValueError("Can not determine return type of Function {}".format(name))



def field_from_rename_op(expr, dataset, schema):
  field = field_from_expr(expr.expr, dataset, schema)
  return field.new(name=expr.name)

op_type_to_schemas = {
  LoadOp: schema_from_load,
  ProjectionOp: schema_from_projection_op,
  AliasOp: schema_from_alias_op,
  JoinOp: schema_from_join_op,
  Function: schema_from_function_op
}
########NEW FILE########
__FILENAME__ = table
from .schema import Schema

class Table(object):
  def __init__(self, adapter, name, schema):
    """

    Initialize a table with a name or a schema.

    Args:
      name (str):  The name of the table.
      schema (Schema | dict): Schema for the table specified as a dict
      or instance of the Schema class.

    """


    self.adapter = adapter
    self.name = name

    if isinstance(schema, dict):
      schema = Schema(**schema)
    self.schema = schema

  @property
  def fields(self):
    return self.schema.fields


  def __iter__(self):
    """
    Returns an iterator of tuples. 

    Table implementations from remote adapters should use qualifiers
    and columns to reduce the set of results if they can. This will
    save network bandwidth or disk load time. Splicer will filter and 
    take the columns it needs from the rows returned by this method 
    regardless so adapters without filtering capabilities are free
    to return the data as is.
    """
    return iter([])
########NEW FILE########
__FILENAME__ = employee_adapter
from splicer.adapters.dict_adapter import DictAdapter
from datetime import date

employee_records = [
  dict(
    employee_id=1234, 
    full_name="Tom Tompson", 
    employment_date=date(2009,1,17)
  ),
  dict(
    employee_id=4567, 
    full_name="Sally Sanders",
    employment_date=date(2010,2,24),
    manager_id = 1234
  ),
  dict(
    employee_id=8901, 
    full_name="Mark Markty",
    employment_date=date(2010,3,1),
    manager_id = 1234,
    roles = ('sales', 'marketing')
  )
]

class EmployeeAdapter(DictAdapter):
  def __init__(self):
    super(self.__class__, self).__init__(
      employees = dict(
        schema = dict(
          fields=[
            dict(name="employee_id", type="INTEGER"),
            dict(name="full_name", type="STRING"),
            dict(name="employment_date", type="DATE"),
            dict(name="manager_id", type="INTEGER"),
            dict(name="roles", type="STRING", mode="REPEATED")
          ]
        ),
        rows = employee_records
      )
    )




########NEW FILE########
__FILENAME__ = mock_adapter
from splicer import Table
from splicer.adapters import Adapter

class MockAdapter(Adapter):
  def __init__(self):
    
    table = Table(
      adapter = self,
      name = 'bogus',
      schema = dict(
        fields = [
          dict(
            name='x',
            type='INTEGER'
          ),
          dict(
            name='y',
            type='INTEGER'
          )
        ]
      )
    )

    self._tables = {
      table.name: table  
    }

  @property
  def relations(self):
    return [
      (name, table.schema)
      for name, table in self._tables.items()
    ]

  def has(self, name):
    return self._tables.has_key(name)

  def get_relation(self, name):
    return self._tables.get(name)

########NEW FILE########
__FILENAME__ = test_codec
from nose.tools import *

from splicer import Relation, Schema, Field
from splicer import codecs

from StringIO import StringIO

def test_register_decoder():

  @codecs.decodes('text/plain')
  def decoder(stream):

    return Relation(
      Schema([dict(name="line", type="STRING")]),
      (
        (line.rstrip(),)
        for line in stream
      )
    )

  stream = StringIO("blah\nfoo,1\nbaz")
  relation = codecs.relation_from(stream, mime_type='text/plain')

  eq_(
    relation.schema.fields,
    [Field(name='line', type='STRING')]
  )

  eq_(
    list(relation),
    [
      ('blah',),
      ('foo,1',),
      ('baz',)
    ]
  )

def test_decode_csv():
  stream = StringIO("field1,field2,field3\nfoo,1,0\nbaz,2,0")
  relation = codecs.relation_from(stream, mime_type='text/csv')
  eq_(
    relation.schema.fields,
    [
      Field(name='field1', type='STRING'),
      Field(name='field2', type='STRING'),
      Field(name='field3', type='STRING')
    ]
  )

  assert_sequence_equal(
    list(relation),
    [
      ['foo','1','0'],
      ['baz','2','0']
    ]
  )




########NEW FILE########
__FILENAME__ = test_compiler
from datetime import date

from nose.tools import *

from splicer import DataSet, Query
from splicer.ast import *
from splicer.compilers.local import compile

from .fixtures.employee_adapter import EmployeeAdapter
  

def test_projection():
  dataset = DataSet()
  dataset.add_adapter(EmployeeAdapter())

  q = Query(
    dataset,  
    ProjectionOp(LoadOp('employees'), Var('full_name'))
  )
  evaluate = compile(q)

  assert_sequence_equal(
    list(evaluate(dict(dataset=dataset))),
    [('Tom Tompson',), ('Sally Sanders',), ('Mark Markty',)]
  )

def test_projection_wo_relation():
  """
  This is equivalent to select statements w/o from clauses in sql adapters.
  select 1;

  | col1 |
  +------+
  |   1  |
  +------+
  """
  dataset = DataSet()

  q = Query(
    dataset,  
    ProjectionOp(LoadOp(''), NumberConst(1))
  )

  evaluate = compile(q)
  
  assert_sequence_equal(
    list(evaluate(dict(dataset=dataset))),
    [(1,)]
  )




def test_selection():
  dataset = DataSet()
  dataset.add_adapter(EmployeeAdapter())

  q = Query(
    dataset,
    SelectionOp(
      LoadOp('employees'), 
      EqOp(Var('manager_id'), NullConst())
    )
  )
  
  evaluate = compile(q)

  assert_sequence_equal(
    list(evaluate(dict(dataset=dataset))),
    [
      (1234, 'Tom Tompson', date(2009, 1, 17), None, ()),
    ]
  )


  q = Query(
    dataset, 
    SelectionOp(
      LoadOp('employees'),
      NotOp(EqOp(Var('manager_id'), NullConst()))
    )
  )
  
  evaluate = compile(q)

  assert_sequence_equal(
    list(evaluate(dict(dataset=dataset))),
    [
      (4567, 'Sally Sanders', date(2010, 2, 24), 1234, ()),
      (8901, 'Mark Markty', date(2010, 3, 1), 1234, ('sales', 'marketing'))
    ]
  )


def test_addition():
  dataset = DataSet()
  dataset.add_adapter(EmployeeAdapter())


  q = Query(
    dataset, 
    ProjectionOp(LoadOp('employees'), AddOp(Var('employee_id'), NumberConst(1)))
  )
  evaluate = compile(q)

  assert_sequence_equal(
    list(evaluate(dict(dataset=dataset))),
    [(1235,), (4568,), (8902,)]
  )

def test_order_by():
  dataset = DataSet()
  dataset.add_adapter(EmployeeAdapter())


  q = Query(
    dataset, 
    OrderByOp(LoadOp('employees'), Var('full_name'))
  )
  evaluate = compile(q)

  assert_sequence_equal(
    list(evaluate(dict(dataset=dataset))),
    [
      (8901, 'Mark Markty', date(2010, 3, 1), 1234, ('sales', 'marketing')),
      (4567, 'Sally Sanders', date(2010, 2, 24), 1234, ()),
      (1234, 'Tom Tompson', date(2009, 1, 17), None, ()),
    ]
  )

def test_order_by_asc():
  dataset = DataSet()
  dataset.add_adapter(EmployeeAdapter())


  q = Query(
    dataset, 
    OrderByOp(LoadOp('employees'), Asc(Var('employee_id')))
  )
  evaluate = compile(q)

  assert_sequence_equal(list(evaluate(dict(dataset=dataset)))
    ,
    [
      (1234, 'Tom Tompson', date(2009, 1, 17), None, ()),
      (4567, 'Sally Sanders', date(2010, 2, 24), 1234, ()),
      (8901, 'Mark Markty', date(2010, 3, 1), 1234, ('sales', 'marketing')),
    ]
  )

def test_function_calls():
  dataset = DataSet()
  dataset.add_adapter(EmployeeAdapter())

  dataset.add_function(
    name = 'initials', 
    function = lambda name: ''.join([p[0] for p in name.split()]) if name else None,
    returns = dict(name="initials", type="STRING")
  )


  q = Query(
    dataset, 
    ProjectionOp(LoadOp('employees'), Function('initials', Var('full_name')))
  )
  evaluate = compile(q)

  assert_sequence_equal(
    list(evaluate(dict(dataset=dataset))),
    [
      ('TT',),
      ('SS',),
      ('MM',),
    ]
  )


def test_decorator_function_calls():
  dataset = DataSet()
  dataset.add_adapter(EmployeeAdapter())

  @dataset.function(returns=dict(name="initials", type="STRING"))
  def initials(name):
    if name:
      return ''.join([p[0] for p in name.split()])
    else:
      return None

 

  q = Query(
    dataset,  
    ProjectionOp(LoadOp('employees'), Function('initials', Var('full_name')))
  )
  evaluate = compile(q)

  assert_sequence_equal(
    list(evaluate(dict(dataset=dataset))),
    [
      ('TT',),
      ('SS',),
      ('MM',),
    ]
  )

def test_aggregation_whole_table():
  dataset = DataSet()
  dataset.add_adapter(EmployeeAdapter())

  q = Query(
    dataset,  
    ProjectionOp(LoadOp('employees'), Function('count'))
  )
  evaluate = compile(q)

  assert_sequence_equal(
    list(evaluate(dict(dataset=dataset))),
    [
      (3,),
    ]
  )


def test_aggregation_on_column():
  dataset = DataSet()
  dataset.add_adapter(EmployeeAdapter())

  q = Query(
    dataset, 
    GroupByOp(

      ProjectionOp(LoadOp('employees'), Var('manager_id'), Function('count')), 
      Var('manager_id')
    )
  )
  evaluate = compile(q)

  assert_sequence_equal(
    list(evaluate(dict(dataset=dataset))),
    [
      (None,1),
      (1234,2)
    ]
  )

def test_limit():
  dataset = DataSet()
  dataset.add_adapter(EmployeeAdapter())

  q = Query(
    dataset, 
    SliceOp(LoadOp('employees'), 1)
  )
  evaluate = compile(q)

  assert_sequence_equal(
    list(evaluate(dict(dataset=dataset))),
    [
      (1234, 'Tom Tompson', date(2009, 1, 17), None, ()),
    ]
  )

def test_offset():
  dataset = DataSet()
  dataset.add_adapter(EmployeeAdapter())

  q = Query(
    dataset, 
    SliceOp(LoadOp('employees'), 1,None)
  )
  evaluate = compile(q)

  assert_sequence_equal(
    list(evaluate(dict(dataset=dataset))),
    [
      (4567, 'Sally Sanders', date(2010, 2, 24), 1234, ()),
      (8901, 'Mark Markty', date(2010, 3, 1), 1234, ('sales', 'marketing')),
 
    ]
  )

def test_offset_and_limit():
  dataset = DataSet()
  dataset.add_adapter(EmployeeAdapter())

  q = Query(
    dataset, 
    SliceOp(LoadOp('employees'), 1,2)
  )
  evaluate = compile(q)

  assert_sequence_equal(
    list(evaluate(dict(dataset=dataset))),
    [
      (4567, 'Sally Sanders', date(2010, 2, 24), 1234, ()),
    ]
  )


def test_cross_join():
  dataset = DataSet()
  dataset.add_adapter(EmployeeAdapter())

  q = Query(
    dataset, 
    JoinOp(LoadOp('employees'), LoadOp('employees'))
  )
  evaluate = compile(q)
  eq_(
    len(list(evaluate(dict(dataset=dataset)))),
    9
  )

def test_self_join():
  dataset = DataSet()
  dataset.add_adapter(EmployeeAdapter())


  q = Query(
    dataset,  
    JoinOp(
      AliasOp('employee', LoadOp('employees')),
      AliasOp('manager', LoadOp('employees')),
      EqOp(Var('manager.employee_id'), Var('employee.manager_id'))
    )
  )
  evaluate = compile(q)



  assert_sequence_equal(
    list(evaluate(dict(dataset=dataset))),
    [
      (4567, 'Sally Sanders', date(2010, 2, 24), 1234, (), 1234, 'Tom Tompson', date(2009, 1, 17), None, ()),
      (8901, 'Mark Markty', date(2010, 3, 1), 1234, ('sales', 'marketing'), 1234, 'Tom Tompson', date(2009, 1, 17), None, ())
    ]
  )

def test_self_join_with_projection():
  dataset = DataSet()
  dataset.add_adapter(EmployeeAdapter())

  q = Query(
    dataset,  
    ProjectionOp(
      JoinOp(
        AliasOp('manager', LoadOp('employees')),
        AliasOp('employee', LoadOp('employees')),
        EqOp(Var('manager.employee_id'), Var('employee.manager_id'))
      ),
      SelectAllExpr('employee'),
      RenameOp('manager', Var('manager.full_name'))
    )
  )

  evaluate = compile(q)



  assert_sequence_equal(
    list(evaluate(dict(dataset=dataset))),
    [
      (4567, 'Sally Sanders', date(2010, 2, 24), 1234, (), 'Tom Tompson'),
      (8901, 'Mark Markty', date(2010, 3, 1), 1234,  ('sales', 'marketing'), 'Tom Tompson')
    ]
  )


def test_function_in_from():
  dataset = DataSet()
  dataset.add_adapter(EmployeeAdapter())

  q = Query(
    dataset,  
    ProjectionOp(
      Function('flatten', LoadOp('employees'), StringConst('roles')),
      Var('manager_id'),
      Var('roles')
    )
  )

  evaluate = compile(q)

  assert_sequence_equal(
    list(evaluate(dict(dataset=dataset))),
    [
      (1234, 'sales'),
      (1234, 'marketing')
    ]
  )

  q = Query(
    dataset, 
    Function('flatten', LoadOp('employees'), StringConst('roles'))
  )

  evaluate = compile(q)

  assert_sequence_equal(
    list(evaluate(dict(dataset=dataset))),
    [
      [8901, 'Mark Markty', date(2010, 3, 1), 1234, 'sales'],
      [8901, 'Mark Markty', date(2010, 3, 1), 1234, 'marketing']
    ]
  )





########NEW FILE########
__FILENAME__ = test_dataset
from nose.tools import *

from splicer import DataSet, Table, Query
from splicer.query_builder import QueryBuilder

from splicer.ast import LoadOp, ProjectionOp, Var

from .fixtures.mock_adapter import MockAdapter



def test_get_relation():
  dataset = DataSet()

  adapter = MockAdapter()
  dataset.add_adapter(adapter)

  s_table = adapter.get_relation('bogus')

  table = dataset.get_relation('bogus')
  eq_(table, s_table)

  assert_sequence_equal(
    dataset.relations, 
    [('bogus', s_table)]
  )

def test_query_builder():
  dataset = DataSet()
  adapter = MockAdapter()
  dataset.add_adapter(adapter)
  query = dataset.select('x')

  eq_(isinstance(query, QueryBuilder), True)
  eq_(query.dataset, dataset)
  eq_(query.column_exps, 'x')

def test_complier():
  adapter = MockAdapter()

  def compile(query):
    return lambda ctx, *params: Table(
      adapter,
      'results!', 
      schema = dict(
        fields = [
          dict(name="?column?", type="INTEGER")
        ]
      )
    )


  dataset = DataSet()
  dataset.add_adapter(adapter)
  dataset.set_compiler(compile)

  query = dataset.frm('bogus').query

  table = dataset.execute(query)

def test_query():
  dataset = DataSet()
  dataset.query('select 1').execute()


def test_views():
  dataset = DataSet()
  dataset.add_adapter(MockAdapter())


  # create a view off of an existing table
  dataset.select('x').frm('bogus').create_view('only_x')

  view = dataset.get_view('only_x')

  eq_(
    view,
    ProjectionOp(LoadOp('bogus'), Var('x'))
  )

  # create a view off of a view
  dataset.select('x').frm('only_x').create_view('only_x_from_x')

  view = dataset.get_view('only_x_from_x')

  eq_(
    view,
    # Todo: Implement a query optimizer that eliminates
    # redunant projections ops like the one we see below
    ProjectionOp(
      ProjectionOp(LoadOp('bogus'), Var('x')),
      Var('x')
    )
  )




########NEW FILE########
__FILENAME__ = test_dict_adapter
from datetime import date
from nose.tools import *

from splicer import Query, Schema, Field
from splicer.adapters.dict_adapter import DictAdapter


employee_records = [
  dict(
    employee_id=1234, 
    full_name="Tom Tompson", 
    employment_date=date(2009,1,17)
  ),
  dict(
    employee_id=4567, 
    full_name="Sally Sanders",
    employment_date=date(2010,2,24),
    manager_id = 1234
  ),
  dict(
    employee_id=8901, 
    full_name="Mark Markty",
    employment_date=date(2010,3,1),
    manager_id = 1234
  )
]

def test_dict_adapter_with_schemas():
  adapter = DictAdapter(
    employees = dict(
      schema = dict(
        fields=[
          dict(name="employee_id", type="INTEGER"),
          dict(name="full_name", type="STRING"),
          dict(name="employment_date", type="DATE"),
          dict(name="manager_id", type="INTEGER")
        ]
      ),
      rows = employee_records
    )
  )

  employees = adapter.get_relation('employees')

  assert_sequence_equal(
    employees.schema.fields,
    [
      Field(name="employee_id", type="INTEGER"),
      Field(name="full_name", type="STRING"),
      Field(name="employment_date", type="DATE"),
      Field(name="manager_id", type="INTEGER")  
    ]
  )

  assert_sequence_equal(
    list(employees),
    [
      (1234, 'Tom Tompson', date(2009, 1, 17), None),
      (4567, 'Sally Sanders', date(2010, 2, 24), 1234),
      (8901, 'Mark Markty', date(2010, 3, 1), 1234)
    ]
  )
########NEW FILE########
__FILENAME__ = test_dir_adapter
import os
import tempfile
import shutil

from nose.tools import *

from splicer.ast import *
from splicer.operations import query_zipper
from splicer.adapters.dir_adapter import DirAdapter


def setup_func():
  global path
  path = tempfile.mkdtemp()


def teardown_func():
  global path
  try:
    shutil.rmtree(path)
  finally:
    path =None


@with_setup(setup_func, teardown_func)
def test_evaluate():
  
  adapter = DirAdapter(
    songs = dict(
      root_dir = path,
      pattern = "{artist}/{album}/{track}.{ext}",
      filename_column="path",
    )
  )
  relation = adapter.get_relation('songs')

  op = LoadOp('songs')
  loc = query_zipper(op).leftmost_descendant()
  
  res = adapter.evaluate(loc)

  eq_(
    res.root(),
    Function(
      'extract_path',
      Function('files', Const(relation.root_dir)),
      Const(path + "/{artist}/{album}/{track}.{ext}")
    )
  )


def test_query_field_in_payload():
  """
  Querying a field inside the payload should result
  in the LoadOp being rewritten as

  SelectionOp(Function('decode', Function('extract_path', Function('files'))))
  """

  adapter = DirAdapter(
    employees = dict(
      root_dir = "/",
      pattern = "{department}",
      filename_column="path",
      decode = "auto",
      schema = dict(fields=[
        dict(type='STRING', name='department'),
        dict(type='INTEGER', name='id'),
        dict(type='STRING', name='full_name'),
        dict(type='INTEGER', name='salary'),
        dict(type='INTEGER', name='manager_id'),
      ])
    )
  )


  op = SelectionOp(LoadOp('employees'), GeOp(Var('salary'), Const(40000)))

  loc = query_zipper(op).leftmost_descendant()
  
  res = adapter.evaluate(loc)
  relation = adapter.get_relation('employees')


 
  eq_(
    res.root(),
    SelectionOp(
      Function(
        'decode',
        Function(
          'extract_path',
          Function('files', Const(relation.root_dir)),
          Const(relation.root_dir + "{department}")
        ),
        Const('auto')
      ),
      GeOp(Var('salary'), Const(40000))
    )
  )

def test_query_field_from_path():
  """
  Queries with SelectionOps that reference only fields
  parsed from the directory structre will rewrite
  the query so that the file list is filtered before 
  opening/decoding the files.
  """

  adapter = DirAdapter(
    employees = dict(
      root_dir = "/",
      pattern = "{department}",
      filename_column="path",
      decode = "auto",
      schema = dict(fields=[
        dict(type='STRING', name='department'),
        dict(type='INTEGER', name='id'),
        dict(type='STRING', name='full_name'),
        dict(type='INTEGER', name='salary'),
        dict(type='INTEGER', name='manager_id'),
      ])
    )
  )


  op = SelectionOp(LoadOp('employees'), EqOp(Var('department'), Const('sales')))

  loc = query_zipper(op).leftmost_descendant()
  
  res = adapter.evaluate(loc)
  relation = adapter.get_relation('employees')

  
 
  eq_(
    res.root(),
    Function(
      'decode',
      SelectionOp(
        Function(
          'extract_path',
          Function('files', Const(relation.root_dir)),
          Const(relation.root_dir + "{department}")
        ),
        EqOp(Var('department'), Const('sales'))
      ),
      Const('auto')
    )
  )


def test_query_field_from_path_and_contents():
  """
  Queries with SelectionOps that reference both  fields
  parsed from the directory structre and content will 
  rewrite the query so that the file list is filtered before 
  opening/decoding the files and finally filtered by the field
  from the content
  """

  adapter = DirAdapter(
    employees = dict(
      root_dir = "/",
      pattern = "{department}",
      filename_column="path",
      decode = "auto",
      schema = dict(fields=[
        dict(type='STRING', name='department'),
        dict(type='INTEGER', name='id'),
        dict(type='STRING', name='full_name'),
        dict(type='INTEGER', name='salary'),
        dict(type='INTEGER', name='manager_id'),
      ])
    )
  )


  op = SelectionOp(
    LoadOp('employees'),
    And(
      EqOp(Var('department'), Const('sales')),
      GeOp(Var('salary'), Const(40000)),
    )
  )

  loc = query_zipper(op).leftmost_descendant()
  
  res = adapter.evaluate(loc)
  relation = adapter.get_relation('employees')

  
 
  eq_(
    res.root(),
    SelectionOp(  
      Function(
        'decode',
        SelectionOp(
          Function(
            'extract_path',
            Function('files', Const(relation.root_dir)),
            Const(relation.root_dir + "{department}")
          ),
          EqOp(Var('department'), Const('sales'))
        ),
        Const('auto')
      ),
      GeOp(Var('salary'), Const(40000))
    )
  )

########NEW FILE########
__FILENAME__ = test_field
# test_schema_interpreter.py

from nose.tools import *


from splicer.field import Field


def test_to_dict():

  assert_dict_equal(
    Field(name='x', type="STRING").to_dict(),
    dict(name='x', type='STRING', mode="NULLABLE", fields=[])
  )

########NEW FILE########
__FILENAME__ = test_filesystem
import os
import shutil
import tempfile

from nose.tools import *

from splicer.functions.filesystem import files, extract_path, decode, contents
from splicer import Relation, Schema

def setup_func():
  global path
  path = tempfile.mkdtemp()


def teardown_func():
  global path
  try:
    shutil.rmtree(path)
  finally:
    path = None


@with_setup(setup_func, teardown_func)
def test_files():
  for artists in range(3):
    for album in range(3):
      album_path = os.path.join(
        path,
        'artist{}/album{}/'.format(artists, album)
      )
      os.makedirs(album_path)

      for track in range(4):
        open(os.path.join(album_path,"{}.ogg".format(track)), 'w')

  songs = files(path)

  items = sorted(songs)

  eq_(len(items), 36)

  assert_sequence_equal(
    items,
    [
      ("{}/artist{}/album{}/{}.ogg".format(path, artist,album,track),)
      for artist in range(3)
      for album in range(3)
      for track in range(4)
    ]
  )


from splicer.path import pattern_regex
def test_extract_path():
  r = Relation(
    Schema([dict(type="STRING", name="path")]),
    iter((
      ('/Music/Nirvana/Nevermind/Smells Like Teen Spirit.ogg',),
      ('/Videos/Electric Boogaloo.mp4',)
    ))
  )


  assert_sequence_equal(
    list(extract_path(r, "/Music/{artist}/{album}/{track}.{ext}")),
    [
      (
        '/Music/Nirvana/Nevermind/Smells Like Teen Spirit.ogg', 
        'Nirvana',
        'Nevermind', 
        'Smells Like Teen Spirit',
        'ogg'
      )
    ]
  )

@with_setup(setup_func, teardown_func)
def test_contents():
  p = os.path.join(path, 'test.csv')
  open(p,'w').write('field1,field2\n1,2\n')

  r = Relation(
    Schema([dict(type="STRING", name="path")]),
    iter((
      (p,),
    ))
  )

  assert_sequence_equal(
    list(contents(r,  'path')),
    [
      (p, 'field1,field2\n1,2\n')
    ]
  )

@with_setup(setup_func, teardown_func)
def test_decode():
  p = os.path.join(path, 'test.csv')
  open(p,'w').write('field1,field2\n1,2\n')

  r = Relation(
    Schema([dict(type="STRING", name="path")]),
    iter((
      (p,),
    ))
  )

  assert_sequence_equal(
    list(decode(r, 'auto', 'path')),
    [
      (p, '1', '2')
    ]
  )
########NEW FILE########
__FILENAME__ = test_join
from sys import getsizeof

from nose.tools import *

from splicer import Relation, Schema
from splicer.compilers.join import (
  nested_block_join,
  buffered,
  record_size,
  join_keys,
  join_keys_expr,
  hash_join
)
from splicer.ast import EqOp, And, Var, NumberConst


def t1(ctx=None):
  return Relation(
    Schema(name="t1", fields=[dict(name='x', type="INTEGER")]),
    iter((
      (1,),
      (2,)
    ))
  )

def t2(ctx=None): 
  return Relation(
    Schema(
      name="t2", 
      fields=[
        dict(name='y', type="INTEGER"),
        dict(name='z', type="INTEGER")
      ]
    ),
    iter((
      (1,0),
      (1,1),
      (3,1)
    ))
  )
 

def test_record_size():
  t = (1,2, "blah")
  eq_(
    record_size(t),
    getsizeof(t) + getsizeof(t[0]) + getsizeof(t[1]) + getsizeof(t[2])
  )

def test_buffering():
  t = (1,2, "blah")
  sz = record_size(t)

  l = (t,) * 10

  eq_(
    len(list(buffered(l, sz))),
    10
  )

  eq_(
    len(list(buffered(l, sz*2))),
    5
  )

  eq_(
    len(list(buffered(l, sz*10))),
    1
  )

def test_nested_block_join():
  r = (
    (1,),
    (2,),
    (3,),
  )

  s = (
    ('a',),
    ('b',),
    ('c',),
  )

  j = tuple(nested_block_join(
    lambda ctx: iter(r),
    lambda ctx: iter(s),
    lambda r,ctx: True, 
    {}
  ))

  assert_sequence_equal(
    j,
    (
      (1, 'a'),
      (2, 'a'),
      (3, 'a'),
      (1, 'b'),
      (2, 'b'),
      (3, 'b'),
      (1, 'c'),
      (2, 'c'),
      (3, 'c'),
    )
  )


def test_join_key_expr():

  simple_op = EqOp(Var('t1.x'), Var('t2.y'))
  ((left_key, right_key),) = join_keys_expr(t1(),t2(), simple_op)

  ctx = None
  eq_(
    left_key((1,), ctx),
    1
  )

  eq_(
    right_key((1,0), ctx),
    1
  )

  simple_op = EqOp(Var('t2.y'), Var('t1.x'))
  ((left_key, right_key),) = join_keys_expr(t1(),t2(), simple_op)

  ctx = None
  eq_(
    left_key((1,), ctx),
    1
  )

  eq_(
    right_key((1,0), ctx),
    1
  )

  simple_op = EqOp(Var('t1.x'), NumberConst(1))
  assert_raises(ValueError, join_keys_expr,t1(),t2(), simple_op)



def test_join_keys():
  
  simple_op = EqOp(Var('t1.x'), Var('t2.y'))

  left_key, right_key = join_keys(t1(),t2(), simple_op)

  ctx = None
  eq_(
    left_key((1,), ctx),
    (1,)
  )

  eq_(
    right_key((1,0), ctx),
    (1,)
  )

  multi_key = And(
    EqOp(Var('t1.x'), Var('t2.y')),
    EqOp(Var('t1.x'), Var('t2.z')),
  )

  left_key, right_key = join_keys(t1(),t2(), multi_key)

  eq_(
    left_key((1,), ctx),
    (1,1)
  )

  eq_(
    right_key((1,0), ctx),
    (1,0)
  )

def test_hash_join():
  
  simple_op = EqOp(Var('t1.x'), Var('t2.y'))
  ctx = {}

  comparison = join_keys(t1(),t2(), simple_op)
  j = tuple(hash_join(t1,t2, comparison, ctx))

  eq_(
    j,
    (
      (1,1,0),
      (1,1,1),
    )
  )

  multi_key = And(
    EqOp(Var('t1.x'), Var('t2.y')),
    EqOp(Var('t1.x'), Var('t2.z')),
  )

  comparison = join_keys(t1(),t2(), multi_key)
  j = tuple(hash_join(t1,t2, comparison, ctx))

  eq_(
    j,
    (
      (1,1,1),
    )
  )

########NEW FILE########
__FILENAME__ = test_operations
# test_operations.py
from nose.tools import *

from splicer import DataSet, Relation, Schema
from splicer.operations import walk, replace_views
from splicer.ast import *

from .fixtures.employee_adapter import EmployeeAdapter


def test_walk():
  op = ProjectionOp(LoadOp('employees'), Var('full_name'))

  eq_(
    walk(op, lambda node: node),
    op
  )

def test_relpace_views():
  dataset = DataSet()
  dataset.add_adapter(EmployeeAdapter())

  no_managers = SelectionOp(
    LoadOp('employees'),
    IsOp(Var('manager_id'), NullConst())
  )

  dataset.create_view(
    'no_managers',
    no_managers
  )

  eq_(
    replace_views(LoadOp('no_managers'), dataset),
    no_managers
  )

  eq_(
    replace_views(
      JoinOp(
        LoadOp('no_managers'),
        LoadOp('no_managers')
      ),
      dataset
    ),

    JoinOp(
      no_managers,
      no_managers
    )

  )


########NEW FILE########
__FILENAME__ = test_ops
from nose.tools import *

from splicer import Schema
from .fixtures import mock_data_set


def __test_projection():
  employees = mock_data_set().get_relation('employees')
  op = relational_ops.ProjectOp(
    Schema([dict(
      name="employee_id",
      type="INTEGER"
    )]),
    lambda row, ctx: row['employee_id']
  )


  rows = employees.rows(None, None)

  assert_sequence_equal(
    list(op(rows, None)),
    [(1234,), (4567,), (8901,)]
  )

def __test_selection():
  employees = mock_data_set().get_relation('employees')
  op = relational_ops.SelectionOp(employees.schema)

  assert_sequence_equal(
    list(op(employees, None)),
    list(employees.rows(None, None))
  )
  
########NEW FILE########
__FILENAME__ = test_path
from nose.tools import *

from splicer.path import tokenize_pattern, regex_str, pattern_regex

def test_tokenize():
  eq_(
    list(tokenize_pattern("{artist}/{album}/{track}.{ext}")),
    ['{artist}','/','{album}','/','{track}', '.','{ext}'] 
  )

  eq_(
    list(tokenize_pattern("{artist}/{album}/{track}-{date}.{ext}")),
    ['{artist}','/','{album}','/','{track}', '-', '{date}', '.','{ext}'] 
  )

def test_compile_regex_str():
  eq_(
    regex_str(['{artist}']),
    '(?P<artist>[^/]*)'
  )

  eq_(
    regex_str(['{artist}', '/','ignored', '/', '{foo}']),
    r'(?P<artist>[^/]*)\/ignored\/(?P<foo>[^/]*)'
  )

def test_pattern_regex():
  regex, columns = pattern_regex("{artist}/{album}/{track}.{ext}")

  eq_(
    regex.match("nirvana/nevermind/02.ogg").groups(),
    ("nirvana", "nevermind", "02", "ogg")
  )



########NEW FILE########
__FILENAME__ = test_query_builder
from nose.tools import *

from splicer import Query, Schema, Field
from splicer.query_builder import  QueryBuilder
from splicer.ast import *


from .fixtures import mock_data_set


def test_execute():
  dataset = mock_data_set()
  qb = QueryBuilder(dataset).frm('employees')
  eq_(
    list(qb.execute()),
    list(dataset.frm('employees').execute())
  )


def test_from_bogus():
  dataset = mock_data_set()
  qb = QueryBuilder(dataset)

  qb_w_from = qb.frm('bogus')
  # ensure we maintain immutability by only returning
  # nev versions
  assert_is_not(qb, qb_w_from)


  #eq_(qb_w_from.relation_name, 'bogus')

  q  = qb_w_from.query
  assert_is_instance(q, Query)

  assert_equal(
    q.schema,
    dataset.get_schema('bogus')
  )

  assert_equal(
    q.operations,
    LoadOp('bogus')
  )


def test_select():
  dataset = mock_data_set()

  qb = QueryBuilder(dataset)
  eq_(qb.column_exps, '*')
  qb_w_select = qb.select('x,y')

  assert_is_not(qb, qb_w_select)
  eq_(qb_w_select.column_exps, 'x,y')

  qb_w_select_and_from = qb_w_select.frm('bogus')

  q =  qb_w_select_and_from.query

  assert_is_instance(q, Query)

  assert_sequence_equal(
    q.schema.fields, 
    [
      Field(name="x", type="INTEGER"),
      Field(name="y", type="INTEGER")
    ]
  )

  assert_equal(
    q.operations,
    ProjectionOp(LoadOp('bogus'), Var('x'), Var('y'))
  )

  qb_select_y_from_bogus = qb.select('y').frm('bogus')
  eq_(qb_select_y_from_bogus.column_exps, 'y')

  assert_sequence_equal(
    qb_select_y_from_bogus.query.schema.fields, 
    [
      Field(name="y", type="INTEGER")
    ]
  )

  assert_equal(
    qb_select_y_from_bogus.query.operations,
    ProjectionOp(LoadOp('bogus'),Var('y'))
  )


def test_where():
  dataset = mock_data_set()
  qb = QueryBuilder(dataset).frm('employees').where('employee_id = 123')

  assert_equal(
    qb.query.operations,
    SelectionOp(
      LoadOp('employees'),
      EqOp(Var('employee_id'), NumberConst(123))
    )
  )

def test_projection_and_selection():
  dataset = mock_data_set()

  qb = QueryBuilder(dataset).select(
    'full_name'
  ).frm('employees').where('employee_id = 123')

  query = qb.query
  assert_equal(
    query.operations,
      ProjectionOp(
        SelectionOp(
          LoadOp('employees'), 
          EqOp(Var('employee_id'), NumberConst(123))
        ),
        Var('full_name')
      ) 
  )

  assert_sequence_equal(
    query.schema.fields, 
    [
      Field(name="full_name", type="STRING")
    ]
  )


def test_order_by():
  dataset = mock_data_set()
  qb = QueryBuilder(dataset).frm('employees').order_by('employee_id')

  assert_equal(
    qb.query.operations,
    OrderByOp(LoadOp('employees'), Var('employee_id'))
  )

def test_order_by_multiple():
  dataset = mock_data_set()
  qb = QueryBuilder(dataset).frm('employees').order_by(
    'employee_id, full_name Desc, 123'
  )

  assert_equal(
    qb.query.operations,
    OrderByOp(
      LoadOp('employees'),
      Var('employee_id'), Desc(Var('full_name')), NumberConst(123)
    )
  )

def test_group_by():
  dataset = mock_data_set()

  qb = QueryBuilder(dataset).select(
    'manager_id, count()'
  ).frm(
    'employees'
  ).group_by(
    'manager_id'
  )

  assert_equal(
    qb.query.operations,    
    GroupByOp(
      ProjectionOp(LoadOp('employees'), Var('manager_id'), Function('count')),
      Var('manager_id')
    )
  )


def test_all_aggregates():
  """Aggregation is possible if all the selected columns are aggregate functions"""

  dataset = mock_data_set()

  qb = QueryBuilder(dataset).select(
    'count()'
  ).frm(
    'employees'
  )

  assert_equal(
    qb.query.operations,
    GroupByOp(
      ProjectionOp(LoadOp('employees'), Function('count'))
    )
  )

def test_all_count_aliased():
  """Aggregation is possible if all the selected columns are aggregate functions"""

  dataset = mock_data_set()

  qb = QueryBuilder(dataset).select(
    'count() as total'
  ).frm(
    'employees'
  )

  assert_equal(
    qb.query.operations,
    GroupByOp(
      ProjectionOp(LoadOp('employees'), RenameOp('total', Function('count')))
    )
  )

def test_limit():

  dataset = mock_data_set()

  qb = QueryBuilder(dataset).frm(
    'employees'
  ).limit(1)


  assert_equal(
    qb.query.operations,
    SliceOp(LoadOp('employees'), None,1)
  )

def test_offset():

  dataset = mock_data_set()

  qb = QueryBuilder(dataset).frm(
    'employees'
  ).offset(1)

  assert_equal(
    qb.query.operations,
    SliceOp(LoadOp('employees'), 1,None)
  )

def test_offset_and_limit():

  dataset = mock_data_set()

  qb = QueryBuilder(dataset).frm(
    'employees'
  ).offset(1).limit(1)

  assert_equal(
    qb.query.operations,
    SliceOp(LoadOp('employees'), 1,2)
  )


def test_join():

  dataset = mock_data_set()

  query = QueryBuilder(dataset).select(
    'employee.*, manager.full_name as manager'
  ).frm(
    'employees as employee'
  ).join(
    'employees as manager', 
    on="manager.employee_id = employee.manager_id"
  ).query

  
  operations = ProjectionOp(
    JoinOp(
      AliasOp('employee',LoadOp('employees')),
      AliasOp('manager',LoadOp('employees')),
      EqOp(Var('manager.employee_id'), Var('employee.manager_id'))  
    ),
    SelectAllExpr('employee'),
    RenameOp('manager', Var('manager.full_name'))
  )

  eq_(query.operations,operations)

def test_join_subselect():
  dataset = mock_data_set()
  managers = QueryBuilder(dataset).select(
    'employee_id as id, full_name as manager'
  ).frm(
    'employees as manager'
  )

  query = QueryBuilder(dataset).select(
    'employee.*,  manager'
  ).frm(
    'employees as employee'
  ).join(
    managers, 
    on="manager.id = employee.manager_id"
  ).query


  operations =  ProjectionOp(
    JoinOp(
      AliasOp('employee',LoadOp('employees')),
      ProjectionOp(
        AliasOp('manager',LoadOp('employees')),
        RenameOp("id", Var("employee_id")),
        RenameOp("manager", Var("full_name"))
      ),
      EqOp(Var('manager.id'), Var('employee.manager_id'))  
    ),
    SelectAllExpr('employee'),
    Var('manager')
  )

  eq_(
    query.operations,
    operations

  )


########NEW FILE########
__FILENAME__ = test_query_parser
from nose.tools import *
from splicer import query_parser

from splicer.ast import *

def test_parse_int():
  ast = query_parser.parse('123')
  assert_is_instance(ast, NumberConst)
  eq_(ast.const, 123)

def test_parse_neg_int():
  ast = query_parser.parse('-123')
  assert_is_instance(ast, NegOp)
  assert_is_instance(ast.expr, NumberConst)
  eq_(ast.expr.const, 123)

def test_parse_not_int():
  ast = query_parser.parse('not 1')
  assert_is_instance(ast, NotOp)
  assert_is_instance(ast.expr, NumberConst)
  eq_(ast.expr.const, 1)

def test_parse_positive_int():
  ast = query_parser.parse('+1')
  assert_is_instance(ast, NumberConst)
  eq_(ast.const, 1)


def test_parse_mul():
  ast = query_parser.parse('1 *2')
  assert_is_instance(ast, MulOp)

def test_parse_div():
  ast = query_parser.parse('1 / 2')
  assert_is_instance(ast, DivOp)  

def test_parse_add():
  ast = query_parser.parse('1  + 2')
  assert_is_instance(ast, AddOp)

def test_parse_string():
  ast = query_parser.parse('"Hi mom"')
  assert_is_instance(ast, StringConst)
  eq_(ast.const, "Hi mom")

def test_parse_variable():
  ast = query_parser.parse('x')
  assert_is_instance(ast, Var)
  eq_(ast.path, "x")

def test_parse_variable_path():
  ast = query_parser.parse('foo.x')
  assert_is_instance(ast, Var)
  eq_(ast.path, "foo.x")

def test_parse_incomplete_variable_path():
  assert_raises(SyntaxError, query_parser.parse, 'foo.')

def test_parse_tuple():
  ast = query_parser.parse('(1,2, "a")')
  assert_is_instance(ast, Tuple)
  eq_(len(ast.exprs), 3)
  assert_is_instance(ast.exprs[0], NumberConst)
  assert_is_instance(ast.exprs[1], NumberConst)
  assert_is_instance(ast.exprs[2], StringConst)

def test_parse_function_no_args():
  ast = query_parser.parse('foo()')
  assert_is_instance(ast, Function)
  eq_(ast.name, "foo")
  eq_(ast.args, ())



def test_parse_incomplete_function():
  assert_raises(SyntaxError, query_parser.parse, 'foo(')

def test_parse_function_with_args():
  ast = query_parser.parse('foo(1)')
  assert_is_instance(ast, Function)
  eq_(len(ast.args), 1)
  assert_is_instance(ast.args[0], NumberConst)
  eq_(ast.args[0].const, 1)

def test_parse_function_with_multiple_args():
  ast = query_parser.parse('foo(1, x, "hi")')
  assert_is_instance(ast, Function)
  eq_(len(ast.args), 3)
  assert_is_instance(ast.args[0], NumberConst)
  assert_is_instance(ast.args[1], Var)
  assert_is_instance(ast.args[2], StringConst)
  eq_(ast.args[0].const, 1)
  eq_(ast.args[1].path, 'x')
  eq_(ast.args[2].const, 'hi')


def test_parse_is():
  ast = query_parser.parse('x is y')
  eq_(
    ast,
    IsOp(Var('x'), Var('y'))
  )

  ast = query_parser.parse('x is not y')
  eq_(
    ast,
    IsNotOp(Var('x'), Var('y'))
  )

def test_parse_invalid_is():
  assert_raises(SyntaxError, query_parser.parse, 'is')

def test_parse_is_null():
  ast = query_parser.parse('x is null')
  eq_(
    ast,
    IsOp(Var('x'), NullConst())
  )


def test_parse_itemgetter():
  ast = query_parser.parse('$0')
  assert_is_instance(ast, ItemGetterOp)
  eq_(ast.key, 0)

  ast = query_parser.parse('$blah')
  assert_is_instance(ast, ItemGetterOp)
  eq_(ast.key, 'blah')

def test_parse_paramgetter():
  ast = query_parser.parse('?0')
  assert_is_instance(ast, ParamGetterOp)
  eq_(ast.expr, 0)

def test_parse_select_core():

  ast = query_parser.parse_select(LoadOp('bogus'), 'x, x as x2, 49929')
  assert_is_instance(ast, ProjectionOp)

  assert_is_instance(ast.exprs[0], Var)
  eq_(ast.exprs[0].path, 'x')
  
  assert_is_instance(ast.exprs[1], RenameOp)
  eq_(ast.exprs[1].name, 'x2')
  assert_is_instance(ast.exprs[1].expr, Var)
  eq_(ast.exprs[1].expr.path, 'x')
  
  assert_is_instance(ast.exprs[2], NumberConst)
  eq_(ast.exprs[2].const, 49929)

def test_parse_select_all():
  ast = query_parser.parse_select(LoadOp('bogus'), '*')
  eq_(ast, LoadOp('bogus'))

def test_parse_select_all_frm_table():
  ast = query_parser.parse_select(LoadOp('table'), 'table.*, x')
  eq_(ast, ProjectionOp(LoadOp('table'), SelectAllExpr('table'), Var('x')))


def test_parse_and():
  ast = query_parser.parse('x = 1 and z=2')
  assert_is_instance(ast, And)

def test_parse_or():
  ast = query_parser.parse('x = 1 or z=2')
  assert_is_instance(ast, Or)

def test_group_by_core_expr():
  ast = query_parser.parse_group_by('x,y')
  eq_(
    ast,
    [Var('x'), Var('y')]
  )

def test_parse_statement():
  ast = query_parser.parse_statement('select 1')

  eq_(
    ast,
    ProjectionOp(LoadOp(''), NumberConst(1))
  )


  ast = query_parser.parse_statement('select 1 from employees')

  eq_(
    ast,
    ProjectionOp(LoadOp('employees'), NumberConst(1))
  )

  ast = query_parser.parse_statement('select full_name from employees')

  eq_(
    ast,
    ProjectionOp(LoadOp('employees'), Var('full_name'))
  )

  ast = query_parser.parse_statement('''
    select full_name from employees as employee
  ''')

  eq_(
    ast,
    ProjectionOp(
      AliasOp('employee',LoadOp('employees')), 
      Var('full_name')
    )
  )

  ast = query_parser.parse_statement('''
    select full_name 
    from employees as employee, employees as manager
  ''')


  eq_(
    ast,
    ProjectionOp(
      JoinOp(
        AliasOp('employee',LoadOp('employees')),
        AliasOp('manager',LoadOp('employees'))
      ), 
      Var('full_name')
    )
  )


  ast = query_parser.parse_statement('''
    select full_name 
    from employees
    where manager_id is not null
  ''')

  eq_(
    ast,
    ProjectionOp(
      SelectionOp(
        LoadOp('employees'),
        IsNotOp(Var('manager_id'), NullConst())
      ), 
      Var('full_name')
    )
  )

  ast = query_parser.parse_statement('''
    select full_name 
    from employees as employee, employees as managers on employee.manager_id = managers.employee_id

  ''')


  ast = query_parser.parse_statement('''
    select manager_id, count() from employees
  ''')

  ast = query_parser.parse_statement('''
    select manager_id, count() from 
    employees
    group by manager_id
  ''')

  ast = query_parser.parse_statement('''
    select manager_id, count() from 
    employees
    group by manager_id
    order by count
  ''')

  op = OrderByOp(
    GroupByOp(
      ProjectionOp(
        LoadOp('employees'),
        Var('manager_id'),
        Function('count')
      ),
      Var('manager_id')
    ),
    Var('count')
  )

  eq_(
    ast,
    op
  )

  ast = query_parser.parse_statement(''' 
    select count() from top_10, top_10
  ''')

  ast = query_parser.parse_statement(''' 
  select t1.* from top_10 as t1
  ''')

  ast = query_parser.parse_statement(''' 
  select manager_id, count() from docs group by manager_id order by count desc limit 10
  ''')


def test_table_functions():

  ast = query_parser.parse_statement(''' 
  select *
  from flatten(docs, 'scripts')
  ''')
  
  op = Function('flatten', LoadOp('docs'), StringConst('scripts'))

  eq_(ast,op)

  ast = query_parser.parse_statement(''' 
  select *
  from flatten(select * from docs, "scripts")
  ''')

  eq_(ast,op)



########NEW FILE########
__FILENAME__ = test_schema_interpreter
# test_schema_interpreter.py

from nose.tools import *

from splicer import schema_interpreter

from splicer.ast import LoadOp, StringConst, Function

from .fixtures import mock_data_set


def test_no_operations():
  """
  Without any operations the schema should be equal to the given
  relation.
  """

  dataset   = mock_data_set()
  employees = dataset.get_relation('employees')
  schema    = schema_interpreter.interpret(dataset, LoadOp('employees'))

  assert_sequence_equal(
    schema.fields,
    employees.schema.fields
  )

def test_table_function():
  """
  """

  dataset   = mock_data_set()

  schema    = schema_interpreter.interpret(
    dataset, 
    Function('flatten', LoadOp('employees'), StringConst('roles'))
  )

  eq_(schema['roles'].type,'STRING')
  eq_(schema['roles'].mode,'NULLABLE')




########NEW FILE########
